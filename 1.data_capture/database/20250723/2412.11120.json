{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.11120",
    "link": "https://arxiv.org/abs/2412.11120",
    "pdf_link": "https://arxiv.org/pdf/2412.11120.pdf",
    "title": "Latent Reward: LLM-Empowered Credit Assignment in Episodic Reinforcement Learning",
    "authors": [
        "Yun Qu",
        "Yuhang Jiang",
        "Boyuan Wang",
        "Yixiu Mao",
        "Qi Cheems Wang",
        "Chang Liu",
        "Xiangyang Ji"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "2024-12-15",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 8,
    "influential_citation_count": 0,
    "institutions": [
        "Tsinghua University"
    ],
    "paper_content": "# Latent Reward: LLM-Empowered Credit Assignment in Episodic Reinforcement Learning\n\nYun $\\mathbf { Q } \\mathbf { u } ^ { * }$ , Yuhang Jiang\\*, Boyuan Wang, Yixiu Mao, Cheems Wang, Chang Liu, Xiangyang Ji\n\nTsinghua University {qy22, jiangyh19, wangby22, myx21} $@$ mails.tsinghua.edu.cn, cheemswang $@$ mail.tsinghua.edu.cn, liuchang $2 0 2 2 @$ tsinghua.edu.cn, xyji@tsinghua.edu.cn\n\n# Abstract\n\nReinforcement learning (RL) often encounters delayed and sparse feedback in real-world applications, even with only episodic rewards. Previous approaches have made some progress in reward redistribution for credit assignment but still face challenges, including training difficulties due to redundancy and ambiguous attributions stemming from overlooking the multifaceted nature of mission performance evaluation. Hopefully, Large Language Model (LLM) encompasses fruitful decision-making knowledge and provides a plausible tool for reward redistribution. Even so, deploying LLM in this case is non-trivial due to the misalignment between linguistic knowledge and the symbolic form requirement, together with inherent randomness and hallucinations in inference. To tackle these issues, we introduce LaRe, a novel LLM-empowered symbolic-based decision-making framework, to improve credit assignment. Key to LaRe is the concept of the Latent Reward, which works as a multidimensional performance evaluation, enabling more interpretable goal attainment from various perspectives and facilitating more effective reward redistribution. We examine that semantically generated code from LLM can bridge linguistic knowledge and symbolic latent rewards, as it is executable for symbolic objects. Meanwhile, we design latent reward selfverification to increase the stability and reliability of LLM inference. Theoretically, reward-irrelevant redundancy elimination in the latent reward benefits RL performance from more accurate reward estimation. Extensive experimental results witness that LaRe (i) achieves superior temporal credit assignment to SOTA methods, (ii) excels in allocating contributions among multiple agents, and (iii) outperforms policies trained with ground truth rewards for certain tasks.\n\n# Introduction\n\nEpisodic reinforcement learning is dedicated to solving problems of receiving only episodic rewards, a frequently encountered situation in real-world applications of RL, such as autonomous driving (Kiran et al. 2021) and healthcare (Zeng et al. 2022). Credit assignment (Sutton et al. 2011; Zhang, Veeriah, and Whiteson 2020), which involves assessing the contributions of single-step decisions (Ren et al. 2021), is challenging in episodic RL due to delayed and sparse feedback. Return decomposition (Arjona-Medina et al. 2019), which estimates proxy rewards by using stateaction pairs to redistribute episodic rewards, has emerged in the literature as a promising direction to remedy this issue. Subsequent works often focus on model architectures (Liu et al. 2019; Widrich et al. 2021) or human-designed regression principles (Ren et al. 2021; Lin et al. 2024), overlooking the training difficulty posed by redundant information. Zhang et al. (2024b) attempted to address this redundancy by employing a causal approach to filter out rewardirrelevant features but still struggled with the lack of semantic interpretation.\n\nA prominent observation in human problem-solving is that contribution assessments often encompass a range of qualitative and quantitative factors. For instance, soccer players‚Äô performance is evaluated not only by goals scored but also by injury prevention and coordination. Similarly, the rewards designed in RL are commonly a combination of multiple factors (Todorov, Erez, and Tassa 2012; Qu et al. 2023). Previous methods (Arjona-Medina et al. 2019; Ren et al. 2021) mainly focus solely on the values of final returns without tapping into the multifaceted nature of performance evaluation, resulting in poor semantic interpretability and ambiguous credit assignment. Recently, the demonstrated capabilities of pre-trained LLM (Achiam et al. 2023) suggest that integrating its prior knowledge for improved credit assignment is a promising solution. However, the misalignment between LLM‚Äôs linguistic knowledge and the symbolic representations required for specific tasks poses significant challenges, while the inherent randomness and hallucinations in LLM inference further diminish its effectiveness (Peng et al. 2023; Carta et al. 2023).\n\nMotivated by the urgent demand of depicting multifaceted performance evaluation, we propose a key concept for credit assignment, termed Latent Reward, where different dimensions capture various aspects of task performance while eliminating reward-irrelevant redundancy. We then devise a framework LaRe, which (i) derives semantically interpretable latent rewards by incorporating task-related priors from LLM and (ii) utilizes them to enhance reward decomposition. With the insight that semantically generated code can bridge linguistic knowledge in LLM and targets in symbolic form due to its executability for symbolic objects, LaRe presents a general paradigm for integrating LLM‚Äôs prior knowledge into symbolic tasks. Specifically, pre-trained LLM is instructed by standardized prompts to code encoding functions, which encode environment information into semantically interpretable latent rewards, eliminating the need for task-specific training. To alleviate the randomness and hallucinations in LLM reasoning, LaRe designs a self-verification mechanism for stable and reliable latent reward derivation.\n\nOur main contributions are summarized as follows:\n\n1. We propose the concept of Latent Reward with semantical interpretability and reveal the multifaceted nature of step-wise contributions by introducing it in the probabilistic model of episodic rewards, aligning with human preferences and reducing redundancy.   \n2. We devise a latent reward-based framework, LaRe, to leverage LLM‚Äôs task-related priors for more accurate and interpretable credit assignment, which paves a way for integrating LLM into symbolic-based decision-making.   \n3. We demonstrate the superiority of our method both theoretically and empirically and validate LLM‚Äôs effectiveness as a generalized information encoder for latent reward derivation in practical implementation.\n\nThe surprising phenomenon that LaRe outperforms policies trained with ground truth dense rewards for certain tasks highlights the significance of the semantically interpretable latent rewards derived through LLM‚Äôs reasoning capability. Our work reveals that merely fitting the final reward value, which primarily reflects overall performance, may be insufficient for effective reward decomposition. This suggests that RL can be further enhanced through multifaceted performance assessments informed by task-related priors.\n\n# Related Works Reward Redistribution\n\nReward redistribution seeks to transform episodic rewards into immediate and dense proxy rewards $\\hat { r } _ { t }$ , re-assigning credit for each state-action pair (Ren et al. 2021; Zhang et al. 2024b). Some previous methods focus on reward shaping (Ng, Harada, and Russell 1999; Hu et al. 2020) and intrinsic reward design (Pathak et al. 2017; Zheng et al. 2021). Return decomposition has emerged as a promising approach for tackling scenarios with severely delayed rewards. RUDDER (Arjona-Medina et al. 2019) analyzes the return-equivalent condition for invariant optimal policy and proposes return decomposition via a regression task. Subsequent works build on it by aligning demonstration sequences (Patil et al. 2020), using sequence modeling (Liu et al. 2019), or Hopfield networks (Widrich et al. 2021). Ren et al. (2021) propose randomized return decomposition to bridge between return decomposition (Efroni, Merlis, and Mannor 2021) and uniform reward redistribution (Gangwani, Zhou, and Peng 2020). Other redistribution principles have been adopted in recent works, such as causal treatment (Zhang et al. 2024b) and randomly cutting sub-trajectories (Lin et al. 2024). Recently, some methods have used attention-based approaches to decompose returns across time and agents in multi-agent settings (She,\n\nGupta, and Kochenderfer 2022; Xiao, Ramasubramanian, and Poovendran 2022; Chen et al. 2023). Despite significant progress, previous studies have neglected redundant rewardirrelevant features and the multifaceted nature of mission performance evaluation, which impede training and cause ambiguous attributions. While Zhang et al. (2024b) have acknowledged this issue to some extent, they focus solely on extracting reward-related state elements. In contrast, we propose the latent reward as a semantically interpretable multidimensional performance measurement and achieve rewardirrelevant redundancy elimination with task-related priors.\n\n# LLM-Empowered Decision Making\n\nThe remarkable capabilities of LLMs, as demonstrated across various downstream tasks (Touvron et al. 2023; Brown et al. 2020), underscores their potential as a promising solution for decision-making (Wang et al. 2023b). Some works focus on high-level control by employing LLMs as planners with predefined skills or APIs, which have proven highly successful (Liang et al. 2023; Yao et al. 2022; Shinn et al. 2023; Zhu et al. 2023; Wang et al. 2023a; Zhang et al. 2024a). However, when directly applied to low-level control without predefined skills, the misalignment between LLMs‚Äô linguistic knowledge and the symbolic states and actions required for specific tasks poses a significant challenge (Peng et al. 2023; Qu et al. 2024b). Some works address this issue by constructing text-based environments but at the cost of considerable manual effort (Du et al. 2023; Carta et al. 2023). Recently, LLMs have been integrated with RL to enhance low-level control (Cao et al. 2024). Some approaches fine-tune LLMs as policies (Carta et al. 2023; Shi et al. 2024) or use LLM for history compression (Paischer et al. 2022). Other studies (Zhang et al. 2023; Su and Zhang 2023; Shukla et al. 2023) focus on goal-conditioned RL with LLMs as subgoal selectors, but these often require predefined skills or subgoals. We seek to leverage LLMs as tools to enhance RL, aligning with LLM-based reward design methods (Kwon et al. 2023; Song et al. 2023; Wang et al. 2024). However, our method ensures a more reliable and optimized use of LLM priors by strategically designing for improved response quality and integrating them into latent rewards during the training process for optimization rather than relying on unreliable direct use.\n\n# Preliminary\n\nThe environments in reinforcement learning are generally formulated by a Markov Decision Process (MDP; Bellman (1966)), which can be defined as a tuple $\\begin{array} { l l } { \\mathcal { M } } & { = } \\end{array}$ $\\langle S , A , \\gamma , P , r \\rangle$ , where $s$ and $\\mathcal { A }$ denote the state space and action space with cardinalities $\\| S \\|$ and $\\| \\mathcal { A } \\|$ , respectively. $\\gamma \\in [ 0 , 1 )$ is the discount factor. $P ( s ^ { \\prime } | s , a )$ represents the environment‚Äôs state transition distribution, and $r ( s , a )$ denotes the reward function. The goal of reinforcement learning is to find an optimal policy $\\pi : { \\mathcal { S } }  A$ that maximizes the expected cumulative rewards with the initial state distribution $\\eta$ and episode length $T$ , which is expressed as $J ( \\pi ) =$ $\\begin{array} { r } { \\mathbb { E } \\left[ \\sum _ { t = 1 } ^ { T } \\gamma ^ { t } r \\left( s _ { t } , \\pi ( s _ { t } ) \\right) \\big | s _ { 0 } \\sim \\eta , s _ { t + 1 } \\sim P \\left( \\cdot | s _ { t } , \\pi ( s _ { t } ) \\right) \\right] . } \\end{array}$ .\n\nTimestep t = 1.,.,.T Environment Prompting self-vertificatiin Contribution Allocation S a Role Instruction You are good at understanding .. Candidate Response Œæi Encoder Standarized Latentd for all tasks Please think step by step and... Self-prompting RL Training Response JSON Format Improved Response ¬ß ‚àö Decoder Policy Train Task Instruction Pre-verification byLRD Task Description The 3D bipedal robot is designed Fredback Lateningfunction a Task information to simulatea human... extracted from Environment Obtain rt the document State-Action Form Information rpisards t=1 Pre-collected $\\textcircled{5}$ random states Env (a) (b)\n\nReal-world scenarios often pose challenges such as delayed and sparse feedback (Ke et al. 2018; Han et al. 2022). An extreme case is episodic RL, where only a non-zero reward $R ( \\tau )$ at the end of each trajectory $\\tau$ is received (Ren et al. 2021). The goal of episodic reinforcement learning is to maximize the expected episodic rewards, i.e., $J _ { e p } ( \\pi ) =$ $\\mathbb { E } \\left[ R ( \\tau ) | s _ { 0 } \\sim \\eta , a _ { t } \\sim \\pi ( \\cdot | s _ { t } ) , \\tau = \\langle s _ { 0 } , a _ { 0 } , s _ { 1 } , . . . , s _ { T } \\rangle \\right]$ . A common assumption in episodic RL is the existence of a sum-form decomposition of the episodic rewards, i.e., $\\begin{array} { r } { R ( \\tau ) = \\sum _ { t = 1 } ^ { T } r \\big ( s _ { t } , \\mathbf { \\bar { \\boldsymbol { a } } } _ { t } \\big ) } \\end{array}$ (Zhang et al. 2024b).\n\n# Latent Reward\n\nThis section elaborates on LaRe‚Äôs motivation and implementation. We explain the rationale behind the Latent Reward and analyze the underlying probabilistic model. We propose a framework $L a R e$ that leverages LLM‚Äôs reasoning and generalization capabilities while addressing the challenges of its application to incorporate task-related prior for reliably deriving the latent reward. We theoretically prove that by reducing reward-irrelevant redundancy, the latent reward enhances reward modeling and improves RL performance.\n\n# Motivation\n\nIn human endeavors, individual contributions are typically assessed from multiple angles for a comprehensive evaluation. However, current research on episodic credit assignment often focuses solely on regressing the final reward values (Arjona-Medina et al. 2019; Efroni, Merlis, and Mannor 2021), overlooking that rewards are derived from the evaluation of various implicit factors, such as costs and efficiency. Inspired by the intrinsic need to evaluate task performance from multiple perspectives, we propose the concept of the\n\nLatent Reward. Conceptually, the different dimensions of latent reward capture various aspects of task performance.\n\nFormally, the reward $r$ is a projection of the latent reward $z _ { r }$ from a space $\\mathcal { D }$ with cardinality $\\| \\mathcal D \\|$ onto the real number field $\\mathbb { R }$ . A function $f : \\mathcal { D }  \\mathbb { R }$ should exist such that each reward in the reward codomain has at least one latent reward encoding. With the introduction of the latent reward, as illustrated in Figure 1a, we construct a new probabilistic model of the episodic reward, revealing the multifaceted nature of the step-wise contribution, which better serves RL training. We have,\n\n$$\n\\begin{array} { l } { \\displaystyle p ( R | s _ { 1 : T } , a _ { 1 : T } ) = \\int p ( R , r _ { 1 : T } , z _ { r , 1 : T } | s _ { 1 : T } , a _ { 1 : T } ) \\operatorname { d } z \\operatorname { d } r } \\\\ { \\displaystyle = \\int \\left[ \\prod _ { t = 1 } ^ { T } \\underbrace { p ( r _ { t } | z _ { r , t } ) } _ { d e c o d e r f } \\underbrace { p ( z _ { r , t } | s _ { t } , a _ { t } ) } _ { e n c o d e r \\phi } \\right] p ( R | r _ { 1 : T } ) \\operatorname { d } z \\operatorname { d } r { \\quad \\mathrm { ~ ( 1 ) } } } \\end{array}\n$$\n\nwhere the $\\phi : \\mathcal { S } \\times \\mathcal { A }  \\mathcal { D }$ is the function deriving the latent reward from environment information. Intuitively, the latent reward‚Äôs multiple dimensions are obtained by compressing environmental information based on prior knowledge, thus acting as an information bottleneck (Tishby, Pereira, and Bialek 2000) tailored to the task objectives.\n\nCompared to directly estimating step-wise rewards from raw states, the latent reward offers significant advantages in interpretability, as each dimension reflects a specific aspect of task performance. Additionally, in episodic RL, where only the return of an episode provides weak signals, directly modeling rewards can be challenging. Learning from latent rewards better aligns with task objectives and simplifies network training by reducing reward-irrelevant redundancy.\n\nA naive approach is to obtain the latent reward via an information bottleneck method, which suffers from limited\n\n# Algorithm 1: LaRe\n\nInput: LLM $\\mathcal { M }$ , task information $\\mathrm { \\ t a s } { k }$ , role instruction role, candidate responses number $n$ , pre-collected random state-action pairs s¬Ø, max episodes max\n\nOutput: policy network $\\pi _ { \\boldsymbol { \\theta } }$ , reward decoder model $f _ { \\psi }$\n\n1: Initialize the policy network parameter $\\theta$ , the reward decoder model parameter $\\psi$ , and the replay buffer $\\boldsymbol { B }$ .   \n2: Obtain response $\\xi$ by executing Eq. (3) and Eq. (4).   \n3: Repeat Eq. (5) until obtaining an executable $\\phi$ .   \n4: for episode $= 1$ to $\\mathcal { N } ^ { m a x }$ do   \n5: Sample a trajectory $\\tau$ using current policy.   \n6: $B  B \\cup \\{ \\tau \\}$ . Sample a batch $B = \\{ \\tau _ { i } \\} _ { i = 1 } ^ { | B | }$ from $\\boldsymbol { B }$ .   \n7: Estimate latent reward enhanced return decomposition loss $\\mathcal { L } _ { R D } ^ { \\phi } ( \\psi )$ with Eq. (6) and update reward decoder model $f _ { \\psi }$\n\n$$\n\\psi  \\psi - \\alpha \\nabla _ { \\psi } \\mathcal { L } _ { R D } ^ { \\psi } ( \\psi )\n$$\n\n8: Perform policy optimization using any RL algorithm with predicted proxy rewards $\\hat { r } ^ { \\psi , \\breve { \\phi } } = \\stackrel { \\bf { \\bar { f } } _ { \\psi } } { f _ { \\psi } } ( \\phi ( s , a ) )$ .\n\n9: end for\n\nlinguistic interpretability and high computational costs due to separate encoder training for each task. In contrast, LLM‚Äôs pre-training has captured more compact representations in the form of tokens, facilitating better cross-task generalization. Therefore, leveraging LLM‚Äôs prior knowledge enables more efficient extraction of interpretable and multifaceted task performance metrics, the latent reward, from the redundant environmental information.\n\n# Framework\n\nLeveraging LLM‚Äôs prior knowledge and reasoning capabilities to derive latent rewards for credit assignment presents three main challenges: (1) instructing LLM to derive latent rewards for various tasks with minimal information and effort, (2) addressing the linguistic-symbolic misalignment while mitigating randomness and hallucinations in LLM inference to derive symbolic latent rewards reliably, and (3) applying latent rewards to enhance contribution allocation at each timestep. This section introduces three specifically designed components in the proposed LaRe, as demonstrated in Fig. 1b and Algorithm 1:\n\nEnvironment Prompting. To instruct LLM, we design standardized prompts easily transferable across environments, which consist of a templated role instruction $( r o l e )$ and specific task instruction $( t a s k )$ , as shown in Fig. 1b. The role instruction is consistent across tasks and guides LLM to think in a predefined manner: understand the task and state $$ identify reward-related factors $$ generate the latent reward encoding function. Only the necessary task description and state forms for a specific task are required, which can be easily extracted from the task document. The task description mainly includes the environment profile and task objective. The state forms detail the meanings of dimensions in the state space. Our design significantly reduces the burden of labor-intensive prompt engineering across tasks.\n\nLatent Reward Self-verification. Since LLM‚Äôs knowledge is encoded in language while underlying tasks are represented by symbolic states, this misalignment impedes LLM‚Äôs direct application. To effectively integrate LLM, we propose generating the latent reward encoding function using LLM‚Äôs coding capabilities. The rationale is that semantically generated code can bridge the gap between linguistic knowledge and symbolic latent rewards, as its execution is symbolic and tailored to specific tasks, as previously confirmed (Wang et al. 2024). Given the inherent randomness and hallucinations in LLM inference, inspired by recent work (Shinn et al. 2023; Ma et al. 2023), we propose a latent reward LLM generation process with self-verification, which includes self-prompting and pre-verification to enhance stability and reliability.\n\nIn the self-prompting phase, LLM $\\mathcal { M }$ firstly generates $n$ candidate responses, each including a code implementation of the latent reward encoding function:\n\n$$\n\\xi _ { 1 } , \\xi _ { 2 } , \\ldots , \\xi _ { n } \\gets { \\mathcal { M } } ( t a s k , r o l e )\n$$\n\nThese candidate responses are then fed into the prompt, and LLM is prompted to summarize an improved response:\n\n$$\n\\xi \\gets \\mathcal { M } ( t a s k , r o l e , \\xi _ { 1 \\ldots n } )\n$$\n\nRegarding pre-verification, leveraging the standardized response template, the latent reward encoding function $\\phi$ can be easily extracted from the response $\\xi$ , which takes in a state-action pair $s , a$ and outputs a latent reward $z _ { r } =$ $\\phi ( s ) = [ z _ { r } ^ { 1 } , . . . , \\bar { z } _ { r } ^ { d } ]$ . We then verify $\\phi$ with pre-collected random state-action pairs $\\bar { s }$ and provide error feedback to LLM until $\\phi$ is executable:\n\n$$\ne r r \\gets v e r i f y ( \\phi , \\bar { s } ) ; \\xi \\gets \\mathcal { M } ( t a s k , r o l e , \\xi _ { 1 . . . n } , e r r )\n$$\n\nSelf-verification significantly improves response quality by reducing randomness in identifying latent rewards and ensuring code executability. LLM‚Äôs clear linguistic responses and transparent thought processes provide high interpretability, facilitating human evaluation and manual intervention. Empirical results demonstrate that our framework achieves satisfactory results without requiring multiiteration evolutionary optimization (Ma et al. 2023).\n\nContribution Allocation. Building on the latent reward encoding function, we adopt a latent reward enhanced return decomposition, implemented based on Efroni, Merlis, and Mannor (2021). Let $f _ { \\psi }$ be a neural network decoder parameterized by $\\psi$ . The new objective of reward modeling can be formulated as:\n\n$$\n\\operatorname* { m i n } _ { \\psi } \\mathcal { L } _ { R D } ^ { \\phi } ( \\psi ) = \\mathbb { E } _ { \\tau \\sim D } \\left[ \\left( R ( \\tau ) - \\sum _ { t = 1 } ^ { T } f _ { \\psi } ( \\phi ( s _ { t } , a _ { t } ) ) \\right) ^ { 2 } \\right]\n$$\n\nProxy rewards, $\\hat { r } ^ { \\psi , \\phi } = f _ { \\psi } ( \\phi ( s , a ) )$ , derived from latent rewards, are incorporated into the RL training process. Leveraging the enhanced temporal credit assignment enabled by the latent reward‚Äôs multifaceted nature, these rewards improve RL training performance by alleviating the issue of delayed and sparse feedback.\n\nAdditionally, we empirically find that the latent reward enhances credit assignment among agents. This well\n\nLaRe-RRDu (Ours) LaRe-RD (Ours) RD RRD RRD_unbiased IRCR Diaster TD3 TD3-DR Reacher-v4 Walker2d-v4 HalfCheetah-v4 HumanoidStandup-v4   \nSR 5000 140000 5.0 10000   \n120.50 34000 8000 1020000 2000 4000 80000   \n175.50 1000 2000 60000 0 40000 \\ 0 2000 20000 0 100 200 300 400 0 200 400 600 800 0 200 400 600 800 0 200 400 600 800 Timesteps (x103) Timesteps (x103) Timesteps (x103) Timesteps (x103)\n\nmatches the intuition, as evaluating agents within a team is also a form of multifaceted credit assignment. Consequently, our method provides a practical solution for episodic multiagent RL, with reduced computational costs and improved performance, making it well-suited for real-world scenarios.\n\nIn implementations, we use GPT-4o from OpenAI API, with prompt details provided in Appendix A (Qu et al. 2024a). In practice, we have set the random variables deterministically for the sake of convenience, which is a common setting in previous works (Arjona-Medina et al. 2019).\n\n# Analysis\n\nLLM-empowered latent rewards retain semantic interpretability while reducing reward-irrelevant redundancy, which is theoretically proven to boost RL performance by learning a better reward model than the state-based methods.\n\nPrevious works commonly minimize the least squares error between the episodic rewards and the sum of predicted proxy rewards $\\hat { r } \\big ( s _ { t } , a _ { t } \\big )$ to learn reward models with raw states as inputs (Ren et al. 2021). The surjective function $\\phi ( s , a ) : \\bar { \\mathcal { S } } \\times \\mathcal { A } \\to \\mathcal { D } , \\| \\mathcal { D } \\| < \\| \\mathcal { S } \\| \\| A \\|$ reduces redundant, reward-irrelevant features from the state-action space. Theoretically, built upon Efroni, Merlis, and Mannor (2021), assuming access to a latent reward function $\\phi$ that satisfies $\\exists f ^ { * } , s . t . , r \\ : = \\ : \\hat { r } \\ : = \\ : f ^ { * } ( \\phi ( s , a ) )$ , we derive a more precise concentration bound for estimating $r$ and a tighter RL regret bound compared to the case without the latent reward. Please refer to Appendix B for the proof.\n\n# Proposition 1 (Tighter Concentration Bound of Reward).\n\nLet $\\lambda > 0$ and $A _ { k } ^ { \\phi } \\ { \\overset { d e f } { = } } \\ ( H _ { k } ^ { \\phi } ) ^ { T } H _ { k } ^ { \\phi } + \\lambda I _ { \\| { \\mathcal { D } } \\| }$ . For any $\\delta \\in \\mathbf { \\Xi }$ $( 0 , 1 )$ , with probability greater than $1 - \\delta / 1 0$ uniformly for all episode indexes $k \\geq 0$ , it holds that\n\n$$\n\\| r - \\hat { r } _ { k } ^ { \\phi } \\| _ { A _ { k } ^ { \\phi } } \\leq \\sqrt { \\frac { 1 } { 4 } T \\| \\mathcal { D } \\| \\log \\left( \\frac { 1 + k T ^ { 2 } / \\lambda } { \\delta / 1 0 } \\right) } + \\sqrt { \\lambda \\| \\mathcal { D } \\| } \\overset { d e f } { = } l _ { k } ^ { \\phi } < l _ { k } .\n$$\n\nProposition 2 (Tighter Regret Bound). For any $\\delta \\in ( 0 , 1 )$ and all episode numbers $K \\in \\mathbb { N } ^ { + }$ , the regret of $\\ell L \\rho ^ { \\phi } ( K ) \\overset { d e f } { = }$ $\\begin{array} { r } { \\sum _ { k = 1 } ^ { K } \\left( V ^ { \\ast } - V ^ { \\phi , \\pi _ { k } } \\right) } \\end{array}$ holds with probability greater than $1 - \\delta$ that,\n\n$$\n\\rho ^ { \\phi } ( K ) \\leq \\mathcal { O } \\bigg ( T \\| \\mathcal { D } \\| \\sqrt { K } \\log \\bigg ( \\frac { K T } { \\delta } \\bigg ) \\bigg ) < \\mathcal { O } \\bigg ( T \\| \\mathcal { S } \\| \\| \\mathcal { A } \\| \\sqrt { K } \\log \\bigg ( \\frac { K T } { \\delta } \\bigg ) \\bigg ) .\n$$\n\nThe concentration bound reflects the performance of the reward model by quantifying the distance between proxy rewards $\\hat { r } _ { k } ^ { \\phi }$ and true rewards $r$ , while the regret quantifies RL performance. Proposition 1 and 2 show that these bounds are proportional to $\\| \\mathcal D \\|$ , which are lower than the bound with raw state-action space. Overall, the latent reward improves reward function learning and boosts RL performance.\n\n# Experiments\n\nWe evaluate LaRe1 on two widely used benchmarks in both single-agent and multi-agent settings: MuJoCo locomotion benchmark (Todorov, Erez, and Tassa 2012) and MultiAgent Particle Environment (MPE) (Lowe et al. 2017). Additionally, we perform ablation studies and further analyses to validate LaRe‚Äôs components and assess its properties.\n\n# Experimental Setups\n\nFor MuJoCo, we adopt four tasks from Gymnasium (Towers et al. 2023). For MPE, we employ six tasks from two scenarios, Cooperative-Navigation (CN) and Predator-Prey $( P P )$ , featuring varying numbers of agents (6, 15, 30), which are based on Chen et al. (2023) with minor modifications to provide individual rewards to each agent at every step. All tasks are episodic, with a single non-zero episodic reward, equivalent to the cumulative rewards. Thus, multiagent tasks require both temporal and inter-agent credit assignment. Moreover, we evaluate LaRe in more complex scenarios from SMAC (Samvelyan et al. 2019) and a newly designed task, Triangle Area, in Appendix D and E.\n\nWe compare LaRe with SOTA return decomposition baseline algorithms: RD (Efroni, Merlis, and Mannor 2021), IRCR (Gangwani, Zhou, and Peng 2020), Diaster (Lin et al. 2024), RRD and RRD unbiased (Ren et al. 2021), as well as those designed for multi-agent settings: AREL (Xiao, Ramasubramanian, and Poovendran 2022) and STAS (Chen et al. 2023). The introduction and implementation details of these baselines are provided in Appendix C.\n\nLaRe is compatible with various RL algorithms, and we adopt TD3 (Fujimoto, Hoof, and Meger 2018) for singleagent and IPPO (Yu et al. 2022) for multi-agent as the base algorithm, consistent with prior works (Ren et al. 2021;\n\n![](images/d52c98fad80550ed37cca10ef1a7a96afee8c7164910f826f261b5728fd73ed8.jpg)  \nFigure 3: Average episode return for tasks with a varied number of agents in MPE. Notably, IPPO-DR is trained with dense rewards and LaRe w/o AD represents LaRe without credit assignment among agents.\n\nChen et al. 2023). Each algorithm runs on five random seeds, with the mean performance and standard deviation reported. Further details and results are available in the Appendix.\n\n# The Superiority of LaRe\n\nSingle-Agent. To verify the compatibility of our method with various return decomposition algorithms, we implement two variants, LaRe-RD and LaRe-RRDu, based on RD and RRD-unbiased, respectively. As shown in Fig. 2, the poor performance of TD3 and IRCR highlights the importance of assigning individual credits. Our method, LaRe, consistently outperforms SOTA baselines on MuJoCo tasks, demonstrating higher sample efficiency and better convergence. Both variants of LaRe surpass the corresponding baselines, highlighting the efficacy of semantically interpretable latent rewards in credit assignment. The effectiveness of LaRe in tasks with large state spaces significantly supports our analysis, underscoring the significance of redundancy elimination with task-related priors in the latent reward and explaining the poor performance of baselines.\n\nMulti-Agent. Fig. 3 depicts comprehensive comparisons between LaRe and various baselines in MPE. LaRe is implemented based on RD and demonstrates superior performance across tasks with different numbers of agents compared to all SOTA baselines, confirming the efficacy of latent rewards in temporal credit assignment on multi-agent tasks. We also include a variant LaRe w/o AD (without agent decomposition), where the proxy rewards of different agents are averaged at the same time step. The significant performance drop highlights the necessity of credit assignment at the agent level and the effectiveness of LaRe in this regard. We believe the semantically interpretable latent rewards account for this since assessing different agents‚Äô contributions is also intuitively a form of multifaceted credit assignment. AREL and STAS perform relatively poorly, particularly as the number of agents increases, likely because reward-irrelevant items in the original state interfere significantly with attention-based credit assignment.\n\nComparable with Dense Rewards. We include TD3-DR and IPPO-DR in MuJoCo and MPE, respectively, training with ground truth dense rewards. Remarkably, LaRe‚Äôs performance is comparable to or even exceeds theirs despite not relying on manually designed rewards. The reason is that while ground true rewards reflect agents‚Äô performance levels, overall stability is still affected by implicit factors like costs and efficiency, which are adequately captured by our proposed LLM-based latent reward. This finding emphasizes leveraging task-related prior information for multifaceted performance evaluation can further enhance RL performance beyond merely relying on final reward values.\n\n# Delving into Latent Rewards\n\nWe conduct experiments to analyze the specific nature of the latent rewards and the reason for their superior performance.\n\nSemantic Analysis of Multifaceted Measurement. We analyze the LLM-generated latent reward functions and use HumanoidStandup- $\\cdot \\nu 4$ as an instance. The task objective is to have the humanoid robot stand up and maintain balance by applying torques to hinges (Towers et al. 2023). As shown in Fig. 4(b), LLM demonstrates a correct understanding of the task and derives latent rewards as interpretable performance measures across multiple dimensions, such as height and safe control, which align with the ground truth (GT) reward function. Additionally, LLM considers stability, which better aligns with the task‚Äôs objectives, further elucidating its superior performance compared to baselines with dense rewards. Further details can be found in Appendix A.\n\nReduced Reward-irrelevant Redundancy. We calculate the Pearson correlation coefficient (Cohen et al. 2009) between each dimension of original states or LLM-generated latent rewards and ground truth dense rewards. As shown in Table 1, latent rewards are tighter correlated with ground\n\nLaRe vs LaRe RD vs LaRe LaRe vs RD LLM-Generated 40 39.84 ToGrTso TorsSotabilitJyoint 0 Prey 10.08 3.77 height vels angles -420 -16.85 Torso Torso Angular -25.42 norms stability vels -60 -51.8 (a) (b) (c)\n\nTable 1: corr denotes the average Pearson correlation coefficient. dims represents the average number of dimensions of original states or latent rewards. Additionally, we record the average execution rate exe rate of LLM-generated latent reward functions without pre-verification (w/o PV).   \n\n<html><body><table><tr><td rowspan=\"2\">Tasks</td><td colspan=\"2\">StateSor (dtimntRewards</td><td colspan=\"2\">wexe-rat/Pv</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>CN (6 agents)</td><td>0.02 (26)</td><td>0.50 (5.6)</td><td>0%</td><td>100%</td></tr><tr><td>PP (6 agents)</td><td>0.01 (28)</td><td>0.12 (5.4)</td><td>20%</td><td>100%</td></tr><tr><td>HalfCheetah-v4</td><td>0.22 (17)</td><td>0.53 (4.8)</td><td>40%</td><td>100%</td></tr><tr><td>HumanoidStandup-v4</td><td>0.20 (376)</td><td>0.49 (5.6)</td><td>40%</td><td>100%</td></tr></table></body></html>\n\ntruth rewards across tasks. Meanwhile, latent rewards‚Äô dimensions are significantly fewer than those of original states. The results confirm that the latent reward reduces the reward-irrelevant redundancy with task-related priors, improving reward prediction, as shown in Appendix F.5.\n\nAlgorithm Agnostic. Notably, latent rewards for estimating proxy rewards are transferable to various RL backbones. This property ensures LaRe‚Äôs application prospects, opening up possibilities to combine with real-world approaches. We conduct detailed experiments in Appendix F.3.\n\nLaRe (Ours) LaRe w/o RM LaRe w/o SP VIB TD3   \nGU Walker2d-v4 HumanoidStandup-v4 5000 125000 4000 100000 3000 75000 2000   \n1000 50000 25000 0 200 400 600 800 0 200 400 600 800 Timesteps (x103) Timesteps (x103)\n\nCompatible with Heterogeneous Agents. Latent rewards can help re-assign credits among heterogeneous agents, even in competitive scenarios. Like Lowe et al. (2017), we jointly train policies for competitive predators and preys in task Predator-Prey. We have the policies trained by LaRe and RD respectively compete, with preys and predators controlled by different ones. As shown in Fig. 4(c), LaRe learns superior policies for both predators and preys compared to RD, suggesting enhanced credit assignment in competitive multi-agent scenarios. This advantage can be attributed to the multifaceted nature of the latent reward.\n\n# Ablation Studies\n\nReward Attributes in Latent Reward. To distinguish latent rewards from mere state representation, we conduct an ablation study by removing the reward decoder model, termed ‚ÄúLaRe w/o RM‚Äù, which estimates proxy rewards by summarizing latent rewards with a sign: $\\hat { r } _ { s i g n } =$ $\\begin{array} { r } { \\sum _ { i = 1 } ^ { d } \\mathrm { s i g n } ( z _ { r } ^ { i } ) \\cdot z _ { r } ^ { i } } \\end{array}$ . The signs are obtained by minimizing the estimation loss between episodic rewards and the sum of proxy rewards. As shown in Figure 5, this significant simplification outperforms the baseline with episodic rewards (TD3), confirming that the latent reward possesses genuine reward attributes rather than just representing states.\n\nSelf-Verification. We propose Self-prompting (SP) and pre-verification (PV) to reduce randomness and hallucinations in LLM inference. The superior performance shown in Fig. 5 indicates that SP effectively reduces randomness in LLM inference, resulting in improved LLM responses and RL performance. In Table. 1, we compare the execution rate of LLM-generated latent reward encoding functions with and without pre-verification, highlighting the significance of pre-verification. The results validate the efficacy of our designs for integrating LLMs into RL tasks. Appendix F.1 examines the impact of the number of candidate responses.\n\nVariational Information Bottleneck. We propose an alternative method to utilize the Variational Information Bottleneck (VIB) (Alemi et al. 2017) to derive latent rewards. However, as shown in Fig. 5 and Appendix F.2, LaRe significantly outperforms VIB, which underscores the importance of LLMs serving as generalized encoders of environmental information to integrate task-related priors effectively. We compare LaRe with LLM reward design in Appendix F.4.\n\n# Conclusion\n\nIn this work, we present LaRe, a LLM-empowered framework for credit assignment in episodic reinforcement learning with task-related prior. The framework is centered on the latent reward, whose dimensions reflect distinct aspects of task performance evaluation. We utilize LLM‚Äôs coding abilities to address the linguistic-symbolic misalignment in integrating LLM into RL tasks and propose self-verification to ensure reliable LLM inference. This work (i) addresses previously overlooked research questions, including training difficulties caused by redundancy and the multifaceted nature of mission performance evaluation; (ii) develops a practical algorithm that achieves superior performance both theoretically and empirically; (iii) advances the integration of LLM prior knowledge into RL through semantically interpretable multifaceted performance evaluation.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáËß£ÂÜ≥‰∫ÜÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâ‰∏≠Áî±‰∫éÂª∂ËøüÂíåÁ®ÄÁñèÂèçÈ¶àÂØºËá¥ÁöÑ‰ø°Áî®ÂàÜÈÖçÔºàcredit assignmentÔºâÈóÆÈ¢òÔºåÁâπÂà´ÊòØÂú®‰ªÖÊé•Êî∂ÁâáÊÆµÂ•ñÂä±Ôºàepisodic rewardsÔºâÁöÑÂú∫ÊôØ‰∏ã„ÄÇËøô‰∏ÄÈóÆÈ¢òÂú®Áé∞ÂÆû‰∏ñÁïåÁöÑRLÂ∫îÁî®‰∏≠Â∞§‰∏∫Â∏∏ËßÅÔºåÂ¶ÇËá™Âä®È©æÈ©∂ÂíåÂåªÁñó‰øùÂÅ•È¢ÜÂüü„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Â•ñÂä±ÂÜçÂàÜÈÖçÊñπÈù¢ÂèñÂæó‰∫Ü‰∏Ä‰∫õËøõÂ±ïÔºå‰ΩÜ‰ªçÈù¢‰∏¥ÂÜó‰Ωô‰ø°ÊÅØÂíåÊ®°Á≥äÂΩíÂõ†ÁöÑÊåëÊàòÔºåËøô‰∫õÈóÆÈ¢òÊ∫ê‰∫éÂøΩËßÜ‰∫Ü‰ªªÂä°Áª©ÊïàËØÑ‰º∞ÁöÑÂ§öÈù¢ÊÄß„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫LaReÁöÑÊñ∞ÂûãÊ°ÜÊû∂ÔºåÂà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÂÖàÈ™åÁü•ËØÜÔºåÈÄöËøáÊΩúÂú®Â•ñÂä±ÔºàLatent RewardÔºâÁöÑÊ¶ÇÂøµÔºåÂÆûÁé∞Â§öÁª¥Â∫¶ÁöÑÊÄßËÉΩËØÑ‰º∞ÂíåÊõ¥ÂáÜÁ°ÆÁöÑ‰ø°Áî®ÂàÜÈÖç„ÄÇLaReÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÈÄöËøáLLMÁîüÊàêÁöÑËØ≠‰πâËß£ÈáäÊÄßÊΩúÂú®Â•ñÂä±ÔºåÊ∂àÈô§‰∏éÂ•ñÂä±Êó†ÂÖ≥ÁöÑÂÜó‰Ωô‰ø°ÊÅØÔºå‰ªéËÄåÊõ¥ÂáÜÁ°ÆÂú∞ËØÑ‰º∞ÊØè‰∏ÄÊ≠•ÂÜ≥Á≠ñÁöÑË¥°ÁåÆ„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   ÊèêÂá∫‰∫ÜÊΩúÂú®Â•ñÂä±ÁöÑÊ¶ÇÂøµÔºåÂÖ∑ÊúâËØ≠‰πâÂèØËß£ÈáäÊÄßÔºåÂπ∂ÈÄöËøáLLMÁöÑÂÖàÈ™åÁü•ËØÜÂáèÂ∞ë‰∫ÜÂ•ñÂä±Êó†ÂÖ≥ÁöÑÂÜó‰Ωô‰ø°ÊÅØ„ÄÇ\\n> *   ËÆæËÆ°‰∫ÜLaReÊ°ÜÊû∂ÔºåÈÄöËøáLLMÁîüÊàêÁöÑËØ≠‰πâ‰ª£Á†ÅÂ∞ÜËØ≠Ë®ÄÁü•ËØÜ‰∏éÁ¨¶Âè∑‰ªªÂä°ÂØπÈΩêÔºåÂπ∂ÂºïÂÖ•Ëá™È™åËØÅÊú∫Âà∂ÊèêÈ´òÊé®ÁêÜÁöÑÁ®≥ÂÆöÊÄß„ÄÇ\\n> *   ÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLaReÂú®ÂçïÊô∫ËÉΩ‰ΩìÂíåÂ§öÊô∫ËÉΩ‰Ωì‰ªªÂä°‰∏≠ÂùáÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊüê‰∫õ‰ªªÂä°‰∏≠ÁîöËá≥Ë∂ÖËøá‰∫Ü‰ΩøÁî®ÁúüÂÆûÂØÜÈõÜÂ•ñÂä±ËÆ≠ÁªÉÁöÑÊ®°ÂûãÔºàÂ¶ÇTD3-DRÂíåIPPO-DRÔºâ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   ÊΩúÂú®Â•ñÂä±ÈÄöËøáÂ§öÁª¥Â∫¶ÁöÑÊÄßËÉΩËØÑ‰º∞ÔºåÊçïÊçâ‰ªªÂä°ÁöÑ‰∏çÂêåÊñπÈù¢Ôºå‰ªéËÄåÊèê‰æõÊõ¥ÂÖ®Èù¢ÁöÑ‰ø°Áî®ÂàÜÈÖç„ÄÇLLMÁöÑÂÖàÈ™åÁü•ËØÜÁî®‰∫éÁîüÊàêËØ≠‰πâÂèØËß£ÈáäÁöÑÊΩúÂú®Â•ñÂä±ÔºåÂáèÂ∞ëÂÜó‰Ωô‰ø°ÊÅØ„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** ÂÖàÂâçÊñπÊ≥ï‰∏ªË¶ÅÂÖ≥Ê≥®ÊúÄÁªàÂ•ñÂä±ÂÄºÁöÑÂõûÂΩíÔºåÂøΩÁï•‰∫ÜÊÄßËÉΩËØÑ‰º∞ÁöÑÂ§öÁª¥ÊÄßÔºåÂØºËá¥ËØ≠‰πâÂèØËß£ÈáäÊÄßÂ∑ÆÂíåÊ®°Á≥äÁöÑ‰ø°Áî®ÂàÜÈÖç„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** ÈÄöËøáLLMÁîüÊàêÁöÑËØ≠‰πâ‰ª£Á†ÅÂ∞ÜËØ≠Ë®ÄÁü•ËØÜ‰∏éÁ¨¶Âè∑‰ªªÂä°ÂØπÈΩêÔºåÂπ∂ÂºïÂÖ•Ëá™È™åËØÅÊú∫Âà∂ÊèêÈ´òÊé®ÁêÜÁöÑÁ®≥ÂÆöÊÄß„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> *   1. **ÁéØÂ¢ÉÊèêÁ§∫ÔºàEnvironment PromptingÔºâÔºö** ËÆæËÆ°Ê†áÂáÜÂåñÊèêÁ§∫ÔºåÊåáÂØºLLMÁîüÊàêÊΩúÂú®Â•ñÂä±ÁºñÁ†ÅÂáΩÊï∞„ÄÇ\\n> *   2. **ÊΩúÂú®Â•ñÂä±Ëá™È™åËØÅÔºàLatent Reward Self-verificationÔºâÔºö** ÈÄöËøáËá™ÊèêÁ§∫ÂíåÈ¢ÑÈ™åËØÅÊú∫Âà∂ÔºåÁ°Æ‰øùLLMÁîüÊàêÁöÑ‰ª£Á†ÅÁ®≥ÂÆöÂèØÈù†„ÄÇ\\n> *   3. **Ë¥°ÁåÆÂàÜÈÖçÔºàContribution AllocationÔºâÔºö** ‰ΩøÁî®ÊΩúÂú®Â•ñÂä±Â¢ûÂº∫ÁöÑÂõûÊä•ÂàÜËß£ÊñπÊ≥ïÔºå‰ºòÂåñRLËÆ≠ÁªÉËøáÁ®ã„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   RD (Efroni, Merlis, and Mannor 2021)\\n> *   IRCR (Gangwani, Zhou, and Peng 2020)\\n> *   Diaster (Lin et al. 2024)\\n> *   RRD and RRD unbiased (Ren et al. 2021)\\n> *   AREL (Xiao, Ramasubramanian, and Poovendran 2022)\\n> *   STAS (Chen et al. 2023)\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®‰ªªÂä°ÊÄßËÉΩ‰∏äÔºö** LaReÂú®MuJoCoÂíåMPE‰ªªÂä°‰∏≠ÂùáÊòæËëó‰ºò‰∫éÊâÄÊúâÂü∫Á∫øÊ®°ÂûãÔºåË°®Áé∞Âá∫Êõ¥È´òÁöÑÊ†∑Êú¨ÊïàÁéáÂíåÊõ¥Â•ΩÁöÑÊî∂ÊïõÊÄß„ÄÇ‰æãÂ¶ÇÔºåÂú®HumanoidStandup-v4‰ªªÂä°‰∏≠ÔºåLaReÁöÑÊúÄÁªàÂõûÊä•ËææÂà∞5000ÔºåËøúÈ´ò‰∫éRDÔºà3000ÔºâÂíåIRCRÔºà2000Ôºâ„ÄÇ\\n> *   **Âú®Â§öÊô∫ËÉΩ‰Ωì‰ªªÂä°‰∏äÔºö** LaReÂú®6„ÄÅ15Âíå30‰∏™Êô∫ËÉΩ‰ΩìÁöÑ‰ªªÂä°‰∏≠ÂùáË°®Áé∞Âá∫Ëâ≤Ôºå‰ºò‰∫éÂÖ∂‰ªñÂü∫Á∫øÊñπÊ≥ïÔºåÂ∞§ÂÖ∂ÊòØÂú®‰ø°Áî®ÂàÜÈÖçÊñπÈù¢„ÄÇ\\n> *   **‰∏éÂØÜÈõÜÂ•ñÂä±Ê®°ÂûãÁöÑÂØπÊØîÔºö** LaReÂú®Êüê‰∫õ‰ªªÂä°‰∏≠ÁîöËá≥Ë∂ÖËøá‰∫Ü‰ΩøÁî®ÁúüÂÆûÂØÜÈõÜÂ•ñÂä±ËÆ≠ÁªÉÁöÑÊ®°ÂûãÔºàÂ¶ÇTD3-DRÂíåIPPO-DRÔºâÔºåË°®ÊòéÂÖ∂ÊΩúÂú®Â•ñÂä±ÁöÑÂ§öÁª¥ÊÄßËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâ‰ªªÂä°ÊÄßËÉΩ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n> **ÊèêÂèñ‰∏éÊ†ºÂºèÂåñË¶ÅÊ±Ç**\\n> *   Âº∫ÂåñÂ≠¶‰π† (Reinforcement Learning, RL)\\n> *   ‰ø°Áî®ÂàÜÈÖç (Credit Assignment, N/A)\\n> *   ÊΩúÂú®Â•ñÂä± (Latent Reward, N/A)\\n> *   Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã (Large Language Model, LLM)\\n> *   ËØ≠‰πâÂèØËß£ÈáäÊÄß (Semantic Interpretability, N/A)\\n> *   Â§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü (Multi-Agent System, MAS)\\n> *   Â•ñÂä±ÂÜçÂàÜÈÖç (Reward Redistribution, N/A)\\n> *   Ëá™È™åËØÅÊú∫Âà∂ (Self-Verification Mechanism, N/A)\"\n}\n```"
}