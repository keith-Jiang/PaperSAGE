{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.13525",
    "link": "https://arxiv.org/abs/2412.13525",
    "pdf_link": "https://arxiv.org/pdf/2412.13525.pdf",
    "title": "Hybrid Data-Free Knowledge Distillation",
    "authors": [
        "Jialiang Tang",
        "Shuo Chen",
        "Chen Gong"
    ],
    "categories": [
        "cs.CV"
    ],
    "publication_date": "2024-12-18",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Nanjing University of Science and Technology",
        "Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education",
        "Jiangsu Key Laboratory of Image and Video Understanding for Social Security",
        "Center for Advanced Intelligence Project, RIKEN",
        "Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University",
        "Shanghai Jiao Tong University"
    ],
    "paper_content": "# Hybrid Data-Free Knowledge Distillation\n\nJialiang $\\mathbf { T a n g ^ { 1 , 2 , 3 } }$ , Shuo Chen4\\*, Chen Gong5\\*\n\n1School of Computer Science and Engineering, Nanjing University of Science and Technology, China   \n2Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, China 3Jiangsu Key Laboratory of Image and Video Understanding for Social Security, China 4Center for Advanced Intelligence Project, RIKEN, Japan   \n5Department of Automation, Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, China tangjialiang $@$ njust.edu.cn, shuo.chen.ya@riken.jp, chen.gong $@$ sjtu.edu.cn\n\n# Abstract\n\nData-free knowledge distillation aims to learn a compact student network from a pre-trained large teacher network without using the original training data of the teacher network. Existing collection-based and generation-based methods train student networks by collecting massive real examples and generating synthetic examples, respectively. However, they inevitably become weak in practical scenarios due to the difficulties in gathering or emulating sufficient real-world data. To solve this problem, we propose a novel method called Hybrid Data-Free Distillation (HiDFD), which leverages only a small amount of collected data as well as generates sufficient examples for training student networks. Our HiDFD comprises two primary modules, i.e., the teacher-guided generation and student distillation. The teacher-guided generation module guides a Generative Adversarial Network (GAN) by the teacher network to produce high-quality synthetic examples from very few real-world collected examples. Specifically, we design a feature integration mechanism to prevent the GAN from overfitting and facilitate the reliable representation learning from the teacher network. Meanwhile, we drive a category frequency smoothing technique via the teacher network to balance the generative training of each category. In the student distillation module, we explore a data inflation strategy to properly utilize a blend of real and synthetic data to train the student network via a classifier-sharing-based feature alignment technique. Intensive experiments across multiple benchmarks demonstrate that our HiDFD can achieve state-of-the-art performance using 120 times less collected data than existing methods.\n\n# Code â€” https://github.com/tangjialiang97/HiDFD\n\n# Introduction\n\nThe success of Deep Neural Networks (DNNs) (He et al. 2016; Hao et al. 2024) is usually accompanied by significant computational and storage demands, which hinders their deployment on practical resource-limited devices. Knowledge Distillation (KD) (Hinton, Vinyals, and Dean 2015; Miles and Mikolajczyk 2024) has served as an effective compression technology that transfers knowledge from a complex pre-trained teacher network to improve the performance of a lightweight student network. However, in practice, the training data of the teacher network is usually inaccessible due to privacy concerns and only the pre-trained teacher network itself can be used to learn the student network. This is because users may prefer sharing a pre-trained black box DNN rather than disclosing their sensitive data. In such cases, vanilla KD methods can hardly train a reliable student network owing to the absence of original training data. To address this issue, various Data-Free Knowledge Distillation (DFKD) approaches (Binici et al. 2022; Chen et al. 2019, 2021b; Tang et al. 2023) have been developed to enable training the student network without using any original data.\n\nAmong existing DFKD methods, collection-based approaches (Chen et al. 2021b; Tang et al. 2023) can achieve satisfactory performance by amassing numerous real examples to train the student network. However, it is still difficult for the collection-based methods to train a reliable student network in practical tasks, e.g., medical image classification because gathering sufficient training examples can be challenging. On the other hand, generation-based methods (Yin et al. 2020; Chen et al. 2019) leverage the teacher network to guide a generative model (Creswell et al. 2018) in producing fake examples, thereby successfully training the student network without reliance on real examples. Nevertheless, the synthesized examples may exhibit low quality in the absence of real data supervision, leading to suboptimal student performance, especially for many challenging recognition tasks on ImageNet (Deng et al. 2009). The inherent constraints of both collection-based and generation-based DFKD methods prompt an essential question: Can we train an effective generative model only using a small number of collected examples and then learn reliable student networks with the hybrid data comprising both collected and synthetic examples?\n\nTo answer the above question under the practical data-free distillation scenario, we need a generative model that not only possesses powerful generative capabilities but also has the ability to acquire valuable knowledge from the teacher network. Recent studies (Cui et al. 2023; Rangwani, Mopuri, and Babu 2021) suggest that the Generative Adversarial Network (GAN) (Mirza and Osindero 2014) can easily learn from pre-trained models and then generate high-quality synthetic examples, so we employ this great approach as our generative module. The standard GAN consists of a generator and a discriminator trained in an adversarial manner, where the generator attempts to produce fake examples to deceive the discriminator while the discriminator strives to distinguish between real and fake examples. However, the collected data in practice tasks like medical image classification has two inherent characteristics that may impede the training of the GAN, namely: 1) Limited data quantity, as capturing medical images requires expensive and complex equipment; and 2) Imbalanced class distribution, where certain diseases (e.g., â€œvascular lesionsâ€) are more rarely than others (e.g., â€œnevusâ€). When training on the collected data with limited examples and imbalanced class distribution, the discriminator is susceptible to overfitting (Huang et al. 2022; Jiang et al. 2021). It implies that the discriminator tends to memorize all real examples and almost perfectly distinguish them from fake examples, resulting in the disappearance of the gradient for the generator. Moreover, the generator training is dominated by a few classes occupying the majority of examples, which prevents it from generating diverse examples. Therefore, it is critical to overcome the overfitting issue of discriminator and data imbalance issue of generator when training with scarce collected examples.\n\nIn this paper, we propose a novel approach called Hybrid Data-Free Distillation (HiDFD), which learns reliable student networks on the hybrid data comprising synthetic examples and very few real collected examples. Our HiDFD is composed of two pivotal modules of teacher-guided generation and student distillation. In the teacher-guided generation module, we aim to solve the critical issues in the GAN mentioned above, and thus generating high-quality synthetic examples. Specifically, we propose a feature integration mechanism to aggregate the features of both the collected and synthetic examples between the teacher network and GAN. Such an integration mechanism not only mitigates the overfitting of the discriminator, which forcibly distinguishes those closely resembling examples, but also transfers valuable representations to guide the discriminator to capture category dependencies. Meanwhile, we also develop a new technique called category frequency smoothing to alleviate the imbalanced training of the generator. In the student distillation module, we develop a data inflation operation to adjust the contribution of collected examples among the hybrid data when training the student network. Finally, we design a classifier-sharing-based strategy to closely align the features of student network with those of teacher network to enhance student performance. Thanks to effectively transferring knowledge from the teacher network to both the GAN and student network, our HiDFD can successfully train reliable student networks using very few collected realworld examples. The contributions of our HiDFD are summarized as follows:\n\nâ€¢ By considering the difficulties in gathering or emulating real-world data, we propose a novel data-free distillation method called HiDFD, which only requires a small number of collected data to generate high-quality synthetic examples for training the student network. â€¢ We design a teacher-guided generation module to effectively tackle the critical issues of discriminator overfitting and imbalanced learning in generating synthetic ex\n\namples, which empowers the distillation module to learn reliable student networks from the teacher network. â€¢ Our HiDFD can achieve State-Of-The-Art (SOTA) performance using only 1/120 (5,000/600,000) of examples required by existing collection-based DFKD methods.\n\n# Related Works\n\nIn this section, we review the relevant works, including knowledge distillation and generative models.\n\n# Knowledge Distillation\n\nTraditional KD methods (Chen et al. 2021a; Li et al. 2023) learn a compact and reliable student network by encouraging it to mimic a variety of knowledge, i.e., softened logits (Zhao et al. 2022a), intermediate features (Chen et al. 2022), and representation relationships (Peng et al. 2019), from a large teacher network using ample original training data. However, in practical applications, these approaches might be ineffective because the original data is usually unavailable due to privacy concerns.\n\nTo address the above issue, generation-based (Tran et al. 2024; Wang et al. 2024a,b) and collection-based (Chen et al. 2021b; Tang et al. 2023) DFKD methods have been proposed to train student networks using synthetic and collected data, respectively. The generation-based methods utilize the teacher network to guide a generator in producing examples from statistics in the teacher network or random noise. However, the resulting student network still achieves suboptimal performance due to the flawed synthetic examples. Conversely, collection-based methods assume that there are numerous easily accessible examples in the real-world, and they acquire an oversized collected data (e.g., 600,000 examples on CIFAR10) to train the student network. In practical tasks, it is hard to gather so many examples, and thus they still fail to train reliable student networks.\n\nIn this paper, our HiDFD only utilizes a small collected data that contains fewer examples than the original data, which initially guides the GAN in training on such collected data by the teacher and then trains the student on adequate data composed of the synthetic and collected examples.\n\n# Generative Models\n\nRecent advances in generative models, including Variational AutoEncoders (Kingma and Welling 2013; Zhao, Song, and Ermon 2019), diffusion models (Ho, Jain, and Abbeel 2020; Mei and Patel 2023), and GAN (Hou et al. 2021; Mirza and Osindero 2014) have significantly propelled the data generation. This paper focuses on the powerful GAN due to its ability to learn from pre-trained models (Cui et al. 2023; Rangwani, Mopuri, and Babu 2021). The traditional GAN (Goodfellow et al. 2020) consists of a generator and a discriminator, where the generator produces fake examples to deceive the discriminator, and the discriminator tries to accurately distinguish between real and fake examples. Recently, Auxiliary Discriminative Classifier GAN (ADCGAN) (Hou et al. 2022) captures dependencies between generated examples and class labels by encouraging the discriminator to classify synthetic examples into specific categories, which effectively improves the quality of synthetic data.\n\nIn our method, we hope the GAN can produce highquality synthetic examples that are easily classifiable, and thus training a precise student network. Therefore, we adopt ADCGAN as the foundational generative model. The ADCGAN composed of a generator $\\mathcal { N } _ { \\mathrm { G } } : \\mathcal { Z } \\times \\mathcal { Y }  \\mathcal { X }$ maps a noise-label pair $( z , y )$ to a fake example $\\mathcal { N } _ { \\mathrm { G } } ( z , y ) \\in \\mathcal { X }$ that can be precisely predicted as $y \\in \\mathcal { V }$ ; and a discriminator $\\mathcal { N } _ { \\mathrm { D } } : \\mathcal { X } \\overset { \\vartriangle } {  } \\{ 0 , \\dot { 1 } \\}$ determines whether the input example is real (i.e., 1) or fake $( i . e . , 0 )$ , which also has a classifier $\\Psi _ { \\mathrm { D } } : \\mathcal { X }  \\mathcal { Y } ^ { + } \\cup \\mathcal { Y } ^ { - }$ $y ^ { + } \\in \\mathcal { V } ^ { + }$ and $y ^ { - } \\in \\mathcal { V } ^ { - }$ denote the labels for real and fake examples, respectively). Mathematically, the objective functions for the discriminator and generator in the ADCGAN are defined as $\\mathcal { L } _ { \\mathrm { a d c \\mathrm { _ { - } d } } }$ and $\\mathcal { L } _ { \\mathrm { a d c . g } }$ , respectively, as follows:\n\n$$\n\\left\\{ \\begin{array} { l l } { \\mathcal { L } _ { \\mathrm { a d c \\mathrm { - } } \\mathrm { d } } = - \\mathcal { L } _ { \\mathrm { d } } + \\mathbb { E } _ { { \\boldsymbol x } , { \\boldsymbol y } \\sim P _ { X , Y } } [ \\log \\Psi _ { \\mathrm { D } } ( { \\boldsymbol y } ^ { + } \\mid { \\boldsymbol x } ) ] } \\\\ { \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad + \\mathbb { E } _ { { \\boldsymbol x } , { \\boldsymbol y } \\sim Q _ { X , Y } } [ \\log \\Psi _ { \\mathrm { D } } ( { \\boldsymbol y } ^ { - } \\mid { \\boldsymbol x } ) ] , } \\\\ { \\mathcal { L } _ { \\mathrm { a d c \\mathrm { - } } \\mathrm { g } } = \\mathrm { ~ \\mathcal { L } _ { \\mathrm { g } } - \\mathbb { E } _ { { \\boldsymbol x } , { \\boldsymbol y } \\sim Q _ { X , Y } } [ \\log \\Psi _ { \\mathrm { D } } ( { \\boldsymbol y } ^ { + } \\mid { \\boldsymbol x } ) ] ~ } } \\\\ { \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad + \\mathbb { E } _ { { \\boldsymbol x } , { \\boldsymbol y } \\sim Q _ { X , Y } } [ \\log \\Psi _ { \\mathrm { D } } ( { \\boldsymbol y } ^ { - } \\mid { \\boldsymbol x } ) ] , } \\end{array} \\right.\n$$\n\n# Approach\n\nData-free distillation aims to train a compact student network $\\mathcal { N } _ { \\mathrm { S } }$ using a pre-trained teacher network $\\mathcal { N } _ { \\mathrm { T } }$ without accessing the teacherâ€™s original training data $\\mathcal { D } _ { \\mathrm { o } }$ . Both $\\mathcal { N } _ { \\mathrm { T } }$ and $\\mathcal { N } _ { \\mathrm { S } }$ consist of a feature extractor $\\Phi$ and classifier $\\Psi$ , where the subscripts $\\mathrm { \\Delta T }$ and S indicate â€œteacherâ€ and â€œstudentâ€, respectively. Existing collection-based DFKD methods (Tang et al. 2023) usually rely on the collected data $\\mathcal { D } _ { \\mathrm { c } }$ with overwhelming examples searched based on the categories of the original data. Here, the data amount $| \\mathcal { D } _ { \\mathrm { c } } | \\gg | \\mathcal { D } _ { \\mathrm { o } } |$ , which is hard to satisfy in practice tasks. To overcome this limitation, we propose a more practical method that only requires a small number of collected examples for DFKD, i.e., the data amount $| \\mathcal { D } _ { \\mathrm { c } } | \\leq | \\mathcal { D } _ { \\mathrm { o } } |$ . To this end, we develop a hybrid framework to generate abundant synthetic examples from very few collected examples, and then we integrate them as the hybrid data for training the reliable student network.\n\n# Motivation of the Hybrid Learning\n\nFormally, we denote the distribution of the collected data $\\mathcal { D } _ { \\mathrm { c } }$ and synthetic data $\\mathcal { D } _ { \\mathrm { s } }$ as $P$ and $Q$ , respectively, while the distribution of the hybrid data $\\mathcal { D } = \\mathcal { D } _ { \\mathrm { c } } \\cup \\mathcal { D } _ { \\mathrm { s } }$ is represented as $U = \\alpha P + ( 1 - \\alpha ) Q$ . Here $\\alpha = \\left| \\mathcal { D } _ { \\mathrm { c } } \\right| / \\left( \\left| \\mathcal { D } _ { \\mathrm { c } } \\right| + \\left| \\mathcal { D } _ { \\mathrm { s } } \\right| \\right)$ represents the proportion of collected examples in the hybrid data. In general, the synthetic and collected examples usually exhibit a significant distribution gap. This can cause substantial fluctuations during the training of the student network on hybrid data, ultimately leading to poor performance (Wang, Zhang, and Wang 2024). Therefore, it is essential to align the distribution of synthetic data with that of collected data, thereby forming reliable hybrid data. Here, the synthetic data is generated under the supervision of the collected data, so we assume that synthetic data, collected data, and hybrid data have the same support set $\\chi$ . Then, the distribution gap between the reliable hybrid data and synthetic data can be characterized by the Total Variation Distance (TVD), which is defined as\n\n$$\n\\mathrm { T V D } ( U , Q ) = \\frac { 1 } { 2 } \\sum _ { { \\pmb x } \\in { \\pmb X } } | U ( { \\pmb x } ) - Q ( { \\pmb x } ) | ,\n$$\n\nwhere $U ( \\pmb { x } ) ~ \\in ~ ( 0 , 1 )$ and $Q ( \\pmb { x } ) \\ \\in \\ ( 0 , 1 )$ measure the distribution probability of $\\scriptstyle { \\mathbf { { \\vec { x } } } }$ in the hybrid and synthetic data, respectively. Here $\\mathrm { T V D } ( \\cdot , \\cdot ) \\geq 0$ , and $\\mathrm { T V D } ( \\dot { U } , Q ) =$ $\\begin{array} { r } { \\frac { 1 } { 2 } \\sum _ { { \\pmb x } \\in { \\pmb X } } | U ( { \\pmb x } ) - Q ( { \\pmb x } ) | \\leq \\frac { 1 } { 2 } \\sum _ { { \\pmb x } \\in { \\pmb X } } ( | U ( { \\pmb x } ) | + | Q ( { \\pmb x } ) | ) = 1 } \\end{array}$ Based on the triangle inequality (Steerneman 1983) of TVD, we easily have that\n\n$$\n\\mathrm { T V D } ( U , Q ) \\leq \\mathrm { T V D } ( U , P ) + \\mathrm { T V D } ( P , Q ) .\n$$\n\nThen, given that $U = \\alpha P + ( 1 - \\alpha ) Q$ with parameter $\\alpha$ controlling the weight of collected data, we can compute $\\mathrm { T V D } ( U , \\bar { P } )$ as\n\n$$\n\\begin{array} { l } { \\displaystyle \\mathrm { T V D } ( U , P ) = \\frac { 1 } { 2 } \\sum _ { x \\in \\mathcal { X } } \\left| U ( x ) - P ( x ) \\right| } \\\\ { \\displaystyle = \\frac { 1 } { 2 } \\sum _ { x \\in \\mathcal { X } } \\left| \\alpha P ( x ) + ( 1 - \\alpha ) Q ( x ) - P ( x ) \\right| } \\\\ { \\displaystyle \\quad = \\frac { 1 } { 2 } ( 1 - \\alpha ) \\sum _ { x \\in \\mathcal { X } } \\left| Q ( x ) - P ( x ) \\right| } \\\\ { \\displaystyle \\quad = ( 1 - \\alpha ) \\mathrm { T V D } \\left( Q , P \\right) . } \\end{array}\n$$\n\nBy invoking the symmetry of TVD and Eq. (3), we obtain\n\n$$\n\\mathrm { T V D } ( U , Q ) \\leq ( 2 - \\alpha ) \\mathrm { T V D } ( P , Q ) .\n$$\n\nHere Eq. (5) reveals that the high-quality synthetic data $\\mathcal { D } _ { \\mathrm { s } }$ and the mix proportion $\\alpha$ are two critical factors influencing the distribution gap $\\mathrm { T V D } ( U , P )$ .\n\nThe above observation inspires us to employ two modules to align the distribution of synthetic data with that of collected data, as shown in Fig. 1(c). In the teacher-guided generation module, we employ the teacher network to guide the GAN to enhance the quality of synthetic data, which solves its intrinsic issues when trained on the small and imbalanced collected data, including the overfitting of discriminator and imbalanced learning of generator:\n\n![](images/21d89a931d863d73d09ad7520133aa06cfcb7ef979f680444ca631e68e269448.jpg)  \nFigure 1: The diagram of (a) generation-based methods (Fang et al. 2021; Yin et al. 2020; Chen et al. 2019; Micaelli and Storkey 2019), (b) collection-based methods (Chen et al. 2021b; Tang et al. 2023), and (c) our HiDFD. In HiDFD, the teacher-guided generation module employs the teacher network to guide the training of the GAN on limited collected data. Subsequently, the student distillation module closely aligns the features of the student network with those of the teacher network on the hybrid data comprising high-quality synthetic examples and properly inflated collected examples.\n\nDiscriminator Overfitting. When trained with very few collected data, the discriminator is prone to be overconfident in determining fake examples, i.e., $\\mathrm { \\bar { E } } _ { { \\pmb x } \\sim Q _ { X } } [ \\mathcal { N } _ { \\mathrm { D } } ( { \\pmb x } ) ]$ tends to be 0. As a result, the gradient of ${ \\mathcal { L } } _ { \\mathrm { g } }$ in Eq. (1), which specialized in promoting generator to produce high-quality examples, may become ineffective, namely\n\n$$\n\\nabla _ { \\mathcal { N } _ { \\mathrm { G } } } \\mathbb { E } _ { { \\boldsymbol x } \\sim Q _ { X } } [ \\log ( 1 -  { \\mathcal { N } _ { \\mathrm { D } } } ( \\pmb { x } ) ) ] = \\mathbb { E } _ { \\pmb { x } \\sim Q _ { X } } \\bigg [ - \\frac { \\nabla _ { \\mathcal { N } _ { \\mathrm { G } } }  { \\mathcal { N } _ { \\mathrm { D } } } ( \\pmb { x } ) } { 1 -  { \\mathcal { N } _ { \\mathrm { D } } } ( \\pmb { x } ) } \\bigg ] \\approx 0 ,\n$$\n\nas the parameters of $\\mathcal { N } _ { \\mathrm { D } }$ and $\\mathcal { N } _ { \\mathrm { G } }$ are independent of each other, and Eq. (6) is proved by (Arjovsky and Bottou 2022). Meanwhile, the discriminator also has a classifier that provides valuable category dependencies for the generator by precisely predicting input examples, and thus promoting the generator to generate classifiable examples. However, multiclass classification is more challenging than binary determination of true and fake. Given very few collected examples, the discriminator is difficult to learn powerful representations for its classifier to achieve precise classification.\n\nImbalanced Generator Learning. Given the optimal classifier $\\Psi _ { \\mathrm { D } } ^ { * \\ 1 }$ of the discriminator, optimizing the generator to produce the classifiable examples2 is equivalent to\n\n$$\n\\operatorname* { m a x } _ { \\mathcal { N } _ { \\mathrm { G } } } [ \\mathbb { E } _ { { \\pmb x } , { y } \\sim Q _ { X , Y } } \\mathrm { l o g } ( \\frac { p ( { \\pmb x } , { y } ) } { q ( { \\pmb x } , { y } ) } ) ] \\Rightarrow \\operatorname* { m i n } _ { \\mathcal { N } _ { \\mathrm { G } } } \\mathrm { K L } ( Q _ { X , Y } \\| P _ { X , Y } ) ,\n$$\n\nwhere KL represents the Kullback-Leibler divergence, and the proof of Eq. (7) is provided in Appendix. The above Eq. (7) indicates that optimizing the generator will force the joint distribution $Q _ { X , Y }$ of synthetic data toward the $P _ { X , Y }$ of the imbalanced collected data, inevitably resulting in synthetic examples with poor diversity.\n\nIn student distillation, we properly inflate the collected examples to construct the hybrid data with a moderate mix proportion $\\alpha$ for effectively training the student network.\n\n# Teacher-Guided Generation\n\nIn this section, we promote GAN to generate high-quality examples by solving its critical issues guided by the teacher network. To mitigate the discriminator overfitting, we design a feature integration mechanism to force the aggregation between the features of both real collected examples and fake synthetic examples. Specifically, we blend the boundaries between real and fake examples to increase the difficulty for the discriminator to accurately discriminate them, and thus preventing the discriminator from overconfidence, i.e.,\n\n$$\n\\begin{array} { r l } &  \\mathcal { L } _ { \\mathrm { b l e n d } } { = } \\mathbb { E } _ { \\boldsymbol { x } , \\boldsymbol { y } \\sim P _ { \\boldsymbol { X } , \\boldsymbol { Y } } , \\hat { \\boldsymbol { x } } , \\boldsymbol { y } \\sim Q _ { \\boldsymbol { X } , \\boldsymbol { Y } } [ \\mathbb { I } ( \\boldsymbol { p } > \\boldsymbol { q } ) ( | | \\Phi _ { \\mathrm { T } } ( \\boldsymbol { x } ) - \\Phi _ { \\mathrm { D } } ( \\hat { \\boldsymbol { x } } ) | | _ { 2 }   } \\\\ & { \\quad \\quad \\quad +   | \\Phi _ { \\mathrm { T } } ( \\hat { \\pmb { x } } ) - \\Phi _ { \\mathrm { D } } ( \\boldsymbol { x } ) | | _ { 2 } ) ] , } \\end{array}\n$$\n\nwhere $\\mathbb { I } ( p > q )$ is an indicator function to control $\\mathcal { L } _ { \\mathrm { b l e n d } }$ be applied with a probability of $q$ and its value is 1 if $p > q$ and 0 otherwise $\\overset { \\cdot } { p }$ is sampled from $[ 0 , 1 ]$ , $\\scriptstyle q = 0 . 7$ and it is analyzed in Appendix). Meanwhile, we transfer the expressive features of the teacher network to enhance the representation ability of the discriminator, i.e.,\n\n$$\n\\begin{array} { r l } & { \\mathcal { L } _ { \\mathrm { t r a n s } } = \\mathbb { E } _ { { \\pmb x } , { \\pmb y } \\sim P _ { X , Y } , { \\hat { \\pmb x } } , { \\pmb y } \\sim Q _ { X , Y } } \\big [ \\big ( \\| \\Phi _ { \\mathrm { T } } ( { \\pmb x } ) - \\Phi _ { \\mathrm { D } } ( { \\pmb x } ) \\| _ { 2 } } \\\\ & { \\qquad + \\left\\| \\Phi _ { \\mathrm { T } } ( { \\pmb \\hat { x } } ) - \\Phi _ { \\mathrm { D } } ( { \\pmb \\hat { x } } ) \\right\\| _ { 2 } \\big ) \\big ] . } \\end{array}\n$$\n\nTo alleviate the imbalanced learning of the generator, we regularize the GAN training across all categories. During generator training, we dynamically update the class frequencies $\\{ n _ { c } ^ { t } \\} _ { c = 1 } ^ { C }$ ( $C$ represents the number of categories) at the beginning of iteration $t$ via the following exponential moving average function with a weight $\\gamma \\in [ \\bar { 0 } , 1 ]$ , namely\n\n$$\nn _ { c } ^ { t } = ( 1 - \\gamma ) n _ { c } ^ { t - 1 } + \\gamma \\bar { n } _ { c } ^ { t - 1 } ,\n$$\n\nwhere $\\bar { n } _ { c } ^ { t - 1 }$ is the number of synthetic examples belonging to class $c$ in iteration $t - 1$ , $n _ { c } ^ { t }$ is initially set as a constant, and $\\scriptstyle \\gamma = 0 . 5$ (analyzed in Appendix). Then, each class frequency $\\dot { n } _ { c } ^ { t } \\in \\{ \\dot { n } _ { c } ^ { t } \\} _ { c = 1 } ^ { \\check { C } }$ is normalized as\n\n$$\n\\hat { n } _ { c } ^ { t } = \\frac { n _ { c } ^ { t } } { \\sum _ { j = 1 } ^ { C } n _ { j } ^ { t } } .\n$$\n\nThereafter, the generator is regulated to produce balanced examples by minimizing the loss function:\n\n$$\n\\mathcal { L } _ { \\mathrm { r e g } } = \\sum _ { c = 1 } ^ { C } \\frac { p _ { \\mathrm { T } } ^ { c } \\log \\left( p _ { \\mathrm { T } } ^ { c } \\right) } { \\hat { n } _ { c } ^ { t } } ,\n$$\n\nwhere $\\pmb { p } _ { \\mathrm { T } } = \\mathbb { E } _ { \\pmb { x } , \\pmb { y } \\sim Q _ { X , Y } } [ \\mathrm { S o f t M a x } ( \\mathcal { N } _ { \\mathrm { T } } ( \\pmb { x } ) ) ]$ is the average softmax vector output by the teacher network. The teacher network is well-trained on the original data, so it can precisely predict synthetic examples. In such a case, $\\pmb { p } _ { T } ^ { c }$ can be regarded as the proportion of examples in category $c$ within the synthetic data. In Eq. (12), the generation of examples in a category $c$ with the lower (or higher) $\\pmb { p } _ { \\mathrm { T } } ^ { c }$ is adjusted by the larger (or smaller) $1 / \\hat { n } _ { c } ^ { t }$ .\n\nThe loss functions of discriminator and generator in our teacher-guided GAN are summarized as\n\n$$\n\\left\\{ \\begin{array} { l l } { \\mathcal { L } _ { \\mathrm { D } } = \\mathcal { L } _ { \\mathrm { a d c } \\mathrm { { d } } } + \\lambda _ { \\mathrm { d } } ( \\mathcal { L } _ { \\mathrm { b l e n d } } + \\mathcal { L } _ { \\mathrm { t r a n s } } ) , } \\\\ { \\mathcal { L } _ { \\mathrm { G } } = \\mathcal { L } _ { \\mathrm { a d c } \\mathrm { { g } } } + \\lambda _ { \\mathrm { g } } \\mathcal { L } _ { \\mathrm { r e g } } , } \\end{array} \\right.\n$$\n\nwhere $\\mathcal { L } _ { \\mathrm { a d c . d } }$ and $\\mathcal { L } _ { \\mathrm { a d c . g } }$ are defined in Eq. (1), and the tradeoff parameters $\\lambda _ { \\mathrm { d } } > 0$ and ${ \\lambda _ { \\mathrm { g } } } > 0$ .\n\n# Student Distillation\n\nIn the teacher-guided generation module, we successfully trained an effective GAN for generating high-quality synthetic examples, which are then combined with collected examples to construct the hybrid data $\\mathcal { D }$ for training the student network. However, directly composing the limited collected examples with numerous synthetic examples will result in a small mix ratio $\\alpha$ (i.e., a large distribution gap $\\mathrm { T V D } ( U , Q ) )$ to disturb the training of the student network. Therefore, we inflate the collected data via example repeating to enlarge the $\\alpha$ from $\\left| \\mathcal { D } _ { \\mathrm { c } } \\right| / \\left( \\left| \\mathcal { D } _ { \\mathrm { c } } \\right| + \\left| \\mathcal { D } _ { \\mathrm { s } } \\right| \\right)$ to $N \\times$ $\\left| \\mathcal { D } _ { \\mathrm { c } } \\right| / \\left( N \\times \\left| \\mathcal { D } _ { \\mathrm { c } } \\right| + \\left| \\mathcal { D } _ { \\mathrm { s } } \\right| \\right)$ , where $N$ is the inflation factor. We adopt a moderate inflation factor of $N = \\lfloor \\lvert \\mathcal { D } _ { \\mathrm { s } } \\rvert / \\lvert \\mathcal { D } _ { \\mathrm { c } } \\rvert \\rfloor$ and further details are available in Extended Experiments.\n\nRecent works (Chen et al. 2021b; Tang et al. 2023) indicate that the collected data usually contains many noisy examples, which may mislead the GAN to produce undesired synthetic examples with wrong labels. As a result, these potentially noisy examples will harm the training of the student network, particularly affecting its classifier. In DFKD, the teacher network is well-trained on the original data and possesses an accurate classifier. Recent studies (Tang et al. 2023; Chen et al. 2022) show that the teacherâ€™s classifier contains useful category information regarding the original data. Therefore, we share the classifier of the teacher network with the student network. Then, we closely align the feature of the student network with that of the teacher network as follows:\n\n$$\n\\begin{array} { r } { \\mathcal { L } _ { \\mathrm { a l i g n } } = \\mathbb { E } _ { { \\pmb x } \\sim \\mathcal { D } } \\left[ \\| \\Phi _ { \\mathrm { S } } ( { \\pmb x } ) - \\Phi _ { \\mathrm { T } } ( { \\pmb x } ) \\| _ { 2 } \\right] . } \\end{array}\n$$\n\nBy minimizing the $\\mathcal { L } _ { \\mathrm { a l i g n } }$ , the feature of the student network is closely aligned with that of the teacher network, and the aligned feature is inputted into the shared classifier can produce predictions as accurately as the teacher network. The student network did not use any example labels during the training process, thereby avoiding the negative impact of potentially noisy labels. The whole algorithm of our proposed HiDFD is given in Appendix.\n\n# Experiments Datasets and Implementation Details\n\nOriginal Datasets. We evaluate the effectiveness of our HiDFD on popular datasets, including CIFAR (Krizhevsky 2009), CINIC (Darlow et al. 2018), and TinyImageNet (Le and Yang 2015), which are widely used by existing DFKD methods (Chen et al. 2019, 2021b). Additionally, we also conduct experiments on the large-scale ImageNet (Deng et al. 2009) and the practical medical image dataset HAM (Tschandl, Rosendahl, and Kittler 2018), which are challenging for existing DFKD methods.\n\nCollected Datasets. When using CIFAR and CINIC as the original datasets, we search for examples from ImageNet. With TinyImageNet and ImageNet as the original datasets, we utilize WebVision (Li et al. 2017) as our source of collected data. Moreover, we collect examples from ISIC (Codella et al. 2018) when using HAM as the original dataset. We follow (Chen et al. 2021b) and sample a part of examples from the corresponding dataset as collected data $\\mathcal { D } _ { \\mathrm { c } }$ . Here, we define the ratio between the collected data $\\mathcal { D } _ { \\mathrm { c } }$ and original data $\\mathcal { D } _ { \\mathrm { o } }$ as $\\rho = | \\mathcal { D } _ { \\mathrm { c } } | / | \\mathcal { D } _ { \\mathrm { o } } |$ . We construct small $( \\rho { = } 0 . 1 )$ and moderate $\\left( \\rho { = } 1 . 0 \\right)$ collected data for the experiments of collection-based DFKD methods. Notably, the original dataset is solely required for the pre-training of the teacher network. Detailed information regarding these datasets and the corresponding synthesized examples are provided in Appendix.\n\nImplementation Details. All student networks in our HiDFD employ SGD with weight decay as $5 \\times \\ 1 0 ^ { - 4 }$ and momentum as 0.9 as the optimizer. The student networks are trained over 240 epochs with a learning rate of 0.05, which is sequentially divided by 10 at the 150th, 180th, and 210th epochs. Meanwhile, the generator and discriminator in GAN utilize Adam for optimization with learning rates $1 \\times 1 0 ^ { - 4 }$ and $4 \\times 1 0 ^ { - 4 }$ , respectively, and both of them are trained over 500 epochs. Additionally, the hyper-parameters in Eq. (13) are configured as $\\lambda _ { \\mathrm { d } } = 0 . 1$ and $\\lambda _ { \\mathrm { g } } = 0 . 1$ .\n\n<html><body><table><tr><td rowspan=\"2\">Dataset</td><td rowspan=\"2\">Arch</td><td rowspan=\"2\">ACCT</td><td rowspan=\"2\">ACCS</td><td rowspan=\"2\">DAFL</td><td colspan=\"3\">Generation-Based</td><td rowspan=\"2\">SSNet</td><td colspan=\"7\">Collection-Based</td></tr><tr><td>DDAD</td><td>DI</td><td>CMI</td><td>DeGAN</td><td></td><td>DFND</td><td></td><td>KD3</td><td>p=0.1</td><td>HiDFD (ours)</td></tr><tr><td rowspan=\"3\">CIFAR10</td><td></td><td>95.70</td><td>95.20</td><td>92.22</td><td>93.08</td><td>93.26</td><td>94.84</td><td>95.39</td><td>p=0.1 90.39</td><td>p=1.0 p=0.1 91.95 48.82</td><td>p=1.0 85.82</td><td>p=0.1 65.70</td><td></td><td>p=1.0 93.37 94.74</td><td>p=1.0 95.11</td></tr><tr><td>â–¡</td><td>94.07</td><td>92.69</td><td>86.92</td><td>90.85</td><td>85.27</td><td>88.49</td><td>92.00</td><td>87.52</td><td>90.37 48.65</td><td>89.22</td><td>48.93</td><td>91.49</td><td>92.28</td><td>93.14</td></tr><tr><td>â–³</td><td>95.70</td><td>92.69</td><td>83.36</td><td>89.76</td><td>90.24</td><td>86.63</td><td>92.03</td><td>86.40</td><td>89.69 49.48</td><td>90.60</td><td>65.10</td><td>93.05</td><td>92.90</td><td>93.76</td></tr><tr><td rowspan=\"3\">CIFAR100</td><td></td><td>78.05</td><td>77.10</td><td>74.47</td><td>73.64</td><td>61.32</td><td>77.04</td><td>77.41</td><td>53.20 62.94</td><td>21.45</td><td>64.73</td><td>26.96</td><td>72.90</td><td>76.93</td><td>78.35</td></tr><tr><td>â–¡</td><td>74.53</td><td>72.28</td><td>65.36</td><td></td><td>60.00</td><td>59.70</td><td>71.16</td><td>53.97</td><td></td><td></td><td>21.27</td><td></td><td></td><td>74.18</td></tr><tr><td>â–³</td><td></td><td></td><td>68.33</td><td></td><td></td><td></td><td></td><td>61.80 56.44</td><td>23.48 23.86</td><td>63.90 64.54</td><td>25.25</td><td>71.44 72.46</td><td>71.26 73.44</td><td>75.65</td></tr><tr><td rowspan=\"3\">CINIC</td><td></td><td>78.05 86.62</td><td>72.28 85.09</td><td>45.28 60.54</td><td>68.59 80.10</td><td>61.07 78.57</td><td>61.80 78.47</td><td>72.38 83.47</td><td>46.82 57.59</td><td>76.78 24.53</td><td>80.94</td><td>39.35</td><td></td><td></td><td></td></tr><tr><td>â–¡</td><td>84.22</td><td>83.28</td><td>59.08</td><td>77.90</td><td>68.90</td><td>74.99</td><td>79.63</td><td>54.36</td><td></td><td></td><td></td><td>82.68</td><td>85.62</td><td>86.68</td></tr><tr><td></td><td>86.62 83.28</td><td>44.62</td><td>77.63</td><td>59.52</td><td>75.46</td><td></td><td></td><td>76.11 54.43 74.40</td><td>29.53 33.40</td><td>77.41 79.33</td><td>29.88 71.57</td><td>78.18</td><td>81.92</td><td>82.27 82.88</td></tr><tr><td></td><td>â–³</td><td></td><td></td><td></td><td></td><td></td><td></td><td>80.30</td><td></td><td></td><td></td><td></td><td>80.28</td><td>81.90</td><td></td></tr><tr><td>Tiny-</td><td>â–¡</td><td>66.4</td><td>64.85</td><td>52.20</td><td>5.24</td><td>12</td><td>64.01</td><td>64.04</td><td>25.73</td><td>44.65 26.36</td><td>68.9</td><td>20.26</td><td>63.06</td><td>65.6</td><td>66.619</td></tr><tr><td>ImageNet</td><td>â–³</td><td>66.44</td><td>61.55</td><td>52.46</td><td>44.20</td><td>2.27</td><td>20.57</td><td>59.16</td><td>21.09</td><td>48.12 25.53</td><td>58.18</td><td>27.37</td><td>61.98</td><td>61.67</td><td>65.27</td></tr><tr><td>HAM</td><td></td><td>81.18</td><td>79.64</td><td>32.05</td><td>44.68</td><td>62.79</td><td>67.34</td><td>74.52</td><td>34.75</td><td>64.43 27.55</td><td>62.59</td><td>64.10</td><td>68.44</td><td>77.08</td><td>81.52</td></tr><tr><td>ImageNet</td><td></td><td>73.27</td><td>67.00</td><td>1.92</td><td>1.46</td><td>1.14</td><td>1.84</td><td>5.74</td><td>22.28</td><td>43.96 28.99</td><td>45.66</td><td>35.02</td><td>55.05</td><td>65.36</td><td>66.89</td></tr></table></body></html>\n\nTable 1: Accuracies (in $\\%$ ) of student networks trained by various methods on six image classification datasets. The columns â€œACCTâ€ and â€œACCSâ€ report the accuracies yielded by the teacher network and student network trained on the full original data, respectively. The best and the second-best results are highlighted in bold and underlined, respectively. The notations $\\diamondsuit$ , â–¡, and $\\bigtriangleup$ represent the teacher-student pairs ResNet34-ResNet18, ResNet34-VGG13, and VGG16-VGG13, respectively.\n\n# Experiments on Benchmark Datasets\n\nIn this section, we conduct comprehensive experiments on various benchmark datasets to evaluate the performance of our proposed HiDFD against SOTA generation-based (Chen et al. 2019; Zhao et al. 2022b; Yin et al. 2020; Binici et al. 2022; Fang et al. 2021; Yu et al. 2023) and collectionbased (Addepalli et al. 2020; Chen et al. 2021b; Tang et al. 2023) DFKD methods. These methods are reproduced by using their official source codes.\n\nTab. 1 reports the results of the compared methods and our proposed HiDFD. Firstly, our proposed HiDFD using only a small quantity of collected examples $( \\rho { = } 0 . 1 )$ achieves comparable performance with those trained on the full original data. Secondly, when trained on the modestly sized collected data $\\left( \\rho { = } 1 . 0 \\right)$ , our proposed HiDFD significantly outperforms compared methods on most tasks, especially on the challenging HAM and ImageNet. Thirdly, those generationbased methods, which utilize generative models to produce training examples without the supervision of real examples, tend to perform unsatisfactorily due to the deficiencies in their synthetic examples. These results demonstrate that our proposed HiDFD can train robust student networks by effectively generating training examples from limited real-world examples and properly utilizing all realistic examples.\n\n# Ablation Studies & Parametric Sensitivities\n\nIn this section, we evaluate the effectiveness of our method with a small collected data $\\scriptstyle ( \\rho = 0 . 1 )$ , where CIFAR and ImageNet serve as the original and collected datasets, respectively. Moreover, ResNet34 and ResNet18 are used as the teacher network and student network, respectively.\n\nAblation Studies. We evaluate three key operations $\\cdot { \\mathcal { L } } _ { \\mathrm { b l e n d } }$ , $\\mathcal { L } _ { \\mathrm { t r a n s } }$ , and $\\mathcal { L } _ { \\mathrm { r e g } } \\dot { } .$ ) in teacher-guided generation and the classifier-sharing-based strategy in the student distillation. The experimental results are reported in Tab. 2, and the contributions of these components are analyzed as follows:\n\n1) Teacher-Guided Generation. The feature blending ${ \\mathcal { L } } _ { \\mathrm { b l e n d } }$ in Eq. (8) and feature transferring $\\mathcal { L } _ { \\mathrm { t r a n s } }$ in Eq. (9)\n\nTable 2: Accuracies (in $\\%$ ) of ablation studies.   \n\n<html><body><table><tr><td>Type</td><td>Algorithm</td><td>CIFAR10</td><td>CIFAR100</td></tr><tr><td rowspan=\"7\">Teacher- Guided Generation</td><td>w/o Lblend</td><td>92.87 (â†“1.87)</td><td>74.18 (â†“2.75)</td></tr><tr><td>w/o Ltrans</td><td>91.86 (â†“2.88)</td><td>74.40 (â†“2.53)</td></tr><tr><td>w/o Lreg</td><td>92.76 (â†“1.98)</td><td>73.95 (â†“2.98)</td></tr><tr><td>w/o Lblend,Ltrans</td><td>91.10 (â†“3.64)</td><td>71.02 (â†“5.91)</td></tr><tr><td>w/o Lblend,Lreg</td><td>90.77 (â†“3.97)</td><td>71.83 (â†“5.10)</td></tr><tr><td>W/o Ltrans,Lreg</td><td>91.42 (â†“3.32)</td><td>72.10 (â†“4.83)</td></tr><tr><td>W/o Lblend,trans,reg</td><td>89.55 (â†“5.19)</td><td>70.25 (â†“6.68)</td></tr><tr><td rowspan=\"8\">Student Distillation</td><td>OFAKD</td><td>92.88 (â†“1.86)</td><td>70.86 (â†“6.07)</td></tr><tr><td>VKD</td><td>92.69 (â†“2.05)</td><td>66.96 (â†“9.97)</td></tr><tr><td>SemcKD</td><td>93.49 (â†“1.25)</td><td>70.93 (â†“6.00)</td></tr><tr><td>CC</td><td>92.63 (â†“2.11)</td><td>69.52 (â†“7.41)</td></tr><tr><td>DKD</td><td>92.95 (â†“1.79)</td><td>68.25 (â†“8.68)</td></tr><tr><td>RKD</td><td>92.40 (â†“2.34)</td><td>70.53 (â†“6.40)</td></tr><tr><td>CATKD</td><td>92.49 (â†“2.25)</td><td>68.69 (â†“8.24)</td></tr><tr><td>NKD</td><td>93.26 (â†“1.48)</td><td>65.31 (â†“11.62)</td></tr><tr><td></td><td>HiDFD (ours)</td><td>94.74</td><td>76.93</td></tr></table></body></html>\n\nfor preventing the overfitting of discriminator and enhancing its representation ability. Meanwhile, the generator regulation ${ \\mathcal { L } } _ { \\mathrm { r e g } }$ in Eq. (12) is also essential for maintaining the balanced training of the generator. Therefore, the omission of any components among them leads to a noticeable reduction in the performance of the student network. Particularly, training the student network only on synthetic examples without any guidance from the teacher network results in the poorest performance (as shown in the term â€œw/o $\\mathcal { L } _ { \\mathrm { b l e n d } } , \\mathcal { L } _ { \\mathrm { t r a n s } } , \\mathcal { L } _ { \\mathrm { r e g } } \\mathrm { \\bar { \\Psi } } ^ { , } )$ . These results indicate the importance of these operations for robust GAN training with limited collected examples, thereby generating high-quality examples for training reliable student networks.\n\n2) Student Distillation. We examine the impact of replacing the classifier-sharing-based feature alignment with traditional KD methods (Hinton, Vinyals, and Dean 2015; Chen et al. 2021a). Both the student networks are trained on the hybrid data composed of collected and synthetic examples. We can find that the student networks trained by these methods generally achieve suboptimal performance due to their inability to effectively handle the potentially noisy examples\n\n100 100 100 å¹¿ 9474 7693 _93.68__93.55 75 2525375 F 9594.69__.94.63 76.23__7655__7693__76.91_ 9474 94.71__94.56 __Z6.7375 T 78 â˜…CIFAR10 â˜… CIFAR10 â†DFND 65 â˜…ï¼šCIFAR1y HiDFD 65 UIFAR10HiDFD â˜… UiDd by HiDFD HIDFD 600.001 600.001 72 20010.20.3040.506070.80.910 0.01 0.1 i 10 0.01 0.1 i 10 5 10 15 20 å…¥d å…¥g Inflation factor N p (a) Analysis of (b) Analysis of (c) Accuracy vs inflation factor (d) Accuracy vs  Ï\n\namong the hybrid data. These results highlight the suitability of our training strategy for reliable student networks in the data-free distillation scenarios.\n\nParametric Sensitivity. There are two tuning parameters in our HiDFD, including $\\lambda _ { \\mathrm { d } }$ and $\\lambda _ { \\mathrm { g } }$ in Eq. (13). To analyze the sensitivities, we individually vary each parameter while keeping the others constant during training. The accuracies of the corresponding student networks are shown in Fig. 2(a) and Fig. 2(b). Despite the large fluctuations in these parameters, where $\\lambda _ { \\mathrm { d } }$ , $\\bar { \\lambda _ { \\mathrm { g } } } \\in \\{ 0 . 0 0 1 , 0 . 0 1 , 0 . 1 , 1 , 1 0 \\}$ , the accuracy curve of the student network remains relatively stable. These results indicate the robustness of our HiDFD against parameter variations. Additionally, the student network achieved the best performance when $\\lambda _ { \\mathrm { d } } = \\lambda _ { \\mathrm { g } } = 0 . 1$ , so we adopted such parameter configuration in our method.\n\n# Extended Experiments\n\nExperiments with Various Backbones. We evaluate our HiDFD across many widely used teacher-student pairs to assess its adaptability to different networks. The results are shown in Tab. 3, we can observe that our HiDFD consistently achieves satisfactory performance across different teacher-student pairs, where both the trained students perform comparably to those trained on the original data.\n\nExperiments with Varying Inflation Factors. We report the accuracies of the student networks trained on collected data with various inflation factors in Fig. 2(c). The student network performs better with increasing $N$ , and the best accuracy is observed at $N { = } 1 0$ . Furthermore, excessive inflation may reduce the diversity brought by synthetic data, so that the student network encounters performance degradation when $N { > } 1 0$ . Therefore, we adopt a moderate inflation factor of $N = \\lfloor \\vert \\mathcal { D } _ { \\mathrm { s } } \\vert / \\vert \\mathcal { D } _ { \\mathrm { c } } \\vert \\rfloor$ . These experiments demonstrate that appropriately inflating the collected examples, which are crucial for reducing the distribution gap between synthetic and collected data, can effectively improve the performance of the student network.\n\nExperiments on Collected Data with Various Data Quantities. We explore the impact of varying the volume of collected data on the performance of student networks, with $\\rho$ values ranging from 0.1 to 1. As shown in Fig. 2(d), student networks trained by the compared collection-based DFKD methods (Chen et al. 2021b; Tang et al. 2023) tend to underperform with small values of $\\rho$ . Conversely, our HiDFD consistently achieves satisfactory performance across a spectrum of $\\rho$ values. These results further demonstrate the effectiveness of our HiDFD in training reliable student networks leveraging limited collected data.\n\nTable 3: Accuracies (in $\\%$ ) of various networks $\\left( \\rho { = } 1 . 0 \\right)$ .   \n\n<html><body><table><tr><td>Dataset</td><td>Teacher</td><td>Student</td><td>ACCS</td><td>HiDFD</td></tr><tr><td rowspan=\"4\">CIFAR10</td><td>ResNet32Ã—4</td><td>ResNet110</td><td>93.37</td><td>95.04</td></tr><tr><td>ResNet32Ã—4</td><td>ShuffleNet</td><td>93.23</td><td>93.62</td></tr><tr><td>ResNet110Ã—2</td><td>ResNet116</td><td>93.21</td><td>94.83</td></tr><tr><td>ResNet110Ã—2</td><td>WRN40Ã—2</td><td>94.86</td><td>95.35</td></tr><tr><td rowspan=\"4\">CIFAR100</td><td>ResNet32Ã—4</td><td>ResNet110</td><td>74.31</td><td>75.69</td></tr><tr><td>ResNet32Ã—4</td><td>ShuffleNet</td><td>72.60</td><td>75.03</td></tr><tr><td>ResNet110Ã—2</td><td>ResNet116</td><td>74.46</td><td>74.49</td></tr><tr><td>ResNet110Ã—2</td><td>WRN40Ã—2</td><td>76.31</td><td>75.65</td></tr></table></body></html>\n\n# Conclusion\n\nIn this paper, we proposed a new data-free distillation approach termed HiDFD to train the student networks on the hybrid data comprising high-quality synthetic examples and scarce collected examples, which well meets practical requirements. Our investigation reveals that bridging the distribution gap between the hybrid and synthetic data is crucial for training reliable student networks, and it implies that the quality of synthetic data and the weight of collected data are two key factors in reducing this gap. This observation inspired us to propose a novel hybrid distillation framework, where the teacher-guided generation module can effectively generate high-quality synthetic examples from the limited collected data by leveraging the teacher network to guide the GAN training process, and the student distillation module properly enhances the influence of collected examples within the hybrid data by inflating their frequency. Consequently, we can naturally define a classifier-sharingbased feature alignment to distill the student network, and we achieve state-of-the-art performance using significantly fewer examples than existing methods. The limitations and broader impacts of our HiDFD are discussed in Appendix.",
    "summary": "```json\n{\n  \"core_summary\": \"### ğŸ¯ æ ¸å¿ƒæ¦‚è¦\\n\\n> **é—®é¢˜å®šä¹‰ (Problem Definition)**\\n> *   æ•°æ®æ— çŸ¥è¯†è’¸é¦ï¼ˆData-Free Knowledge Distillation, DFKDï¼‰æ—¨åœ¨ä¸ä½¿ç”¨æ•™å¸ˆç½‘ç»œçš„åŸå§‹è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œä»é¢„è®­ç»ƒçš„å¤§å‹æ•™å¸ˆç½‘ç»œä¸­å­¦ä¹ ä¸€ä¸ªç´§å‡‘çš„å­¦ç”Ÿç½‘ç»œã€‚ç°æœ‰çš„åŸºäºæ”¶é›†å’ŒåŸºäºç”Ÿæˆçš„æ–¹æ³•åˆ†åˆ«é€šè¿‡æ”¶é›†å¤§é‡çœŸå®ç¤ºä¾‹å’Œç”Ÿæˆåˆæˆç¤ºä¾‹æ¥è®­ç»ƒå­¦ç”Ÿç½‘ç»œï¼Œä½†åœ¨å®é™…åœºæ™¯ä¸­ç”±äºéš¾ä»¥æ”¶é›†æˆ–æ¨¡æ‹Ÿè¶³å¤Ÿçš„çœŸå®æ•°æ®è€Œè¡¨ç°ä¸ä½³ã€‚\\n> *   è¯¥é—®é¢˜çš„é‡è¦æ€§åœ¨äºï¼Œè®¸å¤šå®é™…åº”ç”¨ä¸­åŸå§‹è®­ç»ƒæ•°æ®ç”±äºéšç§é—®é¢˜æ— æ³•è·å–ï¼Œè€Œç°æœ‰æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºæˆ–ç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µä¸‹éš¾ä»¥è®­ç»ƒå‡ºå¯é çš„å­¦ç”Ÿç½‘ç»œã€‚\\n\\n> **æ–¹æ³•æ¦‚è¿° (Method Overview)**\\n> *   æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ··åˆæ•°æ®æ— è’¸é¦ï¼ˆHybrid Data-Free Distillation, HiDFDï¼‰çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…åˆ©ç”¨å°‘é‡æ”¶é›†çš„æ•°æ®å¹¶ç”Ÿæˆè¶³å¤Ÿçš„ç¤ºä¾‹æ¥è®­ç»ƒå­¦ç”Ÿç½‘ç»œã€‚HiDFDåŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šæ•™å¸ˆå¼•å¯¼ç”Ÿæˆå’Œå­¦ç”Ÿè’¸é¦ã€‚\\n\\n> **ä¸»è¦è´¡çŒ®ä¸æ•ˆæœ (Contributions & Results)**\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹1ï¼š** æå‡ºäº†ä¸€ç§æ··åˆæ•°æ®æ— è’¸é¦æ¡†æ¶ï¼Œä»…éœ€å°‘é‡æ”¶é›†æ•°æ®å³å¯ç”Ÿæˆé«˜è´¨é‡åˆæˆç¤ºä¾‹ã€‚\\n>   *   **å…³é”®æ•°æ®ï¼š** åœ¨CIFAR10æ•°æ®é›†ä¸Šï¼ŒHiDFDä»…ä½¿ç”¨1/120ï¼ˆ5,000/600,000ï¼‰çš„æ”¶é›†æ•°æ®å³å¯è¾¾åˆ°ä¸å…¨é‡æ•°æ®ç›¸å½“çš„æ€§èƒ½ï¼ˆ94.74%å‡†ç¡®ç‡ï¼‰ã€‚\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹2ï¼š** è®¾è®¡äº†æ•™å¸ˆå¼•å¯¼ç”Ÿæˆæ¨¡å—ï¼Œè§£å†³äº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åœ¨å°‘é‡æ•°æ®ä¸‹çš„åˆ¤åˆ«å™¨è¿‡æ‹Ÿåˆå’Œç”Ÿæˆå™¨ä¸å¹³è¡¡å­¦ä¹ é—®é¢˜ã€‚\\n>   *   **å…³é”®æ•°æ®ï¼š** åœ¨HAMæ•°æ®é›†ä¸Šï¼ŒHiDFDçš„å‡†ç¡®ç‡è¾¾åˆ°81.52%ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹3ï¼š** æå‡ºäº†åˆ†ç±»å™¨å…±äº«çš„ç‰¹å¾å¯¹é½ç­–ç•¥ï¼Œæœ‰æ•ˆæå‡äº†å­¦ç”Ÿç½‘ç»œçš„æ€§èƒ½ã€‚\\n>   *   **å…³é”®æ•°æ®ï¼š** åœ¨ImageNetæ•°æ®é›†ä¸Šï¼ŒHiDFDçš„å‡†ç¡®ç‡ä¸º66.89%ï¼Œä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚\",\n  \"algorithm_details\": \"### âš™ï¸ ç®—æ³•/æ–¹æ¡ˆè¯¦è§£\\n\\n> **æ ¸å¿ƒæ€æƒ³ (Core Idea)**\\n> *   HiDFDçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡æ•™å¸ˆç½‘ç»œå¼•å¯¼ç”Ÿæˆé«˜è´¨é‡åˆæˆæ•°æ®ï¼Œå¹¶ç»“åˆå°‘é‡æ”¶é›†æ•°æ®è®­ç»ƒå­¦ç”Ÿç½‘ç»œã€‚è¯¥æ–¹æ³•é€šè¿‡ç‰¹å¾æ•´åˆæœºåˆ¶å’Œç±»åˆ«é¢‘ç‡å¹³æ»‘æŠ€æœ¯è§£å†³äº†GANåœ¨å°‘é‡æ•°æ®ä¸‹çš„è¿‡æ‹Ÿåˆå’Œä¸å¹³è¡¡é—®é¢˜ã€‚\\n\\n> **åˆ›æ–°ç‚¹ (Innovations)**\\n> *   **ä¸å…ˆå‰å·¥ä½œçš„å¯¹æ¯”ï¼š** ç°æœ‰åŸºäºæ”¶é›†çš„æ–¹æ³•éœ€è¦å¤§é‡çœŸå®æ•°æ®ï¼Œè€ŒåŸºäºç”Ÿæˆçš„æ–¹æ³•ç”Ÿæˆçš„åˆæˆæ•°æ®è´¨é‡è¾ƒä½ã€‚\\n> *   **æœ¬æ–‡çš„æ”¹è¿›ï¼š** HiDFDé€šè¿‡æ•™å¸ˆç½‘ç»œå¼•å¯¼ç”Ÿæˆé«˜è´¨é‡åˆæˆæ•°æ®ï¼Œå¹¶åˆ©ç”¨å°‘é‡æ”¶é›†æ•°æ®å¢å¼ºç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œè§£å†³äº†æ•°æ®ç¨€ç¼ºå’Œç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚\\n\\n> **å…·ä½“å®ç°æ­¥éª¤ (Implementation Steps)**\\n> 1.  **æ•™å¸ˆå¼•å¯¼ç”Ÿæˆæ¨¡å—ï¼š**\\n>     *   è®¾è®¡ç‰¹å¾æ•´åˆæœºåˆ¶ï¼ˆEq. 8ï¼‰é˜²æ­¢åˆ¤åˆ«å™¨è¿‡æ‹Ÿåˆã€‚\\n>     *   æå‡ºç±»åˆ«é¢‘ç‡å¹³æ»‘æŠ€æœ¯ï¼ˆEq. 12ï¼‰å¹³è¡¡ç”Ÿæˆå™¨çš„è®­ç»ƒã€‚\\n> 2.  **å­¦ç”Ÿè’¸é¦æ¨¡å—ï¼š**\\n>     *   é€šè¿‡æ•°æ®è†¨èƒ€ç­–ç•¥è°ƒæ•´æ”¶é›†æ•°æ®åœ¨æ··åˆæ•°æ®ä¸­çš„è´¡çŒ®ã€‚\\n>     *   ä½¿ç”¨åˆ†ç±»å™¨å…±äº«çš„ç‰¹å¾å¯¹é½æŠ€æœ¯ï¼ˆEq. 14ï¼‰æå‡å­¦ç”Ÿç½‘ç»œæ€§èƒ½ã€‚\\n\\n> **æ¡ˆä¾‹è§£æ (Case Study)**\\n> *   è®ºæ–‡æœªæ˜ç¡®æä¾›æ­¤éƒ¨åˆ†ä¿¡æ¯ã€‚\",\n  \"comparative_analysis\": \"### ğŸ“Š å¯¹æ¯”å®éªŒåˆ†æ\\n\\n> **åŸºçº¿æ¨¡å‹ (Baselines)**\\n> *   ç”ŸæˆåŸºçº¿æ–¹æ³•ï¼šDAFLã€DDADã€DIã€CMIã€DeGANã€‚\\n> *   æ”¶é›†åŸºçº¿æ–¹æ³•ï¼šDFNDã€KD3ã€SSNetã€‚\\n\\n> **æ€§èƒ½å¯¹æ¯” (Performance Comparison)**\\n> *   **åœ¨å‡†ç¡®ç‡ä¸Šï¼š** æœ¬æ–‡æ–¹æ³•åœ¨CIFAR10æ•°æ®é›†ä¸Šè¾¾åˆ°äº†94.74%ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹DFNDï¼ˆ90.39%ï¼‰å’ŒKD3ï¼ˆ93.37%ï¼‰ã€‚ä¸è¡¨ç°æœ€ä½³çš„åŸºçº¿ç›¸æ¯”ï¼Œæå‡äº†1.37ä¸ªç™¾åˆ†ç‚¹ã€‚\\n> *   **åœ¨æ•°æ®æ•ˆç‡ä¸Šï¼š** æœ¬æ–‡æ–¹æ³•ä»…éœ€5,000ä¸ªæ”¶é›†ç¤ºä¾‹å³å¯è¾¾åˆ°ä¸å…¨é‡æ•°æ®ç›¸å½“çš„æ€§èƒ½ï¼Œè€ŒåŸºçº¿æ–¹æ³•DFNDéœ€è¦600,000ä¸ªç¤ºä¾‹ã€‚\\n> *   **åœ¨HAMæ•°æ®é›†ä¸Šï¼š** æœ¬æ–‡æ–¹æ³•çš„å‡†ç¡®ç‡ä¸º81.52%ï¼Œè¿œé«˜äºåŸºçº¿æ¨¡å‹DAFLï¼ˆ32.05%ï¼‰å’ŒDFNDï¼ˆ64.43%ï¼‰ã€‚\",\n  \"keywords\": \"### ğŸ”‘ å…³é”®è¯\\n\\n*   æ•°æ®æ— çŸ¥è¯†è’¸é¦ (Data-Free Knowledge Distillation, DFKD)\\n*   ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (Generative Adversarial Network, GAN)\\n*   æ•™å¸ˆå¼•å¯¼ç”Ÿæˆ (Teacher-Guided Generation, N/A)\\n*   å­¦ç”Ÿè’¸é¦ (Student Distillation, N/A)\\n*   æ··åˆæ•°æ® (Hybrid Data, N/A)\\n*   ç‰¹å¾å¯¹é½ (Feature Alignment, N/A)\\n*   ç±»åˆ«é¢‘ç‡å¹³æ»‘ (Category Frequency Smoothing, N/A)\"\n}\n```"
}