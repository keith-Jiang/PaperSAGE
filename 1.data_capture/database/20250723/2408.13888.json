{
    "source": "Semantic Scholar",
    "arxiv_id": "2408.13888",
    "link": "https://arxiv.org/abs/2408.13888",
    "pdf_link": "https://arxiv.org/pdf/2408.13888.pdf",
    "title": "Enhancing SQL Query Generation with Neurosymbolic Reasoning",
    "authors": [
        "Henrijs Princis",
        "Cristina David",
        "Alan Mycroft"
    ],
    "categories": [
        "cs.DB",
        "cs.AI",
        "cs.SE"
    ],
    "publication_date": "2024-08-25",
    "venue": "arXiv.org",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 2,
    "influential_citation_count": 0,
    "institutions": [
        "University of Cambridge",
        "University of Bristol"
    ],
    "paper_content": "# Enhancing SQL Query Generation with Neurosymbolic Reasoning\n\nHenrijs Princis1, Cristina David2, Alan Mycroft1\n\n1University of Cambridge, Cambridge CB3 0FD, UK 2University of Bristol, Bristol BS8 1QU, UK princish@cardiff.ac.uk, alan.mycroft@cl.cam.ac.uk, cristina.david@bristol.ac.uk\n\n# Abstract\n\nWe propose a neurosymbolic architecture aimed at boosting the performance of any Language Model (LM) for SQL query generation. This approach leverages symbolic reasoning to guide the LM’s exploration of the search space by considering multiple paths, symbolically evaluating choices at each decision point to choose the next step, with the added novel ability to backtrack. A key innovation is the use of symbolic checks on both incomplete and fully generated SQL queries, enabling early truncation of unsuccessful search paths. Input consists of textual requirements on the desired query, along with optional example tuples to be selected by the query. Experiments on Xander, our open-source implementation, show it both reduces runtime and increases accuracy of the generated SQL. A specific result is an LM using Xander outperforming a four-times-larger LM.\n\nCode — https://github.com/henrijsprincis/Xander Spider dataset — https://huggingface.co/datasets/xlangai/spider Extended version — https://arxiv.org/pdf/2408.13888\n\n# Introduction\n\nLanguage models (LMs)1 in particular large language models (LLMs) have recently been successfully applied to a wide range of tasks including code generation (Li et al. 2022; Chen et al. 2021; Wang et al. 2021; Le et al. 2022). While initially LMs were used as black-box, monolithic entities, recently there has been a shift towards architectures that allow some form of logical reasoning. For example, the widely-used Chain of Thought (CoT) approach (Wei et al. 2022) enables logical reasoning by breaking down complex problems into smaller steps represented as intermediate thoughts.\n\nHowever, LMs are still predominantly constrained to linear search during inference. For example, in the case of CoT, at each step, the LM commits to a single path, sequentially constructing a solution while disregarding alternative candidates. This linear search strategy has been shown to impede long-term proof planning (Saparov and He 2023; Bubeck et al. 2023), especially in scenarios where multiple valid deduction steps exist. Even when multiple paths are considered (Yao et al. 2023), decisions are still based on the LM self-evaluating its choices.\n\nIn this work, we propose an architecture for SQL query generation that leverages symbolic reasoning to enable the nonlinear exploration of the search space. Our approach navigates a solution tree using Best-First Search (Pearl 1984), with support for backtracking. This architecture is compatible with any pretrained LM, and we apply it to several smaller, open-source LMs, where we show that it improves their performance out-of-the-box. While our focus is on SQL, our broader goal is to illustrate that a neurosymbolic approach provides an alternative to scaling model size. This addresses the high computational cost of LMs, where accuracy hinges heavily on the parameter count (Kaplan et al. 2020).\n\nOur architecture explores the search space guided by a symbolic query checker, a neural query checker, and a symbolic test and repair module. When integrated with a generic pretrained LM, these modules prune bad queries before they have finished being generated and correct errors made by the LM. As far as we are aware LM-based nonlinear solutionbuilding has not been investigated in the context of code generation, where the solution space is particularly large.\n\nOne challenge when generating code is that the result can only be checked for correctness (i.e. adherence to the original intent) once it is completely generated— here defined as either including a semicolon or reaching an end-of-sequence token predicted by the language model. In order to address this, one of the innovations of our work is that the symbolic query checker is designed so that it can test incomplete queries. The ability to exclude erroneous candidates as early as possible is critical when exploring a vast solution space.\n\nAs an additional challenge, SQL’s flexibility in expressing the same query in multiple ways can hinder the LM’s performance, as the model is penalised for predicting logically equivalent but syntactically different queries. To address this, we adapt NLP techniques (Tabassum and Patil 2020; Chai 2023) to eliminate unnecessary information from SQL queries before training an LM. In particular, we propose Standardised SQL, a form of SQL that minimises stylistic variations. Standardised SQL enhances both LM fine-tuning and inference, where it can be easily verified by symbolic modules. Our results show that Standardised SQL significantly improves overall LM accuracy.\n\nQuery synthesis The task of query synthesis has been previously considered in literature either as text-to-SQL or Query-by-Example (QBE): text-to-SQL denotes the task of generating SQL code from a textual description (Wang et al. 2022; Herzig and Berant 2018; Seneff et al. 1991; Li et al. $2 0 2 3 \\mathrm { a }$ ; Wang et al. 2019; Qi et al. 2022; Scholak, Schucher, and Bahdanau 2021), whereas QBE aims to find a query that returns user-provided output examples (Lissandrini et al. 2018; Tran, Chan, and Parthasarathy 2014; Psallidas et al. 2015; Shen et al. 2014; Tran, Chan, and Parthasarathy 2014; Wang, Cheung, and Bodik 2017).\n\nIn this work, we focus on the combined task: generating queries from natural language descriptions optionally accompanied by output examples. Our decision to include examples was inspired by tools like Excel’s FlashFill (Menon et al. 2013), a Programming-by-Example (PbE) approach, where users provide a few examples and Excel infers the pattern to complete the data. From the point of usability, examples are an easy way for novice users to clarify their intent, a core motivation in the area of PbE. Accompanying natural language descriptions with input-output examples is commonly used for code generation in general-purpose languages (Li et al. 2022; Le et al. 2022; Jiang et al. 2024), it has been far less studied for SQL—the only work we are aware of is (Baik et al. 2020).\n\nWhile approaches to text-to-SQL mostly rely on custom neural architectures, QBE techniques typically use symbolic reasoning. We explore a unified approach where, instead of designing a custom neural model, we propose a neurosymbolic design that can be used to enhance any pretrained LM without modifications.\n\nProblem Definition We consider the task of generating SQL code from a textual description and zero or more userprovided output examples. We call this task text-to-SQLwith-examples and it can be defined as follows. Given a natural language question $q$ , the database $\\mathcal { D }$ which has a schema $\\mathcal { D } _ { s c h e m a }$ , and possibly an empty set of output examples $o$ , we wish to find an SQL query $l$ which when executed on database $\\mathcal { D }$ answers question $q$ by returning a set $s$ of tuples such that $s \\supseteq o$ . (This “open-world” formulation is appropriate as we cannot expect the user to provide an exhaustive set of output examples $o _ { \\cdot }$ .)\n\n# Contributions\n\n1. We propose a neurosymbolic design for generating SQL queries from natural language and examples; our design can be used to enhance the performance of any pretrained LM without modifications. This approach incorporates symbolic reasoning to guide the LM’s solution search by exploring multiple paths, symbolically evaluating choices at each decision point to choose the next step, with the added ability to backtrack. This advancement is particularly significant because LMs typically commit to a single path when presented with multiple valid deduction options, lacking the systematic exploration of alternatives.\n\nRequire: D: Database, q: question, o: output examples, BPE: Byte-pair encoding, BPD: Byte-pair decoding, LM: Language Model, PQC: Partial-Query Checker, QTR: Query Test-and-Repair   \n1: $\\mathbf { x } \\gets \\mathbf { B P E } ( D _ { \\mathrm { s c h e m a } } , q , o )$ $D$ The byte pair encoding of Database schema, question and output examples is used as input.   \n2: priority queue.setempty();   \n3: $l , p _ { l } \\gets { } ^ { \\mathrm { 6 9 } } , 1$   \n4: while True do   \n5: if is complete query $( l )$ then   \n6: $l \\gets \\bar { { \\bf Q } } { \\bf T } { \\bf R } ( \\bar { l } , D ) \\$ $D$ Test and Attempt Repair   \n7: if QTR succeeds then   \n8: return l   \n9: end if   \n10: else   \n11: $\\begin{array} { r l } { \\mathbf { y }  \\mathbf { B P E } ( l ) } \\\\ { l _ { - c o n t i n u a t i o n s }  \\mathbf { L M } ( \\mathbf { x } , \\mathbf { y } ) } & { { } \\triangleright \\mathrm { P r e d i c t n e x t } } \\end{array}$   \n12: tokens with probabilities   \n13: for $t$ , $p _ { t }$ in $l$ continuations do   \n14: $l ^ { \\prime } \\gets l + \\mathbf { B P D } ( t )$ $D$ Proposed new query   \n15: $p _ { l } ^ { \\prime } \\gets p _ { l } \\times p _ { t }$ $D$ Proposed probability   \n16: if $\\mathbf { P Q C } ( l ^ { \\prime } , o , D _ { s c h e m a } )$ then   \n17: priority queue. $\\operatorname { a d d } ( ( l ^ { \\prime } , p _ { l } ^ { \\prime } ) )$ 1   \n18: end if   \n19: end for   \n20: end if   \n21: $l , p _ { l } \\gets$ priority queue.pop() $D$ Pop next $\\ell$ to explore and its corresponding probability.   \n22: end while 2. We investigate the use of normalisation in code generation by introducing Standardised SQL.   \n3. We implemented a prototype tool called Xander, and showed it leads to significant improvements in runtime and accuracy, offering a compelling alternative to scaling model size.\n\n# General Architecture of Xander\n\nIn this section, we present the design for our neurosymbolic technique, and its corresponding implementation Xander. A high-level overview of Xander during inference is given in Figure 1, and a detailed description is captured in Algorithm 1. Xander takes as input a 3-tuple $( \\mathcal { D } , q , o )$ consisting of database $\\mathcal { D }$ , natural language question $q$ , and outputexample set $o$ . We then use $( \\mathcal { D } _ { s c h e m a } , q , o )$ , where $\\mathcal { D } _ { s c h e m a }$ is the schema of $D$ , to prompt the LM.\n\nIn Algorithm 1, $\\mathbf { x }$ denotes the task description consisting of the concatenation of database schema $\\mathcal { D } _ { s c h e m a }$ , question $q$ and examples $o$ , and $\\mathbf { y }$ represents the possibly empty partial query that will fulfill the task description once the query is fully generated.\n\nWe use Byte-Pair Encoding (BPE) (Gage 1994) to encode the task description and partial query as sequences of tokens meaning that one can think of $\\mathbf { x }$ and $\\mathbf { y }$ at lines 1 and 11 as: $\\mathbf { x } = [ x ^ { 1 ^ { \\smile } } , x ^ { 2 } , . . . , x ^ { n } ]$ and $\\mathbf { y } = [ y ^ { 1 } , y ^ { 2 } , . . . , y ^ { m - 1 } ]$ , respectively.\n\n![](images/42ef7e81f3ae7a73b4838032b7441ec9aa33784877771c50450b92235e761eb2.jpg)  \nFigure 1: During inference Xander consists of three main modules connected by Best First Search (BFS). BFS explores the solution tree by choosing the most promising incomplete query (yellow box) and passing it to the LM. The LM then provides five candidates that extend the query by a single token. A Partial-Query Checker dismisses candidate queries containing errors (red box). The Partial-Query Checker passes queries without errors back to BFS if they are still incomplete or to Query Testand-Repair if they are complete. Finally, Query Test-and-Repair module executes the full query and verifies that it yields the correct result, in which case it is returned to the user (green box). If it does not, they are dismissed (red box) and BFS backtracks.\n\nIn each iteration of the while loop starting at line 4, we explore the current query $\\ell$ (initially the empty string). We first look at the scenario where query $\\ell$ is not yet complete, meaning that we have to continue generating tokens for it. This is captured starting with line 10 in Algorithm 1. After encoding $\\ell$ into $\\mathbf { y }$ , we call the LM to provide us the next token at line 12. Generally speaking, the LM defines a probability distribution $p$ over the possible continuations of $\\mathbf { y }$ , where the most likely next token $y ^ { m }$ in the partial query is obtained with the following equation:\n\n$$\ny ^ { m } = \\arg \\operatorname* { m a x } _ { t } p ( t | \\mathbf { x } , \\mathbf { y } )\n$$\n\nIn Algorithm 1, the call to the LM returns the most promising five pairs of candidate next tokens and their respective probabilities, which get stored in $l$ continuations.\n\nEach newly obtained query $\\ell ^ { \\prime }$ (the concatenation of the previous incomplete query $\\ell$ and the decoding of a newly generated token $t$ ) are checked for plausibility by the PartialQuery Checker PQC (line 16 in Algorithm 1). As we explain later, the default PQC module applies symbolic checks to detect invalid queries. Given that the LM generates queries one word at a time, completely generating a query and then checking it for errors is computationally wasteful. Instead, Xander checks incomplete queries, thus discarding many invalid ones early on.\n\nThe queries that pass the PQC are added to priority queue. This priority queue allows us to avoid generating the same query twice, as well as to use Best First Search (BFS) in order to explore the space of candidate queries. To enable the latter, the priority queue is ordered by the probability of each individual query. Then, BFS explores the solution tree by expanding the most promising node, which corresponds to an incomplete query with the highest probability. For this purpose, in each iteration of the while loop at line 4, we pop an incomplete query with the highest probability from the priority queue and further explore it (line 21 in Algorithm 1).\n\nIntuitively, the priority queue corresponds to a tree of possible candidates as shown on the RHS of Figure 1, where each node represents a query (while some leaves may be complete queries, the rest are incomplete), and we can think of edges as corresponding to a token that concatenated to the parent query produces the child query. Notably, we can have cases where, for a given incomplete query, all the generated next tokens result in queries that have lower probability than some of the queries already in the priority queue. Consequently, the query that is popped (in order to be explored in the next iteration of the while loop) is an ancestor of the current query. This corresponds to backtracking in the tree of candidate queries.\n\nIf we find a complete query, it is checked by Query Testand-Repair QTR (line 6 in Algorithm 1). Essentially, the query is executed on the full database $D$ to give a tuple set s. If $s \\supseteq o$ , then the corresponding complete query is reported to the user as the proposed query. Otherwise the QTR module tries to repair $\\ell$ using fuzzed variants of $\\ell$ that are at a Hamming distance of one. We regard any query that differs by exactly one aggregation operation (e.g. AVG), conditional operation (AND), comparison operator $( \\geq )$ or table column combination $( t 1 . c 1 )$ as satisfying the requirement. Each variant is executed and its tuple set compared with $o$ as before.\n\nThe output of Xander is Standardised SQL—a modified form of SQL that enforces stricter syntax and a consistent style. Standardised SQL increases accuracy and reduces the difficulty of checking whether a partial query contains an error (more details in the following section).\n\n# Standardised SQL\n\nIn this section, we address the limitations of SQL in the context of LMs and introduce Standardised SQL as a solution.\n\nSQL allows multiple ways to express the same query, leading to inconsistencies in training datasets that can hurt LM performance. During training, LMs may be penalised for predicting a logically correct but stylistically different query, causing unnecessary penalties due to variations in query style (e.g. capitalisation differences). This inconsistency acts like random noise during training, similar to issues encountered in natural language processing (NLP).\n\nTo mitigate this, NLP techniques suggest pre-processing\n\nSTANDARDISED SQL   \nSELECT avg( singer.age ), min( singer.age ) FROM singer   \nWHERE singer.country $\\ c =$ “FRANCE”   \nGROUP BY NONE   \nORDER BY NONE   \nHAVING NONE   \nLIMIT NONE   \nINTERSECT NONE   \nEXCEPT NONE   \nUNION NONE; Input Output Output request Standardised SQL SQL 国 圖 国   \nText: List all book names SELECT names FROM book Select names From book   \nBPE: 3 5 17 199 2 199 4 17 9 199 1 17   \nText: List all film names SELECT names FROM film select names from film   \nBPE: 3 5 19 199 2 199 4 19 6 199 8 19\n\n<html><body><table><tr><td colspan=\"3\">Database D</td><td colspan=\"2\">Question q</td></tr><tr><td colspan=\"3\">Singers</td><td rowspan=\"3\">What are the ages of all singers?</td><td rowspan=\"3\">Example o (21)</td></tr><tr><td>name</td><td>age</td><td>country</td></tr><tr><td>Alice</td><td>21</td><td>USA</td></tr><tr><td>Bob</td><td>31</td><td>Sweden Runtime</td><td rowspan=\"5\"></td></tr><tr><td>Syntax Error</td><td></td><td>Error</td><td>Query-Output Error</td></tr><tr><td>ELECT age FROM Singers</td><td></td><td>SELECT ages FROMSingers</td><td>SELECT name FROM Singers</td></tr><tr><td>not SQL syntax</td><td>ages is not a column</td><td></td><td>wruery gutpst</td></tr></table></body></html>\n\ndata to eliminate unnecessary variations before training (Tabassum and Patil 2020; Chai 2023). Inspired by these methods, we developed Standardised SQL—a stricter syntax that normalises SQL queries to reduce stylistic variations. Standardised SQL adheres to the following rules: (1) SQL keywords are uppercase, following standard typing conventions; (2) Keywords are followed by a space, which makes vocabulary checking easier; (3) All clauses are included and non-optional (this is to eliminate the need for the LM to learn the clause order, e.g. a query where FROM precedes SELECT will error); (4) All aliases are expanded (during testing, we discovered that LMs frequently struggle to determine the association between columns and tables when aliases are used in queries).\n\nAn example of an SQL query and its corresponding Standardised SQL is given in Figure 2. Then, in Figure 3, we have two natural language requests: “List all book names” and “List all film names”, and their corresponding SQL and Standardised SQL output queries. The figure also contains the byte-pair encodings. For Standardised SQL, the difference in the output’s encoding between the two queries is considerably smaller than for SQL. The variation in SQL comes down to low-level stylistic choices chosen by different annotators which makes the BPE output look artificially more different than it is.\n\nOne of the benefits of Standardised SQL is that it enables the checking of partial queries by the Partial-Query Checker.\n\nIntuitively, the stricter syntax of Standardised SQL makes it easier to find faulty queries early on.\n\nThe Partial-Query Checker is an important part of Xander’s architecture. It takes as input the user request (i.e. the question in natural language and any output examples), the database schema and a partial SQL query y, and investigates whether the query is valid (i.e. it can be completed such that the final query corresponds to the initial question) or invalid (i.e. there is no way of completing it in a way that corresponds to the initial question). For this purpose, the Partial-Query Checker checks the query for common syntax, runtime, and query-output errors (see Figure 4 for instances of such errors). A query-output error occurs when the generated query does not return all the output examples provided by the user in the user request. At first it is surprising that query-output errors can be detected without running the query on the database, but sometimes (often enough) query-output errors can be detected just given the database schema—using techniques described below.\n\nAs aforementioned, the Partial-Query Checker only has access to the database schema and not the full database (by contrast the Query Test-and-Repair module described later tests queries on the full database). As part of our evaluation, the Partial-Query Checker is pluggable—we have a standard Partial-Query Checker, the Symbolic Query Checker, but also a neural alternative, the Neural Query Checker. Both are explained below.\n\nSymbolic Query Checker In order to detect the aforementioned errors, the PQC performs three types of checks: vocabulary, scope and type checking.\n\nVocabulary Checking The Standardised SQL query is checked clause by clause. Every clause is checked for containing the right vocabulary. For example, the SELECT clause can only contain aggregation operations, brackets and table-column names. In this step we also reject table-column combinations which do not belong to the schema $\\mathcal { D } _ { s c h e m a }$ . Most errors caught by vocabulary checking will be syntax errors; however, it will also catch the runtime error of selecting an invalid table-column combination.\n\nScope Checking We check that all of the tables referred to by SELECT, WHERE, GROUPBY, and HAVING, clauses also appear in the FROM clause. Scope checking can only detect runtime errors.\n\nType Checking Finally, the types of the tuples which are returned by the SELECT clause are compared to the type of the tuple(s) provided as examples. This ensures that the right number of columns with the right types are chosen. Type checking only catches query-output errors. For illustration, in the example in Figure 4, the query illustrating “queryoutput error” returns a string, whereas the expected output is an integer.\n\nNeural Query Checker We explored an alternative plugin to the Symbolic Query Checker and conducted an experiment comparing it to the Neural Query Checker. While the Symbolic Query Checker proved more accurate and was used in our experiments, we also discuss the Neural Query Checker for completeness.\n\nThe Neural Query Checker is a neural network designed to classify whether a partial query is correct or contains an error, functioning as a critic in reinforcement learning. The main challenge is identifying the location of errors within the query. To address this, we use a method from CodeRL (Le et al. 2022), where the Neural Query Checker processes the query word by word, generating hidden states that encode information about potential mistakes in the partial query. These hidden states are then aggregated using max pooling, producing a single representation that determines whether the entire query contains an error (see Figure 5).\n\nTo train the Neural Query Checker, we generate queries using the training dataset, one of the fine-tuned LMs, and Beam Search (Freitag and Al-Onaizan 2017), labelling each query based on its execution result. Although training uses only complete queries, we expect the network’s accuracy on incomplete queries to be similar due to the max pooling layer, which helps identify mistakes as they occur.\n\n![](images/ad401dcdbcefff416ea0882a546674c92bf3d9c0a38080e9b0b4717d3cb011fa.jpg)  \nFigure 5: Illustration of the Neural Query Checker. $\\mathbf { y _ { 1 : i } }$ denotes the subsequence of the tokenised partial Standardised SQL query generated by a language model up to token $i$ . The length of the partial query is $m$ . A hidden state $\\mathbf { H _ { i } }$ is produced for every subsequence starting at position 1. This hidden state $\\mathbf { H _ { i } }$ is then used as input to max pooling which works across sequence length. The result is input into another Feed Forward Neural Network and finally Softmax is applied to generate the output probabilities which correspond to “correct”, “query-output error”, “runtime error”, and “syntax error”.\n\n# Query Test-and-Repair\n\nGiven a complete query, after we checked it with the PartialQuery Checker, we execute it to verify that it produces a subset of the user-provided output examples. If it does not, the Query Test-and-Repair module tries to repair it. An important design consideration of the Query Test-and-Repair module is that it should keep the (Hamming) difference between the repaired query and the generated query to a minimum. If the repaired query diverges significantly from the original, it may indicate a flawed correction that overlooks the user’s language specification, resulting in a potential false positive.\n\nIn this work the Query Test-and-Repair module generates all queries that differ from the original by exactly one enumerative SQL token (these are SQL operations, column names, or table names). An example is given in Figure 6. It does not try to repair off-by-one errors for constants (e.g. by changing “France” to “Grance”) due to very large searchspace and low likelihood of success.\n\n# Experimental Setup\n\nEnvironment Experiments used Python with the Hugging Face transformers library (Wolf et al. 2020). Except those for Microsoft Phi-1.5, experiments were performed using Tesla P100 GPU and Intel Xeon 6142 CPU. For Microsoft Phi-1.5, due to the larger model size, we used an Amazon EC2 G5.xlarge instance with an A10G (24GB) GPU.\n\nLMs We considered the following LMs for which weights are publicly available: CodeT5 (Wang et al. 2021), BART (Lewis et al. 2019), CodeGen (Nijkamp et al. 2023), and Microsoft Phi-1.5 (Li et al. 2023b). As a comparison point, we used GPT-3.5 Turbo, GPT-4o mini, and GPT-4o, which are not open-source.\n\nDataset We used the Spider (Yu et al. 2018) dataset, which is the most challenging benchmark for the crossdomain and multi-table text-to-SQL. The training set was used to finetune the network and the validation set was used to measure real-world accuracy and runtime.\n\nSince the Spider dataset does not include output examples in the user description, we generate them by executing the golden queries and taking the first returned tuple. As we work with Standardised SQL, we rewrote the Spider dataset to follow Standardised SQL’s constraints. There are a few queries in the original Spider dataset that use complex joins and, therefore, cannot be expressed in Standardised SQL. After removing those, we have 6779 queries in the training dataset, and 1018 in the validation dataset (compared to 7000 and 1034, respectively, in the original Spider dataset).\n\nSELECT avg ( singer.age ) FROM singer WHERE singer.country $\\ c =$ “France”   \nFigure 6: The QTR module takes in a given query (top) and generates all queries of Hamming distance 1, i.e. they differ by one table name (blue), column name (green), aggregation (red), or comparison operator (purple). One such generated query is SELECT max(singer.age) FROM singer WHERE singer.country $\\ c =$ “France”; this differs from the given query by using “max” instead of “avg”.   \n\n<html><body><table><tr><td>Tables</td><td>Columns</td><td>Aggregation</td><td>Comparison Op</td></tr><tr><td>singer</td><td>singer.country</td><td>avg</td><td>= v= <=</td></tr><tr><td>dancer</td><td>singer.age</td><td>min</td><td>!= <</td></tr><tr><td>concert</td><td>singer.sname</td><td>max</td><td></td></tr></table></body></html>\n\nTraining Throughout our experiments we use pretrained LMs. These LMs are further trained (finetuned) to generate SQL or Standardised SQL on the Spider dataset or its Standardised SQL version, depending on the experiment. All networks except Microsoft Phi-1.5 were fitted for 50 epochs with batch size of 10. The Adam (Kingma and Ba 2017) optimiser with a learning rate of $4 e ^ { - 5 }$ was used to find the optimal weights. For Microsoft Phi-1.5, to save memory, batchsize of 1 was used and RMSProp optimiser was used instead of Adam. To account for the larger network size, the learning rate was reduced to $4 \\mathrm { e } ^ { - 6 }$ and the network was fitted for 5 epochs.\n\nEncoding The input to the transformer was the byte-pair encoding $B P E$ of the serialised concatenation of the user question, database schema, and output examples, i.e.\n\n$$\nB P E ( C o n c a t ( s t r ( q ) , s t r ( \\mathcal { D } _ { s c h e m a } ) , s t r ( o ) ) , 3 2 1 0 0 )\n$$\n\nwhere 32100 is the size of the vocabulary. The database contents were not included in the specification because doing so would require too much memory. The golden output was the tokenisation of SQL or Standardised SQL depending on the experiment, both for training and validation datasets.\n\nQuery Evaluation Queries were assessed using Distilled Test Suites (Zhong, Yu, and Klein 2020), leveraging two key metrics: exact-match accuracy and execution accuracy. Exact-match accuracy determines whether the abstract syntax tree (AST) of the generated query aligns perfectly with that of the reference query contained in the Spider dataset, excluding constants. Execution accuracy, evaluates whether the generated query retrieves the correct tuples when executed on a predefined database instance. The reference query serves as the “gold standard,” representing the ideal output, while the generated query is tested against it to measure how closely the language model’s output adheres to the expected behaviour.\n\n# Experimental Results Generalisability\n\nThe aim of this experiment is to explore whether a neurosymbolic approach is faster in terms of runtime and accuracy compared to a purely neural approach. The secondary goal of the experiment is to investigate whether a neurosymbolic approach could provide similar accuracy benefits as scaling the model size.\n\nIn all instances, we provide both the natural language description and the user-provided output tuple, and allow a one-minute time limit to generate a query which returns the user-provided output tuple when executed. In order to enable a fair comparison with Xander, we also enable search of the solution space for the approach without Xander, which also constructs the solution by querying the LM for one token at a time and building a solution tree that gets explored using BFS. Essentially, it follows the same algorithm as the one for Xander in Algorithm 1, with the only exceptions that, at line 6, it only tests the complete query without attempting repair, and it does not call the Partial-Query Checker at line 16. Instead, all incomplete queries are added to the priority queue.\n\nFor the GPT models, due to monetary limitations, we accessed them (through OpenAI’s API) using a single query, which may explain their poorer performance. This allowed us to explore whether smaller open-source LMs enhanced with Xander, can rival much larger closed-source models used off-the-shelf.\n\nThe results of this experiment can be seen in Table 1. The main conclusions are:\n\n1. Xander increases execution accuracy by an average of $1 0 . 9 \\%$ , exact-match accuracy by $6 . 0 \\%$ and it finds the correct query $28 \\%$ faster than using a purely neural approach in the form of a LM. Xander always improves execution accuracy, but we note that for BART model it decreased exact-match accuracy. This is likely because exact-match accuracy has a high false-negative rate.\n\n2. A neurosymbolic approach offers a compelling alternative to scaling model size. The CodeT5 small model beats the four-times-larger CodeT5 base model when CodeT5 small is used with Xander. Also on this point, all the GPT models performed worse than up to an order of magnitude smaller fine-tuned LMs with Xander.\n\n# Xander Ablation\n\nFor the ablation experiment we keep the same experimental setup from the generalisability experiment and only consider the CodeT5 small and CodeT5 base models.\n\nThe aim of the ablation experiment is to investigate what the most important factors for determining Xander’s performance are. To this end, we measure the accuracy and runtime benefits of Standardised SQL, Partial-Query Checker, Query Test-and-Repair, and whether output examples are provided as a part of the user request. The experimental results are reported in Table 2. Training configurations are in bold. The word “examples” in training configuration refers to whether or not user-provided output examples were included in the task description. In normal font, we have the settings used for inference. The entry “1 attempt” means that we stop generating queries after we find a single complete query. The entry “multiple attempts” means we repeatedly generate queries, stopping when we find one that generates the user-provided output examples (or we reach one-minute time limit). The entry “PQC” refers to including the PartialQuery Checker and the entry “repair” refers to including Enumerative Repair in the Query Test-and-Repair module.\n\nTable 1: Table showcasing the results of various LMs with and without using Xander. High accuracy, low training and generation times are desirable. The best performing model is highlighted.   \n\n<html><body><table><tr><td>Model</td><td>#parameters</td><td>Training Time</td><td>Validation Time per sample (s)</td><td>Exact-Match Accuracy (%)</td><td>Execution Accuracy (%)</td></tr><tr><td>CodeT5small</td><td>(62M)</td><td>3h8m</td><td>15.7</td><td>60.7</td><td>69.9</td></tr><tr><td>Dittowith Xander</td><td></td><td>2h59m</td><td>9.1</td><td>66.0</td><td>78.6</td></tr><tr><td>CodeT5base</td><td>(222M)</td><td>10h8m</td><td>10.7</td><td>63.5</td><td>73.6</td></tr><tr><td>Dittowith Xander</td><td></td><td>9h39m</td><td>7.3</td><td>68.9</td><td>80.5</td></tr><tr><td>BART</td><td>(139M)</td><td>6h8m</td><td>25.3</td><td>51.5</td><td>58.1</td></tr><tr><td>Dittowith Xander</td><td></td><td>5h50m</td><td>19.5</td><td>39.0</td><td>59.0</td></tr><tr><td>CodeGen</td><td>(350M)</td><td>19h22m</td><td>54.6</td><td>4.4</td><td>5.9</td></tr><tr><td>Ditto with Xander</td><td></td><td>18h31m</td><td>47.5</td><td>6.8</td><td>14.4</td></tr><tr><td>Microsoft Phi-1.5</td><td>(1.3B)</td><td>3h49m</td><td>34.1</td><td>30.5</td><td>42.3</td></tr><tr><td>Ditto with Xander</td><td></td><td>3h41m</td><td>18.1</td><td>59.8</td><td>72.0</td></tr><tr><td>GPT-3.5 Turbo</td><td>(Unknown)</td><td>N/A</td><td>0.9</td><td>33.5</td><td>69.1</td></tr><tr><td>GPT-4o mini</td><td>(Unknown)</td><td>N/A</td><td>1.0</td><td>30.0</td><td>75.2</td></tr><tr><td>GPT-40</td><td>(Unknown)</td><td>N/A</td><td>1.3</td><td>38.8</td><td>76.1</td></tr></table></body></html>\n\nThe most important factor in determining Xander’s accuracy is Standardised SQL which provides a $9 . 5 \\%$ execution accuracy increase when a single attempt is allowed. The addition of the Partial-Query Checker gives further $4 . 2 \\%$ improvement to execution accuracy. Using output examples as part of the description, only improves exaction accuracy by $0 . 3 \\%$ . Finally, the Query Test-and-Repair module improves exaction accuracy only by $0 . 2 \\%$ , but improves the time to find a query by an average of $2 5 \\%$ .\n\n# Partial-Query Checker Comparison Experiment\n\nQueries from the validation set were used to compare the efficacy of Neural Query Checker and Symbolic Query Checker. In particular, we generate top four most promising queries for each validation question. Then, we compare the classifications obtained by using the Symbolic Query Checker with the classification obtained by using the Neural Query Checker. Finally, we construct two confusion matrices to compare their accuracy (see Figure 7). The results allow us to draw the following conclusions:\n\n1. The Symbolic Query Checker is $22 \\%$ more accurate than the Neural Query Checker at recognising when a query contains a mistake. This can be at least partly attributed to the Neural Query Checker’s inability to recognise runtime errors which occurred in $28 \\%$ of the queries.\n\n2. The Symbolic Query Checker cannot differentiate between query-output errors and correct queries. This is because only column types are used to detect query-output errors and column type mistakes are rarely $\\bar { 1 . 3 \\% }$ of all queries) made by the LM.\n\n3. The Neural Query Checker can detect most syntax errors. While this is a promising finding, it is worth noting that the LM also rarely makes syntax errors. Indeed, only $5 \\%$ of the queries contain a syntax error.\n\nSymbolic Partial Query Checker Neural Partial Query Checker Accuracy: $6 8 . 7 \\%$ Accuracy: $4 6 . 3 \\%$ correct 1293 0 0 2 correct 869 357 69 0   \nexearrmoprle798 53 30 8 exearrmoprle445 415 28 1   \nruntime 87 1 1320 249 runtime 738 507 403 9 error error syntax error 5 0 94 124 syntax error 8 19 0 196 syntax runtime example correct error error error syntax runtime example correct error error error predicted class predicted class\n\n# Comparison to Existing Works\n\nTo the best of our knowledge, the only other work that accepts both NL and output examples is Duoquest (Baik et al. 2020). While they do not support 445 $( 4 3 \\% )$ queries of the Spider validation dataset (e.g. clauses with multiple selection predicates), for the supported queries, they report an execution accuracy of $6 3 . 5 \\%$ .\n\nFrom text-to-SQL, Picard (Scholak, Schucher, and Bahdanau 2021) is the closest to our approach, as it enhances fine-tuned LMs out-of-the-box by rejecting inadmissible tokens. However, Picard does not use output examples, explores the solution space differently by using beam search instead of best-first search, and does not attempt to repair solutions. In experiments, Picard improves exact-match accuracy by $5 . 1 5 \\%$ and execution accuracy by $6 . 5 \\%$ over base LMs. These improvements are smaller than those achieved by Xander $6 \\%$ and $1 0 . 9 \\%$ , respectively). Since Picard does not use output examples, a direct comparison is not possible.\n\n<html><body><table><tr><td colspan=\"2\"></td><td rowspan=\"2\">Training Configuration Training Time</td><td rowspan=\"2\">Validation Time per sample (s)</td><td rowspan=\"2\">Exact Match Accuracy (%)</td><td rowspan=\"2\">Execution Accuracy (%)</td></tr><tr><td>Model</td><td>Xander options</td></tr><tr><td rowspan=\"2\">CodeT5 small</td><td>SQL + No Examples + No Finetuning</td><td>0h0m</td><td>1</td><td></td><td>-</td></tr><tr><td>1 attempt</td><td>1</td><td>30.6</td><td>0</td><td>0</td></tr><tr><td rowspan=\"2\">CodeT5 small*</td><td>SQL + No Examples</td><td>3h4m</td><td></td><td>1</td><td>1</td></tr><tr><td>1 attempt</td><td>1</td><td>11.6</td><td>1.7</td><td>2.4</td></tr><tr><td rowspan=\"2\">CodeT5 small</td><td>SQL + No Examples</td><td>3h4m</td><td></td><td></td><td>1</td></tr><tr><td>1 attempt</td><td>-</td><td>0.5</td><td>46.9</td><td>48.7</td></tr><tr><td>CodeT5 small</td><td>Standardised SQL + No Examples</td><td>2h59m</td><td>1</td><td></td><td>1</td></tr><tr><td>CodeT5 small</td><td>1 attempt Standardised SQL + Examples</td><td>1 2h59m</td><td>0.9</td><td>56.4</td><td>58.4</td></tr><tr><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td rowspan=\"8\">CodeT5 base</td><td>1 attempt</td><td>-</td><td>0.8</td><td>56.8</td><td>59.7</td></tr><tr><td>PQC + 1 attempt</td><td>-</td><td>1.1</td><td>59.0</td><td>63.9</td></tr><tr><td>multiple attempts</td><td></td><td>12.8</td><td>66.8</td><td>76.0</td></tr><tr><td>PQC + multiple attempts</td><td>-</td><td>10.2</td><td>68.3</td><td>78.4</td></tr><tr><td>repair + PQC + multiple attempts</td><td>-</td><td>9.1</td><td>66.0</td><td>78.6</td></tr><tr><td>Standardised SQL+Examples</td><td>9h39m</td><td></td><td></td><td></td></tr><tr><td>PQC + 1 attempt</td><td>-</td><td>2.7</td><td>62.9</td><td>66.5</td></tr><tr><td>PQC + multiple attempts repair + PQC + multiple attempts</td><td>- -</td><td>10.3 7.3</td><td>71.2 68.9</td><td>80.4 80.5</td></tr></table></body></html>\n\nTable 2: Table showcasing a Xander ablation study. The “\\*” denotes that weights were initialised randomly as opposed to using the pretrained weights. The row containing the best accuracy is highlighted.\n\nThe majority of the works on text-to-SQL require customising existing LMs. For instance, RASAT (Qi et al. 2022) augments the self-attention modules in a model’s encoder and introduces new parameters to the model. When customising T5, it achieves exact-match accuracy of $7 2 . 6 \\%$ and execution accuracy of $7 6 . 6 \\%$ on the Spider validation set. REDSQL (Li et al. 2023a) breaks SQL generation into the generation of a skeleton of SQL keywords, which is then filled in with the missing values. For this purpose, it relies on a a ranking-enhanced encoder to alleviate the effort of the schema linking and a skeleton aware decoder to implicitly guide the SQL parsing. By customising T5-base, REDSQL achieves exact-match accuracy of $7 1 . 7 \\%$ and execution accuracy of $7 7 . 9 \\%$ on the Spider validation set. Both RASAT and REDSQL achieve marginally better exact-match accuracy but lower execution accuracy. We hypothesise that by customising the base LM, RASAT and REDSQL are able to better capture the syntactic patterns in the Spider dataset which leads to higher exact-match accuracy, while Xander captures the intent of the query by deriving information from the user-provided output examples which increases execution accuracy.\n\nMulti-agent Architectures with LMs While, initially, LMs were used as black-box, monolithic entities, recently, there has been a shift towards architectures that foster some form of logical reasoning as part of the problem-solving process, sometimes by leveraging additional, possibly nonneural systems (Karpas et al. 2022; Creswell and Shanahan 2022; Shen et al. 2023; Wei et al. 2022). Given that LLMs were shown to have difficulty with proof planning when using a linear search strategy (Saparov and He 2023), other works are focused on decision-making and space exploration (Schlag et al. 2023; Liu et al. 2023; Yao et al. 2023; Long 2023). As opposed to these works, we propose a neurosymbolic architecture for generating SQL queries that uses verification and repair modules to guide a non-linear exploration of the solution space.\n\n# Conclusions\n\nThis work explored whether neurosymbolic approaches could serve as an alternative to scaling model size. Specifically, we built a tool called Xander for the text-to-SQLwith-examples task. Xander is able to dismiss large parts of the search-space by standardising SQL and pruning bad queries before they have finished generating. Evaluation of Xander showed that neurosymbolic approaches can lead to significant improvements in runtime and accuracy, enabling a smaller LM to outperform its four-times-larger counterpart. This means that neurosymbolic approaches provide a compelling alternative to the de facto industry standard of improving performance by scaling model size.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决的核心问题是：如何提升语言模型（LM）在SQL查询生成任务中的性能，特别是在处理非线性搜索空间和多种有效推理路径时的局限性。现有方法主要依赖线性搜索策略，导致无法系统性地探索替代方案，从而限制了性能。\\n> *   该问题的重要性在于：SQL查询生成在数据库交互、自动化数据处理等领域具有广泛应用，而现有方法在准确性和效率上存在瓶颈，尤其是在处理复杂查询时。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种神经符号架构（Xander），通过结合符号推理和语言模型，实现非线性搜索空间的探索，从而提升SQL查询生成的性能和效率。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **创新贡献1：** 提出了一种神经符号设计，通过符号推理引导LM的搜索空间探索，支持回溯和多路径评估。实验显示，该方法平均提升执行准确率10.9%，并减少28%的运行时。\\n> *   **创新贡献2：** 引入了标准化SQL（Standardised SQL），通过减少语法风格差异提升LM的训练和推理效率。标准化SQL使执行准确率提升9.5%。\\n> *   **创新贡献3：** 实现了开源工具Xander，实验证明其能够使小型LM的性能超越四倍参数量的更大模型。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   核心原理是通过符号推理模块（Partial-Query Checker和Query Test-and-Repair）与语言模型协同工作，实现对搜索空间的非线性探索。符号模块能够在不完整查询阶段进行早期错误检测和修复，从而减少无效路径的生成。\\n> *   设计哲学是“早修剪、早修复”，通过符号检查提前终止无效路径，避免生成完整查询后再验证的低效问题。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 现有方法（如Chain of Thought）依赖线性搜索，无法回溯或多路径探索；而Xander通过符号推理实现了非线性搜索和回溯能力。\\n> *   **本文的改进：** 引入标准化SQL减少语法噪声；设计Partial-Query Checker支持不完整查询的早期验证；Query Test-and-Repair模块支持查询修复。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  输入任务描述（数据库模式、自然语言问题、输出示例）并编码为字节对（BPE）。\\n> 2.  使用优先级队列（Best-First Search）管理候选查询，初始为空查询。\\n> 3.  对于每个不完整查询，调用LM生成5个最可能的下一令牌候选。\\n> 4.  通过Partial-Query Checker（符号或神经）验证候选查询的合理性，剔除错误路径。\\n> 5.  将有效候选加入优先级队列，按概率排序。\\n> 6.  若查询完整，则通过Query Test-and-Repair模块执行修复验证；否则继续扩展。\\n> 7.  输出通过验证的标准化SQL查询。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   对比模型包括：CodeT5-small/base、BART、CodeGen、Microsoft Phi-1.5、GPT-3.5 Turbo、GPT-4o mini、GPT-4o。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在执行准确率上：** 本文方法在Spider数据集上达到了78.6%（CodeT5-small + Xander），显著优于基线模型CodeT5-small（69.9%）和CodeT5-base（73.6%）。与表现最佳的基线相比，提升了4.7个百分点。\\n> *   **在推理速度上：** 本文方法的处理速度为9.1秒/样本（CodeT5-small + Xander），远低于纯神经方法的15.7秒/样本，同时与轻量级模型CodeT5-base（7.3秒/样本）的速度相当，但在准确率上远超后者。\\n> *   **在模型规模效率上：** 使用Xander的CodeT5-small（62M参数）在准确率上超越四倍参数量的CodeT5-base（222M参数）。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   SQL查询生成 (SQL Query Generation, N/A)\\n*   神经符号架构 (Neurosymbolic Architecture, N/A)\\n*   语言模型 (Language Model, LM)\\n*   标准化SQL (Standardised SQL, N/A)\\n*   部分查询检查器 (Partial-Query Checker, PQC)\\n*   查询测试与修复 (Query Test-and-Repair, QTR)\\n*   最佳优先搜索 (Best-First Search, BFS)\\n*   字节对编码 (Byte-Pair Encoding, BPE)\"\n}\n```"
}