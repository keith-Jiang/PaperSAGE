{
    "source": "Semantic Scholar",
    "arxiv_id": "2408.11281",
    "link": "https://arxiv.org/abs/2408.11281",
    "pdf_link": "https://arxiv.org/pdf/2408.11281.pdf",
    "title": "BearLLM: A Prior Knowledge-Enhanced Bearing Health Management Framework with Unified Vibration Signal Representation",
    "authors": [
        "Haotian Peng",
        "Jiawei Liu",
        "Jinsong Du",
        "Jie Gao",
        "Wei Wang"
    ],
    "categories": [
        "cs.AI"
    ],
    "publication_date": "2024-08-21",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "institutions": [
        "Shenyang Institute of Automation, Chinese Academy of Sciences",
        "Liaoning Liaohe Laboratory",
        "Key Laboratory on Intelligent Detection and Equipment Technology of Liaoning Province",
        "University of Chinese Academy of Sciences"
    ],
    "paper_content": "# BearLLM: A Prior Knowledge-Enhanced Bearing Health Management Framework with Unified Vibration Signal Representation\n\nHaotian Peng1, 2, 3, 4\\*, Jiawei $\\mathbf { L i u } ^ { 1 , 2 , 3 * }$ , Jinsong $\\mathbf { D } \\mathbf { u } ^ { 1 , 2 , 3 }$ , Jie ${ \\bf G a o } ^ { 1 , 2 , 3 \\dagger }$ , Wei Wang1, 2, 3‚Ä† ,\n\n1Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China 2Liaoning Liaohe Laboratory, Shenyang, China 3Key Laboratory on Intelligent Detection and Equipment Technology of Liaoning Province, Shenyang, China 4University of Chinese Academy of Sciences, Beijing, China {penghaotian, liujiawei, jsdu, gaojie, wangwei2}@sia.cn\n\n# Abstract\n\nWe propose a bearing health management framework leveraging large language models (BearLLM), a novel multimodal model that unifies multiple bearing-related tasks by processing user prompts and vibration signals. Specifically, we introduce a prior knowledge-enhanced unified vibration signal representation to handle various working conditions across multiple datasets. This involves adaptively sampling the vibration signals based on the sampling rate of the sensor, incorporating the frequency domain to unify input dimensions, and using a fault-free reference signal as an auxiliary input. To extract features from vibration signals, we first train a fault classification network, then convert and align the extracted features into word embedding, and finally concatenate these with text embedding as input to an LLM. To evaluate the performance of the proposed method, we constructed the first large-scale multimodal bearing health management (MBHM) dataset, including paired vibration signals and textual descriptions. With our unified vibration signal representation, BearLLM using one set of pre-trained weights achieves stateof-the-art performance on nine publicly available fault diagnosis benchmarks, outperforming specific methods designed for individual datasets. We provide a dataset, our model, and code to inspire future research on building more capable industrial multimodal models.\n\n# Code ‚Äî https://github.com/SIA-IDE/BearLLM\n\n# 1 Introduction\n\nBearings are the core components of mechanical rotating equipment but have high failure rates due to complex operational and environmental conditions (Wang et al. 2020b). Bearing health management (e.g., anomaly detection, fault diagnosis, and maintenance recommendations) is of great practical significance in industrial safety production to reduce economic losses and maintenance costs (Peng et al. 2022; Xiao et al. 2022; Ruan et al. 2023).\n\nCurrent bearing health management frameworks rely on designing specialized methods for different working conditions and tasks, as shown in Fig. 1 (a). To apply specific methods to complex real-world industrial scenarios, domain\n\nMultiple Multiple Multiple Conditions Methods Tasks Our MBHM Multiple Dataset Tasks   \nDataset M1 Diag. #1 M2 RUL Diag. BearLLM M3 Detect Treat   \nDataset #2 rwh M4 Diag. Risk (262 Conditions) (a) Existing bearing health (b) Our Proposed bearing management framework health management framework\n\nadaptation, and generalization have attracted widespread attention. Domain adaptation enables a model trained on one source domain to perform well on different but related target domains by reducing the domain shift or discrepancy (Wu et al. 2022; Zhang et al. 2022), but it suffers from low accuracy when the source and target domains are categoryinconsistent (e.g., transitioning from working condition $C _ { 1 }$ with four fault types to $C _ { 2 }$ with five types). Domain generalization aims to extract domain-invariant features to improve performance on unseen domain (Li et al. 2020; Zheng et al. 2021; Chen et al. 2022a), but it is often constrained to a limited number of working conditions with small differences, e.g., fewer than ten working conditions in (Chen et al. 2022b; Lin et al. 2023). These purely data-driven methods often fail to strike an optimal balance between high accuracy and strong generalization for fault diagnosis.\n\nIn this paper, we propose a prior knowledge-enhanced bearing large language model (BearLLM), which can unify multiple bearing health management tasks over hundreds of different working conditions from multiple datasets, as shown in Fig. 1 (b). To handle various working conditions, we introduce a prior knowledge-enhanced unified vibration signal representation. Unlike most fault diagnosis methods that use fixed-length input segments, we sample vibration signals as variable-length but fixed-duration segments. These duration-consistent segments are then converted to the frequency domain and are aligned. We further utilize a faultfree reference signal as a prior input, eliminating the need for complex mechanism analysis for various bearing designations (Zheng et al. 2021).\n\nSpecifically, we first design a fault classification network (FCN) to extract fault features based on the differences in frequency components between the query signal segment and the fault-free reference signal segment. This new frequency-based feature extraction paradigm for bearing fault diagnosis is more efficient (i.e., faster convergence and higher accuracy) and achieves stronger generalization, compared to previous methods that extract fault features directly from vibration signals. The extracted features are then transformed and aligned into word embedding, which is subsequently connected to user text embedding as inputs to the LLM. To evaluate the performance of the proposed method, we construct the first large-scale multimodal bearing health management (MBHM) dataset, including paired vibration signals and textual descriptions. Although the vibration signals from the nine public datasets differ significantly in distribution, BearLLM with a set of pre-trained weights achieves state-of-the-art performance using a unified vibration signal representation, outperforming specialized methods designed for individual datasets. The contributions of this paper are summarized as follows:\n\n‚Ä¢ We propose a novel bearing multimodal large language model, unifying multiple bearing health management tasks by aligning vibration signals and textual prompts.   \n‚Ä¢ We propose a prior knowledge-enhanced unified vibration signal representation to handle various working conditions from multiple datasets.   \n‚Ä¢ We construct the first large-scale multimodal dataset for bearing health management (MBHM), involving vibration signals with associated textual descriptions.   \n‚Ä¢ Experimental results show that our BearLLM outperforms state-of-the-art fault diagnosis methods on nine publicly available benchmarks.\n\n# 2 Related Works\n\nMultiple Working Condition: Fault diagnosis under various working conditions from multiple datasets presents a challenge due to the heterogeneity of collected signals arising from variations in the test rigs, sensors, and environment, making it difficult to obtain unified features (Wen, Guo, and Li 2023). Existing domain adaptation methods(Chen, Gryllias, and Li 2020; Wan et al. 2022; Ma et al. 2023; Huo et al. 2023) typically involves training a model under known working conditions (source domain) and subsequently transferring knowledge to an unknown working condition (target domain). However, these approaches still necessitate individual transfer fine-tuning for each working condition in practice, hindering its ability to generalize across multiple scenarios. Domain generalization methods leverage training on multiple working conditions and aim to align the feature distributions of different domains through the design of network architectures and loss functions (Jia et al. 2023; Zhu, Chen, and Tang 2023; Hou et al. 2023b). However, these approaches often rely on complex data preprocessing and augmentation techniques to help models learn fault features from vibration signals.\n\nMultiple Tasks: Data-driven machinery health management have gained significant traction (Trabelsi et al. 2020).The concept of health management usually involves multiple tasks (Omri et al. 2021; Zio 2022), including anomaly detection, fault diagnosis, degradation prediction, maintenance decision-making, etc. LLMs such as ChatGPT4 (OpenAI et al. 2024) have demonstrated exceptional capabilities across a wide range of tasks. The emergence of opensource foundational models like LLaMA 3 (Meta 2024) and Qwen 2 (Bai et al. 2023) have further empowered researchers in various disciplines to integrate these models into their own applications. In the aviation domain, Liu et al. (2024) applied generalized linear models to achieve multiple tasks, including assembly guidance and assembly error identification for aircraft engines. In the petroleum industry, Eckroth et al. (2023) designed a question-answering system based on LLM and knowledge graph, enabling retrieval of functionalities such as stratigraphy data and geological age determination. However, research integrating multiple tasks using LLMs for bearing health management remains limited (Li, Wang, and Sun 2024).\n\n# 3 A Multimodal Bearing Health Management Dataset\n\nAlthough several bearing-related datasets in Tab. 1 are available, they generally collect vibration signals on a single test rig, have a limited number of working conditions, and have no corresponding textual descriptions for training LLM. We have constructed a large-scale publicly multimodal dataset for bearing health management (MBHM).\n\nTable 1: Comparison of different datasets. Our MBHM dataset has the largest number of working conditions, the most complete fault types, and the longest time, paired textual prompts/responses.   \n\n<html><body><table><tr><td>Dataset</td><td>Sample Rate (kHz)</td><td>Condi- tions*</td><td>Fault Types</td><td>Time (s)</td><td>Text</td></tr><tr><td>CWRU</td><td>12/48</td><td>12</td><td>10</td><td>3932</td><td>√ó</td></tr><tr><td>DIRG</td><td>51.2</td><td>102</td><td>7</td><td>7140</td><td>√ó</td></tr><tr><td>HIT</td><td>20</td><td>40</td><td>3</td><td>9648</td><td>√ó</td></tr><tr><td>IMS</td><td>20</td><td>16</td><td>7</td><td>46480</td><td>√ó</td></tr><tr><td>JNU</td><td>100</td><td>45</td><td>4</td><td>3600</td><td>√ó</td></tr><tr><td>JUST</td><td>50</td><td>36</td><td>4</td><td>43986</td><td>√ó</td></tr><tr><td>MFPT</td><td>48.8/97.6</td><td>1</td><td>3</td><td>78</td><td>√ó</td></tr><tr><td>PU</td><td>64</td><td>4</td><td>5</td><td>7316</td><td>√ó</td></tr><tr><td>XJTU</td><td>25.6</td><td>6</td><td>10</td><td>13336</td><td>√ó</td></tr><tr><td>MBHM</td><td>12 ~100</td><td>262</td><td>10</td><td>135516</td><td>‚àö</td></tr></table></body></html>\n\nThe same working conditions represent the same load, speed, and sensor.\n\nThe MBHM contains 135,516 pairs of vibration signal segments and fault types, and 542,064 pairs of text cues and responses, of which each sample is shown in Fig. 3, contains a vibration signal, a fault label, an operating condition id, a user prompt, and a text response, ie, $( X _ { v } , \\bar { L } _ { v } , C , X _ { t } , L _ { t } ) \\in$ MBHM. Our dataset contains 262 working conditions collected from nine publicly accessible datasets, i.e., CWRU\n\ndiuut X Fault Diagnosis [1, ] Fine-tuning   \nQuery Vibration Signal Maintenance (Multi-Condition) DCN y [3, 24000] FCN MLP [7, 1356] LLM Recommendations [128, 47] w Feature Encoder Alignment Layer Abnormal Detect   \nFault-free Reference Signal [1, ] Unified Signal Representation Multi-Task U(seMruIlntis-trTuacstki)on Xt Tokeizer & Embedding [ , 1356] Ht NetuRreaslpLoansegsuage BearLLM   \nVibration Signal $X _ { v }$   \nFault Label $L _ { v }$ 3 (severe inner ring)   \nWorking Condition $c$ 12 (DIRG, 100rpm, 500N, sensor#1)   \nPrompt Text $X _ { t }$ ... provide targeted maintenance recommendations based on the fault location and severity.   \nResponse Text $L _ { t }$ The bearing is in a severe faulty state of inner ring fault. Maintenance recommendation: Replace the bearing immediately to prevent further damage ...\n\nFigure 3: Sample case of our MBHM dataset, includes vibration signal $X _ { v }$ , fault label $L _ { v }$ , working condition $C$ , the specific task prompt text $X _ { t }$ , and the response text $L _ { t }$ .\n\n(Case Western Reserve University 2008), DIRG (Daga et al. 2019), HIT (Hou et al. 2023a), IMS (Qiu et al. 2006), JNU (Jiangnan University 2012), JUST (Ren 2023), MFPT (Eric 2012), PU (Lessmeier et al. 2016), XJTU (Wang et al. 2020a). For each vibration signal, we have four different tasks, i.e., anomaly detection, fault diagnosis, maintenance recommendations, and potential risk analysis by generating text responses using ChatGPT (OpenAI et al. 2024). Detailed methodologies for dataset construction are provided in Appendix A.3. Our MBHM dataset contains the following features:\n\n‚Ä¢ Multi-modal: Each vibration signal is paired with four text prompts and responses, supporting the training and development of multimodal multi-task models. ‚Ä¢ Multiple working conditions: Our dataset covers a wider range of working conditions, more accurately modeling real-world industrial production scenarios.\n\n# 4 Method\n\nIn this section, we propose BearLLM, a novel multimodal model that unifies multiple bearing-related tasks. To handle various working conditions across multiple datasets, we introduce a prior knowledge-enhanced unified vibration signal representation in Section 4.1. The unified vibration signal is fed to a fault classification network to extract features in Section 4.2. We convert and align the extracted features into word embedding, and finally concatenate these with text embedding as input to an LLM in Section 4.3.\n\n# 4.1 Prior Knowledge-Enhanced Unified Vibration Signal Representation\n\nBearLLM aims to manage multiple bearing-related tasks across hundreds of working conditions. The basis for this is to build a unified vibration signal representation, involving adaptively sampling the vibration signal segments based on the sensor sampling rate, incorporating the frequency domain to unify input dimensions, and using a fault-free reference signal to calculate residual as auxiliary inputs to improve data utilization efficiency.\n\nAdaptive Sampling To monitor various mechanical devices across different working conditions and industrial scenarios, vibration sensors are deployed with varying designations and sampling rates. However, most fault diagnosis methods (Zhu, Chen, and Tang 2023; Dong et al. 2024) use fixed-length signal segments in the time domain as inputs, where the fault frequency components in the inputs deviate from their original intrinsic values and vary with the sampling rate, hindering accurate fault diagnosis. Instead of sampling fixed-length signal segments, we adaptively sample vibration signals as variable-length but fixed-duration segments using prior knowledge of the sensor sampling rate. We extract the $m$ -th query signal segment $X _ { v } \\in \\bar { \\mathbb { R } } ^ { 1 \\times s }$ from the original signal $X _ { o }$ by\n\n$$\nX _ { v } = X _ { o } [ m s , ( m + 1 ) s ] ,\n$$\n\nwhere $s$ denotes the sampling rate of the sensor and controls the length of the $X _ { v }$ .\n\nFrequency-Domain Input Alignment After adaptive sampling, each query segment $( X _ { v } )$ has an equal duration, and the frequencies of $X _ { v }$ are aligned. However, varying lengths of $X _ { v }$ (due to different sampling rates) result in different numbers of frequency components, making them unsuitable for input to the network. We design a discrete cosine normalization (DCN) that consists of converting the vibration signal to the frequency domain using the discrete cosine transform (DCT), unifying the number $n _ { f }$ of frequency components using a pad or cut, and standardizing the amplitude using the normalization $\\mathcal { N }$ . The normalized frequency representation $F _ { v } \\in \\mathbb { R } ^ { 1 \\times n _ { f } }$ is obtained by\n\n$$\nF _ { v } = \\left\\{ \\underset { \\mathcal { N } ( \\mathrm { D C T } ( X _ { v } ) \\cup [ 0 ] _ { n _ { f } - s } ) , \\quad \\mathrm { i f } ~ s < n _ { f } } { \\mathcal { N } ( \\mathrm { D C T } ( X _ { v } ) \\cup [ 0 ] _ { n _ { f } - s } ) , \\quad \\mathrm { i f } ~ s \\ge n _ { f } } \\right.\n$$\n\nSignals with sampling rates below ${ { n } _ { f } }$ are zero-padded, while those exceeding $n _ { f }$ are cut. To balance computational resources and fault classification accuracy, we empirically set $n _ { f } = 2 4 0 0 0$ (more detail in Tab. 3). To enhance training stability, the amplitude of the frequency sequence is normalized to $[ - 1 , 1 ]$ ,\n\n$$\n\\mathcal { N } ( x ) = \\beta \\frac { \\sqrt { n } x } { \\| x \\| _ { 2 } } ,\n$$\n\nwhere $\\beta$ is a scaling factor and is set to 0.01 by statistically analyzing our MBHM dataset.\n\nFault-Free Reference Signal To eliminate distributional differences from different inputs under various operating conditions, we introduce fault-free signals as reference signals. 1) In practical use, the reference signal segment $\\tilde { X } _ { v }$ can be collected and saved when the equipment is working properly, such as after factory acceptance or maintenance; 2) in training on the MBHM dataset, $\\tilde { X } _ { v }$ is acquired by\n\n$$\n\\begin{array} { r } { \\tilde { X } _ { v } \\sim \\left\\{ X _ { v } ^ { \\ast } | ( X _ { v } ^ { \\ast } , L _ { v } ^ { \\ast } , C ^ { \\ast } , X _ { t } ^ { \\ast } , L _ { t } ^ { \\ast } ) \\in \\mathrm { M B H M } , \\right. } \\\\ { \\left. L _ { v } ^ { \\ast } = 0 , C ^ { \\ast } = C \\right\\} . \\qquad } \\end{array}\n$$\n\nThis indicates that $\\tilde { X } _ { v }$ is selected when a signal $( X _ { v } ^ { * } )$ of our MBHM dataset is fault-free (i.e., $L _ { v } ^ { * } = 0 \\$ ) and has the same working conditions as $X _ { v }$ (i.e., $C ^ { * } = C$ ).\n\nWe combine the query frequency signal $( F _ { v } )$ , the faultfree frequency signal $( \\tilde { F _ { v } } )$ , and the residual frequency signal $( F _ { r e s } = \\bar { F _ { v } } - \\tilde { F _ { v } } )$ as unified vibration signal representation,\n\n$$\nR _ { v } = [ F _ { v } , \\tilde { F } _ { v } , F _ { r e s } ] .\n$$\n\n# 4.2 Feature Extraction\n\nAlgorithm 1: Training Algorithm\n\nInput: $\\theta _ { E } , \\theta _ { C } , \\theta _ { A } , \\theta _ { L }$ the weights of feature encoder, linear classification layer, alignment layer, and LLM in our BearLLM; $X _ { v } , L _ { v } , X _ { t } , L _ { t }$ the vibration signal, fault label, prompt text, and response text from MBHM dataset   \nOutput: The optimal parameters of BearLLM $\\theta _ { E } ^ { * } , \\theta _ { A } ^ { * } , \\theta _ { L } ^ { * }$ Step 1: Pre-training FCN for $e \\gets 1$ to 50 epoches do $R _ { v }  X _ { v }$ ‚óÅ get unified representation by Eq. 5 $P = \\mathrm { F C N } ( R _ { v } )$ $\\theta _ { E } ^ { * } , \\theta _ { C } ^ { * } \\xleftarrow { + } - \\nabla _ { \\theta _ { E } , \\theta _ { C } } ( \\mathrm { C E } ( P , L _ { v } ) )$ ‚óÅ cross-entropy end for Step 2: Fine-tuning BearLLM Init. $\\theta _ { A }$ by Eq. 7 for $e \\gets 1$ to 20 epoches do $Y = { \\mathrm { B e a r L L M } } ( X _ { t } , X _ { v } )$ Œ∏ $\\mathbf { \\Phi } _ { A } ^ { * } , \\theta _ { L } ^ { * } \\gets \\mathrm { P E F T } ( Y , L _ { t } )$ end for return $\\theta _ { E } ^ { * } , \\theta _ { A } ^ { * } , \\theta _ { L } ^ { * }$   \nInput Feature Encoder F Conv1D (16) [8, 3000] MSCAB [6016] Linear [128, 750] [128] [60, 3000] F Conv1D (16) MSCAB Linear 2 [128, 188] Conv1D (16) [60, 3000] Wide Conv. MSCAB [128, 47] [10] Output   \n[3, 24000] Fault 128 Conv1D (3) 32 MSCAB Channel 128 BN 96 Attenti√óon 96 Conv1D (5) CAM ReLU 128 Conv1D (7) 96 128 Multi-Scale Conv1D (1) 32 128\n\nTo extract the features of vibration signals, we propose a fault diagnosis network (FCN) containing a feature encoder parameterized by $\\theta _ { E }$ and a linear classification layer parameterized by $\\theta _ { C }$ , as shown in Fig. 4. We extract features from the unified vibration signal representation $( R _ { v } )$ using three separate convolutional layers with large kernels (Zhang et al. 2017) and no weight sharing. We then transform features by three multiscale channel attention blocks (MSCAB) where the multiscale features are fused using the channel attention module (CAM) (Woo et al. 2018). Finally, we use two linear layers for fault classification.\n\nOur FCN takes unified representation $( R _ { v } )$ as input and outputs the fault type $( P )$ . The shape of $P$ is $[ 1 , \\gamma ]$ and $\\gamma$ denotes the number of fault types. We use cross-entropy loss for training with fault label $L _ { v }$ as ground truth. The training procedure is described in Algo. 1. The well-trained feature encoder weights $( \\theta _ { E } ^ { * } )$ of FCN are then used and frozen in BearLLM (see Fig. 2), while the classifier weights $( \\theta _ { C } ^ { * } )$ of FCN are used to initialize the alignment layer.\n\n# 4.3 Feature Alignment\n\nWe propose a feature alignment layer to embed vibration features into word embedding, which is an MLP consisting of three linear layers (i.e., $l _ { 1 } , l _ { 2 } , l _ { 3 } )$ . The weights of alignment layer is $\\theta _ { A } = [ \\theta _ { C } ^ { * } , \\theta _ { l _ { 3 } } ]$ , where $\\theta _ { C } ^ { * }$ is the weights of $l _ { 1 } \\& l _ { 2 }$ (i.e., two linear classification layers in FCN) and $\\theta _ { l _ { 3 } }$ is the weights of $l _ { 3 }$ . We use $l _ { 3 }$ transforms the output $P$ of $l _ { 2 }$ into the word embedding $H _ { v } = \\mathrm { r e s h a p e } ( l _ { 3 } ( P ) )$ , i.e.,\n\n$$\nP \\in \\mathbb { R } ^ { 1 \\times \\gamma } \\xrightarrow { l _ { 3 } } \\mathbb { R } ^ { 1 \\times \\tau h } \\xrightarrow { \\mathrm { r e s h a p e } } H _ { V } \\in \\mathbb { R } ^ { \\tau \\times h } ,\n$$\n\nwhere $\\tau$ signifies the token length after transformed, $h$ is the hidden size of the LLM.\n\nThe weight $\\theta _ { l _ { 3 } }$ of $l _ { 3 }$ is initialized from the textual descriptions $K$ of all fault categories by\n\n$$\nK \\in \\mathbb { T } ^ { \\gamma \\times 1 } \\xrightarrow { \\mathbf { T } } \\mathbb { R } ^ { \\gamma \\times \\tau } \\xrightarrow { \\mathbf { E } } \\mathbb { R } ^ { \\gamma \\times \\tau \\times h } \\xrightarrow { \\mathrm { r e s h a p e } } \\theta _ { l _ { 3 } } \\in \\mathrm { R } ^ { \\gamma \\times \\tau h } ,\n$$\n\n<html><body><table><tr><td colspan=\"2\">Method</td><td>CWRU</td><td>DIRG</td><td>HIT</td><td>IMS</td><td>JNU</td><td>JUST</td><td>MFPT</td><td>PU</td><td></td><td>XJTU</td><td>MBHM</td></tr><tr><td colspan=\"2\">Train and Test on Individual Datasets</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan=\"2\">Methods WDCNN</td><td>71.60</td><td>41.23</td><td>94.42</td><td></td><td>96.03</td><td>55.71</td><td>49.67</td><td>75.00</td><td>75.84</td><td>95.99</td><td>-</td></tr><tr><td colspan=\"2\">for specific TCNN conditions QCNN</td><td>81.56</td><td>39.31</td><td>91.03</td><td></td><td>91.48</td><td>70.37</td><td>48.83</td><td>87.50</td><td>72.76</td><td>94.36</td><td></td></tr><tr><td colspan=\"2\">Train on MBHM Dataset and Test on Individual Datasets</td><td>80.55</td><td></td><td>46.58</td><td>96.04</td><td>95.46</td><td>51.06</td><td>48.67</td><td>87.50</td><td>78.46</td><td>96.49</td><td></td></tr><tr><td rowspan=\"6\">Methods for specific conditions</td><td>WDCNN TCNN</td><td>34.22 15.98</td><td>25.01 14.98</td><td>72.56 54.05</td><td>93.60 91.87</td><td>40.20 29.91</td><td>41.38 26.43</td><td>37.50 25.00</td><td></td><td>56.82 51.69</td><td>88.98 85.00</td><td>65.79 57.20</td></tr><tr><td>QCNN</td><td>48.01</td><td>27.54</td><td>78.38</td><td>94.23</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>WDCNN+DCN</td><td>80.35</td><td>68.76</td><td></td><td></td><td>43.99</td><td>42.56</td><td></td><td>62.50</td><td>63.06</td><td>90.26</td><td>67.87</td></tr><tr><td></td><td>48.40</td><td>28.19</td><td>96.51</td><td>96.39</td><td>90.87</td><td></td><td>77.78</td><td>62.50</td><td>84.86</td><td>95.66</td><td>87.60</td></tr><tr><td>TCNN+DCN QCNN+DCN</td><td>95.55</td><td>91.32</td><td>68.60 99.41</td><td>92.41 97.36</td><td>46.98 98.96</td><td></td><td>48.19 85.91</td><td>12.50 100.00</td><td>68.49</td><td>90.96</td><td>68.98</td></tr><tr><td>MagNet</td><td>12.07</td><td>15.80</td><td>42.50</td><td>91.96</td><td>29.09</td><td></td><td>27.31</td><td>25.00</td><td>92.32 30.68</td><td>98.27 85.23</td><td>87.60</td></tr><tr><td rowspan=\"5\">Methods for cross conditions</td><td>BearingFM</td><td>41.18</td><td>14.29</td><td>33.33</td><td>92.18</td><td>48.39</td><td>58.78</td><td></td><td>33.33</td><td>54.22</td><td>88.45</td><td>55.54 75.18</td></tr><tr><td>MagNet+DCN</td><td>95.31</td><td>79.94</td><td>89.78</td><td>88.79</td><td></td><td>94.95</td><td>65.85</td><td>87.50</td><td>74.43</td><td>95.85</td><td>81.18</td></tr><tr><td>BearingFM+FCN</td><td>81.16</td><td>20.69</td><td>80.00</td><td>97.74</td><td></td><td>81.18</td><td>83.26</td><td>25.00</td><td>95.31</td><td>95.17</td><td>90.07</td></tr><tr><td>Ours</td><td>100.00</td><td>99.72</td><td>99.90</td><td>99.39</td><td></td><td>99.44</td><td>98.16</td><td>100.00</td><td>99.41</td><td>98.79</td><td>99.02</td></tr><tr><td></td><td>(+108%)</td><td>(+262%)</td><td>(+28%)</td><td>(+6%)</td><td></td><td>(+106%)</td><td>(+67%)</td><td>(+60%)</td><td>(+58%)</td><td>(+10%)</td><td>(+32%)</td></tr></table></body></html>\n\nTable 2: Accuracy comparison with existing methods. $ { \\mathrm {  ~ \\cdot ~ } } _ { \\mathrm { + D C N } } , ,$ denotes the addition of DCN to the original method, while $\\scriptstyle \\cdot _ { + \\mathrm { F C N } } ,$ indicates the replacement of the network of the original method with FCN, ‚Äú $\\left( + 1 0 8 \\% \\right) ^ { , }$ represents a relative improvement from $4 8 . 0 1 \\%$ to $100 \\%$ . Our approach not only surpasses the SOTA accuracy on the MBHM dataset but also achieves results superior to those obtained from models trained specifically for individual datasets. The DCN and FCN components demonstrate broad applicability across diverse scenarios.\n\nwhere $\\mathbb { T }$ stands for the text domain. $\\mathbf { E }$ and $\\mathbf { T }$ indicate the embedding layer and tokenizer of the pre-trained LLM, respectively. Using a tokenizer $\\mathbf { T }$ and an embedding layer $\\mathbf { E }$ , we generate a word embedding from $K$ , which is then reshaped into the weight matrix $\\theta _ { l _ { 3 } }$ . See Appendix C.3 for more details on initializing weights.\n\nWe use the pre-trained Qwen2-1.5B (Bai et al. 2023) as our LLM parameterized by $\\theta _ { L }$ , achieving basic humancomputer interaction. However, its knowledge of specific domains and generation quality still requires improvement. We used the existing LoRA technique (Hu et al. 2022) and a general pipeline PEFT (Mangrulkar et al. 2022) for simultaneous fine-tuning of the LLM and our proposed alignment layer, which is detailed in Algo. 1.\n\n# 5 Experiments\n\n# 5.1 Experimental Setup\n\nWe implemented the proposed method using PyTorch (Paszke et al. 2019). Both pre-training and fine-tuning are performed on a single Nvidia RTX 4090 GPU. For pretraining, comparison trials, and ablation experiments, we used AdamW (Loshchilov and Hutter 2017) as the optimizer, and the batch size was set to 1024 for up to 50 epochs of training. Fine-tuning was performed using the existing PEFT (Mangrulkar et al. 2022) library.\n\nTo evaluate the effectiveness of our method, we provide quantitative comparison results for fault diagnosis, ablation of key components, and a user study to assess the quality of language responses. We addressed potential label leakage by dividing the 9 public datasets into a 7:2:1 ratio individualy. The training set for the MBHM dataset consists of the concatenated training sets of these individual datasets, ensuring no overlap with their corresponding test sets. Other tasks including anomaly detection, maintenance recommendations, and potential risk analysis can be found in Appendix D.\n\n# 5.2 Comparison With Fault Diagnosis Methods\n\nWe compared BearLLM with the following fault diagnosis methods. BearFM (Lai et al. 2024) and MagNet (Shi et al. 2023) are intended for diagnosing faults under crossworking conditions, while WDCNN (Zhang et al. 2017), TCNN (Chen, Gryllias, and Li 2020), and QCNN (Liao et al. 2023) are aimed at handling specific working conditions. Detailed descriptions of these methods can be found in Appendix B. To ensure a fair comparison, we re-implement these methods and test them under the same setup in section 5.1. The results are displayed in Tab. 2.\n\n![](images/a97e80900f5934d803561f56b36be2c8d6f13ae67d6632a113e037a010193c5e.jpg)  \nFigure 5: Accuracy and learning rate trends during training for different models. (a) Replacing the network of BearingFM with FCN resulted in increased accuracy and accelerated convergence. (b) Incorporating DCN into QCNN significantly mitigated overfit. (c) Our proposed method exhibits the fastest convergence and highest accuracy.\n\nOur DCN achieves greater accuracy compared to BearingFM (Lai et al. 2024) when used with the same FCN (see\n\nFig. 5 (a)). The reason for this enhancement is likely due to BearingFM using absolute values after the FFT of the envelope spectrum. This method captures only the amplitude and ignores crucial phase information. In contrast, DCN leverages real-number computations, which help to reduce potential information loss, and operates in less than $20 \\%$ of the time required by the comparison method. Combining DCN with MagNet (Shi et al. 2023) and utilizing aligned data for fusion augmentation has noticeably improved performance on datasets with substantial distribution differences.\n\nReflected in Tab. 2, the three methods (WDCNN, TCNN, QCNN) lacking data augmentation or alignment indicate strong accuracy on some specific datasets. However, their capacity to manage massive distribution differences is restricted when trained on the MBHM dataset. Including DCN eases the marked overfitting in QCNN (Liao et al. 2023), leading to a substantial improvement in validation accuracy (see Fig. 5 (b)). Similarly, adding DCN to both WDCNN (Zhang et al. 2017) and TCNN (Chen, Gryllias, and Li 2020) led to higher accuracy. Among all the methods tested, our proposed method achieves the highest accuracy and converges the fastest (within 20 epochs on the MBHM dataset as shown in Fig. 5 (c)).\n\n5.3 Ablation Experiments And Generalization   \n\n<html><body><table><tr><td>nf</td><td>Param.</td><td>FLOP</td><td>Accuracy</td></tr><tr><td>6,000</td><td>0.4013M</td><td>0.0177G</td><td>97.70%</td></tr><tr><td>12,000</td><td>0.5979M</td><td>0.0353G</td><td>98.32%</td></tr><tr><td>24,000</td><td>0.9747M</td><td>0.0704G</td><td>99.02%</td></tr><tr><td>48,000</td><td>1.7448M</td><td>0.1408G</td><td>99.20%</td></tr></table></body></html>\n\nTable 3: Comparison of the number of parameters and FLOP of FCN under different $n _ { f }$ settings as well as the accuracy on the MBHM dataset.\n\nThe tests were carried out using four different $n _ { f }$ settings in DCN (see Eq. 2), as depicted in Tab. 3. As vibration information primarily resides in the low-frequency range, cut is unlikely to significantly impact accuracy. By increasing the number of frequency components, the distortion due to cut can be minimized, which enhances the precision on the MBHM dataset; however, this also raises parameters and computation in FCN. To achieve a balance between accuracy and performance, we opt for 24,000 as the $n _ { f }$ .\n\nTable 4: A comparison of accuracy and generalization for different ablation setups is presented.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td>Accuracy</td><td>Generalization</td><td></td></tr><tr><td>MBHM</td><td>JUST</td><td>IMS</td></tr><tr><td>Ours</td><td>99.02%</td><td>90.22%</td><td>98.52%</td></tr><tr><td>w/o DCN</td><td>72.15%</td><td>37.00%</td><td>91.46%</td></tr><tr><td>wlo Fu,Fres</td><td>98.35%</td><td>87.52%</td><td>97.81%</td></tr><tr><td>w/o Fres</td><td>98.82%</td><td>87.83%</td><td>97.96%</td></tr><tr><td>w/oFu</td><td>98.63%</td><td>90.34%</td><td>98.29%</td></tr></table></body></html>\n\nAblation studies were conducted to further validate the effectiveness of each component in our proposed method.\n\nWe evaluated the performance by directly using raw timedomain vibration signals (fixed-length segments) as input and removing fault-free channels and residual channels separately and together.\n\n![](images/84cb9a0562b3c81614730ee9f6349adfd4d0b05b5707d3ecbd21a54fb287f511.jpg)  \nFigure 6: Visualization of output features with t-SNE. (a) Our method demonstrates clear inter-class separability. (b) Removing the fault-free and residual channels results in the signals from the same dataset exhibiting similar features.\n\nExperimental results in Tab. 4 demonstrate significant accuracy and generalization drops when using time-domain signals only, further highlighting the efficacy of DCN. Applying the t-SNE, we compared the visualization of output with and without fault-free and residual channels. The blue box in Fig. 6 (b) shows how signal segments from the same dataset cluster closely in the feature space. This indicates that the model first identifies the dataset type before refining fault classification. Conversely, our proposed method, as shown in Fig. 6 (a), reduces inter-dataset differences. The model targets the residual between the query signal segments and the fault-free signal segments, creating a unified feature representation across the varying working conditions, and improving the generalization.\n\nWe evaluate the generalization ability of our proposed method using zero-shot settings. Among the publicly available datasets employed, JUST (Ren 2023) and IMS (Qiu et al. 2006) are the largest. We trained on the MBHM(w/o JUST&IMS) dataset, comprising only $3 5 \\%$ of the MBHM training data, and performed zero-shot tests on the JUST and IMS datasets separately. On the JUST dataset, our method achieves an accuracy of $9 0 . 2 2 \\%$ without any fine-tuning. In contrast, the method without fault-free and residual channels achieves an accuracy of only $8 7 . 5 4 \\%$ .\n\nFig. 7 (a,b) illustrates a comparison of confusion matrices for zero-shot testing on the IMS dataset (Qiu et al. 2006), with and without fault-free and residual channels. Given that the IMS dataset is unbalanced (most samples are fault-free), the overall accuracy drops slightly from $9 8 . 5 2 \\%$ to $9 7 . 8 1 \\%$ .\n\n$\\begin{array} { r l } { \\mathfrak { O } } & { - \\frac { 1 0 1 0 } { 1 7 } ( \\begin{array} { l l l l l l l l l } { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } \\\\ { - \\frac { 1 } { 2 1 7 8 8 } } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } \\\\ { \\le \\mathtt { e } \\mathtt { e } \\mathtt { e } \\mathtt { e } } & { - 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } \\\\ { \\le \\mathtt { e } \\mathtt { e } \\mathtt { e } } & { - 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } \\\\ { \\le \\mathtt { e } \\mathtt { e } \\mathtt { e } } & { - 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } \\\\ { \\le \\mathtt { e } \\mathtt { e } \\mathtt { e } } & { - 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } \\\\ { \\le \\mathtt { e } \\mathtt { e } \\mathtt { e } } & { - 3 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } \\\\ { \\le \\mathtt { e } \\mathtt { e } \\mathtt { e } } & { - 3 } & { 0 } & { 0 } & { 0 } & { 0 } & { 1 1 8 9 } & { 0 } & { 0 } \\\\ { \\le \\mathtt { e } \\mathtt { e } \\mathtt { e } } & { - 1 7 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 1 6 7 } \\end{array} ) } \\\\ { \\le } & { - \\frac { 4 } { 3 } ( \\begin{array} { l l l l l l l l l } { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 3 } & { \\mathtt { B } \\le 0 } \\\\ { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 3 4 \\underline { { \\xi } } \\mathtt { e } \\mathtt { e } } \\\\ { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } \\end{array} ) } \\\\ { \\le } &  - \\frac { 1 } { 1 1 } ( \\begin{array} { l l l l l l l } { 1 } & { 1 } & { 1 } & { 1 } & { 1 } & { 1 } & { 1 } & { 1 } & { 1 } \\\\ { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } & { 0 } \\\\ { 1 } & { 1 } & { 0 } & { 0 } &  \\end{array} \\end{array}$ 0 $\\begin{array} { r l } & { - \\frac { \\| \\mathbf { J } \\| } { 2 0 } 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 } \\\\ & { - \\frac { \\| \\mathbf { J } \\| } { 2 0 2 0 8 0 0 } 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 } \\\\ & { - 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 } \\\\ & { - 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 } \\\\ & { - \\frac { \\| \\mathbf { J } \\| } { 0 } 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 } \\\\ & { - \\frac { \\| \\mathbf { J } \\| } { 0 } 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 } \\\\ & { - \\frac { \\| \\mathbf { J } \\| } { 0 } 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 } \\\\ & { - \\frac { \\| \\mathbf { J } \\| } { 0 } 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 } \\\\ & { - \\frac { \\| \\mathbf { J } \\| } { 0 } 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 } \\\\ & { - \\frac { \\| \\mathbf { J } \\| } { 0 } 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 } \\\\ & { - \\frac { \\| \\mathbf { J } \\| } { 1 } 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\frac { \\| \\mathbf { J } \\| } { 3 } \\mathrm { b i j k l } \\| } \\\\ & { - \\frac { \\| \\mathbf { J } \\| } { 0 } 1 1 1 1 1 1 1 1 1 1 4 1 5 6 1 6 1 1 6 1 7 1 8 1 } \\\\ & { 0 1 1 2 3 ~ \\mathrm { P r e c l i c t i s t e d s t e d s } } \\\\ & { ~ ( \\mathbf { b } ) ~ \\mathrm { T o l } ~ \\mathrm { P r e d i c l e d s t r e d s } } \\end{array}$ 0 -1000 00 0 0 0 00 0 0 1000 00 0 00 000 1 1 090 5 5 0 0 0 0 0 0 0 5 0 5 0- Fault-free 1 1 1 0 060 5 0 0 020015 0 0 10150 0 1- Minor Inner Ring 0 0 0 96.0 4 0 0 0 4 4 4 4 2- Moderate Inner Ring 0 8 5 0 00 3 5 8 80005 5 500 3- Severe Inner Ring G 88 87 8 8788 567- SMeiovndeorereraOtBeuatlBelrallRing 1 0 1 2 3 4 5.6 789 0 1 2 3 4 5.6 789 8- Moderate Outer Ring (c) (d) Ôºâ Total acc: 89.14% Predicted 9- Severe Outer Ring\n\nHowever, the method without two auxiliary channels tends to grossly underestimate the severity. For example, $61 \\%$ of severe outer ring faults are classified as moderate, and $23 \\%$ of moderate outer ring faults are identified as minor.\n\nThe CWRU (Case Western Reserve University 2008) and XJTU (Wang et al. 2020a) datasets are the only ones that include all ten types of faults. To confirm the potential to create a unified representation, we trained our model on the MBHM(w/o CWRU) and MBHM(w/o CWRU&XJTU) datasets, respectively. We then performed zero-shot testing on the commonly used CWRU dataset, with the results of the confusion matrices displayed in Fig. 7 (c,d). Our method achieves remarkable accuracies of $9 0 . 2 6 \\%$ and $8 9 . 1 4 \\%$ on the untrained CWRU dataset for each setting, respectively. This result is even better than some methods trained on CWRU, which shows the generalization of our unified representation method and does not depend on any specific complete dataset for training.\n\n# 5.4 User Study\n\n<html><body><table><tr><td>Task</td><td>A</td><td>B</td><td>C</td><td>D</td></tr><tr><td>FCN</td><td>9%</td><td>10%</td><td>1</td><td>1</td></tr><tr><td>w/o fine-tune</td><td>44%</td><td>32%</td><td>46%</td><td>45%</td></tr><tr><td>BearLLM</td><td>47 %</td><td>58%</td><td>54%</td><td>55%</td></tr></table></body></html>\n\nTable 5: Voting results from user study. Tasks A-D corresponds to anomaly detection, fault diagnosis, maintenance recommendations, and potential risk analysis. The finetuned BearLLM was the most favored across all tasks.\n\nTab. 5 summarizes the outcomes of four different tasks, with users choosing the best outputs from FCN, untuned BearLLM, and fine-tuned BearLLM in blind trials. Notably, in simpler tasks, few users chose the fault code output, while most preferred the natural language output. Fig. 8 illustrates examples of outputs before and after fine-tuning. Appendix D provides further comparisons for various tasks. Fine-tuning did not significantly affect the output of the simple anomaly detection task. In the fault diagnosis task, the model without fine-tuning sometimes missed information on fault severity, an issue that was resolved with fine-tuning. For the two more complex tasks, the fine-tuned model produced more accurate and detailed responses. Our method addresses the challenge faced by non-experts in utilizing maintenance systems due to their complexity.\n\nLPNNMPPTT Based on the bearing condition description, report the current state of the bearing. If the bearing is in a faulty state, provide targeted maintenance recommendations based on the fault location and severity. User The bearing has a minor fault of its inner ring. No immediate action is required for this type of fault. However, it's imp-ortant to monitor the condition closely over time. Maintenance recommendations: - Regular inspection and monitoring of the bearing. - Consider replacing the bearing if it shows signs of wear or damage over time. If the fault persists, additional checks might be needed, such as a more thorough inspection or a BearLLM replacement of the entire assembly (if necessary). The bearing is in a minor fault state of the inner ring. Targeted maintenance recommendation for this situation would be to replace the inner ring of the bearing immediately to prevent further damage and ensure proper BearLLM functioning. w/o fine-tune\n\n# 6 Conclusion\n\nWe propose BearLLM, a novel multimodal bearing health management framework that is the first attempt to unify multiple bearing-related tasks using LLMs, including anomaly detection, fault diagnosis, maintenance recommendations, and potential risk analysis. To build this unified framework, we introduce a prior knowledge-enhanced vibration signal representation for hundreds of different working conditions and construct the first large-scale multimodal bearing health management (MBHM) dataset. Experimental results on nine public fault diagnosis datasets show that BearLLM outperforms state-of-the-art methods, even surpassing those specifically trained on individual datasets. In addition, our frequency domain input alignment and feature extraction modules are plug-and-play, significantly improving the performance of other fault diagnosis models. We hope our work can inspire future research on building more capable industrial multimodal models.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ÂΩìÂâçËΩ¥ÊâøÂÅ•Â∫∑ÁÆ°ÁêÜÊ°ÜÊû∂ÈúÄË¶Å‰∏∫‰∏çÂêåÂ∑•ÂÜµÂíå‰ªªÂä°ËÆæËÆ°‰∏ìÈó®ÊñπÊ≥ïÔºåÂ≠òÂú®‰ΩéÂáÜÁ°ÆÁéáÂíåÊ≥õÂåñËÉΩÂäõÂ∑ÆÁöÑÈóÆÈ¢ò„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÂú®Â∑•‰∏öÂÆâÂÖ®Áîü‰∫ß‰∏≠ÂÖ∑ÊúâÂÖ≥ÈîÆ‰ª∑ÂÄºÔºåËÉΩÂ§üÂáèÂ∞ëÁªèÊµéÊçüÂ§±ÂíåÁª¥Êä§ÊàêÊú¨„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ÊèêÂá∫BearLLMÔºå‰∏ÄÁßçÂü∫‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ§öÊ®°ÊÄÅËΩ¥ÊâøÂÅ•Â∫∑ÁÆ°ÁêÜÊ°ÜÊû∂ÔºåÈÄöËøáÁªü‰∏ÄÊåØÂä®‰ø°Âè∑Ë°®Á§∫ÂíåÊñáÊú¨ÊèêÁ§∫ÂØπÈΩêÔºåÂÆûÁé∞Â§ö‰ªªÂä°Áªü‰∏ÄÂ§ÑÁêÜ„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ1Ôºö** ÊèêÂá∫ÂÖàÈ™åÁü•ËØÜÂ¢ûÂº∫ÁöÑÁªü‰∏ÄÊåØÂä®‰ø°Âè∑Ë°®Á§∫ÔºåÂ§ÑÁêÜÂ§öÊï∞ÊçÆÈõÜ‰∏çÂêåÂ∑•ÂÜµ„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ2Ôºö** ÊûÑÂª∫È¶ñ‰∏™Â§ßËßÑÊ®°Â§öÊ®°ÊÄÅËΩ¥ÊâøÂÅ•Â∫∑ÁÆ°ÁêÜÊï∞ÊçÆÈõÜMBHMÔºåÂåÖÂê´ÈÖçÂØπÁöÑÊåØÂä®‰ø°Âè∑ÂíåÊñáÊú¨ÊèèËø∞„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ3Ôºö** BearLLMÂú®9‰∏™ÂÖ¨ÂºÄÊïÖÈöúËØäÊñ≠Âü∫ÂáÜ‰∏äËææÂà∞SOTAÊÄßËÉΩÔºåÊúÄÈ´òÊèêÂçá262%ÔºàDIRGÊï∞ÊçÆÈõÜÔºâ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   ÈÄöËøáÈ¢ëÁéáÂüüËæìÂÖ•ÂØπÈΩêÂíåÊïÖÈöúÊó†ÂÖ≥ÂèÇËÄÉ‰ø°Âè∑Ê∂àÈô§‰∏çÂêåÂ∑•ÂÜµ‰∏ãÁöÑÂàÜÂ∏ÉÂ∑ÆÂºÇÔºåÂÆûÁé∞Áªü‰∏ÄÁâπÂæÅË°®Á§∫„ÄÇ\\n> *   ËÆæËÆ°Âì≤Â≠¶ÔºöÂà©Áî®ÂÖàÈ™åÁü•ËØÜÔºà‰º†ÊÑüÂô®ÈááÊ†∑Áéá„ÄÅÊïÖÈöúÊó†ÂÖ≥‰ø°Âè∑ÔºâÂ¢ûÂº∫Ê®°ÂûãÊ≥õÂåñËÉΩÂäõ„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **ÂÖàÂâçÂ∑•‰ΩúÂ±ÄÈôêÔºö** Âõ∫ÂÆöÈïøÂ∫¶Êó∂Âüü‰ø°Âè∑ÊÆµÂØºËá¥ÊïÖÈöúÈ¢ëÁéáÊàêÂàÜÂÅèÁ¶ªÂéüÂßãÂÄºÔºõÁ∫ØÊï∞ÊçÆÈ©±Âä®ÊñπÊ≥ïÈöæ‰ª•Âπ≥Ë°°È´òÁ≤æÂ∫¶ÂíåÂº∫Ê≥õÂåñ„ÄÇ\\n> *   **Êú¨ÊñáÊîπËøõÔºö** Ëá™ÈÄÇÂ∫îÈááÊ†∑ÂèòÈïø‰ΩÜÂõ∫ÂÆöÊåÅÁª≠Êó∂Èó¥ÊÆµÔºõÁ¶ªÊï£‰ΩôÂº¶ÂΩí‰∏ÄÂåñ(DCN)Áªü‰∏ÄÈ¢ëÁéáÊàêÂàÜÊï∞ÈáèÔºõÂºïÂÖ•ÊïÖÈöúÊó†ÂÖ≥ÂèÇËÄÉ‰ø°Âè∑ËÆ°ÁÆóÊÆãÂ∑Æ„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.  Ëá™ÈÄÇÂ∫îÈááÊ†∑ÔºöÊ†πÊçÆ‰º†ÊÑüÂô®ÈááÊ†∑ÁéáÊèêÂèñÂèòÈïø‰ΩÜÂõ∫ÂÆöÊåÅÁª≠Êó∂Èó¥‰ø°Âè∑ÊÆµ„ÄÇ\\n> 2.  È¢ëÁéáÂüüÂØπÈΩêÔºöÈÄöËøáDCTËΩ¨Êç¢Âà∞È¢ëÂüüÔºå‰ΩøÁî®pad/cutÁªü‰∏ÄÈ¢ëÁéáÊàêÂàÜÊï∞Èáè$n_f=24000$„ÄÇ\\n> 3.  ÂèÇËÄÉ‰ø°Âè∑Â§ÑÁêÜÔºöÈÄâÊã©Áõ∏ÂêåÂ∑•ÂÜµ‰∏ãÊïÖÈöúÊó†ÂÖ≥‰ø°Âè∑‰Ωú‰∏∫ÂèÇËÄÉÔºåËÆ°ÁÆóÊÆãÂ∑Æ$F_{res}=F_v-\\\\tilde{F_v}$„ÄÇ\\n> 4.  ÁâπÂæÅÊèêÂèñÔºö‰ΩøÁî®ÊïÖÈöúÂàÜÁ±ªÁΩëÁªú(FCN)ÊèêÂèñÁâπÂæÅÔºåÈÄöËøáÂØπÈΩêÂ±ÇËΩ¨Êç¢‰∏∫ËØçÂµåÂÖ•„ÄÇ\\n> 5.  Â§öÊ®°ÊÄÅËæìÂÖ•ÔºöÂ∞ÜÊåØÂä®ÁâπÂæÅ‰∏éÁî®Êà∑ÊñáÊú¨ÊèêÁ§∫ÂµåÂÖ•ËøûÊé•ÔºåËæìÂÖ•LLM(Qwen2-1.5B)„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   WDCNN„ÄÅTCNN„ÄÅQCNNÔºàÈíàÂØπÁâπÂÆöÂ∑•ÂÜµËÆæËÆ°Ôºâ\\n> *   BearingFM„ÄÅMagNetÔºàÈíàÂØπË∑®Â∑•ÂÜµËÆæËÆ°Ôºâ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®ÊïÖÈöúËØäÊñ≠ÂáÜÁ°ÆÁéá‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®MBHMÊï∞ÊçÆÈõÜ‰∏äËææÂà∞‰∫Ü99.02%ÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãBearingFM(55.54%)ÂíåMagNet(87.60%)„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü32‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®Ë∑®Êï∞ÊçÆÈõÜÊ≥õÂåñËÉΩÂäõ‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®Êú™ËÆ≠ÁªÉÁöÑJUSTÊï∞ÊçÆÈõÜ‰∏äÈõ∂Ê†∑Êú¨ÊµãËØïÂáÜÁ°ÆÁéáËææ90.22%ÔºåËøúÈ´ò‰∫éÊú™‰ΩøÁî®DCNÁöÑÁâàÊú¨(37.00%)„ÄÇ\\n> *   **Âú®Êî∂ÊïõÈÄüÂ∫¶‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®MBHMÊï∞ÊçÆÈõÜ‰∏ä20‰∏™epochÂÜÖÊî∂ÊïõÔºåÊØîBearingFM+FCNÊõ¥Âø´‰∏îÂáÜÁ°ÆÁéáÊõ¥È´ò(90.07% vs 99.02%)„ÄÇ\\n> *   **Âú®DIRGÊï∞ÊçÆÈõÜ‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïËææÂà∞‰∫Ü99.72%ÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãWDCNN (41.23%) Âíå TCNN (39.31%)„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü60.41‰∏™ÁôæÂàÜÁÇπ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   ËΩ¥ÊâøÂÅ•Â∫∑ÁÆ°ÁêÜ (Bearing Health Management, BHM)\\n*   Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã (Multimodal Large Language Model, MLLM)\\n*   Áªü‰∏ÄÊåØÂä®‰ø°Âè∑Ë°®Á§∫ (Unified Vibration Signal Representation, UVSR)\\n*   Á¶ªÊï£‰ΩôÂº¶ÂΩí‰∏ÄÂåñ (Discrete Cosine Normalization, DCN)\\n*   ÊïÖÈöúÂàÜÁ±ªÁΩëÁªú (Fault Classification Network, FCN)\\n*   Èõ∂Ê†∑Êú¨Â≠¶‰π† (Zero-shot Learning, N/A)\\n*   Â∑•‰∏öÂ§öÊ®°ÊÄÅÊ®°Âûã (Industrial Multimodal Model, N/A)\\n*   ÁâπÂæÅÂØπÈΩê (Feature Alignment, N/A)\"\n}\n```"
}