{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.15499",
    "link": "https://arxiv.org/abs/2412.15499",
    "pdf_link": "https://arxiv.org/pdf/2412.15499.pdf",
    "title": "A Robust Prototype-Based Network with Interpretable RBF Classifier Foundations",
    "authors": [
        "S. Saralajew",
        "Ashish Rana",
        "Thomas Villmann",
        "Ammar Shaker"
    ],
    "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
    ],
    "publication_date": "2024-12-20",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "institutions": [
        "NEC Laboratories Europe",
        "University of Applied Sciences Mittweida"
    ],
    "paper_content": "# A Robust Prototype-Based Network with Interpretable RBF Classifier Foundations\n\nSascha Saralajew,1 Ashish Rana,1 Thomas Villmann,2 and Ammar Shaker1\n\n1NEC Laboratories Europe, Germany 2University of Applied Sciences Mittweida, Germany sascha.saralajew, ashish.rana, ammar.shaker $@$ neclab.eu, villmann $@$ hs-mittweida.de\n\n# Abstract\n\nPrototype-based classification learning methods are known to be inherently interpretable. However, this paradigm suffers from major limitations compared to deep models, such as lower performance. This led to the development of the socalled deep Prototype-Based Networks (PBNs), also known as prototypical parts models. In this work, we analyze these models with respect to different properties, including interpretability. In particular, we focus on the Classification-byComponents (CBC) approach, which uses a probabilistic model to ensure interpretability and can be used as a shallow or deep architecture. We show that this model has several shortcomings, like creating contradicting explanations. Based on these findings, we propose an extension of CBC that solves these issues. Moreover, we prove that this extension has robustness guarantees and derive a loss that optimizes robustness. Additionally, our analysis shows that most (deep) PBNs are related to (deep) RBF classifiers, which implies that our robustness guarantees generalize to shallow RBF classifiers. The empirical evaluation demonstrates that our deep PBN yields state-of-the-art classification accuracy on different benchmarks while resolving the interpretability shortcomings of other approaches. Further, our shallow PBN variant outperforms other shallow PBNs while being inherently interpretable and exhibiting provable robustness guarantees.\n\n# 1 Motivation and Context\n\nTwo principal streams exist in the field of explainable machine learning: (1) post-processing methods (post-hoc approaches) that try to explain the prediction process of an existing model, such as LIME and SHAP (see Marcinkevicˇs and Vogt 2023, for an overview), and (2) the design of machine learning methods with inherently interpretable prediction processes (Rudin 2019). While the former could create non-faithful explanations due to only approximating the output distribution of a black box model without explaining its internal logic, it is claimed that inherently interpretable methods always generate faithful explanations (Rudin 2019). According to Molnar (2022), a model is called interpretable if its behavior and predictions are understandable to humans. Moreover, when the provided explanations lead to a correct interpretation of the model, this interpretation enriches the user (or developer) with an understanding of how the model works, how it can be fixed or improved, and whether it can be trusted (Ribeiro, Singh, and Guestrin 2016).\n\nA well-known category of interpretable models for classification tasks is (shallow) Prototype-Based Networks (PBN) such as LVQ (e. g., Biehl, Hammer, and Villmann 2016). These models are interpretable because (1) the learned classspecific prototypes1 are either from the input space or can be easily mapped to it; belonging to the input space helps summarize the differentiating factors of the input data and provides trusted exemplars for each class, (2) the dissimilarity computations are given by human comprehensible equations such that differences between inputs and learned prototypes can be understood, (3) the classification rule based on the dissimilarities is intelligible (e. g., winner-takes-all principle); see Bancos et al. (2020) for an interpretability application. Despite being interpretable, these models also face limitations: (1) The number of parameters becomes large on complex data since the prototypes are class-specific and are defined in the input space.2 (2) The classification performance is behind that of deep neural architectures as the dissimilarity functions and the classification rules are straightforward to ensure interpretability (Villmann, Bohnsack, and Kaden 2017).\n\nTo fix these limitations, researchers investigated the integration of prototype-based classification heads with deep neural feature extractors to build deep interpretable PBNs and designed numerous architectures such as ProtoPNet (Chen et al. 2019), ProtoPool (Rymarczyk et al. 2022), CBC (Classification-By-Components; Saralajew et al. 2019), and PIPNet (Nauta et al. 2023). The generated results of these models are impressive as they achieve state-of-the-art classification accuracy on fine-grained image classification, and some show a good performance in rejecting Out-OfDistribution (OOD) examples (e. g., PIPNet). The high-level structure of these models follows the same principles (see Fig. 1): (1) embedding of the input data in a latent space by a Neural Network (NN), denoted as feature extractor backbone; (2) measuring the dissimilarity (or similarity) between the embedding and the latent prototypes; (3) prediction computation after aggregating the dissimilarities by a shallow model (realizes the classification rule), denoted as classification head. In this paradigm, the differences between the proposed architectures are often subtle, such as imposing sparsity, the usage of negative reasoning, and whether they can be used as a shallow model. Moreover, all architectures are supposed to generate interpretable models. But is this genuinely accurate?\n\n![](images/24183c2848085f9b2046d87b7fac28609b6e7cf389b11e926463ff37a24e7331.jpg)  \nFigure 1: General architecture of deep PBNs.\n\nIn this paper, we investigate PBNs and make the following contributions:\n\n1. We show that deep PBNs are related to deep RBF classifiers. Building on this finding, we explain why these models are effective for OOD detection.   \n2. We discuss why current deep PBNs are not interpretable and demonstrate how the interpretability level of the models varies between the different architectures.   \n3. Building on CBCs and their relation to RBF networks, we design a prototype-based classification head that can use negative reasoning in a sound probabilistic way and fixes the interpretability issue of other heads.   \n4. We derive robustness bounds for our classification head (shallow PBN), including a loss that provably optimizes robustness. Further, the relation shown gives the first loss that optimizes the robustness of RBF classifiers.\n\nThe paper’s outline is as follows: In Sec. 2, we review deep PBNs and discuss their relation to RBF networks (Broomhead and Lowe 1988) and several properties. Based on the identified shortcomings, in Sec. 3, we propose an extension of CBC so that the interpretability is sound and negative reasoning is used. Additionally, we show that the shallow version of this architecture has provable robustness guarantees. Sec. 4 presents the experimental evaluation of our claims. Finally, a discussion and conclusion are presented. Please see Saralajew et al. (2024) for the corresponding appendix.\n\n# 2 Review of Deep Prototype-based Networks\n\nIn the following section, we review the differences between deep PBNs and show their relation to RBF networks. Thereafter, we discuss the interpretability of these methods using the established relation. Later, we explain why PBNs are suitable for OOD detection and analyze the role of negative reasoning.\n\nDifferences between the architectures and their relation to RBF networks. Fig. 1 shows the general architecture of most deep PBNs. We use the shown building blocks to characterize existing approaches in Tab. 1 along the following dimensions:\n\n• Backbone: Single, multiple, or Siamese feature extractor, and whether the method has been tested without a feature extractor (shallow model). • Latent prototypes: Whether the prototypes are defined in the input or the latent space and if they are back-projected to training samples (Chen et al. 2019). This dimension also indicates if prototypes are class-specific. • Similarity: The used similarity function. RBF refers to the standard squared exponential kernel. If a different nonlinear function is used to construct the RBF, it is specified in parenthesis. Note that all RBFs use the Euclidean norm. • Linear layer constraints: The constraints on the final linear prediction layer or the stated approach to compute the output if no linear output layer is used. The $l _ { 1 }$ regularization is only applied to connections that connect similarity scores (slots, etc.) with incorrect classes. • Single loss term: Whether multiple loss terms are used. • Main contribution: The primary contribution of the proposed architecture compared to previous work.\n\nWe identified the following architectures by reviewing toptier venue papers: LeNet5 (LeCun et al. 1998), ProtoPNet, CBC, Hierarchical ProtoPNet (Hase et al. 2019), ProtoAttend (Arik and Pfister 2020), ProtoTree (Nauta, van Bree, and Seifert 2021), ProtoPShare (Rymarczyk et al. 2022), TesNet (Wang et al. 2021), Deformable ProtoPNet (Donnelly, Barnett, and Chen 2022), ProtoPool, and PIPNet. Moreover, we added LucidPPN (Pach et al. 2024) and ProtoViT (Ma et al. 2024) as it is the most recent publication in the field.\n\nConsidering Fig. 1, we realize that the head of a deep PBN is an RBF network if a linear layer is used for prediction. Combined with a feature extractor, we obtain deep RBF networks (e. g., Asadi et al. 2021). Notably, the first deep PBN is LeNet5, where RBF heads are used to measure the similarity between inputs and the so-called “model” (prototype) of the class. Starting with ProtoPNet, the existing architectures (see Tab. 1) build on each other (except for CBC and ProtoAttend), and almost all use an RBF network with some constraints or regularizers as classification heads. Consequently, changes between the architectures are incremental, and concepts persist for some time once introduced. Recently, researchers abandoned the idea of back-projecting prototypes and started using dot products instead of RBF functions, which implicitly defines prototypes as convolutional filter kernels (PIPNet, LucidPPN).\n\nOn the interpretability of deep PBNs. Using the definition of interpretability in Sec. 1 and the relation to RBF networks, we discuss the interpretability of deep PBNs. First, it should be noted that RBF networks and shallow PBNs learn representations in the input space (centroids and prototypes, respectively), and both use these representations to measure the (dis)-similarity to given samples. At the same time, these two paradigms differ in two aspects: (1) RBFs’ usage of non-class specific centroids and (2) PBNs’ usage of the human-comprehensible winner-takes-all rule instead of a linear predictor over the prototypes.\n\nTable 1: Characterization of existing architectures along the specified dimensions. Note that the order is chronological. Methods that are not directly based on the previously published methods are marked with an asterisk. The asterisk in the remaining columns stands for the ability to omit the feature extractor in Backbone, the usage of back-projection of latent prototypes in Latent Prototype, and an alternative approach for the output computation in Linear Layer Constraints (i. e., no application of a linear layer with a regularization or constraint). The italic typeface in Latent Prototype states that the prototypes are class-specific.   \n\n<html><body><table><tr><td></td><td>Backbone</td><td>Lateot</td><td>Similarity</td><td>Linear Liayer</td><td>Singe</td><td>Main Contribution</td></tr><tr><td>LeNet5</td><td>single</td><td>yes</td><td>RBF</td><td>none</td><td>no</td><td>CNN with RBF head</td></tr><tr><td>ProtoPNet*</td><td>single</td><td>yes*</td><td>RBF (log)</td><td>l reg.</td><td>no</td><td>(deep) N with protatype</td></tr><tr><td>CBC*</td><td>Siamese*</td><td>no</td><td>RBF or ReLU-cosine</td><td>probabilistic</td><td>yes</td><td>negativeposieinleie</td></tr><tr><td>Hier. ProtoPNet</td><td>single</td><td>yes*</td><td>RBF (log)</td><td>l reg.</td><td>no</td><td>hierarchical classification</td></tr><tr><td>ProtoAttend*</td><td>Siamese</td><td>no</td><td>relational attention</td><td>none</td><td>no</td><td>attention for prototype selection</td></tr><tr><td>ProtoTree</td><td>single</td><td>yes*</td><td>RBF</td><td>(soft) tree*</td><td>yes</td><td>tree upon similarities</td></tr><tr><td>ProtoPShare</td><td>single</td><td>yes*</td><td>RBF (log)</td><td>l reg.</td><td>no</td><td> prototype shring between</td></tr><tr><td>TesNet</td><td>single</td><td>yes*</td><td>dot-product</td><td>l reg.</td><td>no</td><td>orthogonal prototypes</td></tr><tr><td>Def. ProtoPNet</td><td>single</td><td>yes*</td><td>RBF (cosine)</td><td>l1 reg.</td><td>no</td><td>deformable prtonpes shift</td></tr><tr><td>ProtoPool</td><td>single</td><td>yes*</td><td>RBF (focal similarity)</td><td>l reg.</td><td>no</td><td>differentiabl orototype</td></tr><tr><td>PIPNet</td><td>single</td><td>yes</td><td>softmax dot product</td><td>non-negative</td><td>no</td><td>self-supervised pre-training</td></tr><tr><td>LucidPPN</td><td>multiple</td><td>yes</td><td>sigmoid dot product</td><td>average*</td><td>no</td><td>color and shape backbone</td></tr><tr><td>ProtoViT</td><td>single</td><td>yes*</td><td>scaled sum of cosine</td><td>l1 reg.</td><td>no</td><td>deformable protostyrsthrough</td></tr><tr><td>Ours</td><td>single*</td><td>yes</td><td>RBF or softmax dot product</td><td>probabilistic</td><td>yes</td><td>trainable priorsand provable</td></tr></table></body></html>\n\nThe first aspect overcomes the Limitation (1) mentioned in Sec. 1 without harming the interpretability. The second aspect poses a problem for interpretation, which explains the lack of studies applying RBF networks for interpretable machine learning. The problem starts with the unconstrained weights in the linear layer, which lead to unbounded and incomparable scores (e. g., it is unclear how to interpret a high score or weight). Further, this could result in situations where the closest (most similar) centroids do not contribute the most to the classification score compared to less similar centroids that are overemphasized by large weights. Hence, this breaks the paradigm that the most similar centroids (or prototypes) define the class label.\n\nWhat does this imply for deep PBNs? First, the interpretation of the classification head suffers from the same difficulties as an RBF network if no appropriate constraints are applied (e. g., the average computation of LucidPPN). Therefore, the most similar prototypes for an input do not necessarily define the class label. For example, PIPNet trained on CUB (Wah et al. 2011) uses average weights of 14.1 for class blue jay and 8.7 for green jay. This indicates that PIPNet overemphasizes small similarity values so that the interpretation of the influential prototypes could be incorrect; further results in Sec. 4. Second, since the similarity is computed in a latent space defined by a deep NN, it is unclear why two samples are close or distant due to the black-box nature of deep NNs. Thus, it is misleading to denote a deep PBN as interpretable. In the best case, it can be denoted as partially interpretable as it gives insights into the final classification step, assuming that the classification head is well-designed. Note that the interpretability of these methods is also questioned by others (e. g., Hoffmann et al. 2021; Pazzani et al. 2022; Sacha et al. 2024; Wolf et al. 2024).\n\nOn the OOD detection properties. In CBC (rejection of predictions), Hierarchical ProtoPNet (novel class detection), ProtoAttend (OOD detection), and PIPNet (OOD detection), it was shown that deep PBNs are suitable for identifying OOD samples. This ability can be attributed to the RBF architecture if the model is clearly related to RBF models. Hein, Andriushchenko, and Bitterwolf (2019) proved that RBF networks produce low-confidence predictions when $a$ given sample is far away from all centroids (prototypes) because the applied softmax squashing enforces the predictions of all classes to be uniform. Van Amersfoort et al. (2020) built on this idea and empirically showed that deep feature extractors with an RBF head and a winner-takes-all rule (so a deep PBN) can be used for uncertainty estimation and, thus, OOD detection. The published results for deep PBNs also confirm this property van Amersfoort et al. (2020) observed. Empirically, this property transfers beyond RBF-related architectures, as architectures like PIPNet show a remarkable\n\nP(I|k=1,c=1) P(R|k=1,c=1) P(D|k=1,x) k= R D P(k=1) P(D|k=1,x) c=1 P(R|k=1,c=1) not D   \nP(c=1)P(k=K) P(I|k=1,c=1) P(D|k=1,x) k=K not I notR D   \nP(c= C) P(D|k=1,x) C=C RePove Iy Pdr eplace not D\n\nOOD performance using a non-RBF similarity.\n\nThe role of negative reasoning. Positive reasoning is welldefined as retrieving evidence of a given class from present features, but the literature does not reach a consensus about negative reasoning. In CBC, it means the retrieval of evidence from absent features. In contrast, in ProtoPNet, this refers to the reduction of the final score due to a negative weight associated with an active prototype. Other methods in the literature either penalize negative reasoning (e. g., ProtoPNet) by a regularization term or avoid it by a constraint (e. g., PIPNet); see the Constraints column in Tab. 1 . The challenge posed by negative reasoning in these architectures is mainly about interpretation, as it is not an intuitive reasoning principle of humans (according to Chen et al. 2019) and complicates the explanation strategies. In a notable contrast, in CBC, inspired by cognitive science results (e. g., Hsu et al. 2017), the authors modeled negative reasoning from a probabilistic perspective, making its interpretation mathematically sound. For the remainder of the paper, we refer by negative reasoning to the retrieval of evidence from features that have to be absent.\n\n# 3 Classification-by-Components Networks\n\nWe now review the original CBC architecture and show its limitations. Based on that, we propose our CBC—simply denoted as CBC and the old version is denoted as original CBC—that overcomes these limitations and realizes a strong link to RBF networks. Then, we show how a CBC can be learned efficiently and derive robustness lower bounds.\n\nReview of the original CBC method. Components are the core concept of the original CBC, where a component is a pattern that contributes to the classification process by its presence (positive reasoning; the component must be close) or absence (negative reasoning; the component must be far) without being tied to a specific class label. A component can also abstain from the classification process, which is called indefinite reasoning (modeled via importance). The original CBC is based on a probability tree diagram to model the interaction between the detection of components in input samples and the usage of detection responses to model the output probability (called reasoning). The probability tree, Fig. 2, employs five random variables: $c$ , the class label; $k$ , the component; $I$ , the importance of a component (binary); $R$ , the requiredness for reasoning (binary); $D$ , the detection of a component (binary). The probability tree constructs the following:\n\n$P \\left( k \\right)$ , the prior of the $k$ -th component to appear; $P ( I | k , c )$ and $P ( \\vec { R } | k , c )$ are the importance and the requiredness probabilities of the $k$ -th component for the class $c$ ; $P ( D | k , \\mathbf { x } )$ , the detection probability of the $k$ -th component in the input $\\mathbf { x }$ . $P \\left( \\overline { { D } } | k , \\mathbf { x } \\right)$ is the complementary probability, that is, not detecting the $k$ -th component in $\\mathbf { x }$ . An agreement $A$ is a path in the tree (see solid lines in Fig. 2) that depicts the positive influence of the $k$ -th component on class $c$ by either being detected $( D )$ and required $( R )$ or not detected $( \\overline { { D } } )$ and not required $( \\overline { { R } } )$ . The output probability $p _ { c } \\left( \\mathbf { x } \\right) = P \\left( A | I , \\mathbf { x } , c \\right)$ for class $c$ is derived from the agreement $A$ using the following expression:\n\n$$\n\\frac { \\sum _ { k } \\left( P \\left( R , I | k , c \\right) P \\left( D | k , \\mathbf { x } \\right) + P \\left( \\overline { { R } } , I | k , c \\right) P \\left( \\overline { { D } } | k , \\mathbf { x } \\right) \\right) P \\left( k \\right) } { \\sum _ { k } \\left( P \\left( R , I | k , c \\right) + P \\left( \\overline { { R } } , I | k , c \\right) \\right) P \\left( k \\right) } .\n$$\n\nThe defined probabilities and components are learned by minimizing the margin loss (maximizing the probability gap)\n\n$$\n\\operatorname* { m i n } \\left\\{ \\operatorname* { m a x } _ { c ^ { \\prime } \\neq y } p _ { c ^ { \\prime } } \\left( \\mathbf { x } \\right) - p _ { y } \\left( \\mathbf { x } \\right) + \\gamma , 0 \\right\\} ,\n$$\n\nwith $\\gamma \\in [ 0 , 1 ]$ being the margin value, $y$ being the correct class label of $\\mathbf { x }$ , and $c ^ { \\prime }$ being any class label other than $y$ The model can be used without or with a feature extractor (see Fig. 1); that is, the distance computation occurs in the learned latent space. An original CBC without a feature extractor realizes an extension of traditional PBNs, overcoming Limitation (1) while posing new difficulties.\n\nThe architecture is difficult to train as it often converges to a bad local minimum (see Sec. 4), and the explanations can be counterintuitive. To see this, note that $\\overset { \\cdot } { P } \\left( R , I | k , c \\right) + P \\left( \\overline { { R } } , I | k , c \\right) + P \\left( \\overline { { I } } | k , c \\right) = 1$ for each $k$ . Thus, one can scale the reasoning probabilities $P ( R , I | k , c )$ and $P \\left( \\overline { { R } } , I | k , c \\right)$ in Eq. (1) by any factor $\\alpha > 0$ as long as $P \\left( R , I | k , c \\right) , P \\left( \\overline { { R } } , I | k , c \\right) \\in \\left[ 0 , 1 \\right]$ remains valid without changing the output probability $p _ { c } ( \\mathbf { x } )$ . Assuming that $p _ { c } \\left( \\mathbf { x } \\right) = \\bar { 1 }$ , this result can be obtained from nearly zero reasoning probabilities, giving confident predictions from infinitesimal reasoning evidence. This contradicts the design principle of the original CBC approach, as $p _ { c } \\left( \\mathbf { x } \\right) = 1$ should only be generated if the model is certain in its reasoning. At the same time, this result implies that the optimal output $\\begin{array} { r } { ( p _ { c } \\left( \\mathbf { x } \\right) = 1 , } \\end{array}$ is not unique with a wide range of flawed feasible solutions, thus causing the model to converge to bad local minima.\n\nOur extension of the original CBC method. In CBC, both problems mentioned above are caused by the indefinite reasoning probability $P \\left( \\overline { { I } } | k , c \\right)$ together with the component prior $P ( k )$ . These probabilities model the extent to which a component is used in the classification process; hence, they both serve the same purpose, as confirmed by fixing $P ( k )$ to be uniform in the original CBC. Removing $P \\left( \\overline { { I } } | k , c \\right)$ from the model eliminates the problematic model’s tolerance towards scaling by a factor $\\alpha$ . Still, it causes missing support for allowing components to remain irrelevant (to abstain), as explained by Saralajew et al. (2019) in Figure 1. Similarly, allowing the prior $P ( k )$ to be trainable does not generalize to cover the property of class-specific component priors.\n\nWe now present our modification to the original CBC to overcome the difficulties. We propose to remove the importance variable $I$ and substitute it with the trainable class-wise component prior $P ( k \\mid c )$ , see Fig. 2. The output probability $p _ { c } \\left( \\mathbf { x } \\right) = P \\left( A \\mid \\mathbf { x } , c \\right)$ , using the agreement, becomes\n\n$$\n\\begin{array} { l } { P \\left( A \\mid \\mathbf { x } , c \\right) = } \\\\ { \\displaystyle \\sum _ { k } \\left( P \\left( R , D \\mid x , c , k \\right) + P \\left( \\overline { { R } } , \\overline { { D } } \\mid \\mathbf { x } , c , k \\right) \\right) P \\left( k \\mid c \\right) = } \\\\ { \\displaystyle \\sum _ { k } \\left( P \\left( R \\mid c , k \\right) P \\left( D \\mid \\mathbf { x } , k \\right) + P \\left( \\overline { { R } } \\mid c , k \\right) P \\left( \\overline { { D } } \\mid \\mathbf { x } , k \\right) \\right) P \\left( k \\mid c \\right) } \\end{array}\n$$\n\nWe introduce the following notations:\n\n• The requiredness possibility vector $\\mathbf { r } _ { c } \\in [ 0 , 1 ] ^ { K }$ contains the probabilities $P \\left( R \\mid c , k \\right)$ for all $k$ . • The detection possibility vector $\\mathbf { d } \\left( \\mathbf { x } \\right) \\in \\left[ 0 , 1 \\right] ^ { K }$ contains the probabilities $P ( D \\mid \\mathbf { x } , k )$ for all $k$ . • The component prior probability vector $\\mathbf { b } _ { c } \\in [ 0 , 1 ] ^ { K }$ contains the probabilities $P ( k \\mid c )$ for all $k$ .\n\nNote that $\\begin{array} { r } { \\sum _ { k } b _ { c , k } = \\sum _ { k } P ( k \\mid c ) = 1 } \\end{array}$ , which is not necessarily true for $\\mathbf { d }$ and $\\mathbf { r } _ { c }$ . Now, Eq. (3) can be written as\n\n$$\np _ { c } \\left( \\mathbf { x } \\right) = \\left( \\mathbf { r } _ { c } \\circ \\mathbf { d } \\left( \\mathbf { x } \\right) + \\left( \\mathbf { 1 } - \\mathbf { r } _ { c } \\right) \\circ \\left( \\mathbf { 1 } - \\mathbf { d } \\left( \\mathbf { x } \\right) \\right) \\right) ^ { \\mathrm { T } } \\mathbf { b } _ { c } ,\n$$\n\nwhere $0$ is the Hadamard product. The detection probability can be any suitable function, like the following RBF:3\n\n$$\nP ( D \\mid \\mathbf { x } , k ) = \\exp \\left( - \\frac { d _ { E } \\left( \\mathbf { x } , \\mathbf { w } _ { k } \\right) } { \\sigma _ { k } } \\right) ,\n$$\n\nwhere $d _ { E }$ is the Euclidean distance, $\\sigma _ { k }$ is the (trainable) component-dependent temperature, and $\\mathbf { w } _ { k }$ is the vector representation of component $k$ . Using Eq. (4), similarly to other deep PBNs, the architecture is trained by optimizing the parameters of the components $\\mathbf { w } _ { k }$ , the prior probabilities ${ \\bf b } _ { c }$ , and the reasoning possibility vector $\\mathbf { r } _ { c }$ . For the optimization, the margin loss Eq. (2) can be used.\n\nLearning the parameters in CBC models. When adopted without a feature extractor, learning a CBC model realizes an extension of shallow PBNs using components instead of prototypes (Limitation (1) in Sec. 1) and constitutes an interpretable RBF network (fixes the interpretability issues mentioned in Sec. 2). Note that in the computation of Eq. (3), the requiredness probabilities $P \\left( R \\mid c , k \\right)$ and the component prior probabilities $P ( k \\mid c )$ occur jointly and provide the reasoning probabilities $\\dot { P ( R , k \\mid c ) } \\stackrel { \\cdot } { = } P ( \\dot { R } \\mid c , \\bar { k ) } P ( k \\mid c )$ . This simplification makes the association to RBF networks more explicit by rewriting Eq. (3) as $\\begin{array} { r } { p _ { c } \\left( \\mathbf { x } \\right) = \\sum _ { k } \\alpha _ { k } P \\left( D \\mid \\mathbf { x } , k \\right) + \\beta } \\end{array}$ , where $\\vec { \\alpha _ { k } } = P \\left( R , \\bar { k \\mid c } \\right) - P \\left( \\overline { { R } } , k \\mid c \\right)$ is the weight and $\\beta =$ $\\textstyle \\sum _ { k } P \\left( { \\overline { { R } } } , k \\mid c \\right)$ is the bias.\n\nMoreover, the network is simplified during training, and only the reasoning probabilities $P \\left( R , k \\mid c \\right)$ are learned, leading to fewer multiplications of trainable parameters (simpler gradient computation graph). In practice, the trainable parameters $P \\left( R , k \\mid c \\right)$ and $\\breve { P } \\left( \\breve { R } , k \\mid c \\right)$ take the form of the vector $\\mathbf { v } _ { c } \\in \\mathbb { R } ^ { 2 K }$ for each class, which is normalized to achieve $\\textstyle \\sum _ { i }$ softmax $\\left( \\mathbf { v } _ { c } \\right) _ { i } = 1$ . Within $\\mathbf { v } _ { c }$ , the first half of the parameters represent the positive and the second half the negative reasoning probabilities. The computation of $p _ { c } \\left( \\mathbf { x } \\right)$ becomes $\\mathbf { v } _ { c } ^ { \\mathrm { T } } \\left[ \\mathbf { d } \\left( \\mathbf { x } \\right) , \\left( 1 - \\mathbf { d } \\left( \\mathbf { x } \\right) \\right) \\right]$ , where the detection and no detection vectors are concatenated into one vector. Consequently, and again, the model realizes an RBF network that uses negative reasoning. If we block negative reasoning by setting the respective probabilities to zero, we obtain an RBF network with class-wise weights constrained while solving the interpretability issues from Sec. 2.\n\nThe proven robustness of the CBC architecture. In this section, we derive the robustness lower bound. We analyze the stability of the classification decision when $n o$ feature extractor is applied; with a feature extractor, the same stability analysis applies in the latent space. Given a data point $\\mathbf { x } \\in \\mathbb { R } ^ { n }$ with the target label $y$ , the input is correctly classified if the probability gap is positive:\n\n$$\np _ { y } \\left( \\mathbf { x } \\right) - \\operatorname* { m a x } _ { c ^ { \\prime } \\neq y } p _ { c ^ { \\prime } } \\left( \\mathbf { x } \\right) > 0 .\n$$\n\nRobustness comes from deriving a non-trivial lower bound for the maximum applicable perturbation $\\pmb { \\varepsilon } ^ { * } \\in \\mathbb { R } ^ { n }$ without having the predicted class label of $\\mathbf { x }$ changed, that is,\n\n$$\np _ { y } \\left( \\mathbf { x } + \\pmb { \\varepsilon } ^ { * } \\right) - \\operatorname* { m a x } _ { c ^ { \\prime } \\neq y } p _ { c ^ { \\prime } } \\left( \\mathbf { x } + \\pmb { \\varepsilon } ^ { * } \\right) > 0 ;\n$$\n\nthe strength of the perturbation is given by $\\| \\varepsilon ^ { * } \\|$ . Thm. 1 derives a lower bound of $\\| \\varepsilon ^ { * } \\|$ for detection probability functions of the form Eq. (5) where $d _ { E }$ is any distance function induced by the selected norm $\\left\\| \\cdot \\right\\|$ . Thm. 2 extends this derivation to squared norms (e. g., Gaussian kernel) so that the result can be applied to standard Gaussian RBF networks using the established relation.\n\nTheorem 1. The robustness of a correctly classified sample x with class label y is lower bounded by\n\n$$\n\\| \\varepsilon ^ { * } \\| \\geq \\underbrace { \\kappa \\operatorname* { m i n } _ { c ^ { \\prime } \\neq y } \\left( \\ln \\left( - \\frac { B _ { c ^ { \\prime } } + \\sqrt { B _ { c ^ { \\prime } } ^ { 2 } - 4 A _ { c ^ { \\prime } } C _ { c ^ { \\prime } } } } { 2 A _ { c ^ { \\prime } } } \\right) \\right) } _ { = : \\delta } > 0 ,\n$$\n\nwhen $A _ { c ^ { \\prime } } \\neq 0$ , where\n\n$$\n\\begin{array} { r l } & { { \\cal A } _ { c ^ { \\prime } } = \\left( \\left( { { \\bf r } _ { y } } - { \\bf 1 } \\right) \\circ { \\bf b } _ { y } - { \\bf r } _ { c ^ { \\prime } } \\circ { \\bf b } _ { c ^ { \\prime } } \\right) ^ { \\mathrm { T } } { \\bf d } \\left( { \\bf x } \\right) , } \\\\ & { { \\cal B } _ { c ^ { \\prime } } = \\left( { \\bf 1 } - { \\bf r } _ { y } \\right) ^ { \\mathrm { T } } { \\bf b } _ { y } - \\left( { \\bf 1 } - { \\bf r } _ { c ^ { \\prime } } \\right) ^ { \\mathrm { T } } { \\bf b } _ { c ^ { \\prime } } , } \\\\ & { { \\cal C } _ { c ^ { \\prime } } = \\left( { { \\bf r } _ { y } } \\circ { \\bf b } _ { y } - \\left( { { \\bf r } _ { c ^ { \\prime } } } - { \\bf 1 } \\right) \\circ { \\bf b } _ { c ^ { \\prime } } \\right) ^ { \\mathrm { T } } { \\bf d } \\left( { \\bf x } \\right) , } \\end{array}\n$$\n\nand $\\kappa = \\sigma _ { m i n } = \\operatorname* { m i n } _ { k } \\sigma _ { k }$ .\n\nAll proofs can be found in Appx. B. Additionally, it can be shown that $\\delta$ in Eq. (8) is negative if the sample is incorrectly classified. Therefore, $\\delta$ in Eq. (8) can be used as a loss function to optimize the model for stability. Of course, this loss can be clipped at a threshold $\\gamma > 0$ so that the network optimizes for robustness of at most $\\gamma$ .\n\nTheorem 2. If we use the standard RBF kernel (squared norm), then Eq. (8) becomes ∥ε∗∥ ≥ − β3 + q β9  + δ > 0 with $\\begin{array} { r } { \\kappa = \\frac { \\sigma _ { m i n } } { 3 } } \\end{array}$ and $\\beta = \\mathrm { m a x } _ { k } d \\left( \\mathbf { x } , \\mathbf { w } _ { k } \\right)$ .\n\nAgain, this result helps to construct a loss function that maximizes robustness. For standard Gaussian kernel RBF networks with class-wise weights $\\mathbf { v } _ { c }$ constrained to probability vectors, the main part $\\delta$ of the function is simplified to\n\n$$\n\\frac { \\sigma _ { m i n } } { 6 } \\operatorname* { m i n } _ { c ^ { \\prime } \\neq y } \\ln \\left( \\frac { \\mathbf { v } _ { y } ^ { \\mathrm { T } } \\mathbf { d } \\left( \\mathbf { x } \\right) } { \\mathbf { v } _ { c ^ { \\prime } } ^ { \\mathrm { T } } \\mathbf { d } \\left( \\mathbf { x } \\right) } \\right) ,\n$$\n\na log-likelihood ratio loss (e. g., Seo and Obermayer 2003).\n\nThe robustness with alternative distance functions. Similar to other shallow PBNs, CBCs can use alternative distance functions such as the Mahalanobis distance or the tangent distance (e. g., Haasdonk and Keysers 2002)\n\n$$\nd _ { T } \\left( \\mathbf { x } , S \\right) = \\operatorname* { m i n } _ { \\theta \\in \\mathbb { R } ^ { r } } d _ { E } \\left( \\mathbf { x } , \\mathbf { w } + \\mathbf { W } \\theta \\right) ,\n$$\n\nwhere $S = \\left\\{ \\mathbf { w } + \\mathbf { W } \\theta \\mid \\theta \\in \\mathbb { R } ^ { r } \\right\\}$ is a trainable $r$ -dimensional affine subspace with $\\mathbf { W }$ being a basis. By learning affine subspaces instead of points for the components, the discriminative power of the architecture is significantly improved (Saralajew, Holdijk, and Villmann 2020). Moreover, if this distance is used in a deep PBN, it realizes an extension of TesNet by learning disentangled concepts (each basis vector in $\\mathbf { W }$ is a basis concept) but measures the distance with respect to $d _ { T }$ . See Appx. A for further details about this distance. Next, Thm. 3 extends the lower bound derived in Thm. 1 for the tangent distance.\n\nTheorem 3. If we use the tangent distance in Eq. (5), Eq. (8) holds with $\\kappa \\dot { = } \\frac { 1 } { 2 } \\sigma _ { m i n }$ and $\\left\\| \\cdot \\right\\|$ being the Euclidean norm.\n\nA similar result was proven for LVQ with the tangent distance (Saralajew, Holdijk, and Villmann 2020).\n\nFinal remarks. Our proposed CBC resolves the original approach’s drawbacks. Further, the architecture can be derived from RBF networks by introducing interpretability constraints and negative reasoning. The method can be used as a head for deep PBNs or as a standalone for prototype-based classification learning. In all cases, the interpretability of the learned weights is guaranteed by the relation to the probability events. Appx. C presents further theoretical results.\n\n# 4 Experiments\n\nIn this section, we test our CBC and the presented theories: (1) We analyze the accuracy and interpretability of our CBC and compare it to PIPNet. (2) We compare shallow CBCs with other shallow models, such as the original CBC. (3) To demonstrate our theorems, we analyze the adversarial robustness of shallow PBNs. Note that all accuracy results are reported in percentage; we train each model five times, and report the mean and standard deviation. 4\n\nTable 2: Test accuracy on different benchmark datasets. If available, we copied the accuracy values from the respective papers. Otherwise, we computed them (marked by an asterisk).   \n\n<html><body><table><tr><td></td><td>CUB</td><td>CARS</td><td>PETS</td><td></td></tr><tr><td>PIPNet</td><td>84.3±0.2</td><td>88.2±0.5</td><td>92.0±0.3</td><td rowspan=\"5\"></td></tr><tr><td>ProtoPool</td><td>85.5±0.1</td><td>88.9±0.1</td><td>87.2*±0.1</td></tr><tr><td>ProtoViT</td><td>85.8±0.2</td><td>92.4±0.1</td><td>93.3*±0.2</td></tr><tr><td>CBC</td><td>87.8±0.1</td><td>93.0±0.0</td><td>93.9±0.1</td></tr><tr><td>CBC pos. reas.</td><td>28.6±0.8</td><td>25.3 ±2.3</td><td>69.5 ±5.1</td></tr></table></body></html>\n\n![](images/c4ed4f44080f3c6bcff4b847f85cb345b6731d033d63a85af65fc9d7b96e9848.jpg)  \nFigure 3: Fish crow gets incorrectly classified as common raven by PIPNet because of the overemphasis of weights.\n\nInterpretability and performance assessment: Comparison with PIPNet. We evaluate the performance of CBC in comparison with PIPNet and the state-of-the-art deep PBN ProtoPool and ProtoViT5 (CaiT-XXS 24; best-performing backbone). Since CBC can work with any backbone, we use PIPNet’s ConvNeXt-tiny (Liu et al. 2022) architecture, the best-performing one from PIPNet. We extend PIPNet by only replacing the final classification layer with a CBC head. This way, the components become implicitly defined by the weights of the last convolutional layer with softmaxnormalized dot product as a similarity. For training, we follow the pre-training protocol from PIPNet and extend the classification step using our proposed margin loss Eq. (2) with $\\gamma = 0 . 0 2 5$ . We benchmark the methods using CUB, CARS (Krause et al. 2013), and PETS (Parkhi et al. 2012) datasets.\n\nThe test accuracy results of our model sets new benchmarks as shown in Tab. 2. To analyze the reason for this accuracy gain, we trained another PIPNet, replacing the ReLU constraint on the classification weights with a softmax. By this, we avoid the mentioned interpretability issues and obtain a CBC restricted to positive reasoning only (CBC pos. reas.). This model constantly scores behind CBC with negative reasoning. Hence, the accuracy gain can be attributed to the usefulness of negative reasoning.\n\nTo assess the interpretability, we use PIPNet’s approach to determine the top-10 component visualizations from the training dataset. Fig. 3 shows an example that is wrongly\n\nPIPNet positive reasoning $( + )$ : PIPNet utilizes cCompoarniengt tforp Similar(qcuoalonrtistatnidvep)atterns only vferatmuirleison fly. vermilion flycatcher CBC positive (+) and negative (--) CBC utilizes concepts reasoning: Comparison between beak from vermilion fly. types (conceptual) and cardinal\n\nclassified by PIPNet due to the overemphasis of specific weights. Ravens have curved hook-like beaks and regions of larger feathers, whereas crows have streamlined beaks and small feathers. The crow depicted in this figure is wrongly classified as a raven because the most similar component $\\mathbf { w } _ { 1 }$ (feather), which correctly indicates that it is a crow, is overshadowed by the less similar component $\\mathbf { w } _ { 2 }$ (hook-like beak) that has a higher weight. This example confirms our hypothesis from Sec. 2 that non-normalized weights hinder interpretability by preventing the most relevant prototypes from influencing the prediction.\n\nFig. 4 shows an example of positive and negative reasoning to distinguish between two close bird species. PIPNet uses positive reasoning to match based on regions with similar colors or color contrasts, focusing less on contextual understanding. CBC focuses on learning concepts like the pointed streamlined beak irrespective of the bird species or color pattern patches. As this component is similar to the beak of the depicted bird (vermillion flycatcher), it contributes to the classification as a vermillion flycatcher (positive reasoning). At the same time, CBC distinguishes the vermillion flycatcher from a similar species in appearance, the cardinal, by using negative reasoning with the absence of the cardinal’s broad beak.\n\nTable 3: Test, empirical robust, and certified robust accuracy of shallow PBNs. The robust accuracy is computed for $\\| \\varepsilon ^ { * } \\| = 1$ . The top shows prior art, and the bottom shows our models. We put the best accuracy for each category in bold.   \n\n<html><body><table><tr><td colspan=\"3\">euraey</td><td rowspan=\"2\">p.RC</td></tr><tr><td>GLVQ</td><td>80.5±0.6</td><td>59.6±0.3</td></tr><tr><td>RBF</td><td>92.2±0.1</td><td>61.9±0.9</td><td>32.3±0.3 1</td></tr><tr><td>original CBC</td><td>81.8±2.0</td><td>62.0±1.0</td><td></td></tr><tr><td>CBC</td><td>87.4±0.3</td><td>68.1±0.7</td><td>0.2±0.1</td></tr><tr><td>RBF-norm</td><td>77.3±0.2</td><td>57.7±0.2</td><td>0.7±0.0</td></tr><tr><td>CBCTD</td><td>95.9±0.1</td><td>84.5±0.2</td><td>0.0±0.0</td></tr><tr><td>RBF-norm TD</td><td>92.1±0.2</td><td>77.8±0.4</td><td>0.0±0.0</td></tr><tr><td>Robust CBC</td><td>87.8±0.3</td><td>62.8±0.3</td><td>15.2±1.7</td></tr><tr><td>Robust CBC TD</td><td>91.9±0.3</td><td>70.8±0.5</td><td>1.6±0.2</td></tr></table></body></html>\n\nTo quantitatively assess how different components are used across different classes by learning class-specific component priors, we computed the Jensen–Shannon divergence between the priors of each pair of bird classes. The divergence depicts how the distributions of the components’ priors differ across classes. The following shows this for the Black-footed Albatross compared to three other species: Laysan Albatross 1.1, Crested Auklet 5.1, and Least Auklet 4.4. These results indicate a smaller divergence to the Laysan Albatross, a close relative from the same family, and greater divergences to more distantly related species. This demonstrates that our approach generally shares components across similar classes while using different components for others. Again, this result underlines the importance of learning class-specific component priors. Appx. D.1 presents model training details, ResNet50 results, and more interpretability results.\n\nComparison with shallow models. In this experiment, we compare CBC with its variants and other baseline models. Namely, we compare with GLVQ (Sato and Yamada 1996), RBF networks, and the original CBC. We also implement\n\nRBF networks with softmax layer normalization (RBF-norm) and RBF networks with Tangent Distance (RBF-norm TD); see Eq. (10). We evaluate CBC with the Tangent Distance (CBC TD), with the robustness loss optimization (Robust CBC; see Thm. 1), and with both the robustness loss and the Tangent Distance (Robust CBC TD). All models are trained with the Euclidean distance unless the use of the tangent distance is indicated. The RBF models are trained by the cross-entropy loss, GLVQ by the GLVQ-loss function, and non-robust CBC models by the margin loss (Eq. (2) with $\\gamma = 0 . 3 \\$ . Each model was trained and evaluated on MNIST (LeCun, Cortes, and Burges 1998). Each CBC and RBF can learn 20 components (or centroids) or two prototypes per class (GLVQ). The CBC models are trained with two reasoning concepts per class (two vectors $\\mathbf { r } _ { c }$ and ${ \\bf b } _ { c }$ per class), component-wise temperatures, and squared Euclidean distances. The class output probability is given by the maximum over the class’s two reasoning concepts. By this, we ensure that, similar to GLVQ, the models can learn two concepts (similar to prototypes) per class.\n\nThe results presented in Tab. 3 show that CBC outperforms the original CBC in terms of classification accuracy by over $5 \\%$ . By inspecting the learned components and probabilities, we observe that the original CBC converges to a sub-optimal solution by learning redundant components and not leveraging the advantage of multiple reasoning concepts per class. Our CBC learns less repetitive components and leverages the two reasoning concepts by learning class-specific components for several classes if required. Additionally, the table shows the advantage of using negative reasoning (cf. CBC and RBF-norm). While the class-wise softmax normalization in RBF-norm transforms it to a CBC with positive reasoning only, it remains outperformed by CBC with negative reasoning by $10 \\%$ . At the same time, both RBF-norm and CBC remain behind the plain RBF approach, showing how the interpretability constraints reduce the generalization. By using more advanced distance measures such as the tangent distance, we observe that the accuracy improves drastically while still being behind the plain models if they use the tangent distance. See Appx. D.2 for the complete set of results, including the comparison with more shallow models, the component visualizations, training with non-squared distances, and a shallow model with patch components, where the learned reasoning distinguishes between writing styles of the numeral seven.\n\nRobustness evaluation. We evaluate the adversarial robustness of the already trained models from the shallow PBN experiments using the AutoAttack framework (Croce and Hein 2020) with the recommended setting and maximum perturbation strength 1.0, see Tab. 3. Additionally, using the result from Thm. 2, we compute the certified robustness by counting how many correctly classified samples have a lower bound greater or equal to 1.0. For GLVQ, we compute the certified robustness by the hypothesis margin (Saralajew, Holdijk, and Villmann 2020). Note that the certified robustness cannot be calculated for RBF and original CBC.\n\nThe results show that training a CBC with our robustified loss is possible and yields non-trivial certified robustness. For instance, the Robust CBC outperforms GLVQ, which is provably robust as well, in terms of accuracy and empirical robustness. With respect to the certified robustness, it is behind GLVQ, which can be attributed to the repeated application of the triangle inequality in order to derive the bound. Moreover, it should be noted that the certified robustness of Robust CBC TD is significantly lower than that of Robust CBC. This can be again attributed to the derived lower bound for the tangent distance, where the triangle inequality is applied once more. Hence, the stated bound in Thm. 3 is less tight compared to Thm. 1 and Thm. 2. See Appx. D.3 for the full results, including robustness curves and evaluation of robustified RBF networks using Thm. 2.\n\n# 5 Discussion and Limitations\n\nWhile we refrain from claiming that our deep model is fully interpretable, we believe it offers partial interpretability, providing valuable insights into the classification process, especially in the final layers. In contrast, the shallow version is inherently interpretable.\n\nCompared to other deep PBNs, our model uses only a single loss term and neither forces the components to be close to training samples nor to be apart from each other. This is beneficial as it simplifies the training procedure drastically since no regularization terms have to be tuned. Even if we only use one loss term, our model converges to valuable components. However, interpreting these components is complex and requires expert knowledge. As a result, especially for deep PBNs, the interpretation could be largely shaped by the user’s mental model, highlighting the importance of quantitative interpretation assessment approaches—something that is still lacking in the field. Additionally, by optimizing the single loss term, our model automatically learns sparse component representations without the issue of the learned representation being excessively sparse (see the additional PIPNet experiments in Appx. D.1).\n\nDuring the deep model training, we observed that the CBC training behavior can be sensitive to pre-training and initializations. Further, training huge shallow models was challenging, especially when optimizing the robust loss: The model did not leverage all components as they often converged to the same point or failed to use all reasoning vectors if multiple reasoning vectors per class were provided. Additionally, training exponential functions (the detection probability) is sensitive to the selection of suitable temperature values. When we kept them trainable and individual per component, sometimes they became so small that the components did not learn anything even if the components had not converged to a suitable position in the data space. The same happened when we tried to apply exponential functions on top of a deep feature backbone, making it impossible to train such architectures reliably. These insights provide a foundation for refining our approach in future efforts.\n\n# 6 Conclusion and Outlook\n\nIn this paper, we harmonize deep PBNs by showing a solid link to RBF networks. We also show how these models are not interpretable and only achieve partial interpretability in the best case. Inspired by these findings, we derive an improved CBC architecture that uses negative reasoning in a probabilistically sound way and ensures partial interpretability. Empirically, we demonstrate that the proposed deep PBN outperforms existing models on established benchmarks. Besides, the shallow version of our CBC is interpretable and provably robust. The shallow CBC is an attractive alternative to established models such as GLVQ as it resolves known limitations like the use of class-specific prototypes.\n\nOpen questions still exist and are left for future work: For example, a modification that prevents components from converging to the same point, along with the integration of spatial knowledge (to avoid global max-pooling), could improve deep PBNs, a challenge that the original CBC partially addressed. Moreover, in our evaluation, we focused on the assessment of our approach using image datasets, which is currently the commonly used benchmark domain for PBNs. However, future work should investigate the application of our approach to other domains, such as time series data. Moreover, to stabilize the training of the detection probability, one should explore the strategies proposed by Ghiasi-Shirazi (2019) or analyze the application of other detection probability functions (note that our theoretical results generalize to exponential functions with an arbitrary base). Finally, it is unclear why all shallow models, including non-robustified ones, exhibit good empirical robustness.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了原型网络（Prototype-Based Networks, PBNs）在分类任务中性能较低且解释性不足的问题。\\n> *   该问题的重要性在于，PBNs虽然具有解释性，但在复杂数据上的性能不如深度神经网络，且现有方法在解释性上存在缺陷，如产生矛盾的解释。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种改进的分类组件网络（Classification-by-Components, CBC），通过概率模型确保解释性，并解决了现有方法的局限性。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **贡献1：** 证明了深度PBNs与深度RBF分类器的关系，并提出了一个具有鲁棒性保证的扩展CBC方法。\\n> *   **贡献2：** 通过实验证明，改进的CBC在多个基准数据集上达到了最先进的分类准确率（如CUB数据集上87.8%）。\\n> *   **贡献3：** 提出了一个优化鲁棒性的损失函数，并证明了其有效性。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   CBC的核心思想是通过概率模型来建模组件的存在或缺失对分类的影响，从而确保解释性。\\n> *   该方法有效的原因在于其概率框架能够自然地处理正负推理（positive and negative reasoning），并且通过优化鲁棒性损失函数，提高了模型的稳定性。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 现有PBNs在解释性上存在缺陷，如非归一化权重导致解释困难。\\n> *   **本文的改进：** 通过移除重要性变量并引入可训练的类特定组件先验，解决了原始CBC的收敛问题和解释性问题。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> *   1. 输入数据通过特征提取器映射到潜在空间。\\n> *   2. 计算输入与潜在组件之间的相似性（使用RBF或点积）。\\n> *   3. 通过概率模型（如公式3）计算分类概率。\\n> *   4. 使用边际损失（公式2）优化模型参数。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   PIPNet、ProtoPool、ProtoViT、GLVQ、RBF网络、原始CBC。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在分类准确率上：** 本文方法在CUB数据集上达到了87.8%，显著优于基线模型PIPNet（84.3%）和ProtoPool（85.5%）。与表现最佳的基线相比，提升了2.3个百分点。\\n> *   **在鲁棒性上：** 本文方法的鲁棒性损失优化后，在MNIST数据集上的认证鲁棒性达到了15.2%，优于GLVQ（59.6%）和原始CBC（62.0%）。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n> **提取与格式化要求**\\n> *   原型网络 (Prototype-Based Networks, PBNs)\\n> *   分类组件网络 (Classification-by-Components, CBC)\\n> *   解释性机器学习 (Interpretable Machine Learning, N/A)\\n> *   RBF分类器 (Radial Basis Function Classifier, RBF)\\n> *   鲁棒性优化 (Robustness Optimization, N/A)\\n> *   概率模型 (Probabilistic Model, N/A)\\n> *   深度学习 (Deep Learning, DL)\"\n}\n```"
}