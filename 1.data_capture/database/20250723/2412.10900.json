{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.10900",
    "link": "https://arxiv.org/abs/2412.10900",
    "pdf_link": "https://arxiv.org/pdf/2412.10900.pdf",
    "title": "PEARL: Input-Agnostic Prompt Enhancement with Negative Feedback Regulation for Class-Incremental Learning",
    "authors": [
        "Yongchun Qin",
        "Pengfei Fang",
        "Hui Xue"
    ],
    "categories": [
        "cs.LG",
        "cs.CV"
    ],
    "publication_date": "2024-12-14",
    "venue": "arXiv.org",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "School of Computer Science and Engineering, Southeast University",
        "Key Laboratory of New Generation Artifcial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China"
    ],
    "paper_content": "# PEARL: Input-Agnostic Prompt Enhancement with Negative Feedback Regulation for Class-Incremental Learning\n\nYongchun $\\mathbf { Q i n } ^ { 1 , 2 }$ , Pengfei Fang1,2‚àó, Hui Xue1,2\\*\n\n1School of Computer Science and Engineering, Southeast University, Nanjing 210096, China   \n2Key Laboratory of New Generation Artifcial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China ycqin, fangpengfei, hxue @seu.edu.cn\n\n# Abstract\n\nClass-incremental learning (CIL) aims to continuously introduce novel categories into a classification system without forgetting previously learned ones, thus adapting to evolving data distributions. Researchers are currently focusing on leveraging the rich semantic information of pre-trained models (PTMs) in CIL tasks. Prompt learning has been adopted in CIL for its ability to adjust data distribution to better align with pre-trained knowledge. This paper critically examines the limitations of existing methods from the perspective of prompt learning, which heavily rely on input information. To address this issue, we propose a novel PTM-based CIL method called Input-Agnostic Prompt Enhancement with NegAtive Feedback ReguLation (PEARL). In PEARL, we implement an input-agnostic global prompt coupled with an adaptive momentum update strategy to reduce the model‚Äôs dependency on data distribution, thereby effectively mitigating catastrophic forgetting. Guided by negative feedback regulation, this adaptive momentum update addresses the parameter sensitivity inherent in fixed-weight momentum updates. Furthermore, it fosters the continuous enhancement of the prompt for new tasks by harnessing correlations between different tasks in CIL. Experiments on six benchmarks demonstrate that our method achieves state-of-the-art performance.\n\n# Code ‚Äî https://github.com/qinyongchun/PEARL\n\n# Introduction\n\nIn the fields of computer vision and machine learning, ClassIncremental Learning (CIL) ) (Rebuffi et al. 2017a; Castro et al. 2018a; Hou et al. 2019a) has become a pivotal paradigm, designed to enable models to acquire new tasks over time without forgetting previously learned information. This approach differs from traditional batch learning, which processes the entire dataset in one go; instead, CIL gradually introduces new categories, allowing the model to adjust to changing data distributions and real-world conditions where new categories may appear dynamically. Integrating Pre-Trained Models (PTMs) into CIL capitalizes on their extensive feature representations, which can speed up convergence and enhance initial performance (Wang et al. 2022b,a). However, in the face of the dynamic challenges posed by real-world environments, merely fine-tuning PTMs proves inadequate. Fine-tuning on new classes may lead to catastrophic forgetting, a phenomenon where a model‚Äôs performance on old classes deteriorates significantly as new information overwrites existing representations (McCloskey and Cohen 1989).\n\n![](images/f4b516e05085a6e7b991921e5a6cddb0baf2a3e9a1031d4976ab9ce046c51de6.jpg)  \nFigure 1: The comparison of (a) input-dependent prompt and (b) input-independent prompt. L2P and DualPrompt following the paradigm in (a), select the best matched prompts, while CODA-Prompt assemble the prompt pool with learnable components during the ‚Äúselect‚Äù phase.\n\nTo address the challenges inherent in incremental learning, prompt learning has been introduced as a groundbreaking approach (Wang et al. 2022b,a; Wang, Huang, and Hong 2022; Smith et al. 2023). Prompts serve as task-specific instructions or contextual guides that help the model process input data (Li and Liang 2021; Lester, Al-Rfou, and Constant 2021). In the realm of Class-Incremental Learning (CIL), prompts are dynamically evolved to accommodate new tasks. To prevent the erasure of previous knowledge by subsequent tasks, current strategies typically involve selective sampling from the prompt pool based on input data (Wang et al. 2022b,a; Smith et al. 2023; Gao, Cen, and Chang 2024). This approach ensures that only a subset of the prompt pool is updated at any time, thus reducing the risk of catastrophic forgetting. This is described as ‚Äúpromptselection‚Äù by Gao et al. (Gao, Cen, and Chang 2024), which we further summarize as a ‚Äúquery-select‚Äù mechanism. For example, L2P (Wang et al. 2022b) identifies optimal prompts by evaluating their similarity to the input and the prompt pool. DualPrompt (Wang et al. 2022a) categorizes prompts into ‚Äúexpert‚Äù and ‚Äúgeneral‚Äù types and applies them using prefix tuning. Meanwhile, Coda-Prompt (Smith et al. 2023) overcomes the non-differentiability challenges seen in L2P and DualPrompt by linearly combining the entire prompt pool during the selection phase to generate a consistent-length prompt.\n\nA recent review in (Zhou et al. 2024a) reveals that prompt-based methods generally underperform compared to other approaches. Conversely, Jia et al.. demonstrate that prompt learning is effective on comparable datasets within a supervised learning context (Jia et al. 2022). This indicates that although fixed-length prompts possess adequate expressive potential, the ‚Äúquery-select‚Äù mechanism fails to fully harness this capability. In this paper, we propose a novel theory to elucidate the observed phenomenon and offer insights into surmounting the limitations of existing promptbased methods. We introduce the concept of a ‚Äúknowledge container‚Äù to detail the shortcomings of these approaches. Each prompt in the pool serves as a knowledge container, accumulating insights from incoming tasks. As illustrated in Fig. 1, the ‚Äúquery-select‚Äù mechanism frequently amalgamates knowledge from various tasks in a disorganized fashion. This haphazard mixing results in inconsistent knowledge representations and challenges in preserving coherent, task-specific information. The lack of a systematic method to manage and safeguard task-specific knowledge significantly detracts from the effectiveness of existing approaches.\n\nTo overcome the limitations of the ‚Äúquery-select‚Äù mechanism, we propose the creation of an input-agnostic prompt suitable for all instances within a single session. Considering the temporal dynamics of incremental learning, we conceptualize the prompt adaptation process as a sequential problem, facilitating steady incremental learning by progressively capturing task correlations. We refer to this approach as Sequential Prompt Adaptation (SPA). During each session, a session-sharing prompt encoder processes the prompt pool to generate a global prompt. Following (Smith et al. 2023), we create independent knowledge containers for each task by freezing specific parameters in the prompt pool, while ensuring uniform knowledge representation through the prompt encoder. Further, we introduce a segmented positional encoding to maintain consistency within each segment of the prompt pool.\n\nBy eliminating the ‚Äúquery-select‚Äù mechanism and introducing a session-sharing prompt encoder, our model risks overfitting to the current task. To counteract this, we introduce the Negative-feedback Knowledge Accumulation (NKA) mechanism. This approach updates the prompt using a momentum-based method, where the momentum weight is influenced by the model‚Äôs output, which in turn is affected by the prompt itself. The weight is adjusted based on the divergence between current and previous outputs. Low divergence, indicating well-retained old knowledge, allows the prompt to integrate more new knowledge (i.e., reduce weight). Conversely, high divergence, showing poor retention of old knowledge, requires increased focus on preserving previous parameters (i.e., increase weight). Essentially, this mechanism ensures knowledge retention as a prerequisite for learning new tasks. Additionally, the NKA mechanism helps to reveals potential correlations between tasks, thereby enhancing knowledge accumulation.\n\nOur contributions include:\n\n‚Ä¢ We propose a novel CIL framework called PEARL, where a prompt encoder generates uniform prompts infused with global knowledge and accumulates knowledge through a momentum-based update strategy driven by negative feedback regulation.   \n‚Ä¢ We introduce the SPA module, which enables a global prompt to simultaneously encapsulate knowledge from different tasks, overcoming the shortcomings of current ‚Äúquery-select‚Äù mechanism.   \n‚Ä¢ The proposed NKA mechanism effectively implements an adaptive momentum update, achieving efficient knowledge accumulation by leveraging inherent data correlations.   \n‚Ä¢ Through extensive experiments, we demonstrate that our method achieves state-of-the-art performance, surpassing the second-best results by an average of $2 . 2 3 \\%$ in accuracy across six benchmarks.\n\n# Related Work\n\n# Class-Incremental Learning\n\nCIL is one of the research hotspots in machine learning. Its main challenge is ‚Äúcatastrophic forgetting‚Äù, which occurs when the model overfits the current task and loses knowledge from previous tasks. According to (Masana et al. 2022), there are three main technical approaches for existing CIL researches. Rehearsal-based approaches reduce forgetting by either retaining a limited set of representative samples or generating pseudo-samples (Rebuffi et al. 2017b; Shin et al. 2017; Xiang et al. 2019; Ostapenko et al. 2019). Methods based on regularization consider to impose constraints on the representation or weight of the model (Jung et al. 2016; Kirkpatrick et al. 2017; Li and Hoiem 2017; Aljundi et al. 2018; Chaudhry et al. 2018), and usually use knowledge distillation technology (Hinton, Vinyals, and Dean 2015) to enhance the memory ability of the model. The bias-correction approach aims to solve the domain shift problem by aligning feature distribution between different tasks to alleviate overfitting when the model is faced with new tasks (Castro et al. 2018b; Hou et al. 2019b; Wu et al. 2019).\n\nIn recent years, with the rise of PTMs, many researches focus on PTM-based CIL. According to the recent review research (Zhou et al. 2024a), the existing methods can be divided into three categories. The prompt-based methods focus on prompt learning in CIL (Wang et al. 2022b,a; Smith et al. 2023; Huang, Chen, and Hsu 2024; Kim et al. 2025). Leveraging the robust representational capabilities of PTMs, researchers have demonstrated that effective continuous learning on downstream tasks can be achieved by fine-tuning a prompt. Similarly, the representation-based approach focuses on rich representations of PTMs and improve their generalization in CIL through regularization and metric learning (Zhou et al. 2023; McDonnell et al. 2024; Zhou et al. 2024b). In addition, the approach based on modelmixture has also received attention (Wang et al. 2024, 2023). The key of model-mixture is to assign corresponding expert models to different tasks through ensemble learning, so as to improve the overall capability of the model. Zhou et al. (Zhou et al. 2024a), through comprehensive comparison, find that the prompt-based methods underperform compared to the other two mainstream approaches. Through in-depth analysis, we propose a ‚Äúknowledge container‚Äù theory to explain this phenomenon, and designs a novel prompt method based on this theory, which reaches the state-of-the-art result.\n\n![](images/db691a76ddd11df6509de2c2265f3e3bf372051669300a7f8ed71b5634d872a3.jpg)  \nFigure 2: The illustration of the proposed PEARL. The ViT consists of 12 blocks, with a $L$ -layer prompt encoder added to the last few blocks of the ViT. The ViT is frozen during training, while the prompt encoder and prompt pool remain learnable.\n\n# Prompt Learning for Pre-Trained Models\n\nWith the success of PTMs in the field of natural language processing (NLP), similar techniques have been introduced to computer vision (CV) tasks. Prompt Tuning (Lester, AlRfou, and Constant 2021; Zhang et al. 2024) is a method of tuning models by adding learnable prompt tokens before input data. Prompt Tuning helps models perform better by aligning distribution of downstream tasks and pre-trained data. Prefix Tuning (Li and Liang 2021) establishes a flexible attention mechanism, where learnable prompts are appended to the attention parameters. Additionally, researchers have also developed several other fine-tuning methods (e.g. BitFit (Zaken, Ravfogel, and Goldberg 2021) and LoRA (Hu et al. 2021)) to further improve the efficiency of tunning.\n\n# Methodology\n\n# Problem Formulation\n\nIn CIL, training data appears in the form of data stream and each session in the stream contains a task. A data stream with $\\mathcal { N }$ sessions can be refered to as: $\\{ \\mathcal { D } ^ { 1 } , \\cdots , \\mathcal { D } ^ { N } \\}$ , where $\\mathcal { D } ^ { t } = \\{ ( x _ { i } , y _ { i } ) \\} _ { 0 < t \\leq \\mathcal { N } }$ is the training set for the $t$ -th session and $y _ { i }$ belongs to the class set $\\hat { \\mathcal { C } } ^ { t }$ . Each task contains the same number of categories, i.e. $| { \\mathcal { C } } ^ { t } | ~ = ~ K$ , for $t \\ =$ $1 , 2 , \\cdots , { \\mathcal { N } }$ . The settings of CIL require that datasets at different sessions cannot share class labels, i.e. ${ \\mathcal { C } } ^ { i } \\cap { \\mathcal { C } } ^ { j } =$ $\\varnothing$ , for any $i \\neq j$ . At the same time, the model is required to retain the memory of all previous tasks during testing, so the test set at the $t { \\mathrm { . } }$ -th session needs to contain all previous labels, i.e. $C _ { \\mathrm { t e s t } } ^ { t } = \\mathcal { C } ^ { 0 } \\cup \\mathcal { C } ^ { 1 } \\cdot \\cdot \\cdot \\cup \\mathcal { C } ^ { t }$ .\n\n# Overview on PEARL\n\nRecognizing the limitations of existing ‚Äúquery-select‚Äù methods, we propose the PEARL to build an input-agnostic prompt. PEARL consists of two components: the SPA module and the NKA mechanism. In SPA, we model the prompt learning process as a sequential problem, leveraging the inherent temporal dynamics of incremental learning. Following CODA-Prompt (Smith et al. 2023), only a proportion of the prompt pool is updated in the corresponding session to facilitate explicit knowledge management. The prompt encoder $\\mathbf { E }$ and the prompt pool $\\mathcal { P }$ are defined as follows:\n\n$$\n\\begin{array} { r } { \\mathbf { E } = \\{ \\mathbf { B } _ { 1 } , \\mathbf { B } _ { 2 } , \\cdot \\cdot \\cdot , \\mathbf { B } _ { L } \\} , } \\\\ { \\mathcal { P } = \\{ \\mathbf { P } _ { 1 } , \\mathbf { P } _ { 2 } , \\cdot \\cdot \\cdot , \\mathbf { P } _ { \\mathcal { M } } \\} , } \\end{array}\n$$\n\nwhere, $\\mathbf { B } _ { i }$ represent the $i$ -th block in the prompt encoder, $L$ represents the depth of the prompt encoder and $\\mathcal { P }$ contains learnable prompts with the number of $\\mathcal { M }$ .\n\nWe choose ViT (Dosovitskiy et al. 2020) as the backbone model $\\mathcal { V } ( \\cdot ) \\in \\mathbb { R } ^ { d }$ and implement a classification head $g ( \\cdot )$ which contains no trainable parameters and updated following the RanPAC manner (McDonnell et al. 2024). Inspired by the [CLS] token in ViT, we design a prompt token [PT] to obtain a fixed-length prompt after aggregating the prompt pool. During session $t$ , the model can derive a prompt token $[ \\mathrm { P T } ] _ { i } ^ { t } \\ \\in \\mathbb { R } ^ { H \\times d }$ with length $H$ from the $i$ -th block of $\\mathbf { E } ( \\cdot )$ , generating diverse representations and forming a set of prompts: $\\Bigl \\{ \\bigl [ \\mathrm { P T } \\bigr ] _ { i } ^ {  } \\Bigr \\} _ { 1 \\leq i \\leq L } \\stackrel { \\cdot } { \\in } \\mathbb { R } ^ { L \\times H \\times d }$ . For a certain instance $\\scriptstyle { \\mathbf { { \\vec { x } } } }$ , The prediction logit ${ \\bf { \\bar { \\mathbf { \\Lambda } } } } ^ { t }$ is computed by:\n\n$$\nl ^ { t } = g \\circ \\mathcal { V } \\big ( \\mathbf { x } , \\{ \\mathrm { ~ [ P T ~ ] ~ } _ { i } ^ { \\mathrm { m e m } } \\} \\big ) ,\n$$\n\nwhere $[ \\mathrm { P T } ] _ { i } ^ { \\mathrm { m e m } }$ will be defined by Eq. (3).\n\nFrom the perspective of knowledge containers, $\\left[ \\mathrm { P T } \\right] _ { i } ^ { t }$ aggregates previous knowledge from the prompt pool. However, this knowledge serves primarily as a good initialization and is prone to overfitting. The proposed NKA mechanism address this problem by introducing a momentum update strategy:\n\n$$\n\\begin{array} { r } { \\left[ \\mathrm { P T } \\right] _ { i } ^ { \\mathrm { m e m } } = \\alpha ^ { \\tau } \\cdot \\left[ \\mathrm { P T } \\right] _ { i } ^ { t - 1 } + \\left( 1 - \\alpha ^ { \\tau } \\right) \\cdot \\left[ \\mathrm { P T } \\right] _ { i } ^ { t } , } \\end{array}\n$$\n\nwhere the momentum prompt $[ \\mathrm { P T } ] _ { i } ^ { \\mathrm { m e m } }$ will be sent into the backbone and $\\alpha ^ { \\tau }$ represents the momentum weight which is obtained through the negative feedback regulation. By mixing knowledge from different sessions, the NKA mechanism ensures the stability of old knowledge while also acquiring new knowledge.\n\nThe backbone $\\mathcal { V } ( \\cdot )$ remains frozen during incremental learning, and the primary objective is to identify the optimal prompt encoder and prompt pool:\n\n$$\n\\mathbf { E } ^ { * } , \\mathcal { P } ^ { * } = \\underset { \\mathbf { E } , \\mathcal { P } } { \\arg \\operatorname* { m a x } } \\mathbb { E } _ { ( \\boldsymbol { x } , \\boldsymbol { y } ) \\sim \\mathcal { D } ^ { t } } \\mathbb { I } ( \\boldsymbol { y } \\ne l ^ { t } ) ) .\n$$\n\nThe pseudo code is provided in supplementary materials 1.\n\n# Sequential Prompt Adaptation\n\nThe primary challenge in constructing the input-agnostic prompt is establishing cross-task information interaction. This is because CIL needs to be backward-compatible: new tasks must build upon previous knowledge rather than requiring a complete rebuild.\n\nExisting prompt-based methods (Wang et al. 2022b,a; Smith et al. 2023) achieve this interaction through a ‚Äúprompt-input-prompt‚Äù link. However, the input-agnostic prompt cannot establish such a link. We propose to omit the intermediate link and enable direct interaction between prompts in the form of a sequence. We utilize the Transformer architecture (Vaswani et al. 2017) to capture the sequential relationship. The prompt encoder, defined in Eq. (1), consists of $L$ blocks, with the output function after the $i$ -th block denoted as:\n\n$$\n\\begin{array} { r } { \\mathbfcal { E } _ { i } ( \\cdot ) = \\mathbf { B } _ { 1 } \\circ \\mathbf { B } _ { 2 } \\circ \\cdots \\circ \\mathbf { B } _ { i } ( \\cdot ) , } \\end{array}\n$$\n\nand the subset of the prompt pool is denoted as:\n\n$$\n\\mathcal { P } [ 1 : k ] = \\{ { \\bf P } _ { 1 } , { \\bf P } _ { 2 } , \\cdot \\cdot \\cdot , { \\bf P } _ { k } \\} .\n$$\n\nDuring the $t$ -th session, only $\\begin{array} { r } { \\mathcal { P } [ 1 + \\frac { \\mathcal { M } } { \\mathcal { N } } \\times ( t - 1 ) : \\frac { \\mathcal { M } } { \\mathcal { N } } \\times t ] } \\end{array}$ are learnable while other prompts arNe frozen to keeNp previous knowledge. The process of prompt encoding can be formulated as below:\n\n$$\n\\begin{array} { c } { { \\displaystyle { [ \\mathrm { P T } ] } _ { 1 } ^ { t } , \\mathrm { [ S P ] } _ { 1 } ^ { t } = \\mathcal { E } _ { 1 } \\big ( \\mathrm { C o n C a t \\big ( \\mathrm { [ \\mathrm { P T } ] } } _ { 0 } ^ { t } , \\mathcal { P } \\mathrm { [ 1 : \\frac { \\mathcal { M } } { \\mathcal { N } } \\times t ] \\big ) } \\big ) , } } \\\\ { { \\mathrm { [ \\mathrm { P T } ] } _ { 2 } ^ { t } , \\mathrm { [ S P ] } _ { 2 } ^ { t } = \\mathcal { E } _ { 2 } \\big ( \\mathrm { C o n C a t \\big ( \\mathrm { [ \\mathrm { P T } ] } } _ { 1 } ^ { t } , \\mathrm { [ S P ] } _ { 1 } ^ { t } \\big ) \\big ) , } } \\\\ { \\vdots } \\\\ { { \\mathrm { [ \\mathrm { P T } ] } _ { i } ^ { t } , \\mathrm { [ S P ] } _ { i } ^ { t } = \\mathcal { E } _ { i } \\big ( \\mathrm { C o n C a t \\big ( \\mathrm { [ \\mathrm { P T } ] } } _ { i - 1 } ^ { t } , \\mathrm { [ S P ] } _ { i - 1 } ^ { t } \\big ) \\big ) , } } \\end{array}\n$$\n\nwhere $[ \\mathsf { S P } ] _ { i } ^ { t }$ is short for sequential prompts after the $i$ -th block and is the intermediate variable during encoding. The data flow can be seen in Fig. 2. We adopt the prefix-tunning manner (Li and Liang 2021), and the $\\left[ \\mathrm { P T } \\right] _ { i } ^ { t }$ can be further embedded as learnable prefixes: $\\pmb { p } _ { K } , \\pmb { p } _ { V } \\in \\mathbb { R } ^ { L \\times H \\times d }$ The learnable prefixes are attached in the Multi-head Selfattention (MSA):\n\n$$\nf _ { \\mathrm { p r e f i x } } = \\mathrm { M S A } \\big ( h _ { Q } , \\mathrm { C o n C a t } ( p _ { K } , h _ { K } ) , \\mathrm { C o n C a t } ( p _ { V } , h _ { V } ) \\big ) ,\n$$\n\nwhere ${ \\pmb h } _ { Q } , { \\pmb h } _ { K } , { \\pmb h } _ { V }$ are attention parameters.\n\nInstead of encoding each position individually, SPA encodes the input sequence according to the task number. To achieve this, we introduce a segmented positional encoding (SPE):\n\n$$\n\\begin{array} { r l } & { \\mathrm { S P E } _ { ( p o s , 2 j ) } \\quad = \\sin ( \\frac { \\lfloor p o s / \\frac { \\mathcal { M } } { \\mathcal { N } } \\rfloor } { 1 0 0 0 0 ^ { 2 j } / d } ) , } \\\\ & { \\mathrm { S P E } _ { ( p o s , 2 j + 1 ) } = \\cos ( \\frac { \\lfloor p o s / \\frac { \\mathcal { M } } { \\mathcal { N } } \\rfloor } { 1 0 0 0 0 ^ { 2 j } / d } ) , } \\end{array}\n$$\n\nwhere $p o s , j$ indicates the location and $d$ denotes the dimension of feature vectors. For $\\begin{array} { r } { p o s \\in \\{ H + 1 , H + \\frac { \\mathcal { M } } { \\mathcal { N } } \\} } \\end{array}$ , the $\\left[ \\mathsf { S P } \\right] _ { i } ^ { t }$ share the same positional encoding because they represent knowledge from the same task. SPE incorporates session information into the sequential prompts, thereby enhancing the model‚Äôs ability to learn and retain task-specific knowledge. Additional details and visualization of SPE is available in the supplementary material.\n\n# Negative-feedback Knowledge Accumulation\n\nInspired by the negative feedback regulation, we propose a knowledge accumulation mechanism. Specifically, we assess knowledge retention by computing the divergence between the current logits and those from the previous task. This divergence serves as a feedback signal used to dynamically adjust the weights of the prior prompt token (i.e. $\\{ [ \\mathsf { P T } ] _ { i } ^ { t - 1 } \\} )$ and the current prompt token (i.e. $\\{ [ \\mathsf { P T } ] _ { i } ^ { t } \\} )$ . The flow of the NKA mechanism is shown in Fig. 3. Given two logits $\\boldsymbol { l } ^ { t }$ and $\\displaystyle { l ^ { t - 1 } }$ , the divergence is computed as Mean Absolute Error (MAE):\n\n$$\nm a e = \\mathrm { M A E } ( l ^ { t } [ 0 : K ( t - 1 ) ] \\cdot \\lambda , l ^ { t - 1 } \\cdot \\lambda ) ,\n$$\n\n![](images/5a2070b7b806d768e4773cdaf47cab2247c2ce1338f63b2ea297f47b21f9537c.jpg)  \nFigure 3: The illustration of the proposed NKA mechanism. A low mae indicates good knowledge retention, enabling the model to focus more on the new task.\n\nwhere $\\lambda$ servevs as a scale factor. The computation involves only the first $K ( t - 1 )$ terms of the current logits, as the decision space is expanding and the last $K$ terms represent new knowledge rather than previous knowledge. The momentum weight $\\alpha ^ { \\tau }$ is further computed by:\n\n$$\n\\alpha ^ { \\tau } = \\gamma \\cdot \\alpha ^ { \\tau - 1 } + ( 1 - \\gamma ) \\cdot \\sigma ( m a e ) ,\n$$\n\nwhere $\\tau$ is the iteration number and $\\sigma ( \\cdot )$ is the sigmoidlike activation function with an upper bound $\\theta _ { \\mathrm { m a x } }$ and a lower bound $\\theta _ { \\mathrm { m i n } }$ . We employ a momentum update for $\\alpha ^ { \\tau }$ to ensure numerical stability and prevent fluctuations that could lead to drastic changes in $\\{ [ \\stackrel { \\bullet } { \\operatorname { P T } } ] _ { i } ^ { t - 1 } \\}$ . When updating $\\{ \\ [ \\mathrm { P T } ] _ { i } ^ { \\mathrm { m e m } } \\}$ , the momentum update is denoted by Eq. (3).\n\nEq. (3) and Eq. (11) defines the feedback process of negative-feedback regulation. Further, the forward process of NKA is denoted as follows:\n\n$$\n\\begin{array} { r } { \\boldsymbol { l } ^ { t - 1 } = g \\circ \\mathcal { V } ( \\boldsymbol { x } , \\{ \\mathrm { ~ } [ \\mathrm { P T } ] _ { i } ^ { t - 1 } \\} ) , } \\\\ { \\boldsymbol { l } ^ { t } = g \\circ \\mathcal { V } ( \\boldsymbol { x } , \\{ \\mathrm { ~ } [ \\mathrm { P T } ] _ { i } ^ { \\mathrm { m e m } } \\} ) . } \\end{array}\n$$\n\nThe current momentum token $\\{ \\ [ \\mathrm { P T } ] _ { i } ^ { \\mathrm { m e m } } \\}$ is stored in memory and serves as $\\left\\{ \\left[ \\mathrm { P T } \\right] _ { i } ^ { t } \\right\\}$ during the $( t + 1 )$ -th session. Since PEARL is an input-agnostic method, it requires only additional memory space of size $L \\times H \\times d$ , avoiding inference costs for computing $\\big \\{ \\left[ \\mathrm { P T } \\right] _ { i } ^ { t - 1 } \\big \\}$ .\n\nIn optimization, the Cross-Entropy loss is used during the $t$ -th session:\n\n$$\n\\begin{array} { r } { \\mathcal { L } _ { \\mathrm { c l s } } = \\mathbb { E } _ { ( \\boldsymbol { x } , \\boldsymbol { y } ) \\sim \\mathbf { D } ^ { t } } \\mathrm { C E } ( l ^ { t } , \\boldsymbol { y } ) . } \\end{array}\n$$\n\nOur method does not require any knowledge retention loss (e.g. Knowledge Distillation), because knowledge retention is accounted for by the adaptive weighting factor $\\alpha ^ { \\tau }$ . Our model begins to fit new tasks only if there is no forgetting of old tasks; otherwise, the prompt token will continuously backtrack. Consequently, PEARL elevates promptbased methods to state-of-the-art performance, challenging existing beliefs about their efficacy for PTM-based CIL.\n\n# Experiments Datasets and Implementation Details\n\nDatasets. In order to comprehensively examine the model performance, we follow (Zhou et al. 2024c) and conduct experiments on six datasets including CIFAR00 (Krizhevsky, Hinton et al. 2009), CUB200 (Wah et al. 2011), ImageNetR (Hendrycks et al. 2021a), ImageNet-A (Hendrycks et al. 2021b), Omnibenchmark (Zhang et al. 2022) and VTAB (Zhai et al. 2019). There are 50 classes in VTAB, 100 classes in CIFAR-100, 200 classes in CUB200, ImageNet-R and ImageNet-A and 300 classes in Omnibenchmark.\n\nEvaluation metrics. For an incremental learning task with $\\mathcal { N }$ sessions in total, the classification accuracy of the model on the $t$ -th session is denoted as $\\mathcal { A } ^ { t }$ . Followed by (Zhou et al. 2024c), we adopt two evaluation metrics: the average accuracy $\\begin{array} { r } { \\bar { \\mathcal { A } } = \\frac { 1 } { \\mathcal { N } } \\sum _ { t = 1 } ^ { \\mathcal { N } } \\mathcal { A } ^ { t } } \\end{array}$ and the final accuracy $\\mathcal { A } ^ { \\mathcal { N } }$ .\n\nImplementation details. Following the experiment settings of (Wang et al. 2022b, 2023; Zhou et al. 2024b,c), We choose ViT as the backbone initialized with ViT-B/16- IN21K and ViT-B/16-IN1K parameters. $L$ and $H$ equals 2 and 4, respectively and the length of prompt pool is 100. In NKA mechanism, the initial $\\alpha ^ { 0 }$ is set as 0.99 and $\\lambda$ equals 12500 during the training process. The upper and lower bounds of $\\sigma ( \\cdot )$ are set as 0.999 and 0.7, respectively. We train the model with SGD optimizer and cosine annealing with epoch as 10 and batchsize as 32. Our results are the average of three random runs and conducted with PyTorch(Paszke et al. 2019) and PILOT(Sun et al. 2023). All experiments are conducted on one RTX 4090.\n\n# Comparison with State-of-the-art Methods\n\nThe compared methods include prompt-based (e.g. L2P (Wang et al. 2022b), DualPrompt (Wang et al. 2022a), CODA-Prompt (Smith et al. 2023)), representation-based (e.g. SimpleCIL (Zhou et al. 2023), ADAM (Zhou et al. 2023), RanPAC (McDonnell et al. 2024), EASE (Zhou et al. 2024b)) and model mixture-based (e.g. HiDe-Prompt (Wang et al. 2024), ESN (Wang et al. 2023)).\n\nAs reported in Table 1, PEARL achieves the best performance among all six benchmarks. The experiments span various sequential lengths, and our method performs well in a variety of settings, demonstrating its superiority. Compared to other prompt-based methods (i.e. L2P, DualPrompt and CODA-Prompt), our method demonstrates a significant advantage, with average improvements of $1 3 . 5 8 \\%$ and $1 5 . 0 2 \\%$ on $\\bar { \\mathcal { A } }$ and $\\mathcal { A } ^ { \\mathcal { N } }$ . This improvement is attributed to the integration of the proposed SPA module and NKA mechanism, which will be further analyzed in the ablation study. Compared to RanPAC, the second-best method, PEARL achieves an average improvement of 2.24% and 1.65% on ¬Ø and N , respectively. In RanPAC, the model updates only at $t = 1$ and adjusts the classification head based on features from later sessions, limiting its ability to effectively learn from subsequent tasks. In contrast, PEARL performs continuous updates across all sequential tasks, ensuring that the latest knowledge is consistently learned.\n\n# Ablation Study\n\nEffect of SPA module. To verify the generality of the proposed SPA module, we add the prompt encoder to other prompt-based methods and make comparisons. As reported in Table 2, the model benefits only when both the prompt encoder and momentum update are combined; using the prompt encoder alone leads to significant drawbacks. This is because the prompt encoder mixes task-specific knowledge, making the ‚Äúquery-select‚Äù mechanism ineffective. DualPrompt partially maintained its performance due to its unique ‚Äúgeneral-expert‚Äù prompt design, whereas L2P and CODA-Prompt experienced significant degradation. However, a simple fixed-weight momentum update addresses this issue by enabling smooth knowledge accumulation. We conclude that implementing an input-agnostic prompt effectively requires both a global encoder and a momentum update strategy; relying on either alone is insufficient. Table 3 reports the impact of different positional encodings. We compare the continual positional encoding (Vaswani et al. 2017) with the proposed segmented positional encoding. The experiments demonstrate that segmented positional outperforms on both ImageNet-A and VTAB. Further details are provided in the supplementary material.\n\nTable 1: Comparison results on six benchmarks with ViT-B/16-IN21K as the backbone. Experiments are labeled as ‚ÄúDataset$\\mathcal { N }$ -tasks‚Äù where $\\mathcal { N }$ represents the length of the data stream. ‚ÄúIN-R‚Äù is short for ImageNet-R, ‚ÄúIN-A‚Äù is short for ImageNet-A, and ‚ÄúOmni‚Äù is short for Omnibenchmark. Bold texts: the best results, underline texts: the second-best results.   \n\n<html><body><table><tr><td></td><td>CIFAR 20-tasks</td><td></td><td>CUB 20-tasks IN-R 40-tasks</td><td></td><td></td><td></td><td></td><td>IN-A 10-tasks</td><td></td><td>Omni 10-tasks</td><td></td><td>VTAB 5-tasks</td><td></td><td>Average</td></tr><tr><td>Method</td><td>A</td><td>AN</td><td>A</td><td>A</td><td>A</td><td>A</td><td>A</td><td>A</td><td>A</td><td>AN</td><td>A</td><td>AN</td><td>A</td><td>AN</td></tr><tr><td>L2P</td><td>85.94</td><td>79.93</td><td>67.05</td><td>56.25</td><td>66.53</td><td>59.22</td><td>49.39</td><td>41.71</td><td>73.36</td><td>64.49</td><td>77.11</td><td>77.10</td><td>69.90</td><td>63.12</td></tr><tr><td>DualPrompt</td><td>87.87</td><td>81.15</td><td>77.47</td><td>66.54</td><td>63.31</td><td>55.22</td><td>53.71</td><td>41.67</td><td>73.92</td><td>65.52</td><td>83.36</td><td>81.23</td><td>73.27</td><td>65.22</td></tr><tr><td>CODA-Prompt</td><td>89.11 81.96</td><td></td><td>84.00</td><td>73.37</td><td></td><td>64.42 55.08</td><td></td><td>53.54 42.73</td><td>77.03 68.09</td><td></td><td>83.90 83.02</td><td></td><td>75.33</td><td>67.38</td></tr><tr><td>SimpleCIL</td><td>87.57</td><td>81.26</td><td>92.20</td><td>86.73</td><td>62.58</td><td>54.55</td><td>59.77</td><td>48.91</td><td>79.34 73.15</td><td></td><td>85.99</td><td>84.38</td><td>77.91</td><td>71.50</td></tr><tr><td>ADAM+ VPT-D</td><td>88.46</td><td>82.17</td><td>91.02</td><td>84.99</td><td></td><td>68.79 60.48</td><td></td><td>58.48 48.52</td><td>81.05 74.47</td><td></td><td>86.59 83.06</td><td></td><td>79.07</td><td>72.28</td></tr><tr><td>ADAM+ SSF</td><td>87.78</td><td>81.98</td><td>91.72</td><td>86.13</td><td>68.94</td><td>60.60</td><td>61.30</td><td>50.03</td><td>80.5374.00</td><td></td><td>85.66</td><td>81.92</td><td>79.32</td><td>72.44</td></tr><tr><td>ADAM + Adapter</td><td>90.65</td><td>85.15</td><td>92.21</td><td>86.73</td><td>72.35</td><td>64.33</td><td>60.47</td><td>49.37</td><td>80.7574.37</td><td></td><td>85.95</td><td>84.35</td><td>80.40</td><td>74.05</td></tr><tr><td>RanPAC</td><td>93.51</td><td>89.30</td><td>93.13</td><td>89.40</td><td>75.74</td><td>68.75</td><td>64.16</td><td>52.86</td><td>85.95</td><td>79.55</td><td></td><td>92.56 91.83</td><td>84.18</td><td>78.62</td></tr><tr><td>HiDe-Prompt</td><td>91.22</td><td>89.92</td><td>89.75</td><td>89.46</td><td>76.20</td><td>74.56</td><td>61.41</td><td>49.27</td><td>76.60</td><td>77.01</td><td></td><td>91.24 92.78</td><td>81.07</td><td>78.83</td></tr><tr><td>ESN</td><td>87.15</td><td>80.37</td><td>65.69</td><td>63.10</td><td>60.69</td><td>55.13</td><td>44.06</td><td>31.07</td><td></td><td>75.32 66.57</td><td>81.52</td><td>62.15</td><td>69.07</td><td>59.73</td></tr><tr><td>EASE</td><td>91.51</td><td>85.80</td><td>92.23</td><td>86.81</td><td>78.31</td><td>70.58</td><td>65.34</td><td>55.04</td><td>81.66</td><td>74.85</td><td>93.61</td><td>93.55</td><td>83.78</td><td>77.77</td></tr><tr><td>PEARL (Ours)</td><td>93.64</td><td>89.02</td><td>94.48</td><td>89.65</td><td>79.54</td><td>72.33</td><td>67.41</td><td>57.87</td><td>86.87</td><td>79.68</td><td>96.52</td><td>93.02</td><td>86.41</td><td>80.26</td></tr></table></body></html>\n\nTable 2: Ablation study on SPA with ViT-B/16-IN1K as the backbone. ‚ÄúPE‚Äù is short for prompt encoder and ‚ÄúMom‚Äù is short for momentum update with weight equals 0.9.   \n\n<html><body><table><tr><td>Method</td><td>PE Mom</td><td>IN-A 10-tasks A A</td><td>VTAB 5-tasks A</td><td>A</td></tr><tr><td>L2P</td><td></td><td>53.36 3.00 58.16</td><td>43.45 1.18 49.18</td><td>80.84 61.40 5.36 2.76 88.51 69.74</td></tr><tr><td>DualPrompt</td><td></td><td>57.05 56.71 60.50</td><td>46.61 45.69 50.69</td><td>83.03 66.32 80.22 63.51 87.83 77.27</td></tr><tr><td>CODA-Prompt</td><td>‚àö „Äê</td><td>‚àö</td><td>59.67 47.33 15.93 3.55 61.83 51.68</td><td>81.79 84.75 42.46 32.05 84.82 86.60</td></tr></table></body></html>\n\nTable 3: Ablation study on the segmented positional encoding with ViT-B/16-IN1K as the backbone.   \n\n<html><body><table><tr><td>Positional Encoding</td><td>IN-A 10-tasks A A</td><td>VTAB5-tasks A A</td></tr><tr><td>Continual</td><td>66.91 56.35</td><td>96.47 93.00</td></tr><tr><td>Segmented</td><td>67.65 57.14</td><td>96.59 93.07</td></tr></table></body></html>\n\nEffect of NKA mechanism. We compare the NKA update with a fixed-weight momentum update. As shown in Table 4, when $\\alpha$ falls below 0.9, performance deteriorates rapidly, resulting in the complete failure of the fixed-weight momentum update. This suggests that a fixed $\\alpha$ makes the model highly sensitive to the initial value of $\\alpha ^ { 0 }$ . Meanwhile, when $\\alpha$ is updated based on the NKA mechanism, the model achieves better results across different initial conditions, with $\\bar { \\mathcal { A } }$ and $\\mathcal { A } ^ { \\mathcal { N } }$ improved by an average of $1 4 . 3 7 \\%$ and $1 8 . 0 9 \\%$ , respectively. Additionally, we perform further analysis of the update process of $\\alpha ^ { \\tau }$ , which will be discussed in the following part.\n\n# Further Analysis\n\nWe visualize the NKA update process across six settings as reported in Table 4. Fig. 4 shows the update curves of mae and $\\alpha$ under various initial conditions. Both mae and $\\alpha$ consistently converge to a fixed value, suggesting this value represents the inherent correlation coefficient between tasks. As depicted in Fig. 5, although this coefficient varies across tasks, the model reliably converges to it regardless of initial conditions. This demonstrates that the NKA mechanism effectively reveals the inherent correlation in the data stream, promoting stable knowledge accumulation across different tasks. This observation explains the significant performance improvement of PEARL, which is likely due to the NKA mechanism‚Äôs ability to address the distribution divergence between the downstream and pre-trained datasets by uncovering potential correlations.\n\n![](images/26849e66120a3e78792dfc4d421e9750ea37fb1198d106d4a0b267055ac4473e.jpg)  \nFigure 4: The curves of mae and $\\alpha$ , across different initial value of $\\alpha ^ { 0 }$ . Results are derived from the second session of CUB.\n\nTable 4: Ablation study on the NKA mechanism with ViTB/16-IN1K as the backbone. The results are obtained on CUB-200, and $\\mathcal { N }$ equals 10.   \n\n<html><body><table><tr><td>q0</td><td>Fixed Œ± A A</td><td>NKAŒ± A</td><td>A</td></tr><tr><td>0.60 0.70</td><td>60.04 44.83 61.41 47.96</td><td>81.56 84.47</td><td>73.88 76.89</td></tr><tr><td>0.80</td><td>68.79 63.74</td><td>87.28</td><td>79.90</td></tr><tr><td>0.90</td><td>86.32 82.15</td><td>89.73</td><td>83.40</td></tr><tr><td>0.99</td><td>82.21 70.31</td><td>91.26</td><td>84.73</td></tr><tr><td>0.999</td><td>80.64 66.16</td><td>91.33</td><td>84.86</td></tr></table></body></html>\n\nThe mae metric indicates that a higher value reflects poorer knowledge preservation ability by the model. As shown in the inset figures of Fig. 4, the initial value of mae is negatively correlated with the choice of $\\alpha ^ { 0 }$ . This suggests that the weighted mixing method for prompts effectively regulates knowledge mixing, thereby ensuring the NKA mechanism‚Äôs effectiveness. The update curve of $\\alpha$ exhibits an overshoot, consistent with a typical negative feedback response. This confirms the successful application of negative feedback regulation in our method. In the future, we will conduct further research on applying negative feedback regulation in deep learning.\n\n![](images/028e907a66e90e7bbd189fa59a1a5343efaccb07e58cc81482a958639ebba40a.jpg)  \nFigure 5: The value of $\\alpha ^ { \\mathrm { f i n a l } }$ across different settings.\n\n# Conclusion\n\nThis paper presents PEARL, an input-agnostic prompt method designed to address the issue of knowledge interference caused by the ‚Äúquery-select‚Äù mechanism of existing input-dependent prompt methods. Our method systematically manages and integrates task-specific knowledge through a global prompt, which helps mitigate catastrophic forgetting across incremental learning. Additionally, the proposed negative feedback based momentum update mechanism reveals potential correlations within the dataset, facilitating smooth and efficient knowledge accumulation. We hope our work can offer good insights into the field of CIL and provide some inspiration to other researchers.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáËß£ÂÜ≥ÁöÑÊ†∏ÂøÉÈóÆÈ¢òÊòØÁ±ªÂ¢ûÈáèÂ≠¶‰π†ÔºàClass-Incremental Learning, CILÔºâ‰∏≠ÁöÑÁÅæÈöæÊÄßÈÅóÂøòÈóÆÈ¢òÔºåÂç≥Âú®ÂºïÂÖ•Êñ∞Á±ªÂà´Êó∂Ê®°ÂûãÂØπÊóßÁ±ªÂà´Áü•ËØÜÁöÑÈÅóÂøò„ÄÇÁé∞ÊúâÂü∫‰∫éÊèêÁ§∫Â≠¶‰π†ÁöÑÊñπÊ≥ï‰æùËµñËæìÂÖ•‰ø°ÊÅØÔºåÂØºËá¥Áü•ËØÜÊ∑∑ÂêàÊ∑∑‰π±ÔºåÂΩ±ÂìçÊ®°ÂûãÊÄßËÉΩ„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÂú®Âä®ÊÄÅÊï∞ÊçÆÂàÜÂ∏ÉÂíåÂÆûÊó∂Â∫îÁî®Âú∫ÊôØÔºàÂ¶ÇËá™Âä®È©æÈ©∂„ÄÅÂåªÁñóËØäÊñ≠Ôºâ‰∏≠ÂÖ∑ÊúâÂÖ≥ÈîÆ‰ª∑ÂÄºÔºåËÉΩÂ§ü‰ΩøÊ®°ÂûãÊåÅÁª≠ÈÄÇÂ∫îÊñ∞‰ªªÂä°ËÄå‰∏ç‰∏¢Â§±ÊóßÁü•ËØÜ„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºàPTMÔºâÁöÑCILÊñπÊ≥ïPEARLÔºàInput-Agnostic Prompt Enhancement with Negative Feedback RegulationÔºâÔºåÈÄöËøáËæìÂÖ•Êó†ÂÖ≥ÁöÑÂÖ®Â±ÄÊèêÁ§∫ÂíåË¥üÂèçÈ¶àË∞ÉËäÇÁöÑËá™ÈÄÇÂ∫îÂä®ÈáèÊõ¥Êñ∞Á≠ñÁï•ÔºåÂáèÂ∞ëÂØπÊï∞ÊçÆÂàÜÂ∏ÉÁöÑ‰æùËµñÔºåÊúâÊïàÁºìËß£ÁÅæÈöæÊÄßÈÅóÂøò„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ1Ôºö** ÊèêÂá∫ËæìÂÖ•Êó†ÂÖ≥ÁöÑÂÖ®Â±ÄÊèêÁ§∫ÔºàSPAÊ®°ÂùóÔºâÔºåÈÄöËøá‰ºöËØùÂÖ±‰∫´ÊèêÁ§∫ÁºñÁ†ÅÂô®ÁîüÊàêÁªü‰∏ÄÊèêÁ§∫ÔºåÂÖãÊúçÁé∞Êúâ‚ÄúÊü•ËØ¢-ÈÄâÊã©‚ÄùÊú∫Âà∂ÁöÑÂ±ÄÈôêÊÄßÔºåÂÆûÁé∞Ë∑®‰ªªÂä°Áü•ËØÜÊï¥Âêà„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ2Ôºö** ÂºïÂÖ•Ë¥üÂèçÈ¶àÁü•ËØÜÁßØÁ¥ØÔºàNKAÔºâÊú∫Âà∂ÔºåÈÄöËøáËá™ÈÄÇÂ∫îÂä®ÈáèÊõ¥Êñ∞Á≠ñÁï•Âä®ÊÄÅË∞ÉÊï¥Êñ∞ÊóßÁü•ËØÜÊùÉÈáçÔºåÊèêÂçáÊ®°ÂûãÁ®≥ÂÆöÊÄß„ÄÇ\\n> *   **ÊïàÊûúÔºö** Âú®ÂÖ≠‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÔºåPEARLÁöÑÂπ≥ÂùáÂáÜÁ°ÆÁéáÊèêÂçá2.23%ÔºåÊúÄÁªàÂáÜÁ°ÆÁéáÊèêÂçá1.65%ÔºåËææÂà∞ÊúÄÂÖàËøõÊÄßËÉΩ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   PEARLÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÈÄöËøáËæìÂÖ•Êó†ÂÖ≥ÁöÑÂÖ®Â±ÄÊèêÁ§∫ÂíåË¥üÂèçÈ¶àË∞ÉËäÇÔºåÂÆûÁé∞Áü•ËØÜÁöÑÁ®≥ÂÆöÁßØÁ¥Ø„ÄÇÂÖ∂ËÆæËÆ°Âì≤Â≠¶ÊòØÂ∞ÜÊèêÁ§∫Â≠¶‰π†Âª∫Ê®°‰∏∫Â∫èÂàóÈóÆÈ¢òÔºåÂà©Áî®Â¢ûÈáèÂ≠¶‰π†ÁöÑÊó∂Èó¥Âä®ÊÄÅÊÄßÊçïÊçâ‰ªªÂä°Áõ∏ÂÖ≥ÊÄß„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** Áé∞ÊúâÊñπÊ≥ïÔºàÂ¶ÇL2P„ÄÅDualPromptÔºâ‰æùËµñËæìÂÖ•‰ø°ÊÅØÔºåÂØºËá¥Áü•ËØÜÊ∑∑ÂêàÊ∑∑‰π±ÔºåÈöæ‰ª•‰øùÊåÅ‰ªªÂä°ÁâπÂºÇÊÄß„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** PEARLÈÄöËøáSPAÊ®°ÂùóÁîüÊàêÂÖ®Â±ÄÊèêÁ§∫ÔºåÈÅøÂÖçËæìÂÖ•‰æùËµñÔºõÈÄöËøáNKAÊú∫Âà∂Âä®ÊÄÅË∞ÉÊï¥Êñ∞ÊóßÁü•ËØÜÊùÉÈáçÔºåÁ°Æ‰øùÁü•ËØÜÁ®≥ÂÆöÁßØÁ¥Ø„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.   **SPAÊ®°ÂùóÔºö** ‰ΩøÁî®TransformerÊû∂ÊûÑÁöÑÊèêÁ§∫ÁºñÁ†ÅÂô®ÁîüÊàêÂÖ®Â±ÄÊèêÁ§∫ÔºåÈÄöËøáÂàÜÊÆµ‰ΩçÁΩÆÁºñÁ†ÅÔºàSPEÔºâ‰øùÊåÅ‰ªªÂä°ÁâπÂºÇÊÄß„ÄÇ\\n> 2.   **NKAÊú∫Âà∂Ôºö** ËÆ°ÁÆóÂΩìÂâç‰∏éÂÖàÂâç‰ªªÂä°ÁöÑËæìÂá∫Â∑ÆÂºÇÔºàMAEÔºâÔºåÂä®ÊÄÅË∞ÉÊï¥Âä®ÈáèÊùÉÈáçÔºàŒ±ÔºâÔºåÂÆûÁé∞Ë¥üÂèçÈ¶àË∞ÉËäÇ„ÄÇ\\n> 3.   **Áü•ËØÜÁßØÁ¥ØÔºö** ÈÄöËøáÂä®ÈáèÊõ¥Êñ∞ÂÖ¨ÂºèÔºàEq. 3ÔºâÊ∑∑ÂêàÊñ∞ÊóßÊèêÁ§∫ÔºåÁ°Æ‰øùÁü•ËØÜÁ®≥ÂÆöÁßØÁ¥Ø„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   L2P„ÄÅDualPrompt„ÄÅCODA-PromptÔºàÊèêÁ§∫ÊñπÊ≥ïÔºâÔºõSimpleCIL„ÄÅADAM„ÄÅRanPACÔºàË°®Á§∫ÊñπÊ≥ïÔºâÔºõHiDe-Prompt„ÄÅESNÔºàÊ®°ÂûãÊ∑∑ÂêàÊñπÊ≥ïÔºâ„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®Âπ≥ÂùáÂáÜÁ°ÆÁéáÔºà¬ØAÔºâ‰∏äÔºö** PEARLÂú®CIFAR-100‰∏äËææÂà∞93.64%ÔºåÊòæËëó‰ºò‰∫éL2PÔºà85.94%ÔºâÂíåDualPromptÔºà87.87%Ôºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øRanPACÔºà93.51%ÔºâÁõ∏ÊØîÔºåÊèêÂçá0.13‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®ÊúÄÁªàÂáÜÁ°ÆÁéáÔºàA^NÔºâ‰∏äÔºö** PEARLÂú®VTAB‰∏äËææÂà∞93.02%ÔºåËøúÈ´ò‰∫éCODA-PromptÔºà83.02%ÔºâÂíåEASEÔºà93.55%ÔºâÔºå‰∏éRanPACÔºà91.83%ÔºâÁõ∏ÂΩì‰ΩÜÊõ¥Á®≥ÂÆö„ÄÇ\\n> *   **Âú®‰ªªÂä°Áõ∏ÂÖ≥ÊÄßÊçïÊçâ‰∏äÔºö** PEARLÈÄöËøáNKAÊú∫Âà∂Ëá™Âä®Êî∂ÊïõÂà∞‰ªªÂä°Áõ∏ÂÖ≥ÊÄßÁ≥ªÊï∞Ôºå‰ºò‰∫éÂõ∫ÂÆöÂä®ÈáèÊùÉÈáçÁöÑÂü∫Á∫øÔºàÂ¶ÇŒ±=0.9Êó∂ÊÄßËÉΩ‰∏ãÈôçÊòæËëóÔºâ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   Á±ªÂ¢ûÈáèÂ≠¶‰π† (Class-Incremental Learning, CIL)\\n*   ÁÅæÈöæÊÄßÈÅóÂøò (Catastrophic Forgetting, CF)\\n*   ÊèêÁ§∫Â≠¶‰π† (Prompt Learning, N/A)\\n*   Ë¥üÂèçÈ¶àË∞ÉËäÇ (Negative Feedback Regulation, NFR)\\n*   È¢ÑËÆ≠ÁªÉÊ®°Âûã (Pre-Trained Model, PTM)\\n*   Ëá™ÈÄÇÂ∫îÂä®ÈáèÊõ¥Êñ∞ (Adaptive Momentum Update, AMU)\\n*   ÂàÜÊÆµ‰ΩçÁΩÆÁºñÁ†Å (Segmented Positional Encoding, SPE)\\n*   Áü•ËØÜÂÆπÂô® (Knowledge Container, N/A)\"\n}\n```"
}