{
    "source": "Semantic Scholar",
    "arxiv_id": "2405.02576",
    "link": "https://arxiv.org/abs/2405.02576",
    "pdf_link": "https://arxiv.org/pdf/2405.02576.pdf",
    "title": "CTD4 -- A Deep Continuous Distributional Actor-Critic Agent with a Kalman Fusion of Multiple Critics",
    "authors": [
        "David Valencia",
        "Henry Williams",
        "Trevor Gee",
        "Bruce A MacDonaland",
        "Minas Liarokapis"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "2024-05-04",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 2,
    "influential_citation_count": 0,
    "institutions": [
        "The University of Auckland",
        "New Dexterity Lab"
    ],
    "paper_content": "# CTD4 - a Deep Continuous Distributional Actor-Critic Agent with a Kalman Fusion of Multiple Critics\n\nDavid Valencia1, Henry Williams1, Yuning Xing1, Trevor Gee1, Bruce A MacDonald1, Minas Liarokapis 2\n\n1Centre for Automation and Robotic Engineering Science, The University of Auckland, Auckland, New Zealand 2New Dexterity Lab, Auckland, New Zealand dval035, yxin683 @aucklanduni.ac.nz, henry.williams, t.gee, b.macdonald, minas.liarokapis $@$ auckland.ac.nz\n\n# Abstract\n\nCategorical Distributional Reinforcement Learning (CDRL) has demonstrated superior sample efficiency in learning complex tasks compared to conventional Reinforcement Learning (RL) approaches. However, the practical application of CDRL is encumbered by challenging projection steps, detailed parameter tuning, and domain knowledge. This paper addresses these challenges by introducing a pioneering Continuous Distributional Model-Free RL algorithm tailored for continuous action spaces. The proposed algorithm simplifies the implementation of distributional RL, adopting an actorcritic architecture wherein the critic outputs a continuous probability distribution. Additionally, we propose an ensemble of multiple critics fused through a Kalman fusion mechanism to mitigate overestimation bias. Through a series of experiments, we validate that our proposed method provides a sample-efficient solution for executing complex continuouscontrol tasks.\n\n# Introduction\n\nThe real world has a stochastic nature characterized by different levels of uncertainty that are complex to model or predict with deterministic systems. Thus, the adoption of stochastic strategies becomes imperative to obtain accurate information on environmental dynamics. This necessity is particularly important in RL, where an intelligent agent is tasked with learning effective behaviours in the presence of noisy, stochastic environments. In addressing this challenge, Distributional RL emerges as a viable solution, offering a framework to navigate the stochastic nature of the world effectively, an aspect that traditional RL methods struggle to handle directly.\n\nCurrently, most of the state-of-the-art RL algorithms use the expectations of returns $Q ^ { \\pi }$ (as a scalar value) for each action-state pair following a policy $\\pi$ (Fujimoto, Hoof, and Meger 2018; Schulman et al. 2017; Haarnoja et al. 2018). RL requires an accurate approximation of the Q-function to guarantee sample efficiency and stability. An adequate approximation is essential to calculate the Temporal Difference (TD) error or action selection, whether in policybased or value-based approaches (Kuznetsov et al. 2020).\n\nTherefore, better and more stable approximations of the Qfunction are needed.\n\nDistributional RL presents the capability to learn a better approximation of the Q-function as a complete distribution as well as the intrinsic behaviours and randomness associated with the environment and the policy. By learning a distribution of the returns rather than its expectations, policies can be learned more efficiently and achieve higher performance (Bellemare, Dabney, and Munos 2017; Choi, Lee, and Oh 2019).\n\nThe works of (Lyle, Bellemare, and Castro 2019), (Rowland et al. 2018), (Bellemare, Dabney, and Munos 2017; Choi, Lee, and Oh 2019) are examples of related literature that use distributions in RL for discrete control problems, where the output of the Q-function approximator is a categorical distribution. This type of distribution is usually highly dependent on several hyperparameters, such as the number of categories or bounding values. Furthermore, to get acceptable performance and reduce the approximation error, task-specific knowledge with additional steps such as projections, truncations, or auxiliary predictions are needed (Bellemare, Dabney, and Munos 2017). Although the results may outperform traditional deterministic RL approaches, computing and training on categorical distributions are often complex.\n\nLikewise, either in traditional RL or distributional RL, the overestimation bias is one of the primary obstacles to accurate policy learning. When estimating a Q-function, an overestimation is inevitable due to the imperfect nature of the estimator. That is, the approximation may be significantly higher than the actual value. Prior studies addressed this problem through an ensemble of various Q-approximators in multiple ways.\n\nFor example, (Fujimoto, Hoof, and Meger 2018) and (Lan et al. 2020) address overestimation in control problems by utilizing minimum values from multiple network approximations. Similarly, (Kuznetsov et al. 2020) and (Chen et al. 2021) enhance Q estimation stability through ensemble learning and averaging techniques.\n\nWe argue that taking the minimum value among the $Q$ approximations is not the best option since the opposite effect of overestimation could happen, i.e. underestimation, ending up in discouragement of exploration (Lan et al. 2020). Additionally, using the minimum value among the approximations makes the ensemble lose its significance since the other values are ignored. Taking the average among the approximations is not optimal either, especially when working with distributions of returns. Weighting all of the distributions the same when they are not equally good could result in sub-optimal or incorrect approximations.\n\nIn this work, to mitigate the issues of CDRL and overestimation bias, we present a novel Continuous Distributional Reinforcement Learning algorithm called Continuous Twin Delayed Distributed Deep Deterministic Policy Gradient CTD4. We extend and transform TD3, from a deterministic actor-critic method to a continuous distributional actorcritic approach. This transformation preserves the simplicity of the original implementation while incorporating the advantages of ensemble distributions to enhance the learning of an RL control policy for continuous action problems.\n\nWe propose using a normal distribution to parameterize the approximation of the return distribution. With the normal distribution described by its mean $\\mu$ and standard deviation $\\sigma$ , the disjoint support issues of categorical distributions and the high dependency on task-specific hyperparameters can be avoided. In the same way, no customized loss/distance metrics are needed. Furthermore, to alleviate the overestimation, we propose an ensemble of the distributional Q-approximators fused through a Kalman method, preserving the power of the ensemble without the need for complex tuning parameters. The paper contributions are:\n\n1. Introduce a novel continuous distributional Actor-Critic RL framework tailored for continuous action domains. This framework streamlines prior distributional RL methods by eliminating the reliance on categorical representations less suited for continuous spaces while concurrently improving sample efficiency.   \n2. Mitigation of overestimation bias through the utilization of an ensemble of continuous distributed critics.   \n3. Fuse the ensemble of distributional critics efficiently through a Kalman fusion approach, maximizing the collective impact of the ensemble.\n\n# Related Work\n\nLearning a complete distribution of returns for each action $a$ and state $s$ pair instead of its mean $Q ^ { \\pi } ( s , a )$ has been studied and proven to be an effective solution that significantly improves the performance of RL agents (Bellemare, Dabney, and Rowland 2023; Lyle, Bellemare, and Castro 2019; Rowland et al. 2018; Bellemare, Dabney, and Munos 2017). After the introduction of C51 by (Bellemare, Dabney, and Munos 2017), considered the first distributional RL algorithm, several studies have been done, and research on distributional RL emerged.\n\nC51 proposes to learn a categorical probability distribution of the returns for each action (for a discrete action space), where the distribution range and the number of finite categories are empirically selected. A concern with C51 is minimizing the distance (viewed as a cross-entropy loss) between the categorical predicted distribution and the categorical target distribution, but this is impractical due to the disjoint of the categories. The authors solve this by projecting the Bellman update, which aligns the distributions.\n\nUsing C51 as a baseline, QR-DQN uses quantile regression to automatically transpose and adjust the distributionsâ€™ locations (Dabney et al. 2018b). This allows the minimisation of the Wasserstein distance to the target distribution. QR-DQN is extended by including an Implicit Quantile Network (IQN) in (Dabney et al. 2018a). The authors propose to learn the full quantile values through the network. Therefore, the return distribution is controlled by the size of the IQN. In this work, the output is not a distribution as C51 or QR-DQN; it re-parameterises samples from a base distribution to the respective quantile values of a target distribution.\n\nTo avoid the categorical distributionâ€™s disjoint problem (Choi, Lee, and Oh 2019) propose MoG-DQN an RL method using a mixture of Gaussians to parameterize the value distribution. The authors also present a customised distance metric since the cross-entropy between mixtures could be analytically impractical. MoG-DQN solves the distributionâ€™s disjoint problem. However, employing a mixture of Gaussians could be computationally intense, where selecting the right number of Gaussians is an additional handtuned parameter where the parameterisation is often numerically unstable.\n\nThe previously mentioned works can operate in discrete action spaces only and were tested predominantly on Atari games. Even when the results presented show a substantial improvement (against DQN (Mnih et al. 2013) mostly), the acceptable performance is restricted to task/domain-specific configurations, hyperparameters auxiliary projection functions or extra networks.\n\nDistributional RL has also been presented for continuous control problems. D4PG (Barth-Maron et al. 2018) where the authors use a categorical distributional critic based on C51, along with a prioritizing experience replay buffer and an N-step reward discount sum. Another categorical distributed RL method capable of working with continuous control problems is TQC, presented by (Kuznetsov et al. 2020). The authors present an actor-multiple-critics method using C51 and QR-DQN as a baseline. An ensemble of critics is used to alleviate the overestimation in the predicted distributions. Each critic produces a truncated category distribution, followed by dropping several atoms. The average between the ensemble is taken to generate the final approximation distribution. A comparable approach using distributions is presented in (Duan et al. 2021). The authors introduce an extension of SAC called DSAC, along with TD4, a variation of TD3. Both approaches utilize continuous distributions to mitigate overestimation biases. DSAC models the Q-values as distributions using Soft Policy Improvement. However, the paper still derives a single Q-value estimate, which remains the primary source of overestimation. Additionally, the authors employ the PABAL architecture, which parallelized training by incorporating four learners, which is computationally demanding.\n\nDespite the promising results demonstrated in these works, recent literature on distributional RL lacks alternatives tailored for continuous control problems. Previous proposals are computationally expensive with complex and slow training steps and/or are still based on categorical distributions (Kuznetsov et al. 2020). CDRL encompasses issues such as projection steps, customized loss/metric functions, or alternative techniques characterized by a significant reliance on tuning parameters. We tackle the majority of these issues, presenting a novel, easy-to-train, scalable, and stable method that is able to solve control tasks in continuous action space. As far as we are aware, we present the first distributional RL algorithm that uses continuous distributions in an actor-critics architecture for continuous control tasks addressing overestimation through a fusion of an ensemble of multiple distributed critics using a Kalman approach.\n\n![](images/4b498a14d06d49597e64972de459b205ea58e772e810d46b451e466946bac668.jpg)  \nFigure 1: Categorical Distribution Computing Problem. The top part represents the sequence of the target distribution being shrunk and shifted by the discount factor $\\gamma$ and Reward $R$ , respectively. Then, it is projected to the original support through probabilities inversely proportional to the distance from the nearest support. The bottom part describes the current distribution used to calculate the distance error.   \nFigure 2: Diagram of the proposed continuous distributional method. The CTD4 architecture network consists of an actor network and $N$ critic networks. Each critic network comprising the ensemble consists of two hidden fully connected layers with 256 nodes, each with ReLU as an activation function. A linear layer is added for each output layer, i.e., the mean $\\mu$ and standard deviation $\\sigma$ . A softplus activation function is also included for $\\sigma$ to guarantee positive values. The actor has Tanh as an activation function for the output layer along with two hidden fully connected layers with 256 nodes, each with ReLU.\n\n# Background\n\nIn standard RL, the Q-function is described as a scalar value, i.e. the expected return $\\begin{array} { r } { Q ^ { \\pi } ( s , a ) : = E [ \\sum _ { t = 0 } ^ { \\infty } \\gamma ^ { t } R ( s _ { t } , a _ { t } ) ] } \\end{array}$ . However, the expectation is removed from the return equation in distributional RL. That is, distributional RL focuses on learning the full distribution of the random variable $\\begin{array} { r } { Z ^ { \\pi } ( s , a ) : = \\sum _ { t = 0 } ^ { \\infty } \\gamma ^ { t } R ( s _ { t } , a _ { t } ) } \\end{array}$ . Therefore, the distributed $Z ^ { \\pi }$ function akes the central role and replaces the scalar $Q ^ { \\pi }$ function. The Bellman equation for distributional RL is presented as $Z ^ { \\pi } ( s , a ) \\overset { D } { = } R ( s , a ) + \\gamma Z ^ { \\pi } ( s ^ { \\prime } , a ^ { \\prime } )$ . What primarily differentiates distributional RL from standard RL is the loss function. As with the scalar setting, the TD error can be estimated as the difference between the estimation and target values. However, in the case of distributional RL, the TD error is calculated as the distance between the current estimation distribution and the target distribution, presented as the cross-entropy term of the Kullback-Leibler (KL) divergence $\\delta = D _ { K L } [ Z ^ { \\pi } ( s , a ) - Y ( s ^ { \\prime } , a ^ { \\prime } ) ]$ where the target distribution is defined as $Y ( s ^ { \\prime } , a ^ { \\prime } ) = R + \\gamma Z ^ { \\pi } ( s ^ { \\prime } , a ^ { \\prime } )$ .\n\nIn CDRL, the process of computing the loss function is not straightforward. $Z ^ { \\pi } ( s , a )$ and $Y ( \\bar { s } ^ { \\prime } , a ^ { \\prime } )$ have disjoint supports since the target distribution supports are modified by $\\bar { R } + \\gamma Z ^ { \\pi } ( s ^ { \\prime } , a ^ { \\prime } )$ . That is, the discount factor $\\gamma$ shrinks the distribution, while the reward $R$ shifts it. Therefore, the minimization of the KL divergence is not always directly possi\n\nritic Ensemble Main Critics Random sp ä¸­ â†’U DPGUpdate Decreasing 0 P TDeraor Deresing Actor Net F u Kalman 2 0 ã€‹ Mini Bateg ble, needing a projecting or approximation step to match the target supports onto the current prediction supports, see Figure 1.\n\n# TD3\n\nThe Twin Delayed Deep Deterministic (TD3) approach introduced by (Fujimoto, Hoof, and Meger 2018) is an actorcritic algorithm built on the DDPG (Barth-Maron et al. 2018) for continuous control problems. TD3 not only improves the results with respect to DDPG but also reduces the sensitivity to the hyperparameters. The key improvement of TD3 is how the overestimation is mitigated. TD3 learns two (scalar) $\\mathrm { \\Delta Q }$ -function approximations instead of one, as DDPG does, but it uses the minimum value of the two Qapproximators, $m i n ( Q _ { \\theta 1 } , Q _ { \\theta _ { 2 } } )$ , to calculate the Bellman error loss function. This process leans toward the underestimation of Q values. That is, $T D = ( Y - Q _ { \\theta } ( s , a ) )$ where $Y \\ = \\ R + \\gamma m i n _ { i = 1 , 2 } Q _ { \\theta _ { i } } ( s ^ { \\prime } , a ^ { \\prime } )$ . Inspired by Double $\\mathrm { Q }$ - learning (Van Hasselt, Guez, and Silver 2016), TD3 also includes target networks (for both actors and critics) to promote stability during policy training. Further, TD3 updates the actor network (i.e., the policy) less frequently than the two critic networks (i.e. Q-function approximators). This allows the critic networks to become more stable and reduce errors before it is used to update the actor network. The target networks are soft-updated and step-delayed with respect to the critics.\n\nThe policy $\\pi$ is learned using the deterministic policy gradient by maximizing one of the Q approximators. It is unclear why the authors use just the $Q _ { \\theta 1 }$ , completely ignoring the other $\\mathrm { \\Delta Q }$ approximator. A random noise is also included in the target action. This makes the policy avoid exploiting actions with high Q-value estimates. In the same way, to encourage exploration during the training, random noise is included in the environmentâ€™s actions following the policy.\n\n# CTD4\n\nWe present CTD4 that builds upon TD3 and follows the same training dynamics. However, in CTD4, the critic network structure is modified to output the $\\mu$ and $\\sigma$ that parameterize the normal distribution of the $Z$ approximation, making it a continuous distributional RL algorithm. See Figure 2 for a full graphical representation of the proposed architecture.\n\nTo improve the Z estimation and, consequently, the policy, an ensemble of $N$ critics are trained to mitigate the overestimation issue. CTD4 distinguishes itself from comparable approaches (Choi, Lee, and Oh 2019; Kuznetsov et al. 2020), which integrate approximations through either averaging or selecting the minimum value among them. In contrast, we employ a distinctive method by fusing the ensemble of critics through a Kalman filter (Kalman 1960). This integration process yields the output parameters $\\mu _ { k }$ and $\\sigma _ { k }$ , that parameterize the normal distribution of the $Z ^ { \\pi }$ approximation. More specifically, we propose to train the critics $Z _ { \\theta n }$ for $n \\in [ 1 . . \\bar { N } ]$ of the policy-conditioned return distribution $Z ^ { \\pi }$ where each critic $Z _ { \\theta n }$ maps the current $( s , a )$ to a continuous distribution parameterize by $\\mu _ { n }$ and $\\sigma _ { n }$ and the fusion of the critics is calculated by:\n\n$$\n\\begin{array} { c } { { k = \\displaystyle \\frac { \\sigma _ { n } ^ { 2 } } { \\left( \\sigma _ { n } ^ { 2 } + \\sigma _ { n + 1 } ^ { 2 } \\right) } } } \\\\ { { \\sigma _ { k } ^ { 2 } = \\left( 1 - k \\right) \\sigma _ { n } ^ { 2 } + k \\sigma _ { n + 1 } ^ { 2 } } } \\\\ { { \\mu _ { k } = \\mu _ { n } + k ( \\mu _ { n + 1 } - \\mu _ { n } ) } } \\end{array}\n$$\n\nTo minimize the TD error, characterized as the discrepancy between distributions in distributional RL, and facilitate the training of the approximators $Z _ { \\theta n }$ for $n \\in [ 1 . . N ]$ , it is imperative to formulate the Bellman target function as a normal distribution. Specifically, we express the target distribution as $Z _ { \\mathrm { t a r g e t } } = R \\bar { + } \\gamma Z ( \\bar { \\mu _ { k } , \\sigma _ { k } } )$ . Since we are assuming a Gaussian normal distribution $X \\sim { \\mathcal { N } } ( \\mu , \\sigma )$ , the linear Gaussian transformation rule $Y = a X + b$ can be applied; therefore the target distribution can be defined as:\n\n$$\n\\begin{array} { r l } & { Z _ { t a r g e t } = R + \\gamma ~ Z ( \\mu _ { k } , \\sigma _ { k } ) } \\\\ & { \\mu _ { t a r g e t } \\to \\gamma \\mu _ { k } + R } \\\\ & { \\sigma _ { t a r g e t } \\to \\gamma \\sigma _ { k } } \\\\ & { Z _ { t a r g e t } = \\mathcal N ( \\mu _ { t a r g e t } , \\sigma _ { t a r g e t } ) } \\end{array}\n$$\n\nA crucial aspect to note for CTD4 is the utilization of continuous distributions instead of categorical distributions. Consequently, the support of the distribution extends along a real continuous line. This characteristic enables us to engage directly with the target distributions, eliminating the necessity for any projection, truncation, or transformation steps. See Figure 3 for a visual representation. The objective of the loss function is to minimize the distance metric between the current approximation distribution and the Bellman $Z _ { \\mathrm { t a r g e t } }$ distribution. To quantify the dissimilarity between these distributions, we employ the KL divergence. This parallels the conventional RL framework, where the loss function is designed to minimize the TD error. Consequently, our approach directly minimizes the KL distance\n\na) b) c) Target Distribution Z(s',a) y Z(s',a\") R +y Z(s',a') Z(s,a) Z(s,a) Z(s,a)\n\nbetween each $Z _ { \\theta n }$ for $n \\in [ 1 . . N ]$ and the Bellman $Z _ { \\mathrm { t a r g e t } }$ distribution as:\n\n$$\n\\begin{array} { l } { T \\mathrm { \\cal D } _ { e r r o r } = \\mathrm { \\cal D } _ { K L } \\big ( Z _ { c u r r e n t } , Z _ { t a r g e t } \\big ) } \\\\ { T \\mathrm { \\cal D } _ { e r r o r } = \\mathrm { \\cal D } _ { K L } \\big ( Z ( \\mu _ { i } , \\sigma _ { i } ) , ( R + \\gamma ~ Z ( \\mu _ { k } , \\sigma _ { k } ) ) } \\\\ { T \\mathrm { \\cal D } _ { e r r o r } = \\mathrm { \\cal D } _ { K L } \\big ( \\mathcal { N } ( \\mu _ { i } , \\sigma _ { i } ) , \\mathcal { N } ( \\mu _ { t a r g e t } , \\sigma _ { t a r g e t } ) \\big ) } \\end{array}\n$$\n\nIt is essential to highlight that despite the introduction of the ensemble-based architecture, a deterministic policy actor $\\pi ( s )$ remains used. In other words, the actor network produces a deterministic scalar value vector as its output. Specifically, the actor network, characterized by parameters $\\phi$ , takes the current state $s$ as input and generates the corresponding action to maximise the long-term reward. The Deterministic Policy Gradient (Silver et al. 2014) is employed for updating the actor parameters at each training step to maximize the expected discounted reward. Therefore, when considering a sample mini-batch of $M$ transitions for the fusion of the ensemble:\n\n$$\n\\nabla _ { \\phi } J ( \\phi ) \\approx \\frac { 1 } { M } \\sum _ { i = 1 } ^ { M } \\nabla _ { a } \\mu _ { k } \\nabla _ { \\phi } \\pi _ { \\phi } ( s _ { t } )\n$$\n\nTo encourage exploration, TD3 disturbs the actions selected by the policy through the incorporation of fixed stochastic random noise at each training step $a \\backsim \\pi _ { \\phi } ( s ) + \\varepsilon$ . Incorporating this idea may prove essential during the preliminary phases, we assert that its continued application may pose a detriment to policy optimization in the final stages of training, particularly in tasks demanding heightened precision. Consequently, we propose a decrease in the magnitude of random noise exploration added to the action. This entails a progressive attenuation of the noise level, denoted as $\\varepsilon \\backsim \\bar { \\mathcal { N } } ( 0 , \\bar { \\sigma } )$ , at each time step. This strategic adjustment aims to encourage a more stable learning policy, minimizing disturbances.\n\nThe same concept as TD3 is followed for the target networks, wherein we periodically update the target parameters\n\nCHEETAH RUN CARTPOLE SWINGUP 600 800 885358333   \n0 WNAGOWAN 700 e500   \n9300 9   \n8400 Kalman Fusion A300 Kalman Fusion 100 Minimum Value Minimum Value 200 AverageValue Average Value 100 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Steps 1e6 Steps 1e6 REACHERHARD CARTPOLESWINGUP 800 800   \n  \n1 GR 200 N=3 N=5 200 N=3 N=5 100 N=2 N=4 N=10 100 N=2 N=4 N=10 0.0 0.2 Steps 0.8 16 0.0 0.2 0.4 0.6 0.8 1.0 Steps 1e6\n\n$\\theta$ and $\\phi$ employing the most recent critic and actor parameter values, respectively. In alignment with the idea of decreasing random noise, we extend this concept to the target actions too.\n\n# Optimal Fusion Strategy: Why a Kalman Fusion?\n\nUtilizing an ensemble of approximations has been a recurring strategy in deterministic and categorical RL methods to address overestimation issues inherent in $Q$ approximations. Common practices involve selecting the minimum value among Q-Values (Fujimoto, Hoof, and Meger 2018; Lan et al. 2020) or averaging all the approximations (Kuznetsov et al. 2020; Chen et al. 2021) as methods to process the output of the ensemble critics. A clear example of this is REDQ which averages the predictions from the ensemble. However, REDQ is limited to deterministic setups and cannot handle distributional updates, whether categorical or continuous. Moreover, its high update rate $( \\mathrm { G } { = } 2 0 ) ,$ ) significantly escalates computational costs and risks overfitting without diverse training data. Additionally, although employing an ensemble approach, REDQ only employs two critics randomly selected from the ensemble.We argue that these approaches have limitations. Opting for the minimum value prioritizes a singular value, underutilizing the potential power of the ensemble. On the other hand, taking the average assigns equal significance to all approximations, irrespective of their potential sub-optimally or inaccuracy.\n\nNotably, in our proposed framework, the output of each critic is a normal distribution parameterized by $\\mu$ and $\\sigma$ . This perspective allows us to interpret each approximation as a sensor reading, enabling the application of sensor fusion methods. To enhance Q-value estimation and fully leverage the collective strength of the ensemble, we employ a Kalman filter to fuse multiple distribution approximations. The Kalman fusion identifies the most probable approximation within the ensemble by calculating the joint probability of the distributions. This strategic fusion optimises $\\mathrm { Q }$ -value estimates and endows the RL agent with resilience to noise, errors, or inaccuracies inherent in the individual approximations. It is crucial to emphasize that we do not keep track of all the previous approximations; we only fuse them in each training step.\n\nUnder identical conditions, only the ensemble fusion method is altered among Kalman fusion, minimum value fusion, and average fusion. The agent is then trained for a fixed number of steps. The results for two complex tasks from the DMCS (Cheetah run and Cartpole Swingup) are displayed in Figure 4. Each fusion method for each task was trained using five different seeds. The results indicate that the proposed method, employing Kalman fusion, exhibits greater stability and achieves higher rewards with fewer samples compared to other fusion methods.\n\n# Ensemble Size Optimization: Determining the Optimal Number of Critics\n\nSelecting an appropriate number of approximations is crucial for ensuring stability throughout the training process. Previous related work, such as TD3, does not explain the number of approximators chosen in the ensemble and their effect on the training.\n\nWe conducted an analysis to comprehend the influence of ensemble size. Increasing the number of critics can significantly mitigate overestimation, leading to a potential enhancement in overall performance. In other words, a higher number of approximations corresponds to a more pronounced alleviation of overestimation (Lan et al. 2020; Kuznetsov et al. 2020). Nevertheless, if the number of critics becomes excessively high, there is an elevated risk of instability, potentially leading to underestimation, discouraging exploration, and, consequently, yielding a suboptimal policy (Lan et al. 2020). Additionally, the computational cost experiences a substantial increase with the growing number of critics in the ensemble. To determine the correct ensemble size, an agent is trained under the same conditions, using a Kalman fusion for the approximation but varying only the number of approximators. The analysis results, conducted across five different seeds for two complex unrelated tasks (Reacher Hard and Cartpole Swingup), are illustrated in Figure 5. Suboptimal performance may occur if the number of critics is too high (for instance, $N = 1 0$ ). Based on these results, an ensemble $N = 3$ was selected.\n\n# Experiments\n\nOur experimental testing aims to evaluate the sample efficiency and ease of training of Continuous Distributional approximators for continuous action spaces. We conduct comparative analyses against TD3 and REDQ, using the official source-code implementations provided by the authors. REDQ is selected for its similarity to using an ensemble method, aligning with the approach taken in this article. TD3 is chosen for its well-established dominance across a range of algorithms designed for continuous action spaces.\n\nWe purposefully avoid applying or comparing our method with previous approaches, such as C51 or IQN, which are tailored for discrete action spaces. Our research squarely focuses on addressing the challenges posed by complex tasks characterized by continuous action spacesâ€”a domain notorious for its heightened difficulty level. Additionally, we do not compare our proposal against TD4 and DSAC, as they rely on parallelized learning. Instead, our focus is on improving learning efficiency in a single-agent setup, which is more practical and resource-efficient.\n\nTo conduct experiments with our proposed method, we select the challenging continuous control tasks from the DeepMind Control Suite (DMCS) (Tassa et al. 2018). We choose ten environments, each presenting distinct levels of complexity and task requirements. The default reward signal provided for each environment ensures consistency and comparability across evaluations. The experiments are conducted over a training duration of $1 \\times 1 0 ^ { 6 }$ steps for each environment, employing identical hyperparameters for five independently seeded runs. We regularly evaluate the agentâ€™s performance in each environment during the training; after every $1 \\times 1 0 ^ { 4 }$ training steps, we compute the average reward over 10 consecutive episodes. It is important to note that no gradient updates are executed during the evaluation phase. Notably, all model parameters are optimized using the Adam optimizer, with a single gradient update executed per environment step. The training process was completed on a PC with an i9 CPU, 128GB of RAM, and a GeForce RTX4090 NVIDIA GPU with Ubuntu 20.04.\n\nTo facilitate the replication of our work, we provide all raw data plots corresponding to each independent seed, a comprehensive list of hyperparameters, a training loop, the entire source code, and task demonstration videos. This supplementary material is available on the paperâ€™s website at: https://sites.google.com/view/ctd4.\n\n# Results and Discussion\n\nFigure 6 shows the average evaluation reward across multiple seeds, comparing the performance of CTD4, TD3, and REDQ under identical conditions.\n\nUsing a continuous distribution approach parametrized by the mean and standard deviation, coupled with the fusion of multiple critics through a Kalman method, is effective in achieving comparable or enhanced performance across diverse tasks. The superiority of our proposed methodology becomes evident, particularly in complex scenarios where TD3 and REDQ struggled to solve tasks or exhibit patterns of task completion.\n\nIt is crucial to note that we deliberately selected exceptionally challenging tasks to assess the performance of our proposal. Specific tasks, such as Reacher Hard, Finger Spin Hard, or Ball-in-Cup, pose increased difficulty due to their sparse reward structures, adding complexity to task-solving.\n\nConversely, tasks like Walker Walk and Humanoid Run demand intricate coordination among numerous joints, making them inherently complex. The Acrobot Swingup task necessitates balance and precise control, while the Fish Swim task involves navigating a 3D environment, introducing an additional layer of complexity to the learning process. The results affirm that the stochastic nature of CTD4 can discover superior solutions with higher rewards. This becomes particularly evident in environments such as Hopper Hop or Acrobot Swingup, where the performance curves demonstrate that our proposal can outperform TD3 and REDQ without the need for complex hyperparameter tuning or intricate projection steps as required by other distributed RL methods.\n\n# Conclusions\n\nCDRL has demonstrated efficacy and enhanced the performance of traditional RL. Nonetheless, its practical implementation is complex, marked by numerous computations and tuning steps. A noteworthy limitation in the existing literature is the inability of several CDRL approaches to address complex tasks within continuous action spaces effectively. In this paper, we set out to investigate the use of continuous distributions as a mechanism for improving and simplifying the use of distribution in RL. Our proposed solution involves leveraging continuous distributions, specifically a normal distribution parameterized by mean and standard deviation. This approach addresses and mitigates the majority of the problems associated with CDRL, facilitating a more straightforward implementation process. Importantly, our method bypasses the need for custom loss functions, as we minimize the KL divergence between two distributions, a reasonably manageable loss metric to minimize.\n\nOur study additionally addresses the prevalent issue of overestimation bias in actor-critic RL methods. To tackle this challenge, we propose utilizing an ensemble of multiple approximations (critics) integrated through a Kalman approach. Our findings reveal that the Kalman fusion effectively mitigates overestimation, surpassing the limitations associated with conventional fusion methods, including average fusion or minimum value selection. Likewise, we introduce a noise decay approach combined with the actions generated by the policy. Our results illustrate that the random noise decay approach effectively promotes exploration in the early stages, and the gradual reduction ensures that it does not affect learning in the concluding stages of the training. This idea can greatly improve performance, especially in environments that need high precision. Finally, our modifications and contributions are straightforward to integrate and implement across other actor-critic algorithms transitioning from deterministic to distributional RL.\n\nFuture work will focus on deploying this implementation in real-world scenarios that demand a stochastic nature and involve complex situations, such as robot bipedals or dexterous hand manipulations. Further analysis is also crucial, exploring diverse continuous probability distributions and their impact on policy, opening the door for future research in the realm of continuous distributions RL.\n\n![](images/6d59269c5f7facfceec5678292e73438e7b7d33e258a98803d958d4d88ff8cf2.jpg)  \nFigure 6: Learning curves for CTD4, REDQ and TD3 on ten DeepMind Control Suite continuous control tasks. In each plot, the solid line represents the average, while the shaded region represents the standard deviation of the average evaluation over five independent seeds.",
    "summary": "```json\n{\n  \"core_summary\": \"### ğŸ¯ æ ¸å¿ƒæ¦‚è¦\\n\\n> **é—®é¢˜å®šä¹‰ (Problem Definition)**\\n> *   è®ºæ–‡è§£å†³äº†ä¼ ç»Ÿåˆ†ç±»åˆ†å¸ƒå¼ºåŒ–å­¦ä¹ ï¼ˆCDRLï¼‰åœ¨è¿ç»­åŠ¨ä½œç©ºé—´ä¸­åº”ç”¨æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤æ‚çš„æŠ•å½±æ­¥éª¤ã€è¯¦ç»†çš„å‚æ•°è°ƒæ•´å’Œé¢†åŸŸçŸ¥è¯†ä¾èµ–ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†CDRLåœ¨å®é™…ä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨ã€‚\\n> *   è¯¥é—®é¢˜çš„é‡è¦æ€§åœ¨äºï¼Œè¿ç»­åŠ¨ä½œç©ºé—´çš„ä»»åŠ¡ï¼ˆå¦‚æœºå™¨äººæ§åˆ¶ï¼‰åœ¨ç°å®ä¸–ç•Œä¸­éå¸¸æ™®éï¼Œè€Œç°æœ‰çš„CDRLæ–¹æ³•éš¾ä»¥é«˜æ•ˆå¤„ç†è¿™äº›ä»»åŠ¡ã€‚\\n\\n> **æ–¹æ³•æ¦‚è¿° (Method Overview)**\\n> *   è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCTD4çš„æ–°å‹è¿ç»­åˆ†å¸ƒå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé‡‡ç”¨æ¼”å‘˜-è¯„è®ºå®¶æ¶æ„ï¼Œå…¶ä¸­è¯„è®ºå®¶è¾“å‡ºè¿ç»­æ¦‚ç‡åˆ†å¸ƒï¼ˆæ­£æ€åˆ†å¸ƒï¼‰ï¼Œå¹¶é€šè¿‡å¡å°”æ›¼èåˆæœºåˆ¶é›†æˆå¤šä¸ªè¯„è®ºå®¶ä»¥å‡è½»é«˜ä¼°åå·®ã€‚\\n\\n> **ä¸»è¦è´¡çŒ®ä¸æ•ˆæœ (Contributions & Results)**\\n> *   **è´¡çŒ®1ï¼š** æå‡ºäº†ä¸€ç§é’ˆå¯¹è¿ç»­åŠ¨ä½œç©ºé—´çš„è¿ç»­åˆ†å¸ƒå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç®€åŒ–äº†åˆ†å¸ƒå¼ºåŒ–å­¦ä¹ çš„å®ç°ï¼Œé¿å…äº†åˆ†ç±»åˆ†å¸ƒçš„ä¾èµ–ã€‚\\n> *   **è´¡çŒ®2ï¼š** é€šè¿‡é›†æˆå¤šä¸ªåˆ†å¸ƒè¯„è®ºå®¶å¹¶ä½¿ç”¨å¡å°”æ›¼èåˆæœºåˆ¶ï¼Œæœ‰æ•ˆå‡è½»äº†é«˜ä¼°åå·®ã€‚\\n> *   **è´¡çŒ®3ï¼š** åœ¨å¤šä¸ªå¤æ‚è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­éªŒè¯äº†æ–¹æ³•çš„æ ·æœ¬æ•ˆç‡å’Œæ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹TD3å’ŒREDQã€‚ä¾‹å¦‚ï¼Œåœ¨Cheetah Runä»»åŠ¡ä¸­ï¼ŒCTD4çš„å¹³å‡å›æŠ¥è¾¾åˆ°äº†600ï¼Œæ˜¾è‘—ä¼˜äºTD3ï¼ˆ500ï¼‰å’ŒREDQï¼ˆ400ï¼‰ã€‚\",\n  \"algorithm_details\": \"### âš™ï¸ ç®—æ³•/æ–¹æ¡ˆè¯¦è§£\\n\\n> **æ ¸å¿ƒæ€æƒ³ (Core Idea)**\\n> *   CTD4çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†ä¼ ç»Ÿçš„ç¡®å®šæ€§Qå‡½æ•°æ›¿æ¢ä¸ºè¿ç»­åˆ†å¸ƒZå‡½æ•°ï¼Œé€šè¿‡æ­£æ€åˆ†å¸ƒå‚æ•°åŒ–ï¼ˆå‡å€¼å’Œæ ‡å‡†å·®ï¼‰æ¥å»ºæ¨¡è¿”å›å€¼çš„åˆ†å¸ƒã€‚è¿™ç§æ–¹æ³•é¿å…äº†åˆ†ç±»åˆ†å¸ƒçš„ä¸è¿ç»­æ”¯æŒé—®é¢˜ï¼Œç®€åŒ–äº†å®ç°ã€‚\\n> *   é€šè¿‡å¡å°”æ›¼èåˆæœºåˆ¶é›†æˆå¤šä¸ªè¯„è®ºå®¶ï¼Œå……åˆ†åˆ©ç”¨é›†æˆå­¦ä¹ çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶é¿å…ç®€å•å¹³å‡æˆ–æœ€å°å€¼é€‰æ‹©å¸¦æ¥çš„é—®é¢˜ã€‚\\n\\n> **åˆ›æ–°ç‚¹ (Innovations)**\\n> *   **ä¸å…ˆå‰å·¥ä½œçš„å¯¹æ¯”ï¼š** å…ˆå‰çš„å·¥ä½œï¼ˆå¦‚C51ã€QR-DQNï¼‰ä¸»è¦é’ˆå¯¹ç¦»æ•£åŠ¨ä½œç©ºé—´ï¼Œä¸”ä¾èµ–åˆ†ç±»åˆ†å¸ƒï¼Œéœ€è¦å¤æ‚çš„æŠ•å½±æ­¥éª¤å’Œè¶…å‚æ•°è°ƒæ•´ã€‚\\n> *   **æœ¬æ–‡çš„æ”¹è¿›ï¼š** CTD4é‡‡ç”¨è¿ç»­åˆ†å¸ƒï¼ˆæ­£æ€åˆ†å¸ƒï¼‰é¿å…äº†è¿™äº›é—®é¢˜ï¼Œå¹¶é€šè¿‡å¡å°”æ›¼èåˆæœºåˆ¶ä¼˜åŒ–äº†é›†æˆè¯„è®ºå®¶çš„è¾“å‡ºï¼Œæé«˜äº†ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚\\n\\n> **å…·ä½“å®ç°æ­¥éª¤ (Implementation Steps)**\\n> *   **æ­¥éª¤1ï¼š** æ„å»ºæ¼”å‘˜-è¯„è®ºå®¶æ¶æ„ï¼Œå…¶ä¸­è¯„è®ºå®¶ç½‘ç»œè¾“å‡ºæ­£æ€åˆ†å¸ƒçš„å‚æ•°ï¼ˆå‡å€¼å’Œæ ‡å‡†å·®ï¼‰ã€‚\\n> *   **æ­¥éª¤2ï¼š** è®­ç»ƒå¤šä¸ªè¯„è®ºå®¶ç½‘ç»œï¼Œæ¯ä¸ªç½‘ç»œç‹¬ç«‹è¾“å‡ºåˆ†å¸ƒå‚æ•°ã€‚\\n> *   **æ­¥éª¤3ï¼š** ä½¿ç”¨å¡å°”æ›¼èåˆæœºåˆ¶é›†æˆå¤šä¸ªè¯„è®ºå®¶çš„è¾“å‡ºï¼Œè®¡ç®—èåˆåçš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚\\n> *   **æ­¥éª¤4ï¼š** é€šè¿‡KLæ•£åº¦æœ€å°åŒ–å½“å‰åˆ†å¸ƒä¸ç›®æ ‡åˆ†å¸ƒï¼ˆè´å°”æ›¼æ›´æ–°åçš„åˆ†å¸ƒï¼‰ä¹‹é—´çš„è·ç¦»ã€‚\\n> *   **æ­¥éª¤5ï¼š** æ¼”å‘˜ç½‘ç»œé€šè¿‡ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦æ›´æ–°ï¼Œæœ€å¤§åŒ–é•¿æœŸå¥–åŠ±ã€‚\\n\\n> **æ¡ˆä¾‹è§£æ (Case Study)**\\n> *   è®ºæ–‡æœªæ˜ç¡®æä¾›æ­¤éƒ¨åˆ†ä¿¡æ¯ã€‚\",\n  \"comparative_analysis\": \"### ğŸ“Š å¯¹æ¯”å®éªŒåˆ†æ\\n\\n> **åŸºçº¿æ¨¡å‹ (Baselines)**\\n> *   TD3ï¼šä¸€ç§åŸºäºç¡®å®šæ€§æ¼”å‘˜-è¯„è®ºå®¶çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚\\n> *   REDQï¼šä¸€ç§ä½¿ç”¨é›†æˆè¯„è®ºå®¶çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚\\n\\n> **æ€§èƒ½å¯¹æ¯” (Performance Comparison)**\\n> *   **åœ¨å¹³å‡å›æŠ¥ä¸Šï¼š** CTD4åœ¨å¤šä¸ªDeepMind Control Suiteä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºTD3å’ŒREDQã€‚ä¾‹å¦‚ï¼Œåœ¨Cheetah Runä»»åŠ¡ä¸­ï¼ŒCTD4çš„å¹³å‡å›æŠ¥è¾¾åˆ°äº†600ï¼Œè€ŒTD3å’ŒREDQåˆ†åˆ«ä¸º500å’Œ400ã€‚\\n> *   **åœ¨è®­ç»ƒç¨³å®šæ€§ä¸Šï¼š** CTD4é€šè¿‡Kalmanèåˆæœºåˆ¶ï¼Œæ˜¾è‘—å‡å°‘äº†é«˜ä¼°åå·®ï¼Œæå‡äº†è®­ç»ƒçš„ç¨³å®šæ€§ã€‚\\n> *   **åœ¨æ ·æœ¬æ•ˆç‡ä¸Šï¼š** CTD4åœ¨ç›¸åŒè®­ç»ƒæ­¥æ•°ä¸‹ï¼Œèƒ½å¤Ÿæ›´å¿«åœ°æ”¶æ•›åˆ°æ›´é«˜çš„å›æŠ¥å€¼ï¼Œè¡¨ç°å‡ºæ›´å¥½çš„æ ·æœ¬æ•ˆç‡ã€‚\",\n  \"keywords\": \"### ğŸ”‘ å…³é”®è¯\\n\\n*   å¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL)\\n*   è¿ç»­åˆ†å¸ƒå¼ºåŒ–å­¦ä¹  (Continuous Distributional Reinforcement Learning, CDRL)\\n*   æ¼”å‘˜-è¯„è®ºå®¶æ¶æ„ (Actor-Critic Architecture, N/A)\\n*   å¡å°”æ›¼èåˆ (Kalman Fusion, N/A)\\n*   é«˜ä¼°åå·® (Overestimation Bias, N/A)\\n*   è¿ç»­åŠ¨ä½œç©ºé—´ (Continuous Action Space, N/A)\\n*   æ­£æ€åˆ†å¸ƒ (Normal Distribution, N/A)\\n*   æ ·æœ¬æ•ˆç‡ (Sample Efficiency, N/A)\"\n}\n```"
}