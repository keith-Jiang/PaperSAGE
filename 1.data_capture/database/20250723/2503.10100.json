{
    "source": "Semantic Scholar",
    "arxiv_id": "2503.10100",
    "link": "https://arxiv.org/abs/2503.10100",
    "pdf_link": "https://arxiv.org/pdf/2503.10100.pdf",
    "title": "SOLA-GCL: Subgraph-Oriented Learnable Augmentation Method for Graph Contrastive Learning",
    "authors": [
        "Tianhao Peng",
        "Xuhong Li",
        "Haitao Yuan",
        "Yuchen Li",
        "Haoyi Xiong"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2025-03-13",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 2,
    "influential_citation_count": 0,
    "institutions": [
        "Beihang University",
        "Baidu Inc.",
        "Nanyang Technological University",
        "Shanghai Jiao Tong University"
    ],
    "paper_content": "# SOLA-GCL: Subgraph-Oriented Learnable Augmentation Method for Graph Contrastive Learning\n\nTianhao Peng1, Xuhong $\\mathbf { L i } ^ { 2 }$ , Haitao Yuan3âˆ—, Yuchen $\\mathbf { L i } ^ { 2 , 4 }$ , Haoyi Xiong2\n\n1Beihang University 2Baidu Inc. 3Nanyang Technological University 4Shanghai Jiao Tong University pengtianhao $@$ buaa.edu.cn, lixuhong $@$ baidu.com, yuchenli $@$ sjtu.edu.cn haitao.yuan $@$ ntu.edu.sg, haoyi.xiong.fr@ieee.org\n\n# Abstract\n\nGraph contrastive learning has emerged as a powerful technique for learning graph representations that are robust and discriminative. However, traditional approaches often neglect the critical role of subgraph structures, particularly the intra-subgraph characteristics and inter-subgraph relationships, which are crucial for generating informative and diverse contrastive pairs. These subgraph features are crucial as they vary significantly across different graph types, such as social networks where they represent communities, and biochemical networks where they symbolize molecular interactions. To address this issue, our work proposes a novel subgraph-oriented learnable augmentation method for graph contrastive learning, termed SOLA-GCL, that centers around subgraphs, taking full advantage of the subgraph information for data augmentation. Specifically, SOLA-GCL initially partitions a graph into multiple densely connected subgraphs based on their intrinsic properties. To preserve and enhance the unique characteristics inherent to subgraphs, a graph view generator optimizes augmentation strategies for each subgraph, thereby generating tailored views for graph contrastive learning. This generator uses a combination of intra-subgraph and inter-subgraph augmentation strategies, including node dropping, feature masking, intra-edge perturbation, inter-edge perturbation, and subgraph swapping. Extensive experiments have been conducted on various graph learning applications, ranging from social networks to molecules, under semi-supervised learning, unsupervised learning, and transfer learning settings to demonstrate the superiority of our proposed approach.\n\n# Introduction\n\nGraph structures are used to represent data such as social networks (Zhou et al. 2020), molecules (Li et al. 2023b; Zhang et al. 2023b), and traffic flows (Yuan and Li 2021; Yuan et al. 2023). Graph neural networks (GNNs) (Kipf and Welling 2017; Velickovic et al. 2018; Xu et al. 2019; Yun et al. 2019; Li et al. 2022) have become popular for graph representation learning (Lin et al. 2023; Tang et al. 2023; Li et al. 2023f; Cai et al. 2023; Yuan et al. 2021; Li et al. 2025), but they require labeled data. To learn from unlabeled data, contrastive learning has been adapted to graph data (You et al. 2020, 2021;\n\nVelickovic et al. 2019; Suresh et al. 2021; Yin et al. 2022). This involves generating two views by perturbing the graph and learning representations by maximizing feature consistency (You et al. 2020, 2021; Velickovic et al. 2019; Li et al. 2023c). Graph-structured data are more complex than image or text data due to their properties and distribution shifts (Hassani 2022; Li et al. 2023e; Yuan, Cong, and Li 2024). Methods like DGI (Velickovic et al. 2019), GRACE (Zhu et al. 2020), and GraphCL (You et al. 2020) use random augmentations, but lack automatic selection of augmentation policies, affecting the graphsâ€™ semantic integrity.\n\nRecent advances in Graph Contrastive Learning (GCL) leverage methods like JOAO (You et al. 2021) and LG2AR (Hassani and Ahmadi 2022) adaptively select one augmentation strategy from a predefined pool for specific graph data, followed by random graph view generation process such as node masking and edge dropping. While these methods improve the adaptability of selecting augmentation strategies for individual graphs, their reliance on random view generation may change the original graph semantics. Despite some advancements with end-to-end methods like AD-GCL and AutoGCL, which introduce learnable augmentations, the full potential of subgraph information is still largely unexplored. Methods like MSSGCL (Liu et al. 2023) and SUBG-CON (Jiao et al. 2020) attempt to address this by focusing on the relationships between sampled subgraphs and the entire graph or the central nodes, respectively. However, they still struggle to capture the intra-subgraph characteristics and inter-subgraph relationships (e.g., communities in social networks). Furthermore, while these learnable methods manage to preserve the semantics of original graphs, they lack adaptability across different datasets due to their reliance on uniform augmentation strategies (You et al. 2021, 2020; Li et al. 2024; Lyu et al. 2024), which ultimately limits their flexibility. Here, we conclude the limitations of existing methods as follows.\n\nâ€¢ Loss of intra-subgraph and inter-subgraph information: Graphs contain important details within subgraphs (intrasubgraph characteristics) and between different subgraphs (inter-subgraph relationships). For instance, in social networks, distinct communities exhibit unique characteristics, while in chemistry, combinations of functional groups determine molecular functionalities. Previous studies have not effectively captured the critical information both within and between subgraphs. How to effectively capture both intra-subgraph characteristics and intersubgraph relationships remains an ongoing challenge.\n\nâ€¢ Poor adaptability and losing semantic information: Different graph structures necessitate distinct augmentation strategies. For example, molecular graphs may require edge perturbation, whereas social community graphs might benefit more from node dropping (You et al. 2021, 2020). Methods like JOAO, which lack a learnable view generation process, can offer improved adaptability but at the cost of losing semantic information. Conversely, methods with uniform augmentation strategies that include a learnable view generation process, such as AD-GCL, preserve semantic information but often fail to adapt effectively across diverse datasets. Improving the adaptability of GCL methods while preserving semantic integrity remains a significant challenge.\n\nTo address above challenges, We propose a novel Subgraph-Oriented Learnable Augmentation method for Graph Contrastive Learning (SOLA-GCL) to tackle existing challenges by leveraging subgraph information for data augmentation. The process begins by dividing the original graph into connected subgraphs using partitioning algorithms, such as the Louvain method (Blondel et al. 2008) for community graphs or RDKit (Landrum 2013) for molecular structures, to identify functional groups. A graph view generator then applies multiple augmentation strategies (node dropping, edge perturbation, subgraph swapping) to these subgraphs, assembling them into a new graph view. This involves a subgraph augmentation selector, which learns the optimal augmentation strategy distribution; a subgraph view generator for implementing these strategies; and a subgraph view assembler. Our method demonstrated superior performance in extensive graph classification experiments across various learning tasks, outperforming state-of-the-art graph contrastive learning approaches.\n\nThe main contributions of this study are as follows:\n\nâ€¢ A novel graph contrastive learning framework, termed SOLA-GCL, is proposed. To the best of our knowledge, this is the first work to build learnable generative augmentation policies that specifically focus on the intrasubgraph characteristics and inter-subgraph relationships within graphs.   \nâ€¢ The proposed SOLA-GCL introduces an end-to-end differentiable training algorithm, enabling automatic augmentation strategy selection and graph view generation.   \nâ€¢ Extensive experiments are conducted on a variety of graph classification datasets with semi-supervised, unsupervised, and transfer learning settings, showcasing the robustness and effectiveness of our SOLA-GCL framework on graph classification tasks.\n\n# Related Work\n\nIn this section, we first introduce the studies relevant to our work from the perspectives of graph neural networks, graph partition algorithms, and graph contrastive learning (GCL) algorithms upon views generated by various graph data augmentation strategies. Later, we discuss the unique contributions made by this work compared to previous studies.\n\n# Graph Neural Networks\n\nGraph neural networks (GNNs) are the extension of the neural network models onto graph data (Wu et al. 2021; Li et al. 2023d; Zhang et al. 2023a; Yuan, Li, and Bao 2022; Xiong et al. 2024). Most existing GNNs adopt the messagepassing framework and use permutation-invariant local aggregation schemes to update node representations. For instance, GCN (Kipf and Welling 2017) averages features of all neighboring nodes. GAT (Velickovic et al. 2018) uses an attention mechanism to assign different weights to neighboring nodes. GraphSAGE (Hamilton, Ying, and Leskovec 2017) samples fixed-size neighbors of a node and aggregates their features for realizing fast and scalable GNN training. GIN (Xu et al. 2019) adjusts the weight of the central node by a learnable parameter to distinguish different graph structures based on the graph embedding. ResGCN (Pei et al. 2022) combine the residual connection with GCN to build deeper GNNs.\n\nIn this study, we employ two state-of-the-art GNNs, GIN (Xu et al. 2019) and ResGCN (Pei et al. 2022), as our backbone GNNs, following the existing graph contrastive learning literature (You et al. 2020, 2021; Yin et al. 2022). We believe our work could complement with the line of research on GNN while incorporating advanced GNN models for potential improvement in future studies.\n\n# Graph Partition Algorithm\n\nGraph partitioning divides a graph into communities based on node and edge connectivity. Densely connected nodes form communities, while sparsely connected nodes do not. Spectral clustering approximates graph partition solutions using eigenvalue decomposition on the normalized graph Laplacian, but it is computationally expensive. The Louvain method (Blondel et al. 2008) quickly optimizes modularity, a measure of intra-community edge density relative to inter-community edges. The Girvanâ€“Newman (GN) algorithm (Girvan and Newman 2002) identifies communities by removing edges iteratively. However, these methods often ignore node features, despite their richness in recent applications. SGCN (Wang et al. 2021) addresses this by detecting community centers without prior labels. RDKit is a software suite for cheminformatics, computational chemistry, and predictive modeling, which offers graph partition algorithms for molecular graphs (Landrum 2013). In our work, we use the Louvain method (Blondel et al. 2008) and RDKit (Landrum 2013) for graph partitioning before GCL training, but our framework is adaptable to other partition algorithms, including deep learning-based approaches.\n\n# GCL with Data Augmentation\n\nIn recent years, GCL with data augmentation has attracted significant attention for self-supervised graph learning. The main idea is to maximize the agreement between representations of a graph in augmented views (Li et al. 2023a).\n\n![](images/309f531441a102385ac673ef26d00d3910b40efc201659b30ea65b36d6d205d2.jpg)  \nFigure 1: An illustration of the proposed SOLA-GCL framework. The graph view generator is composed of three critical components: the subgraph augmentation selector, the subgraph view generator, and the subgraph view assembler. The subgraph augmentation selector learns to choose the optimal augmentation strategy for each subgraph, and the subgraph view generator outputs augmented subgraph views according to the selected strategies. The subgraph view assembler constructs an augmented graph view based on these augmented subgraph views.\n\nRecently, GRACE (Zhu et al. 2020) introduces two general types of augmentations (edge perturbation and attribute masking), while GraphCL (You et al. 2020) proposes four (node dropping, edge perturbation, attribute masking, and subgraph sampling), applying these strategies randomly, which limits task adaptability. To address this, JOAO (You et al. 2021) and GPA (Zhang et al. 2022) adaptively select suitable augmentation strategies by learning the distribution of graph datasets. However, these methods often apply random augmentations to specific graphs, resulting in sub-optimal performance. To enhance adaptability in GCL models, AD-GCL (Suresh et al. 2021) uses adversarial training to perturb edges and generate graph views, while AutoGCL (Yin et al. 2022) focuses on perturbing nodes for augmented views. Despite their success, these approaches often overlook the importance of subgraphs. Methods like SUBG-CON (Jiao et al. 2020) and MSSGCL (Liu et al. 2023) have explored subgraph sampling for contrastive learning by generating global and local views at different scales. However, they typically fail to leverage the intra-subgraph characteristics and inter-subgraph relationships fully. To address these limitations, we propose a subgraph-centered method that generates augmented subgraph views and assembles them into an augmented graph, utilizing subgraph information in a learnable manner.\n\n# Discussion on Most Relevant Works\n\nOur GCL framework introduces a learnable graph view generator that focuses on subgraphs to enhance graph understanding (Adhikari et al. 2018; Zhu et al. 2021a). The most relevant works to our study are JOAO and MSSGCL, from the perspectives of model adaptability and subgraph information modeling, respectively.\n\nFrom the perspective of model adaptability, both JOAO (You et al. 2021) and our SOLA-GCL learn the probability distribution of data augmentation strategies. While\n\nJOAO applies the random graph view generation process that risks altering the original graphâ€™s semantics, our SOLAGCL generates new graph views in a learnable manner, thus preserving the semantic integrity. From the perspective of subgraph information modeling, our proposed SOLA-GCL method differs significantly from MSSGCL (Liu et al. 2023), which focuses on the relationship between the original graph and its sampled subgraph. In contrast, SOLA-GCL method delves deeper, fully exploiting both the crucial intra-subgraph characteristics and inter-subgraph relationships by applying learnable targeted augmentation strategies within and between subgraphs.\n\n# Methodology\n\nThis section elaborates on the proposed SOLA-GCL framework for graph classification in detail. With an overview shown in Fig. 1, SOLA-GCL first partitions the graph into several densely connected subgraphs, and then trains the subgraph augmentation selector and subgraph view generator jointly in an end-to-end manner to generate graph views for graph contrastive learning.\n\n# The Basic GNN Module\n\nSOLA-GCL employs a vanilla GNN with $L$ local aggregation layers, enabling nodes to access information from $\\breve { L }$ -hop neighbors, as the basic module. Thus, for a given node $v \\in$ $\\nu$ , the $l$ -th layerâ€™s calculation formula in an $L$ -layer GNN $( l { = } 1 , 2 , . . . , L )$ is as follows:\n\n$$\nh _ { v } ^ { ( l ) } = \\mathrm { U P D A T E } ^ { ( l ) } \\left( h _ { v } ^ { ( l - 1 ) } , \\mathrm { A G G R E G A T E } ^ { ( l ) } \\left( \\left\\{ h _ { u } ^ { ( l - 1 ) } \\right\\} \\right) \\right)\n$$\n\nwhere $h _ { v } ^ { ( l ) }$ is the feature of a node $v$ in the $l$ th layer, and h(v =xv, xv is the original feature vector of node v;\n\n$\\mathrm { A G G R E G A T E } ^ { ( l ) } ( \\cdot )$ and $\\mathrm { U P D A T E } ^ { ( l ) } ( \\cdot )$ represent the feature aggregation function (e.g., mean, LSTM, and max pooling) and feature update function (e.g., linear-layer combination and MLP) (Ding et al. 2022), respectively; $\\mathcal { \\bar { N } } ( v )$ represents the one-hop neighboring node set of a node $v$ .\n\n# Learnable Graph View Generator\n\nGiven a graph $G { = } ( \\nu , \\mathcal { E } , X )$ , where $\\mathcal { V } { = } \\{ v _ { 1 } , v _ { 2 } , { \\ldots } , v _ { N } \\}$ denotes a node set, E âŠ† V Ã— V is an edge set, X âˆˆ RNÃ—d represents node features, the graph view generator aims to create an augmented view without semantic labels during training. With an overview shown in Fig. 1, the graph view generator is composed of three critical components: the subgraph augmentation selector, the subgraph view generator, and the subgraph view assembler. The subgraph augmentation selector aims to select the optimal augmentation strategy for each subgraph, then the subgraph view generator generates augmented views for subgraphs, and the subgraph view assembler generates an augmented graph view by combining the augmented subgraph views.\n\nSubgraph Augmentation Selector In studying different types of networks such as social and module graphs, it becomes crucial to extract and differentiate key information specific to each network type. To effectively implement GCL on various dataset-specific subgraphs, we utilize graph partition algorithms like the Louvain algorithm (Blondel et al. 2008), which partitions a graph into densely connected subgraphs. For a graph $G \\mathrm { = } ( \\nu , \\mathbf { \\bar { \\mathcal { E } } } , \\mathbf { \\bar { \\cal { X } } } )$ , the partition process can be formally expressed as follows: $\\{ \\mathbf { S _ { 1 } } , \\mathbf { S _ { 2 } } , . . . , \\mathbf { S _ { k } } \\} { = } \\mathrm { A L G O } ( G )$ , where $\\bf { S _ { i } }$ represents the $i$ -th subgraph partitioned by algorithm ALGO( ). Later, an $L$ -layer GNN processes node attributes to extract embeddings, which are then aggregated to form subgraph embeddings used for selecting appropriate augmentation strategies from a pool (node drop, feature mask, edge perturbation, subgraph swap) as detailed in . The Gumbel-Softmax technique (Jang, Gu, and Poole 2017) is employed to probabilistically assign augmentation operations to these partitions, enhancing the distinctiveness of the subgraph analysis.\n\nFor each subgraph S, the selection of the augmentation strategy can be formulated as follows:\n\n$$\n\\begin{array} { r l } & { p _ { \\mathbf { S } } = \\mathrm { F U N C } \\big ( \\mathrm { R E A D O U T } \\left( \\left\\{ h _ { v } ^ { ( L ) } : v \\in \\mathbf { S } \\right\\} \\right) \\big ) } \\\\ & { f _ { \\mathbf { S } } = \\mathrm { G u m b e l S o f t m a x } \\left( p _ { \\mathbf { S } } \\right) } \\end{array}\n$$\n\nwhere $h _ { v } ^ { ( L ) }$ denotes the embedding of node $v$ . READOUT denotes a function that summarizes over all node embeddings in subgraph $S$ . FUNC is a function (e.g. MLP or a linear layer) that transforms the embedding of the subgraph to the probability distribution $p _ { \\mathbf { S } }$ . This distribution represents the probabilities across all potential augmentations for each subgraph. $f _ { \\mathbf { S } }$ represents the augmentation choice for subgraph S. $f _ { \\mathbf { S } }$ is a differentiable one-hot vector sampled via GumbelSoftmax using the reparameterization trick.\n\nSubgraph View Generator For each subgraph, the subgraph view generator would select an augmentation strategy based on $f _ { \\mathbf { S } }$ , and generate a subgraph view for the subgraph.\n\nIn this study, we use five augmentation strategies, including three intra-subgraph strategies (node dropping, feature masking, and intra-edge perturbation) and two inter-subgraph strategies (inter-edge perturbation and subgraph swapping). The details are as follows.\n\n(1) Node dropping is conditioned on the node representations to decide which nodes within a subgraph to drop. Given the subgraph S, the process of node dropping can be formulated as:\n\n$$\n\\begin{array} { r } { p _ { v } = \\mathrm { F U N C } ( h _ { v } ^ { ( L ) } ) \\qquad } \\\\ { f _ { v } = \\mathrm { G u m b e l S o f t m a x } \\left( p _ { v } \\right) } \\\\ { \\tilde { X } _ { d r o p } , \\tilde { \\mathcal { E } } _ { d r o p } = \\mathrm { A U G } _ { d r o p } ( X , \\mathcal { E } , f _ { v } , f _ { \\mathbf { S } } ) } \\end{array}\n$$\n\nwhere FUNC is a function (e.g. MLP or a linear layer) that transforms the embeddings of nodes to the probability distribution $p _ { v }$ . This distribution indicates the likelihood of each node being dropped or retained. $f _ { v }$ is a one-hot vector sampled from this distribution via Gumbel-Softmax, $\\operatorname { A U G } _ { d r o p } ( X , \\mathcal { E } , f _ { v } , f _ { \\mathbf { S } } )$ is the augmentation function that outputs the augmented node features $\\tilde { X } _ { d r o p }$ and edges $\\tilde { \\mathcal { E } } _ { d r o p }$ . The underlying assumption is that missing part of nodes does not damage the semantic information of the graph.\n\n(2) Feature masking is to mask the feature of nodes within a subgraph. The process is similar to node dropping augmentation. Feature masking implies that the absence of some node features does not affect the semantics.\n\n(3) Intra-edge perturbation is conditioned on head and tail nodes to decide which edges that are within a subgraph to add or remove. It would be a heavy burden for backpropagation to predict the full adjacency matrix when dealing with large-scale graphs. To achieve efficient computation, we randomly sample negative edges within subgraphs. The underlying prior is that the semantic meaning of the graph is robust to the variance of edges.\n\n(4) Inter-edge perturbation focuses on perturbing the edges between two subgraphs $\\mathbf { S } _ { i }$ and $\\mathbf { S } _ { j }$ . Similarly, we first randomly sample $| \\mathcal { E } |$ negative edges between subgraphs, and the process of inter-edge perturbation can be formulated as:\n\n$$\n\\begin{array} { r l r } {  { p _ { e } = \\mathrm { F U N C } ( h _ { v } ^ { ( L ) } \\parallel h _ { u } ^ { ( L ) } \\parallel h _ { \\mathbf { S } _ { i } } ^ { ( L ) } \\parallel h _ { \\mathbf { S } _ { j } } ^ { ( L ) } : v \\in \\mathbf { S } _ { i } , u \\in \\mathbf { S } _ { j } ) } } \\\\ & { } & { \\quad \\quad ( \\mathrm { ~  ~ \\lambda ~ } _ { \\mathrm { ~ f ~ e ~ } } ) } \\\\ & { } & { \\quad \\quad \\tilde { \\mathcal { E } } _ { i n t e r } = \\mathrm { G u m b e l S o f t m a x } ( p _ { e } ) } \\\\ & { } & { \\quad \\quad \\tilde { \\mathcal { E } } _ { i n t e r } = \\mathrm { A U G } _ { i n t e r } ( \\mathcal { E } \\cup \\tilde { \\mathcal { E } } , f _ { e } , f _ { \\mathbf { S } } ) } \\end{array}\n$$\n\nwhere $h _ { { \\bf S } _ { i } } ^ { ( L ) }$ is the embedding of subgraph $\\mathbf { S } _ { i }$ . FUNC is a function (e.g. MLP or a linear layer) that transform the embeddings of edges to the probability distribution $p _ { e }$ . This distribution indicates the likelihood of each edge that are between the subgraph $\\mathbf { S } _ { i }$ and $\\mathbf { S } _ { j }$ being dropped or retained. $\\parallel$ denotes the concatenation operation, $f _ { e }$ is a one-hot vector sampled from this distribution via Gumbel-Softmax, and $\\operatorname { A U G } _ { i n t e r } ( \\mathcal { E } \\cup \\tilde { \\mathcal { E } } , f _ { e } , f _ { \\mathbf { S } } )$ is the augmentation function that outputs the augmented edge table $\\tilde { \\mathcal { E } } _ { i n t e r }$ . The inter-edge perturbation differs from the intra-edge perturbation in that it takes into account the subgraph embedding when calculating the edge probability distribution, as shown in Eq. (7).\n\n(5) Subgraph swapping is to swap the position of the subgraphs by changing the edges between subgraphs. The process of subgraph swapping can be formulated as:\n\n$$\n\\tilde { \\mathcal { E } } _ { s u b } = \\mathrm { A U G } _ { s u b } ( \\mathcal { E } , f _ { \\mathbf { S } } )\n$$\n\nwhere $\\mathrm { A U G } _ { s u b } ( \\mathcal { E } , f _ { \\bf S } )$ is the augmentation function that outputs the augmented edge table $\\tilde { \\mathcal { E } } _ { s u b }$ . It believes that most of the semantic meaning of the graph can be preserved in its local structure. The augmentation function $\\mathrm { { A \\bar { U } G ( \\cdot ) } }$ integrates the node attribute (and adjacency matrix) with the $f _ { v }$ (and $f _ { e }$ , the one-hot vector for edges sampled via Gumbel-Softmax) using differentiable operations such as multiplication. Consequently, the gradients of the weights of the subgraph view generator are retained in the augmented node features (edges) and can be computed using back-propagation.\n\nSubgraph View Assembler For a partitioned graph $\\{ \\mathbf { S } _ { 1 } , \\mathbf { S } _ { 2 } , . . . , \\mathbf { S } _ { k } \\}$ , we denote the augmented subgraph view of $\\mathbf { S } _ { i }$ as $\\tilde { \\bf S } _ { i } { = } ( \\tilde { X } _ { { \\bf S } _ { i } } , \\tilde { \\mathcal { E } } _ { { \\bf S } _ { i } } )$ . The augmented graph view $\\tilde { G } = ( \\tilde { X } , \\tilde { \\mathcal { E } } )$ can be obtained via $\\scriptstyle \\tilde { X } = \\operatorname { A S S E M B L E } _ { x } ( \\tilde { X } _ { \\mathbf { S } _ { i } } )$ and $\\begin{array} { r } { \\tilde { \\mathcal { E } } { = } \\sum _ { i = 1 } ^ { k } ( \\tilde { \\mathcal { E } } _ { \\mathbf { S } _ { i } } ) } \\end{array}$ , where $\\mathrm { A S S E M B L E } _ { x }$ is a matrix computation operation. For the augmented graph, the edge table and node features both participate in the gradient computation, and the parameters of the graph view generator can be updated in a differentiable manner. Therefore, our graph view generator is end-to-end differentiable.\n\n# Complexity Analysis\n\nIn SOLA-GCL, we use GIN as the graph embedding model, with a complexity of $O ( L { \\cdot } ( E _ { \\mathrm { a v g } } { \\cdot } C _ { \\mathrm { a g g } } { + } N _ { \\mathrm { a v g } } { \\cdot } C _ { \\mathrm { u p d a t e } } ) )$ , where $L$ is the number of layers, $E _ { \\mathrm { a v g } }$ is the average number of edges, $N _ { \\mathrm { a v g } }$ is the average number of nodes, $C _ { \\mathrm { a g g } }$ is the cost of aggregation, and $C _ { \\mathrm { u p d a t e } }$ is the cost of updating node features. For the subgraph augmentation selector, the complexity is approximated as $O ( ( N _ { \\mathrm { a v g } } + k _ { \\mathrm { a v g } } ) \\cdot H )$ , where $H$ is the feature dimension, and $k _ { \\mathrm { a v g } }$ is the average number of subgraphs. This includes an average pooling layer, a linear layer, and a Gumbel-Softmax operation. The subgraph view generator has a complexity of $O ( H \\cdot ( N _ { \\mathrm { a v g } } + E _ { \\mathrm { a v g } } ) )$ , involving a linear layer and a Gumbel-Softmax to generate augmented node features and edge tables. For the subgraph view assembler, the complexity is $O ( k _ { \\mathrm { a v g } } \\cdot N _ { \\mathrm { a v g } } \\cdot H + E _ { \\mathrm { a v g } } )$ , where assembling node features has a complexity of $O ( \\bar { k } _ { \\mathrm { a v g } } \\cdot N _ { \\mathrm { a v g } } \\cdot H )$ and assembling the edge table is $O ( E _ { \\mathrm { a v g } } )$ , summing up the edges across all subgraphs.\n\nIn summary, the overall complexity of the framework is $O ( L \\cdot ( E _ { \\mathrm { a v g } } \\cdot C _ { \\mathrm { a g g } } + N _ { \\mathrm { a v g } } \\cdot C _ { \\mathrm { u p d a t e } } ) + \\dot { k } _ { \\mathrm { a v g } } \\cdot N _ { \\mathrm { a v g } } \\cdot H + E _ { \\mathrm { a v g } } )$\n\n# Graph Contrastive Learning in SOLA-GCL\n\nIn this work, we define contrastive loss $\\mathcal { L } _ { \\mathrm { c l } }$ , similarity loss $\\mathcal { L } _ { \\mathrm { s i m } }$ , and classification loss $\\mathcal { L } _ { \\mathrm { c l s } }$ . The contrastive loss enforces maximizing the consistency between positive pairs $z _ { i } , z _ { j }$ compared with negative pairs. The similarity loss minimizes the mutual information between the views generated by the two view generators. The classification loss is used in the semi-supervised learning task to encourage the graph view generator to generate label-preserving augmentations.\n\nFor the contrastive loss, we follow the previous works (Chen et al. 2020; You et al. 2020; Yin et al. 2022) and use the normalized temperature-scaled cross entropy loss (NT-XEnt) (Sohn 2016). The cosine similarity function is defined as sim(zi, zj)= zi iÂ· jzj . During the training process, a data batch of $M$ graphs is randomly sampled and we pass the batch to the two graph view generators to obtain $2 M$ graph views. The two augmented views from the same input graph are regarded as the positive view pair. We denote $\\ell _ { ( i , j ) }$ as the instance-level contrastive loss between a positive pair of samples $( i , j )$ , and the contrastive loss of this data batch $\\mathcal { L } _ { \\mathrm { c l } }$ can be formulated as:\n\n$$\n\\begin{array} { l } { \\ell _ { ( i , j ) } = - \\log \\frac { \\exp ( \\sin ( z _ { i } , z _ { j } ) / \\tau ) } { \\sum _ { k = 1 , k \\neq i } ^ { 2 M } \\exp ( \\sin ( z _ { i } , z _ { k } ) / \\tau ) } } \\\\ { \\ell _ { \\mathrm { c l } } = \\frac { 1 } { 2 M } \\displaystyle \\sum _ { k = 1 } ^ { N } [ \\ell ( 2 k - 1 , 2 k ) + \\ell ( 2 k , 2 k - 1 ) ] } \\end{array}\n$$\n\nwhere $\\tau$ is the temperature parameter. The final loss is computed across all positive pairs per batch.\n\nFor the similarity loss, we simultaneously minimize the mutual information between the augmentation selections generated by the two subgraph augmentation selectors and between the views generated by the two subgraph view generators. During the view generation process, the subgraph augmentation selector outputs a sampled state matrix $S$ indicating the corresponding augmentation operation of each subgraph, and the subgraph view assembler outputs the edge table $\\vec { \\mathcal { E } }$ . As the augmentation process involves edge negative sampling, the length of edge table $\\overline { { \\mathcal { E } } }$ is not equal between the two view generators. We transform the edge table $\\overline { { \\mathcal { E } } }$ to adjacency matrix $A$ to compute the similarity loss between views. For a graph $G$ , we denote the sampled state matrix of each view generator as $S _ { 1 } , S _ { 2 }$ , and the adjacency matrix as $A _ { 1 } , A _ { 2 }$ . The similarity loss can be formulated as:\n\n$$\n\\mathcal { L } _ { \\mathrm { s i m } } = \\mathrm { s i m } ( S _ { 1 } , S _ { 2 } ) + \\mathrm { s i m } ( A _ { 1 } , A _ { 2 } )\n$$\n\nFor the classification loss, we employ the cross entropy loss $\\ell _ { c l s }$ . Give a graph sample $G$ with its corresponding class label $y$ , and its augmented views denoted as $\\tilde { G } _ { 1 }$ and $\\tilde { G _ { 2 } }$ , with CLS representing the classifier, the classification loss $\\ell _ { c l s }$ is expressed as follows:\n\n$$\n\\begin{array} { r } { \\mathcal { L } _ { \\mathrm { c l s } } = \\ell _ { c l s } ( \\mathbf { C L S } ( G ) , y ) + \\ell _ { c l s } ( \\mathbf { C L S } ( \\tilde { G } _ { 1 } ) , y ) + \\ell _ { c l s } ( \\mathbf { C L S } ( \\tilde { G } _ { 2 } ) , y ) } \\end{array}\n$$\n\n$\\mathcal { L } _ { \\mathrm { c l s } }$ is utilized in the pre-training phase of the semisupervised learning task to encourage the view generator to generate label-preserving augmentations.\n\n# Experiments\n\nIn this section, we assess the performance of SOLA-GCL on graph classification across various applications by comparing it against state-of-the-art (SOTA) methods in semi-supervised, unsupervised, and transfer learning tasks. We also explore the effectiveness of SOLA-GCL through visualizations of subgraph importance. Additionally, we conduct an ablation study to determine the impact of different components and provide analyses on running time to demonstrate its efficiency.\n\nTable 1: Comparison with the existing methods for unsupervised learning. Bold numbers indicate the best performance, while blue numbers highlight the second best.   \n\n<html><body><table><tr><td>Model</td><td>MUTAG</td><td>NCI1</td><td>COLLAB</td><td>IMDB-B</td><td>IMDB-M</td><td>Infections</td><td>Facebook</td><td>DBLP</td></tr><tr><td>GL</td><td>81.66Â±2.11</td><td></td><td></td><td>65.87Â±0.98</td><td></td><td></td><td></td><td></td></tr><tr><td>WL</td><td>80.72Â±3.00</td><td>80.01Â±0.50</td><td>-</td><td>72.30Â±3.44</td><td></td><td></td><td></td><td></td></tr><tr><td>DGK</td><td>87.44Â±2.72</td><td>80.31Â±0.46</td><td></td><td>66.96Â±0.56</td><td>=</td><td>=</td><td></td><td></td></tr><tr><td>node2vec</td><td>72.63Â±10.20</td><td>54.89Â±1.61</td><td>54.57Â±0.37</td><td>38.60Â±2.30</td><td></td><td></td><td>=</td><td></td></tr><tr><td>sub2vec</td><td>61.05Â±15.80</td><td>52.84Â±1.47</td><td>55.26Â±1.54</td><td>55.26Â±1.54</td><td></td><td></td><td></td><td></td></tr><tr><td>graph2vec</td><td>83.15Â±9.25</td><td>73.22Â±1.81</td><td>71.10Â±0.54</td><td>71.10Â±0.54</td><td></td><td>=</td><td>â– </td><td></td></tr><tr><td>InfoGraph</td><td>89.01Â±1.13</td><td>76.20Â±1.06</td><td>70.65Â±1.13</td><td>73.03Â±0.87</td><td>49.80Â±1.50</td><td>53.50Â±1.38</td><td>52.35Â±1.12</td><td>50.72Â±1.51</td></tr><tr><td>GraphCL</td><td>86.80Â±1.34</td><td>77.87Â±0.41</td><td>71.36Â±1.15</td><td>71.14Â±0.44</td><td>49.20Â±1.62</td><td>50.50Â±1.04</td><td>49.05Â±0.69</td><td>48.81Â±1.10</td></tr><tr><td>JOAO</td><td>87.35Â±1.02</td><td>78.07Â±0.47</td><td>69.50Â±0.36</td><td>70.21Â±3.08</td><td></td><td></td><td></td><td></td></tr><tr><td>JOAOv2</td><td>87.67Â±0.79</td><td>72.99Â±0.75</td><td>70.40Â±2.21</td><td>71.60Â±0.86</td><td>48.73Â±1.54</td><td>47.00Â±2.09</td><td>48.54Â±1.01</td><td>50.35Â±1.77</td></tr><tr><td>AD-GCL</td><td></td><td>69.67Â±0.51</td><td>73.32Â±0.61</td><td>71.57Â±1.01</td><td>49.87Â±1.44</td><td>52.50Â±1.08</td><td>52.66Â±1.67</td><td>48.21Â±1.93</td></tr><tr><td>AutoGCL</td><td>88.64Â±1.08</td><td>82.00Â±0.29</td><td>70.12Â±0.68</td><td>73.30Â±0.40</td><td>48.47Â±1.88</td><td>49.00Â±1.43</td><td>49.83Â±1.50</td><td>49.40Â±1.74</td></tr><tr><td>SimGRACE</td><td>89.01Â±1.31</td><td>79.12Â±0.44</td><td>71.72Â±0.82</td><td>71.30Â±0.77</td><td></td><td></td><td></td><td></td></tr><tr><td>MSSGCL</td><td>89.68Â±0.57</td><td>81.45Â±0.48</td><td>73.48Â±0.83</td><td>73.14Â±0.38</td><td></td><td></td><td></td><td></td></tr><tr><td>Ours</td><td>90.49Â±2.22</td><td>82.07Â±1.32</td><td>73.80Â±1.32</td><td>74.20Â±1.89</td><td>51.27Â±1.05</td><td>53.50Â±1.92</td><td>52.97Â±1.42</td><td>53.25Â±1.90</td></tr></table></body></html>\n\n<html><body><table><tr><td>Model</td><td>BBBP</td><td>ToxCast</td><td>SIDER</td><td>ClinTox</td><td>HIV</td><td>BACE</td></tr><tr><td>NoPretrain</td><td>65.8Â±4.5</td><td>63.4Â±0.6</td><td>57.3Â±1.6</td><td>58.0Â±4.4</td><td>75.3Â±1.9</td><td>70.1Â±5.4</td></tr><tr><td>Infomax</td><td>68.8Â±0.8</td><td>62.7Â±0.4</td><td>58.4Â±0.8</td><td>69.9Â±3.0</td><td>76.0Â±0.7</td><td>75.9Â±1.6</td></tr><tr><td>EdgePred</td><td>67.3Â±2.4</td><td>64.1Â±0.6</td><td>60.4Â±0.7</td><td>64.1Â±3.7</td><td>76.3Â±1.0</td><td>79.9Â±0.9</td></tr><tr><td>AttrMasking</td><td>64.3Â±2.8</td><td>64.2Â±0.5</td><td>61.0Â±0.7</td><td>71.8Â±4.1</td><td>77.2Â±1.1</td><td>79.3Â±1.6</td></tr><tr><td>ContextPred</td><td>68.0Â±2.0</td><td>63.9Â±0.6</td><td>60.9Â±0.6</td><td>65.9Â±3.8</td><td>77.3Â±1.0</td><td>79.6Â±1.2</td></tr><tr><td>GraphCL</td><td>69.68Â±0.67</td><td>62.40Â±0.57</td><td>60.53Â±0.88</td><td>75.99Â±2.65</td><td>78.47Â±1.22</td><td>75.38Â±1.44</td></tr><tr><td>JOAOv2</td><td>71.39Â±0.92</td><td>63.16Â±0.45</td><td>60.49Â±0.74</td><td>80.97Â±1.64</td><td>77.51Â±1.17</td><td>75.49Â±1.27</td></tr><tr><td>AD-GCL</td><td>70.01Â±1.07</td><td>63.07Â±0.72</td><td>63.28Â±0.79</td><td>79.78Â±3.52</td><td>78.28Â±0.97</td><td>78.51Â±0.80</td></tr><tr><td>AutoGCL</td><td>73.36Â±0.77</td><td>63.47Â±0.38</td><td>62.51Â±0.63</td><td>80.99Â±3.38</td><td>78.35Â±0.64</td><td>83.26Â±1.13</td></tr><tr><td>Ours</td><td>74.08Â±0.82</td><td>64.50Â±0.41</td><td>62.79Â±0.91</td><td>81.49Â±1.31</td><td>78.68Â±1.01</td><td>82.64Â±1.01</td></tr></table></body></html>\n\nTable 2: Comparison with the existing methods for transfer learning.\n\n# Experimental Settings\n\nWe compare SOLA-GCL with the SOTA methods on 16 datasets under unsupervised, semi-supervised and transfer learning settings. The baseline models are as follows: (1) three SOTA kernel-based methods: graphlet kernel (GL) (Shervashidze et al. 2009), Weisfeiler-Lehman subtree kernel (WL) (Shervashidze et al. 2011), and deep graph kernel (DGK) (Yanardag and Vishwanathan 2015); (2) six unsupervised graph-level representation learning methods: node2vec (Grover and Leskovec 2016), sub2vec (Adhikari et al. 2018), graph2vec (Narayanan et al. 2017), EdgePred (Hu et al. 2020), AttrMasking (Hu et al. 2020), and ContextPred (Hu et al. 2020); (3) six classic GCL methods that randomly generate graph views: InfoGraph (Sun et al. 2020), Infomax (Velickovic et al. 2019), GCA (Zhu et al. 2021b), GraphCL (You et al. 2020), JOAO (You et al. 2021), and JOAOv2 (You et al. 2021); (4) four learning-based GCL method: AD-GCL (Suresh et al. 2021), AutoGCL (Yin et al. 2022), SimGRACE (Xia et al. 2022), and MSSGCL (Liu et al. 2023).\n\n# Performance Comparison\n\nUnsupervised Representation Learning. As shown in Table 1, several observations can be made: (1) The consistent top performance across diverse datasets suggests that our method is not only versatile but also highly effective in extracting meaningful patterns from complex graph structures, surpassing both traditional graph learning methods and SOTA deep learning approaches. (2) Traditional methods like WL and DGK focus on graph kernels and rely on explicit feature engineering. They lack the adaptability of deep learning models, which can automatically extract features through training. (3) Deep learning methods like AD-GCL and AutoGCL show varying performance across datasets, illustrating the nuanced capabilities and limitations of these approaches. (4) MSSGCL can potentially capture nuanced relationships within the graph data by focusing on subgraph and graph features. In contrast, our approach leverages a wider spectrum of subgraph interactions, not merely focusing on local versus global relationships but also extensively exploring the intricate relationships within and between subgraphs. The superior performance across various datasets demonstrate the effectiveness of our SOLA-GCL.\n\nTransfer Learning. According to results shown in Table 2, three observations can be made: (1) Our model consistently achieves SOTA or second-best performance across all evaluated datasets, demonstrating its robustness and effectiveness under the transfer learning setting. (2) In the SIDER and BACE datasets, our model trails behind AD-GCL and AutoGCL, likely due to their more effective handling of sparse and imbalanced data typical in side effect prediction and biochemical field. (3) AD-GCL and AutoGCL often outperform other models like GraphCL and JOAO due to their advanced strategies in a learnable manner. In contrast, GraphCL and JOAO typically employ more generalized augmentation and optimization strategies, which may not effectively capture the nuanced details of the data during large-scale pre-training.\n\nSemi-Supervised Learning. For semi-supervised learning, we perform the semi-supervised graph classification exper\n\nTable 3: Comparison with existing methods and different strategies for semi-supervised learning.   \n\n<html><body><table><tr><td>Model</td><td>PROTEINS</td><td>DD</td><td>NCI1</td><td>COLLAB</td></tr><tr><td>FullData</td><td>78.25Â±1.61</td><td>80.73Â±3.78</td><td>83.65Â±1.16</td><td>83.44Â±0.77</td></tr><tr><td>10%Data</td><td>69.72Â±6.71</td><td>74.36Â±5.86</td><td>75.16Â±2.07</td><td>74.34Â±2.00</td></tr><tr><td>10% GAE</td><td>70.51Â±0.17</td><td>74.54Â±0.68</td><td>74.36Â±0.24</td><td>75.09Â±0.19</td></tr><tr><td>10% Infomax</td><td>72.27Â±0.40</td><td>75.78Â±0.34</td><td>74.86Â±0.26</td><td>73.76Â±0.29</td></tr><tr><td>10% ContextPred</td><td>70.23Â±0.63</td><td>74.66Â±0.51</td><td>73.00Â±0.30</td><td>73.69Â±0.37</td></tr><tr><td>10% GCA</td><td>73.85Â±5.56</td><td>76.74Â±4.09</td><td>68.73Â±2.36</td><td>74.32Â±2.30</td></tr><tr><td>10% GraphCL</td><td>74.21Â±4.50</td><td>76.65Â±5.12</td><td>73.16Â±2.90</td><td>75.50Â±2.15</td></tr><tr><td>10% JOAO</td><td>72.13Â±0.92</td><td>75.69Â±0.67</td><td>74.48Â±0.27</td><td>75.30Â±0.32</td></tr><tr><td>10% JOAOv2</td><td>73.31Â±0.48</td><td>75.81Â±0.73</td><td>74.86Â±0.39</td><td>75.53Â±0.18</td></tr><tr><td>10% AD-GCL</td><td>73.96Â±0.47</td><td>77.91Â±0.73</td><td>75.18Â±0.31</td><td>75.82Â±0.26</td></tr><tr><td>10% AutoGCL</td><td>75.65Â±2.40</td><td>77.50Â±4.41</td><td>73.75Â±2.25</td><td>77.16Â±1.48</td></tr><tr><td>10% SimGRACE</td><td>74.03Â±0.51</td><td>76.48Â±0.52</td><td>74.60Â±0.41</td><td>74.74Â±0.28</td></tr><tr><td>10% MSSGCL</td><td>75.76Â±0.52</td><td>78.89Â±0.18</td><td>74.77Â±0.31</td><td>76.02Â±0.13</td></tr><tr><td>10% Ours</td><td>77.63Â±3.10</td><td>79.43Â±4.31</td><td>74.94Â±1.96</td><td>76.97Â±1.31</td></tr></table></body></html>\n\n<html><body><table><tr><td></td><td>MUTAG</td><td>NCI1</td><td>COLLAB</td><td>IMDB-B</td></tr><tr><td>Louvain</td><td>90.49Â±2.22</td><td>82.07Â±1.32</td><td>73.80Â±1.32</td><td>74.20Â±1.89</td></tr><tr><td>GN</td><td>89.88Â±2.98</td><td>81.56Â±2.11</td><td>72.32Â±1.88</td><td>74.10Â±2.84</td></tr><tr><td>SOLA-GCL-P</td><td>88.33Â±2.04</td><td>79.93Â±1.74</td><td>70.92Â±1.91</td><td>71.70Â±2.80</td></tr><tr><td>SOLA-GCL-S</td><td>88.80Â±3.46</td><td>81.58Â±1.57</td><td>71.86Â±1.63</td><td>74.00Â±2.63</td></tr><tr><td>SOLA-GCL-R</td><td>88.80Â±3.24</td><td>80.95Â±1.75</td><td>69.38Â±1.96</td><td>73.70Â±2.84</td></tr></table></body></html>\n\nTable 4: Ablation study on the critical modules.\n\niments on TUDataset (Morris et al. 2020). As shown in Table 3, two observations can be made: (1) Our SOLA-GCL consistently achieves SOTA or comparative performance compared to other models due to its effective use of subgraphspecific information, enhancing learning from limited labeled data across various datasets. (2) MSSGCL ranks second on PROTEINS and DD because of its effective multi-scale subgraph sampling strategy. However, it falls short of SOLAGCL which better captures and utilizes subgraph interactions.\n\n# Visualization\n\nTo validate the effectiveness of our SOLA-GCL framework (Fig.2), we trained a view generator on the MUTAG dataset(Debnath et al. 1991; Kriege and Mutzel 2012), a realworld chemical dataset. We evaluate SOLA-GCLâ€™s ability to identify important subgraphs, such as carbon rings and $\\mathrm { N O } _ { 2 }$ groups, known to be mutagenic (Debnath et al. 1991). The visualization results explain the capability of SOLA-GCL as it has successfully identified the carbon rings and $\\mathrm { N O } _ { 2 }$ groups as critical subgraphs. The results emphasize the importance of capturing subgraph features and relationships among subgraphs, and prove the effectiveness of our SOLA-GCL.\n\n# Ablation Study\n\nThe proposed method first partitions the graph into a set of subgraphs, and then jointly trains the subgraph augmentation selector and the subgraph view generator to generate graph views. In this subsection, we present an ablation study on the unsupervised learning task to evaluate the contribution of the three components as shown in Table 4. We use \"P\" as a suffix of the model name (SOLA-GCL-P) to denote the model without a subgraph partition module, a suffix \"S\" (SOLA-GCL-S) to denote the model with a random selector for subgraph augmentation strategy rather than a learnable one, and a suffix \"R\" (SOLA-GCL-R) to denote the model with a random\n\nRing structure NO2 group   \nGround Truth High Low   \nOriginal Importance Original Importance Subgraph Visualization Visualization importance\n\n![](images/e30572d192d9aa489e7f0cdb0cf7c7142bf061feda896b375e62be321ed82492.jpg)  \nFigure 2: We visualize the graphs in the MUTAG dataset, the deeper color indicates the more important subgraphs and the ground truth is ring structure and $\\mathrm { N O } _ { 2 }$ group.   \nFigure 3: Comparisons in terms of real running time. Each model is trained for 100 epochs on each dataset and the average training time of one epoch is reported.\n\nsubgraph view generator (e.g., random node dropping in one subgraph) rather than a learnable one. For variations of the subgraph partitioning algorithm, we evaluate SOLA-GCL using the Louvain algorithm and Girvan-Newman (GN) algorithm respectively, and provide the average subgraph number of a partitioned graph in each dataset. The results indicate that the SOLA-GCL framework performs well across different subgraph partitioning algorithms, highlighting its effectiveness and robustness. Furthermore, the extensive experiments across several datasets demonstrate the robustness and adaptability of SOLA-GCL, indicating its performance is resilient to variations in the partitioned subgraph sizes.\n\n# Efficiency Analysis\n\nAs shown in Fig. 3, we present the comparisons on training time consumption for each epoch in the training process. While GraphCL and AutoGCL show the lowest training time consumption, SOLA-GCL strikes an optimal balance between efficiency and effectiveness. It achieves better performance than JOAO and JOAOv2 and consumes less training time, demonstrating a significant advantage in both performance and efficiency.\n\n# Conclusion\n\nWe present SOLA-GCL, a novel data augmentation method for graph contrastive learning. In contrast to existing methods that overlook the importance of the critical role of subgraph structures, SOLA-GCL takes a comprehensive approach by considering the information of intra-subgraph characteristics and inter-subgraph relationships. This is achieved through the joint training of a subgraph augmentation selector and a subgraph view generator, enabling the generation of learnable graph views. It consistently outperforms existing methods across various datasets, highlighting its effectiveness.",
    "summary": "```json\n{\n  \"core_summary\": \"### ğŸ¯ æ ¸å¿ƒæ¦‚è¦\\n\\n> **é—®é¢˜å®šä¹‰ (Problem Definition)**\\n> *   ç°æœ‰çš„å›¾å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼ˆGraph Contrastive Learning, GCLï¼‰å¾€å¾€å¿½è§†å­å›¾ç»“æ„çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯å­å›¾å†…éƒ¨ç‰¹å¾ï¼ˆintra-subgraph characteristicsï¼‰å’Œå­å›¾é—´å…³ç³»ï¼ˆinter-subgraph relationshipsï¼‰ï¼Œå¯¼è‡´ç”Ÿæˆçš„å¯¹æ¯”å¯¹ä¿¡æ¯ä¸è¶³ä¸”å¤šæ ·æ€§æœ‰é™ã€‚è¿™ä¸€é—®é¢˜åœ¨ä¸åŒç±»å‹çš„å›¾æ•°æ®ï¼ˆå¦‚ç¤¾äº¤ç½‘ç»œä¸­çš„ç¤¾åŒºç»“æ„ã€ç”ŸåŒ–ç½‘ç»œä¸­çš„åˆ†å­ç›¸äº’ä½œç”¨ï¼‰ä¸­å°¤ä¸ºå…³é”®ã€‚\\n> *   è¯¥é—®é¢˜çš„è§£å†³å¯¹äºæå‡å›¾è¡¨ç¤ºå­¦ä¹ çš„é²æ£’æ€§å’Œåˆ¤åˆ«æ€§å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå°¤å…¶åœ¨ç¼ºä¹æ ‡æ³¨æ•°æ®çš„åœºæ™¯ä¸‹ï¼ˆå¦‚åŠç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œè¿ç§»å­¦ä¹ ï¼‰ã€‚\\n\\n> **æ–¹æ³•æ¦‚è¿° (Method Overview)**\\n> *   æœ¬æ–‡æå‡ºäº†ä¸€ç§é¢å‘å­å›¾çš„å¯å­¦ä¹ å¢å¼ºæ–¹æ³•ï¼ˆSubgraph-Oriented Learnable Augmentation Method for Graph Contrastive Learning, SOLA-GCLï¼‰ï¼Œé€šè¿‡å­å›¾åˆ†å‰²å’Œå¯å­¦ä¹ çš„å¢å¼ºç­–ç•¥ç”Ÿæˆå¤šæ ·åŒ–çš„å›¾è§†å›¾ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰å­å›¾å†…å’Œå­å›¾é—´çš„å…³é”®ä¿¡æ¯ã€‚\\n\\n> **ä¸»è¦è´¡çŒ®ä¸æ•ˆæœ (Contributions & Results)**\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹1ï¼š** æå‡ºäº†é¦–ä¸ªä¸“æ³¨äºå­å›¾å†…ç‰¹å¾å’Œå­å›¾é—´å…³ç³»çš„å¯å­¦ä¹ å¢å¼ºæ¡†æ¶ï¼ˆSOLA-GCLï¼‰ï¼Œé€šè¿‡å­å›¾åˆ†å‰²å’Œå¯å­¦ä¹ çš„å¢å¼ºç­–ç•¥ç”Ÿæˆå¤šæ ·åŒ–çš„å›¾è§†å›¾ã€‚\\n>   *   **æ•ˆæœï¼š** åœ¨16ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSOLA-GCLåœ¨æ— ç›‘ç£å­¦ä¹ ä»»åŠ¡ä¸­å¹³å‡å‡†ç¡®ç‡æå‡ `1.5-3.0%`ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ï¼ˆå¦‚MSSGCLå’ŒAutoGCLï¼‰ã€‚\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹2ï¼š** è®¾è®¡äº†ä¸€ç§ç«¯åˆ°ç«¯çš„å¯å¾®åˆ†è®­ç»ƒç®—æ³•ï¼Œæ”¯æŒè‡ªåŠ¨é€‰æ‹©å¢å¼ºç­–ç•¥å’Œç”Ÿæˆå›¾è§†å›¾ã€‚\\n>   *   **æ•ˆæœï¼š** åœ¨è¿ç§»å­¦ä¹ ä»»åŠ¡ä¸­ï¼ŒSOLA-GCLåœ¨BBBPæ•°æ®é›†ä¸Šè¾¾åˆ° `74.08%` çš„å‡†ç¡®ç‡ï¼Œæ¯”AutoGCLæå‡ `0.72%`ã€‚\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹3ï¼š** é€šè¿‡å¯è§†åŒ–å®éªŒéªŒè¯äº†SOLA-GCLèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å…³é”®å­å›¾ï¼ˆå¦‚ç”ŸåŒ–ç½‘ç»œä¸­çš„ç¢³ç¯å’Œ `NO2` åŸºå›¢ï¼‰ã€‚\\n>   *   **æ•ˆæœï¼š** åœ¨MUTAGæ•°æ®é›†ä¸Šï¼ŒSOLA-GCLçš„å‡†ç¡®ç‡è¾¾åˆ° `90.49%`ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚\",\n  \"algorithm_details\": \"### âš™ï¸ ç®—æ³•/æ–¹æ¡ˆè¯¦è§£\\n\\n> **æ ¸å¿ƒæ€æƒ³ (Core Idea)**\\n> *   SOLA-GCLçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å­å›¾åˆ†å‰²å’Œå¯å­¦ä¹ çš„å¢å¼ºç­–ç•¥ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„å›¾è§†å›¾ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰å­å›¾å†…å’Œå­å›¾é—´çš„å…³é”®ä¿¡æ¯ã€‚å…¶è®¾è®¡å“²å­¦æ˜¯ï¼šå­å›¾æ˜¯å›¾æ•°æ®ä¸­çš„åŸºæœ¬è¯­ä¹‰å•å…ƒï¼Œå¢å¼ºç­–ç•¥åº”é’ˆå¯¹å­å›¾çš„ç‰¹æ€§è¿›è¡Œä¼˜åŒ–ã€‚\\n\\n> **åˆ›æ–°ç‚¹ (Innovations)**\\n> *   **ä¸å…ˆå‰å·¥ä½œçš„å¯¹æ¯”ï¼š** ç°æœ‰æ–¹æ³•ï¼ˆå¦‚GraphCLã€JOAOï¼‰é€šå¸¸é‡‡ç”¨éšæœºå¢å¼ºç­–ç•¥ï¼Œå¯èƒ½ç ´åå›¾çš„è¯­ä¹‰ä¿¡æ¯ï¼›è€ŒAD-GCLå’ŒAutoGCLè™½ç„¶å¼•å…¥äº†å¯å­¦ä¹ å¢å¼ºï¼Œä½†æœªå……åˆ†åˆ©ç”¨å­å›¾ä¿¡æ¯ã€‚\\n> *   **æœ¬æ–‡çš„æ”¹è¿›ï¼š** SOLA-GCLé€šè¿‡å­å›¾åˆ†å‰²å’Œå¯å­¦ä¹ çš„å¢å¼ºç­–ç•¥ï¼ˆå¦‚èŠ‚ç‚¹ä¸¢å¼ƒã€ç‰¹å¾æ©ç ã€å­å›¾äº¤æ¢ï¼‰ï¼Œå®ç°äº†å¯¹å­å›¾å†…å’Œå­å›¾é—´å…³ç³»çš„é’ˆå¯¹æ€§å¢å¼ºã€‚\\n\\n> **å…·ä½“å®ç°æ­¥éª¤ (Implementation Steps)**\\n> 1.  **å­å›¾åˆ†å‰²ï¼š** ä½¿ç”¨Louvainç®—æ³•æˆ–RDKitå°†åŸå§‹å›¾åˆ†å‰²ä¸ºå¤šä¸ªå¯†é›†è¿æ¥çš„å­å›¾ã€‚\\n> 2.  **å­å›¾å¢å¼ºé€‰æ‹©å™¨ï¼š** é€šè¿‡GNNæå–å­å›¾åµŒå…¥ï¼Œä½¿ç”¨Gumbel-Softmaxé€‰æ‹©æœ€ä¼˜å¢å¼ºç­–ç•¥ï¼ˆå¦‚èŠ‚ç‚¹ä¸¢å¼ƒã€å­å›¾äº¤æ¢ï¼‰ã€‚\\n> 3.  **å­å›¾è§†å›¾ç”Ÿæˆå™¨ï¼š** æ ¹æ®é€‰æ‹©çš„ç­–ç•¥ç”Ÿæˆå¢å¼ºåçš„å­å›¾è§†å›¾ï¼ŒåŒ…æ‹¬å­å›¾å†…ç­–ç•¥ï¼ˆèŠ‚ç‚¹ä¸¢å¼ƒã€ç‰¹å¾æ©ç ï¼‰å’Œå­å›¾é—´ç­–ç•¥ï¼ˆå­å›¾äº¤æ¢ï¼‰ã€‚\\n> 4.  **å­å›¾è§†å›¾ç»„è£…å™¨ï¼š** å°†å¢å¼ºåçš„å­å›¾è§†å›¾ç»„è£…ä¸ºå®Œæ•´çš„å›¾è§†å›¾ï¼Œç”¨äºå¯¹æ¯”å­¦ä¹ ã€‚\\n> 5.  **æŸå¤±å‡½æ•°ï¼š** ç»“åˆå¯¹æ¯”æŸå¤±ï¼ˆNT-XEntï¼‰ã€ç›¸ä¼¼æ€§æŸå¤±å’Œåˆ†ç±»æŸå¤±è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚\\n\\n> **æ¡ˆä¾‹è§£æ (Case Study)**\\n> *   è®ºæ–‡æœªæ˜ç¡®æä¾›æ­¤éƒ¨åˆ†ä¿¡æ¯ã€‚\",\n  \"comparative_analysis\": \"### ğŸ“Š å¯¹æ¯”å®éªŒåˆ†æ\\n\\n> **åŸºçº¿æ¨¡å‹ (Baselines)**\\n> *   ä¼ ç»Ÿæ–¹æ³•ï¼šGLã€WLã€DGKã€‚\\n> *   æ— ç›‘ç£æ–¹æ³•ï¼šnode2vecã€sub2vecã€graph2vecã€‚\\n> *   GCLæ–¹æ³•ï¼šInfoGraphã€GraphCLã€JOAOã€AD-GCLã€AutoGCLã€MSSGCLã€‚\\n\\n> **æ€§èƒ½å¯¹æ¯” (Performance Comparison)**\\n> *   **åœ¨æ— ç›‘ç£å­¦ä¹ ä»»åŠ¡ä¸Šï¼š** æœ¬æ–‡æ–¹æ³•åœ¨MUTAGæ•°æ®é›†ä¸Šè¾¾åˆ°äº† `90.49%` çš„å‡†ç¡®ç‡ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹GraphCL (`86.80%`) å’ŒAutoGCL (`88.64%`)ã€‚ä¸è¡¨ç°æœ€ä½³çš„åŸºçº¿ï¼ˆMSSGCL, `89.68%`ï¼‰ç›¸æ¯”ï¼Œæå‡äº† `0.81%`ã€‚\\n> *   **åœ¨è¿ç§»å­¦ä¹ ä»»åŠ¡ä¸Šï¼š** æœ¬æ–‡æ–¹æ³•åœ¨BBBPæ•°æ®é›†ä¸Šè¾¾åˆ°äº† `74.08%` çš„å‡†ç¡®ç‡ï¼Œä¼˜äºAutoGCL (`73.36%`) å’ŒAD-GCL (`70.01%`)ã€‚ä¸è¡¨ç°æœ€ä½³çš„åŸºçº¿ï¼ˆAutoGCLï¼‰ç›¸æ¯”ï¼Œæå‡äº† `0.72%`ã€‚\\n> *   **åœ¨åŠç›‘ç£å­¦ä¹ ä»»åŠ¡ä¸Šï¼š** æœ¬æ–‡æ–¹æ³•åœ¨PROTEINSæ•°æ®é›†ä¸Šè¾¾åˆ°äº† `77.63%` çš„å‡†ç¡®ç‡ï¼Œä¼˜äºMSSGCL (`75.76%`) å’ŒAutoGCL (`75.65%`)ã€‚ä¸è¡¨ç°æœ€ä½³çš„åŸºçº¿ï¼ˆMSSGCLï¼‰ç›¸æ¯”ï¼Œæå‡äº† `1.87%`ã€‚\",\n  \"keywords\": \"### ğŸ”‘ å…³é”®è¯\\n\\n*   å›¾å¯¹æ¯”å­¦ä¹  (Graph Contrastive Learning, GCL)\\n*   å­å›¾å¢å¼º (Subgraph Augmentation, N/A)\\n*   å¯å­¦ä¹ å¢å¼º (Learnable Augmentation, N/A)\\n*   å›¾ç¥ç»ç½‘ç»œ (Graph Neural Network, GNN)\\n*   æ— ç›‘ç£å­¦ä¹  (Unsupervised Learning, N/A)\\n*   è¿ç§»å­¦ä¹  (Transfer Learning, N/A)\\n*   ç¤¾äº¤ç½‘ç»œ (Social Network, N/A)\\n*   åˆ†å­å›¾ (Molecular Graph, N/A)\"\n}\n```"
}