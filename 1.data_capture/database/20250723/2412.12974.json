{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.12974",
    "link": "https://arxiv.org/abs/2412.12974",
    "pdf_link": "https://arxiv.org/pdf/2412.12974.pdf",
    "title": "Attentive Eraser: Unleashing Diffusion Model's Object Removal Potential via Self-Attention Redirection Guidance",
    "authors": [
        "Wenhao Sun",
        "Benlei Cui",
        "Xue-Mei Dong",
        "Jingqun Tang"
    ],
    "categories": [
        "cs.CV"
    ],
    "publication_date": "2024-12-17",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 14,
    "influential_citation_count": 2,
    "institutions": [
        "Zhejiang Gongshang University",
        "Alibaba Group",
        "ByteDance Inc."
    ],
    "paper_content": "# Attentive Eraser: Unleashing Diffusion Model’s Object Removal Potential via Self-Attention Redirection Guidance\n\nWenhao Sun1\\*, Xue-Mei Dong1†, Benlei $\\mathbf { C u i } ^ { 2 * }$ , Jingqun Tang3\n\n1School of Statistics and Mathematics, Zhejiang Gongshang University, Hangzhou, China 2Alibaba Group, Hangzhou, China 3ByteDance Inc., Hangzhou, China 22020040141@pop.zjgsu.edu.cn, dongxuemei $@$ zjgsu.edu.cn, cuibenlei.cbl $@$ alibaba-inc.com, tangjingqun $@$ bytedance.com\n\n# Abstract\n\nRecently, diffusion models have emerged as promising newcomers in the field of generative models, shining brightly in image generation. However, when employed for object removal tasks, they still encounter issues such as generating random artifacts and the incapacity to repaint foreground object areas with appropriate content after removal. To tackle these problems, we propose Attentive Eraser, a tuning-free method to empower pre-trained diffusion models for stable and effective object removal. Firstly, in light of the observation that the self-attention maps influence the structure and shape details of the generated images, we propose Attention Activation and Suppression (ASS), which re-engineers the self-attention mechanism within the pretrained diffusion models based on the given mask, thereby prioritizing the background over the foreground object during the reverse generation process. Moreover, we introduce Self-Attention Redirection Guidance (SARG), which utilizes the self-attention redirected by ASS to guide the generation process, effectively removing foreground objects within the mask while simultaneously generating content that is both plausible and coherent. Experiments demonstrate the stability and effectiveness of Attentive Eraser in object removal across a variety of pre-trained diffusion models, outperforming even training-based methods. Furthermore, Attentive Eraser can be implemented in various diffusion model architectures and checkpoints, enabling excellent scalability.\n\n# Introduction\n\nThe widespread adoption of diffusion models (DMs) (Ho, Jain, and Abbeel 2020; Song et al. 2021) in recent years has enabled the generation of high-quality images that match the quality of real photos and provide a realistic visualization based on user specifications. This raises a natural question of whether the image-generating capabilities of these models can be harnessed to remove objects of interest from images. Such a task, termed object removal (Yu et al. 2018; Suvorov et al. 2022), represents a specialized form of image inpainting, and requires addressing two critical aspects. Firstly, the user-specified object (usually given as a binary mask) must be successfully and effectively removed from the image. Secondly, the mask area must be filled with content that is realistic, plausible, and appropriate to maintain overall coherence within the image.\n\nTraditional approaches for object removal are the patchbased methods (Guo et al. 2018; Lu et al. 2018), which fill in the missing regions after removal by searching for well-matched replacement patches (i.e. candidate patches) in the undamaged part of the image and copying them to the corresponding removal locations. However, such processing methods often lead to inconsistency and unnaturally between the removed region and its surroundings. In recent years, convolutional neural networks (CNNs) have demonstrated considerable potential for object removal tasks. However, CNNs-based methods (Yan et al. 2018; Oleksii 2019; Suvorov et al. 2022) typically utilize a fixed-size convolutional kernel or network structure, which constrains the perceptual range of the model and the utilization of contextual information (Fang et al. 2023a; Xu et al. 2024). Consequently, the model’s performance is sub-optimal when confronted with large-scale removal or complex scenes.\n\nWith the rapid development of generative models (Shen et al. 2024b) in deep learning(Fang et al. 2024c), a proliferation of generative models has been applied to object removal. Among these, the most common are generative adversarial network (GAN) (Goodfellow et al. 2014)- based methods and DMs-based methods. GAN-based methods (Chen and Hu 2019; Shin et al. 2020) employ neural networks of varying granularity, with the context-focused module exhibiting robust performance and efficacy in image inpainting. However, their training is inherently slow and unstable, and they are susceptible to issues such as mode collapse or failure to converge (Salimans et al. 2016).\n\nIn current times, DMs have made new waves in the field of deep generative models, broken the long-held dominance of GANs, and achieved new state-of-the-art performance in many computer vision tasks (Shen et al. 2024a,b; Shen and Tang 2024; Zhao et al. 2024b). The most prevalent opensource pre-trained model in DMs is Stable Diffusion (SD) (Rombach et al. 2022), which is a pre-trained latent diffusion model. To apply SD to the object removal task, fine-tuned from SD, SD-inpainting (Rombach et al. 2022) was developed into an end-to-end model with a particular focus on inpainting, to incorporate a mask as an additional condition within the model. However, even after spending a considerable cost in terms of resources, its object removal ability is not stable, and it often fails to completely remove the object or generates random artifacts(as shown in Figure 4). An additional methodology entails guiding the model to perform object removal via prompt instruction (Yildirim et al. 2023; Brooks, Holynski, and Efros 2023). The downside of this method is that to achieve a satisfactory result, these models often necessitate a considerable degree of prompt engineering and fail to allow for accurate interaction even with a mask. Additionally, they often necessitate substantial resources for fine-tuning.\n\nTo address these problems, we propose a tuning-free method, Attentive Eraser, a simple yet highly effective method for mask-guided object removal. This method ensures that during the reverse diffusion denoising process, the content generated within the mask tends to focus on the background rather than the foreground object itself. This is achieved by modifying the self-attention mechanism in the SD model and utilizing it to steer the sampling process. We show that when Attentive Eraser is combined with the prevailing diffusion-based inpainting pipelines (Couairon et al. 2023; Avrahami, Fried, and Lischinski 2023), these pipelines enable stable and reliable object removal, fully exploiting the massive prior knowledge in the pre-trained SD model to unleash its potential for object removal (as shown in Figure 1). The main contributions of our work are presented as follows:\n\n• We propose a tuning-free method Attentive Eraser to unleash DM’s object removal potential, which comprises two components: (1) Attention Activation and Suppression (AAS), a self-attention-modified method that enables the generation of images with enhanced attention to the background while simultaneously reducing attention to the foreground object. (2) Self-Attention Redirection Guidance (SARG), a novel sampling guidance method that utilizes the proposed AAS to steer sampling towards the object removal direction. • Experiments and user studies demonstrate the effectiveness, robustness, and scalability of our method, with both removal quality and stability surpassing SOTA methods.\n\n# Related Works\n\n# Diffusion Models for Object Removal\n\nExisting diffusion model-based object removal methods can be classified into two categories, tuning-free (Zhao et al. 2024a) vs. training-based (Fang et al. 2023b), depending on whether they require fine-tuning or not. In the case of the training-based methods, DreamInpainter (Xie et al. 2023b) captures the identity of an object and removes it by introducing the discriminative token selection module. Powerpaint (Zhuang et al. 2023) introduces learnable task prompts for object removal tasks. Inst-Inpaint (Yildirim et al. 2023) constructs a dataset for object removal, and uses it to fine-tune the pre-trained diffusion model. There are other instructionbased methods achieving object removal via textual commands (Huang et al. 2024; Yang et al. 2024b; Geng et al. 2024). In the case of the tuning-free methods, Blended Diffusion (Avrahami, Fried, and Lischinski 2023) and ZONE (Li et al. 2024) perform local text-guided image manipulations by introducing text conditions to the diffusion sampling process. Magicremover (Yang et al. 2023) implements object removal by modifying cross-attention to direct diffusion model sampling. However, these methods can lead to artifacts in the final result or incomplete removal of the target due to the stochastic nature of the diffusion model itself and imprecise guiding operations. To address the above issues and to avoid consuming resources for training, we propose a tuning-free method SARG to gradually steer the diffusion process towards object removal.\n\n![](images/6c565af793ef4b419c6a82885d3c761b9d72721ce9352951d81e33c0e56c754d.jpg)  \nFigure 1: Qualitative comparison between Stable Diffusion (baseline) and self-attention redirection guided Stable Diffusion for object removal.\n\n# Sampling Guidance for Diffusion Models\n\nSampling guidance for diffusion models involves techniques that steer the sampling process toward desired outcomes. Classifier guidance (Dhariwal and Nichol 2021) involves the incorporation of an additional trained classifier to generate samples of the desired category. Unlike the former, Classifier-free Guidance (Ho and Salimans 2021) does not rely on an external classifier but instead constructs an implicit classifier to guide the generation process. There are two methods that combine self-attention with guidance, SAG (Hong et al. 2023) and PAG (Ahn et al. 2024), which utilize or modify the self-attention mechanism to guide the sampling process, thereby enhancing the quality of the generated images. Our work is similar to PAG in that it modifies the self-attention map to guide sampling, but the purpose and approach to modification are different.\n\n# Preliminaries\n\n# Diffusion Models\n\nDMs are a class of probabilistic generative models that learn a given data distribution $q \\left( x \\right)$ by progressively adding noise to the data to destroy its structure and then learning a corresponding inverse process of a fixed Markov chain of length $\\mathrm { \\Delta T }$ to denoise it. Specifically, given a set of data $x _ { 0 } \\sim q \\left( x _ { 0 } \\right)$ , the forward process could be formulated by\n\n$$\nq \\left( x _ { t } \\mid x _ { t - 1 } \\right) = \\mathcal { N } \\left( x _ { t } ; \\sqrt { 1 - \\beta _ { t } } x _ { t - 1 } , \\beta _ { t } \\mathbf { I } \\right) ,\n$$\n\nwhere $t \\in \\{ 1 , 2 , . . . , T \\}$ denotes the time step of diffusion process, $\\boldsymbol { x } _ { t }$ is the noisy data at step $t$ , $\\beta _ { t } \\in [ 0 , 1 ]$ is the variance schedule at step $t$ and represents the level of noise.\n\nStarting from $x _ { T }$ , the reverse process aims to obtain a true sample by iterative sampling from $q \\left( x _ { t - 1 } \\mid x _ { t } \\right)$ . Unfortunately, this probability is intractable, therefore, a deep neural network with parameter $\\theta$ is used to fit it:\n\n$$\nS _ { s e l f } = Q _ { s e l f } \\left( K _ { s e l f } \\right) ^ { T } / \\sqrt { d } ,\n$$\n\n$$\nA _ { s e l f } = \\operatorname { s o f t m a x } \\left( S _ { s e l f } \\right) ,\n$$\n\n$$\nO P _ { s e l f } = A _ { s e l f } V _ { s e l f } ,\n$$\n\nwhere $d$ is the dimension of query matrix $Q _ { s e l f }$ , and the similarity matrix $S _ { s e l f } \\in \\mathbb { R } ^ { ( h \\times w ) \\times ( h \\times w ) }$ and self-attention map $A _ { s e l f } \\ \\in \\ \\mathbb { R } ^ { ( h \\times w ) \\times ( h \\times w ) }$ can be seen as the query-key similarities for structure (Ahn et al. 2024), which represent the correlation between image-internal spatial features, influence the structure and shape details of the generated image. In SD, each such spatial feature is indicative of a particular region of the generated image. Inspired by this insight, we achieve object removal by changing the associations between different image-internal spatial features within the self-attention map.\n\n# Guidance\n\nA key advantage of diffusion models is the ability to integrate additional information into the iterative inference process for guiding the sampling process, and the guidance can be generalized as any time-dependent energy function from the score-based perspective. Modifying $\\epsilon _ { \\theta } ^ { ( \\bar { t } ) } \\left( z _ { t } \\right)$ with this energy function can guide the sampling process towards generating samples from a specifically conditioned distribution, formulated as:\n\n$$\np _ { \\theta } \\left( x _ { t - 1 } \\mid x _ { t } \\right) = \\mathcal { N } \\left( x _ { t - 1 } ; \\mu _ { \\theta } ^ { ( t ) } ( x _ { t } ) , \\Sigma _ { \\theta } ^ { ( t ) } ( x _ { t } ) \\right) ,\n$$\n\nWith the parameterization\n\n$$\n\\mu _ { \\theta } ^ { \\left( t \\right) } \\left( x _ { t } \\right) = \\frac { 1 } { \\sqrt { \\alpha _ { t } } } \\left( x _ { t } - \\frac { \\beta _ { t } } { \\sqrt { 1 - \\bar { \\alpha } _ { t } } } \\epsilon _ { \\theta } ^ { \\left( t \\right) } \\left( x _ { t } \\right) \\right) ,\n$$\n\nproposed by ${ \\mathrm { H o ( H o ) } }$ , Jain, and Abbeel 2020), a U-net (Ronneberger, Fischer, and Brox 2015) $\\epsilon _ { \\theta } ^ { ( t ) } \\left( x _ { t } \\right)$ is trained to predict the noise $\\epsilon \\sim \\mathcal { N } ( \\mathbf { 0 } , \\mathbf { I } )$ that is introduced to $x _ { 0 }$ to obtain $x _ { t }$ , by minimizing the following object:\n\n$$\n\\underset { \\theta } { \\operatorname* { m i n } } \\mathbb { E } _ { \\boldsymbol { x } _ { 0 } , \\boldsymbol { \\epsilon } \\sim \\mathcal { N } ( \\mathbf { 0 } , \\mathbf { I } ) , t \\sim \\mathrm { U n i f o r m } ( 1 , T ) } \\left. \\boldsymbol { \\epsilon } - \\boldsymbol { \\epsilon } _ { \\theta } ^ { ( t ) } \\left( \\boldsymbol { x } _ { t } \\right) \\right. _ { 2 } ^ { 2 } ,\n$$\n\nAfter training, a sample $x _ { 0 }$ can be generated following the reverse process from $x _ { T } \\sim \\mathcal { N } ( \\mathbf { 0 } , \\mathbf { I } )$ .\n\n# Self-Attention in Stable Diffusion\n\nRecent studies (Patashnik et al. 2023; Nam et al. 2024; Liu et al. 2024) have elucidated the significant role of the selfattention module within the stable diffusion U-net. It harnesses the power of attention mechanisms to aggregate features (Tang et al. 2022; Shen et al. 2023; Fang et al. 2023c), allowing for a more nuanced control over the details of image generation. Specifically, given any latent feature map $\\bar { z } \\in \\mathbf { \\mathbb { R } } ^ { h \\times w \\times c }$ , where $h , w$ and $c$ are the height, width and channel dimensions of $z$ respectively, the according query matrix $Q _ { s e l f } \\in \\mathbb { R } ^ { ( h \\times w ) \\times d }$ , key matrix $K _ { s e l f } \\in \\mathbb { R } ^ { ( \\bar { h } \\times \\bar { w } ) \\times \\bar { d } }$ and value matrix $V _ { s e l f } \\in \\mathbb { R } ^ { ( h \\times w ) \\times d }$ can be obtained through learned linear layers $\\ell _ { Q }$ , $\\ell _ { K }$ and $\\ell _ { V }$ , respectively. The similarity matrix $S _ { s e l f }$ , self-attention map $A _ { s e l f }$ and output $O P _ { s e l f }$ can be defined as follows:\n\n$$\n\\begin{array} { r } { Q _ { s e l f } = \\ell _ { Q } \\left( z \\right) , K _ { s e l f } = \\ell _ { K } \\left( z \\right) , V _ { s e l f } = \\ell _ { V } \\left( z \\right) , } \\end{array}\n$$\n\n$$\n\\hat { \\epsilon } _ { \\theta } ^ { \\left( t \\right) } \\left( z _ { t } ; C \\right) = \\epsilon _ { \\theta } ^ { \\left( t \\right) } \\left( z _ { t } ; C \\right) - s \\mathrm { \\bf ~ g } \\left( z _ { t } ; y \\right) ,\n$$\n\nwhere $C$ represents conditional information, $\\mathbf { g } \\left( z _ { t } ; y \\right)$ is an energy function and $y$ represents the imaginary labels for the desirable sample and $s$ is the guidance scale. There are many forms of $\\mathbf { g }$ (Nichol et al. 2021; Dhariwal and Nichol 2021; Ho and Salimans 2021; Bansal et al. 2023; Epstein et al. 2023; Mo et al. 2024), the most prevalent of which is classifier-free guidance (Ho and Salimans 2021), where $C$ represents textual information (Liu et al. 2023; Fang et al. 2024a,b), $\\mathbf { g } = \\epsilon _ { \\theta }$ and $y = \\emptyset$ .\n\n# Methodology\n\n# Overview\n\nThe overall framework diagram of the proposed method is depicted in Figure 2. There are two principal components: AAS and SARG, which will be elucidated in more detail in the following sections.\n\n# Attention Activation and Suppression\n\nConsider $l$ to be a specific self-attention layer in the Unet that accepts features of dimension $N \\times N$ , the corresponding similarity matrix and attention map at timestep $\\bar { S _ { l , t } ^ { s e l f } , A _ { l , t } ^ { s e l f } } \\ \\in \\ \\bar { \\mathbb { R } ^ { N ^ { 2 } \\times N ^ { 2 } } }$ can be obtained. The magnitude $t$ , of the value of $A _ { l , t } ^ { s e l f } \\left[ i , j \\right]$ in the self-attention map represents the extent to which the token $i$ generation process is influenced by the token $j$ . In other words, row $i$ in the map indicates the extent to which each token in the feature map influences the generation process of token $i$ , while column $j$ in the map indicates the extent to which token $j$ influences the generation process of all tokens in the feature map.\n\n(a) Attention Activation and Suppression(AAs)  Similarity Suppression(Ss) :Interaction of bg10b1 ob : Similarity :biect and background b1+ ■:Attention objectandobject 入 Softmax ob2+ w/o Ss ob b93 +:Attention Activation bgm X   \nObject obj→bg - :Attention Suppression/ 8 8 obn+-+-+   \nAttention Background 01010 b\\* ob→bg1bg2bg3bg4   \nobj→obj bg→obj↓ bg1ob1obn Mt /Eq.(13) bg10b1 ob ss bg1 bg1 - - obi obi ob2 Eq.(14) Softmob2 w/ SS ob bg3 bgm bgm - - obn s obn Abg\\* ob→bg1bg2 bg3 bg4 Q AAS((2) Self ? Eq.(15,16)Eq(17) Self Attn\\ Mit Attn K Similarity Matrix v Output pe(zt-1|zt) ①↓   \n2t (2t) S 2t-1 (b) Self-Attention Redirection Guidance(SARG) (2）\n\nTo facilitate computation and adaptation, we regulate selfattention map $A _ { l , t } ^ { s e l f }$ corporally by changing the similarity matrix $S _ { l , t } ^ { s e l f }$ . Specifically, suppose $M _ { l , t } \\ \\in \\ \\mathbb { R } ^ { 1 \\times N ^ { 2 } }$ is the corresponding flattened mask, among these $N ^ { 2 }$ tokens, we denote the set of tokens belonging to the foreground object region as $F _ { l , t } ^ { o b j }$ and the set of remaining tokens as $F _ { l , t } ^ { b g }$ . Correspondingly, $M _ { l , t }$ can be expressed by the following equation:\n\n$$\n\\begin{array} { r } { M _ { l , t } [ i ] = \\left\\{ \\begin{array} { l l } { 1 , } & { i \\in F _ { l , t } ^ { o b j } } \\\\ { 0 , } & { i \\in F _ { l , t } ^ { b g } . } \\end{array} \\right. } \\end{array}\n$$\n\nWe define $S _ { l , t } ^ { o b j  b g } = \\{ S _ { l , t } [ i , j ] | i \\in F _ { l , t } ^ { o b j } , j \\in F _ { l , t } ^ { b g }  \\}$ to reflect the relevance of the content to be generated in the foreground object area to the background, while information about the appearance of the foreground object is reflected in $S _ { l , t } ^ { o b j  o b j } = \\{ S _ { l , t } [ i , j ] | i \\in \\bar { F _ { l , t } ^ { o b j } } , j \\in \\bar { F _ { l , t } ^ { o b j } }  \\}$ . In the object removal task, we are dealing with foreground objects, and the background should remain the same. As shown in Figure 3, after DDIM inversion (Song, Meng, and Ermon 2020), we utilize PCA (Mac´kiewicz and Ratajczak 1993) and clustering to visualize the average self-attention maps over all time steps for different layers during the reverse denoising process. It can be observed that self-attention maps resemble a semantic layout map of the components of the image (Yang et al. 2024a), and there is a clear distinction between the self-attention corresponding to the generation of the foreground object and background. Consequently, to facilitate object removal during the generation process, an intuitive approach would be to ”blend” the self-attention of foreground objects into the background, thus allowing them to be clustered together. In other words, the region corresponding to the foreground object should be generated with a greater degree of reference to the background region than to itself during the generation process. This implies that the attention of the region within the mask to the background region should be increased and to itself should be decreased. Furthermore, the background region is fixed during the generation process and should remain unaffected by the changes in the generated content of the foreground area. Thus, the attention of the background region to the foreground region\n\n![](images/449c845fe45d09c4f20b87e631479ba15231fefb0fd3817444ef3f0c25f85cfe.jpg)  \nFigure 2: The overview of our proposed Attentive Eraser which consists of two parts: (a) Attention Activation and Suppression (AAS), a self-attention mechanism modification operation tailored to address the challenges inherent to the object removal task, aims to make the foreground object area’s generation more attentive to the background while erasing the object’s appearance information. Additionally, Similarity Suppression (SS) serves to suppress the heightened attention to similar objects that may arise due to the inherent nature of self-attention. (b) Self-Attention Redirection Guidance (SARG), a guidance method applied in the diffusion reverse sampling process, which utilizes redirected self-attention through AAS to guide the sampling process towards the direction of object removal.   \nFigure 3: Visualization of the average self-attention maps over all time steps for different layers.\n\nshould also be decreased.\n\nCombining the above analysis, we propose an approach that is both simple and effective: AAS (as shown in Figure 2(a)). Activation refers to increasing $A _ { l , t } ^ { o b j  b g }$ , which serves to enhance the attention of the foreground-generating decreasing region to the background. In contrast, Suppression refers to $A _ { l , t } ^ { o b j  o b j }$ and $A _ { l , t } ^ { b g  o b j }$ , which entails the suppression of the foreground region’s information about its appearance and its effect on the background. Given the intrinsic characteristics of the Softmax function, AAS can be simply achieved by assigning $S _ { l , t } ^ { o b j  o b j }$ to $- \\infty$ , thereby the original semantic information of the foreground objects is progressively obliterated throughout the denoising process. In practice, the aforementioned operation is achieved by the following equation:\n\n$$\nS _ { l , t } ^ { s e l f * } = S _ { l , t } ^ { s e l f } - M _ { l , t } * i n f ,\n$$\n\n$$\nO P _ { l , t } ^ { * } = A _ { l , t } ^ { s e l f * } V _ { l , t } = \\mathrm { s o f t m a x } \\left( S _ { l , t } ^ { s e l f * } \\right) V _ { l , t } ,\n$$\n\nwhere $V _ { l , t }$ represents the corresponding value matrix for the time step $t$ of layer $l$ .\n\nNevertheless, one of the limitations of the aforementioned theory is that if the background contains content that is analogous to the foreground object, due to the inherent nature of self-attention, the attention in that particular part of the generative process will be higher than in other regions, while the above theory exacerbates this phenomenon, ultimately leading to incomplete object removal (see an example on the right side of Figure 2(a)). Accordingly, to reduce the attention devoted to similar objects and disperse it to other regions, we employ a straightforward method of reducing the variance of $S _ { l , t } ^ { o b j  b g }$ , which is referenced in this paper as SS. To avoid interfering with the process of generating the background, we address the foreground and background generation in separate phases:\n\n$$\nS _ { l , t } ^ { o b j * } = \\lambda S _ { l , t } ^ { s e l f } - M _ { l , t } * i n f ,\n$$\n\n$$\nS _ { l , t } ^ { b g * } = S _ { l , t } ^ { s e l f } - M _ { l , t } * i n f ,\n$$\n\n$$\nO P _ { l , t } ^ { o b j * } = A _ { l , t } ^ { o b j * } V _ { l , t } = \\mathrm { s o f t m a x } \\left( S _ { l , t } ^ { o b j * } \\right) V _ { l , t } ,\n$$\n\n$$\nO P _ { l , t } ^ { b g * } = A _ { l , t } ^ { b g * } V _ { l , t } = \\mathrm { s o f t m a x } \\left( S _ { l , t } ^ { b g * } \\right) V _ { l , t } ,\n$$\n\nwhere $\\lambda$ is the suppression factor less than 1. Finally, to guarantee that the aforementioned operations are executed on the appropriate corresponding foreground and background regions, we integrate the two outputs $O P _ { l , t } ^ { o b j * }$ and $O P _ { l , t } ^ { b g * }$ to obtain the final output $O P _ { l , t } ^ { * }$ according to $M _ { l , t } ^ { \\top }$ :\n\n$$\nO P _ { l , t } ^ { * } = M _ { l , t } ^ { \\top } \\odot O P _ { l , t } ^ { o b j * } + \\left( 1 - M _ { l , t } ^ { \\top } \\right) \\odot O P _ { l , t } ^ { b g * } ,\n$$\n\nTo ensure minimal impact on the subsequent generation process, we apply SS at the beginning of the denoising process timesteps, for $t ~ \\in ~ [ T _ { I } , \\bar { T } _ { S S } ]$ , and still use Eq.(11), Eq.(12) to get output $O P _ { l , t } ^ { * }$ for $t \\doteq ( T _ { S S } , 1 ]$ , where $T _ { I }$ denotes the diffusion steps and $T _ { S S }$ signifies the final time-step of SS. In the following, we denote the U-net processed by the AAS approach as AAS $\\left( \\epsilon _ { \\theta } \\right)$ .\n\n# Self-Attention Redirection Guidance\n\nTo further enhance the capability of object removal as well as the overall quality of the generated images, inspired by PAG (Ahn et al. 2024), AAS $\\left( \\epsilon _ { \\theta } \\right)$ can be seen as a form of perturbation during the epsilon prediction process, we can use it to steer the sampling process towards the desirable direction. Therefore, the final predicted noise $\\hat { \\epsilon } _ { \\theta } ^ { \\left( t \\right) } \\left( z _ { t } \\right)$ at each time step can be defined as follows:\n\n$$\n\\hat { \\epsilon } _ { \\theta } ^ { ( t ) } \\left( z _ { t } \\right) = \\epsilon _ { \\theta } ^ { ( t ) } \\left( z _ { t } \\right) + s \\left( \\mathrm { A A S } \\left( \\epsilon _ { \\theta } ^ { ( t ) } \\left( z _ { t } \\right) \\right) - \\epsilon _ { \\theta } ^ { ( t ) } \\left( z _ { t } \\right) \\right) ,\n$$\n\nwhere $s$ is the removal guidance scale. Subsequently, the next time step output latent $z _ { t - 1 }$ is obtained by sampling using the modified noise $\\hat { \\epsilon } _ { \\theta } ^ { \\left( t \\right) } \\left( z _ { t } \\right)$ . In this paper, we refer to the aforementioned guidance process as SARG.\n\nThrough the iterative inference guidance, the sampling direction of the generative process will be altered, causing the distribution of the noisy latent to shift towards the object removal direction we have specified, thereby enhancing the capability of removal and the quality of the final generated images. For a more detailed analysis refer to Appendix A.\n\n# Experiments\n\n# Experimental Setup\n\nImplementation Details We apply our method on all mainstream versions of Stable Diffusion (1.5, 2.1, and XL1.0) with two prevailing diffusion-based inpainting pipelines (Couairon et al. 2023; Avrahami, Fried, and Lischinski 2023) to evaluate its generalization across various diffusion model architectures. Based on the randomness, we refer to pipelines as the stochastic inpainting pipeline (SIP) and the deterministic inpainting pipeline (DIP), respectively. Detailed descriptions of SIP and DIP are provided in Appendix B, with further experimental details available in Appendix C.\n\nBaseline We select the state-of-the-art image inpainting methods as our baselines, including two mask-guided approaches SD-Inpaint (Rombach et al. 2022), LAMA (Suvorov et al. 2022) and two text-guided approaches InstInpaint (Yildirim et al. 2023), Powerpaint (Zhuang et al. 2023), to demonstrate the efficacy of our method, we have also incorporated SD2.1 with SIP into the baseline for comparative purposes.\n\nTesting Datasets We evaluate our method on a common segmentation dataset OpenImages V5 (Kuznetsova et al. 2018), which contains both the mask information and the text information of the corresponding object of the mask. This facilitates a comprehensive comparison of the entire baseline. We randomly select 10000 sets of data from the OpenImages V5 test set as the testing datasets, a set of data including the original image and the corresponding mask, segmentation bounding box, and segmentation class labels.\n\nEvaluation Metrics We first use two common evaluation metrics FID and LPIPS to assess the quality of the generated images following LAMA(Suvorov et al. 2022) setup, which can indicate the global visual quality of the image.\n\n<html><body><table><tr><td>Method</td><td>Training</td><td>Mask</td><td>Text</td><td>FID↓</td><td>LPIPS↓</td><td>Local FID↓</td><td>CLIP consensus↓</td><td>CLIP score↑</td></tr><tr><td>SD2.1inp</td><td>√</td><td>√</td><td>X</td><td>3.805</td><td>0.3012</td><td>8.852</td><td>0.1143</td><td>21.89</td></tr><tr><td>SD2.1inp</td><td>√</td><td>√</td><td>√</td><td>4.019</td><td>0.3083</td><td>7.194</td><td>0.1209</td><td>22.27</td></tr><tr><td>PowerPaint</td><td>√</td><td>√</td><td>X</td><td>6.027</td><td>0.2887</td><td>10.02</td><td>0.0984</td><td>22.74</td></tr><tr><td>Inst-Inpaint</td><td>√</td><td>X</td><td>√</td><td>11.42</td><td>0.4095</td><td>43.47</td><td>0.0913</td><td>23.02</td></tr><tr><td>LAMA</td><td>√</td><td>√</td><td>X</td><td>7.533</td><td>0.2189</td><td>6.091</td><td></td><td>23.57</td></tr><tr><td>SD2.1+SIP w/o SARG</td><td>X</td><td>√</td><td>X</td><td>5.98</td><td>0.2998</td><td>15.58</td><td>0.1347</td><td>22.05</td></tr><tr><td>SD2.1+SIP w/SARG(ours)</td><td>X</td><td>√</td><td>X</td><td>7.352</td><td>0.3113</td><td>5.835</td><td>0.0734</td><td>23.56</td></tr><tr><td>SD2.1+DIP w/ SARG(ours)</td><td>X</td><td>√</td><td>X</td><td>7.012</td><td>0.2995</td><td>5.699</td><td></td><td>23.43</td></tr></table></body></html>\n\nTable 1: Quantitative comparison with other methods. We have indicated in the table whether each method requires training and whether it necessitates mask or prompt text as conditional inputs. In the CLIP consensus metric, deterministic process methods (lacking randomness) are denoted with a ’-’. The optimal result and object removal-related metrics are represented in bold, and the sub-optimal result is represented in underlining.\n\n![](images/217b69ff0cefed37358cbd8cbb4cfe485fce8f8e96d7039d5b8189fe80ba57e6.jpg)  \nFigure 4: Visual comparison with other methods. The mask is indicated with a red highlight in the input image. Our methods are highlighted in bold.\n\n![](images/e305d64de032d9969a7512845f7a30242ed852505742c69614901f868fd63e6e.jpg)  \nFigure 5: Visual comparison of object removal stability with other methods using three distinct random seeds.\n\nTo further assess the quality of the generated content in the mask region, we adopt the metrics Local-FID to assess the local visual quality of the image following (Xie et al. 2023a). To assess the effectiveness of object removal, we select CLIP consensus as the evaluation metric following (Wasserman et al. 2024), which enables the evaluation of the consistent diversity of the removal effect. High diversity is often seen as a sign of failed removal, with random objects appearing in the foreground area. Finally, to indicate the degree of object removal, we calculate the CLIP score (Radford et al. 2021) by taking the foreground region patch and the prompt ”background”. The greater the value, the greater the degree of alignment between the removed region and the background, effectively indicating the degree of removal.\n\n# Qualitative and Quantitative Results\n\nThe quantitative analysis results are shown in Table 1. For global quality metrics FID and LPIPS, our method is at an average level, but these two metrics do not adequately reflect the effectiveness of object removal. Subsequently, we can observe from the local FID that our method has superior performance in the local removal area. Meanwhile, the CLIP consensus indicates the instability of other diffusion-based methods, and the CLIP score demonstrates that our method effectively removes the object and repaints the foreground area that is highly aligned with the background, even reaching a competitive level with LAMA, which is a Fast Fourier Convolution-based inpainting model. Qualitative results are shown in Figure 4, where we can observe the significant differences between our method and others. LAMA, due to its lack of generative capability, successfully removes the object but produces noticeably blurry content. Other diffusionbased methods share a common issue: the instability of removal, which often leads to the generation of random artifacts. To further substantiate this issue, we conducted experiments on the stability of removal. Figure 5 presents the results of removal using three distinct random seeds for each method. It can be observed that our method achieves stable erasure across various SD models, generating more consistent content, whereas other methods have struggled to maintain stable removal of the object.\n\n<html><body><table><tr><td>Method</td><td colspan=\"2\">User Study GPTEvaluation</td></tr><tr><td>SD2.1inp</td><td>10%</td><td>1</td></tr><tr><td>SD2.1inp(w/ text)</td><td>15.4%</td><td></td></tr><tr><td>PowerPaint</td><td>7.6%</td><td>-</td></tr><tr><td>Inst-Inpaint</td><td>2.4%</td><td>1</td></tr><tr><td>LAMA</td><td>19.7%</td><td>25.53%</td></tr><tr><td>SD2.1+SIP w/ SARG(ours)</td><td>44.9%</td><td>74.47%</td></tr></table></body></html>\n\nTable 2: User study and GPT-4o Evaluation results.\n\n# User Study and GPT-4o Evaluation\n\nDue to the absence of effective metrics for the object removal task, the metrics mentioned above may not be sufficient to demonstrate the superiority of our method. Therefore, to further substantiate the effectiveness of our approach, we conduct a user preference study. Table 2 presents the user preferences for various methods, revealing consistent results with the quantitative results and highlighting that our method is strongly preferred over other methods. Furthermore, we design fairly and reasonably prompts, utilizing GPT-4o (OpenAI 2024) to conduct a further assessment of object removal performance between our method and the runner-up method LAMA. The results also indicate that our method significantly outperforms LAMA, demonstrating exceptional performance. Please refer to Appendix D for more details and visualizations of user study and GPT evaluation.\n\n# Ablations\n\nTo validate the effectiveness of the proposed Attentive Eraser, we conduct ablation studies. We use SD2.1 with SIP as the baseline for comparison, Figure 6 provides a visual representation of the ablation study concerning our method’s components. Figure 6(a) shows that the application of AAS alone cannot completely remove the foreground object, but integrating it with the sampling process through SARG can effectively remove the object and generate content consistent with the background. At the same time, we also verify the impact of SS, and it can be seen that SS effectively suppresses the generation of similar objects while maintaining the removal efficacy of the general image. As shown in In Figure 6(b), we visualize the heatmaps of the top-1 component of the self-attention maps at each step of the denoising process after SVD (Kalman 1996), demonstrating that SARG gradually, as previously stated, ”blends” the foreground objects’ self-attention into the background to remove objects. In Figure 6(c), we discuss the effect of two parameters (removal guidance $s$ and suppression factor $\\lambda$ ) upon the removal process. It is depicted that as $\\lambda$ decreases, the generation of similar objects decreases progressively, thereby reaffirming the efficacy of SS. On the other hand, the intensity of the removal process escalates with an increase in $s$ . This suggests that $s$ acts as a pivotal control in modulating the strength of the removal, allowing for a more nuanced and tailored approach to removing objects.\n\n![](images/95f5b623f8f99bc76b20328b6337169f2faa97b5d76f896db3c351dc95843de0.jpg)  \nFigure 6: Visualization of ablation experiments on Attentive Eraser.\n\n# Conclusion\n\nWe present a novel tuning-free method Attentive Eraser, which adeptly harnesses the rich repository of prior knowledge embedded within pre-trained diffusion models for the object removal task. Extensive experiments and user studies demonstrate the stability, effectiveness, and scalability of our proposed method, and also reveal that our method significantly outperforms existing methods.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   扩散模型（Diffusion Models, DMs）在图像生成领域表现出色，但在物体移除任务中存在生成随机伪影和无法在移除前景物体后合理重绘的问题。\\n> *   该问题的重要性在于物体移除是图像修复（Image Inpainting）的一种特殊形式，需要同时满足移除指定物体和生成与背景一致的内容两个关键要求。\\n\\n> **方法概述 (Method Overview)**\\n> *   提出了一种无需微调的方法 **Attentive Eraser**，通过重新设计预训练扩散模型中的自注意力机制（Self-Attention）来引导生成过程，实现稳定且高效的物体移除。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **Attention Activation and Suppression (AAS)**：通过修改自注意力机制，增强背景区域的注意力并抑制前景物体的注意力。\\n> *   **Self-Attention Redirection Guidance (SARG)**：利用AAS引导采样过程，实现物体移除。\\n> *   实验表明，Attentive Eraser在多个预训练扩散模型上均表现出色，移除质量和稳定性优于现有方法（如Local FID降低至5.835，CLIP consensus降低至0.0734）。\\n> *   用户研究和GPT-4o评估显示，44.9%的用户偏好本文方法，显著优于其他方法。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   自注意力图（Self-Attention Maps）影响生成图像的结构和形状细节。通过修改自注意力机制，使生成过程更关注背景而非前景物体。\\n> *   设计哲学：通过重新分配自注意力权重，逐步“混合”前景物体的自注意力到背景中，从而实现物体移除。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 现有方法（如SD-Inpaint、LAMA）在物体移除任务中表现不稳定，容易生成伪影或无法完全移除物体。\\n> *   **本文的改进：** 提出AAS和SARG，通过修改自注意力机制和引导采样过程，显著提升移除效果和稳定性。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.   **AAS（Attention Activation and Suppression）**：\\n>      *   对自注意力图进行激活（增强背景注意力）和抑制（减少前景物体注意力）。\\n>      *   使用公式：\\n>          $$\\n>          S _ { l , t } ^ { s e l f * } = S _ { l , t } ^ { s e l f } - M _ { l , t } * i n f\\n>          $$\\n>      *   引入Similarity Suppression (SS)来抑制相似物体的生成。\\n> 2.   **SARG（Self-Attention Redirection Guidance）**：\\n>      *   利用AAS修改后的自注意力图引导采样过程。\\n>      *   使用公式：\\n>          $$\\n>          \\\\hat { \\\\epsilon } _ { \\\\theta } ^ { ( t ) } \\\\left( z _ { t } \\\\right) = \\\\epsilon _ { \\\\theta } ^ { ( t ) } \\\\left( z _ { t } \\\\right) + s \\\\left( \\\\mathrm { A A S } \\\\left( \\\\epsilon _ { \\\\theta } ^ { ( t ) } \\\\left( z _ { t } \\\\right) \\\\right) - \\\\epsilon _ { \\\\theta } ^ { ( t ) } \\\\left( z _ { t } \\\\right) \\\\right)\\n>          $$\\n\\n> **案例解析 (Case Study)**\\n> *   论文中提供了视觉对比图（Figure 4和Figure 5），展示了Attentive Eraser在不同随机种子下的稳定移除效果，以及与其他方法的对比。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   SD-Inpaint、LAMA、PowerPaint、Inst-Inpaint。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在Local FID上：** 本文方法在OpenImages V5数据集上达到了 **5.835**，显著优于基线模型SD-Inpaint（8.852）和PowerPaint（10.02）。与表现最佳的基线LAMA（6.091）相比，提升了0.256。\\n> *   **在CLIP consensus上：** 本文方法的CLIP consensus为 **0.0734**，远低于SD-Inpaint（0.1143）和Inst-Inpaint（0.0913），表明移除效果更稳定。\\n> *   **在CLIP score上：** 本文方法的CLIP score为 **23.56**，与LAMA（23.57）相当，但优于其他基线（如SD-Inpaint的21.89）。\\n> *   **在用户偏好上：** 44.9%的用户偏好本文方法，显著高于其他方法（如LAMA的19.7%）。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   扩散模型 (Diffusion Models, DMs)\\n*   物体移除 (Object Removal, N/A)\\n*   自注意力机制 (Self-Attention Mechanism, N/A)\\n*   图像修复 (Image Inpainting, N/A)\\n*   生成对抗网络 (Generative Adversarial Network, GAN)\\n*   稳定扩散 (Stable Diffusion, SD)\\n*   采样引导 (Sampling Guidance, N/A)\\n*   注意力重定向 (Attention Redirection, N/A)\"\n}\n```"
}