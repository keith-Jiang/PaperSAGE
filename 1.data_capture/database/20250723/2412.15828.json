{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.15828",
    "link": "https://arxiv.org/abs/2412.15828",
    "pdf_link": "https://arxiv.org/pdf/2412.15828.pdf",
    "title": "Measuring Cross-Modal Interactions in Multimodal Models",
    "authors": [
        "Laura Wenderoth",
        "Konstantin Hemker",
        "Nikola Simidjievski",
        "M. Jamnik"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2024-12-20",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 2,
    "influential_citation_count": 0,
    "institutions": [
        "University of Cambridge",
        "Department of Computer Science and Technology, University of Cambridge",
        "PBCI, Department of Oncology, University of Cambridge"
    ],
    "paper_content": "# Measuring Cross-Modal Interactions in Multimodal Models\n\nLaura Wenderoth1, Konstantin Hemker1, Nikola Simidjievski2,1, Mateja Jamnik1\n\n1Department of Computer Science and Technology, University of Cambridge, Cambridge, United Kingdom 2PBCI, Department of Oncology, University of Cambridge, Cambridge, United Kingdom lw457, kh701, ns779, mj201 @cam.ac.uk\n\n# Abstract\n\nIntegrating AI in healthcare can greatly improve patient care and system efficiency. However, the lack of explainability in AI systems (XAI) hinders their clinical adoption, especially in multimodal settings that use increasingly complex model architectures. Most existing XAI methods focus on unimodal models, which fail to capture cross-modal interactions crucial for understanding the combined impact of multiple data sources. Existing methods for quantifying cross-modal interactions are limited to two modalities, rely on labelled data, and depend on model performance. This is problematic in healthcare, where XAI must handle multiple data sources and provide individualised explanations. This paper introduces InterSHAP, a cross-modal interaction score that addresses the limitations of existing approaches. InterSHAP uses the Shapley interaction index to precisely separate and quantify the contributions of the individual modalities and their interactions without approximations. By integrating an opensource implementation with the SHAP package, we enhance reproducibility and ease of use. We show that InterSHAP accurately measures the presence of cross-modal interactions, can handle multiple modalities, and provides detailed explanations at a local level for individual samples. Furthermore, we apply InterSHAP to multimodal medical datasets and demonstrate its applicability for individualised explanations.\n\n# Code — https://github.com/LauraWenderoth/InterSHAP Extended version — https://arxiv.org/abs/2412.15828\n\n# 1 Introduction\n\nIn medical decision-making, there is a growing trend toward utilising multimodal machine learning approaches that integrate multiple data sources to improve predictive and diagnostic tasks. These approaches acknowledge that medical data analysis is inherently multimodal, leveraging methods that are able to fuse heterogeneous data that includes clinical, genomic, and imaging modalities. Recent advancements in large multimodal models in the medical field, such as Google’s Med-PaLM (Tu et al. 2023) and Microsoft’s BiomedCLIP (Zhang et al. 2023), further highlight the utility of multimodal approaches in healthcare.\n\nHowever, an inherent limitation of many of the current multimodal approaches pertains to their explainability.\n\nInterSHAP Interactions   \nM1 Model Shapely M1   \nM2 Interaction   \nM3 Index M2 M3 Interaction Output Behaviour\n\nThus, healthcare applications of machine learning are often left to rely on high-performance but opaque models that obscure the reasoning behind their predictions. Such models can accurately predict diagnoses, but these predictions may not reflect true causal relationships in the data (Han et al. 2022). The lack of explainability and transparency is widely recognized as a key barrier to the clinical adoption of ML models, as noted by policymakers such as the OECD (Anderson and Sutherland 2024) and emphasized in academic studies (Yang, Ye, and Xia 2022).\n\nIn response to the urgent need for XAI, numerous methods such as SHAP (Lundberg and Lee 2017) and LIME (Ribeiro, Singh, and Guestrin 2016) have emerged to explain the results of machine learning systems. While these have shown promise for unimodal models, there are few explanation methods that work effectively across data structures, meaning that cross-modal interactions can often not be quantified with unimodal methods. Multiple modalities allow a machine learning model to utilise and combine information from different sources, enabling cross-modal interactions that often lead to improved performance. However, simply identifying these interactions is insufficient for a meaningful interpretation of the model predictions. In this context, it is crucial to comprehend both the individual impact of each modality and their combined influence. Several methods, such as PID (Liang et al. 2023), EMAP (Hessel and Lee 2020), and SHAPE (Hu, Li, and Zhou 2022), have been developed to assess whether models learn cross-modal interactions. However, these methods are often (1) limited to only two modalities; (2) applied to the entire dataset, rather than individual samples; and (3) either require labelled data or are not performance-agnostic, which can lead to an incomplete understanding of cross-modal interactions.\n\nIn this paper, we introduce InterSHAP, a comprehensive, interpretable metric that quantifies the influence of crossmodal interactions on model behaviour. InterSHAP supports any number of modalities, provides local explanations, is agnostic to model performance, and works with unlabelled data. To our knowledge, our proposed variant of the Shapley interaction index is the first approach that is able to effectively separate cross-modal interactions from individual modality contributions. Figure 1 provides a schematic overview of InterSHAP. To validate the effectiveness of InterSHAP, we conduct extensive empirical analyses using synthetic datasets with either no synergy (no cross-modal interactions are needed to solve the task) or exclusively synergy (complete information can only be obtained as a result of cross-modal interaction across all modalities). The results demonstrate that InterSHAP can accurately detect a lack of cross-modal interactions $( 0 \\% )$ on datasets with no synergy, and $9 9 . 7 \\%$ on those with exclusive synergy, aligning precisely with the expected behaviour. Furthermore, we demonstrated that InterSHAP is extendable to more than two modalities and achieves favourable results at a local level with individual data points. Additionally, we test InterSHAP on two medical datasets to show its utility in a real-world setting. InterSHAP is not confined to medical domains and can be applied to models trained on any modality.\n\nOur main contributions can be summarised as follows:\n\n• Novel cross-modal interaction score InterSHAP. We introduce InterSHAP, the first model-agnostic cross-modal interaction score that operates with any number of modalities, offering both local and global explanations, applies to unlabelled datasets and does not rely on the model’s performance. We conducted extensive experiments on synthetic datasets and models to validate its functionality. • Application to multimodal healthcare datasets. We demonstrate the benefits of InterSHAP on a cell type classification task based on multimodal protein and RNA data as well as a mortality prediction tasks. • Open-source implementation with integration into the SHAP Package. We have developed an open-source implementation of InterSHAP that seamlessly integrates with the well-known SHAP visualisation package (Lundberg and Lee 2017).\n\n# 2 Related Work\n\nExisting approaches like MM-SHAP (Parcalabescu and Frank 2023) and Perceptual Score (Gat, Schwartz, and Schwing 2021) focus on understanding the contributions of individual modalities rather than explicitly examining their interactions. They explain modality contributions in isolation without considering or quantifying cross-modal interactions within multimodal networks. In contrast, EMAP (Hessel and Lee 2020) and SHAPE (Hu, Li, and Zhou 2022) detect cross-modal interactions by analysing how perturbations to the input impact the model’s output. They systematically vary different modalities or combinations of modalities to assess their importance within the model. Another metric for detecting interactions, Partial Information Decomposition (PID) (Liang et al. 2023), quantifies synergy (interactions between modalities in a dataset with a combined effect surpassing individual contributions, indicating a non-additive relationship) in datasets, which can be interpreted as cross-modal interactions.\n\nTable 1: InterSHAP overcomes the limitations of other cross-modal interaction scores: it is unsupervised, performance agnostic, applicable to more than two modalities, and allows for dataset- (global) and sample-level (local) explainability.   \n\n<html><body><table><tr><td></td><td>Modalities >2</td><td></td><td>Unsupervised</td><td>Performance Agnostic</td></tr><tr><td>Score PID</td><td>×</td><td>Local ×</td><td></td><td></td></tr><tr><td>EMAP</td><td>×</td><td>×</td><td>×</td><td>×</td></tr><tr><td>SHAPE</td><td>√</td><td>×</td><td>×</td><td>×</td></tr><tr><td>InterSHAP</td><td>√</td><td>√</td><td>√</td><td>√</td></tr></table></body></html>\n\nHowever, these existing approaches exhibit various limitations, summarised in Table 1. First, most of them restrict the analysis to just two modalities, which prevents them from being applied in domains that typically have more modalities, like in healthcare. Second, the results cannot be analysed at the level of individual samples, which is particularly critical for medical tasks where explainability for each patient is crucial. Third, some approaches necessitate the availability of labels, rendering them unsuitable for unlabelled datasets employed in self- or unsupervised learning scenarios. Fourth, certain methods assess interactions solely based on performance gain, rendering them highly performance-dependent. However, when assessing a model, it is crucial to understand how each interaction contributed to the present prediction without neglecting interactions only because the prediction was incorrect.\n\n# 3 Methodology\n\nWe introduce InterSHAP to quantify cross-modal interaction learned by multimodal models while overcoming the limitations of SOTA explainability approaches. Following the definition of Liang, Zadeh, and Morency (2022), we define cross-modal interaction as a change in the model response that cannot be attributed to the information gained from a single modality alone, but arises only when both modalities are present. To ensure the accuracy of InterSHAP, we use the Shapley Interaction Index (SII), which has been shown to effectively capture model behaviour (Lundberg et al. 2020) and decompose unimodal feature interactions into its constituent parts (Ittner et al. 2021).\n\n# 3.1 Preliminaries\n\nModel Fusion. To train multimodal models is model fusion, which comes in three types: (i) early fusion, which merges data features at model input, (ii) intermediate fusion, which combines data within the model, and (iii) late fusion, which combines the outputs of models trained on different individual data modalities (Stahlschmidt, Ulfenborg, and Synnergren 2022; Azam, Ryabchykov, and Bocklitz 2022).\n\nShapley Value. The Shapley value $\\phi$ , from cooperative game theory, measures each feature’s contribution to overall performance via marginal contributions to feature coalitions (Roth 1988). In machine learning, a model $f _ { M }$ is viewed as a coalition game with $M$ features or modalities as players. To avoid inefficient retraining, masking strategies are applied to features $j \\not \\in S$ , where $S \\subseteq M$ .\n\nThe Shapley value $\\phi$ of a feature $\\mathbf { \\chi } _ { i }$ is calculated as the weighted average of their marginal contributions to all possible coalitions:\n\n$$\n\\begin{array} { l } { \\displaystyle \\phi _ { i } ( M , f ) = \\sum _ { S \\subseteq M \\backslash \\{ i \\} } \\frac { | S | ! ( | M | - | S | - 1 ) ! } { | M | ! } \\Delta } \\\\ { \\Delta = \\left[ f _ { S \\cup \\{ i \\} } \\left( S \\cup \\{ i \\} \\right) - f _ { S } \\left( S \\right) \\right] . } \\end{array}\n$$\n\nShapley Interaction Index. The Shapley Interaction Index (SII) (Grabisch and Roubens 1999), initially devised to quantify interaction effects between players in cooperative game theory, can be employed to discern interactions between features or modalities. We define its directly adapted version $\\phi _ { i j } ( M , f )$ for $M$ modalities and a model $f$ , where $i , j \\in M$ (Lundberg et al. 2020) and $i \\neq j$ :\n\n$$\n\\phi _ { i j } ( M , f ) = \\sum _ { S \\subseteq M \\setminus \\{ i , j \\} } { \\frac { | S | ! ( M - | S | - 2 ) ! } { 2 ( M - 1 ) ! } } \\nabla i j ( S , f ) ,\n$$\n\n$$\n\\begin{array} { r l } & { \\nabla _ { i j } ( S , f ) = \\left[ f _ { S \\cup \\{ i j \\} } \\left( S \\cup \\{ i j \\} \\right) - f _ { S \\cup \\{ i \\} } \\left( S \\cup \\{ i \\} \\right) \\right. } \\\\ & { \\qquad \\left. - f _ { S \\cup \\{ j \\} } \\left( S \\cup \\{ j \\} \\right) + f _ { S } ( S ) \\right] . } \\end{array}\n$$\n\nThe interaction index for a modality with itself, denoted as $\\phi _ { i i }$ , is defined as the difference between the Shapley value $\\phi _ { i }$ and the sum of all interaction values:\n\n$$\n\\phi _ { i i } ( M , f ) = \\phi _ { i } ( M , f ) - \\sum _ { j \\in M } \\phi _ { i j } ( M , f ) \\quad \\forall i \\not = j .\n$$\n\n# 3.2 Global InterSHAP\n\nWe aim to quantify the impact of cross-modal interactions on model behaviour using InterSHAP. Model behaviour is a cumulative process, beginning with the model’s base output and progressively adding each modality and their interactions to shape the final prediction. To isolate interactions from the model behaviour, we apply the SII. Let $f \\in \\mathbb { R } ^ { c }$ be a trained model, where $c$ is the number of per-class probabilities, evaluated on a dataset with $N$ samples and $M$ modalities, represented as $\\{ ( m _ { 1 } ^ { i } , \\ldots , m _ { M } ^ { i } ) \\} _ { i = 1 } ^ { \\cal N }$ and corresponding labels $\\{ y ^ { i } \\} _ { i = 1 } ^ { N }$ . The SII value $\\phi _ { i j }$ measures the interaction between modalities $i$ and $j$ for sample $a$ . $\\Phi _ { i j }$ is defined as the absolute value of the mean over $\\phi _ { i j }$ of all samples in the dataset:\n\n$$\n\\Phi _ { i j } = \\left| \\frac { 1 } { N } \\sum _ { a = 1 } ^ { N } \\phi _ { i j } ( m _ { 1 } ^ { a } , \\dots , m _ { M } ^ { a } , f ) \\right| ,\n$$\n\nresulting in the matrix $\\Phi$ containing only positive values:\n\n$$\n\\Phi = \\left[ \\begin{array} { c c c c } { \\Phi _ { 1 1 } } & { \\Phi _ { 1 2 } } & { . . . } & { \\Phi _ { 1 M } } \\\\ { \\Phi _ { 2 1 } } & { \\Phi _ { 2 2 } } & { . . . } & { \\Phi _ { 2 M } } \\\\ { \\vdots } & { \\vdots } & { \\ddots } & { \\vdots } \\\\ { \\Phi _ { M 1 } } & { \\Phi _ { M 2 } } & { . . . } & { \\Phi _ { M M } } \\end{array} \\right] ,\n$$\n\nwhere interactions appear in green, while modality-specific contributions are shown in black. Note that, our choice of obtaining $\\Phi _ { i j }$ allows for a global interpretability view, accounting for opposing effects that may cancel out across dataset. This prevents score inflation and aligns with our main motivation – a general and robust measure of interaction strength. In turn, these are aggregated to obtain the total interactions contributions as well as the overall model behaviour:\n\n$$\nI n t e r a c t i o n s = \\sum _ { \\stackrel { i , j = 1 } { i \\neq j } } ^ { M } \\Phi _ { i j } , \\quad B e h a v i o r = \\sum _ { \\stackrel { i } { i , j = 1 } } ^ { M } \\Phi _ { i j } .\n$$\n\nFinally, these are used to calculate InterSHAP, which is the fraction of model behaviour that can be explained through cross-modal interactions:\n\n$$\nI n t e r S H A P = \\frac { I n t e r a c t i o n s } { B e h a v i o r } .\n$$\n\n# 3.3 Modality Contributions\n\nBased on the definition of cross-modal interactions in Equation 7, the contribution of all modalities $\\mathcal { M }$ and the contribution of each modality $\\mathcal { M } _ { i }$ to the overall result is:\n\n$$\n\\mathcal { M } = 1 - I n t e r S H A P , \\quad \\mathcal { M } _ { i } = \\frac { | \\Phi _ { i i } | } { B e h a v i o u r } .\n$$\n\n# 3.4 Local InterSHAP\n\nAfter determining the amount of interactions that a model $f$ has learned across the entire dataset, we assess the extent to which cross-modal interactions among the $M$ modalities contribute to the model’s behaviour for each sample. Thus, we apply the definition of InterSHAP directly to a sample $a$ :\n\n$$\nI _ { a } = \\frac { \\sum _ { i , j = 1 } ^ { M } \\varphi _ { i j } ^ { a } } { \\sum _ { i , j = 1 } ^ { M } \\varphi _ { i j } ^ { a } } ,\n$$\n\nwhere $\\varphi _ { i j } ^ { a } { = } | \\phi _ { i j } ( ( m _ { 1 } ^ { a } , \\ldots , m _ { M } ^ { a } ) , f ) |$ , and $i , j { \\in } \\{ 1 , \\ldots , M \\}$ . To determine if InterSHAP is effective at the level of all individual data points, we aggregate $I _ { a }$ across the entire dataset to obtain the local interaction score for the dataset as a whole:\n\n$$\nI n t e r S H A P _ { l o c a l } = \\frac { 1 } { N } \\sum _ { a = 1 } ^ { N } I _ { a } .\n$$\n\n# 3.5 Asymptotic Bound\n\nInterSHAP is characterised by a computational complexity of $O ( N ^ { M } )$ , where $N$ represents the number of samples and $M$ signifies the number of modalities (Grabisch and Roubens 1999; Lundberg et al. 2020). The constraint of exponential growth emerges with the increasing number of modalities rather than the feature count. InterSHAP’s dependency on the number of features within a modality follows a constant pattern, meaning its runtime remains unaffected by the feature count.\n\nTable 2: Local and global InterSHAP values are presented in percentages for both the XOR function and the FCNN with early fusion on HD-XOR dataset with two modalities. $\\mathbf { E M A P } _ { g a p }$ (EMAP - F1 of the model) and SHAPE indicate F1 score improvement from cross-modal interactions, with negative values signalling performance decline. For PID, the synergy value is provided, though it is not expressed in any standard unit. For all metrics, higher scores indicate greater measured cross-modal interactions. Results for the XOR function align with expectations, confirming the effectiveness of the InterSHAP implementation.   \n\n<html><body><table><tr><td rowspan=\"2\"></td><td colspan=\"2\">Uniqueness</td><td colspan=\"2\">Synergy</td><td colspan=\"2\">Redundancy</td></tr><tr><td>XOR</td><td>FCNN</td><td>XOR</td><td>FCNN</td><td>FCNN</td><td>FCNN</td></tr><tr><td>InterSHAP</td><td>0.0</td><td>0.2 ±0.1</td><td>99.7</td><td>98.0 ±0.5</td><td>38.6 ±0.5</td><td>57.8 ±1.1</td></tr><tr><td>InterSHAPlocal</td><td>1.8</td><td>3.4 ±5.2</td><td>96.9</td><td>85.8 ±12.1</td><td>37.3 ±25.0</td><td>40.0 ±13.3</td></tr><tr><td>PID</td><td>0.01</td><td>0.01 ±0.01</td><td>0.39</td><td>0.39 ±0.01</td><td>0.14 ±0.02</td><td>0.48 ±0.01</td></tr><tr><td>EMAPgap</td><td>0</td><td>0±0</td><td>49.1</td><td>43.5 ±1.0</td><td>1.8 ±0.4</td><td>0.6 ±0.4</td></tr><tr><td>SHAPE</td><td>1.6</td><td>16.5 ±0.6</td><td>33.1</td><td>27.6 ±1.5</td><td>-47.1 ±0.5</td><td>15.1 ±1.9</td></tr></table></body></html>\n\n# 4 Experimental Validation on Synthetic Data\n\nTo verify InterSHAP’s functionality, we control three factors influencing cross-modal interactions: dataset synergy (non-additive interactions between modalities), model learning capacity, and metric effectiveness.\n\n# 4.1 Setup\n\nWe utilise synthetic high-dimensional tabular datasets, generated using an XOR function (HD-XOR), with varying degrees of synergy to regulate the extent of cross-modal interactions. Each generated dataset contains 20,000 samples, with two to four modalities represented as 1D vectors consisting of around 100 features. The datasets are categorised on their degrees of synergy. Uniqueness represents datasets with no synergy, where all information is contained within a single modality. Synergy refers to datasets with complete synergy, where information is distributed across a number of modalities and can only be obtained through cross-modal interactions. Redundancy denotes datasets where the same information is present across modalities. Random applies to datasets with no meaningful information. For each number of modalities (two, three and four), we created a synergy, uniqueness and redundancy dataset as well as a random dataset for the case with two modalities.\n\nTo control for model variability in our experiments, we use the XOR function for the uniqueness and synergy datasets. The XOR function cannot be meaningfully applied to random and redundancy datasets, because it is unclear which information from each modality contributes to the solution, given the multiple possibilities. We train fully connected neural networks (FCNNs) on all datasets with three layers: input, hidden (half the input size), and output (half the hidden size). Three fusion strategies are tested: early fusion (data concatenated before input), intermediate fusion (data fused before the hidden layer), and late fusion (data fused before the output layer). Since early fusion captured the most cross-modal interactions, we use it in all subsequent FCNN experiments. Further details on the dataset generation and experimental setup are provided in Appendix A. The appendices are available in our code repository and extended version of the manuscript.\n\n# 4.2 Results\n\nVerification of InterSHAP. Generally, we anticipate that InterSHAP for the uniqueness dataset will show less crossmodal interaction (expected to be close to $0 \\%$ ) compared to the synergy dataset (expected to be close to $100 \\%$ ). For the random and redundancy datasets, it is unclear how many cross-modal interactions the FCNN should learn. In the redundancy setting, the FCNN might focus on just one modality or use all modalities since information is distributed across them. In the random setting, there is no inherent information, making the FCNN’s learning unpredictable.\n\nTable 2 presents the results of the experiments conducted on the HD-XOR datasets. InterSHAP predicts the expected amount of cross-modal interaction in the XOR function with $0 \\%$ cross-modal interaction for uniqueness and $9 9 . 7 \\%$ for synergy. In contrast, the FCNN model showed an increase in cross-modal interaction for the uniqueness dataset $0 . 2 \\%$ $\\pm \\ : 0 . 1 )$ and decrease in synergy $( 9 8 . 0 \\% \\pm 0 . 5 )$ for the synergy setting. This indicates that the FCNN does not fully capture the systematic structure underlying the dataset’s creation with the XOR function. Instead, it sometimes uses both modalities for predictions even when unnecessary, and conversely, may underutilise them when they are required.\n\nInterSHAP indicates that the FCNN attributes approximately $40 \\%$ of its behaviour to cross-modal interactions on the redundancy dataset. Especially noteworthy is the outcome on the random dataset, which shows that InterSHAP operates performance-agnostic, as only random performance could be achieved on this dataset. Nevertheless, the model appears to have learned something – though not relevant to the F1 Score – evidenced by approximately $60 \\%$ of cross-modal interactions. In summary, InterSHAP demonstrates the expected behaviour and adequately quantifies cross-modal interaction within a model.\n\nModality Scalability. To test InterSHAP’s scalability to more than two modalities, we create HD-XOR datasets with\n\n<html><body><table><tr><td></td><td>Uniqueness</td><td>Synergy</td><td>Redundancy</td></tr><tr><td>2 Modalities</td><td>0.2 ±0.1</td><td>98.0 ±0.5</td><td>38.6 ±0.5</td></tr><tr><td>3Modalities</td><td>0.6 ±0.2</td><td>88.8 ±0.5</td><td>51.9 ±0.3</td></tr><tr><td>4 Modalities</td><td>1.2 ±0.1</td><td>64.1 ±0.8</td><td>40.2 ±0.2</td></tr></table></body></html>\n\nTable 3: InterSHAP values, expressed as percentages, for FCNN with early fusion on HD-XOR datasets with two, three, and four modalities. The results indicate, that InterSHAP works for more than two modalities.\n\nthree and four modalities. The InterSHAP values for the FCNN with early fusion across uniqueness, synergy, and redundancy datasets are presented in Table 3.\n\nThe expected behaviour is that InterSHAP values for three and four modalities should be the same or similar to those for two modalities, given that the dataset creation process remains unchanged except for the synergy setting. InterSHAP exhibits the expected behaviour on the uniqueness dataset, with values ranging from $0 . 6 \\pm 0 . 2$ to $2 . 6 \\pm 0 . 6$ . Similarly, values on the redundancy setting show consistent behaviour with larger fluctuations, ranging from a minimum of $3 8 . 6 \\pm 0 . 5$ to a maximum of $5 1 . 9 \\pm 0 . 3$ . However, for the synergy setting, InterSHAP indicates a decrease from $9 8 \\%$ cross-modal interactions to $6 4 \\%$ as the number of modalities increases.\n\nExtension to Local Method. InterSHAP can be applied to individual samples, as outlined in Section 3.4. InterSHAP values are averaged over each data point for three runs and standard deviation is calculated, as shown in Table 2.\n\nOn the HD-XOR uniqueness dataset using the XOR baseline function, the local method overestimates the synergy effect, with averages of $1 . 8 \\%$ , compared to the global averages of $0 \\%$ . Conversely, for the synergy setting, the local method underestimates the synergy effect, with averages of $9 6 . 9 \\%$ locally compared to $9 9 . 7 \\%$ globally. This difference is likely attributable to the masking error source. While this error averages out when considering the entire dataset, it is more pronounced at the data point level. The difference becomes more apparent when examining the FCNN results. Particularly noteworthy is the increased standard deviation, which indicates a discrepancy between data points. In contrast to the XOR function, this difference is caused by the neural network and reflects the model’s behaviour.\n\nIn summary, we have shown that with a controlled model (XOR), cross-modal interactions are slightly overestimated for datasets with near $0 \\%$ synergy and slightly underestimated for datasets with near $100 \\%$ synergy. For an overall interaction score across the entire dataset, we recommend using the global InterSHAP. However, if the goal is to understand the interaction score at the level of individual data points, local InterSHAP is more appropriate.\n\nComparison with SOTA. To compare InterSHAP with SOTA cross-modal interaction scores, we calculated PID, EMAP and SHAPE, as shown in Table 2.\n\nPID. As expected, PID measures fewer cross-modal interactions on the uniqueness compared to the synergy dataset. However, PID is not able to quantify the extent of\n\nf(x) = 0.993 $\\begin{array} { r } { f ( x ) = 0 . 9 5 6 } \\end{array}$   \nM1 +0.49 I +0.45   \nM2 +0 M1 +0.01 I +0 M2 +0 0.6 0.8 1.0 0.6 0.8 1.0 (a) Uniqueness (b) Synergy f(x) = 0.983 $f ( x ) = 0 . 7 9 8$   \nM1 +0.27 I +0.18 I +0.19 M1 +0.09   \nM2 +0.03 M2 +0.03 0.6 0.8 1.0 E[f(X)0] .5= 0.5 0.6 0.7 0.8 (c) Redundancy (d) Random\n\ncross-modal interactions, unlike InterSHAP. In random and redundancy datasets PID also aligns with InterSHAP. Both show higher values for the random dataset than for the redundancy dataset, indicating that the model learned synergistic relationships.\n\nEMAP. For clearer comparison, we present EMAPgap, calculated as EMAP minus the model’s F1 score (see Appendix C, Table 11), which reflects the extent of cross-modal interactions. Like other metrics, EMAP also follows the expected pattern, with less synergy observed for the uniqueness dataset than for the synergy dataset. However, EMAP measures no interaction in the model on the random dataset, because it is not performance-agnostic.\n\nSHAPE. SHAPE exhibits the expected behaviour on the baseline XOR function for uniqueness and synergy datasets but not for the FCNN model. There are major deviations between the baseline XOR and FCNN results that are not due to model behaviour. While the uniqueness dataset shows similar values for the baseline XOR, the FCNN results differ significantly, with $1 6 . 5 \\pm 0 . 0 6$ for uniqueness. Additionally, the redundancy dataset results do not align with the previous three metrics. We attribute these discrepancies to the masking and calculation of base values.\n\n# 4.3 Visualisation\n\nWe designed InterSHAP to be compatible with the visualisation modules of the SHAP implementation (Lundberg and Lee 2017). Figure 2 illustrates the breakdown of model behaviour on the HD-XOR datasets into their components. This representation uniquely shows the relevance of modalities in absolute numbers in addition to cross-modal interactions. For instance, in Figure 2 (d), it is evident that modality 1 is, on average, more important for the prediction than the interactions. Additionally, the prediction on the random dataset is significantly less certain, with an average probability of only $7 9 . 8 \\%$ , compared to $9 5 { \\cdot } 9 9 \\%$ for the other datasets.\n\n<html><body><table><tr><td rowspan=\"2\"></td><td colspan=\"3\">Single-Cell</td></tr><tr><td>early</td><td>intermediate</td><td>late</td></tr><tr><td>InterSHAP PID EMAPgab</td><td>1.9 ±0.4 0.08 ±0.01</td><td>1.5 ±0.4 0.08 ±0.01</td><td>0.4 ±0.1 0.06 ±0.0 0±0</td></tr></table></body></html>\n\nTable 4: Cross-modal interactions scores on the multimodal single-cell dataset for FCNN with early, intermediate and late fusion. InterSHAP aligns with other SOTA methods, capturing the decline in cross-modal information from early to late fusion.\n\n# 5 Application to Healthcare Domain\n\nThe importance of cross-modal interactions in real-world healthcare datasets is examined by analysing two publicly available multimodal datasets with two modalities with distinct input types.\n\nThe first dataset, multimodal single-cell (Burkhardt et al. 2022), includes RNA and protein data from 7,132 single cells of $\\mathrm { C D } 3 4 +$ haematopoietic stem and progenitor cells from three donors. Its classification task spans four cell types: neutrophil progenitor (3,121), erythrocyte progenitor (3,551), B-lymphocyte progenitor (97), and monocyte progenitor (363). The data was reduced to 140 proteins and 4,000 genes and split into training (4,993), validation (1,069), and test (1,070) sets (70:15:15 ratio).\n\nThe second dataset, MIMIC-III (Johnson et al. 2016), contains anonymised clinical data from ICU patients at Beth Israel Deaconess Medical Center (2001–2012). It includes time series data (12 physiological measurements taken hourly over 24 hours, vector size $1 2 \\times 2 4$ ) and static data (5 variables like age, vector size 5). The dataset has 36,212 data points, and is split into training (28,970), validation (3,621), and test (3,621) sets (80:10:10 ratio) (Liang et al. 2021). The tasks are binary ICD-9 code prediction (group 1) and 6-class mortality prediction. Preprocessing follows Liang et al. (2021).\n\nWe use FCNN with all three fusion methods described in Section 4.1 on the multimodal single-cell dataset. For the MIMIC-III dataset, we train two models from MultiBench (Liang et al. 2021): the MultiBench baseline, which combines an MLP for patient information and a GRU encoder, and the MVAE, which uses a product-of-experts mechanism to combine latent representations from an MLP and a recurrent neural network. Table 4 presents the results concerning the amount of cross-modal interactions for each dataset.\n\n# 5.1 Multimodal Single-Cell\n\nFor the multimodal single-cell dataset, we trained three FCNNs using early, intermediate, and late fusion (performance details in Appendix C), and calculated InterSHAP, PID, and EMAP for each model. As shown in Table 4, InterSHAP values range from $0 . 4 \\%$ to $1 . 9 \\%$ , indicating minimal learned cross-modal interactions. A consistent pattern is observed whereby cross-modal interactions decrease from early to late fusion, which is in accordance with findings from synthetic datasets (see Appendix C.1).\n\n![](images/3175bb0078ef68bad2b7304dc4d7480f9741b98046bee490a276d04fb97ead7c.jpg)  \nFigure 3: SHAP visualisations of InterSHAP computed interactions and modality contributions derived from the FCNN early fusion model trained on the multimodal singlecell dataset. (a) Force plot of model’s behaviour on the whole dataset, with the $\\mathbf { \\boldsymbol { x } }$ -axis denoting the class probability of the highest probability class. (b)-(d) Breakdown by predicted class. R denotes RNA, P Protein, and I interactions.\n\nWe employ SHAP visualisations, as shown in Figure 3, to illustrate modality contributions by predicted class. A clear distinction in measured cross-modal interactions is observed between the larger classes (erythrocytes and neutrophils, comprising $9 3 . 5 \\%$ of the training dataset) and the smaller classes (monocytes and B-cells). For smaller classes, the model’s predictions depend largely on cross-modal interactions, while for larger classes, RNA is the most significant factor influencing the prediction.\n\nThe absence of ground truth in real-world datasets is a significant limitation, as the level of synergy or redundancy is unknown. To address this, additional cross-modal interaction scores, PID and EMAP, were computed for comparison with InterSHAP in Table 4. PID displayed behaviour similar to InterSHAP across the three fusion methods, with higher values for early fusion and lower for late fusion. However, PID indicated much higher synergy within the dataset, which is expected since PID focuses on the dataset rather than directly accounting for model behaviour. The authors also note that PID measures consistently exceed those of the dataset (Liang et al. 2023, 30). Therefore, to quantify the extent of cross-modal interactions utilised by a model in its predictions, InterSHAP is a more effective solution. In contrast, EMAP did not detect cross-modal interactions in any models. This difference likely arises because EMAP targets performance rather than directly measuring model behaviour. As depicted in Figure 3, smaller classes benefit significantly from interactions. However, due to their underrepresentation in the dataset, their overall impact on performance remains limited.\n\nTable 5: Cross-modal interactions scores on the MIMIC III dataset for baseline model and MVAE model from MultiBench implementation. InterSHAP aligns with other SOTA methods, capturing greater cross-modal interaction in the baseline compared to MVAE, while uniquely quantifying the proportional contribution of cross-modal interactions.   \n\n<html><body><table><tr><td></td><td colspan=\"2\">ICD-9</td><td colspan=\"2\">Mortality</td></tr><tr><td></td><td>baseline</td><td>MVAE</td><td>baseline</td><td>MVAE</td></tr><tr><td>InterSHAP</td><td>1.2 ±0.2</td><td>6.8 ±1.3</td><td>11.0 ±0.5</td><td>12.3 ±2.8</td></tr><tr><td>PID</td><td>0.06±0.01</td><td>0.09 ±0.01</td><td>0.10 ±0.01</td><td>0.11 ±0.01</td></tr><tr><td>EMAPgap</td><td>0±0</td><td>1.2 ±0.0</td><td>-0.8 ±0.1</td><td>0.9 ±0.1</td></tr><tr><td>SHAPE</td><td>0.2 ±0</td><td>0.6 ±0</td><td>0.2 ±0.2</td><td>0.7 ±0.2</td></tr></table></body></html>\n\nWe can conclude that models trained on the multimodal single-cell dataset utilise minimal cross-modal interactions to solve the classification task. Exceptions are the smaller classes, for which integrating both modalities and leveraging cross-modal interactions appears more relevant.\n\n# 5.2 MIMIC III\n\nFor the MIMIC dataset, we trained a baseline model and the MVAE model provided in the MultiBench (Liang et al. 2021) (details in Appendix B).\n\nFrom the cross-modal interactions presented in Table 5, we observe a consistent pattern: the baseline model shows fewer cross-modal interactions than the more sophisticated MVAE model. InterSHAP values vary between tasks. For deciding if the diagnosis is in ICD-9, lower interactions are measured, ranging between $1 . 2 \\%$ and $6 . 8 \\%$ , compared to the more challenging task of determining mortality, which shows higher interactions around $12 \\%$ .\n\nComparing InterSHAP with PID and EMAP reveals similar behaviour to the multimodal single-cell dataset, discussed above. PID mirrors the behaviour of InterSHAP but with higher synergy values, while EMAP does not indicate performance-relevant cross-modal scores for the ICD-9 task, but does for the mortality task. This behaviour is expected and aligns well with InterSHAP, which, as it measures performance independently, captures even small amounts of cross-modal interaction, whereas EMAP captures larger performance-relevant ones.\n\n# 6 Discussion and Future Work\n\nInterSHAP quantifies interaction strength as an interpretable percentage and works effectively with unlabelled data. Although the value of a performance-agnostic metric may be debated, InterSHAP is particularly useful for model development and debugging. For example, in our imbalanced single-cell study, smaller classes with minimal impact on the overall performance exhibited a higher reliance on cross-modal interactions, highlighting areas where modality integration could be improved. By capturing all crossmodal interactions – not just those linked to performance – InterSHAP provides deeper insights into low-performing models and can conclusively identify the absence of interactions, avoiding spurious results seen with performancedependent methods.\n\nDespite these benefits, we note some limitations of InterSHAP. The first is the exponential increase in runtime with the number of modalities, as all possible coalitions must be computed due to the lack of general approximation methods for SII. However, this is less critical in practice, as models are typically trained on fewer than ten modalities (Barua, Ahmed, and Begum 2023; Xu et al. 2024). Future work could explore approximation methods to address this computational challenge.\n\nThe second limitation is that we could not clearly demonstrate that InterSHAP functions as intended for synergy datasets with more than two modalities due to two factors: (1) The synergy in the training dataset decreases as the number of modalities increases. This phenomenon is attributed to the definition of XOR for more than two modalities: only one occurrence of 1 will result in the label being true, with the rest being 0. Consequently, if a 1 is present in two modalities, the label (i.e., false) is already known. (2) The F1 score decreases: $9 4 . 4 \\pm \\ : 0 . 3$ for two modalities, $7 9 . 2 \\pm 0 . 2$ for three modalities, and $7 6 . 4 \\pm 0 . 2$ for four modalities. This indicates that the synergy present in the dataset is not fully learned, leading to the model likely relying less on crossmodal interactions for prediction. The observed decrease is likely attributable to the two factors, although the potential contribution of InterSHAP cannot be ruled out. Although InterSHAP relies on the Shapley Interaction Index (SII) and captures pairwise modality interactions, it may not fully address complex cross-modality interactions. Future research could explore alternative interaction measures than the SII. Other unimodal indices at the feature level could be also applied to quantify cross-modal interactions in datasets with more than two modalities to overcome this limitation. Potential alternatives for evaluation include Faith-SHAP (Tsai, Yeh, and Ravikumar 2023) and the Shapley-Taylor Interaction Index (Sundararajan, Dhamdhere, and Agarwal 2020).\n\n# 7 Conclusion\n\nWe introduce InterSHAP, a multimodal explainability metric designed to quantify cross-modal interactions and modality contributions. InterSHAP provides a straightforward interpretation of cross-modal interactions, expressed as a percentage of overall model behaviour, which can be applied to multimodal learning tasks with any number of modalities. Besides explainability, it allows for investigating the capabilities of different model architectures in how they exploit multimodal interactions across real-world datasets. As such, InterSHAP overcomes the limitations of existing methods, akin to constraints to only two modalities and the lack of reliable, performance-independent methods. Our experiments on synthetic and real-world data demonstrate the ability of InterSHAP to accurately measure the presence (or absence) of cross-modal interactions, while allowing for global (dataset-level) and local (sample-level) explanations.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了多模态模型中的跨模态交互量化问题。现有方法（如PID、EMAP、SHAPE）局限于两模态、依赖标注数据且与模型性能相关，无法满足医疗等领域对多模态解释的需求（表1）。\\n> *   该问题在医疗决策等场景中至关重要，因为多模态数据的融合解释直接影响模型的可信度和临床接受度（OECD及学术研究强调）。\\n\\n> **方法概述 (Method Overview)**\\n> *   提出InterSHAP——基于Shapley交互指数（SII）的跨模态交互评分方法，可精确分离和量化单模态贡献与跨模态交互，支持任意数量模态、无需标注数据且与模型性能无关（3.2节）。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **首创性能无关的跨模态交互评分**：在合成数据集上，对无协同效应数据测得交互0.2%±0.1（接近理论值0%），完全协同数据测得99.7%（表2）。\\n> *   **多模态扩展性**：支持3-4模态分析，在四模态完全协同数据中测得64.1%交互（表3）。\\n> *   **医疗场景验证**：在单细胞分类任务中，发现小类别（如B细胞）预测依赖跨模态交互（图3），早期融合模型交互贡献（1.9%）高于晚期融合（0.4%）（表4）。\\n> *   **开源实现**：集成至SHAP可视化工具包，增强可复现性（Abstract）。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   模型行为可分解为基输出、单模态贡献和跨模态交互的累加过程（公式7-9）。\\n> *   通过Shapley交互指数（SII）量化模态间协同效应：$\\\\phi_{ij}$反映两模态交互，$\\\\phi_{ii}$反映单模态贡献（公式5）。\\n\\n> **创新点 (Innovations)**\\n> *   **先前局限**：PID仅量化数据集协同性，EMAP/SHAPE依赖性能且仅限两模态（表1）。\\n> *   **本文改进**：\\n>     1.  模态无关性：SII理论支持任意数量模态交互计算（公式5）。\\n>     2.  数据无关性：特征掩码策略避免重训练（3.1节）。\\n>     3.  局部解释：样本级交互分析（公式10-11），如单细胞数据中B细胞依赖交互（图3）。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  计算所有模态子集$S \\\\subseteq M$的模型输出$f_S(S)$（3.1节）。\\n> 2.  通过SII公式计算两两模态交互指数$\\\\phi_{ij}$（公式5）。\\n> 3.  聚合全局交互矩阵$\\\\Phi$（公式6），对角线为单模态贡献$\\\\Phi_{ii}$。\\n> 4.  计算交互总和$Interactions$与行为总和$Behavior$，最终InterSHAP得分为其比值（公式9）。\\n\\n> **案例解析 (Case Study)**\\n> *   **HD-XOR数据集**：\\n>     - 无协同（Uniqueness）：InterSHAP≈0%，模型仅用单模态（图2a）。\\n>     - 完全协同（Synergy）：InterSHAP≈99.7%，模型依赖跨模态交互（图2b）。\\n> *   **单细胞数据**：早期融合模型交互贡献（1.9%）高于晚期融合（0.4%），验证融合策略影响（表4）。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   PID（部分信息分解）、EMAP（基于扰动的交互检测）、SHAPE（性能增益评估）。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在跨模态交互检测准确性上**：在HD-XOR完全协同数据中，InterSHAP测得99.7%交互，显著优于PID（0.39）和EMAP（43.5）（表2）。\\n> *   **在多模态扩展性上**：InterSHAP支持四模态分析（表3），而PID/EMAP仅限两模态（表1）。\\n> *   **在医疗数据集上**：MIMIC-III死亡率预测任务中，InterSHAP测得MVAE模型交互贡献（12.3%±2.8）高于基线模型（11.0%±0.5），与PID趋势一致但量化更直观（表5）。\\n> *   **在计算效率上**：InterSHAP复杂度为$O(N^M)$，与模态数呈指数关系但不受特征数影响（3.5节）。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   跨模态交互 (Cross-Modal Interaction, CMI)\\n*   可解释人工智能 (Explainable AI, XAI)\\n*   Shapley交互指数 (Shapley Interaction Index, SII)\\n*   多模态学习 (Multimodal Learning, N/A)\\n*   医疗决策支持 (Medical Decision Support, N/A)\\n*   模型行为量化 (Model Behavior Quantification, N/A)\\n*   开源工具 (Open-Source Implementation, N/A)\\n*   特征掩码 (Feature Masking, N/A)\"\n}\n```"
}