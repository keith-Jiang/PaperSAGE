{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.08894",
    "link": "https://arxiv.org/abs/2412.08894",
    "pdf_link": "https://arxiv.org/pdf/2412.08894.pdf",
    "title": "SMMF: Square-Matricized Momentum Factorization for Memory-Efficient Optimization",
    "authors": [
        "Kwangryeol Park",
        "Seulki Lee"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "2024-12-12",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Artificial Intelligence Graduate School UNIST",
        "Department of Computer Science and Engineering, UNIST"
    ],
    "paper_content": "# SMMF: Square-Matricized Momentum Factorization for Memory-Efficient Optimization\n\nKwangryeol Park1, Seulki Lee2\n\n1Artificial Intelligence Graduate School UNIST, South Korea 2Department of Computer Science and Engineering, UNIST, South Korea\n\n# Abstract\n\nWe propose SMMF (Square-Matricized Momentum Factorization), a memory-efficient optimizer that reduces the memory requirement of the widely used adaptive learning rate optimizers, such as Adam, by up to $96 \\%$ . SMMF enables flexible and efficient factorization of an arbitrary rank (shape) of the first and second momentum tensors during optimization, based on the proposed square-matricization and one-time single matrix factorization. From this, it becomes effectively applicable to any rank (shape) of momentum tensors, i.e., bias, matrix, and any rank- $\\mathbf { \\nabla } \\cdot d$ tensors, prevalent in various deep model architectures, such as CNNs (high rank) and Transformers (low rank), in contrast to existing memory-efficient optimizers that applies only to a particular (rank-2) momentum tensor, e.g., linear layers. We conduct a regret bound analysis of SMMF, which shows that it converges similarly to non-memory-efficient adaptive learning rate optimizers, such as AdamNC, providing a theoretical basis for its competitive optimization capability. In our experiment, SMMF takes up to $9 6 \\%$ less memory compared to state-of-the-art memoryefficient optimizers, e.g., Adafactor, CAME, and SM3, while achieving comparable model performance on various CNN and Transformer tasks.\n\n# Code — https://github.com/eai-lab/SMMF Extended version — https://arxiv.org/abs/2412.08894\n\n# 1 Introduction\n\nTo identify the optimal weight parameters of deep neural networks, various optimization methods (Abdulkadirov, Lyakhov, and Nagornov 2023; Martens 2016; Amari 2010; Liu and Nocedal 1989) have been studied. One of the most popular approaches is SGD (Stochastic Gradient Descent) (Ruder 2016) which takes the weight update direction towards the current gradient with a learning rate uniformly applied to all weight parameters. To further improve SGD’s optimization performance, many adaptive learning rate optimizers, such as Adam (Kingma and Ba 2014) and RMSProp (Hinton, Srivastava, and Swersky 2012), have been proposed to leverage 1) history of the gradients to compute the momentum direction (Ruder 2016) and 2) the squared gradients to compute the adaptive learning rate for each weight parameter. Despite their lack of theoretical convergence guarantee in non-convex settings of many deep learning tasks, those adaptive learning rate optimizers have been empirically found to outperform SGD in practice.\n\nHowever, since the momentum value of each weight parameter, which linearly increases over the size of a deep learning model, should be maintained in memory during the whole training process, the adaptive learning rate optimizers can easily limit the size of models that can be trained on memory-constrained platforms, e.g., embedded systems. Even when training small models like Transformerbase (Vaswani et al. 2017), $1 . 4 \\mathrm { G i B }$ of memory is required. This means it would be unusable in environments with extremely limited memory devices, such as Raspberry Pi (1 GiB). To tackle the memory challenge of the adaptive learning rate optimization, several memory-efficient optimizers have been proposed. Adafactor (Shazeer and Stern 2018) and CAME (Luo et al. 2023) factorize the $2 ^ { n d }$ momentum in the form of a matrix into a set of vectors to decrease the memory space required to store momentums, achieving comparable performance to Adam. SM3 (Anil et al. 2019) reduces memory usage by approximating the similar elements of the $2 ^ { n { \\dot { d } } }$ momentum into a smaller set of variables. Although they effectively reduce the memory space of adaptive learning rate optimizers by projecting a gradient tensor onto several rank-one vectors, 1) they apply only to a specific rank (shape) and pattern of momentum tensors, 2) their memory space is still huge (1.1 GiB) making them unsuitable for memory constrained devices, and 3) their optimization performance has not been theoretically analyzed and compared to that of Adam family (Kingma and Ba 2014).\n\nIn this paper, we propose SMMF (Square-Matricized Momentum Factorization), a memory-efficient optimizer amicable to an arbitrary rank (shape) and pattern of both the $1 ^ { s t }$ and $2 ^ { n d }$ momentum tensors, i.e., a vector, matrix, and rank$d$ tensor, which reduces the amount of memory required in model optimization by up to $9 6 \\%$ compared to existing memory-efficient optimizers, e.g., Adafactor, CAME, and SM3. Unlike such existing memory-efficient optimizers, either confined to a particular 1) momentum rank (shape) (i.e., a rank-2 matrix) and/or 2) momentum pattern (i.e., a set of similar elements in a matrix) (Anil et al. 2019), the proposed SMMF performs competitive optimization without being restricted by the rank (shape) and pattern of momentums allowing the models to be trained on extremely memory constrained embedded systems from ${ \\sim } 0 . 0 0 1$ to ${ \\sim } 1$ GiB.\n\nGiven a rank- $d$ momentum tensor as $\\mathbb { R } ^ { n _ { 1 } \\times \\ldots \\times n _ { d } }$ , SMMF first finds ${ \\hat { n } } , { \\hat { m } } \\ = \\ \\arg \\operatorname* { m i n } _ { n , m } | n - m |$ such that $n m \\ =$ $\\textstyle \\prod _ { r = 1 } ^ { d } n _ { r }$ . Next, it converts the momentum $\\mathbb { R } ^ { n _ { 1 } \\times \\ldots \\times n _ { d } }$ into a matrix closest to square matrix $\\mathbb { R } ^ { \\hat { n } \\times \\hat { m } }$ with $\\hat { n }$ and $\\hat { m }$ , which we call square-matricization. Then, the matrix $\\mathbb { R } ^ { \\hat { n } \\times \\hat { m } }$ is factorized into two vectors, $\\mathbb { R } ^ { \\hat { n } \\times 1 }$ and $\\mathbb { R } ^ { 1 \\times \\hat { m } }$ at one go, by using NNMF (Non-Negative Matrix Factorization) (Finesso and Spreij 2006). Since SMMF only stores the resulting two vectors $\\mathbb { R } ^ { \\hat { n } \\times 1 }$ and $\\mathbb { R } ^ { 1 \\times \\hat { m } }$ in memory, factorized from both the $1 ^ { s t }$ and $2 ^ { n d }$ momentum $\\mathbb { R } ^ { \\hat { n } \\times \\hat { m } }$ that has been squaredmatricized from the original rank- $d$ momentum $\\mathbb { R } ^ { n _ { 1 } \\times . . . \\times n _ { d } }$ , it can decrease more memory when given high-rank momentums, e.g., the rank-4 weight tensors in CNNs. It is different from existing memory-efficient optimizers, e.g., Adafactor and CAME, that store $\\Pi _ { r = 1 } ^ { d - 2 } n _ { r }$ pairs of vectors factorized from a rank- $d$ momentuQm $\\mathbb { R } ^ { \\dot { n } _ { 1 } \\times \\dots \\times n _ { d } }$ in memory.\n\nWe analyze the regret bound of the proposed SMMF, proving that its optimization performance in a convex setup is similar to one of the Adam-based optimizers, i.e., AdamNC (Reddi, Kale, and Kumar 2019) that applies the beta schedule to Adam. To the best of our knowledge, SMMF is the first factorization-based memory-efficient optimizer that conducts a regret bound analysis; none of the existing memory-efficient optimizers, e.g., Adafactor and CAME, provides such a theoretical study. The experiments on various CNN and Transformer models (Section 5) show the competitive results substantiating our analysis.\n\n# 2 Related Work\n\nAdafactor (Shazeer and Stern 2018) factorizes the $2 ^ { n d }$ momentum matrix via Non-Negative Matrix Factorization (NNMF) (Finesso and Spreij 2006) that decomposes a nonnegative matrix into two vectors by differentiating the Idivergence (Lee and Seung 1999). Theoretically, it reduces the memory complexity of the $2 ^ { n d }$ momentum in the form of a non-negative matrix, i.e., $V \\in \\mathbb { R } ^ { n \\times m }$ , from $\\mathcal { O } ( n m )$ to $\\mathcal { O } ( n + m )$ with the two factorized vectors. Empirically, it shows comparable optimization to Adam on Transformers. CAME (Luo et al. 2023), a variant of Adafactor, is proposed as a memory-efficient optimizer for large batch optimization. To alleviate the unstable behavior of Adafactor, it introduces the factorized confidence term that guides the optimization direction, empirically achieving faster convergence on language models (Raffel et al. 2020; Radford et al. 2019) at the cost of using more memory than Adafactor. Since CAME also requires a momentum to be a nonnegative matrix to be factorized with NNMF, it slices a highrank weight tensor, appearing in CNN models such as MobileNet (Dong et al. 2020), into multiple matrices and factorize them separately. Hence, given a rank- $\\textit { d } 2 ^ { n d }$ momentum $V \\in \\mathbb { R } ^ { n _ { 1 } \\times \\ldots \\times n _ { d } }$ , the memory complexity of CAME becomes $\\begin{array} { r } { \\mathcal { O } ( ( n _ { d - 1 } + n _ { d } ) \\prod _ { r = 1 } ^ { d - 2 } n _ { r } ) } \\end{array}$ , which is similar to Adafactor. SM3 (Anil et a l. 2019), unlike Adafactor and its variants such as CAME, applies the min-max scheme to approximate the similar elements of the $2 ^ { n d }$ momentum to a smaller set of variables. It shows competitive optimization performance to Adam and Adafactor on Transformers that exhibit a grid pattern of similar elements in their weight matrices. Given a rank- $d$ momentum tensor $\\mathbb { R } ^ { n _ { 1 } \\times \\ldots \\times n _ { d } }$ , the memory complexity of SM3 becomes O( rd= $\\scriptstyle { \\mathcal { O } } ( \\sum _ { r = 1 } ^ { d } n _ { r } )$ if similar elements appear on each axis of the weight tensor, which can be found in some Transformer weight matrices (Anil et al. 2019).\n\nAlgorithm 1: Overall SMMF applied to each layer. The elements of $r , c , M , V$ , and $\\boldsymbol { s }$ are initially set to zeros.   \n\n<html><body><table><tr><td>Input:Stept,total step T,model f(·)with rank-d weight tensor Wt ∈ Rn1 X..xnd,learning-rate nt,regularization constant , 1st and 2nd momentum hyper-parameters β1,t and β2,t. for t=1 to Tdo Gt=Vf(Wt-1) Gt = Square-Matricization(Gt,n1...nd) [Algo 2] Mt-1 = Decompression(rMt-1,CMt-1,SMt-1) [Algo 3] Vt-1 = Decompression(rVt-1, CVt-1,1) [Algo 3]</td></tr><tr><td>Mt=β1,tMt-1+(1-β1,t)Gt Vt=β2,tVt-1+(1-β2,t)G² (rMt,CMt,SMt)=Compression(Mt) [Algo 4] ） 二 Compression(Vt) [Algo 4] U=Reshape(Mt/√Vt+∈,n1...nd) Wt=Wt-1-ntU end for (rVt, CVt</td></tr></table></body></html>\n\nAlthough those existing memory-efficient optimizers effectively reduce the memory requirement and perform competitive optimization primarily on the Transformer architectures (Vaswani et al. 2017) by projecting the gradient onto rank-1 vectors, each optimizer has limitations. First, since Adafactor and CAME rely on matrix factorization (Finesso and Spreij 2006), a momentum tensor should be first sliced into multiple matrices before being factorized, degrading the memory reduction effect given a high-rank momentum tensor. Next, SM3 needs sets of similar elements in a momentum tensor to perform effective optimization, neither easy nor guaranteed to find in the huge weight parameter space of many deep neural networks. However, unlike Adafactor and CAME, the proposed SMMF applies one-time single matrix factorization to any rank (shape) of momentum tensors based on the proposed square-matricization without the memory increase caused by tensor-to-matrices slice. Also, since the proposed SMMF utilizes NNMF, it does not require strong patterns on the weight parameter space, readily applying to an arbitrary pattern of weight tensors, in contrast to SM3 that assumes the existence of specific element patterns on each axis in a weight tensor.\n\n# 3 SMMF (Square-Matricized Momentum Factorization)\n\nAlgorithm 1 shows the overall procedure of the proposed SMMF (Square-Matricized Momentum Factorization), applied to the weight tensor (momentum) at each layer of the model. In short, given the $1 ^ { s t }$ and $2 ^ { n d }$ momentum tensors as $M , V ~ \\in ~ \\bar { \\mathbb { R } ^ { n _ { 1 } \\times \\ldots \\times n _ { d } } }$ , SMMF reduces the memory complexity required for optimization into $\\mathcal { O } _ { M } ( \\hat { n } + \\hat { m } )$ and $\\mathcal { O } _ { V } \\bar { ( n + \\hat { m } ) }$ for $M$ and $V$ , respectively, with ${ \\hat { n } } , { \\hat { m } } =$ arg $\\mathrm { m i n } _ { n , m } | n \\mathrm { ~ - ~ } m |$ such that $\\begin{array} { l r } { n m } & { { } = } & { \\prod _ { r = 1 } ^ { d } n _ { r } } \\end{array}$ where $n _ { r } , n , m , { \\hat { n } }$ , and $\\hat { m }$ are in $\\mathbb { N }$ . It first transforms $M , V \\in$ $\\mathbb { R } ^ { n _ { 1 } \\times \\ldots \\times n _ { d } }$ into a matrix closest to the square (squarematricization), i.e., $M , V \\in \\mathbb { R } ^ { \\hat { n } \\times \\hat { m } }$ where $\\hat { n } \\simeq \\hat { m }$ , and then applies NNMF (Algorithm 5) to $M , V \\in \\mathbb { R } ^ { \\hat { n } \\times \\hat { m } }$ as onetime single matrix factorization (compression). Since the $1 ^ { s t }$ momentum $M$ can be negative, unlike the $2 ^ { n d }$ momentum $V$ that is non-negative, we apply NNMF to the absolute values of $M$ and store the sign of each element of $M$ as a separate set of binary values (1-bit). Although it incurs extra memory overhead $\\mathcal { O } _ { M } ( \\hat { n } \\hat { m } )$ on top of $\\mathcal { \\bar { O } } _ { M } ( \\hat { n } + \\hat { m } )$ and $\\mathcal { O } _ { V } ( \\hat { n } + \\hat { m } )$ , its memory footprint is 32 times smaller than storing the original $M$ with the 32-bit floating-point format. The following subsections describe each step of SMMF.\n\n<html><body><table><tr><td>Algorithm 2: Square-Matricization.It needs to be calculated only once before starting model training (optimization).</td></tr><tr><td>Input: Rank-d tensor G ∈ Rn1 ×..×nd and the length of each axis n1...nd Output: Reshaped matrix G ∈ Ri ×m</td></tr><tr><td>N = II=1 ni s = [VN] fori=sto1 do if (Nmod i)==0 then</td></tr><tr><td>G = Reshape(G,n,m) where n = N/i and m = i; break end if</td></tr></table></body></html>\n\n# 3.1 Square-Matricization\n\nIn Algorithm 1, SMMF first obtains a rank- $d$ gradient $G _ { t } \\in$ $\\mathbb { R } ^ { n _ { 1 } \\times \\ldots \\times n _ { d } }$ for the weight and bias tensor at each layer of the model and converts it into a matrix closest to a square matrix $\\bar { \\pmb { G } } _ { t } \\in \\mathbb { R } ^ { \\hat { n } \\times \\hat { m } }$ where $\\hat { n } \\simeq \\hat { m }$ , for factorization, naturally leading to the square-matricization of the $1 ^ { s t }$ and $2 ^ { n d }$ momentum, $M$ and $V$ . To this end, we propose a squarematricization method that reshapes $G _ { t } \\ \\in \\ \\bar { \\mathbb { R } ^ { n _ { 1 } \\times \\ldots \\times n _ { d } ^ { - } } }$ into a matrix closest to a square matrix $\\bar { \\pmb { G } } _ { t } \\in \\mathbb { R } ^ { \\hat { n } \\times \\hat { m } }$ such that $\\begin{array} { r } { n m \\ = \\ \\prod _ { r = 1 } ^ { d } n _ { r } } \\end{array}$ and $( { \\hat { n } } , { \\hat { m } } ) \\ = \\ \\operatorname { a r g m i n } _ { n , m } ( n + m ) \\ =$ ar $\\mathrm { g } \\mathrm { m i n } _ { n , m } | n - m |$ , where $\\hat { n } , \\hat { m } , n , m \\in \\mathbb { N }$ . Following theorems show that the square-matricization of $\\bar { G } _ { t } \\in \\mathbb { R } ^ { \\hat { n } \\times \\hat { m } }$ , i.e., having $\\hat { n } \\simeq \\hat { m }$ , also minimizes $\\hat { n } + \\hat { m }$ .\n\nTheorem 3.1. Given $n _ { r } \\in \\mathbb { N } ,$ , $r \\in [ 1 , d ]$ , and a constant $\\begin{array} { r } { N = \\prod _ { r = 1 } ^ { d } n _ { r } } \\end{array}$ , then $\\begin{array} { r } { \\prod _ { r = 1 } ^ { d - 2 } n _ { r } ( n _ { d - 1 } + n _ { d } ) } \\end{array}$ decreases if both $n _ { d - 1 }$ and $n _ { d }$ increase (Proof provided in Appendix $C$ ).\n\nCorollary 3.1.1. Given $\\begin{array} { r } { N = \\prod _ { r = 1 } ^ { d } n _ { r } } \\end{array}$ , there exist $N = { \\hat { n } } { \\hat { m } }$ such that $\\begin{array} { r } { \\hat { n } + \\hat { m } = \\operatorname* { m i n } \\prod _ { r = 1 } ^ { d - 2 } n _ { r } ( n _ { d - 1 } + n _ { d } ) } \\end{array}$ , $( { \\hat { n } } , { \\hat { m } } ) \\in \\mathbb { N }$ . Theorem 3.2. Given $n _ { r } , n , m \\in \\mathbb { N }$ , $r \\in [ 1 , d ]$ , $N \\ =$ $\\begin{array} { r } { \\prod _ { r = 1 } ^ { d } n _ { r } = n m } \\end{array}$ , and $n \\leq m$ , then $\\begin{array} { r } { \\hat { n } , \\hat { m } = \\arg \\operatorname* { m i n } _ { n , m } ( n + } \\end{array}$ $m \\big | = \\arg \\operatorname* { m i n } _ { n , m } \\mathopen { } \\mathclose \\bgroup \\left| n - m \\aftergroup \\egroup \\right|$ (Proof provided in Appendix $D$ ).\n\nFrom Corollary 3.1.1, square-matricizing $\\begin{array} { r l } { G _ { t } } & { { } \\in \\mathbf { \\Gamma } } \\end{array}$ $\\mathbb { R } ^ { n _ { 1 } \\times \\ldots \\times n _ { d } }$ into $\\bar { G _ { t } } \\in \\mathbb { R } ^ { \\hat { n } \\times \\hat { m } }$ reduces the memory complex∈- ity since $\\begin{array} { r } { \\prod _ { r = 1 } ^ { d - 2 } n _ { r } ( n _ { d - 1 } + n _ { d } ) \\leq \\prod _ { r = 1 } ^ { d } n _ { r } } \\end{array}$ . Also, based on Theorem 3.2, minimizing $| n - m |$ is equivalent to minimizing $n + m$ . From this, we derive the square-matricization algorithm (Algorithm 2) that finds $\\hat { n }$ and $\\hat { m }$ , which minimizes $\\boldsymbol { \\hat { n } } + \\boldsymbol { \\hat { m } }$ by solving $( { \\hat { n } } , { \\hat { m } } ) \\ = \\ \\operatorname { a r g m i n } _ { n , m } | n - m |$ .\n\nInput: Factorized vectors $\\boldsymbol { r } \\in \\mathbb { R } ^ { \\hat { n } \\times 1 }$ and $\\pmb { c } \\in \\mathbb { R } ^ { 1 \\times \\hat { m } }$ , and a binary sign matrix $S \\in \\{ 0 , 1 \\} ^ { \\hat { n } \\times \\hat { m } }$ . $[ M _ { i , j }$ is $i ^ { t h }$ row and $j ^ { t h }$ column element in a matrix $M \\in \\mathbb { R } ^ { \\hat { n } \\times \\hat { m } } ]$ Output: Decompressed matrix $\\boldsymbol { M } \\in \\mathbb { R } ^ { \\hat { n } \\times \\hat { m } }$ is the outer product operator]   \n\n<html><body><table><tr><td>Algorithm 3: Decompression.</td></tr></table></body></html>\n\nBy reshaping a rank- $d$ gradient into a matrix closest to a square matrix through square-matricization, it becomes able to perform one-time single matrix factorization, which minimizes the memory complexity of $\\mathcal { O } _ { M , V } ( \\hat { n } \\ + \\ \\hat { m } )$ for the $1 ^ { s t }$ and $2 ^ { n d }$ momentum $M$ and $V$ . Thus, the memory usage of SMMF becomes smaller than those of existing memory-efficient optimizers (Shazeer and Stern 2018; Luo et al. 2023) that should slice a high-rank tensor into a bunch of matrices for multiple factorizations, i.e., $\\begin{array} { r } { \\mathcal { O } ( \\prod _ { r = 1 } ^ { d - 2 } n _ { r } ( n _ { d - 1 } + n _ { d } ) ) } \\end{array}$ given $V \\in \\mathbb { R } ^ { n _ { 1 } \\times . . . \\times n _ { d } }$ . That is, the memory complexity of CNNs having high-rank gradient tensors grows over the rank of gradients (Shazeer and Stern 2018; Luo et al. 2023), whereas that of SMMF does not.\n\n# 3.2 Decompression and Compression\n\nDecompression $$ Compression. After square-matricizing the gradient, SMMF decompresses the $1 ^ { s t }$ and $2 ^ { n d }$ momentum from two vectors factorized at the previous step $t { - } 1$ to update the momentums, as in Algorithm 1. Then, it compresses the momentums obtained at step $t$ into vectors and updates the weight $W$ using the decompressed momentums. We call this process the decompression $$ compression scheme, in which the gradient $\\hat { G } _ { t }$ at the current step $t$ is reflected to the $1 ^ { s t }$ and ${ \\bar { 2 } } ^ { n d }$ momentum before it is factorized, enabling the precise update of the weight.\n\nThe significance of information in the current gradient, e.g., tensor patterns, has been emphasized in some previous works (Anil et al. 2019). Since reshaping and factorization of the gradient may ruin some potentially important patterns of the momentums contained in the previous gradient $G _ { \\tau < t }$ , reflecting the current gradient $\\pmb { G } _ { t }$ (and its pattern) is crucial in model performance. Thus, by performing decompression with $\\pmb { G } _ { t }$ prior to updating and compressing (factorizing) $\\boldsymbol { M } _ { t }$ and $\\mathbf { \\nabla } V _ { t }$ , it is expected to improve the optimization performance. On the contrary, existing optimizers, such as Adafactor, first compress $\\pmb { G } _ { t }$ and then updates momentums through decompression, which we call the compression $$ decompression scheme. In this scheme, some useful information of $\\pmb { G } _ { t }$ would be lost by compression (factorization), implying that an optimizer can hardly utilize the intact state of $\\pmb { G } _ { t }$ , likely to degrade the model performance.\n\nDecompression. First, in the decompression phase (Algorithm 3), the $1 ^ { s t }$ and $2 ^ { n d }$ momentum, $\\{ | \\hat { M } _ { t - 1 } | , \\hat { V } _ { t - 1 } \\}$ $\\in \\mathbb { R } ^ { \\hat { n } \\times \\hat { m } }$ , are defactorized from the two vectors for each, i.e., $\\{ r _ { M _ { t - 1 } } , c _ { M _ { t - 1 } } \\}$ for $| \\hat { M } _ { t - 1 } |$ and $\\{ r _ { V _ { t - 1 } } , ~ c _ { V _ { t - 1 } } \\}$ for $\\hat { V } _ { t - 1 }$ , which have been factorized from the squarematricized momentums at the previous step $t { - } 1$ , by performing outer product between them. To apply NNMF to the $1 ^ { s t }$ momentum $\\hat { M } _ { t }$ , its sign values are stored as a binary matrix $S _ { M _ { t - 1 } } \\in \\{ 0 , 1 \\} ^ { \\hat { n } \\times \\hat { m } }$ in the compression phase and restored back to the defactorized $1 ^ { s t }$ momentum $\\hat { M } _ { t }$ in an element-wise manner. Then, $\\boldsymbol { M } _ { t }$ and $\\mathbf { \\nabla } _ { V _ { t } }$ are updated by using two coefficients $\\beta _ { 1 , t }$ and $\\beta _ { 2 , t }$ .\n\n<html><body><table><tr><td>Algorithm 4: Compression.1d is a vector that all elements areoneanditslengthisd.</td></tr><tr><td>Input: Matrix M ∈ Rˆ×m to be factorized Output: Factorized vectors r ∈ Rn×1 and c ∈ R1×m,</td></tr><tr><td>and binary sign matrix S ∈ {0,1}n ×m ʃ1，ifMi,j≥0 (r,c)=(|M|1m,1π|M|) Si,j</td></tr><tr><td>0,otherwise’ (r/(1πr)，ifn≤m {c/(clm)，ifn>m r C r， otherwise' otherwise</td></tr></table></body></html>\n\nCompression. Next, in the compression phase (Algorithm 4), the sign values of $\\boldsymbol { M } _ { t }$ is stored as a binary sign matrix $S _ { M _ { t } }$ for the next step of optimization, and both $\\lvert \\boldsymbol { M } _ { t } \\rvert$ and $\\mathbf { \\nabla } _ { V _ { t } }$ are factorized into two vectors for each, i.e., $\\{ \\pmb { r } _ { M _ { t } } , \\pmb { c } _ { M _ { t } } \\}$ and $\\{ r _ { V _ { t } } , c _ { V _ { t } } \\}$ , respectively. To reduce the computation required for compression, it determines whether to normalize $\\boldsymbol { r }$ or $c$ based on the shape of the matrix. Weight Update. Lastly, the weight update term $\\pmb { U } \\in \\mathbb { R } ^ { \\hat { n } \\times \\hat { m } }$ is computed as $M _ { t } / \\dot { \\sqrt { V _ { t } + \\epsilon } }$ and reshaped back into the original dimension of the gradient $G _ { t } \\in \\mathbb { R } ^ { n _ { 1 } \\times \\ldots \\times n _ { d } }$ to update the weight $\\mathbf { \\nabla } _ { W _ { t } }$ .\n\n# 3.3 Time (Computation) Complexity of SMMF\n\nThe time complexity of SMMF consists of two parts, i.e., square-matricization and decompression/compression. First, computing $\\hat { n }$ and $\\hat { m }$ for square-matricization (Algorithm 2) is $\\bar { \\mathcal { O } } ( \\sqrt { N } )$ , where $N$ is the number of elements in the momentum tensor. However, this computational overhead is negligible since $\\hat { n }$ and $\\hat { m }$ are calculated only once before starting model training (optimization). Next, the time complexity of decompression (Algorithm 3) and compression (Algorithm 4) are both $\\mathcal { O } ( N )$ , which is asymptotically equivalent to existing memory-efficient optimizers, i.e., Adafactor and CAME. While taking a similar computational complexity to existing memory-efficient optimizers (Shazeer and Stern 2018; Luo et al. 2023), SMMF is able to save up to $96 \\%$ of memory, as shown in Section 5.\n\n# 4 Regret Bound Analysis\n\nWe analyze the convergence of SMMF by deriving the upper bound of the regret that indicates an optimizer’s convergence (Kingma and Ba 2014). The regret $R ( T )$ is defined as the sum of differences between two convex functions $f _ { t } ( \\pmb { w } _ { t } )$ and $f _ { t } ( w ^ { * } )$ for all $t \\in [ 1 , T ]$ , where $\\boldsymbol { w } ^ { * }$ is an optimal point.\n\n$$\nR ( T ) = \\sum _ { t = 1 } ^ { T } ( f _ { t } ( \\pmb { w } _ { t } ) - f _ { t } ( \\pmb { w } ^ { \\ast } ) )\n$$\n\nSince SMMF factorizes (compresses) the momentum tensors, unlike Adam, we introduce some compression error terms in our analysis as follows. First, $\\hat { m } _ { t }$ and $\\hat { \\pmb { v } } _ { t }$ are the decompressed and vectorized vectors of the $1 ^ { s t }$ and $2 ^ { n d }$ momentum $\\boldsymbol { M } _ { t }$ and $\\mathbf { \\nabla } _ { V _ { t } }$ , containing $e _ { m , t }$ and $\\boldsymbol { e } _ { v , t }$ , which denote compression errors of $\\boldsymbol { M } _ { t }$ and $\\mathbf { \\nabla } V _ { t }$ , respectively, i.e., $\\mathbf { e } _ { m , t } = { \\hat { m } } _ { t } - m _ { t }$ and $\\boldsymbol { e } _ { v , t } = \\hat { \\boldsymbol { v } } _ { t } - \\boldsymbol { v } _ { t }$ . Similarly, we also define $\\tilde { e } _ { m , t } , \\tilde { e } _ { v , t } , \\tilde { g } _ { m , t } , \\tilde { g } _ { v , t }$ , and $\\hat { \\pmb { g } } _ { m , t }$ . The detailed definitions are given in Lemmas E.3 and E.4 in Appendix E.\n\nTheorem 4.1. Let ${ \\mathbf { } } w _ { t }$ and $\\pmb { v } _ { t }$ be the vectorized $\\boldsymbol { W } _ { t }$ and $\\mathbf { \\nabla } _ { V _ { t } }$ , respectively, in Algorithm $^ { l }$ , and $\\eta _ { t } ~ = ~ \\eta / { \\sqrt { t } } ,$ , $\\beta _ { 1 } ~ = ~ \\beta _ { 1 , 1 }$ , $\\beta _ { 2 } = \\beta _ { 2 , 1 }$ , $\\beta _ { 1 , t } ~ \\le ~ \\beta _ { 1 }$ for all $t \\in [ 1 , T ]$ , and $\\zeta _ { 1 } > 0$ . We assume that $| | \\pmb { w } _ { t } - \\pmb { w } ^ { * } | | _ { 2 } \\ \\leq \\ D$ , $| | \\pmb { w } _ { k } - \\pmb { w } _ { l } | | _ { \\infty } \\le D _ { \\infty }$ , and all $\\textbf { \\em w }$ is in $\\mathcal { F }$ where $D$ is the diameter of the feasible space $\\mathcal { F }$ . Furthermore, let $\\beta _ { 1 , t }$ and $\\beta _ { 2 , t }$ follow the following conditions, where the conditions (a) and $( b )$ are from (Reddi, Kale, and Kumar 2019).\n\n$$\n\\begin{array} { l } { { ( a ) \\eta _ { t - 1 } \\sqrt { v _ { t , i } } \\geq \\eta _ { t } \\sqrt { v _ { t - 1 , i } } } } \\\\ { { ( b ) \\frac { 1 } { \\eta _ { t } } \\sqrt { \\displaystyle \\sum _ { j = 1 } ^ { t } \\prod _ { k = 1 } ^ { t - j } \\beta _ { 2 , t - k + 1 } ( 1 - \\beta _ { 2 , j } \\tilde { g } _ { v , j , i } ) } \\geq \\frac { 1 } { \\zeta _ { 2 } } \\sqrt { \\displaystyle \\sum _ { j = 1 } ^ { t } \\tilde { g } _ { v , j , i } } } } \\end{array}\n$$\n\nGiven the above conditions, the upper bound of the regret $R ( T )$ on a convex function becomes:\n\n$$\n\\begin{array} { r l r } {  { R ( T ) \\leq \\sum _ { i = 1 } ^ { d } \\frac { D ^ { 2 } \\sqrt { { \\pmb v } _ { T , i } } } { 2 \\eta _ { T } ( 1 - \\beta _ { 1 } ) } + \\sum _ { t = 1 } ^ { T } \\sum _ { i = 1 } ^ { d } \\frac { \\beta _ { 1 , t } D _ { \\infty } ^ { 2 } \\sqrt { { \\pmb v } _ { t , i } } } { 2 \\eta _ { t } ( 1 - \\beta _ { 1 , t } ) } } } \\\\ & { } & { + \\ \\frac { \\zeta _ { 1 } \\zeta _ { 2 } ( 1 + \\beta _ { 1 } ) \\sqrt { d } } { ( 1 - \\beta _ { 1 } ) ^ { 3 } } \\sqrt { \\sum _ { i = 1 } ^ { d } \\lvert | { \\pmb g } _ { 1 : T , i } \\rvert | _ { 2 } } . } \\end{array}\n$$\n\nThe full proof of Theorem 4.1 is provided in Appendix E. It shows that SMMF has a similar regret bound ratio to AdamNC (Reddi, Kale, and Kumar 2019) (a variant of Adam), i.e., $\\mathcal { O } ( \\sqrt { T } )$ where $T$ is the total optimization steps, under the conditions (a) and (b). It allows SMMF to perform consistent optimization comparable to Adam-based and existing memory-efficient optimizers, as shown in the experiment (Section 5). The conditions (a) and (b) can be satisfied by properly scheduling $\\beta _ { 2 , t }$ (Reddi, Kale, and Kumar 2019).\n\n# 5 Experiment\n\nWe implement the proposed SMMF using PyTorch (Paszke et al. 2017), which is available both on an GitHub and in Appendix M. For evaluation, we compare the memory usage and optimization performance of SMMF against four (memory-efficient) optimizers, i.e., Adam, Adafactor, SM3, and CAME, with two types of deep learning tasks: 1) CNN models for image tasks and 2) Transformer models for NLP tasks. The detailed experimental setups and training configurations are provided in Appendix L.\n\nCNN-based Models and Tasks. We apply the five optimizers, including SMMF, to two representative image tasks, i.e., image classification and object detection, and evaluate them by 1) training ResNet-50 (He et al. 2016) and MobileNetV2 (Dong et al. 2020) on CIFAR100 (Krizhevsky, Hinton et al. 2009) and ImageNet (Russakovsky et al. 2015), and 2) training YOLOv5s and YOLOv5m (Ultralytics 2021) on COCO (Lin et al. 2015).\n\nTransformer-based Models and Tasks. For NLP tasks, we train several Transformer-based models from small scale to large scale with three training methods, i.e., full-training, pre-training, and fine-tuning. We 1) full-train Transformerbase and big models (Vaswani et al. 2017) on WMT32k (Bojar et al. 2014), 2) pre-train BERT (Devlin et al. 2018), GPT-2 (Radford et al. 2019), and T5 (Raffel et al. 2020) on BookCorpus (Zhu et al. 2015) & Wikipedia, and 3) fine-tune BERT, GPT-2, T5-small, and LLaMA-7b (Touvron et al. 2023a) on QNLI, MNLI, QQP, STSB, and MRPC (Wang et al. 2018) datasets. To fine-tune LLaMA7b, we use LoRA (Hu et al. 2021). Additionally, we train various Transformer models on other NLP tasks such as question-answering (Rajpurkar et al. 2016), translation (Bojar et al. 2016), and summarization (Narayan, Cohen, and Lapata 2018). Due to the page limit, we provide the detailed experiment results in Appendix K.\n\nTable 1: (First and second tables) Image classification: the optimizer memory usage (MiB) including the binary sign matrix $\\boldsymbol { S _ { M } }$ , end-to-end training (one-batch) memory usage (MiB) at 100 iterations, and top-1 validation accuracy of MobileNetV2 and ResNet-50 on CIFAR100 and ImageNet. (Third table) Object detection: the optimizer memory usage (MiB) including $\\boldsymbol { S } _ { M }$ , end-to-end training (onebatch) memory usage (MiB) at 100 iterations, and validation mAP50 of YOLO (v5s and $\\mathrm { v } 5 \\mathrm { m }$ ) on COCO.   \n\n<html><body><table><tr><td colspan=\"6\">CNNModelsandTasks (OptimizerandEnd-to-EndMemory[MiB]),ModelPerformance</td></tr><tr><td>Dataset</td><td></td><td>(1) CIFAR1OO (Image Classification)</td><td></td><td></td><td></td></tr><tr><td>Model</td><td>Adam</td><td>Adafactor</td><td>SM3</td><td>CAME</td><td>SMMF</td></tr><tr><td>(V2)</td><td>73.6</td><td>MobileNet(18，36)(26，43) 69.4</td><td>70.0</td><td>66.2</td><td>9，27(43，60)(0.7，19) 74.1</td></tr><tr><td>ResNet (50)</td><td>72.4</td><td>72.9</td><td>73.8</td><td>67.2</td><td>(184,366)(215,397)(93,227)(340,526)(3.5，185) 74.5</td></tr><tr><td>Dataset</td><td colspan=\"5\">(2)ImageNet (Image Classification)</td></tr><tr><td>(V2)</td><td>68.6</td><td>69.3</td><td>67.6</td><td>69.5</td><td>MobileNet(27，54)(30，58)(14，41)(47，75)(0.8，28) 69.5</td></tr><tr><td>ResNet (50)</td><td>73.7</td><td>(195，394)(220，419) 69.5</td><td>75.8</td><td>72.3</td><td>(99，298)(346,546)(3.7，197) 73.7</td></tr><tr><td>Dataset</td><td colspan=\"6\">(3) COCO(ObjectDetection)</td></tr><tr><td>YOLO (v5s)</td><td>52.7</td><td>53.6</td><td>50.3</td><td>53.3</td><td>(57,121)(61，92)(28，92)(94,159)(1.4,65) 54.1</td></tr><tr><td>YOLO (v5m)</td><td>58.0</td><td>58.8</td><td>57.0</td><td>59.3</td><td>(168,340)(174,258)(84,260)(267,438)(3.4，176) 59.6</td></tr></table></body></html>\n\nOptimization Time Measurement. We measure the optimization time of five optimizers with the CNN and Transformer models, i.e., 1) MobileNetV2 and ResNet-50 on ImageNet, 2) Transformer-base and big on WMT32K.\n\n# 5.1 CNN-based Models and Tasks\n\nImage Classification. The first and second tables of Table 1 summarize the optimizer memory usage, end-to-end training (one-batch) memory usage, and top-1 classification accuracy of MobileNetV2 and ResNet-50 on CIFAR100 and ImageNet, respectively, showing that SMMF achieves comparable accuracy among the five optimizers with the lowest memory footprint. For instance, for ResNet-50 on ImageNet, SMMF substantially reduces the optimizer memory usage from 220 to 3.7 MiB ( $5 9 \\mathrm { x }$ smaller) compared to Adafactor while achieving a higher accuracy $( 7 3 . 7 \\% )$ . Its memory usage is also smaller than the other two memory\n\nO 69 50 67 230 Adam CAME Adam CAME   \n65 SM3 SMMF SM3 SMMF   \n63 Adafactor Adafactor   \n61 175 250 325 40 75 150 225 300 Epoch 0 Epoch\n\nefficient optimizers, i.e., SM3 (99 vs. 3.7 MiB) and CAME (346 vs. 3.7 MiB). On the other hand, both Adafactor and CAME take more memory than not only SMMF but also Adam due to the overhead of slicing the momentum tensor into multiple matrices for factorization based on the last two rank of the tensor corresponding to the size of a CNN kernel (i.e., $C _ { k } ^ { i } \\times C _ { k } ^ { o } \\times H _ { k } \\times \\mathbf { \\bar { W } } _ { k } )$ . Since $H _ { k }$ and $W _ { k }$ are usually small, e.g., $H _ { k } = W _ { k } = 1$ , 3 or 5, whereas $C _ { k } ^ { i }$ and $C _ { k } ^ { o }$ are large, e.g., 512, the memory reduction effect of both Adafactor and CAME becomes marginal in CNNs. Figure 1 (left) plots the top-1 validation accuracy of MobileNetV2 on ImageNet over training steps.\n\nObject Detection. The third table of Table 1 summarizes the optimizer memory usage, end-to-end training (one-batch) memory usage, and the model performance metric (i.e., mAP50) of YOLOv5s and YOLOv5m on the COCO object detection task. Figure 1 (right) shows the mAP50 of YOLOv5s on COCO over training steps. Similar to image classification tasks, SMMF achieves comparable mAP50, e.g., 59.6 in YOLOv5m and 54.1 in YOLOv5s, over all the four optimizers with the lowest memory up to $7 8 \\mathrm { { x } }$ reduction, e.g., 267 vs. 3.4 MiB. This result implies that YOLOv5s (65 MiB) can be trained on an off-the-shelf memory-constrained device, e.g., Coral Dev Micro (64 MiB) (Google 2022), along with other memory-efficient training methods such as gradient checkpointing (Chen et al. 2016).\n\nThese experiment results show that SMMF performs consistent and reliable optimization for both image classification and object detection tasks with different CNN models, taking the smallest memory compared to existing memoryefficient optimizers, i.e., Adafactor, SM3, and CAME.\n\n# 5.2 Transformer-based Models and Tasks\n\nFull-training. As shown in Table 2, SMMF achieves comparable perplexity with up to $7 0 \\mathrm { x }$ smaller optimizer memory when full-training (i.e., training models from scratch) both the Transformer-base and big models. Since SMMF square-matricizes both the $1 ^ { s t }$ and $2 ^ { n d }$ momentums and factorizes them, its memory usage is at least half lower than the other memory-efficient optimizers, i.e., Adafactor, SM3, and CAME. Given that most Transformer architectures consist of two-dimensional matrices, e.g., attention and linear layers, the memory reduction effect of SM3 that is good at compressing a high-rank tensor, becomes insignificant, making its memory usage similar to Adafactor and CAME. On the other hand, SMMF can effectively reduce memory required to factorize a two-dimensional matrix with square-matricization, e.g., saving $69 \\%$ of memory for the embedding weight matrix of BERT, as the weight matrix in $\\mathbb { R } ^ { 3 0 5 2 2 \\times 7 6 8 }$ becomes $\\mathbb { R } ^ { 5 0 8 7 \\times 4 6 0 8 }$ . Although the square-matricization may spoil the gradient pattern of the Transformer’s weight parameters (Anil et al. 2019), SMMF performs comparable optimization (e.g., 6.7 perplexity for WMT32k) using much less memory by fully utilizing the intact latest gradient before it is compressed. It is possible by taking the proposed decompression $$ compression scheme that first reflects the intact gradient pattern, if any, to the momentum and then performs factorization, unlike existing optimizers, e.g., Adafactor, SM3, and CAME, which apply the compression $$ decompression scheme. Figure 2 (left) shows the test perplexity of the Transformer-base model full-trained on WMT32k from scratch.\n\nTable 2: Full-training: the optimizer memory usage (GiB), including the $\\boldsymbol { S _ { M } }$ , end-to-end training (one-batch) memory usage (GiB) at 100 iterations, and test perplexity of the Transformer-base and big models on $\\mathrm { w M T } 3 2 \\mathrm { k }$ . Pretraining: the optimizer memory usage (GiB), including the $\\boldsymbol { S } _ { M }$ , end-to-end training (one-batch) memory usage (GiB) at 100 iterations, and test perplexity of BERT, GPT-2, and T5. We use Adam without the bias correction term to obtain lower perplexity.   \n\n<html><body><table><tr><td colspan=\"6\">TransformerModelsand Tasks (Optimizer and End-to-End Memory [GiB]),Model Performance</td></tr><tr><td>Dataset</td><td colspan=\"5\">WMT32k (Full-Trining)</td></tr><tr><td>Model</td><td></td><td>AdamAdafactor</td><td>SM3</td><td></td><td>CAME|SMMF</td></tr><tr><td>Transformer(0.7,1.4) (base)</td><td>6.6</td><td>(0.4,1.1) 6.6</td><td>(0.4,1.1)(0.4，1.1)(.01,0.8) 7.8</td><td>6.6</td><td>6.7</td></tr><tr><td>Transformer(2.1,4.2) (big)</td><td>6.9</td><td>(1.1,3.2) 6.6</td><td>7.9</td><td>(1.1,3.2)(1.1,3.2)(.04,2.1) 7.5</td><td>6.8</td></tr><tr><td>Dataset</td><td colspan=\"5\">Book Corpus&Wikipedia (Pre-Training)</td></tr><tr><td>BERT</td><td>(2.5,6.3) 16.1</td><td>(1.3,5.0) 30.6</td><td>27.5</td><td>(1.3，5.0)(1.3，5.0)(.04,3.8) 20.1</td><td>20.4</td></tr><tr><td>GPT-2</td><td>(2.6,6.7) 19.2</td><td>(1.3,5.3) NaN</td><td>(1.3，5.3) 19.4</td><td>19.1</td><td>(1.3,5.3)(.04,4.0) 19.2</td></tr><tr><td>T5</td><td>(1.7,4.2) 2.6</td><td>(0.8,3.4) 2.6</td><td>(0.8,3.4) 2.8</td><td>2.6</td><td>(0.9,3.4)(03,2.5) 2.6</td></tr></table></body></html>\n\nPre-training. Table 2 shows the optimizer memory usage including $S _ { M }$ , end-to-end training (one-batch) memory usage, and the perplexity of BERT, GPT-2, and T5 pre-trained for the BookCorpus & Wikipedia dataset. For GPT-2 pretrained with Adafactor, we failed to obtain its perplexity since it diverged (NaN) even with multiple trials of various settings, e.g., different machines, hyper parameters, seeds, etc. On the other hand, SMMF performs competitive optimization for all pre-training of BERT, GPT-2, and T5 using up to $6 0 \\mathrm { x }$ lower optimizer memory. Figure 2 (right) shows that SMMF (yellow) curtails the optimizer memory usage in the pre-training of BERT about 1.26 GiB (from 1.3 to $0 . 0 4 \\mathrm { G i B } ^ { \\cdot }$ ) while exhibiting the similar test perplexity trajectory to CAME (purple) taking much more memory to maintain the similar optimization performance. Overall, Adafactor, CAME, and SMMF show similar perplexity trajectories, confirming that SMMF retains the optimization performance in pre-training of Transformers with the lowest memory, enabled by 1) square-matricization of any rank (shape)\n\n18 Adam CAME 21040 Adam CAME SM3 SMMF SM3 SMMF   \n104 Adafactor 80 Adafactor   \n60 100 200 300 400 20 0 250k 500k 750k 1M Epoch Iteration\n\nof momentums (e.g., a vector, matrix, tensor, etc.) and 2) the decompression $$ compression scheme that uses the uncompressed gradients at the current update step. They jointly allow it to pre-train Transformers aptly during a massive number of pre-training steps by persistently conducting effective and efficient optimization over a long time horizon.\n\nThat being said, SMMF also exhibits some occasional loss spike (Takase et al. 2024) at the early steps of optimization (training), which stabilizes as the training proceeds. It is a well-known phenomenon that commonly occurs in the pre-training of many large language models (Raffel et al. 2020; Radford et al. 2019) optimized with existing optimizers, such as Adam, Adafactor, SM3, and CAME. We discuss it in more detail in Section 6.\n\nFine-tuning. Table 3 summarizes the optimization memory usage including $S _ { M }$ , end-to-end training(one-batch) memory usage, and the model performance of GPT-2, T5-small, and LLaMA-7b, which are fine-tuned for the QNLI, MNLI, QQP, STSB, and MRPC datasets from pre-trained models. As shown in the table, SMMF achieves comparable model performance in the five datasets compared to the other four optimizers with the lowest memory usage. For instance, SMMF provides similar accuracy $( 9 0 . 6 \\% )$ for GPT-2 on QQP compared to CAME, using much smaller optimizer and end-to-end training memory, respectively (16 vs. 489 MiB and 0.96 vs. 1.43 GiB). It demonstrates that SMMF is also apt at fine-tuning Transformer models, which entails delicate and intricate updates of weight parameters, otherwise likely to degrade the learning performance for downstream tasks. In practice, it suggests that some Transformer models, such as T5, would be fine-tuned on a low-end device, e.g., Raspberry Pi Zero $( 0 . 5 \\mathrm { G i B } )$ , with similar model performance to CAME (i.e., $8 3 . 0 \\%$ vs. $8 2 . 8 \\%$ ), as SMMF curtails the end-to-end training memory requirement of finetuning down to 0.47 GiB. On the other way around, SMMF can scale up training of Transformers by enabling memoryefficient optimization of enormous Transformer models that require a gigantic amount of memory, e.g., hundreds of GiB.\n\nSimilar to the CNN-based models and tasks, the experimental results of Transformers demonstrate that SMMF steadily performs competitive optimization for a wide range of Transformer models, tasks, and training methods (i.e., full-, pre-, and fine-tuning) with the smallest memory usage.\n\n# 5.3 Optimization Time Measurement\n\nTable 4 shows the optimization time measured for a single training step of the five optimizers when training MobileNetV2 and ResNet-50 on ImageNet, and Transformerbase and big on WMT32k. Except for Adam, the four optimizers take similar optimization times, where SMMF takes a little more time than the other three. That is because it trades off the memory space and optimization time, i.e., the time required for square-matricization and the sign matrix operations for the $\\bar { \\boldsymbol { 1 } } ^ { s t }$ momentum (Algorithms 3 and 4). However, SMMF offers huge memory reduction compared to the amount of the increased optimization time, e.g., $7 . 9 \\mathrm { x }$ memory reduction vs. $1 . 2 \\mathrm { x }$ time increase for Adam and SMMF with the 8-bit format $\\boldsymbol { S _ { M } }$ in Algorithms 3 and 4 applied to Transformer-big on WMT32k, and $7 . 8 \\mathrm { x }$ vs. 1.6x for Adam and SMMF with $S _ { M }$ applied to ResNet-50 on ImageNet.\n\nTable 3: Fine-tuning: the optimizer and end-to-end training (one-batch) memory usage [MiB, GiB] at 100 iterations including $\\boldsymbol { S _ { M } }$ , and the performance of GPT-2, T5-small, and LLaMA-7b fine-tuned on QNLI, MNLI, QQP, STSB, and MRPC.   \n\n<html><body><table><tr><td colspan=\"7\">Transformer Modelsand Tasks (Fine-Tuning)</td></tr><tr><td>(Optimizer Memory [MiB] and End-to-End Memory [GiB]),Model Performance</td><td>Model</td><td>QNLI (ACC)</td><td>MNLI (ACC)</td><td>QQP (ACC)</td><td></td><td></td></tr><tr><td>Optimizer |</td><td></td><td></td><td>(952,1.89) 72.4</td><td></td><td>STSB (Pearson)</td><td>MRPC(ACC)</td></tr><tr><td>Adam</td><td></td><td>(957,1.89) 84.5</td><td></td><td>(973,1.89) 86.4</td><td>(962,1.89)83.2</td><td>(861，1.89)81.4</td></tr><tr><td>Adafactor</td><td>GPT-2</td><td>(478, 1.43) 74.7</td><td>(478, 1.43) 71.7</td><td>(488,1.43) 80.1</td><td>(481, 1.43) 84.4</td><td>(481,1.43) 82.6</td></tr><tr><td>SM3</td><td></td><td>(478,1.43) 88.0</td><td>(478,1.43) 81.1</td><td>(487,1.43) 88.8</td><td>(481, 1.43) 84.1</td><td>(481,1.43) 83.3</td></tr><tr><td>CAME</td><td></td><td>(468,1.43) 88.6</td><td>(479,1.43) 81.9</td><td>(489,1.43) 90.6</td><td>(481, 1.43) 86.4</td><td>(478,1.43) 83.3</td></tr><tr><td>SMMF</td><td></td><td>(16,0.96) 88.9</td><td>(16,0.96) 82.2</td><td>(16,0.96) 90.6</td><td>(16,0.96) 83.8</td><td>(16,0.96)81.6</td></tr><tr><td>Adam</td><td rowspan=\"4\">T5</td><td>(464,0.92) 88.4</td><td>(464,0.92) 77.5</td><td>(434,0.92) 86.8</td><td>(456,0.92) 84.7</td><td>(464,0.92) 77.5</td></tr><tr><td>Adafactor</td><td>(233,0.70)90.1</td><td>(233,0.70) 80.3</td><td>(233,0.70) 88.7</td><td>(233,0.70) 87.9</td><td>(233,0.70) 80.3</td></tr><tr><td>SM3</td><td>(233,0.70) 88.8</td><td>(233,0.70) 79.4</td><td>(233,0.70) 88.1</td><td>(233, 0.70) 79.4</td><td>(233,0.70) 79.4</td></tr><tr><td>CAME</td><td>(233,0.70) 90.7</td><td>(234,0.70) 83.0</td><td>(233,0.70) 90.4</td><td>(233,0.70) 87.5</td><td>(234,0.70) 83.0</td></tr><tr><td>SMMF</td><td rowspan=\"4\"></td><td>8,0.47) 90.6</td><td>(8,0.47) 82.8</td><td>8, 0.47) 90.2</td><td>8,0.47) 84.7</td><td>8, 0.47) 82.8</td></tr><tr><td>Adam</td><td>(153,24.9)93.0</td><td>(153,24.9)87.5</td><td>(153,24.9)84.4</td><td>(153,24.9) 96.6</td><td>(153，24.9)90.6</td></tr><tr><td>Adafactor</td><td>86,24.9) 93.8</td><td>86,24.9) 84.4</td><td>86,24.9) 93.0</td><td>86,24.9) 96.3</td><td>86,24.9) 85.9</td></tr><tr><td>SM3</td><td>86,24.9) 65.6</td><td>86,24.9) 64.8</td><td>86,24.9) 71.9</td><td>86,24.9) 34.9</td><td>86,24.9) 70.3</td></tr><tr><td>CAME SMMF</td><td>LLaMA-7b</td><td>86,24.9) 69.5 (3.9,24.8) 91.4</td><td>86,24.9)43.0 ( 3.9,24.8) 87.5</td><td>86,24.9) 75.8 ( 3.9,24.8) 90.6</td><td>86,24.9) 34.8 ( 3.9,24.8) 96.5</td><td>86,24.9) 70.3 (3.9,24.8) 89.8</td></tr></table></body></html>\n\nTable 4: The single optimization time (milliseconds) per step of the four optimizers, and SMMF (8-bit format $\\boldsymbol { S _ { M } }$ in Algorithm 3 and 4): 1) MobileNetV2 and ResNet-50 on ImageNet, and 2) Transformer-base and big on WMT32k.   \n\n<html><body><table><tr><td colspan=\"6\">Optimization Time (ms) fora Single Training Step (Iteration)</td></tr><tr><td>Model</td><td></td><td>Adam|Adafactor</td><td></td><td></td><td>SM3CAME|SMMF</td></tr><tr><td>MobileNetV2 ResNet-50</td><td>127±16 273±16</td><td>168±16 316±14</td><td></td><td></td><td>140±17160±17205±13 286±14307±20|349±15</td></tr><tr><td>Transformer-base Transformer-big</td><td>129±7 321±18</td><td>160±7 372±18</td><td></td><td></td><td>134±7156±7171±7 325±18369±18389±18</td></tr></table></body></html>\n\n# 6 Limitations and Discussions\n\nOverhead of Binary Signs. It is not easy to effectively factorize the $1 ^ { s t }$ momentum $M$ having negative elements. While SMMF circumvents this by storing the binary sign matrix (1-bit) that is much smaller than the original matrix (32-bit), there are some methods that can further reduce the memory required to store the sign matrix. For instance, Binary Matrix Factorization (Kumar et al. 2019) can be employed to factorize the binary matrix into two lower-rank matrices, reducing the memory space for storing the binary matrix with a high restoration rate.\n\nOptimization Time. Section 3.3 finds that the time complexity of SMMF is similar to existing memory-efficient optimizers, e.g., Adafactor, showing that SMMF effectively saves a significant amount of memory (up to $9 6 \\%$ ) compared to them with slightly increased optimization time.\n\nLoss Spike at Initial Training Steps. We observe some loss spike at the initial training steps, especially in Transformer models, which is a well-known issue commonly observed in other works (Takase et al. 2024) and many optimizers, i.e., Adam (with the bias correction term), Adafactor, SM3, and CAME. With appropriate hyper-parameter tuning, e.g., learning rate and weight-decay, it can be stabilized as the training proceeds, like other optimizers.\n\nExtremely Large Models and Other Tasks. Due to the limited computing resources, we have not been able to experiment SMMF with extremely large models, e.g, GPT4 (OpenAI et al. 2024), LLaMA-2 70B (Touvron et al. 2023b), and diffusion models (Rombach et al. 2022). From our theoretical study and empirical result, we expect SMMF to perform competitive optimization with them. We hope to have a chance to test SMMF with them.\n\n# 7 Conclusion\n\nWe introduce SMMF, a memory-efficient optimizer that decreases the memory requirement of adaptive learning rate optimization methods, e.g., Adam and Adafactor. Given arbitrary-rank momentum tensors, SMMF reduces the optimization memory usage through the proposed squarematricization and matrix compression of the first and second momentums. The empirical evaluation shows that SMMF reduces up to $9 6 \\%$ of optimization memory when compared to existing memory-efficient optimizers, i.e., Adafactor, SM3, and CAME, with competitive optimization performance on various models (i.e., CNNs and Transformers) and tasks (i.e., image classification, object detection, and NLP tasks over full-training, pre-training, and fine-tuning). Our analysis of regret bound proves that SMMF converges in a convex function with a similar bound of AdamNC.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了自适应学习率优化器（如Adam）在内存受限设备上训练深度神经网络时内存占用过高的问题。现有内存高效优化器（如Adafactor、CAME、SM3）仅适用于特定秩（如秩-2）的动量张量，且内存减少效果有限（仍需1.1 GiB）。\\n> *   该问题在嵌入式系统等内存受限平台上尤为重要，例如训练Transformer-base模型需要1.4 GiB内存，而Raspberry Pi等设备仅有1 GiB内存。\\n\\n> **方法概述 (Method Overview)**\\n> *   提出SMMF（Square-Matricized Momentum Factorization），通过将任意秩的动量张量转换为接近方阵的形式并进行一次性矩阵分解，显著减少内存占用。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **内存效率提升：** 相比现有内存高效优化器（如Adafactor、CAME、SM3），SMMF减少内存占用高达 `96%`（从1.1 GiB降至0.04 GiB）。\\n> *   **通用性：** 支持任意秩（如向量、矩阵、高秩张量）的动量张量，适用于CNN（高秩）和Transformer（低秩）等多种模型架构。\\n> *   **理论保证：** 通过遗憾界分析证明SMMF在凸设置下的收敛性与AdamNC相当，为优化性能提供理论支持。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   SMMF的核心思想是通过“方阵化”（square-matricization）将任意秩的动量张量转换为接近方阵的矩阵，然后通过非负矩阵分解（NNMF）将其分解为两个向量，从而大幅减少内存占用。\\n> *   该方法有效的原因是方阵化最小化了分解后的向量总长度（即内存占用），同时通过“解压缩-压缩”方案保留当前梯度的完整信息，避免优化性能下降。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 现有方法（如Adafactor、CAME）需要将高秩张量切片为多个矩阵分别分解，导致内存复杂度随张量秩增加而增长；SM3则需要动量张量中存在相似元素模式，适用性受限。\\n> *   **本文的改进：** SMMF通过一次性方阵化和分解支持任意秩和模式的动量张量，无需切片或依赖特定模式，同时通过理论分析证明其收敛性。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **方阵化（Square-Matricization）：** 将秩为d的动量张量$\\\\mathbb{R}^{n_1 \\\\times ... \\\\times n_d}$转换为接近方阵的矩阵$\\\\mathbb{R}^{\\\\hat{n} \\\\times \\\\hat{m}}$，其中$\\\\hat{n} \\\\simeq \\\\hat{m}$且$\\\\hat{n} \\\\cdot \\\\hat{m} = \\\\prod_{r=1}^d n_r$。\\n> 2.  **分解（Compression）：** 使用NNMF将方阵化后的矩阵分解为两个向量$\\\\mathbb{R}^{\\\\hat{n} \\\\times 1}$和$\\\\mathbb{R}^{1 \\\\times \\\\hat{m}}$，并存储第一动量的符号矩阵（1-bit）。\\n> 3.  **解压缩（Decompression）：** 在更新动量时，将分解后的向量恢复为矩阵形式，结合符号矩阵恢复原始动量值。\\n> 4.  **权重更新：** 使用解压缩后的动量计算更新项，并应用于模型参数。\\n\\n> **案例解析 (Case Study)**\\n> *   **BERT嵌入层：** 权重矩阵 `ℝ^{30522×768}` 经平方矩阵化后变为 `ℝ^{5087×4608}`，分解后内存从220 MiB降至3.7 MiB（59倍减少）。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   Adam、Adafactor、SM3、CAME。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在内存占用上：** SMMF在ResNet-50（ImageNet）任务中仅占用 `3.7 MiB` 内存，显著优于Adafactor（`220 MiB`）、SM3（`99 MiB`）和CAME（`346 MiB`）。与表现最佳的基线（SM3）相比，内存占用减少了 `96%`。\\n> *   **在模型性能上：** SMMF在MobileNetV2（ImageNet）任务中达到 `69.5%` 的top-1准确率，与Adafactor（`69.3%`）和CAME（`69.5%`）相当，同时远低于其内存占用。\\n> *   **在训练速度上：** SMMF的优化时间略高于基线（如Adafactor），但内存节省效果显著（如Transformer-big任务中内存减少70倍，时间仅增加1.2倍）。\\n> *   **在NLP任务上：** SMMF在Transformer-base（WMT32k）上测试困惑度为 `6.7`，与Adafactor（`6.6`）和CAME（`6.6`）相当，但内存仅需 `0.01 GiB`（Adafactor为 `0.4 GiB`）。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   内存高效优化 (Memory-Efficient Optimization, N/A)\\n*   自适应学习率 (Adaptive Learning Rate, N/A)\\n*   方阵化动量分解 (Square-Matricized Momentum Factorization, SMMF)\\n*   非负矩阵分解 (Non-Negative Matrix Factorization, NNMF)\\n*   深度学习优化 (Deep Learning Optimization, N/A)\\n*   遗憾界分析 (Regret Bound Analysis, N/A)\\n*   卷积神经网络 (Convolutional Neural Network, CNN)\\n*   Transformer模型 (Transformer Model, N/A)\"\n}\n```"
}