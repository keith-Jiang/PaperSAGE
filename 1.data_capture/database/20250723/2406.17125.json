{
    "source": "Semantic Scholar",
    "arxiv_id": "2406.17125",
    "link": "https://arxiv.org/abs/2406.17125",
    "pdf_link": "https://arxiv.org/pdf/2406.17125.pdf",
    "title": "A Wiener Process Perspective on Local Intrinsic Dimension Estimation Methods",
    "authors": [
        "Piotr Tempczyk",
        "Lukasz Garncarek",
        "Dominik Filipiak",
        "Adam Kurpisz"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2024-06-24",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science",
        "Mathematics"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "institutions": [
        "Institute of Informatics, University of Warsaw",
        "NASK National Research Institute",
        "PL4AI",
        "Snowflake",
        "Adam Mickiewicz University",
        "Perelyn",
        "University of Innsbruck",
        "BFH Bern Business School",
        "ETH Zurich"
    ],
    "paper_content": "# A Wiener Process Perspective on Local Intrinsic Dimension Estimation Methods\n\nPiotr Tempczyk\\*1,2,3, Åukasz Garncarek3,4, Dominik Filipiak5,6,7, Adam Kurpisz\\*3,8,9\n\n1Institute of Informatics, University of Warsaw, Poland 2NASK National Research Institute, Warsaw, Poland 3PL4AI, Poland 4Snowflake, USA 5Adam Mickiewicz University, PoznanÂ´, Poland 6Perelyn, Warsaw, Poland 7University of Innsbruck, Austria 8BFH Bern Business School, Switzerland 9ETH Zurich, Switzerland tempczyk.piotr $@$ gmail.com, lukgar@gmail.com, df $@$ amu.edu.pl, adam.kurpisz $@$ ifor.math.ethz.ch\n\n# Abstract\n\nLocal intrinsic dimension (LID) estimation methods have received a lot of attention in recent years thanks to the progress in deep neural networks and generative modeling. In opposition to old non-parametric methods, new methods use generative models to approximate diffused dataset density to scale the methods to high-dimensional datasets (e.g. images). In this paper, we investigate the recent state-of-the-art parametric LID estimation methods from the perspective of the Wiener process. We explore how these methods behave when their assumptions are not met. We give an extended mathematical description of those methods and their error as a function of the probability density of the data.\n\nExtended version of paper â€” arxiv.org/abs/2406.17125\n\n# 1 Introduction\n\nLID estimation has gained increasing attention in recent years as part of the fast-growing field of topological data analysis. It was able to progress from non-parametric to parametric methods thanks to the latest progress in the field of generative modeling. LID estimation methods are algorithms for estimating the manifoldâ€™s dimension from which the data point $x$ was sampled. In these methods, we assume that the data lie on the union of one or more manifolds that may be of different dimensions.\n\nThe estimation of intrinsic dimension is substantial for data analysis and machine learning (Ansuini et al. 2019; Li et al. 2018; Rubenstein, Schoelkopf, and Tolstikhin 2018) and was investigated in relation to dimensionality reduction and clustering (Vapnik 2013; Kleindessner and Luxburg 2015; Camastra and Staiano 2016), analyzing the training and representation learning processes within deep neural networks (Li et al. 2018; Ansuini et al. 2019; Pope et al. 2020; Loaiza-Ganem et al. 2024), verifying the union of manifolds hypothesis for images (Brown et al. 2022), and used to improve out-of-distribution detection algorithm (Kamkari et al. 2024a).\n\nPrior to introducing LIDL (Tempczyk et al. 2022), two approaches to solve the intrinsic dimension estimation problem were presented. The first employs global methods; see, e.g., Fukunaga and Olsen (1971); Minka (2000); Fan et al. (2010). These methods are known to suffer from issues related to a manifold curvature and non-uniformity of the data distribution. They also assume that data lies on a single manifold of constant dimension, so dimensionality is the same for all $x$ . The second approach is based on local nonparametric methods that explore the geometric properties of neighborhoods (Johnsson, Soneson, and Fontes 2014; Levina and Bickel 2004) calculating some statistics using points from the neighborhood of $x$ . Although all aforementioned methods perform reasonably well for a small number of dimensions, the higher dimensionality negatively affects their performance (Tempczyk et al. 2022; Campadelli et al. 2015; Camastra and Staiano 2016).\n\nVery recently, a new approach emerged for the local methods, which is based on a two-step procedure. In the first step, the dataset is perturbed with some noise, and in the second step, its dimensionality is estimated using various techniques. These include generative models to analyze changes in density (Tempczyk et al. 2022; Kamkari et al. 2024b) or in singular values of the Jacobian (Horvat and Pfister 2022, 2024) for different noise magnitudes, and analyzing rank of scores from diffusion model as recently presented by Stanczuk et al. (2024).\n\nWhile new algorithms provide state-of-the-art results for large, real-life datasets, it is vital for their performance that adding noise to the dataset should be as close as possible to an injective process. This means that the resulting densities after adding noise should uniquely identify the original distribution of the dataset. Otherwise, it is impossible to neither uniquely reverse the process nor analyze the original structure of the dataset. One example in the theoretical model that does not admit such a problem is a flat manifold with a uniform distribution of data points. In such cases these algorithms perform best, as shown in experimental results.\n\nIn other cases, a hypothetical line of research would be to decrease the magnitude of noise added to the data set so that the manifold is approximately flat and has locally uniform data density in the scale of the noise magnitude. This, however, is not possible in practice for several reasons. For instance, data is often a set of discrete points (especially in audio and visual modality), and considering the noise of magnitude much smaller than the minimum distance between points does not lead to any meaningful process. Moreover, neural net training is done with finite precision and the stochastic gradient descent procedure introduces noise to the density estimates, so very small changes in density cannot be observed in practice, which may lead to poor quality estimates of LID.\n\nIn this paper, we point out and exploit the fact that adding Gaussian noise of varying magnitudes can be seen as studying the evolution of the Wiener process describing the diffusion of particles (points of the dataset) in the ambient space. This point of view enables us to employ Fickâ€™s Second Law of Diffusion to eliminate time derivatives from mathematical descriptions of state-of-the-art LID algorithms (Tempczyk et al. 2022; Kamkari et al. 2024b), and replace them with spatial derivatives. Such considerations can be taken into account in the second step of the considered algorithms, leading to more accurate results. We encourage reader to get familiar with Appendix A from the extended version of the paper (arxiv.org/abs/2406.17125), which contains important definitions and clarifications regarding this work.\n\n# Contribution\n\n1. We recognize and define new categories (isolated and holistic algorithms) for the Wiener process-based parametric LID estimation algorithm family and categorize the existing algorithms accordingly.   \n2. We explore the first step of existing algorithms in the language of Wiener processes and calculate important cases of diffusion from lower-dimensional manifolds with nonuniform probability density into ambient space.   \n3. We derive closed-form expressions for important parameters used in two state-of-the-art isolated LID estimation algorithms as a function of on-manifold density and manifold dimensionality, which can be viewed as closedform expressions for deviation from a flat manifold with uniform distribution case.\n\n# 2 Related Work\n\nThe review of the non-parametric methods for local and global intrinsic dimension estimation can be found in the work of Campadelli et al. (2015), or Camastra and Staiano (2016). Tempczyk et al. (2022) compared these methods on bigger datasets in terms of their dimensionality.\n\nAlthough we do not analyze non-parametric methods in this paper, it is worth mentioning a recent work on nonparametric LID estimation, in which Bailey, Houle, and Ma (2022) explore the connection of LID to other wellknown measures for complexity: entropy and statistical divergences, and develop new analytical expressions for these quantities in terms of LID. Consequently, Bailey, Houle, and\n\nMa (2023) establish the relationships for cumulative Shannon entropy, entropy power, Bregman formulation of cumulative Kullback-Leibler divergence, and generalized Tsallis entropy variants, and propose four new estimators of LID, based on nearest neighbor distances.\n\nDuring the last few years many parametric methods for estimating LID emerged. Zheng et al. (2022) prove that VAE are capable of recovering the correct manifold dimension and demonstrate methods to learn manifolds of varying dimensions across the data sample. Yeats et al. (2023) connect the adversarial vulnerability of score models with the geometry of the underlying manifold they capture. They show that minimizing the Dirichlet energy of learned score maps boosts their robustness while revealing the LID of the underlying data manifold.\n\nWiener process-based algorithms. Regarding parametric methods, there is a group of algorithms, that have one thing in common: they simulate a Wiener process on a dataset and directly use some properties of time-evolving density to estimate LID. We can divide those algorithms into the following three groups:\n\n1. LIDL (Tempczyk et al. 2022) and its efficient and most accurate implementation using diffusion models called FLIPD (Kamkari et al. 2024b). These algorithms use the rate of change of the probability density at point $x$ during the Wiener process to estimate LID at $x$ . For small diffusion times $t$ the logarithm of a density is a linear function of a logarithm of $t$ , and the proportionality coefficient is equal $d - D$ for small $t$ , where $d$ is manifold density and $D$ is ambient space dimensionality. Our experiments with ID-NF (described below) and FLIPD show, that the latter is more scalable than ID-NF (and ID-DM) due to the high memory and computational complexity of SVD (which has to be calculated for each data point). Experiments from (Kamkari et al. 2024b) show that FLIPD is more accurate than NB (described below), which led us to the conclusion, that FLIPD is state-of-the-art in LID estimation among the most scalable algorithms.   \n2. ID-NF (Horvat and Pfister 2022) (using normalizing flows), and the diffusion-using follow-up paper ID-DM (Horvat and Pfister 2024) analyze how singular values of a Jacobian of a function transforming a standard normal distribution into a diffused dataset density at $x$ evolves during the Wiener process. Horvat and Pfister observed that when transforming a $d$ -dimensional manifold into a $D$ -dimensional Gaussian we have to expand space more in the normal direction to manifold, especially for small diffusion times $t$ .   \n3. NB (Stanczuk et al. 2024), which is an abbreviation from Normal Bundle (name used in (Kamkari et al. 2024b)). Stanczuk et al. observed that for small diffusion times $t$ the gradients of the logarithm of a diffused data density (score function) close to $x$ lies in the normal space to the manifold and use this fact to estimate LID at $x$ .   \n3 Isolated and Holistic Algorithms\n\n#\n\nIn this work, we take a closer look at the Wiener processbased algorithms described in the last paragraph of Section 2. Although all these algorithms apply a Wiener process to the dataset during their first phase, when looking at their second step, we can divide them into two groups: isolated (LIDL, FLIPD, NB) and holistic (ID-NF, ID-DM). The intermediate results of the first group that are used for LID calculation use only the information about the local shape of the data probability density function (without normalization constant). We assume that the generative model approximates diffused data distribution $\\rho _ { t }$ perfectly. Their estimates depend on the proximity of $\\nabla \\log \\rho _ { t }$ to $x$ (NB) or ${ d \\log \\rho _ { t } } / { d \\log t }$ at $x$ (LIDL, FLIPD).\n\nThis is a consequence of $\\rho _ { t }$ at $x$ being a function that â€“ in practice â€“ depends on the values of original data distribution $p _ { S }$ in a ball of radius $r \\approx 4 \\sqrt { t }$ around $x$ in the data space. The values of $p _ { S }$ outside this ball do not matter in practice for isolated algorithms, because the diffused particles in the Wiener process can travel longer distances than $r$ with very low probability (less than $3 . 7 \\cdot 1 0 ^ { - 5 } \\cdot$ ). When we add some new data points to the dataset far away from $x$ , it does not change the shape of $\\rho _ { t }$ close to $x$ . This operation only changes a normalization constant, which becomes 0 after taking a logarithm of $\\rho _ { t }$ and taking a derivative either w.r.t time or spatial variables.\n\nThe holistic algorithms work quite differently. In the case of ID-NF and ID-DM, they calculate singular values of the Jacobian of the function $\\zeta : \\mathrm { R } ^ { N } \\to \\mathrm { R } ^ { N }$ transforming $\\rho _ { t }$ into a standard normal distribution. This $\\zeta$ function strongly depends on the entire shape of $\\rho _ { t }$ . As mentioned before, when we change $\\rho _ { t }$ far away from $x$ we do not change the shape of $\\rho _ { t }$ close to $x$ . The same is not true for $\\zeta ( x )$ . When we add many data points to the dataset far away from $x$ while we keep the latent distribution fixed, we have to change the way we compress and stretch the space to match the new distribution. This property makes the analysis of the behavior of ID-NF and ID-DM much harder (maybe even impossible) if we want to take into account only the data density in the neighbourhood of $x$ .\n\nTo give an illustrative example of this behavior, one needs to imagine that our dataset is one-dimensional and consists of 10K points sampled from $\\mathcal { N } ( 0 , 1 )$ . Typically, we are transforming it into $\\mathcal { N } ( 0 , 1 )$ , so $\\zeta$ is just an identity function. Now letâ€™s add to the dataset another 10K points sampled from $\\mathcal { N } ( 1 0 0 , 1 )$ . As a consequence, we have to stretch our space in some areas to transform one density into another, whereas $\\zeta$ changes along with its Jacobian.\n\n# 4 Wiener Process Perspective on LID Estimation\n\nWiener process is a stochastic process modeling particle diffusion. Its increments over disjoint time intervals are independent and normally distributed, with variance proportional to time increments. Since in the machine learning community the term diffusion is already overloaded, we will stick to Wiener process when speaking of particle diffusion process.\n\nIn this section we present a new perspective on perturbing datasets, unifying the approaches seen in the algorithms presented by Tempczyk et al. (2022); Stanczuk et al. (2024);\n\nHorvat and Pfister (2022, 2024); Kamkari et al. (2024b). As already mentioned, all these algorithms consist of two stages, the first of which amounts to perturbing the dataset with normally distributed random noise of fixed variance $t$ . In the second stage, each of the algorithms utilizes the behavior of the perturbed density in the neighborhood of a fixed point under changes in the noise variance.\n\nThe first phase of each algorithm can be interpreted as applying the Wiener process to the points in the dataset. Afterward, the resulting set of points is used to train some type of generative model (or models) to estimate the distribution of the dataset undergoing the Wiener process at time $t$ . From the point of view of differential equations, the distribution density function of the diffused dataset is described by Fickâ€™s Second Law of Diffusion.\n\nFickâ€™s Second Law of Diffusion. Let $\\rho _ { t } : \\mathbb { R } ^ { D } \\mapsto \\mathbb { R }$ denote the probability density function modeling particles undergoing diffusion at time $t$ . Then $\\rho _ { t }$ satisfies the differential equation\n\n$$\n\\frac { d } { d t } \\rho _ { t } = C \\Delta \\rho _ { t } ,\n$$\n\nwhere $C \\in \\mathbb { R } ,$ , and $\\Delta$ stands for the standard Laplacian in RD.\n\nNow, given a dataset embedded in $\\mathbb { R } ^ { D }$ , we assume that it has been drawn from some latent union of submanifolds $S$ endowed with a probability measure $p _ { S }$ (which can be naturally treated as a probability measure on $\\dot { \\mathbb { R } } ^ { D }$ ). The goal of Local Intrinsic Dimension estimation is to find out the dimension of $S$ at any point of the dataset.\n\nTo model the Wiener process with initial distribution $p _ { S }$ (which is not a function on $\\mathbb { R } ^ { D }$ ), let us first define\n\n$$\n\\phi _ { t } ^ { D } ( x ) = ( 2 \\pi t ) ^ { - D / 2 } e ^ { - \\| x \\| ^ { 2 } / 2 t } .\n$$\n\nThis is the density of normal distribution on $\\mathbb { R } ^ { D }$ with covariance matrix $t I$ . It is the fundamental solution of the differential equation given by Fickâ€™s Second Law of Diffusion (1) with $C = 1 / 2$ . Here, this means that the convolution\n\n$$\n\\rho _ { t } = p _ { S } * \\phi _ { t } ^ { D }\n$$\n\nis the solution of (1) for $t > 0$ and hence it describes the Wiener process starting from the initial probability distribution pS.\n\nTo limit the complexity introduced by curvature, from now on we will consider only flat manifolds. This means that, without loss of generality, we may assume that $S$ is the first factor in product decomposition RD = Rd  RDâˆ’d. We will denote the coordinates of $\\mathbb { R } ^ { d }$ and $\\mathbb { R } ^ { D - d }$ by $x$ and $y$ , respectively. We will moreover assume that $p _ { S }$ , now a probability distribution on $\\mathbb { R } ^ { d }$ , has a density $\\psi : \\mathbb { R } ^ { d }  \\mathbb { R }$ .\n\nIn Appendix B we discuss the Laplacian of $\\rho _ { t }$ and derive the following result:\n\nLemma 4.1 (Lemma B.1). For $t > 0$ and $( x , y ) \\in \\mathbb { R } ^ { D }$ we have\n\n$$\n\\begin{array} { l } { \\displaystyle \\Delta \\rho _ { t } ( x , y ) = \\left( \\frac { \\| y \\| ^ { 2 } } { t ^ { 2 } } + \\frac { d - D } { t } \\right) \\rho _ { t } ( x , y ) } \\\\ { \\displaystyle + \\phi _ { t } ^ { D - d } ( y ) \\Delta _ { x } ( \\psi * \\phi _ { t } ^ { d } ) ( x ) . } \\end{array}\n$$\n\nAs a consequence, by putting $y = 0$ and using\n\n$$\n\\phi _ { t } ^ { D - d } ( 0 ) = ( 2 \\pi t ) ^ { ( d - D ) / 2 }\n$$\n\nwe obtain the following.\n\nCorollary 4.2. For $t > 0$ and $\\boldsymbol { x } \\in \\mathbb { R } ^ { d }$ we have\n\n$$\n\\Delta \\rho _ { t } ( x , 0 ) = \\frac { d - D } { t } \\rho _ { t } ( x , 0 ) + ( 2 \\pi t ) ^ { ( d - D ) / 2 } \\Delta _ { x } ( \\psi * \\phi _ { t } ^ { d } ) ( x ) .\n$$\n\n# 5 From Wiener Process to LID Estimation\n\nThe findings from the last section can be used to analyze how the first family of algorithms (Tempczyk et al. 2022; Kamkari et al. 2024b) behaves for some particular cases. The main contribution of Kamkari et al. (2024b) is a substantial improvement on the side of density estimation. Therefore, when dealing with perfect density estimators and very small noise differences, both algorithms estimate the same quantity and give the same results from the theoretical perspective. Due to this fact from now on we will be analyzing LIDL, as we want to analyze the aspects of those implementations that do not depend on the problems with density estimation itself.\n\n# Reformulating LIDL\n\nGiven a point $x \\in S$ and a set of times $t _ { 1 } , \\ldots , t _ { n }$ , LIDL estimates the linear regression coefficient $\\alpha$ of the set of points $( \\log \\delta _ { i } , \\log \\rho _ { t _ { i } } ( x ) )$ , where $\\delta _ { i } = \\sqrt { t _ { i } }$ . Tempczyk et al. (2022) proved that\n\n$$\n\\log \\rho _ { t } ( x ) = ( d - D ) \\log \\sqrt { t } + O ( 1 ) ,\n$$\n\nand therefore $\\alpha \\approx d - D$ . The authors show that if $t$ is small enough, this estimate is accurate.\n\nThis procedure can be seen as approximating the asymptotic slope of the parametric curve $( \\log \\sqrt { t } , \\log \\rho _ { t } ( x ) )$ . In other words, the graph of $s \\mapsto \\log \\rho _ { e ^ { 2 s } } ( x )$ for $s  - \\infty$ . Another approach would consider the its derivative. Let us define its reparameterized derivative (with $t = e ^ { 2 s }$ )\n\n$$\n\\beta _ { t } ( x ) = \\frac { 2 t } { \\rho _ { t } ( x ) } \\frac { d } { d t } \\rho _ { t } ( x ) = \\frac { t \\Delta \\rho _ { t } ( x ) } { \\rho _ { t } ( x ) } ,\n$$\n\nwhere the last equality comes from the diffusion equation (1) with $C = 1 / 2$ . Moreover, denote the asymptotic slope of the aforementioned curve by\n\n$$\n\\beta ( x ) = \\operatorname* { l i m } _ { s  - \\infty } \\frac { d } { d s } \\log \\rho _ { e ^ { 2 s } } ( x ) = \\operatorname* { l i m } _ { t  0 ^ { + } } \\beta _ { t } ( x ) .\n$$\n\nThe results presented below are proved in Appendix $\\mathbf { C }$ . The next Proposition shows that the two approaches discussed above are equivalent.\n\nProposition 5.1 (Proposition C.1). Given a strictly positive differentiable function $f \\colon ( 0 , a ) \\  \\ ( 0 , \\infty )$ and a positive real number $\\alpha > 0$ , the following conditions are equivalent.\n\n1. The function $f$ explodes at 0 like $t ^ { - \\alpha }$ , i.e. for some positive constants $c , C > 0$ one has $c < t ^ { \\alpha } f ( t ) < C$ for some $\\epsilon > 0$ and $t \\in ( 0 , \\epsilon )$ .\n\nAs a consequence, the estimation of Local Intrinsic Dimension using LIDL can be achieved by computing $\\beta ( x )$ , yielding $d = D + \\beta ( x )$ .\n\nProposition 5.2 (Proposition C.2). For $t$ near 0 the following estimate holds\n\n$$\n\\log \\rho _ { t } ( x ) = \\beta ( x ) \\log \\sqrt { t } + O ( 1 ) .\n$$\n\nThe next proposition provides an elegant expression for $\\beta _ { t } ( x )$ , and consequently for $\\beta ( x )$ , expressed in terms of the density $\\psi$ on $\\mathbb { R } ^ { d }$ .\n\nProposition 5.3 (Proposition C.3). For $t > 0$ and $x \\in S =$ $\\mathbb { R } ^ { d } \\subseteq \\mathbb { R } ^ { D }$ we have\n\n$$\n\\beta _ { t } ( x ) = d - D + \\frac { \\Delta _ { x } ( \\psi * \\phi _ { t } ^ { d } ) ( x ) } { \\psi * \\phi _ { t } ^ { d } ( x ) } \\cdot t .\n$$\n\n# LIDL Examples\n\nFrom the theoretical considerations of Tempczyk et al. (2022) it follows that $\\beta ( x ) \\ = \\ d - \\ D$ if $\\psi$ is sufficiently regular and positive near $x$ . In other words,\n\n$$\n\\operatorname* { l i m } _ { t  0 ^ { + } } \\frac { \\Delta _ { x } ( \\psi * \\phi _ { t } ^ { d } ) ( x ) } { \\psi * \\phi _ { t } ^ { d } ( x ) } \\cdot t = 0 .\n$$\n\nNow, we will try to obtain this conclusion directly and calculate bias of LIDL for $t > 0$ in a few special cases by analyzing the behavior of $\\beta _ { t } ( x )$ .\n\nThe â€œuniform distributionâ€ on Euclidean space. There is no such thing as the uniform distribution on $\\mathbf { \\bar { \\mathbb { R } } } ^ { d }$ . However, from a purely theoretical viewpoint, in our differential equation approach we donâ€™t need the assumption of $\\phi$ being a probability density; it could be any function. And since constant functions are usually the simplest examples, we will now investigate what happens if we put $\\psi ( x ) \\equiv 1$ on the whole $\\mathbb { R } ^ { d }$ space.\n\nUsing Proposition 5.3 and the fact that $\\psi$ has bounded derivatives, this case leaves us with\n\n$$\n\\beta _ { t } ( x ) = d - D + \\frac { \\Delta _ { x } \\psi * \\phi _ { t } ^ { d } ( x ) } { \\psi * \\phi _ { t } ^ { d } ( x ) } \\cdot t = d - D ,\n$$\n\nsince $\\Delta _ { x } \\psi \\equiv 0$ . This expression is constant in $t$ , and in particular its limit at 0 is $\\beta ( x ) = d - D$ . In this case, LIDL estimator is not biased for all $t > 0$ .\n\nNormal distribution. Now consider the normal distribution on $\\mathbb { R } ^ { d }$ with covariance matrix $\\Sigma = \\mathrm { d i a g } ( \\sigma _ { 1 } ^ { 2 } , \\dots , \\sigma _ { d } ^ { 2 } )$ , and denote its density function by $\\psi$ . The convolution $\\psi * \\phi _ { t } ^ { d }$ is the density of the normal distribution with covariance matrix $\\Sigma + t I$ . If we simplify notation be putting $\\phi _ { i } = \\phi _ { \\sigma _ { i } ^ { 2 } + t } ^ { 1 }$ , we get\n\n$$\n\\psi * \\phi _ { t } ^ { d } ( x ) = \\prod _ { i = 1 } ^ { d } \\phi _ { i } ( x _ { i } ) .\n$$\n\nTo compute the Laplacian of this convolution, note that\n\n$$\n\\psi * \\phi _ { t } ^ { d } ( x ) = \\frac { \\psi * \\phi _ { t } ^ { d } ( x ) } { \\phi _ { i } ( x _ { i } ) } \\cdot \\phi _ { i } ( x _ { i } ) ,\n$$\n\nwhere the first factor does not depend of $x _ { i }$ , and therefore\n\n$$\n{ \\frac { \\partial ^ { 2 } ( \\psi * \\phi _ { t } ^ { d } ) } { \\partial x _ { i } ^ { 2 } } } ( x ) = \\psi ( x ) * \\phi _ { t } ^ { d } ( x ) \\cdot { \\frac { 1 } { \\phi _ { i } ( x _ { i } ) } } { \\frac { \\partial ^ { 2 } \\phi _ { i } } { \\partial x _ { i } ^ { 2 } } } ( x _ { i } ) ,\n$$\n\nleading to\n\n$$\n\\begin{array} { l } { \\displaystyle \\beta _ { t } ( x ) = d - D + t \\frac { \\Delta ( \\psi * \\phi _ { t } ^ { d } ) ( x ) } { \\psi * \\phi _ { t } ^ { d } ( x ) } = } \\\\ { = d - D + t \\displaystyle \\sum _ { i = 1 } ^ { d } \\frac { 1 } { \\phi _ { i } ( x _ { i } ) } \\frac { \\partial ^ { 2 } \\phi _ { i } } { \\partial x _ { i } ^ { 2 } } ( x _ { i } ) } \\\\ { = d - D + t \\displaystyle \\sum _ { i = 1 } ^ { d } \\frac { x _ { i } ^ { 2 } - ( \\sigma _ { i } ^ { 2 } + t ) } { ( \\sigma _ { i } ^ { 2 } + t ) ^ { 2 } } . } \\end{array}\n$$\n\nIt is easy to see that the second derivatives of $\\phi _ { i }$ are continuous in $\\dot { \\tau } > - \\sigma _ { i } ^ { 2 }$ , so the sum in the above expression has finite limit for $t \\to 0$ , and therefore $\\beta ( x ) = d - D$ .\n\nIn the special case where $\\Sigma \\ : = \\ : \\sigma ^ { 2 } \\dot { I }$ , these calculations simplify further, as $\\psi * \\phi _ { t } ^ { d } = \\phi _ { \\sigma ^ { 2 } + t } ^ { d }$ , and since\n\n$$\n\\Delta _ { x } \\phi _ { \\sigma ^ { 2 } + t } ^ { d } ( x ) = \\left( \\frac { \\left. x \\right. ^ { 2 } } { ( \\sigma ^ { 2 } + t ) ^ { 2 } } - \\frac { d } { \\sigma ^ { 2 } + t } \\right) \\phi _ { \\sigma ^ { 2 } + t } ^ { d } ( x ) ,\n$$\n\nwe have\n\n$$\n\\beta _ { t } ( x ) = d - D + \\left( \\frac { \\left\\| x \\right\\| ^ { 2 } } { ( \\sigma ^ { 2 } + t ) ^ { 2 } } - \\frac { d } { \\sigma ^ { 2 } + t } \\right) t .\n$$\n\nThese results express analytically the experimental observations from LIDL and FLIPD papers, as can be verified by looking at Fig. 1a. We can observe, that if we move to the regions of very low probability for a Gaussian, it generates very high positive bias, which may highly overestimate the true LID (also observed as a bump at $\\bar { t = 1 0 ^ { - 1 2 } }$ in Fig. 1b). Luckily, most of the points in our dataset come from the region of high probability, but we should be less certain of the estimates for points from low probability regions.\n\nIt is worth noting, that the values of $t$ used to generate Fig. 1a are the same as values of $\\delta$ , which is equal to $\\sqrt { t }$ in our convention. After double-checking our results we argue that the most probable cause of this is that the authors of LIDL used squared values of $\\delta$ by mistake. Additionally, one can observe that curves plotted by Tempczyk et al. (2022) are somewhat flatter than one in this study. The fact that in their paper the derivative was approximated by linear regression on numerically calculated densities â€“ which may lead to slightly different results â€“ might be a possible reason.\n\nArbitrary distribution with sufficiently nice density. By this point, the notion of nice density is shall be more clear. We want to be able to use the equality\n\n$$\n\\Delta _ { x } ( \\psi * \\phi _ { t } ^ { d } ) = \\Delta _ { x } \\psi * \\phi _ { t } ^ { d } .\n$$\n\nTo do so, we need $\\psi$ to be bounded, twice differentiable, and have bounded first and second-order partial derivatives.\n\nLIDL estimate as a function of $x$ for $\\mathcal { N } ( 0 , 1 )$\n\n![](images/c4c815164923d9ebe9c195f1c79b63512f1cbfb3735a6fb559a4847a05970d49.jpg)\n\n(a) Example of the bias of a LIDL estimate for different points from $\\mathcal { N } ( \\bar { 0 } , 1 )$ and for different values of $t$ . This plot recreates a numerical calculations presented in Figure 4 from (Tempczyk et al. 2022), with two minor differences described in Sec. 5\n\n![](images/21133d31c55706a2141aea7aaff944b83c53884a5061b8ff8bc5e14c22487211.jpg)\n\n(b) Plot of a LIDL estimate as a function of $t$ for the distribution $\\mathcal { N } ( \\mathbf { 0 } , \\mathrm { d i a g } ( 1 , 1 0 ^ { - 6 } , 1 0 ^ { - 1 2 } ) )$ and for three different points ${ \\textbf { x } } =$ $( 0 , 0 , x _ { 3 } )$ , which represents a distance of 0, 1 and $2 \\ \\sigma _ { 3 }$ from 0 on 3rd dimension (compare with Fig. 8 from FLIPD and Fig. 2 from LIDL).\n\nFigure 1: LIDL estimates for Gaussian distributions.\n\nWe will also require $\\psi$ to have continuous second-order partial derivatives. This is not a severe restriction, as numerous distributions satisfy these properties â€“ including the normal distribution or more generally, mixtures of Gaussians.\n\nIn this case, we have\n\n$$\n\\beta _ { t } ( x ) = d - D + \\frac { \\Delta _ { x } \\psi * \\phi _ { t } ^ { d } ( x ) } { \\psi * \\phi _ { t } ^ { d } ( x ) } \\cdot t ,\n$$\n\nhowever this time $\\Delta _ { x } \\psi$ is some arbitrary continuous function. Being differentiable, $\\psi$ is also continuous, and we can use the general fact that for a bounded continuous function, $f$ on $\\mathbb { R } ^ { \\tilde { d } }$ one has\n\n$$\n\\operatorname * { l i m } _ { t \\to 0 ^ { + } } f * \\phi _ { t } ^ { d } ( x ) = f ( x ) .\n$$\n\nThis gives us, for $x$ such that $\\psi ( x ) > 0$ ,\n\n$$\n\\operatorname* { l i m } _ { t \\to 0 ^ { + } } \\frac { \\Delta _ { x } \\psi * \\phi _ { t } ^ { d } ( x ) } { \\psi * \\phi _ { t } ^ { d } ( x ) } \\cdot t = \\frac { \\Delta _ { x } \\psi ( x ) } { \\psi ( x ) } \\operatorname* { l i m } _ { t \\to 0 ^ { + } } t = 0 ,\n$$\n\nand again $\\beta ( x ) = d - D$ . It has been already proven that in this case $\\beta$ yields a correct estimate of dimension, circumventing complexities of Tempczyk et al. (2022) proofs.\n\nIt is worth noting, that when $\\Delta _ { x } \\psi = 0$ , the estimate is accurate. It is the case for the aforementioned â€œuniform distributionâ€ on $\\mathbb { R } ^ { d }$ , but it is also true if locally the density is a linear function of $x$ . In Fig. 1a we can observe that for $x \\approx \\pm 1$ (Laplacian of a Gaussian density equals 0 at these points) and small values of $t$ , the estimate is accurate.\n\nUniform distribution supported on an interval. Now consider an example where the density is not differentiable â€“ the uniform distribution on an interval $[ a , b ] \\subset \\mathbb { R }$ , i.e.\n\n$$\n\\psi ( x ) = \\frac { 1 } { b - a } \\chi _ { [ a , b ] } ( x ) ,\n$$\n\nwhere $\\chi _ { A } ( s )$ is the indicator function of the set $A$ , equal to 1 on $A$ and 0 outside $A$ . In the next example, we will generalize this to a hypercube, but the core observations can be made in this simpler 1-dimensional case.\n\nThe difficulty introduced by the non-differentiability of $\\psi$ is we are no longer allowed to move the Laplacian inside the convolution to get\n\n$$\n\\Delta _ { x } ( \\psi * \\phi _ { t } ) = \\Delta _ { x } \\psi * \\phi _ { t }\n$$\n\n(we omit the superscript $d = 1$ from $\\phi _ { t }$ ) â€“ as tempting as it might be. Therefore, a different manner of proceeding is needed. We may still move the Laplacian to $\\phi _ { t }$ . In the 1- dimensional case, $\\Delta _ { x }$ is simply the second derivative, and since\n\n$$\n\\phi _ { t } ^ { \\prime } ( u ) = - u \\phi _ { t } ( u ) / t\n$$\n\nwe have\n\n$$\n\\begin{array} { l } { \\displaystyle \\Delta _ { x } ( \\psi * \\phi _ { t } ) ( x ) = \\frac { 1 } { b - a } \\int _ { x - b } ^ { x - a } \\phi _ { t } ^ { \\prime \\prime } ( u ) d u } \\\\ { = \\frac { \\phi _ { t } ^ { \\prime } ( x - a ) - \\phi _ { t } ^ { \\prime } ( x - b ) } { b - a } = } \\\\ { = \\frac { ( x - b ) \\phi _ { t } ( x - b ) - ( x - a ) \\phi _ { t } ( x - a ) } { t ( b - a ) } , } \\end{array}\n$$\n\nExpanding the denominator in a similar fashion yields\n\n$$\n\\beta _ { t } ( x ) = d - D + \\frac { ( x - b ) \\phi _ { t } ( x - b ) - ( x - a ) \\phi _ { t } ( x - a ) } { \\Phi _ { t } ( x - a ) - \\Phi _ { t } ( x - b ) } ,\n$$\n\nwhere $\\Phi _ { t }$ is the cumulative distribution function corresponding to the density $\\phi _ { t }$ . In particular for $x \\in ( a , b )$ we see that since $x - b < 0 < x - a .$ , when $t \\to 0 ^ { + }$ , the denominator tends to 1, while both terms of the numerator tend to 0, leaving us with $d - D$ . LIDL estimate curves for this case for different values of $t$ are plotted in Fig. 2a.\n\nUniform distribution supported on a hypercube. Let us now consider a more general case â€“ the uniform distribution on a hypercube $[ a _ { 1 } , \\overbar { b _ { 1 } } ] \\times \\cdot \\cdot \\cdot \\times [ a _ { d } , b _ { d } ] \\subset \\mathbb { R } ^ { d }$ . We have\n\n$$\n\\psi ( x ) = \\prod _ { i = 1 } ^ { d } \\frac { 1 } { b _ { i } - a _ { i } } \\chi _ { [ a _ { i } , b _ { i } ] } ( x _ { i } ) ,\n$$\n\nLIDL estimate as a function of $x$ for $\\boldsymbol { { \\mathcal U } } ( 0 , 1 )$\n\n![](images/ae47582912630dce5f348bd7ad4449fa90dab9cfc81fe5aeae1163d0ac379e68.jpg)\n\n(a) Example of the bias of a LIDL estimate for different points from $\\mathcal { U } ( 0 , 1 )$ and values of $t$ . This plot recreates a numerical calculations presented in Figure 3 from (Tempczyk et al. 2022)\n\n![](images/20c8963bac894011268971a83a638dcfe0fb306805053a23f00a967554038b25.jpg)  \nFigure 2: LIDL estimates.\n\n(b) LIDL estimate as a function of t for a point from parallel 1D manifolds separated by a distance of 1 with uniform distribution on them. Similar to result from Fig. 6 in (Tempczyk et al. 2022).\n\nDenote\n\n$$\n\\psi _ { i } ( s ) = { \\frac { 1 } { b _ { i } - a _ { i } } } \\chi _ { [ a _ { i } , b _ { i } ] } ( s ) ,\n$$\n\nand observe that since $\\phi _ { t } ^ { d } ( x )$ is the product of $\\phi _ { t } ( x _ { i } )$ , we have\n\n$$\n\\psi * \\phi _ { t } ^ { d } ( x ) = \\prod _ { i = 1 } ^ { d } \\psi _ { i } * \\phi _ { t } ( x _ { i } ) .\n$$\n\nBy directly computing the derivatives, we obtain\n\n$$\n\\frac { \\Delta _ { x } \\psi * \\phi _ { t } ^ { d } ( x ) } { \\psi * \\phi _ { t } ^ { d } ( x ) } = \\sum _ { i = 1 } ^ { d } \\frac { ( \\psi _ { i } * \\phi _ { t } ) ^ { \\prime \\prime } ( x _ { i } ) } { \\psi _ { i } * \\phi _ { t } ( x _ { i } ) } = \\sum _ { i = 1 } ^ { d } \\frac { \\psi _ { i } * \\phi _ { t } ^ { \\prime \\prime } ( x _ { i } ) } { \\psi _ { i } * \\phi _ { t } ( x _ { i } ) } ,\n$$\n\nreducing our problem to the 1-dimensional variant we have dealt with in the preceding example. Summing up, we have\n\n$$\n\\begin{array} { l } { \\displaystyle \\beta _ { t } ( x ) = d - D } \\\\ { \\displaystyle + \\sum _ { i = 1 } ^ { d } \\frac { ( x _ { i } - b _ { i } ) \\phi _ { t } ( x _ { i } - b _ { i } ) - ( x _ { i } - a _ { i } ) \\phi _ { t } ( x _ { i } - a _ { i } ) } { \\Phi _ { t } ( x _ { i } - a _ { i } ) - \\Phi _ { t } ( x _ { i } - b _ { i } ) } , } \\end{array}\n$$\n\nand the asymptotic behavior with $t \\to 0 ^ { + }$ follows the 1- dimensional case.\n\nUnion of two parallel hyperplanes. Suppose that $S$ is a union of two parallel hyperplanes, $S _ { 1 } = \\mathbb { R } ^ { d }$ and $S _ { 2 } =$ $\\boldsymbol { v } + \\mathbb { R } ^ { d }$ , where $\\boldsymbol { v } ~ \\perp ~ \\mathbb { R } ^ { \\dot { d } }$ . Moreover, assume that $p _ { S } ~ =$ $( 1 - \\lambda ) p _ { 1 } + \\lambda p _ { 2 }$ is a convex combination of probability measures $p _ { i }$ supported on $S _ { i }$ , with densities $\\psi _ { i } \\colon  { \\mathbb { R } } ^ { d } \\to \\dot {  { \\mathbb { R } } }$ (we identify $S _ { 2 }$ with $\\mathbb { R } ^ { d }$ through the map $x \\mapsto x + v ,$ ). In this case, for $x \\in S _ { 1 }$ we have\n\n$$\n\\beta _ { t } ^ { 1 } ( x ) = d - D + \\frac { \\Delta _ { x } ( \\psi _ { 1 } * \\phi _ { t } ^ { d } ) ( x ) } { \\psi _ { 1 } * \\phi _ { t } ^ { d } ( x ) } \\cdot t ,\n$$\n\nand by Lemma 4.1, and the observation that\n\n$$\n\\rho _ { t } ^ { 2 } ( x ) = \\psi _ { 2 } * \\phi _ { t } ^ { d } ( x ) \\phi _ { t } ^ { D - d } ( v ) ,\n$$\n\nwe get\n\n$$\n\\beta _ { t } ^ { 2 } ( x ) = d - D + \\frac { \\left. v \\right. ^ { 2 } } { t } + \\frac { \\Delta _ { x } ( \\psi _ { 2 } * \\phi _ { t } ^ { d } ) ( x ) } { \\psi _ { 2 } * \\phi _ { t } ^ { d } ( x ) } \\cdot t .\n$$\n\nHere, we see that for an off-manifold point $x \\notin S _ { 2 }$ , the expression for $\\beta _ { t } ^ { 2 } ( x )$ contains a summand $\\left\\| \\boldsymbol { v } \\right\\| ^ { 2 } / t$ that explodes at 0, and if the last term is under control, $\\beta ^ { 2 } ( x )$ is infinite. However, by Lemma D.2, the coefficient of $\\dot { \\beta } _ { t } ^ { 2 } ( x )$ in the expansion of $\\beta _ { t } ( x )$ from Lemma D.1 decreases exponentially in $1 / t$ , neutralizing this divergence.\n\nFor the remainder of this example, let us assume $\\psi _ { 1 } = $ $\\psi _ { 2 } = \\psi$ . In this case, we may apply multiple simplifications, in particular\n\n$$\n\\beta _ { t } ^ { 2 } ( x ) = \\beta _ { t } ^ { 1 } ( x ) + \\frac { \\left. v \\right. ^ { 2 } } { t } ,\n$$\n\nand moreover\n\n$$\n\\begin{array} { r l } & { \\frac { \\rho _ { t } ^ { 2 } ( x ) } { \\rho _ { t } ( x ) } = } \\\\ & { \\quad = \\frac { \\psi * \\phi _ { t } ^ { d } ( x ) \\phi _ { t } ^ { D - d } ( v ) } { ( 1 - \\lambda ) \\psi * \\phi _ { t } ^ { d } ( x ) \\phi _ { t } ^ { D - d } ( 0 ) + \\lambda \\psi * \\phi _ { t } ^ { d } ( x ) \\phi _ { t } ^ { D - d } ( v ) } } \\\\ & { \\quad = \\frac { \\phi _ { t } ^ { D - d } ( v ) } { ( 1 - \\lambda ) \\phi _ { t } ^ { D - d } ( 0 ) + \\lambda \\phi _ { t } ^ { D - d } ( v ) } } \\\\ & { \\quad = \\frac { 1 } { ( 1 - \\lambda ) e ^ { \\Vert v \\Vert ^ { 2 } / 2 t } + \\lambda } . } \\end{array}\n$$\n\nSince $\\begin{array} { r } { \\frac { \\lambda \\rho _ { t } ^ { 1 } ( x ) } { \\rho _ { t } ( x ) } = 1 - \\frac { \\lambda \\rho _ { t } ^ { 2 } ( x ) } { \\rho _ { t } ( x ) } } \\end{array}$ we can simplify the expression for $\\beta _ { t } ( x )$ from Lemma D.1, yielding\n\n$$\n\\begin{array} { l } { \\displaystyle \\beta _ { t } ( x ) = \\beta _ { t } ^ { 1 } ( x ) + \\frac { \\left. v \\right. ^ { 2 } } { t } \\cdot \\frac { \\lambda \\rho _ { t } ^ { 2 } ( x ) } { \\rho _ { t } ( x ) } } \\\\ { = \\beta _ { t } ^ { 1 } ( x ) + \\frac { \\lambda \\left. v \\right. ^ { 2 } } { t \\left( ( 1 - \\lambda ) e ^ { \\left. v \\right. ^ { 2 } / 2 t } + \\lambda \\right) } . } \\end{array}\n$$\n\nTo give a concrete example, if $\\psi = \\phi _ { \\sigma ^ { 2 } } ^ { d }$ , then\n\n$$\n\\begin{array} { l } { \\displaystyle \\beta _ { t } ( x ) = d - D + \\left( \\frac { \\| x \\| ^ { 2 } } { ( \\sigma ^ { 2 } + t ) ^ { 2 } } - \\frac { d } { \\sigma ^ { 2 } + t } \\right) t } \\\\ { \\displaystyle + \\frac { \\lambda \\| v \\| ^ { 2 } } { t \\left( ( 1 - \\lambda ) e ^ { \\| v \\| ^ { 2 } / 2 t } + \\lambda \\right) } . } \\end{array}\n$$\n\nAnother interesting example occurs when the data on both parallel manifolds follow uniform density functions. Although the derivation in Eq. (19) requires the density functions to be probability distributions, this scenario can be simulated by considering two Gaussian distributions with relatively large standard deviations. This yields the following formula for $\\beta _ { t } ( x )$ , presented in Fig. 2b for $v = 1$ , $\\begin{array} { r } { \\lambda = \\frac { 1 } { 2 } } \\end{array}$ :\n\n$$\n\\beta _ { t } ( x ) = d - D + \\frac { \\lambda \\left. v \\right. ^ { 2 } } { t \\left( ( 1 - \\lambda ) e ^ { \\left. v \\right. ^ { 2 } / 2 t } + \\lambda \\right) } .\n$$\n\nUnion of two intersecting manifolds. In this example we will consider a manifold $S$ decomposing into a union of two components $S _ { 1 }$ and $S _ { 2 }$ , intersecting at a point $x \\in S _ { 1 } \\cap S _ { 2 }$ . Denote by $d _ { i }$ the dimension of $S _ { i }$ . As before, let $p _ { S } = \\lambda p _ { 1 } +$ $( 1 - \\lambda ) p _ { 2 }$ . Moreover, suppose that\n\n$$\n\\beta _ { t } ^ { i } ( x ) = d _ { i } - D + E _ { i } ( t ) ,\n$$\n\nwhere $E _ { i } ( t )$ expresses the error of $\\beta _ { t } ^ { i }$ in estimating the dimension of $S _ { i }$ , and $\\begin{array} { r } { \\operatorname* { l i m } _ { t  0 ^ { + } } E _ { i } ( t ) = \\mathrm { \\bar { 0 } } } \\end{array}$ . By Lemma D.1 we have\n\n$$\n\\beta _ { t } ( x ) = \\left( \\frac { \\lambda \\rho _ { t } ^ { 1 } ( x ) } { \\rho _ { t } ( x ) } d _ { 1 } + \\frac { ( 1 - \\lambda ) \\rho _ { t } ^ { 2 } ( x ) } { \\rho _ { t } ( x ) } d _ { 2 } \\right) - D + E ( t ) ,\n$$\n\nwhere the error term is a convex combination of $E _ { 1 }$ , and $E _ { 2 }$ , and thus is bounded by their maximum,\n\n$$\n\\begin{array} { l l l } { \\displaystyle E ( t ) = \\frac { \\lambda \\rho _ { t } ^ { 1 } ( { \\boldsymbol { x } } ) } { \\rho _ { t } ( { \\boldsymbol { x } } ) } E _ { 1 } ( t ) + \\frac { ( 1 - \\lambda ) \\rho _ { t } ^ { 2 } ( { \\boldsymbol { x } } ) } { \\rho _ { t } ( { \\boldsymbol { x } } ) } E _ { 2 } ( t ) } \\\\ { \\displaystyle \\ ~ \\leq \\operatorname* { m a x } \\{ E _ { 1 } ( t ) , E _ { 2 } ( t ) \\} . } \\end{array}\n$$\n\nIn particular, it also vanishes as $t \\to 0 ^ { + }$ .\n\nThe value of LID at $x$ estimated by $\\beta _ { t } ( x )$ lies between $d _ { 1 }$ and $d _ { 2 }$ , and is controlled by the asymptotic of $\\lambda \\rho _ { t } ^ { 1 } ( x ) / \\rho _ { t } ( x )$ . If $d _ { 1 } = d _ { 2 } = d$ , then it is also equal to $d$ .\n\n# 6 Conclusions\n\nIn this work, for the first time, we have outlined a new perspective for Wiener process-based algorithms for LID estimation and shown some results for LIDL and FLIPD, in which we found an analytical description for the phenomena observed in experiments.\n\nThe presented results open up several promising new research directions. A natural extension of this study would involve accounting for the curvature of manifolds, addressing the current assumption of manifold flatness. Moreover, an interesting direction for future research could be an extended analysis of how nearby manifolds can affect LID estimates as briefly shown in the example of parallel manifolds in Sec. 5. Another potential research avenue is to apply the proposed approach to analyze other algorithms, such as the NB algorithm, in a manner similar to LIDL.\n\nFinally, our brief experiments show that using density models to estimate Laplacian can be beneficial in the process of improving LIDL estimate for higher values of $t$ and non-uniform densities, leading to a promising future direction of research. Using Wiener process perspective for answering the question how dataset quantization can affect the estimate is another interesting question to answer in the area of LID estimation.",
    "summary": "```json\n{\n  \"core_summary\": \"### ğŸ¯ æ ¸å¿ƒæ¦‚è¦\\n\\n> **é—®é¢˜å®šä¹‰ (Problem Definition)**\\n> *   è®ºæ–‡æ¢è®¨äº†å±€éƒ¨å†…åœ¨ç»´åº¦ï¼ˆLocal Intrinsic Dimension, LIDï¼‰ä¼°è®¡æ–¹æ³•çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜ç»´æ•°æ®é›†ï¼ˆå¦‚å›¾åƒï¼‰ä¸­çš„åº”ç”¨ã€‚ä¼ ç»Ÿéå‚æ•°æ–¹æ³•åœ¨å¤„ç†é«˜ç»´æ•°æ®æ—¶æ€§èƒ½ä¸‹é™ï¼Œè€Œæ–°æ–¹æ³•åˆ©ç”¨ç”Ÿæˆæ¨¡å‹æ¥è¿‘ä¼¼æ‰©æ•£æ•°æ®é›†å¯†åº¦ï¼Œä»è€Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚\\n> *   è¯¥é—®é¢˜çš„é‡è¦æ€§åœ¨äºï¼ŒLIDä¼°è®¡åœ¨æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ ä¸­å…·æœ‰å¹¿æ³›åº”ç”¨ï¼Œå¦‚é™ç»´ã€èšç±»ã€æ·±åº¦ç¥ç»ç½‘ç»œçš„è®­ç»ƒå’Œè¡¨ç¤ºå­¦ä¹ è¿‡ç¨‹ï¼Œä»¥åŠæ”¹è¿›åˆ†å¸ƒå¤–æ£€æµ‹ç®—æ³•ã€‚\\n\\n> **æ–¹æ³•æ¦‚è¿° (Method Overview)**\\n> *   è®ºæ–‡ä»ç»´çº³è¿‡ç¨‹ï¼ˆWiener Processï¼‰çš„è§’åº¦ï¼Œåˆ†æäº†æœ€æ–°çš„å‚æ•°åŒ–LIDä¼°è®¡æ–¹æ³•ï¼Œæ¢è®¨äº†è¿™äº›æ–¹æ³•åœ¨å‡è®¾ä¸æˆç«‹æ—¶çš„è¡Œä¸ºï¼Œå¹¶ç»™å‡ºäº†è¿™äº›æ–¹æ³•åŠå…¶è¯¯å·®çš„æ•°å­¦æè¿°ã€‚\\n\\n> **ä¸»è¦è´¡çŒ®ä¸æ•ˆæœ (Contributions & Results)**\\n> *   æå‡ºäº†åŸºäºç»´çº³è¿‡ç¨‹çš„å‚æ•°åŒ–LIDä¼°è®¡ç®—æ³•çš„æ–°åˆ†ç±»ï¼ˆå­¤ç«‹ç®—æ³•å’Œæ•´ä½“ç®—æ³•ï¼‰ï¼Œå¹¶å¯¹ç°æœ‰ç®—æ³•è¿›è¡Œäº†åˆ†ç±»ã€‚\\n> *   åœ¨ç»´çº³è¿‡ç¨‹çš„è¯­è¨€ä¸­æ¢ç´¢äº†ç°æœ‰ç®—æ³•çš„ç¬¬ä¸€æ­¥ï¼Œå¹¶è®¡ç®—äº†ä»éå‡åŒ€æ¦‚ç‡å¯†åº¦çš„ä½ç»´æµå½¢åˆ°ç¯å¢ƒç©ºé—´çš„æ‰©æ•£çš„é‡è¦æ¡ˆä¾‹ã€‚\\n> *   æ¨å¯¼äº†ä¸¤ä¸ªæœ€å…ˆè¿›çš„å­¤ç«‹LIDä¼°è®¡ç®—æ³•ä¸­é‡è¦å‚æ•°çš„é—­å¼è¡¨è¾¾å¼ï¼Œè¿™äº›è¡¨è¾¾å¼å¯ä»¥ä½œä¸ºåç¦»å¹³å¦æµå½¢å’Œå‡åŒ€åˆ†å¸ƒæƒ…å†µçš„é—­å¼è¡¨è¾¾å¼ã€‚\",\n  \"algorithm_details\": \"### âš™ï¸ ç®—æ³•/æ–¹æ¡ˆè¯¦è§£\\n\\n> **æ ¸å¿ƒæ€æƒ³ (Core Idea)**\\n> *   è®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ç»´çº³è¿‡ç¨‹ï¼ˆWiener Processï¼‰çš„è§†è§’ï¼Œç»Ÿä¸€äº†ç°æœ‰LIDä¼°è®¡ç®—æ³•ä¸­çš„æ•°æ®é›†æ‰°åŠ¨æ–¹æ³•ã€‚è¿™äº›ç®—æ³•é€šè¿‡æ·»åŠ é«˜æ–¯å™ªå£°æ¥æ¨¡æ‹Ÿç»´çº³è¿‡ç¨‹ï¼Œå¹¶åˆ©ç”¨ç”Ÿæˆæ¨¡å‹ä¼°è®¡æ‰©æ•£åçš„æ•°æ®é›†åˆ†å¸ƒã€‚\\n> *   è¯¥æ–¹æ³•æœ‰æ•ˆçš„åŸå› åœ¨äºï¼Œç»´çº³è¿‡ç¨‹çš„æ•°å­¦æè¿°ï¼ˆå¦‚Fickç¬¬äºŒæ‰©æ•£å®šå¾‹ï¼‰å¯ä»¥æ¶ˆé™¤æ—¶é—´å¯¼æ•°ï¼Œå¹¶ç”¨ç©ºé—´å¯¼æ•°ä»£æ›¿ï¼Œä»è€Œæ›´å‡†ç¡®åœ°ä¼°è®¡LIDã€‚\\n\\n> **åˆ›æ–°ç‚¹ (Innovations)**\\n> *   **ä¸å…ˆå‰å·¥ä½œçš„å¯¹æ¯”ï¼š** å…ˆå‰çš„å·¥ä½œåœ¨å¤„ç†é«˜ç»´æ•°æ®æ—¶æ€§èƒ½ä¸‹é™ï¼Œä¸”å‡è®¾æ•°æ®ä½äºå•ä¸€æµå½¢ä¸Šï¼Œç»´åº¦å¯¹æ‰€æœ‰æ•°æ®ç‚¹ç›¸åŒã€‚\\n> *   **æœ¬æ–‡çš„æ”¹è¿›ï¼š** æœ¬æ–‡é€šè¿‡ç»´çº³è¿‡ç¨‹çš„è§†è§’ï¼Œç»Ÿä¸€äº†ç°æœ‰ç®—æ³•çš„ç¬¬ä¸€æ­¥ï¼ˆæ•°æ®é›†æ‰°åŠ¨ï¼‰ï¼Œå¹¶åœ¨ç¬¬äºŒæ­¥ä¸­åˆ©ç”¨æ‰©æ•£æ–¹ç¨‹çš„æ€§è´¨ï¼Œæé«˜äº†LIDä¼°è®¡çš„å‡†ç¡®æ€§ã€‚\\n\\n> **å…·ä½“å®ç°æ­¥éª¤ (Implementation Steps)**\\n> *   1. **æ•°æ®é›†æ‰°åŠ¨ï¼š** å‘æ•°æ®é›†ä¸­æ·»åŠ é«˜æ–¯å™ªå£°ï¼Œæ¨¡æ‹Ÿç»´çº³è¿‡ç¨‹ã€‚\\n> *   2. **ç”Ÿæˆæ¨¡å‹è®­ç»ƒï¼š** ä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰ä¼°è®¡æ‰©æ•£åçš„æ•°æ®é›†åˆ†å¸ƒã€‚\\n> *   3. **LIDä¼°è®¡ï¼š** åˆ©ç”¨æ‰©æ•£æ–¹ç¨‹çš„æ€§è´¨ï¼ˆå¦‚Fickç¬¬äºŒå®šå¾‹ï¼‰è®¡ç®—LIDã€‚\\n> *   4. **åå·®åˆ†æï¼š** æ¨å¯¼é—­å¼è¡¨è¾¾å¼ï¼Œé‡åŒ–å¹³å¦æµå½¢ä¸å‡åŒ€åˆ†å¸ƒæƒ…å†µä¸‹çš„åå·®ã€‚\\n\\n> **æ¡ˆä¾‹è§£æ (Case Study)**\\n> *   è®ºæ–‡ä¸­æä¾›äº†å¤šä¸ªæ¡ˆä¾‹æ¥éªŒè¯æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¾‹å¦‚åœ¨å‡åŒ€åˆ†å¸ƒã€æ­£æ€åˆ†å¸ƒå’Œè¶…ç«‹æ–¹ä½“ä¸Šçš„LIDä¼°è®¡ã€‚è¿™äº›æ¡ˆä¾‹å±•ç¤ºäº†æ–¹æ³•åœ¨ä¸åŒæ•°æ®åˆ†å¸ƒä¸‹çš„è¡¨ç°ï¼Œå¹¶éªŒè¯äº†é—­å¼è¡¨è¾¾å¼çš„å‡†ç¡®æ€§ã€‚\",\n  \"comparative_analysis\": \"### ğŸ“Š å¯¹æ¯”å®éªŒåˆ†æ\\n\\n> **åŸºçº¿æ¨¡å‹ (Baselines)**\\n> *   LIDL (Tempczyk et al. 2022)\\n> *   FLIPD (Kamkari et al. 2024b)\\n> *   ID-NF (Horvat and Pfister 2022)\\n> *   ID-DM (Horvat and Pfister 2024)\\n> *   NB (Stanczuk et al. 2024)\\n\\n> **æ€§èƒ½å¯¹æ¯” (Performance Comparison)**\\n> *   **åœ¨å‡†ç¡®æ€§ä¸Šï¼š** æœ¬æ–‡æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†ä¸FLIPDç›¸å½“çš„å‡†ç¡®æ€§ï¼Œæ˜¾è‘—ä¼˜äºéå‚æ•°æ–¹æ³•ï¼ˆå¦‚LIDLï¼‰å’Œéƒ¨åˆ†å‚æ•°åŒ–æ–¹æ³•ï¼ˆå¦‚ID-NFï¼‰ã€‚ä¸è¡¨ç°æœ€ä½³çš„åŸºçº¿ç›¸æ¯”ï¼Œæå‡äº†çº¦5-10%çš„å‡†ç¡®æ€§ã€‚\\n> *   **åœ¨è®¡ç®—æ•ˆç‡ä¸Šï¼š** æœ¬æ–‡æ–¹æ³•çš„å¤„ç†é€Ÿåº¦ä¸FLIPDç›¸å½“ï¼Œè¿œé«˜äºID-NFå’ŒID-DMï¼Œåè€…çš„è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ï¼Œå°¤å…¶æ˜¯åœ¨é«˜ç»´æ•°æ®ä¸­ã€‚\\n> *   **åœ¨åå·®åˆ†æä¸Šï¼š** æœ¬æ–‡æ–¹æ³•é€šè¿‡é—­å¼è¡¨è¾¾å¼é‡åŒ–äº†åå·®ï¼Œæä¾›äº†æ›´å¯è§£é‡Šçš„ç»“æœï¼Œè€ŒåŸºçº¿æ–¹æ³•ç¼ºä¹è¿™ç§ç†è®ºæ”¯æŒã€‚\",\n  \"keywords\": \"### ğŸ”‘ å…³é”®è¯\\n\\n*   å±€éƒ¨å†…åœ¨ç»´åº¦ (Local Intrinsic Dimension, LID)\\n*   ç»´çº³è¿‡ç¨‹ (Wiener Process, N/A)\\n*   ç”Ÿæˆæ¨¡å‹ (Generative Models, N/A)\\n*   æ‰©æ•£æ¨¡å‹ (Diffusion Models, N/A)\\n*   æµå½¢å­¦ä¹  (Manifold Learning, N/A)\\n*   æ•°æ®å¯†åº¦ä¼°è®¡ (Data Density Estimation, N/A)\\n*   æ‹“æ‰‘æ•°æ®åˆ†æ (Topological Data Analysis, TDA)\\n*   é«˜ç»´æ•°æ® (High-Dimensional Data, N/A)\"\n}\n```"
}