{
    "source": "Semantic Scholar",
    "arxiv_id": "2407.11242",
    "link": "https://arxiv.org/abs/2407.11242",
    "pdf_link": "https://arxiv.org/pdf/2407.11242.pdf",
    "title": "Bridging Sequence-Structure Alignment in RNA Foundation Models",
    "authors": [
        "Heng Yang",
        "Renzhi Chen",
        "Ke Li"
    ],
    "categories": [
        "cs.CL"
    ],
    "publication_date": "2024-07-15",
    "venue": "未找到发表会议",
    "fields_of_study": [
        "Biology",
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "institutions": [
        "University of Exeter",
        "Qiyuan Lab"
    ],
    "paper_content": "# Bridging Sequence-Structure Alignment in RNA Foundation Models\n\nHeng Yang1, Renzhi Chen2, Ke Li1\\*\n\n1 Department of Computer Science, University of Exeter, EX4 4QF, Exeter, UK 2 Qiyuan Lab, Beijing, China {hy345, k.li}@exeter.ac.uk, {chengrenzhi1989}@gmail.com\n\n# Abstract\n\nThe alignment between RNA sequences and structures in foundation models (FMs) has yet to be thoroughly investigated. Existing FMs have struggled to establish sequencestructure alignment, hindering the free flow of genomic information between RNA sequences and structures. In this study, we introduce OmniGenome, an RNA FM trained to align RNA sequences with respect to secondary structures based on structure-contextualised modelling. The alignment enables free and bidirectional mappings between sequences and structures by utilising the flexible RNA modelling paradigm that supports versatile input and output modalities, i.e., sequence and/or structure as input/output. We implement RNA design and zero-shot secondary structure prediction as case studies to evaluate the $\\operatorname { S e q } 2 \\operatorname { S t r }$ and Str2Seq mapping capacity of OmniGenome. Results on the EternaV2 benchmark show that OmniGenome solved $7 4 \\%$ of puzzles, whereas existing FMs only solved up to $3 \\%$ of the puzzles due to the oversight of sequence-structure alignment. We leverage four comprehensive in-silico genome modelling benchmarks to evaluate performance across a diverse set of genome downstream tasks, where the results show that OmniGenome achieves state-of-the-art performance on RNA and DNA benchmarks, even without any training on DNA genomes.\n\nCode — https://github.com/yangheng95/OmniGenBench Datasets — https://huggingface.co/spaces/yangheng/ OmniGenomeLeaderboard/tree/main/benchmarks Extended version — https://arxiv.org/abs/2407.11242\n\n# 1 Introduction\n\nRNA is a critical type of molecule that encodes a vast array of biological regulatory elements that orchestrate crucial aspects of plant growth, development, and adaptation to environmental stresses. To decipher the genomic code in RNA and manipulate RNA engineering and design, current research mainly uses bioinformatics in solving RNA genomeoriented challenges. Recent advancements in large-scale pre-trained foundation models (FMs) have demonstrated their unprecedented potential to back up existing genome analysis, as FMs are capable of learning and predicting the complex ‘genomic language’ (Nguyen et al. 2023) hidden in genome encoding processes. Existing FMs have been widely employed as basic sequence feature extractors to improve the performance of diverse genome analysis tasks, such as secondary structure prediction (Tan et al. 2017; Danaee et al. 2018; Mathews 2019; Kalvari et al. 2021), degradation rate prediction (Yaish and Orenstein 2022; Wayment-Steele et al. 2022), and mRNA vaccine design (Corbett et al. 2020; Runge et al. 2023). In RNA, it is intriguing that the functionality and stability are intertwined with its complex structures in molecular biology (Ganser et al. 2019). However, the role of the structure as a second ‘genomic language’ to interact with sequences and solve various RNA downstream tasks has been largely ignored.\n\nSequence-Structure Alignment in GFMs We define alignment between sequences and secondary structures1 as the bidirectional information flows. Current FMs have been struggling to establish an alignment between RNA nucleotide sequences and their folded structures, thus impeding bidirectional genomic information flows. There has been a deep scientific challenge to align RNA sequences with structures because it is not deterministic to predict sequences from structures and vice versa. In other words, an identical sequence may be folded into different sub-optimal structures because the folding patterns of RNA sequences depend on various in-vivo factors (Tinoco Jr and Bustamante 1999). Further, a structure can be folded from different sequences composed of variational combinations of nucleotide bases. The oversight of such alignment in existing FMs causes outstanding issues in understanding and leveraging RNA structures, such as mRNA design. For example, recent state-ofthe-art RNA FMs, RNA-FM (Chen et al. 2022) and RNAMSM (Zhang et al. 2024), only solved 3 out of $1 0 0 { \\mathrm { ~ p u z } } -$ zles in in-silico RNA design (Lee et al. 2014). This is because they fail to decipher corresponding sequences based on structures to guide RNA design.\n\nTo address the above two problems, we propose sequencestructure alignment in RNA FMs, which leverages the largescale annotations of sequences and structures to build reliable structure to sequence $\\mathrm { ( S t r 2 S e q ) }$ and sequence to structure (Seq2Str) mappings, leading to an aligned FM dubbed OmniGenome. The sequence-structure alignment enables genomic information to freely flow between sequences and structures by introducing a flexible RNA modelling paradigm that supports versatile inputs and outputs modalities, i.e., sequence and/or structure as input/output. The sequence-structure alignment enables genomics information to freely flow between sequences and structures by introducing a flexible RNA modelling paradigm that supports versatile inputs and outputs modalities, i.e., sequence and/or structure as input/output. Furthermore, the sequencestructure alignment is designed to be architecture-agnostic and genome-agnostic. That is to say, it can be easily transferred to large-scale models with new architecture and different genome types like DNA.\n\nStr2Seq Mapping RNA structure serves as a vital input in most of the RNA genome analysis tasks. To induce the ability of Str2Seq mapping in genomic FMs, we formulate a structure-contextualised RNA sequence reconstruction task, which stems from the representation of RNA secondary structures in texts composed of dots and brackets. We first concatenate sequence-structure pairs as inputs and then mask a small portion of nucleotide bases in the sequence. Then, we pre-train OmniGenome to reconstruct the masked nucleotide bases given the structure contexts. This simple but effective formulation of Str2Seq mapping realises structure input awareness in genomics FM pre-training and provides substantial compatibility for structure-contextualised tasks, which has been verified in our RNA design benchmark.\n\nSeq2Str Mapping On the other hand, Seq2Str mapping, such as end-to-end secondary structure prediction (SSP) (Sato, Akiyama, and Sakakibara 2021; Fu et al. 2022), is another critical aspect of achieving the alignment. We generalise end-to-end structure pre-training (Yan, Hamilton, and Blanchette 2022) to OmniGenome pre-training. This large-scale structure pre-training on diversified genomes supervises OmniGenome to perform Seq2Str mapping. The problem of structure pre-training lies in RNA structure annotation scarcity, which leads to biased structure predictions (Chen et al. 2020) and barriers the structure prediction robustness on small datasets. To conduct Seq2Str mapping, tremendous secondary structure annotations are required to avoid data bias. A feasible solution to RNA structure pre-training is leveraging the plausible structures calculated based on the minimum free energy. In this paper, we leverage the popular ViennaRNA (Lorenz et al. 2011) to serve our purpose, ‘computing’ the structures for millions of RNA sequences and perform structure pre-training in OmniGenome.\n\nEvaluations and Results To validate the effectiveness of OmniGenome, we designed four large-scale genome benchmarks with diverse genomics tasks. The first one is the RNA genomics benchmark (RGB) compiled in the study, which contains diverse challenging genomics understanding tasks that benefit from the sequence-structure alignment, such as degradation rate prediction. The second benchmark is the plant genomics benchmark (PGB) (Mendoza-Revilla et al. 2023) which contains millions of DNA sequences to evaluate the DNA sequence understanding tasks. In particular, we want to use this benchmark to evaluate the generalisability of OmniGenome among diversified species and genomes. The overall performance of OmniGenome (up to 186M parameters) on both two benchmarks consistently outperforms existing genomics FMs with up to $3 5 \\%$ improvement, even compared with Agro-NT (Mendoza-Revilla et al. 2023) which contains 1 billion parameters. The last two benchmarks, available in the Appendix2, are the genomics benchmark (GB) (Gresˇova´ et al. 2023) and genomics understanding evaluation (GUE) (Zhou et al. 2023), which serve as two additional DNA benchmarks to evaluate generalisability on non-plant genome modelling.\n\nIn addition, we also conduct zero-shot $\\operatorname { S e q } 2 \\operatorname { S t r }$ and Str2Seq prediction experiments to verify the performance of sequence-structure alignment. As revealed in the experiments in Sections 3.4 and 3.4, OmniGenome achieves up to a $7 4 . 8 5 \\%$ macro-F1 score in zero-shot Seq2Str prediction, i.e., secondary structure prediction, outperforming finetuned FMs and bioinformatics methods like ViennaRNA. In terms of $\\mathrm { S t r } 2 \\mathrm { S e q }$ prediction performance, we evaluate the performance of OmniGenome in the in-silico RNA design task. We solved $7 4 \\%$ of complex puzzles of the EternaV2 benchmark (Lee et al. 2014), while state-of-the-art FMs such as RNA-MSM and RNA-FM only solved up to $3 \\%$ . Besides, OmniGenome only takes less than one hour to solve most of the puzzles, while most RNA design methods need to take up to 24 hours to solve even a single puzzle.\n\nOpen-source Toolkit and Tutorials Open science is always the golden standard to promote this rising area of FM for genome modelling, which unfortunately lacks relevant high-quality resources such as code integrity, data availability, and pre-training pipeline. To address this gap, following the FAIR principles (Wilkinson et al. 2016), we developed an open-source package3 that includes step-by-step tutorials for FM pre-training and downstream tasks fine-tuning, to name a few. It provides ready-to-use genomics benchmarks and uses the API with only a few lines of code to streamline benchmarking purposes. We believe this will be a valuable resource to make this emerging AI for the RNA community to thrive.\n\n# 2 Methodology\n\nThis section delineates the implementation details of OmniGenome including its entire pre-training workflow and downstream benchmarks.\n\n# 2.1 RNA Tokenization for Alignment\n\nWe aim to implement a fine-grained alignment between RNA sequences and structures, where each base in the sequences reflects a structural label in $\\{ \\mathit { \\Sigma } ^ { \\bullet } ( \\mathit { \\Sigma } ^ { \\bullet } , \\mathit { \\Sigma } ^ { \\bullet } ) \\mathit { \\Sigma } ^ { , \\bullet } , \\mathit { \\Sigma } ^ { \\bullet } \\}$ . Therefore, we propose an adapted implementation of the single nucleotide tokenization (SNT) method (Nguyen et al. 2023; Chen et al. 2023) in OmniGenome, where the whole vocabulary, $\\{ { ^ { \\circ } \\mathrm { \\mathbb { A } } } , { ^ { \\circ } \\mathrm { \\mathbb { T } } } , { ^ { \\circ } \\mathrm { C } } ^ { , } , { ^ { \\circ } \\mathrm { G } } ^ { , } , { ^ { \\circ } \\mathrm { U } } ^ { , } , { ^ { \\circ } \\mathrm { N } } ^ { , } , { ^ { \\circ } \\mathrm { \\mathbb { \\Lambda } } } ( { ^ { \\circ } } , { ^ { \\circ } \\mathrm { \\Lambda } } ^ { , } ) , { ^ { \\circ } \\mathrm { \\Lambda } } . { ^ { \\circ } \\mathrm { \\mathbb { J } } } \\} ,$ , contains the nucleotide-level structural labels. We illustrate our tokenization based on an example shown in the extended version.\n\nOur adapted SNT features bidirectional mappings between single nucleotide (SN) bases and structural labels required by sequence-structure alignment. Another reason for the adaption of SNT is that, in the realm of RNA genome modelling, the FM performance highly depends on the tokenization resolution (Nguyen et al. 2023; Chen et al. 2023). For example, the $\\mathbf { k }$ -mers (Yang et al. 2023; Dalla-Torre et al. 2023) and BPE (Devlin et al. 2019; Zhou et al. 2023) tokenization methods combine multiple bases into tokens and embeddings, which compromise modelling resolution and thus fail to the solution of fine-grained genomic tasks like structure prediction as well as base-level degrade rate prediction. Like other encoder-only models, e..g, BERT (Devlin et al. 2019), we incorporated special tokens, e.g., ‘<mask>’, to implement masked language modelling.\n\n# 2.2 Pre-training Objectives\n\nAs discussed in Section 1, a key desideratum for SN-level genome modelling is to build the alignment between RNA sequences with corresponding secondary structures. Bearing this in mind, we formulate two pre-training objectives, i.e., $\\mathcal { L } _ { \\mathrm { { S t r 2 S e q } } }$ and $\\mathcal { L } _ { \\mathrm { { S e q 2 S t r } } }$ , for $\\mathrm { S t r } 2 \\mathrm { S e q }$ and $\\operatorname { S e q } 2 \\operatorname { S t r }$ predictions, respectively. Besides, we aggregate these two objectives with the masked RNA language modelling objective MRLM to pre-train OmniGenome as follows:\n\n$$\n\\begin{array} { r } { \\mathcal { L } _ { \\mathtt { p r e - t r a i n } } = \\mathcal { L } _ { \\mathtt { S t r 2 S e q } } + \\mathcal { L } _ { \\mathtt { S e q 2 S t r } } + \\mathcal { L } _ { \\mathtt { M R L M } } + \\lambda | | \\theta | | _ { 2 } , } \\end{array}\n$$\n\nwhere $\\lambda$ is the $\\ell _ { 2 }$ regularisation weight and $\\theta$ represents the parameters of OmniGenome. The following paragraphs explain the design principles of each objective function used in equation (1).\n\n• $\\mathcal { L } _ { \\mathrm { { s t r 2 S e q } } }$ is designed to enable OmniGenome to predict bases given structure-contextualised sequences with partially masked bases. This objective aims at $\\mathtt { S t r 2 S e q }$ tasks and teaches OmniGenome to interpret structure information and infer the masked sequences. To achieve this objective, we mask $1 5 \\%$ of the bases and structure tokens, encouraging the model to infer masked bases (i.e., $\\{ { ^ { \\bullet } \\mathbb { A } } ^ { \\prime } , { ^ { \\bullet } \\mathbb { T } } ^ { \\prime } , { ^ { \\bullet } \\mathbb { C } } ^ { \\prime } , { ^ { \\bullet } \\mathbb { G } } ^ { \\prime } , { ^ { \\bullet } \\mathbb { U } } ^ { \\prime } , { ^ { \\bullet } \\mathbb { N } } ^ { \\prime } \\}$ ) and structure tokens (i.e., $\\{ ^ { \\cdot } ( ^ { } , ^ { \\cdot } ) ^ { , } , ^ { \\cdot } . ^ { , } \\} \\rangle$ . Specifically, $\\mathcal { L } _ { \\mathrm { s t r } 2 \\mathrm { s } \\ominus \\mathrm { q } }$ is defined as the classic cross-entropy loss widely used in the masked language modelling:\n\n$$\n\\mathcal { L } _ { \\mathrm { S t r } 2 \\mathrm { S e q } } = - \\frac { 1 } { | m | } \\sum _ { i = 1 } ^ { m } \\log p ( x _ { i } \\mid x _ { \\backslash i } ) ,\n$$\n\nwhere $m$ is the number of masked nucleotide and structure tokens, and $p ( \\boldsymbol { x } _ { i } | \\boldsymbol { x } _ { \\backslash i } )$ indicates the probability of predicting the masked nucleotide $x _ { i }$ based on its context.\n\n• In terms of structure-out modelling, we implement $\\mathscr { L } _ { \\mathtt { S e q 2 S t r } }$ to enable OmniGenome for $\\mathrm { S e q 2 S t r }$ predictions. Instead of directly feeding the secondary structure into OmniGenome as inputs, this objective employs the RNA secondary structures as labels for supervised training. This objective is implemented as a token-level classification, where the $\\mathcal { L } _ { \\mathtt { S e q 2 S t r } }$ loss is defined in the following cross-entropy loss:\n\n$$\n\\mathcal { L } _ { \\mathrm { S e q 2 S t r } } = - \\sum _ { i = 1 } ^ { N } \\sum _ { c = 1 } ^ { C } s _ { i c } \\log ( \\hat { s } _ { i c } ) ,\n$$\n\nwhere $s _ { i c }$ denotes the label $c$ of secondary structure at the $i$ -th position, and $\\hat { s } _ { i c }$ is the probability predicted by a linear classifier deployed on OmniGenome. $N$ is the length of an RNA sequence and $C = 3$ denotes the number of the possible labels of structure, i.e., $\\{ ^ { \\cdot } ( ^ { } , ^ { \\cdot } ) ^ { , } , ^ { \\cdot } . ^ { , } \\}$ .\n\n• The last objective, $\\mathcal { L } _ { \\mathrm { { M R L M } } }$ , is adapted to the conventional masked language modelling loss in NLP. It aims to improve the model’s understanding of genomic language in RNA sequences by predicting the masked or replaced $5 \\%$ of nucleotide bases. The definition of $\\mathcal { L } _ { \\mathrm { { M R L M } } }$ is similar to that of $\\mathcal { L } _ { \\mathrm { { S t r 2 S e q } } }$ which only considers the prediction of masked bases rather than randomly replaced bases. The loss function of MRLM is well-known so we omit its formula here.\n\nWe cannot trust structure predictions (in $\\mathcal { L } _ { \\mathrm { { S e q 2 S t r } } } ,$ ) while the structures are leaked in inputs (in $\\mathcal { L } _ { \\mathrm { { s t r 2 S e q } } } )$ , i.e., the sequence inputs and outputs of these two objectives are exclusive. In practice, we only consider objectives either $\\mathcal { L } _ { \\mathtt { S e q 2 S t r } } + \\mathcal { L } _ { \\mathtt { M R L M } }$ or $\\mathcal { L } _ { \\mathrm { S t r } 2 \\mathrm { S e q } } + \\mathcal { L } _ { \\mathrm { M R L M } }$ for each input sequence. In the pre-training, $7 0 \\%$ of RNA sequences are used for the first two objectives, while the remaining $3 0 \\%$ are used for the latter two objectives. This proportion setting is concluded from our empirical experience to balance the capability of $\\mathtt { S t r 2 S e q }$ and Seq2Str predictions.\n\n# 2.3 Model Architecture\n\nOmniGenome adopts the Transformer encoder architecture with bidirectional multi-head attention. We do not adopt recent architectures like Mamba (Gu and Dao 2023; Schiff et al. 2024) and Hyena (Nguyen et al. 2023) because our experiments in Table 4 and Table 5 show that these architectures are not competent at RNA genome understanding. This low performance is probably because RNA sequences are much shorter than DNA sequences in the wild.\n\nWe designed two variants, dubbed OmniGenome52M and OmniGenome186M with 52 and 186 million parameters respectively. Some key model specifications are summarised in Table 1.\n\nTo improve the reproducibility of OmniGenome, we list the pre-training settings and hyperparameters as follows.\n\n• The learning rate is set to $5 \\times 1 0 ^ { - 5 }$ and the weight decay is set to 0.01.   \n• We use AdamW as the optimiser with hyperparameters $\\beta _ { 1 } = 0 . 9$ and $\\beta _ { 2 } = 0 . 9 9 9$ .   \n• We use a linear decay strategy with a warm-up period of 1, 000 steps in the learning rate scheduler.   \n• The batch size is set to 2, 048.   \n• No dropout is applied during pre-training, and we use the rotary position embeddings (Su et al. 2024) to further enhance the model’s scalability to long RNA sequences.\n\nU A M U C A ( M . . ) ) U A MC U C A ( GM . . ) ) Curate Str2Seq Input Input Embeddings Self-Attention Add & Norm Feed Forward Add & Norm Output Layers Predict Reconstructed Seq/Str Pair A C U C A G G U U C ( ( ( . . . ) ) ) . . . Seq2Str Input Predicted Structure U A C U C M G G U U C A A MLM Input x 32 Layers Unmasked Sequence\n\nTable 1: Summary of some key model specifications of two OmniGenome variants. $^ { 6 6 } \\ast ^ { 3 9 }$ means that we used a modelling length of 1024 in the pre-training, while the supports up to 4096 in downstream tasks.   \n\n<html><body><table><tr><td>OmniGenome</td><td>52M</td><td>186M</td></tr><tr><td># of Layers</td><td>16</td><td>32</td></tr><tr><td>Embedding dimension</td><td>480</td><td>720</td></tr><tr><td>Intermediate dimension</td><td>2,400</td><td>2,560</td></tr><tr><td># of heads</td><td>24</td><td>30</td></tr><tr><td>#of parameters</td><td>52M</td><td>186M</td></tr><tr><td>Modelling length</td><td>4,096*</td><td></td></tr></table></body></html>\n\n• We built a distributed training environment with 8 NVIDIA RTX 4090 GPUs, while its configuration is introduced in the Appendix. The pre-training was finished in approximately 1 and 3 weeks for OmniGenome52M and OmniGenome186M, respectively.\n\n# 2.4 Pre-training Database: OneKP\n\nRecent studies (Chen et al. 2023; Zhou et al. 2023) have shown that data diversity can enhance FM performance without significantly increasing model capacity. For the OmniGenome pre-training, we collected transcriptome data from the OneKP initiative4 (Carpenter, Leebens-Mack, and et al. 2019), which compiles large-scale RNA raw sequence database from 1, 124 plant species. The raw sequences are not available for pre-training before processing and filtering.\n\nWe adopt the following raw sequence data curation protocol to fit pre-training.\n\n• To enhance training efficiency and reduce bias, we removed all duplicate sequences.   \n• To tackle incomplete transcriptome data and other noises, we discard sequences shorter than 50 bases.   \n• To facilitate the sequence-structure alignment training, we adopt ViennaRNA5 to obtain the secondary structures for the sequences.\n\n• We use “cd-hit-est” (Li and Godzik 2006) and blast (Altschul et al. 1990) tools to filter the sequences in downstream tasks with similar structures. Please refer to the experiment section for more details.\n\n# 2.5 Benchmark Suites\n\nOmniGenome is designed as a general-purpose RNA FM that can be fine-tuned for a diverse set of downstream genomics predictive tasks. In this paper, we constructed a large-scale benchmark suite for RNA FMs. According to the category of genomes, we split the benchmark into two parts, i.e, RNA Genomic Benchmark (RGB) and Plant Genomic Benchmark (PGB). Please refer to the appendix for the benchmark details.\n\n# 3 Experiments\n\nTo evaluate the performance of OmniGenome across genome modelling, we implement experiments on diverse downstream tasks. We first evaluate the sequence-structure alignment capability of OmniGenome. Subsequently, we evaluate the overall performance of OmniGenome on two comprehensive genomic modelling benchmarks, i.e., RGB and the PGB, respectively. Finally, we include the GB and GUE in the appendix to evaluate the performance on nonplant genomes.\n\n# 3.1 RNA Sequence Filtering\n\nThe pertaining involves RNA sequences and structures prediction, we take the data and annotation leakage problem seriously.\n\n• To avoid structure annotation leakage of downstream benchmarks, the secondary structure predictors for all FMs were randomly initialised for fair comparisons, which means the pre-trained structure predictor of OmniGenome was not used in benchmarks, except for zero-shot SSP experiments. Please find the source codes for details. • To reduce sequence leakage caused by evolutionary conservative sequences across multiple species, we use the ch-hit-est tool to calculate the sequence similarity between sequences from the OneKP database and downstream tasks. We adopt the similarity threshold of $8 0 \\%$ for ch-hit-est to eliminate sequences whose homogeneous sequences appeared in the OneKP database. Subsequently, we exploit the blastn tool to query potentially leaked sequences in downstream benchmark datasets and further alleviate the data leakage problem. The e-value has been set to 1 for rigorous sequence filtering.\n\n# 3.2 Pre-training and Evaluation Environment\n\nThe pre-training of OmniGenome was conducted on a dedicated Linux computation node, equipped with 8 NVIDIA RTX 4090 GPUs. For distributed model training, we employed version 4.37.1 of the Transformers library alongside version 0.26.1 of the Accelerate library. Our implementation framework of choice for OmniGenome was PyTorch, specifically version 2.0.0. The ViennaRNA version is 2.6.4 in our experiments. While some existing code was adapted for the modules within OmniGenome, the majority of the codebase, such as genomic sequences preprocessing, model pre-training, objective functions, and experiments, was meticulously crafted from scratch.\n\n# 3.3 Comparison Baselines\n\nApart from OmniGenome, we implement a plus variant, i.e., OmniGenome $+$ . In the context of OmniGenome+, we assume the structure annotation from ViennaRNA is always available for enhancing the model based on structurecontextualised modelling. In SSP tasks, we can also use the ViennaRNA’s structure annotations as contexts to improve downstream SSP performance. Please refer to the Appendix for brief introductions of these FMs.\n\nWe can compare OmniGenome with the following RNA and DNA FMs shown in the Appendix as baselines to help evaluate the performance of OmniGenome. We are aware that some FMs are also developed for RNA, such as UniRNA (Wang et al. 2023), 5UTR-LM (Chu et al. 2024), etc. However, we cannot compare OmniGenome with them because their source codes are very hard to work with in our efforts or not publicly available.\n\n# 3.4 Sequence-Structure Alignment Evaluation\n\nIn this section, we verify the sequence-structure alignment capability based on two experiments, i.e., Str2Seq prediction and zero-shot Seq2Str prediction via SSP and RNA design tasks, respectively. Overall, the results in Table 2 and Table 3 provide reliable evaluations of the FMs’ capabilities in sequence-structure alignment. This underscores OmniGenome’s efficacy in enabling genomic information to freely flow between structures and sequences.\n\nRNA Design (Str2Seq) Evaluation we demonstrate the Str2Seq prediction capability of OmniGenome based on RNA design. We employed the Eterna (Lee et al. 2014) V2 benchmark, which consists of 100 specified secondary structures. This task aims to design RNA sequences based on reference structures. We develop a genetic algorithm (GA) which exploits masked nucleotide modelling (a.k.a., masked language modelling) to find plausible RNA sequences that solve RNA design puzzles. The implementation details can be found in the Appendix. In the GA, the population size is set at 1000, with 100 iterations, and the mutation rate for each base is 0.5. The evaluation metric is accuracy following existing works which indicates the number of puzzles solved by FMs. The experimental results are available in Table 2.\n\nTable 2: Performance on the EternaV2 RNA design benchmark. The best accuracy is in bold face. “Token.” indicates the tokenization method.   \n\n<html><body><table><tr><td>Model</td><td>Token.</td><td>EternaV2 (Acc)</td></tr><tr><td>RNAInverse</td><td>二</td><td>30</td></tr><tr><td>3UTRBERT</td><td>k-mers</td><td>0</td></tr><tr><td>DNABERT2</td><td>BPE</td><td>0</td></tr><tr><td>SpliceBERT</td><td>SNT</td><td>3</td></tr><tr><td>RNA-MSM</td><td>SNT</td><td>2</td></tr><tr><td>RNA-FM</td><td>SNT</td><td>3</td></tr><tr><td>OmniGenome52M+</td><td>SNT</td><td>71</td></tr><tr><td>OmniGenome186M+</td><td>SNT</td><td>74</td></tr></table></body></html>\n\nTable 3: Performance in zero-shot SSP. The results are based on zero-shot inferences without any fine-tuning or domain adaptation. “Stralign” denotes the RNAStralign dataset.   \n\n<html><body><table><tr><td rowspan=\"2\">Model</td><td colspan=\"3\">Zero-shot SSP (F1)</td></tr><tr><td>Archive2</td><td>bpRNA</td><td>Stralign</td></tr><tr><td>ViennaRNA</td><td>73.99</td><td>65.04</td><td>74.09</td></tr><tr><td>OmniGenome52M</td><td>69.93</td><td>65.85</td><td>74.71</td></tr><tr><td>OmniGenome186M</td><td>74.38</td><td>66.19</td><td>74.91</td></tr><tr><td>OmniGenome52M+</td><td>73.58</td><td>65.95</td><td>75.16</td></tr><tr><td>OmniGenome186M+</td><td>74.72</td><td>66.37</td><td>75.80</td></tr></table></body></html>\n\nWe include a popular baseline of RNAInverse and select recent DNA and RNA FMs which support masked language modelling. We exclude HyenaDNA in this experiment because it does not support masked nucleotide prediction. It is observed from Table 2 that RNAInverse solved 30 of the RNA design puzzles, indicating a promising capability in RNA design. The FMs, such as 3UTRBERT and DNABERT2 fail in RNA design because they cannot handle SN-level modelling. Meanwhile, RNA-MSM, RNA-FM and SpliceBERT demonstrated trivial proficiency in RNA design, solving 2 to 3 puzzles. This observation suggests these FMs cannot precisely predict the bases without any Str2Seq prediction ability. With the help of $\\mathrm { S t r } 2 \\mathrm { S e q }$ , i.e., structure-contextualised sequence reconstruction, OmniGenome $^ { 5 2 \\mathrm { M } } +$ and OmniGenome $^ { 1 8 6 \\mathrm { M } } +$ significantly outperformed other FMs with 71 and 74 puzzles solved, respectively, underscoring the significance of $\\mathtt { S t r 2 S e q }$ in sequence-structure alignment. Besides, we expect an increase in performance with sufficient computational budgets and the findings provide crucial evidence of the significance of Str2Seq for RNA sequence design.\n\nZero-shot SSP (Seq2Str) Evaluation This subsection evaluates both Seq2Str and Str2Seq prediction in sequencestructure alignment. The evaluation of $\\operatorname { S e q } 2 \\operatorname { S t r }$ is based on zero-shot SSP. We use OmniGenome and OmniGenome $+$ without fine-tuning to predict the secondary structures of sequences from the testing datasets and measure the macro-F1 score, where better structure prediction performance indicates a stronger capability for $\\operatorname { S e q } 2 \\operatorname { S t r }$ prediction. The experimental results are available in Table 3.\n\nTable 4: Performance of OmniGenome and baseline FMs on PGB. “PolyA” stands for Polyadenylation, “Chrom Acc” for Chromatin Accessibility, “Prom Str” for Promoter Strength, “Term $\\mathrm { S t r ^ { \\prime \\prime } }$ for Terminator Strength, “Splice” for Splice Site, “Gene Exp” for Gene Expression, and “Enh Reg” for Enhancer Region. Results for OmniGenome $^ { 1 8 6 \\mathrm { M } } +$ are excluded due to the time-intensive nature of the experiments.   \n\n<html><body><table><tr><td rowspan=\"2\">Model</td><td colspan=\"8\">PolyA LncRNA Chrom Acc Prom Str Term Str Splice Gene Exp Enhancer</td></tr><tr><td>F1</td><td>F1</td><td>F1</td><td>RMSE</td><td>RMSE</td><td>F1</td><td>RMSE</td><td>F1</td></tr><tr><td>DNABERT2</td><td>41.35</td><td>72.55</td><td>61.49</td><td>0.99</td><td>0.24</td><td>45.34</td><td>14.78</td><td>36.40</td></tr><tr><td>HyenaDNA</td><td>83.11</td><td>58.21</td><td>52.20</td><td>0.88</td><td>0.26</td><td>90.28</td><td>14.79</td><td>66.17</td></tr><tr><td>Caduceus</td><td>70.89</td><td>68.40</td><td>64.53</td><td>0.91</td><td>0.26</td><td>78.51</td><td>14.72</td><td>60.83</td></tr><tr><td>NT-V2</td><td>71.26</td><td>73.08</td><td>65.71</td><td>0.81</td><td>0.27</td><td>95.05</td><td>14.79</td><td>73.89</td></tr><tr><td>Agro-NT</td><td>78.89</td><td>67.24</td><td>63.27</td><td>0.94</td><td>0.78</td><td>88.45</td><td>15.56</td><td>62.83</td></tr><tr><td>SpliceBERT</td><td>65.23</td><td>71.88</td><td>63.62</td><td>0.75</td><td>0.22</td><td>96.45</td><td>14.70</td><td>69.71</td></tr><tr><td>3UTRBERT</td><td>76.48</td><td>70.75</td><td>63.71</td><td>1.04</td><td>0.36</td><td>94.44</td><td>14.87</td><td>71.67</td></tr><tr><td>RNA-BERT</td><td>78.54</td><td>61.99</td><td>48.94</td><td>1.81</td><td>0.38</td><td>94.45</td><td>14.89</td><td>57.61</td></tr><tr><td>RNA-MSM</td><td>84.25</td><td>67.49</td><td>53.52</td><td>1.28</td><td>0.28</td><td>95.49</td><td>14.87</td><td>61.45</td></tr><tr><td>RNA-FM</td><td>84.94</td><td>68.75</td><td>54.92</td><td>0.95</td><td>0.27</td><td>95.95</td><td>14.83</td><td>57.14</td></tr><tr><td>OmniGenome52M</td><td>85.47</td><td>75.71</td><td>64.23</td><td>0.67</td><td>0.21</td><td>97.40</td><td>14.76</td><td>68.31</td></tr><tr><td>OmniGenome186M</td><td>86.87</td><td>77.53</td><td>66.88</td><td>0.65</td><td>0.19</td><td>98.15</td><td>14.76</td><td>72.45</td></tr><tr><td>OmniGenome52M+</td><td>87.05</td><td>76.23</td><td>65.41</td><td>0.65</td><td>0.20</td><td>97.70</td><td>14.76</td><td>70.71</td></tr><tr><td>OmniGenome186m+ 87.55</td><td></td><td>77.96</td><td>67.69</td><td>0.59</td><td>0.18</td><td>98.41</td><td>14.71</td><td>79.77</td></tr></table></body></html>\n\nThe results in Table 3 indicate that OmniGenome FMs mirrored the zero-shot secondary structure prediction (i.e., Seq2Str) performance of ViennaRNA. Moreover, OmniGenome $^ { 5 2 \\mathrm { M } } +$ and OmniGenome $^ { 1 8 6 \\mathrm { M } } +$ outperform OmniGenome FMs based on structure contexts from ViennaRNA. Given the ablation of structure contexts, OmniGenome186M also achieves performance comparable with ViennaRNA on the Archive2, bpRNA and RNAStralign datasets. Besides, we found that OmniGenome $+$ generally obtains better performance on a wide genome downstream tasks owing to the structure awareness, and random or noise structure contexts have no obvious effects on the structure prediction. We cannot compare with other FMs in the zeroshot SSP experiments, because existing FMs were not pertained for secondary structure prediction.\n\n# 3.5 Results on RGB\n\nThe results in Table 5 demonstrate the performance of OmniGenome and its generalizability across various fine-grained RNA downstream tasks. It is observed that OmniGenome models achieve better results than both RNA and DNA FM baselines, including Agro-NT and DNABERT2, which contain hundreds of millions of parameters. This is because the existing FMs usually adopt $\\mathbf { k }$ -mers or BPE tokenization that cannot handle SN resolution tasks, e.g., single nucleotide mutation detection and repair, and structure prediction. Because of the Seq2Str pretraining, OmniGenome and OmniGenome $+$ models exhibit strong results in secondary structure prediction, underscoring OmniGenome’s capabilities in SN-level RNA sequence understanding and manipulation.\n\n# 3.6 Results on PGB\n\nThe PGB is a plant-oriented genomic benchmark. Although the benchmark datasets in PGB are DNA-based tasks, we can still evaluate the performance of OmniGenome and its generalizability on multi-modal (i.e., DNA and RNA) genomic tasks. The results in Table 4 reveal substantial variability in the performance of different FMs, where OmniGenome52M outperformed other baseline models across most tasks, particularly in tasks like Polyadenylation, Splice Site, and Enhancer Region classification, where they achieved the highest F1 scores. This suggests that OmniGenome’s architecture is particularly adept at handling complex genomic sequences. In comparison, existing FMs, e.g., NT-V2 and Agro-NT, showed lower performance with more parameters than OmniGenome. Besides, the performance of OmniGenome $^ { 5 2 \\mathrm { M } } +$ suggests that the structure context can further enhance the performance of genomic modelling. Overall, OmniGenome models achieve stateof-the-art performance on both benchmarks, especially for OmniGenome $+$ variants. The results underscore the importance of sequence-structure alignment in achieving complex genomic modelling tasks.\n\n# 4 Related Works\n\nBiological sequence modelling, including DNA, RNA, and protein, has attracted attention in recent years. Protein modelling, e.g., AlphaFold (Jumper et al. 2021; Evans et al.\n\nTable 5: The performance of OmniGenome and baseline models on the RGB, with results averaged based on five random seeds. “N.A.” means not available for predictive tasks.   \n\n<html><body><table><tr><td rowspan=\"2\">Model</td><td>mRNA</td><td>SNMD</td><td>SNMR</td><td>Archive2</td><td>Stralign</td><td>bpRNA</td></tr><tr><td>RMSE</td><td>AUC</td><td>F1</td><td>F1</td><td>F1</td><td>F1</td></tr><tr><td rowspan=\"2\">ViennaRNA MXFold2 Ufold</td><td>N.A.</td><td>N.A.</td><td>N.A.</td><td>73.99</td><td>74.09</td><td>65.03</td></tr><tr><td>N.A. N.A.</td><td>N.A. N.A.</td><td>N.A. N.A.</td><td>90.09 89.78</td><td>97.01 95.76</td><td>64.99 78.38</td></tr><tr><td>DNABERT2</td><td>0.8158</td><td>49.94</td><td>15.86</td><td>55.73</td><td>64.09</td><td></td></tr><tr><td>HyenaDNA</td><td>0.8056</td><td>53.32</td><td>39.80</td><td>71.18</td><td>91.24</td><td>33.77 57.43</td></tr><tr><td>Caduceus</td><td>0.8026</td><td>57.01</td><td>39.59</td><td>74.37</td><td>92.28</td><td></td></tr><tr><td>NT-V2</td><td>0.7826</td><td>50.49</td><td>26.01</td><td>68.36</td><td></td><td>59.76</td></tr><tr><td>Agro-NT</td><td>0.7830</td><td>49.99</td><td>26.38</td><td>62.81</td><td>83.18</td><td>56.95</td></tr><tr><td>SpliceBERT</td><td>0.7340</td><td>58.11</td><td>46.44</td><td>79.89</td><td>72.54 93.81</td><td>46.87 71.59</td></tr><tr><td>3UTRBERT</td><td>0.7772</td><td>50.02</td><td>24.01</td><td>68.62</td><td>88.55</td><td>57.90</td></tr><tr><td>RNABERT</td><td>0.8087</td><td>51.32</td><td>29.14</td><td>24.66</td><td></td><td></td></tr><tr><td>RNA-MSM</td><td>0.7321</td><td>57.86</td><td>45.22</td><td>68.72</td><td>83.68 91.15</td><td>47.96</td></tr><tr><td>RNA-FM</td><td>0.7297</td><td>59.02</td><td>42.21</td><td>82.55</td><td>95.07</td><td>64.44</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>78.16</td></tr><tr><td>OmniGenome52M</td><td>0.7191</td><td>62.44</td><td>49.91</td><td>88.48</td><td>97.46</td><td>80.51</td></tr><tr><td>OmniGenome186M</td><td>0.7164</td><td>63.81</td><td>50.80</td><td>90.32</td><td>97.82</td><td>83.09</td></tr><tr><td>OmniGenome52M+</td><td>0.7174</td><td>63.11</td><td>51.21</td><td>88.58</td><td>97.33</td><td>81.29</td></tr><tr><td>OmniGenome186M+</td><td>0.7121</td><td>64.13</td><td>52.44</td><td>91.89</td><td>98.21</td><td>83.18</td></tr></table></body></html>\n\n2021; Abramson et al. 2024) and ESM (Lin et al. 2022), has been studied for many years compared to DNA and RNA modelling. However, the RNA foundation model development has been struggling because the data scale and quality of RNA sequences are limited. Nevertheless, the RNA secondary structures are expensive to verify via in vivo experiments, leading to a grad challenge in the past to model the alignment between RNA sequences and structures.\n\nCurrent RNA FMs focused on sequence-to-structure mapping, e.g., end-to-end secondary structure prediction. However, to the best of our knowledge, the sequencestructure alignment in RNA genome modelling has yet been investigated in the literature. There have been some preliminary works, such as scBERT (Yang et al. 2022), RNABERT (Akiyama and Sakakibara 2022), RNAFM (Chen et al. 2022), RNA-MSM (Zhang et al. 2023), and RNAErnie (Wang et al. 2024), to name a few. However, these methods have only trained the FMs on a limited-scale database, as RNA sequences are generally expensive to obtain. Some FMs focus on specific types of RNA sequences, such as coding sequences (CDS) (Hallee, Rafailidis, and Gleghorn 2023), $5 ^ { \\circ }$ untranslated regions (5’UTR) (Chu et al. 2024), $3 ^ { \\circ }$ untranslated regions (3’UTR) (Yang et al. 2023), or precursor mRNA sequences (Chen et al. 2023), thus limiting the models’ ability to capture the diversity of RNA sequences. Uni-RNA (Wang et al. 2023) has been reported to achieve good performance, however, it is not open-sourced and cannot be compared in the experiments.\n\nIn short, the existing RNA FMs neglect the significance of sequence-structure alignment in RNA genome modelling, while the 5UTR-LM (Chu et al. 2024) adopts the secondary structure prediction as a pre-training objective to achieve $\\operatorname { S e q } 2 \\operatorname { S t r }$ prediction in pre-training. However, these FMs are not available for Str2Seq mapping and suffer from limited model and data scales that fail to uncover the comprehensive efficacy of sequence-structure alignment on a wide set of genomic tasks. ERNIE-RNA (Yin et al. 2024) feeds the RNA structure along with the sequence into the model and improves the downstream tasks. However, it also ignores the significance of Str2Seq prediction capability. In a nutshell, existing FMs fail to achieve sequence-structure alignment without exception.\n\n# 5 Conclusion\n\nWe introduced OmniGenome to tackle the challenge of sequence-structure alignment in genome modelling, which bridges the gap between sequence and structural information and improves the reliability of genome analysis. Experimental results on four comprehensive in-silico RNA and DNA benchmarks demonstrate that OmniGenome outperforms existing FMs across diversified downstream tasks, e.g., up to $9 8 \\%$ F1 score for SSP and $7 4 \\%$ accuracy of RNA design. The superior performance highlights the potential of sequence-structure alignment in the field of genomics.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了RNA基础模型（FMs）中序列与结构对齐不足的问题。现有FMs难以建立序列与结构之间的双向信息流，阻碍了基因组信息在RNA序列与结构之间的自由流动。\\n> *   这一问题在RNA基因组分析和设计中尤为重要，例如mRNA疫苗设计和RNA降解率预测等应用场景。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了OmniGenome，一种基于结构上下文建模的RNA基础模型，通过支持多种输入输出模态（序列和/或结构作为输入/输出），实现了序列与结构的自由双向映射。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **创新贡献点1：** 提出序列-结构对齐（sequence-structure alignment）方法，支持双向映射（Seq2Str和Str2Seq）。\\n>     *   关键数据：在EternaV2基准测试中，OmniGenome解决了74%的RNA设计难题，而现有FMs仅解决3%。\\n>     *   关键数据：在零-shot二级结构预测（SSP）任务中，OmniGenome186M+的宏F1分数达到74.85%，优于ViennaRNA（73.99%）和OmniGenome52M（69.93%）。\\n> *   **创新贡献点2：** 设计了一种灵活的RNA建模范式，支持多种输入输出模态（序列和/或结构作为输入/输出）。\\n>     *   关键数据：在植物基因组基准测试（PGB）中，OmniGenome186M+的性能比现有FMs提升高达35%。\\n> *   **创新贡献点3：** 实现了架构无关和基因组无关的序列-结构对齐，可轻松扩展到不同基因组类型（如DNA）。\\n>     *   关键数据：在RNA基因组基准（RGB）上，OmniGenome186M+在mRNA降解率预测（RMSE为0.7121）和单核苷酸突变检测（AUC为64.13%）上均优于所有基线模型。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   OmniGenome的核心思想是通过结构上下文建模，建立RNA序列与结构之间的双向映射关系。其设计哲学是充分利用大规模序列和结构注释，构建可靠的Str2Seq和Seq2Str映射。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 现有RNA FMs（如RNA-FM和RNA-MSM）仅解决了3%的RNA设计谜题，因为它们无法基于结构解码序列。\\n> *   **本文的改进：** OmniGenome通过结构上下文建模任务（如掩码核苷酸重建）和端到端结构预训练，实现了序列与结构的双向对齐。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> *   1. **RNA标记化：** 采用单核苷酸标记化（SNT）方法，将核苷酸与结构标签（如点、括号）结合，实现细粒度对齐。\\n> *   2. **预训练目标：** 设计了三个目标函数：Str2Seq（结构上下文序列重建）、Seq2Str（序列到结构预测）和MRLM（掩码RNA语言建模）。\\n> *   3. **模型架构：** 基于Transformer编码器，设计了52M和186M参数的两种变体，支持长RNA序列建模。\\n> *   4. **预训练数据：** 使用OneKP数据库中的1,124种植物RNA序列，通过ViennaRNA计算二级结构。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   RNA-FM、RNA-MSM、DNABERT2、HyenaDNA、ViennaRNA等。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在RNA设计（EternaV2基准）上：** OmniGenome186M+解决了74%的谜题，显著优于RNA-FM（3%）和RNA-MSM（2%）。与表现最佳的基线（RNAInverse，30%）相比，提升了44个百分点。\\n> *   **在零-shot二级结构预测（F1分数）上：** OmniGenome186M+在Archive2数据集上达到了74.72%，优于ViennaRNA（73.99%）和OmniGenome52M（69.93%）。\\n> *   **在RNA基因组基准（RGB）上：** OmniGenome186M+在mRNA降解率预测（RMSE为0.7121）和单核苷酸突变检测（AUC为64.13%）上均优于所有基线模型。\\n> *   **在植物基因组基准（PGB）上：** OmniGenome186M+在Polyadenylation任务中的F1分数为87.55%，比DNABERT2（41.35%）和HyenaDNA（83.11%）表现更优。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   RNA基础模型 (RNA Foundation Model, RFM)\\n*   序列-结构对齐 (Sequence-Structure Alignment, N/A)\\n*   二级结构预测 (Secondary Structure Prediction, SSP)\\n*   单核苷酸标记化 (Single Nucleotide Tokenization, SNT)\\n*   RNA设计 (RNA Design, N/A)\\n*   Transformer编码器 (Transformer Encoder, N/A)\\n*   基因组建模 (Genome Modelling, N/A)\\n*   植物基因组 (Plant Genomics, N/A)\"\n}\n```"
}