{
    "source": "Semantic Scholar",
    "arxiv_id": "2502.00026",
    "link": "https://arxiv.org/abs/2502.00026",
    "pdf_link": "https://arxiv.org/pdf/2502.00026.pdf",
    "title": "Pushing the Limits of BFP on Narrow Precision LLM Inference",
    "authors": [
        "Hui Wang",
        "Yuan Cheng",
        "Xiaomeng Han",
        "Zhengpeng Zhao",
        "Dawei Yang",
        "Zhe Jiang"
    ],
    "categories": [
        "cs.AR",
        "cs.AI"
    ],
    "publication_date": "2025-01-21",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "National Center of Technology Innovation for EDA, School of Integrated Circuits, Southeast University",
        "Houmo AI",
        "Nanjing University",
        "Huazhong University of Science and Technology"
    ],
    "paper_content": "# Pushing the Limits of BFP on Narrow Precision LLM Inference\n\nui Wang1∗, Yuan Cheng $^ { 2 , 3 * \\dagger }$ , Xiaomeng $\\mathbf { H a n } ^ { 1 }$ , Zhengpeng Zhao4, Dawei $\\mathbf { Y a n g ^ { 2 \\boxtimes } }$ , Zhe Jiang1B,\n\n1National Center of Technology Innovation for EDA, School of Integrated Circuits, Southeast University 2Houmo AI 3Nanjing University 4Huazhong University of Science and Technology whmio0115@seu.edu.cn, yuancheng $@$ smail.nju.edu.cn, mingzhihan7 $@$ gmail.com, u202114911@hust.edu.cn, dawei.yang $@$ houmo.ai, zhejiang.uk $@$ gmail.com\n\n# Abstract\n\nThe substantial computational and memory demands of Large Language Models (LLMs) hinder their deployment. Block Floating Point (BFP) has proven effective in accelerating linear operations, a cornerstone of LLM workloads. However, as sequence lengths grow, nonlinear operations, such as Attention, increasingly become performance bottlenecks due to their quadratic computational complexity. These nonlinear operations are predominantly executed using inefficient floating-point formats, which renders the system challenging to optimize software efficiency and hardware overhead. In this paper, we delve into the limitations and potential of applying BFP to nonlinear operations. Given our findings, we introduce a hardware-software co-design framework (DB-Attn), including: (i) DBFP, an advanced BFP version, overcomes nonlinear operation challenges with a pivot-focus strategy for diverse data and an adaptive grouping strategy for flexible exponent sharing. (ii) DH-LUT, a novel lookup table algorithm dedicated to accelerating nonlinear operations with DBFP format. (iii) An RTL-level DBFP-based engine is implemented to support DB-Attn, applicable to FPGA and ASIC. Results show that DB-Attn provides significant performance improvements with negligible accuracy loss, achieving $74 \\%$ GPU speedup on Softmax of LLaMA and 10x lowoverhead performance improvement over SOTA designs.\n\n# Introduction\n\nThe noteworthy success of Large Language Models (LLMs) has revolutionized various fields of artificial intelligence. The emerging LLMs like LLaMA families (Meta 2024) and Mistral families (Jiang et al. 2023) continue to push the boundaries of what these models can achieve, promising even greater capabilities in natural language understanding, generation, and problem-solving across diverse domains. While LLMs have exhibited remarkable performance across a range of tasks, their inference process demands substantial computing power and memory bandwidth, severely hindering their application and implementation.\n\nVarious approaches have been explored for efficient large model deployment. While quantization (Lin et al. 2024;\n\n![](images/c902fa8eb173c10d903e85696e3d3370a25aef35248ce62364be0ce4a026eb5e.jpg)  \nFigure 1: Memory overhead and latency of prefill stages for LLaMA3-8B scale superlinearly with sequence length.\n\nMa et al. 2024) and pruning (Frantar and Alistarh 2023; Xia et al. 2023) reduce model size and complexity, they often suffer from accuracy degradation and complex posttraining processes. Alternative numerical formats like BF16 and TF32 (Burgess et al. 2019; Choquette et al. 2021) improve efficiency but remain costly for large-scale inference. Block Floating-Point (BFP) (Drumond et al. 2018; Darvish Rouhani et al. 2020) offers a promising solution by sharing exponents within data blocks, providing the dynamic range for DNN inference with minimal hardware overhead.\n\nWhile BFP research in deep learning has primarily targeted linear operations, such as convolution and fully connected layers, nonlinear operations like Softmax and GELU still depend on floating-point computations, emerging as performance bottlenecks. For instance, Softmax alone consumes over $30 \\%$ of LLM inference time (Stevens et al. 2021). Analysis of LLaMA3-8B shows that longer sequences lead to super-linear growth in memory and latency (Fig. 1), due to the quadratic complexity of Attention layers.\n\nIn this paper, we pioneer the application of BFP to nonlinear operations and identify three key challenges: $\\bullet$ Outlier Sensitivity: accuracy degradation from exponent alignment with outliers. $\\pmb { \\theta }$ Representation Granularity: uniform alignment of exponent drops the representation granularity, which leads to less accurate results. $\\mathbf { \\^ { \\circ } }$ Hardware Complexity: nonlinear operations involve complex logic (transcendental functions, division, etc.), complicating system optimization (detailed in Tab. 1). To this end, we propose: Dynamic-BFP (DBFP), an advanced variant of BFP,\n\nadeptly addresses the challenges of accuracy and efficiency in nonlinear operations. It incorporates two novel strategies: a pivot-focus strategy capable of conforming to various data distributions, and an adaptive grouping strategy that enables a more precise and flexible exponent sharing mechanism.\n\nDB-Attn, a DBFP-based framework with softwarehardware co-optimization for efficient Attention computation. It accelerates Softmax without floating-point operations and streamlines the dataflow between linear (BFP Matmul) and nonlinear (DBFP Softmax) operations by sharing exponents, eliminating explicit conversions.\n\n• Algorithm. We present DH-LUT, a nonlinear operations dedicated lookup table (LUT) algorithm with DBFP format, achieving $74 \\%$ speedup on the GPU for Softmax while maintaining comparable accuracy to floating-point. • Hardware. We design and implement an RTL-level DBFP-based engine applicable to FPGA and ASIC, delivering 10x throughput over SOTA designs.\n\n# Related Work\n\n# Data Formats for LLMs\n\nAs LLMs grow in size and complexity, the standard 32- bit floating-point (FP32) format becomes less practical. Researchers develop low-bit formats such as BF16 (Jouppi et al. 2020) and TF32 (NVIDIA 2022) to address increasing memory and computational demands. Low-bit fixed-point data types convert operations to integer operations (e.g., INT4) by fixing the number of floating-point bits (Nagel et al. 2019; NVIDIA 2020; Mellempudi et al. 2017) , risking accuracy drop with large dynamic ranges. BFP formats (Drumond et al. 2018) share exponents within data blocks, enabling efficient GEMM operations through dense integer logic with reduced hardware complexity. While (Song, Liu, and Wang 2018) explores various BFP grouping methods, they are limited by spatial constraints and ignore data distribution characteristics. Although BSFP (Lo, Lee, and Liu 2023) improves upon BFP, its complex hardware design presents implementation challenges. The MXFP format (Rouhani et al. 2023) represents another step towards efficient floating-point computations in deep learning.\n\n# Nonlinear Operation Algorithms\n\nNonlinear operations present unique challenges in efficient temporal memory utilization and computation, requiring multiple passes over input values held in memory (Kim et al. 2023). Calculation methods for these operations typically fall into two categories: LUT-based method (Zhang et al. 2023; Du et al. 2019; Zhang et al. 2022) and approximation algorithms (Kim et al. 2021; Xia and Zhang 2023; Wang et al. 2018). While LUTs offer high accuracy, they demand substantial storage. Approximation, on the other hand, generally improves in accuracy with larger computational units.\n\n# Methodology Dynamic Block Floating-Point\n\nHere, we introduce the DBFP, including its mathematical model and the corresponding optimization.\n\nBFP Formulation. Let $\\mathbb { F }$ denotes the set of floating-point numbers, and $\\boldsymbol { x } ~ \\in ~ \\mathbb { F }$ is represented as: $x = ( - 1 ) ^ { \\bar { s } } \\cdot 2 ^ { e }$ $m$ where $s \\in \\{ 0 , 1 \\}$ is the sign bit, $e \\in [ e _ { \\operatorname* { m i n } } , e _ { \\operatorname* { m a x } } ]$ is the exponent, and $m \\in [ 1 , 2 )$ is the mantissa. BFP numbers are formally defined as a group: $Y = ( y _ { 1 } , y _ { 2 } , \\dots y _ { n } )$ , where each $y _ { i }$ shares an exponent $e _ { s }$ , such that $y _ { i } = ( - 1 ) _ { i } ^ { s } { \\cdot } m _ { i } { \\cdot } 2 ^ { e _ { s } }$ . BFP numbers can be partitioned into two fields: a shared field $S$ and a private field $P$ . Each $\\hat { s _ { j } } \\in S$ represents a shared exponent for multiple elements in $P$ . Each $p _ { i } \\in P$ is of the form $p _ { i } = ( - 1 ) _ { i } ^ { s } \\cdot m _ { i }$ .\n\nTo convert floating-point numbers to BFP, a uniform exponential alignment is applied as Eq.1:\n\n$$\n\\begin{array} { c } { { x _ { i } = ( - 1 ) _ { i } ^ { s } \\cdot 2 ^ { e _ { i } } \\cdot m _ { i } = ( - 1 ) _ { i } ^ { s } \\cdot 2 ^ { e _ { i } - d _ { i j } } \\cdot ( m _ { i } \\cdot 2 ^ { d _ { i j } } ) } } \\\\ { { = ( - 1 ) _ { i } ^ { s } \\cdot 2 ^ { \\hat { s _ { j } } } \\cdot m _ { i } ^ { \\prime } } } \\end{array}\n$$\n\n, where $\\boldsymbol { d _ { i j } } = \\boldsymbol { e _ { i } } - \\boldsymbol { \\hat { s _ { j } } }$ denotes the difference between the exponent of $x _ { i }$ and the $j$ th shared exponent. This alignment introduces an error due to the finite precision of mantissa multiplication by $2 ^ { d _ { i j } }$ (shifting $d _ { i j }$ bits). The error depends on the distance $d _ { i j }$ , determined by the input exponent $e _ { i }$ . Therefore, selecting appropriate shared exponents $\\hat { s _ { j } }$ , which are determined by many factors, is crucial to minimize $d _ { i j }$ . Problem Formulation: To this end, we formulate the problem as finding a adaptive grouping function $f$ that partitions a set $X$ of floating-point values into $k$ subsets based on their characteristics: $f \\colon X  \\chi \\{ S _ { 1 } , S _ { 2 } , . . . , S _ { k } \\}$ where each subset $S _ { j }$ is determined within a frame of discernment $\\Omega$ .\n\nEach subset $S _ { j }$ has a unique BFP representation $B _ { j }$ with shared and private fields. We then explore factors influencing BFP format accuracy, which define the function $f$ .\n\nObservations and Insights. Through experimental and theoretical analysis, we explore the limitations of vanilla BFP, providing new insights for improving accuracy.\n\nObservation 1 Pivot-focus policy: Vanilla BFP formats typically align a group of $x _ { i }$ (e.g., every 16 elements) to the maximum exponent within that group, leading to significant accuracy drop. Our experimental findings in Softmax show that setting the alignment direction to the default maximum causes up to $9 . 6 \\mathrm { x }$ greater loss than the median one.\n\nInsight 1: This suggests that using maximum values for alignment is suboptimal. Instead, a more representative value (e.g., median) as an alignment pivot better preserves accuracy across the group.\n\nObservation 2 Adaptive grouping strategy: Prior work on BFP relied on fixed bounding boxes, which are particularly vulnerable to outliers. Outliers can cause disproportionate shifts in data exponents and lead to substantial accuracy drop. See Tab. 1 for a more detailed analysis.\n\nInsight 2: If elements with similar magnitude distributions can share exponents within a group, it can reduce the bit shifts (diminish the $d _ { i j }$ ) for individual numbers.\n\nFig. 2 illustrates the difference between the mentioned formats: (a) represents floating-point data formats such as FP16 or TF32; (b) shows the vanilla BFP format; (c) highlights our DBFP, which employs automatic grouping based on the intervals in which the exponent falls.\n\nSign(s) Exponent(e) Mantissa(m) group 2 5-bit exponent group n 16\\~19 bits Fixed Block 8-bit mantissa 8\\~10 bit mantissa (a)  FP16, TF32… (b)  Vanilla BFP (c)  DBFP (Ours)\n\nTheoretical Analysis. For the pivot-focus policy, we introduce directional vectors to characterize the process. Let $\\vec { s _ { j } }$ denote the vector representing the direction and magnitude from the origin to the point $\\hat { s _ { j } }$ in the vector space. The shift vector $\\vec { v _ { i j } } = \\bar { ( } | \\bar { s _ { j } } | - \\bar { e _ { i } } ) \\hat { v }$ determines the shift for each $x _ { i }$ relative to the sharing exponent. $\\vec { m _ { i } ^ { \\prime } }$ and $\\vec { m _ { i } }$ denote the mantissa’s projection before and after conversion, with their Euclidean distance as $d _ { i j } ^ { 2 } = { \\left\\| \\vec { m _ { i } } ^ { \\prime } - \\vec { m _ { i } } \\right\\| } ^ { 2 } . ~ \\mu _ { i j }$ represents the confidence that $x _ { i }$ falls with\nin the inter\nval defined by $\\vec { s _ { j } }$ . It’s aimed to minimize the following objective function:\n\n$$\n\\begin{array} { l } { { \\displaystyle \\underset { \\mu _ { i j } , s _ { j } ^ { + } } { \\arg \\operatorname* { m i n } } \\mathcal { I } _ { \\mathrm { D B F P } } \\triangleq \\sum _ { i = 1 } ^ { n } \\sum _ { S _ { j } \\subseteq \\Omega } \\mu _ { i j } ^ { \\beta } D _ { i j } ^ { 2 } } } \\\\ { { \\displaystyle \\qquad = \\sum _ { i = 1 } ^ { n } \\sum _ { \\left\\{ j / S _ { j } \\neq \\emptyset , S _ { j } \\subseteq \\Omega \\right\\} } \\mu _ { i j } ^ { \\beta } d _ { i j } ^ { 2 } + \\sum _ { i = 1 } ^ { n } \\mu _ { i \\emptyset } ^ { \\beta } \\delta ^ { 2 } } } \\end{array}\n$$\n\n$$\n\\sum _ { j / S _ { j } \\subseteq \\Omega S _ { j } \\neq \\emptyset \\} } { \\mu _ { i j } } + \\mu _ { i \\emptyset } = 1 , \\forall i = 1 , n ,\n$$\n\nIn Eq. 2, hyperparameter $\\beta$ (default 2) regulates $\\mu _ { i j }$ ’s importance. An empty set mitigates outlier impact on $\\vec { s _ { j } }$ selection, with $\\mu _ { i \\emptyset }$ as its confidence. The distance between outliers and $\\hat { s _ { \\emptyset } }$ is $\\delta ^ { 2 }$ , related to the distances of all $S _ { j }$ .\n\n$$\nD _ { i j } ^ { 2 } = \\left\\{ { \\delta ^ { 2 } , S _ { j } = \\emptyset } \\right.\n$$\n\nTo minimize $\\mathcal { I } _ { \\mathrm { D B F P } }$ , we alternately fix one of the variables $\\mu _ { i j }$ or $\\vec { s _ { j } }$ and solve the constrained minimization problem for the other. By introducing $n$ Lagrange multipliers $\\lambda _ { i }$ , the Lagrangian is expressed as:\n\n$$\n{ \\mathcal { L } } ( U , S , \\lambda _ { 1 } , \\cdots , \\lambda _ { n } ) = { \\mathcal { I } } _ { \\mathrm { D B F P } } ( U , S ) - \\sum _ { i = 1 } ^ { n } \\sum _ { S _ { j } \\subseteq \\Omega } \\lambda _ { i } \\mu _ { i j } ^ { \\beta }\n$$\n\nBy differentiating the Lagrangian and setting each element of gradient ∇L to zero, specifically ∂µLij , ∂∂µLi , ∂λLi , we obtain $\\mu _ { i j }$ , the necessary conditions for optimality:\n\n$$\n\\mu _ { i j } = \\frac { d _ { i j } ^ { - 2 / ( \\beta - 1 ) } } { \\sum _ { { \\cal S } _ { k } \\not = 0 } d _ { i k } ^ { - 2 / ( \\beta - 1 ) } + \\delta ^ { - 2 / ( \\beta - 1 ) } }\n$$\n\nSimilarly, we can further calculate the $\\vec { s _ { j } }$\n\n![](images/0e44ad20ac57807c4afcaa80e258ec4db310e532a6d7599eb7c217290abbf69e.jpg)  \nFigure 2: Number system comparison between floatingpoint numbers(a), BFP(b) and our DBFP(c).   \nFigure 3: Non-uniform hierarchical LUT with five intervals.\n\n$$\n\\Vec { s _ { j } } = \\frac { \\sum _ { i = 1 } ^ { n } \\sum _ { S _ { j } \\neq \\emptyset } \\mu _ { i j } ^ { \\beta } \\vec { e _ { i } } } { \\sum _ { i = 1 } ^ { n } \\sum _ { S _ { j } \\neq \\emptyset } \\mu _ { i j } ^ { \\beta } }\n$$\n\nThis system can be solved using a standard linear system solver. Ultimately, this process yields an optimal solution for the set $S$ , i.e., the configuration that complies with the adaptive grouping strategy and pivot-focus policy.\n\n# DB-Attn Algorithm Design\n\nIn this section, we introduce the algorithmic core of DBAttn. Unlike the vanilla BFP format, which struggles with nonlinear operations, our proposed DH-LUT, for the first time, completes nonlinear operations in a BFP-like format. We also apply DBFP to linear operations, optimizing Matmul efficiency through cascade operations.\n\nOptimization for Softmax. The Softmax operation is among the most computationally intensive nonlinear components in Transformers, significantly impacting computational efficiency. Our optimization methodologies using DBFP offer a general approach to efficiently computing nonlinear functions, readily extensible to other nonlinear operations. The Softmax function converts Attention scores into a probability distribution. It can be represented as follows:\n\n$$\n\\operatorname { S o f t m a x } \\left( x _ { i } \\right) = \\frac { e ^ { x _ { i } } } { \\sum _ { j = 0 } ^ { i } e ^ { x _ { j } } } = \\frac { e ^ { x _ { i } - x _ { \\operatorname* { m a x } } } } { \\sum _ { j = 0 } ^ { i } e ^ { x _ { j } - x _ { \\operatorname* { m a x } } } }\n$$\n\n, which is transformed by the down-scaling exponentiation to smooth data distribution and prevent overflow.\n\nSoftmax’s bottleneck stems from the low throughput of exponential functions. Leveraging DBFP’s shared exponents, we propose Dynamic Hierarchical LUT (DH-LUT), a lightweight solution. DH-LUT uses a two-dimensional structure of sub-LUTs, dynamically loaded based on DBFP shared exponents and high $k$ bits of mantissa. For Softmax’s $e ^ { x - x _ { \\mathrm { m a x } } }$ operation, only the $[ - \\infty , 0 ]$ range needs fitting, showing non-uniform distribution. DH-LUT adapts to this through pivot-focus policy and non-uniform fitting. We optimize accuracy and memory with a non-uniform hierarchical allocation method (Algo.1), adaptively partitioning to focus resources on nonlinear regions shown in Fig. 3.\n\nFor the Softmax based on DH-LUT, its computational error stems from the DBFP format and the mapping error of DH-LUT. Eq. 8 introduces the error analysis of vanilla BFP, which is applicable to DBFP. For a BFP block, using the rounding-to-nearest scheme, its quantization error is zeromean with variance $\\sigma ^ { 2 }$ , defined as:\n\n$$\n\\sigma ^ { 2 } = \\frac { 2 ^ { - 2 L _ { m } } } { 1 2 } \\sum _ { i = 1 } ^ { N _ { \\gamma } } p _ { \\gamma _ { i } } 2 ^ { 2 \\gamma _ { i } }\n$$\n\n, where $L _ { m }$ denote the bit length of the block mantissa, and $p _ { \\gamma _ { i } }$ represent the probability mass function (PMF) of the block exponent. $N _ { \\gamma } = 2 ^ { L _ { E } }$ is the number of available block exponential levels. Increasing the bit length of the block mantissa and reducing the shared probability of larger exponents can effectively reduce input value errors. While aligning to the maximum exponent (vanilla BFP) causes significant errors as inputs with tiny exponents are crucial for Softmax shown in Fig.3, and minimum exponent alignment preserves accuracy at the cost of redundant mantissa bit, as the table width of DH-LUT is fixed at the Pareto frontiers, detailed in Fig.5, our approach aligns with the median exponent, balancing accuracy and efficiency.\n\nThe data stored in DH-LUT is all DBFP, and its characteristic of shared exponents allows calculations not only in the floating-point domain but also on computing resourceslimited devices, supporting integer-only computation:\n\n$$\n\\begin{array} { r } { \\mathrm { S o f t m a x } ( x _ { i } ) = \\frac { \\mathrm { e } _ { \\mathrm { { D H } \\mathrm { - } L \\mathrm { { U T } } } } ^ { \\mathrm { { x _ { i } - x _ { m a x } } } } } { \\sum _ { \\mathrm { { j } } } ^ { \\mathrm { { d } } } \\mathrm { e } _ { \\mathrm { { D H } \\mathrm { - } L \\mathrm { { U T } } } } ^ { \\mathrm { { x _ { j } - x _ { m a x } } } } } = \\frac { 2 ^ { \\mathrm { s } } \\cdot \\mathrm { e } _ { \\mathrm { { i } } } ^ { \\mathrm { { i n t } } } } { \\sum _ { \\mathrm { { j } } } ^ { \\mathrm { { d } } } 2 ^ { \\mathrm { s } } \\cdot \\mathrm { e } _ { \\mathrm { { j } } } ^ { \\mathrm { { i n t } } } } = \\frac { \\mathrm { e } _ { \\mathrm { { i } } } ^ { \\mathrm { { i n t } } } } { \\sum _ { \\mathrm { { j } } } ^ { \\mathrm { { d } } } \\mathrm { e } _ { \\mathrm { { j } } } ^ { \\mathrm { { i n t } } } } } \\end{array}\n$$\n\nDB-Attn leverages the DH-LUT with minimal storage overhead to compute the exponential function, enhancing the throughput of nonlinear operations. Its shared exponent feature facilitates the migration of the Softmax algorithm to devices with limited computing resources, achieving significant speedups on both floating-point platforms and edge devices with integer-only capabilities.\n\nOptimization of Matrices. From Eq. 1, a vector in DBFP format can be viewed as the product of a shared coefficient and an integer vector, expressed as: $\\overrightarrow { A _ { D } } = 2 ^ { e _ { A } } \\cdot \\overrightarrow { A _ { I } }$ where $\\stackrel { \\longrightarrow } { A _ { I } }$ represents an integer vector. $e _ { A }$ is the exponent shared by this vector. Applying DBFP to matrices in Attention, the dot product operation of a single vector $\\overrightarrow { Q _ { \\mathrm { D } } }$ and $\\overrightarrow { K _ { \\mathrm { D } } ^ { T } }$ within the Matmul of Query and Key can be described as :\n\n$$\n\\begin{array} { c } { { \\overrightarrow { { Q _ { \\mathrm { D } } } } \\cdot \\overrightarrow { { K _ { \\mathrm { D } } ^ { T } } } = ( 2 ^ { e _ { Q } } \\cdot \\overrightarrow { { Q _ { I } } } ) \\cdot ( 2 ^ { e _ { K } } \\cdot \\overrightarrow { { K _ { I } ^ { T } } } ) } } \\\\ { { = 2 ^ { e _ { Q } + e _ { K } } ( \\overrightarrow { { Q _ { I } } } \\cdot \\overrightarrow { { K _ { I } ^ { T } } } ) } } \\end{array}\n$$\n\nIt can be discerned that vector dot products and Matmul in the DBFP format can be achieved using integer Matmul units and integer adders, bypassing complex floating-point operations and enhancing computational efficiency. Extending this operation to the entire matrix reveals the trait of cascaded DBFP Matmul. The result of multiplying two DBFP matrices remains a DBFP matrix, capable of being seamlessly chained for subsequent Matmul without additional handling. For DBFP matrices with finer-grained blocks, realignment can be introduced during cascading operations, efficiently handled via hardware shift operations, enabling even more efficient computation.\n\nAlgorithm 1: DH-LUT hierarchical algorithm   \n\n<html><body><table><tr><td>Input:TargetFunctionF,lut table size m,all possiblevaluesun dertheFP16 formatV. Output: Optimal Partition Pointsof the LUT OPP.</td></tr><tr><td>1:functionUPDATE_NEXT(OPP,i,m,V)</td></tr><tr><td>2: interval = (len(x)-OPP[i])//(m-1-i)</td></tr><tr><td>3: forj=1to m-1-i do</td></tr><tr><td>4: OPP[i+jl=OPP[i]+ j * interval</td></tr><tr><td>5: end for</td></tr><tr><td>6:end function 7:</td></tr><tr><td>8:function SELECT_BEST_OPP(F,V,m)</td></tr><tr><td>9: interval = len(x)//(m-1)</td></tr><tr><td>10: InitializeOPPwithinterval</td></tr><tr><td>11: fori=1 to m do</td></tr><tr><td>12: Dmax =0</td></tr><tr><td>13: pre= OPP[i-1]</td></tr><tr><td>14: next= OPP[i+1]</td></tr><tr><td>15: outF = F(V)</td></tr><tr><td>16: for j= pre +1 to next do</td></tr><tr><td>17: calculate Interpolation outright</td></tr><tr><td>18: calculate MAE Dright</td></tr><tr><td>19: calculate Interpolation out left</td></tr><tr><td>20: calculate MAE Dleft</td></tr><tr><td>21: D=Dleft +Dright</td></tr><tr><td>22: Update OPP[i], Dmax if D ≤Dmax</td></tr><tr><td>23: UPDATE_NEXT(OPP,i, m, V)</td></tr><tr><td>24: end for</td></tr><tr><td>25: end for</td></tr><tr><td>26: return OPP</td></tr><tr><td>27:end function</td></tr></table></body></html>\n\n# Algorithm-driven Hardware Architecture\n\nWe design and implement an RTL-level engine for Softmax with DBFP and evaluate the hardware resources and throughput advantage. Benefiting from the DBFP, the proposed accelerator offers competitive performance with very light design complexity, making it easily adaptable to other general-purpose GPUs and NPUs. Below, We’ll first detail the proposed architecture and implementation.\n\nOverall Architecture. With the alignment to the algorithms presented above, we split the architecture of the accelerator into four pipeline stages: Max finds the maximum value within the input vector; SE performs Shared Exponent calculation and maximum value subtraction; Exp computes exponents and sum using an adder tree; Div executes division operations to obtain the result.\n\nThe SE stage segments and aligns the exponent of the input vector with the MAX stage’s maximum value, followed by subtraction (Fig. $4 \\textcircled { \\bullet }$ . Simultaneously, it checks if the DH-LUT’s current data exponent matches the required tobe-shared exponent, signalling DMA (Direct Memory Access, a module allowing hardware subsystems to access memory for efficient data transfer between memory and devices) to preload new data required for the following computations (Fig. $4 \\textcircled { \\bullet }$ . These initial stages prepare for computation. The subsequent three phases complete the exponential function (Fig. 4 $\\bullet$ ), denominator summation (Fig. 4 $\\pmb { \\bigcirc }$ ), and division (Fig. $\\mathbf { \\bar { 4 } } \\mathbf { \\Theta } _ { \\mathbf { \\Theta } } ( \\pmb { \\circ } ) )$ – the main operations in Softmax.\n\nInput Vector a e Numerator Path Denominator Path Pipeline Stage FP16/32 DBFP DBFP c Hit Bitmap FP Vector DBFP Vector Module Logic CRoumnpd. Vector Buffer VDeBctFoPr DMBAFXP DSBUFBP GO 8000.00 IdxExpvalue ADBDFePr Vector Buffer DBIFVP FP16 EX Comp. MA Shift MA Add Unit Shared Expb DMA Preload DH-LUT ： Tree AdderTree AdDdeBrFTPr ee 1st. 2nd. 3rd. 4th.   \n1st. 2nd. 3rd. 4th. 42% Reduction (a) DBFP Algorithm-driven Softmax Hardware Architecture Computation Time Sign a, b Exp a, b Mantissa a Mantissa b Sign a,bExp aSEhaxrpedMantissa a Mantissa b (d) The Enhancement of DBFP Replaces FP16 on AdderTree Extensed to double lMenangtihssa Mantissa MA Comp. MA Shift MA ADD Norm. SEuxbtproancteinotn CoMmanptairsastaor Shifter SEuxbtproancteinotn Comp. LUTMantissa DIFVP1U6nit   \n1st. 2nd. 3rd. 4th. 11th. 12th. Quotient Buffer 目三 DBFP   \n00 四 Normalization Normalization 中 DIV Unit 1st. 2nd. 83% Reduction Loop for 11 cycles Just one cycle Computation Time (b) Conversional Division Unit Architecture (c) Our Division Unit Architecture (e) The Enhancement of DBFP Replaces FP16 on Division\n\nLow-bit Storage Implementation of DBFP. Our dynamic hierarchical non-uniform LUT strategy introduced in the previous section enables a compact yet flexible hardware implementation. By extracting $n$ bits from vector elements, we create a $2 ^ { n }$ -entry table that balances accuracy and size. This compact design is suitable for Softmax computation, which requires global input information. Our approach utilizes two tables: a value table storing exp values for approximation and a hit bitmap table recording mantissa occurrences (Fig. $4 \\textcircled { \\bullet }$ . Input vectors perform lookups in parallel, with each exponent index hitting a DH-LUT interval, setting a corresponding bitmap bit. After recording the input vector, we multiply and sum values from both tables using an adder tree structure. This achieves parallel lookup results without extra hardware resources, leveraging the compact table size.\n\nEfficient DBFP Computing. DBFP’s ability to convert floating-point operations into integer operations is a key advantage. By sharing exponents, most floating-point computations simplify basic exponent operations combined with integer mantissa operations, streamlining arithmetic calculations. Vector addition in neural networks exemplifies this efficiency. Traditional floating-point addition requires aligning exponents for each number pair. DB-Attn pre-aligns exponents within groups, reducing the operation to exponential multiplication with mantissa addition. For cascading structures like adder trees (Fig. 4 $\\bullet$ ), a single initial exponent alignment allows direct mantissa calculations, yielding substantial benefits. Fig. 4(d) demonstrates an experimentally proven $42 \\%$ latency reduction in a 4-level adder tree.\n\nDivision operations in Softmax and Layernorm often limit parallelism. DBFP’s shared exponents enable efficient parallel integer division approximation $\\mathbf { ( F i g . 4 \\bullet ) }$ , reducing latency of these complex operations. While traditional FP16 dividers require 11 cycles for 10-bit mantissa calculation, consuming $90 \\%$ of area and power, our DBFP Divider uses LUTs and shift-addition (Jha et al. 2020) to complete division in a single cycle. In Softmax, with fixed divisor and identical DBFP exponents, we need only one exponent subtraction and lookup per 64 divisions, achieving $8 3 \\%$ latency reduction (Fig.4(e)). This approach enables FP16 operations using 10-bit integer operations, particularly benefiting bitwidth sensitive operations.\n\n# Evaluation\n\nBaselines. As described, DB-Attn involves both software and hardware implementation. Hence, we examined DBAttn against the SOTA work in both worlds. For the software, we compare DB-Attn with FP32, FP16, vanilla BFP, and FP8, focusing on the tradeoffs between accuracy and precision. To measure the improvement DBFP brings to the hardware, we compare its hardware metrics with SOTA Softmax acceleration architectures: Hyft (Xia and Zhang 2023), TCAS-I’22 (Zhang et al. 2023), ISCAS’23 (Koca, Do, and Chang 2023) and TCAS-II’22 (S 2021).\n\nModels and Datasets. We evaluate DB-Attn on both LLM and Vision models. For LLMs, we test on LLaMA-7B, LLaMA2-7B, and LLaMA3-8B (Touvron et al. 2023a,b; Meta 2024) , using WikiText2 and C4 datasets for perplexity comparison. Zero-shot accuracy is evaluated on PIQA, ARC, BoolQ, HellaSwag, and Winogrande tasks. For Vision tasks, we test image classification using ViT and Swin (Dosovitskiy et al. 2020; Liu et al. 2021) on ImageNet, and object detection using Detr on COCO dataset.\n\nImplementations. We implement DB-Attn on NVIDIA\n\nTable 1: Accuracy performance of DB-Attn in different LLM tasks   \n\n<html><body><table><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Method</td><td rowspan=\"2\">Nonlinear Op</td><td colspan=\"7\">Zero-Shot</td><td colspan=\"2\">Perplexity</td></tr><tr><td>PIQA(↑)</td><td>ARC-e(↑)</td><td>ARC-c(↑)</td><td>BoolQ(↑)</td><td>HellaSwag(↑)</td><td>Winogrande(↑)</td><td>avg.(↑)</td><td>WikiText2(↓)</td><td>C4(↓)</td></tr><tr><td rowspan=\"6\">LLaMA-7b</td><td>FP16</td><td>FP32</td><td>77.37</td><td>52.27</td><td>41.21</td><td>73.27</td><td>72.87</td><td>67.32</td><td>64.05</td><td>5.68</td><td>7.08</td></tr><tr><td>BFP</td><td>BFP</td><td>53.65</td><td>30.05</td><td>25.34</td><td>61.87</td><td>32.06</td><td>49.41</td><td>42.06</td><td>67.31</td><td>67.13</td></tr><tr><td>FP8 e4m3</td><td>FP32</td><td>49.51</td><td>25.08</td><td>22.69</td><td>37.82</td><td>25.04</td><td>49.57</td><td>34.95</td><td>NaN</td><td>NaN</td></tr><tr><td>FP8 e4m3-S</td><td>FP32</td><td>49.46</td><td>25.17</td><td>22.78</td><td>37.82</td><td>25.04</td><td>49.57</td><td>34.97</td><td>NaN</td><td>NaN</td></tr><tr><td>FP8 e5m2</td><td>FP32</td><td>77.31</td><td>51.98</td><td>41.04</td><td>72.14</td><td>72.24</td><td>65.66</td><td>63.40</td><td>5.80</td><td>7.16</td></tr><tr><td>DBFP</td><td>DH-LUT</td><td>77.64</td><td>51.85</td><td>41.47</td><td>73.30</td><td>72.87</td><td>67.01</td><td>64.02</td><td>5.68</td><td>7.08</td></tr><tr><td rowspan=\"6\">LLaMA2-7b</td><td>FP16</td><td>FP32</td><td>76.88</td><td>53.62</td><td>40.61</td><td>71.07</td><td>72.94</td><td>67.09</td><td>63.70</td><td>5.47</td><td>6.97</td></tr><tr><td>BFP</td><td>BFP</td><td>52.72</td><td>28.24</td><td>25.26</td><td>61.90</td><td>33.64</td><td>49.41</td><td>41.86</td><td>32.72</td><td>41.29</td></tr><tr><td>FP8 e4m3</td><td>FP32</td><td>49.51</td><td>25.08</td><td>22.69</td><td>37.82</td><td>25.04</td><td>49.57</td><td>34.95</td><td>NaN</td><td>NaN</td></tr><tr><td>FP8 e4m3-S</td><td>FP32</td><td>49.56</td><td>25.08</td><td>22.69</td><td>37.82</td><td>25.04</td><td>49.57</td><td>34.96</td><td>NaN</td><td>NaN</td></tr><tr><td>FP8 e5m2</td><td>FP32</td><td>76.98</td><td>52.35</td><td>40.69</td><td>70.55</td><td>72.09</td><td>65.43</td><td>63.02</td><td>5.61</td><td>7.09</td></tr><tr><td>DBFP</td><td>DH-LUT</td><td>76.77</td><td>53.20</td><td>41.47</td><td>71.01</td><td>72.60</td><td>67.32</td><td>63.73</td><td>5.48</td><td>6.98</td></tr><tr><td rowspan=\"6\">LLaMA3-8b</td><td></td><td></td><td></td><td></td><td></td><td></td><td>79.17</td><td>72.61</td><td>74.17</td><td>6.14</td><td>8.88</td></tr><tr><td>FP16</td><td>FP32 BFP</td><td>80.79</td><td>77.74</td><td>53.33</td><td>81.35</td><td>35.67</td><td>49.72</td><td>40.45</td><td>69.65</td><td></td></tr><tr><td>BFP</td><td>FP32</td><td>54.30 49.51</td><td>31.06 25.08</td><td>22.78 22.70</td><td>49.17 37.82</td><td>25.04</td><td>49.56</td><td>34.95</td><td>NaN</td><td>79.49 NaN</td></tr><tr><td>FP8 e4m3 FP8 e4m3-S</td><td>FP32</td><td>49.73</td><td>25.17</td><td>22.61</td><td>37.82</td><td>25.04</td><td>49.56</td><td>34.99</td><td>NaN</td><td>NaN</td></tr><tr><td>FP8 e5m2</td><td>FP32</td><td>79.59</td><td>78.32</td><td>52.55</td><td>79.41</td><td>78.05</td><td>71.50</td><td>73.24</td><td>6.42</td><td>9.24</td></tr><tr><td>DBFP</td><td>DH-LUT</td><td>80.36</td><td>78.66</td><td>53.16</td><td>81.01</td><td>78.86</td><td>73.48</td><td>74.26</td><td>6.14</td><td>8.89</td></tr></table></body></html>\n\nTable 2: Accuracy performance of DB-Attn in different Vision tasks   \n\n<html><body><table><tr><td>Model</td><td>Method</td><td>Nonlinear Op</td><td>Map</td><td>Model</td><td>Method</td><td>Nonlinear Op</td><td>Top-1 acc.</td><td>Model</td><td>Method</td><td>Nonlinear Op</td><td>Top-1 acc.</td></tr><tr><td></td><td>FP32</td><td>FP32 BFP</td><td>41.9 26.8</td><td rowspan=\"6\">ViT-base</td><td>FP32</td><td>FP32</td><td>84.536</td><td rowspan=\"6\"></td><td>FP32</td><td>FP32</td><td>81.378</td></tr><tr><td>BFP</td><td></td><td></td><td></td><td>BFP</td><td>BFP</td><td>39.132</td><td>BFP</td><td>BFP</td><td>72.052 81.312</td></tr><tr><td>FP8 e4m3 DETR</td><td></td><td>FP32</td><td>NaN NaN</td><td>FP8 e4m3</td><td>FP32</td><td>84.482</td><td></td><td>FP8 e4m3</td><td>FP32</td></tr><tr><td>FP8 e4m3-S</td><td>FP32</td><td></td><td></td><td>FP8 e4m3-S</td><td>FP32</td><td>84.442</td><td>Swin-tiny FP8 e4m3-S</td><td>FP32</td><td>81.400</td></tr><tr><td>FP8 e5m2</td><td>FP32</td><td>28.4</td><td></td><td>FP8 e5m2</td><td>FP32</td><td>84.246</td><td>FP8 e5m2</td><td>FP32</td><td>81.268 81.384</td></tr><tr><td>DBFP</td><td>DH-LUT</td><td>41.8</td><td></td><td>DBFP</td><td>DH-LUT</td><td>84.522</td><td>DBFP</td><td></td><td>DH-LUT</td></tr></table></body></html>\n\nA6000 GPU using Pytorch and huggingface, replacing only the Attention layer with DB-Attn. We use 128-element blocks along matrix rows, with 8-bit mantissa and 5-bit shared exponent for both DBFP and BFP formats. DBFP is applied to Softmax operations and matrix multiplications in Attention Modules, using 7-bit DH-LUT for Softmax.\n\nWe implement DBFP in RTL using Chisel , verify with Verilator, and deploy on AMD Alveo™ U280 FPGA using Vivado. We compare our design with other FPGA-based Softmax accelerators. For accurate area and power analysis, we synthesize the accelerator using Synopsys Design Compiler at $2 . 0 \\mathrm { G H z }$ on $2 8 \\mathrm { n m }$ TSMC process.\n\n# Accuracy Results of DB-Attn\n\nLLM Tasks. We examine the accuracy of DB-Attn on the language generation task and six zero-shot tasks on LLaMA LLMs, comparing it against vanilla BFP and FP8 format. Tab. 1 presents the perplexity and zero-shot accuracy of LLMs. Wherein FP8 $\\mathsf { e } 4 \\mathrm { m } 3 . \\mathrm { S }$ denotes the use of scaling factor (the maximum value that $\\mathrm { F P 8 ~ e 4 m } 3$ can represent) to rescale the value within the representable range of FP8 $\\mathrm { e } 4 \\mathrm { m } 3$ . It can be seen in Tab. 1 that the direct application of vanilla BFP format to Softmax operations leads to substantial accuracy drop, with LLaMA3’s average accuracy on zero-shot tasks decreasing by $3 3 . 8 1 \\%$ . Similarly, $\\mathrm { F P 8 } \\mathrm { e } 4 \\mathrm { m } 3$ is inadequate for Attention calculations due to its inability to represent infinity. DB-Attn outperforms both vanilla BFP and FP8 formats across all evaluated tasks, maintaining nearly the same accuracy as floating-point. These results unequivocally demonstrate the comprehensive excellence of DB-Attn in maintaining model performance while potentially reducing computational overhead.\n\nVision Tasks. We assess tasks of object detection and image classification (Tab. 2). It is seen that DB-Attn’s performance is similar to results on LLMs. DB-Attn can be losslessly integrated into existing Transformer models, showing its generalization and versatility across various distributions.\n\nPrecision-to-Accuracy Pareto Frontier. We test the computational error, LUT memory usage, and actual model accuracy of Softmax in DB-Attn under different LUT bit widths. To find the Pareto frontier of the LUT bit width configuration, we visualize some results in Fig. 5(a) and (b) – when the LUT bit width is 5-7, a balance is achieved among computational error, memory usage, and accuracy.\n\n# Hardware Implement Evaluation\n\nAs BFP has been validated on linear operations in previous work (Zou et al. 2024; Yeh et al. 2022) we mainly focus on the performance of hardware deployments for the yet to be well optimized nonlinear operation Softmax.\n\nDBFP GPU Run-time Analysis. We implement a custom CUDA Softmax operator to emulate DBFP formats on NVIDIA A800 GPU. This operator replaces the Softmax in various models (e.g., LLaMA and ViT) and performs inference. Results in Fig. 5(c) demonstrate that DBFP-based Softmax consistently achieves speed improvements of at least $30 \\%$ across diverse model architectures. Notably, on the LLaMA series, we reduced latency by $74 \\%$ on average.\n\nDBFP Hardware Implement on FPGA. We evaluate designs against SOTA based on Softmax processing latency,\n\n国 1 国 1.0 Exp Mean Diff SxpMax Mean Diff 66%70% Baseline A 67% Cadata   \n劉 Softmax Max Diff 46% Memory Usage 26%25%24% Accuracy 5%   \n1 Table Width8 10 2 TableWidth 10 T 0.01 ADP EDP Throughput   \n(a) Trends of Storage and Diff (b) Trends of Storage and Accuracy (c) GPU Run-time Redunction (d) ASIC Evaluation\n\nTable 3: SOTA Softmax accelerators comparison on FPGAs   \n\n<html><body><table><tr><td>Methods</td><td>NUM</td><td>Format</td><td>LUT</td><td>FF</td><td>Fmax (MHz)</td><td>Latency (ns)</td><td>FOM</td></tr><tr><td>Xilinx FP</td><td>8</td><td>FP32</td><td>13254</td><td>18664</td><td>435</td><td>232.3</td><td>3.488</td></tr><tr><td>Hyft16</td><td>8</td><td>FP16</td><td>1072</td><td>824</td><td>625</td><td>12.4</td><td>42.194</td></tr><tr><td>Hyft32</td><td>8</td><td>FP32</td><td>2399</td><td>1528</td><td>526</td><td>19</td><td>34.290</td></tr><tr><td>TCAS-I22</td><td>10</td><td>Fixed 16</td><td>1476</td><td>698</td><td>500</td><td>NA</td><td>36.798</td></tr><tr><td>ISCAS'23</td><td>8</td><td>FP16</td><td>909</td><td>333</td><td>476</td><td>10.5</td><td>49.056</td></tr><tr><td>TCAS-II'22</td><td>1</td><td>FP16</td><td>128</td><td>97</td><td>588</td><td>22.1</td><td>41.813</td></tr><tr><td>Ours</td><td>1024</td><td>DBFP</td><td>10872</td><td>3743</td><td>455</td><td>73</td><td>509.563</td></tr></table></body></html>\n\nFPGA resource utilization (LUT and FF), maximum operating frequency, and FOM (a comprehensive metric).\n\n$$\n\\mathrm { F O M } = { \\frac { F _ { \\mathrm { m a x } } \\times N \\times W } { \\mathrm { L U T } + \\mathrm { F F } } }\n$$\n\n, where $\\mathrm { ~ w ~ }$ and $\\mathbf { N }$ denote the precision and numbers of the inputs. A higher FOM value indicates better performance.\n\nTab. 3 shows our comparison with multiple SOTA designs, including Xilinx FP IP (Koca, Do, and Chang 2023) baseline. While existing Softmax accelerators only support input bandwidths under 16, our design uniquely accommodates the larger bandwidths needed for modern LLMs. Testing with 1024-length sequences, our implementation achieves $5 4 . 2 1 \\%$ less resource usage while operating at higher frequencies, reduces processing latency by $6 2 . 5 \\%$ , and delivers $1 2 8 \\mathrm { x }$ higher computational bandwidth. Notably, it shows a $1 0 \\mathrm { x }$ FOM improvement over ISCAS’23 SOTA, demonstrating the significant potential of our approach for nonlinear operation hardware units.\n\nDesign Evaluation on ASIC. ASIC implementations can be supplemented with more accurate data on power consumption, maximum clock frequency, and scalability for high-volume applications. We evaluate the hardware design on ASIC using four key metrics: Area-Delay-Product (ADP, Area $\\times$ Latency), Energy-Delay-Product (EDP, Energy $\\times$ Latency), and Throughput (Freq $\\times$ Bandwidth).\n\nFor scenarios with a uniform input sequence length of 1024, we normalized the experimental results to $2 8 \\mathrm { n m }$ . Fig. 5 (d) compares our design’s normalized PPA (Power, Performance, and Area) metrics with SOTA designs (Cardarilli et al. 2021; Kouretas and Paliouras 2020). Our design is capable of handling large bandwidth and long input sequences. Hence, there is an average $10 \\%$ increase in area\n\nF Latency Vector Index Vecn MAX SEU EXP DIV 1 ： Vec45 MAX SEU EXP DIV MAX SEU EXP DIV MAX SEU EXP DIV Vec2 MAX SEU EXP DIV   \n0863662 Vec1 MAX SEU EXP DIV Cycle   \n(a) Scaling Scalability (x-axis: Seq. Len.) (b)  Running Pipeline\n\ncompared to the SOTA design. However, this increase is offset by a significant enhancement in both energy consumption and throughput, exceeding 10x. In this context, the additional area requirement is considered a tolerable trade-off for substantial improvements in performance and efficiency.\n\nHardware Scalability. The design keeps scalability in mind. To demonstrate this feature, we conducted tuning tests with input sizes ranging from 8 to 4096 elements. Fig. 6 shows the total latency and each part of the processing pipeline. The total computation time grows exponentially with input size due to the quadratic relationship between input length and the size of the processed matrix. The scaled histogram in Fig. 6 shows the balanced growth in the proportion of time consumed by each pipeline level as the input size increases. No single component shows disproportionate latency growth; instead, pipeline allocation becomes more balanced with longer sequences. These observations evidenced the parallelism and scalability of the design.\n\n# Conclusion\n\nWe present DBFP, an enhanced BFP variant optimizing nonlinear operations. Our DB-Attn framework, enables efficient Attention, advancing narrow-precision LLM inference.\n\nLessons learned. Different from the conventional solutions that attempted to optimize nonlinear computation solely through hardware design or software techniques, this work shows that using an algorithm/hardware co-designed approach (DB-Attn), computation latency and memory accesses can be largely improved with light overhead. The construction of DBFP and the algorithm-driven hardware provide key insights and effective means that foster a collaborative environment of hardware and the algorithm for LLMs research that neither discipline could achieve independently.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决的核心问题是大型语言模型（LLMs）在推理过程中非线性操作（如Softmax）的高计算复杂性和内存需求，这些操作由于依赖浮点运算而成为性能瓶颈。\\n> *   该问题的重要性在于，随着序列长度的增加，非线性操作的时间和内存开销呈超线性增长（如图1所示），严重限制了LLMs的部署效率和应用场景。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种硬件-软件协同设计的框架（DB-Attn），包括动态块浮点（DBFP）、专用于非线性操作的查找表算法（DH-LUT）以及基于RTL的DBFP引擎，显著提升了非线性操作的效率。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **DBFP**：一种改进的块浮点格式，通过动态分组和自适应指数共享策略，解决了非线性操作的精度和效率问题。在LLaMA-7B上实现了与FP16相当的精度（64.02% vs 64.05%）。\\n> *   **DH-LUT**：一种新型查找表算法，专为DBFP格式设计，在GPU上实现了Softmax操作74%的速度提升。\\n> *   **硬件实现**：基于RTL的DBFP引擎在FPGA和ASIC上实现了10倍的性能提升（FOM 509.563 vs SOTA 49.056），同时保持了与浮点运算相当的精度。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   DBFP通过动态分组和自适应指数共享策略，将浮点运算转换为整数运算，从而显著降低了非线性操作的计算复杂性和内存需求。其设计哲学是通过硬件-软件协同优化，在保持精度的同时最大化计算效率。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比**：传统的块浮点（BFP）格式在非线性操作中表现不佳，主要由于固定的指数共享策略和对异常值的敏感性。\\n> *   **本文的改进**：DBFP引入了枢轴聚焦策略（使用中位数而非最大值作为对齐基准）和自适应分组策略（根据数据分布动态调整指数共享），显著提升了精度和效率。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **DBFP格式转换**：将浮点数转换为DBFP格式，通过动态分组和自适应指数共享策略优化精度。\\n> 2.  **DH-LUT构建**：根据DBFP的共享指数和非线性函数的特性，构建非均匀分层的查找表（如图3所示）。\\n> 3.  **硬件实现**：设计并实现基于RTL的DBFP引擎，支持FPGA和ASIC部署，优化了非线性操作的数据流和计算效率（如图4所示）。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   FP32、FP16、vanilla BFP、FP8 e4m3、FP8 e5m2、Hyft、TCAS-I’22、ISCAS’23、TCAS-II’22。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在零-shot任务准确率上**：本文方法在LLaMA-7B模型上的平均准确率为64.02%，显著优于vanilla BFP（42.06%）和FP8 e4m3（34.95%）。与表现最佳的基线FP16（64.05%）相比，保持了相当的精度。\\n> *   **在推理速度上**：本文方法的Softmax操作在GPU上实现了74%的速度提升，处理速度为FP16的1.74倍。在FPGA上，本文方法的FOM（性能指标）为509.563，远高于ISCAS’23（49.056）和TCAS-II’22（41.813）。\\n> *   **在硬件资源利用率上**：本文方法的FPGA实现减少了62.5%的延迟，同时资源使用量比SOTA设计减少了54.21%。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   块浮点 (Block Floating Point, BFP)\\n*   动态块浮点 (Dynamic Block Floating Point, DBFP)\\n*   查找表算法 (Lookup Table Algorithm, DH-LUT)\\n*   硬件-软件协同设计 (Hardware-Software Co-Design, N/A)\\n*   非线性操作 (Nonlinear Operations, N/A)\\n*   大型语言模型 (Large Language Models, LLMs)\\n*   注意力机制 (Attention Mechanism, N/A)\\n*   高性能计算 (High Performance Computing, HPC)\"\n}\n```"
}