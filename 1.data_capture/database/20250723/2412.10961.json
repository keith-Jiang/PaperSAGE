{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.10961",
    "link": "https://arxiv.org/abs/2412.10961",
    "pdf_link": "https://arxiv.org/pdf/2412.10961.pdf",
    "title": "PSMGD: Periodic Stochastic Multi-Gradient Descent for Fast Multi-Objective Optimization",
    "authors": [
        "Mingjing Xu",
        "Peizhong Ju",
        "Jia Liu",
        "Haibo Yang"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "2024-12-14",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 2,
    "influential_citation_count": 2,
    "institutions": [
        "Rochester Institute of Technology",
        "University of Kentucky",
        "The Ohio State University"
    ],
    "paper_content": "# PSMGD: Periodic Stochastic Multi-Gradient Descent for Fast Multi-Objective Optimization\n\nMingjing ${ \\bf X } { \\bf u } ^ { 1 }$ , Peizhong $\\mathbf { J } \\mathbf { u } ^ { 2 }$ , Jia $\\mathbf { L i u } ^ { 3 }$ , Haibo Yang\n\n1 Dept. of Computing and Information Sciences Ph.D., Rochester Institute of Technology 2 Dept. of Computer Science, University of Kentucky 3 Dept. of Electrical and Computer Engineering, The Ohio State University {mx6835, hbycis} $@$ rit.edu, peizhong.ju $@$ uky.edu, liu.1736@osu.edu\n\n# Abstract\n\nMulti-objective optimization (MOO) lies at the core of many machine learning (ML) applications that involve multiple, potentially conflicting objectives. Despite the long history of MOO, recent years have witnessed a surge in interest within the ML community in the development of gradient manipulation algorithms for MOO, thanks to the availability of gradient information in many ML problems. However, existing gradient manipulation methods for MOO often suffer from long training times, primarily due to the need for computing dynamic weights by solving an additional optimization problem to determine a common descent direction that can decrease all objectives simultaneously. To address this challenge, we propose a new and efficient algorithm called Periodic Stochastic Multi-Gradient Descent (PSMGD) to accelerate MOO. PSMGD is motivated by the key observation that dynamic weights across objectives exhibit small changes under minor updates over short intervals during the optimization process. Consequently, our PSMGD algorithm is designed to periodically compute these dynamic weights and utilizes them repeatedly, thereby effectively reducing the computational overload. Theoretically, we prove that PSMGD can achieve state-ofthe-art convergence rates for strongly-convex, general convex, and non-convex functions. Additionally, we introduce a new computational complexity measure, termed backpropagation complexity, and demonstrate that PSMGD could achieve an objective-independent backpropagation complexity. Through extensive experiments, we verify that PSMGD can provide comparable or superior performance to state-of-the-art MOO algorithms while significantly reducing training time.\n\n# Code â€” https://github.com/MarcuXu/PSMG Extended version â€” https://arxiv.org/abs/2412.10961\n\n# 1 Introduction\n\nJust as the human world is full of diverse and potentially conflicting goals, many machine learning paradigms are also multi-objective, such as multi-objective reinforcement learning (Hayes et al. 2022), multi-task learning (Caruana 1997; Sener and Koltun 2018), and learning-to-rank (Mahapatra et al. 2023). At the core of these machine learning paradigms lies multi-objective optimization (MOO), which aims to optimize multiple potentially conflicting objectives simultaneously. Formally, MOO can be mathematically cast as:\n\n$$\n\\operatorname* { m i n } _ { \\mathbf { x } \\in \\mathcal { D } } \\mathbf { F } ( \\mathbf { x } ) : = [ f _ { 1 } ( \\mathbf { x } ) , \\cdot \\cdot \\cdot , f _ { S } ( \\mathbf { x } ) ] ,\n$$\n\nwhere $\\mathbf { x } \\in \\mathcal { D } \\subseteq \\mathbb { R } ^ { d }$ is the model parameter, and $f _ { s } : \\mathbb { R } ^ { d } $ $\\mathbb { R }$ , $s \\in [ S ]$ is the objective function.\n\nThe most common approach for handling MOO is to optimize a weighted average of the multiple objectives, also known as linear scalarization (Kurin et al. 2022; Xin et al. 2022). This approach transforms MOO into a single-objective problem, for which there are various off-the-shelf methods available. The linear scalarization approach has a fast periteration runtime but suffers from slow convergence and poor performance due to potential conflicts among objectives. To address this problem, there has been a surge of interest in recent years in developing gradient manipulation algorithms (Kendall, Gal, and Cipolla 2018; Chen et al. 2018; Yu et al. 2020; Chen et al. 2020; Javaloy and Valera 2021; Liu and Vicente 2021; Chen et al. 2024; Xiao, Ban, and Ji 2024). The key idea is to calculate dynamic weights to avoid conflicts and find a direction $\\mathbf { d } ( \\mathbf { x } )$ that decreases all objectives simultaneously. This common descent vector $\\mathbf { d } ( \\mathbf { x } )$ is often determined by solving an additional optimization problem that involves all objective gradients. Updating the model with the common descent direction d ensures a decrease in every objective. While these approaches show improved performance, the extra optimization process requires a substantial amount of time. This is particularly evident when the number of objectives is large and the model parameters are highdimensional, which could significantly prolong the MOO training process. Existing empirical evaluations consistently demonstrate that gradient manipulation algorithms typically necessitate much longer training times (Kurin et al. 2022; Liu et al. 2024) (also shown in Sec 4). To this end, a natural question arises: Can we design efficient gradient manipulation algorithms for fast MOO with theoretical guarantees?\n\nIn this paper, we present Periodic Stochastic MultiGradient Descent (PSMGD), a simple yet effective algorithm to accelerate the MOO. Our PSMGD is motivated by the key observation that dynamic weights across objectives exhibit small changes under minor updates over short intervals. Thus, it is designed to periodically compute these dynamic weights and utilizes them repeatedly, thereby effectively reducing the computational overload and achieving a faster MOO training process. Our contributions are summarized as follows:\n\nâ€¢ Built upon the key observation, we introduce PSMGD, a new and efficient gradient manipulation method. By calculating dynamic weights infrequently, it significantly alleviates computational burden and reduces training time. â€¢ We conduct a theoretical analysis of PSMGD for strongly convex, convex, and non-convex functions, demonstrating that PSMGD achieves state-of-the-art convergence rates comparable to existing MOO methods (Table 1). We also introduce a new computational complexity measure, backpropagation (BP) complexity, to quantify computational workload, and further show that PSMGD can achieve an objective-independent backpropagation complexity. â€¢ Through comprehensive evaluation, PSMGD shows comparable or even superior performance compared to existing MOO methods, with significantly less training time.\n\n# 2 Related Works\n\nThe MOO problem, as stated in Eq. 1, has a long history that dates back to the 1950s. MOO algorithms can be broadly categorized into two main groups. The first category comprises gradient-free methods, such as evolutionary MOO algorithms and Bayesian MOO algorithms (Zhang and Li 2007; Deb et al. 2002; Belakaria et al. 2020; Laumanns and Ocenasek 2002). These methods are more suitable for small-scale problems but are less practical for high-dimensional models, such as deep neural networks. The second category is the gradientbased approach by utilizing (stochastic) gradients (Fliege and Svaiter 2000; DÃ©sidÃ©ri 2012; Fliege, Vaz, and Vicente 2019; Liu and Vicente 2021), making them more practical for high-dimensional MOO problems. In this work, we primarily focus on gradient-based approaches to solve MOO in high-dimensional deep-learning models. We first provide a brief overview of two typical approaches: linear scalarization (LS) and gradient manipulation methods.\n\n1) LS: A straightforward approach is to transform it into a single objective problem by using pre-defined weights $\\lambda$ : $\\mathrm { m i n } _ { \\mathbf { x } \\in \\mathcal { D } } \\lambda ^ { \\dagger } \\mathbf { F } ( \\mathbf { x } )$ . Thanks to the LS, one can leverage many existing single-objective methods (e.g., gradient descent), for training at each iteration. However, in LS, it is not uncommon to see conflicts among multiple objectives during the optimization process, i.e., $\\langle \\nabla f _ { s } ( \\bar { \\bf x } ) , \\nabla \\bar { f _ { s ^ { \\prime } } } ( { \\bf x } ) \\rangle < 0$ . This implies that the update using static weights may decrease some objectives, while inevitably increasing others at the same time and leading to slow convergence and poor performance.\n\n2) Gradient Manipulation Methods: A popular alternative is to dynamically weight gradients across objectives to avoid such conflicts and obtain a direction $\\mathbf { d }$ to decrease all objectives simultaneously. For example, multiple gradient descent algorithm (MGDA) (Fliege and Svaiter 2000) seeks to find the $\\mathbf { d }$ by solving the following optimization problem given gradients: $\\begin{array} { r } { ( \\mathbf { d } , \\beta ) \\ \\in \\ \\arg \\operatorname* { m i n } _ { \\mathbf { d } \\in \\mathbb { R } ^ { S } , \\beta \\in \\mathbb { R } } \\beta \\ + } \\end{array}$ $\\begin{array} { r } { \\frac { 1 } { 2 } \\| \\mathbf { d } \\| ^ { 2 } , s . t . , \\nabla f _ { s } ( \\mathbf { x } ) ^ { T } \\mathbf { d } \\mathrm { ~ - ~ } \\beta \\leq 0 , \\forall s \\in [ S ] } \\end{array}$ . By doing so, if $\\mathbf { x }$ represents a first-order Pareto stationary point, then $\\mathbf { \\left( d , \\right)} \\beta  \\ = \\ \\left( \\mathbf { 0 , } 0 \\right)$ . Otherwise, we can find the $\\mathbf { d }$ such that $\\nabla f _ { s } ( { \\mathbf x } ) ^ { T } \\mathbf { d } \\ \\le \\ \\beta \\ < \\ 0 , s \\ \\in \\ [ S ]$ as the solution. Using such a non-conflicting direction $\\mathbf { d }$ to update the model, i.e., $\\mathbf { x } \\gets \\mathbf { x } + \\eta \\mathrm { d }$ where $\\eta$ is the learning rate, has been shown to achieve better performance in practice (Sener and Koltun 2018; Liu et al. 2021a). Following this token, many gradient manipulation methods attempt to obtain a common descent direction through various formulations and solutions. For example, GRADDROP (Chen et al. 2020) randomly dropped out highly conflicted gradients, RotoGrad (Javaloy and Valera 2021) rotated objective gradients to alleviate the conflict, and many other methods exploring similar principles (Kendall, Gal, and Cipolla 2018; Chen et al. 2018; Yu et al. 2020; Liu et al. 2021a; Liu and Vicente 2021; Chen et al. 2024; Xiao, Ban, and Ji 2024). However, due to the additional optimization process required to generate such d, gradient manipulation methods often incur an expensive per-iteration cost, thereby prolonging training time. In this work, we propose a new algorithm, PSMGD, which achieves fast per-iteration and overall convergence, along with enhanced performance.\n\n# 3 Periodic Stochastic Multi-Gradient Descent\n\nIn this section, we first present the basic concept of MOO, followed by a pedagogical example comparing linear scalarization with MGDA, a representative of gradient manipulation methods. Based on one observation of the stable variation of dynamic weights for MGDA, we propose the PSMGD algorithm, followed by its convergence analyses.\n\n# 3.1 Preliminaries of MOO\n\nAnalogous to the stationary and optimal solutions in singleobjective optimization, MOO seeks to adopt the notion of Pareto optimality/stationarity:\n\nDefinition 1 ((Weak) Pareto Optimality). For any two solutions x and y, we say x dominates y if and only if $f _ { s } ( { \\bf { x } } ) \\leq$ $f _ { s } ( \\mathbf { y } ) , \\forall s \\in [ S ]$ and $f _ { s } ( \\mathbf { x } ) < f _ { s } ( \\mathbf { y } ) , \\exists s \\in [ S ]$ . A solution $\\mathbf { x }$ is Pareto optimal if it is not dominated by any other solution. One solution x is weakly Pareto optimal if there does not exist a solution y such that $f _ { s } ( \\mathbf { x } ) > f _ { s } ( \\mathbf { y } ) , \\forall s \\in [ S ]$ .\n\nDefinition 2 (Pareto Stationarity). A solution x is said to be Pareto stationary if there is no common descent direction $\\mathbf { d } \\in \\mathbb { R } ^ { d }$ such that $\\dot { \\nabla } \\dot { f _ { s } } ( \\mathbf { x } ) ^ { \\top } \\mathbf { d } < 0 , \\forall s \\in [ S ]$ .\n\nSimilar to solving single-objective non-convex optimization problems, finding a Pareto-optimal solution in MOO is NP-Hard in general. As a result, it is often of practical interest to find a solution satisfying Pareto-stationarity (a necessary condition for Pareto optimality). Following Definition 2, if $\\mathbf { x }$ is not a Pareto stationary point, we can find a common descent direction $\\mathbf { d } \\in \\mathbb { R } ^ { d }$ to decrease all objectives simultaneously, i.e., $\\nabla f _ { s } ( \\mathbf { x } ) ^ { \\top } \\mathbf { d } < 0 , \\forall s \\in [ S ]$ . If no such a common descent direction exists at $\\mathbf { x }$ , then $\\mathbf { x }$ is a Pareto stationary solution. For example, multiple gradient descent algorithm (MGDA) (DÃ©sidÃ©ri 2012) searches for an optimal weight $\\lambda _ { * }$ of gradients $\\nabla \\mathbf { F } ( \\mathbf { x } ) : = \\{ \\nabla f _ { s } ( \\mathbf { x } ) , \\forall s \\in [ S ] \\}$ by solving $\\lambda _ { * } ( \\mathbf { x } ) = \\mathrm { a r g m i n } _ { \\lambda } \\| \\lambda ^ { \\top } \\nabla \\mathbf { F } ( \\mathbf { \\dot { x } } ) \\| ^ { 2 }$ , which is the dual of the original problem. Then, a common descent direction can be chosen as: $\\mathbf { d } = \\lambda _ { * } ^ { \\top } \\nabla \\mathbf { F } ( \\mathbf { x } )$ . MGDA performs the iterative update rule: $\\mathbf { x }  \\mathbf { x } - \\eta \\mathbf { d }$ until a Pareto optimal/stationary point is reached, where $\\eta$ is a learning rate. Many gradient manipulation algorithms have been inspired by MGDA. In the next subsection, we compare two typical gradient-based approaches using a pedagogical example, with MGDA representing the gradient manipulation approach.\n\nTable 1: Comparison of different algorithms for MOO problem in stochastic first order oracle. 1. BP Complexity measures the total number of backpropagation to achieve certain metric (Ïµ), which is defined in Def 3. 2. Notations: T is the number of iterations, S is the number of objectives, $\\mathtt { R }$ is the weight calculation hyper-parameter.   \n\n<html><body><table><tr><td>Convexity</td><td>Algorithm</td><td> Antinme Lipsxhitz)</td><td>Conveargence</td><td>BP Complexity</td></tr><tr><td rowspan=\"3\">Strongly Convex</td><td>SMG (Liu and Vicente 2021)</td><td>âˆš</td><td>ï¼ˆï¼‰</td><td>0ï¼ˆï¼‰</td></tr><tr><td>CR-MOG M zhotal. 20.24022)</td><td>xx</td><td>8</td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"3\">General Convex</td><td>PSMGD</td><td>X</td><td>0()</td><td>Oï¼ˆ+ (R-1)) ER</td></tr><tr><td>SMG (Liu and Vicente 2021)</td><td>âˆš</td><td>0ï¼‰</td><td></td></tr><tr><td>CR-MOGM (Zhou et al. 2022)</td><td>X</td><td>0ï¼‰</td><td>siese</td></tr><tr><td></td><td>PSMGD</td><td></td><td>0(</td><td>Oï¼ˆR+ (-))</td></tr><tr><td rowspan=\"4\">Non-Convex</td><td>CR-MOGM (Zhou et al. 2022)</td><td>X</td><td>0( VT</td><td></td></tr><tr><td>MoCO (Fernando et al. 2022)</td><td>X</td><td>O( VT</td><td></td></tr><tr><td>MoDo (Chen et al. 2024)</td><td>X</td><td>0ï¼ˆï¼‰</td><td></td></tr><tr><td>SDMGrad (Xiao,Ban, and Ji 2024)</td><td>X</td><td>O(</td><td>S-eS-eS-EeS-e</td></tr><tr><td></td><td>PSMGD</td><td>X</td><td></td><td>O(R+ (R-)</td></tr></table></body></html>\n\n# 3.2 A Pedagogical Example\n\nWe consider the Fonseca problem(Fonseca and Fleming 1996; Pardalos et al. 2017), which is to solve a twoobjective optimization problem, $\\mathrm { m i n } _ { \\mathbf { x } } [ f _ { 1 } ( \\mathbf { x } ) , f _ { 2 } ( \\mathbf { x } ) ]$ . These two objective functions are defined by $\\begin{array} { r } { f _ { 1 } ( \\mathbf { x } ) = 1 - \\exp \\left( - \\sum _ { i = 1 } ^ { d } \\left( x _ { d } - \\frac { 1 } { \\sqrt { d } } \\right) ^ { 2 } \\right) , f _ { 2 } ( \\mathbf { x } ) = 1 - } \\end{array}$ $\\begin{array} { r } { \\exp { \\left( - \\sum _ { i = 1 } ^ { d } \\left( x _ { d } + \\frac { 1 } { \\sqrt { d } } \\right) ^ { 2 } \\right) } } \\end{array}$ where $\\mathbf { x } ~ = ~ ( x _ { 1 } , x _ { 2 } , . . . , x _ { d } )$ representing the d-dimensional decision variable. The problem exhibits a non-convex Pareto front in the objective space. We run three algorithms on this pedagogical example: linear scalarization, MGDA, and our proposed PSMGD.\n\nSetting. We run each algorithm 10 times. In Figure 1, we show the trajectory of different methods from random starting points. The blue dashed lines represent the trajectories, with the final solutions highlighted by red dots.\n\nObservation 1: Linear scalarization can only converge to specific points and is unable to explore different trade-offs among objectives. we run the linear scalarization method by using randomly static weights. As shown in Figure 1(a), it is obvious that linear scalarization can only converge to two specific points (see red dots), with either $\\dot { f } _ { 1 } ( { \\bf x } )$ or $f _ { 2 } ( \\mathbf { x } )$ being under-optimized. In other words, linear scalarization cannot fully explore the Pareto front, which is consistent with previous results (Miettinen 1999; Pardalos et al. 2017). In contrast, MGDA can find trade-off points, and our proposed PSMGD successfully identifies a set of well-distributed Pareto solutions with different trade-offs, shown in Figure 1(b,c). We note that linear scalarization usually runs faster for MOO in each iteration than gradient manipulation methods due to its simplicity, as shown in many existing works (Kurin et al. 2022; Liu et al. 2024) and confirmed by our experimental results (see Sec 4). However, our observation highlights the necessity of gradient manipulation methods and motivates us to develop faster gradient manipulation techniques for MOO.\n\nObservation 2: The dynamic weights calculated by MGDA converge quickly and exhibit stability. As shown in Figure 1(d), we visualize the dynamic weights for the Fonseca problem. In this simple problem, the weights converge within 5 iterations, after which they remain unchanged. In more complex problems, such as multi-task learning using deep learning (see Sec B in Appendix), the dynamic weights calculated by gradient manipulation methods also demonstrate stability. We verified stability by examining two scales, epoch and iteration. The weights show consistent behavior with a stable average value and low variability across both scales, epoch and iteration, as depicted in Figure 6(a) and 6(b) in Appendix. This observation suggests a new approach to develop more efficient MOO algorithms. Specifically, we can calculate and update the dynamic weights periodically in a more lazy and thus efficient manner.\n\n# 3.3 PSMGD\n\nBased on our observations, we propose a new and efficient algorithm called Periodic Stochastic Multi-Gradient Descent (PSMGD) for MOO, as shown in Algorithm 1. Our PSMGD algorithm is designed to periodically compute these dynamic weights and utilizes them repeatedly, thereby reducing the computational load effectively. Specifically, in each iteration $t \\in [ T ]$ , we offer two options: 1) If $t \\% R = = 0$ , we calculate the dynamic weights $\\hat { \\lambda } _ { t } ^ { * }$ based on the stochastic gradient of each objective $\\nabla f _ { s } ( \\mathbf { x } _ { t } , \\dot { \\xi } _ { t } ) , s \\in [ S ]$ , by solving optimization problem 2. Following this, we apply momentum to $\\lambda$ to further stabilize the weights: $\\lambda _ { t } = \\alpha _ { t } \\lambda _ { t - R } + ( 1 - \\alpha _ { t } ) \\hat { \\lambda } _ { t } ^ { * }$ 2) Otherwise, we reuse the dynamic weights: $\\lambda _ { t } = \\lambda _ { t - 1 }$ . With predefined $\\lambda _ { t }$ , it is worth pointing out that the MOO can be naturally transformed into a single-objective problem by using pre-defined weighted sum of the objectives, thereby requiring only one backpropagation step. Subsequently, after obtaining the weights across objectives, we can approximate the common descent direction by $\\mathbf { d } _ { t } = \\lambda _ { t } ^ { T } \\nabla \\mathbf { F } ( \\mathbf { x } _ { t } ^ { - } , \\boldsymbol { \\xi } _ { t } )$ . Then the model can be updated by $\\mathbf { x } _ { t + 1 } = \\mathbf { x } _ { t } - \\eta _ { t } \\mathbf { d } _ { t }$ .\n\n![](images/448bc40cea092326a3008b53341164a9a242a8b72812e219561ed0f049350dbb.jpg)  \nFigure 1: The convergence behaviors on a pedagogical example with 10 runs for each algorithm from left to right: (a) Solutions obtained from random linear scalarization. (b) Solutions obtained from the MGDA method. (c) Solutions obtained from the PSMGD method proposed in this paper, with weights updated every 4 iterations $R = 4 ,$ ) through the training. (d) We visualize weights changing curves over iterations in 5 runs. This method successfully generates a set of widely distributed Pareto solutions with different trade-offs. Details of the pedagogical example can be found in Section 3.2.\n\nAlgorithm 1: Periodic Stochastic Multi-Gradient Descent (PSMGD)   \n\n<html><body><table><tr><td colspan=\"2\">1:Initialize model parameter Xo,learning rate Î·,and hyper parameter R. 2:for t=O,...,T-1do</td></tr><tr><td>3:</td><td>If t%R== 0: â†’Weights Calculation</td></tr><tr><td>4:</td><td>Compute X* âˆˆ [0,1]S by solving</td></tr><tr><td></td><td>min Â£ xsVfs(xt, å…¥ sâˆˆ[S] s.t. å…¥s= 1. (2)</td></tr><tr><td></td><td>sâˆˆ[S]</td></tr><tr><td>5:</td><td>Update:Xt =Î±tå…¥t-R+(1-Î±t)*.</td></tr><tr><td>6: 7:</td><td>Otherwise: Xt = Xt-1Â· â†’Reuse the weights Update the model: Xt+1 = Xt - ntdt,where dt = âˆ‘sE[s] At,sVfs(xt,St).</td></tr></table></body></html>\n\n8: end for\n\n# 3.4 Convergence Analysis\n\nAssumption 3.1 (L-Lipschitz continuous). $\\parallel \\nabla f _ { s } ( { \\bf x } ) \\ -$   \n$\\nabla f _ { s } ( \\mathbf { y } ) \\mathbf { \\bar { \\phi } } \\| \\leq L \\| \\mathbf { x } - \\mathbf { y } \\| , \\nabla \\mathbf { x } , \\mathbf { y } \\in \\mathbb { R } ^ { d } , s \\in [ S ] .$ .\n\nAssumption 3.2 (Bounded variance). We assume the stochastic gradient estimation is unbiased with bounded variance.\n\n$$\n\\begin{array} { r l } & { \\mathbb { E } \\left[ \\nabla f _ { s } ( \\mathbf { x } , \\boldsymbol { \\xi } ) \\right] = \\nabla f _ { s } ( \\mathbf { x } ) , } \\\\ & { \\mathbb { E } \\left[ \\| \\nabla f _ { s } ( \\mathbf { x } , \\boldsymbol { \\xi } ) - \\nabla f _ { s } ( \\mathbf { x } ) \\| ^ { 2 } \\right] \\leq \\sigma ^ { 2 } , s \\in [ S ] . } \\end{array}\n$$\n\nAssumption 3.3 (Bounded weights). There exists a constant $B$ s.t., $\\begin{array} { r } { \\bar { 0 } \\le \\lambda _ { t , s } \\le B , \\sum _ { s \\in [ S ] } \\bar { \\lambda _ { t , s } } \\ge 1 , \\forall s \\in [ S ] , t \\in [ T ] } \\end{array}$ .\n\nAssumption 3.4 (Bounded Gradient). The gradient of each objective is bounded, i.e., there exists a constant $H > 0$ such that $\\| \\nabla f _ { s } ( \\mathbf { x } ) \\| \\le H , \\forall s \\in [ S ]$ .\n\nThese four assumptions are widely-used assumptions in MOO (Liu and Vicente 2021; Zhou et al. 2022). For notation clarity, we have the following definition: $G ( \\mathbf { x } _ { t } , \\lambda _ { t } ) =$ $\\begin{array} { r l r } { \\sum _ { s \\in [ S ] } \\lambda _ { t , s } f _ { s } ( \\mathbf { x } _ { t } ) , \\nabla \\mathbf { G } ( \\mathbf { x } _ { t } , \\lambda _ { t } ) } & { { } = } & { \\sum _ { s \\in [ S ] } \\lambda _ { t , s } \\nabla f _ { s } ( \\mathbf { x } _ { t } ) } \\end{array}$ With these assumptions and definitions, we have the following convergence rates:\n\nTheorem 3.5 (Non-Convex Functions). Under Assumptions 3.1- 3.4, when each objective is bounded by $F \\left( f _ { s } ( \\mathbf { x } ) \\leq \\right.$ $F , s \\in [ S ] )$ , the sequence of iterates generated by the PSMGD Algorithm in non-convex functions satisfies:\n\n$$\n\\begin{array} { l } { \\displaystyle \\frac { 1 } { T } \\sum _ { t \\in [ T ] } \\mathbb { E } [ \\| \\nabla \\mathbf { G } ( \\mathbf { x } _ { t } , \\lambda _ { t } ) \\| ^ { 2 } ] } \\\\ { \\displaystyle \\leq \\frac { 4 S B F } { T \\eta _ { T } } + \\frac { 2 F } { T } \\sum _ { t \\not \\in [ T ] } \\frac { \\mathbb { E } \\left[ \\sum _ { s \\in [ S ] } ( 1 - \\alpha _ { t } ) | \\hat { \\lambda } _ { t } ^ { s } - \\lambda _ { t - R } ^ { s } | \\right] } { \\eta _ { t } } } \\\\ { \\displaystyle + 2 L S ^ { 2 } B ^ { 2 } \\sigma ^ { 2 } \\frac { 1 } { T } \\sum _ { t \\in [ T ] } \\eta _ { t } + \\frac { 4 S ^ { 3 / 2 } B ^ { 2 } H \\sigma } { T } \\sum _ { t \\not \\in [ R = - 0 } ( 1 - \\alpha _ { t } ) . } \\end{array}\n$$\n\nSetting $\\begin{array} { r }  1 - \\alpha _ { t } \\ = \\ \\operatorname* { m i n } \\\\{ \\frac { \\eta _ { t } } { \\eta _ { 1 } } , \\frac { \\eta _ { t } } { \\eta _ { 1 } \\sqrt { t } \\operatorname* { m a x } _ { s \\in [ S ] } | \\hat { \\lambda } _ { t } ^ { s } - \\lambda _ { t - R } ^ { s } | } \\} , \\eta _ { t } \\ = \\ } \\end{array}$ $\\begin{array} { r } { \\mathcal { O } ( \\frac { 1 } { \\sqrt { t } } ) } \\end{array}$ , the convergence rate is $\\begin{array} { r l r } { \\langle \\mathcal { O } \\& { } \\mathrel { \\left( \\frac { 1 } { \\sqrt { T } } \\right) } } & { } & { } \\end{array}$ .\n\nTheorem 3.6 (General Convex Functions). Under Assumptions 3.1- 3.4, when the distance from sequence to Pareto set is bounded $( \\| \\mathbf x _ { t } - \\mathbf x _ { * } \\| \\leq D )$ , the sequence of iterates generated by the PSMGD in general convex functions satisfies:\n\n$$\n\\begin{array} { r l } & { \\displaystyle \\frac { 1 } { T } \\sum _ { t \\in [ T ] } \\mathbb { E } [ G ( \\mathbf { x } _ { t } , \\lambda _ { t } ) - G ( \\mathbf { x } _ { * } , \\lambda _ { t } ) ] } \\\\ & { \\quad \\le \\frac { \\| \\mathbf { x } _ { 1 } - \\mathbf { x } _ { * } \\| ^ { 2 } } { T \\eta _ { 1 } } + 2 D \\sigma S ^ { 3 / 2 } B \\displaystyle \\frac { 1 } { T } \\sum _ { t ^ { \\eta _ { 0 } } R = 0 } ( 1 - \\alpha _ { t } ) } \\\\ & { \\quad \\quad + ( 2 S ^ { 2 } B ^ { 2 } \\sigma ^ { 2 } + 2 S ^ { 2 } B ^ { 2 } H ^ { 2 } ) \\displaystyle \\frac { 1 } { T } \\sum _ { t \\in [ T ] } \\eta _ { t } . } \\end{array}\n$$\n\nSetting $\\begin{array} { r } { \\eta _ { t } = \\frac { 1 } { \\sqrt { t } } } \\end{array}$ and $( 1 - \\alpha _ { t } ) = \\eta _ { t }$ , the convergence rate is $\\mathcal { O } ( 1 / \\sqrt { T } )$ .\n\nTheorem 3.7 ( $\\mu$ -Strongly Convex Functions). Under Assumptions 3.1- 3.4 and we further assume each objective function $f _ { s } ( \\mathbf { x } ) , s \\in [ S ]$ is $\\mu -$ strongly convex and each objective is bounded by $F$ ${ \\bf \\zeta } ^ { \\prime } f _ { s } ( { \\bf x } ) \\le F , s \\in [ S ] )$ , the sequence of iterates generated by the PSMGD satisfies:\n\n$$\n\\mathbb { E } [ G ( \\mathbf { x } _ { t + 1 } , \\lambda _ { t + 1 } ) - G ( \\mathbf { x } _ { t + 1 } ^ { * } , \\lambda _ { t + 1 } ) ]\n$$\n\n$$\n\\leq ( 1 - 2 \\mu \\eta _ { t } ) \\mathbb { E } [ G ( \\mathbf { x } _ { t } , \\pmb { \\lambda } _ { t } ) - G ( \\mathbf { x } _ { t } ^ { * } , \\pmb { \\lambda } _ { t } ) ] + \\eta _ { t } ^ { 2 } \\Phi ,\n$$\n\nwhere $\\Phi = 2 L S ^ { 2 } B ^ { 2 } \\sigma ^ { 2 } + 4 F S B { \\bf 1 } \\{ ( t + 1 ) { \\% } R = 0 \\} +$ $4 S ^ { 3 / 2 } B ^ { 2 } H \\sigma { \\bf 1 } \\{ t \\% R = 0 \\}$ and 1 is the indicator function. Set $\\begin{array} { r } { \\eta = \\frac { c } { T } } \\end{array}$ with $\\textstyle c > { \\frac { 1 } { \\mu } }$ , the convergence rate is\n\n$$\n\\begin{array} { r l } & { \\mathbb { E } [ G ( \\mathbf { x } _ { T } , \\lambda _ { T } ) - G ( \\mathbf { x } _ { T } ^ { * } , \\lambda _ { T } ) ] } \\\\ & { \\leq \\frac { \\operatorname* { m a x } \\{ 2 c ^ { 2 } \\Phi ^ { ' } ( 2 \\mu c - 1 ) ^ { - 1 } , G ( \\mathbf { x } _ { 0 } , \\lambda _ { 0 } ) - G ( \\mathbf { x } _ { 0 } ^ { * } , \\lambda _ { 0 } ) \\} } { T } } \\\\ & { = \\mathcal { O } ( 1 / T ) , } \\end{array}\n$$\n\nwhere $\\Phi ^ { ' } = 2 L S ^ { 2 } B ^ { 2 } \\sigma ^ { 2 } + 4 F S B / R + 4 S ^ { 3 / 2 } B ^ { 2 } H \\sigma / R$\n\nRemark 3.8. 1) Convergence Metrics. Different convergence metrics are used in existing studies, and we follow convergence conditions outlined in (Tanabe, Fukuda, and Yamashita 2019; Zhou et al. 2022). In the appendix, we provide an explanation of convergence metrics and a comparison with other works. 2) Convergence Rates. With proper hyperparameters, our algorithm, PSMGD , can achieve $\\mathcal { O } ( 1 / T )$ for strongly convex functions and $\\mathcal { O } ( 1 / \\sqrt { T } )$ for general convex and non-convex functions. These convergence rates match the state-of-the-art rates in existing MOO algorithms.\n\nThe convergence rate reflects the training speed in terms of iterations but does not fully capture the total computational complexity. Similar to how sample complexity is used in single-objective learning, we propose a new metric, Backpropagation (BP) complexity, to quantify the computational workload tailored to MOO in first-order oracle.\n\nDefinition 3 (Backpropogation complexity). We define Backpropagation Complexity as the total number of backpropagation operations required by an algorithm to achieve a specified performance threshold, denoted as Ïµ.\n\nRemark 3.9. Our PSMGD algorithm can achieve BP complexity of $\\begin{array} { r } { \\mathcal { O } ( \\frac { S } { \\epsilon R } + \\frac { \\left( R - 1 \\right) } { \\epsilon R } ) } \\end{array}$ for stongly-convex functions and O( 2S R + (RÏµ2âˆ’R ) for general convex and non-convex functions. Compared to existing MOO methods, PSMGD can achieve a linear speedup in terms of $R$ .\n\nRemark 3.10. If $R = \\Omega ( S )$ , PSMGD exhibits an objectiveindependent $B P$ complexity, i.e., $\\mathcal { O } ( \\textstyle { \\frac { 1 } { \\epsilon } } )$ for stongly-convex functions and $\\begin{array} { r } { \\mathcal { O } ( \\frac { 1 } { \\epsilon ^ { 2 } } ) } \\end{array}$ for general convex and non-convex functions. These rates indicate that PSMGD requires the same order of computations for MOO as classic SGD does for single-objective learning. We provide a comparison of these rates and complexities in Table 1, which are also validated by extensive experiments in Sec 4.\n\n# 4 Experiment\n\nIn this section, we conduct a comprehensive empirical evaluation of the proposed PSMGD algorithm, focusing primarily on multi-task learning with deep neural networks. The primary goal is to verify the following key points:\n\nâ€¢ Improved Performance: Can PSMGD provide improved model performance?   \nâ€¢ Fast Training: Can PSMGD have a fast training process?   \nâ€¢ Ablation Study: How does the hyper-parameter impact the training? (Results In Appendix)\n\nBaseline Algorithms. To conduct an extensive comparison, we use 14 algorithms as the baselines, including a single objective learning baseline, linear scalarization, and 12 advanced MOO algorithms. They are: (1) Single task learning (STL), training independent models $f _ { S } ( { \\bf x } )$ for all objectives; (2) Linear scalarization (LS) baseline; (3) Scale-invariant (SI) that minimizes $\\sum _ { s \\in [ S ] } \\log f _ { s } ( { \\bf x } )$ ; (4) Dynamic Weight Average (DWA) (Liu, Johns, and Davison 2019) adaptively updates weights based on the comparative rate of loss reduction for each objective; (5) Uncertainty Weighting (UW) (Kendall, Gal, and Cipolla 2018) leverages estimated task uncertainty to guide weight assignments; (6) Random Loss Weighting (RLW) (Lin, Feiyang, and Zhang 2021) samples objective weighting with log-probabilities by normal distribution; (7) MGDA (Sener and Koltun 2018) identifies a joint descent direction that concurrently decreases all objectives; (8) PCGRAD (Yu et al. 2020) projects each objective gradient onto the normal plan of the gradients to avoid conflicts; (9) CAGRAD (Liu et al. 2021a) balances the average loss while guaranteeing a controlled minimum improvement for each objective; (10) IMTL-G (Liu et al. 2021b) determines the update direction by having equal projections on objective gradients; (11) GRADDROP (Chen et al. 2020) randomly drops out certain dimensions of the objective gradients for their level of conflict; (12) NASHMTL (Navon et al. 2022) determines the gameâ€™s solution that is advantageous for all goals by a bargaining game; (13) FAMO (Liu et al. 2024) is a dynamic weighting method that reduces objective losses in space and time in a balanced manner; (14) FAIRGRAD (Maheshwari and Perrot 2022) aims to achieve group fairness in MOO by dynamically re-weighting; (15) SDMGRAD (Xiao, Ban, and Ji 2024) utilizes direction-oriented regularization.\n\nDatasets. We evaluate on 5 multi-task learning datasets with objective/task numbers ranging from 2 to 40, covering scenarios in regression, classification, and dense prediction. For regression, we choose QM-9 dataset (Blum and Reymond 2009) (11 tasks), a widely used benchmark in graph neural network learning. For image classification, we use Multi-MNIST (Sener and Koltun 2018) (2 tasks) and CelebA dataset (Liu et al. 2015) (40 tasks). For dense prediction, CityScapes (Cordts et al. 2016) (2 tasks) and NYUv2 (Silberman et al. 2012) (3 tasks) are used in our experiments. NYU-v2 is a dataset of indoor scenes with 1449 RGBD images and dense per-pixel labeling across 13 classes. CityScapes is similar to NYU-v2 but boasts 5K street-view RGBD images with per-pixel annotations. We present the results for QM-9 and NYU-v2 in this section, with remaining results and experimental settings detailed in Appendix B.\n\nMetrics. We have two types of metrics to measure MOO algorithms. I. Model performance metrics. We consider two widely-used metrics to represent the overall performance of one MOO method $m$ : (1) $\\Delta m \\%$ , the average per-task performance drop of a method $m$ relative to the STL baseline denoted as $\\begin{array} { r } { B { : } \\Delta m \\% = \\frac { 1 } { N } \\sum _ { n = 1 } ^ { N } ( - 1 ) ^ { \\delta _ { n } } \\frac { ( M _ { m , n } - M _ { B , n } ) } { M _ { B , n } } \\times 1 0 0 , } \\end{array}$ where $M _ { B , n }$ and $M _ { m , n }$ represent the STL and $m$ â€™s value for metric $M _ { n }$ , respectively. Here, $\\delta _ { n }$ equals 1 if a higher value of $M _ { n }$ is better or 0 if a lower value of $M _ { n }$ is better. (2) Mean Rank (MR), the average rank of each method in the tasks. For example, if a method ranks first in every task (whether higher or lower is better), MR will be 1. Note that in practice, lower values of $\\Delta m \\%$ and MR indicate better overall performance in the final result. II. Convergence metrics. We calculate the training time per epoch and total training time to evaluate the convergence performance of MOO algorithms. Based on various datasets, we also calculate the test loss for a visualization comparison through the training process.\n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td>Î¼</td><td>a</td><td>EHOMO</td><td>ELUMO</td><td>(RÂ²)</td><td>ZPVE</td><td>Uo</td><td>U</td><td>H</td><td>G</td><td>Cu</td><td rowspan=\"2\">MRâ†“</td><td rowspan=\"2\">â–³m%â†“</td></tr><tr><td colspan=\"9\">MAEâ†“</td></tr><tr><td>STL</td><td>0.07</td><td>0.18</td><td>60.6</td><td>53.9</td><td>0.50</td><td>4.53</td><td>58.8</td><td>64.2</td><td>63.8</td><td>66.2</td><td>0.07</td><td></td><td></td></tr><tr><td>LS</td><td>0.11</td><td>0.33</td><td>73.6</td><td>89.7</td><td>5.20</td><td>14.06</td><td>143.4</td><td>144.2</td><td>144.6</td><td>140.3</td><td>0.13</td><td>9.00</td><td>177.6</td></tr><tr><td>SI</td><td>0.31</td><td>0.35</td><td>149.8</td><td>135.7</td><td>1.00</td><td>4.51</td><td>55.3</td><td>55.8</td><td>55.8</td><td>55.3</td><td>0.11</td><td>5.36</td><td>77.8</td></tr><tr><td>RLW</td><td>0.11</td><td>0.34</td><td>76.9</td><td>92.8</td><td>5.87</td><td>15.47</td><td>156.3</td><td>157.1</td><td>157.6</td><td>153.0</td><td>0.14</td><td>10.36</td><td>203.8</td></tr><tr><td>DWA</td><td>0.11</td><td>0.33</td><td>74.1</td><td>90.6</td><td>5.09</td><td>13.99</td><td>142.3</td><td>143.0</td><td>143.4</td><td>139.3</td><td>0.13</td><td>8.64</td><td>175.3</td></tr><tr><td>UW</td><td>0.39</td><td>0.43</td><td>166.2</td><td>155.8</td><td>1.07</td><td>4.99</td><td>66.4</td><td>66.8</td><td>66.8</td><td>66.2</td><td>0.12</td><td>6.64</td><td>108.0</td></tr><tr><td>MGDA</td><td>0.22</td><td>0.37</td><td>126.8</td><td>104.6</td><td>3.23</td><td>5.69</td><td>88.4</td><td>89.4</td><td>89.3</td><td>88.0</td><td>0.12</td><td>8.36</td><td>120.5</td></tr><tr><td>PCGRAD</td><td>0.11</td><td>0.29</td><td>75.9</td><td>88.3</td><td>3.94</td><td>9.15</td><td>116.4</td><td>116.8</td><td>117.2</td><td>114.5</td><td>0.11</td><td>7.18</td><td>125.7</td></tr><tr><td>CAGRAD</td><td>0.12</td><td>0.32</td><td>83.5</td><td>94.8</td><td>3.22</td><td>6.93</td><td>114.0</td><td>114.3</td><td>114.5</td><td>112.3</td><td>0.12</td><td>8.18</td><td>112.8</td></tr><tr><td>IMTL-G</td><td>0.14</td><td>0.29</td><td>98.3</td><td>93.9</td><td>1.75</td><td>5.70</td><td>101.4</td><td>102.4</td><td>102.0</td><td>100.1</td><td>0.10</td><td>6.64</td><td>77.2</td></tr><tr><td>NASHMTL</td><td>0.10</td><td>0.25</td><td>82.9</td><td>81.9</td><td>2.43</td><td>5.38</td><td>74.5</td><td>75.0</td><td>75.1</td><td>74.2</td><td>0.09</td><td>3.82</td><td>62.0</td></tr><tr><td>FAMO</td><td>0.15</td><td>0.30</td><td>94.0</td><td>95.2</td><td>1.63</td><td>4.95</td><td>70.82</td><td>71.2</td><td>71.2</td><td>70.3</td><td>0.10</td><td>5.09</td><td>58.5</td></tr><tr><td>FAIRGRAD</td><td>0.12</td><td>0.25</td><td>87.57</td><td>84.00</td><td>2.15</td><td>5.07</td><td>70.89</td><td>71.17</td><td>71.21</td><td>70.88</td><td>0.10</td><td>4.09</td><td>57.9</td></tr><tr><td>PSMGD</td><td>0.12</td><td>0.25</td><td>77.2</td><td>74.4</td><td>3.01</td><td>6.61</td><td>103.0</td><td>103.5</td><td>103.7</td><td>101.6</td><td>0.09</td><td>5.55</td><td>92.4</td></tr></table></body></html>\n\nTable 2: Results obtained on the QM-9 dataset. Each experiment is conducted using 3 random seeds, and the mean value is presented. The best average result is highlighted in bold.\n\n# 4.1 Regression: QM-9\n\nQM-9 dataset (Blum and Reymond 2009) is a crucial benchmark widely used in the field of graph neural network learning, especially in chemical informatics applications. With over 13K molecules, each molecule is intricately depicted as a graph, with nodes representing atoms and edges symbolizing the chemical bonds between them. These graphs feature detailed node and edge characteristics such as atomic number, atom type, bond type, and bond order. The goal is to predict 11 molecule properties. Utilizing 110K molecules from QM9 in PyTorch Geometric (Fey and Lenssen 2019) for training, 10K for validation, and the remaining 10K for testing.\n\nPerformance. We assess two common metrics, MR and $\\Delta m \\%$ , for various MOO algorithms to ensure effective evaluations. Table 2 shows that PSMGD achieves comparable or even better performance in both MR and $\\Delta m \\bar { \\% }$ when compared with existing baselines. Specifically, it has the best performance in 3 out of the 11 total tasks, namely $\\alpha$ , ÏµLUMO, and $c _ { v }$ . When we take a closer look at the training process, we observe that PSMGD has a faster and more smooth process as illustrated in Figure 2. Specifically, PSMGD demonstrates a smoother descent towards lower test loss at least in the early stages. In addition, PSMGD exhibits resistance to overfitting, distinguishing itself from IMTL-G and NashMTL due to its straightforward yet efficient structure.\n\nConvergence measurements. In Table 3, we show the training time per epoch and the total training time required to achieve a certain loss. In Figure 2, we use the average Mean Absolute Error (MAE) across all molecular property prediction tasks as test loss, calculated after scaling both the predictions and the ground truth values by the standard deviation. PSMGD ranks third in training speed per epoch but achieves the average test loss $( < 1 0 0 , < 7 5 )$ in the shortest overall time. The visualization results can be found in Appendix B.1.\n\n<html><body><table><tr><td>Method</td><td>Training timeâ†“ (mean per epoch)</td><td>Avg loss â†“ <100</td><td>Avg loss â†“ <75</td></tr><tr><td>LS</td><td>1.70</td><td>75.68</td><td>170.28</td></tr><tr><td>MGDA</td><td>12.94</td><td>436.92</td><td>754.68</td></tr><tr><td>PCGRAD</td><td>6.15</td><td>259.56</td><td>630.36</td></tr><tr><td>CAGRAD</td><td>5.29</td><td>233.64</td><td>451.35</td></tr><tr><td>IMTL-G</td><td>5.38</td><td>275.40</td><td>604.80</td></tr><tr><td>FAMO</td><td>1.96</td><td>72.52</td><td>131.32</td></tr><tr><td>NASHMTL</td><td>8.45</td><td>287.30</td><td>371.80</td></tr><tr><td>FAIRGRAD</td><td>5.40</td><td>162.60</td><td>314.36</td></tr><tr><td>PSMGD</td><td>2.23</td><td>69.13</td><td>109.27</td></tr></table></body></html>\n\nTable 3: Results obtained on the QM-9 dataset. Each experiment is conducted using 3 random seeds, and the mean value is presented. The best average result is highlighted in bold.\n\n# 4.2 Dense Prediction: NYU-v2\n\nNYU-v2 (Silberman et al. 2012) is an indoor scene dataset containing 1449 RGBD images, each annotated with dense per-pixel labeling across 13 distinct classes. The dataset supports 3 tasks for MOO experiments: image segmentation, depth prediction, and surface normal prediction.\n\nTable 4: Results obtained on NYU-v2. Each experiment is conducted using 3 random seeds, and the mean value is presented The best average result is highlighted in bold.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">Segmentation</td><td colspan=\"2\">Depth</td><td colspan=\"6\">Surface Normal</td><td rowspan=\"2\">MRâ†“ â–³m%â†“</td></tr><tr><td>mIoUâ†‘</td><td>Pix Acc â†‘</td><td>Abs Errâ†“</td><td>Rel Errâ†“</td><td>Angle Dist â†“</td><td></td><td></td><td>Within tÂ°â†‘</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>Mean</td><td>Median</td><td>11.25</td><td>22.5</td><td>30</td><td></td></tr><tr><td>STL</td><td>38.30</td><td>63.76</td><td>0.6754</td><td>0.2780</td><td>25.01</td><td>19.21</td><td>30.14</td><td>57.20</td><td>69.15</td><td></td></tr><tr><td>LS</td><td>39.29</td><td>65.33</td><td>0.5493</td><td>0.2263</td><td>28.15</td><td>23.96</td><td>22.09 47.50</td><td>61.08</td><td>11.44</td><td>5.59</td></tr><tr><td>SI</td><td>38.45</td><td>64.27</td><td>0.5354</td><td>0.2201</td><td>27.60</td><td>23.37</td><td>22.53 48.57</td><td>62.32</td><td>10.11</td><td>4.39</td></tr><tr><td>RLW</td><td>37.17</td><td>63.77</td><td>0.5759</td><td>0.2410</td><td>28.27</td><td>24.18</td><td>22.26</td><td>47.05 60.62</td><td>14.11</td><td>7.78</td></tr><tr><td>DWA</td><td>39.11</td><td>65.31</td><td>0.5510</td><td>0.2285</td><td>27.61</td><td>23.18</td><td>24.17</td><td>50.18 62.39</td><td>10.44</td><td>3.57</td></tr><tr><td>UW</td><td>36.87</td><td>63.17</td><td>0.5446</td><td>0.2260</td><td>27.04</td><td>22.61</td><td>23.54</td><td>49.05 63.65</td><td>10.11</td><td>4.05</td></tr><tr><td>MGDA</td><td>30.47 38.06</td><td>59.90 64.64</td><td>0.6070 0.5550</td><td>0.2555</td><td>24.88</td><td>19.45 22.80</td><td>29.18</td><td>56.88 69.36</td><td>8.11</td><td>1.38</td></tr><tr><td>PCGRAD</td><td>39.39</td><td>65.12</td><td>0.5455</td><td>0.2325 0.2279</td><td>27.41 27.48</td><td>22.96</td><td>23.86 23.38</td><td>49.83 63.14 49.44 62.87</td><td>10.67 9.56</td><td>3.97 3.58</td></tr><tr><td>GRADDROP CAGRAD</td><td>39.79</td><td>65.49</td><td>0.5486</td><td>0.2250</td><td>26.31</td><td>21.58</td><td>25.61</td><td>52.36 65.58</td><td>7.00</td><td>0.20</td></tr><tr><td>IMTL-G</td><td>39.35</td><td>65.60</td><td>0.5426</td><td>0.2256</td><td>26.02</td><td>21.19</td><td>26.20</td><td>53.13 66.24</td><td>6.33</td><td>-0.76</td></tr><tr><td>NASHMTL</td><td>40.13</td><td>65.93</td><td>0.5261</td><td>0.2171</td><td>25.26</td><td>20.08</td><td>28.40</td><td>55.47 68.15</td><td>4.22</td><td>-4.04</td></tr><tr><td>FAMO</td><td>38.88</td><td>64.90</td><td>0.5474</td><td>0.2194</td><td>25.06</td><td>19.57</td><td>29.21</td><td>56.61 68.98</td><td>5.11</td><td>-4.10</td></tr><tr><td>FAIRGRAD</td><td>39.74</td><td>66.01</td><td>0.5377</td><td>0.2236</td><td>24.84</td><td>19.60</td><td>29.26 56.58</td><td>69.16</td><td>3.22</td><td>-4.66</td></tr><tr><td>SDMGRAD</td><td>40.47</td><td>65.90</td><td>0.5225</td><td>0.2084</td><td>25.07</td><td>19.99</td><td>28.54</td><td>55.74 68.53</td><td>3.44</td><td>-4.84</td></tr><tr><td>PSMGD</td><td>35.44</td><td>63.78</td><td>0.5494</td><td>0.2369</td><td>24.83</td><td>18.89</td><td>30.68</td><td>58.00 69.84</td><td>6.11</td><td>-3.62</td></tr></table></body></html>\n\n<html><body><table><tr><td>Method</td><td>Tranipertimech)</td><td>Semantic 1os </td><td>Semantic 1os </td><td>Depth.oss $</td><td>Depth.oss </td><td>Noral2oss </td><td>Nora.1oss </td></tr><tr><td>LS</td><td>1.36</td><td>32.14</td><td>66.28</td><td>46.18</td><td>159.36</td><td>30.84</td><td>152.76</td></tr><tr><td>MGDA</td><td>3.34</td><td>166.43</td><td>384.32</td><td>218.88</td><td>590.96</td><td>56.44</td><td>249.77</td></tr><tr><td>PCGRAD</td><td>3.31</td><td>72.38</td><td>154.63</td><td>98.79</td><td>299.39</td><td>75.67</td><td>332.29</td></tr><tr><td>CAGRAD</td><td>3.53</td><td>63.18</td><td>137.89</td><td>94.77</td><td>351.06</td><td>66.69</td><td>213.11</td></tr><tr><td>IMTL-G</td><td>4.63</td><td>101.42</td><td>239.72</td><td>147.52</td><td>470.22</td><td>87.59</td><td>318.09</td></tr><tr><td>FAMO</td><td>1.65</td><td>32.69</td><td>68.46</td><td>73.35</td><td>156.26</td><td>34.23</td><td>114.13</td></tr><tr><td>NASHMTL</td><td>3.49</td><td>76.34</td><td>159.62</td><td>79.81</td><td>277.60</td><td>65.93</td><td>201.26</td></tr><tr><td>FAIRGRAD</td><td>2.93</td><td>141.82</td><td>328.86</td><td>148.47</td><td>334.53</td><td>48.20</td><td>177.41</td></tr><tr><td>SDMGRAD</td><td>1.98</td><td>31.52</td><td>72.89</td><td>63.04</td><td>200.94</td><td>35.46</td><td>128.05</td></tr><tr><td>PSMGD</td><td>1.85</td><td>29.28</td><td>64.54</td><td>43.07</td><td>176.66</td><td>31.11</td><td>113.46</td></tr></table></body></html>\n\nTable 5: Training time per epoch [Min.] and convergence process (averaged over 3 random seeds) in all tasks on NYU-v2.\n\nPerformance. As shown in Table 4, PSMGD achieves better performance in both MR and $\\Delta m \\%$ compared to existing MOO algorithms. It performs the best in 5 out of the 9 total tasks. In Figure 3, PSMGD achieves consistently low test losses across all tasks compared to other methods in the initial stage, indicating superior performance across all 3 tasks throughout the 200 training epochs.\n\nConvergence measurements. In Figure 3, PSMGD outperforms other MOO methods in reducing test loss for all three tasks on the NYU-v2 dataset, particularly in the early training stages. Table 5 further highlights PSMGDâ€™s efficiency with its third lowest training time per epoch (1.85), significantly outperforming other classical gradient manipulation methods like IMTL-G (Liu et al. 2021b) and NASHMTL (Navon et al. 2022). PSMGD achieves the fastest training times by reaching semantic loss in 29.28 and 64.54 minutes, depth loss in 43.07 minutes, and normal loss in 113.46 minutes, when considering the following thresholds: ${ < } 1 . 4 0$ and $< 1 . 2 0$ for semantic loss, $< 0 . 6 5$ for depth loss, and $< 0 . 1 6$ for normal loss. The visualization results can be found in Appendix B.1.\n\n# 5 Conclusion\n\nIn this paper, we propose a novel and efficient algorithm, Periodic Stochastic Multi-Gradient Descent (PSMGD), to accelerate MOO. Our PSMGD algorithm periodically computes dynamic weights and reuses them, significantly reducing the computational load and speeding up MOO training. We establish that PSMGD achieves state-of-the-art convergence rates for strongly convex, general convex, and non-convex functions. Moreover, we demonstrate the superior backpropagation (BP) complexity of our PSMGD algorithm. Extensive experiments confirm that PSMGD delivers performance comparable to or better than existing MOO algorithms, with a substantial reduction in training time. We believe future work could explore preference-based solutions or the entire Pareto set using our efficient PSMGD algorithm.",
    "summary": "```json\n{\n  \"core_summary\": \"### ðŸŽ¯ æ ¸å¿ƒæ¦‚è¦\\n\\n> **é—®é¢˜å®šä¹‰ (Problem Definition)**\\n> *   å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆMOOï¼‰åœ¨æœºå™¨å­¦ä¹ ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†çŽ°æœ‰æ¢¯åº¦æ“çºµæ–¹æ³•å› éœ€è¦é¢‘ç¹è®¡ç®—åŠ¨æ€æƒé‡è€Œè®­ç»ƒæ—¶é—´é•¿ã€‚\\n> *   è¯¥é—®é¢˜åœ¨é«˜ç»´æ¨¡åž‹å’Œå¤šç›®æ ‡åœºæ™¯ä¸‹å°¤ä¸ºçªå‡ºï¼Œé™åˆ¶äº†MOOåœ¨å®žé™…åº”ç”¨ä¸­çš„æ•ˆçŽ‡ã€‚\\n\\n> **æ–¹æ³•æ¦‚è¿° (Method Overview)**\\n> *   æå‡ºå‘¨æœŸæ€§éšæœºå¤šæ¢¯åº¦ä¸‹é™ï¼ˆPSMGDï¼‰ï¼Œé€šè¿‡å‘¨æœŸæ€§è®¡ç®—å’Œé‡å¤ä½¿ç”¨åŠ¨æ€æƒé‡ï¼Œæ˜¾è‘—å‡å°‘è®¡ç®—è´Ÿæ‹…ã€‚\\n\\n> **ä¸»è¦è´¡çŒ®ä¸Žæ•ˆæžœ (Contributions & Results)**\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹1ï¼š** PSMGDç®—æ³•åœ¨å¼ºå‡¸ã€ä¸€èˆ¬å‡¸å’Œéžå‡¸å‡½æ•°ä¸Šè¾¾åˆ°æœ€å…ˆè¿›çš„æ”¶æ•›é€ŸçŽ‡ï¼ˆO(1/T) å’Œ O(1/âˆšT)ï¼‰ã€‚\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹2ï¼š** å¼•å…¥åå‘ä¼ æ’­å¤æ‚åº¦ï¼ˆBPå¤æ‚åº¦ï¼‰ä½œä¸ºè®¡ç®—å·¥ä½œé‡åº¦é‡ï¼ŒPSMGDå¯å®žçŽ°ç›®æ ‡æ— å…³çš„BPå¤æ‚åº¦ï¼ˆO(1/Îµ) å’Œ O(1/ÎµÂ²)ï¼‰ã€‚\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹3ï¼š** å®žéªŒè¡¨æ˜ŽPSMGDåœ¨æ€§èƒ½ä¸Šä¸ŽçŽ°æœ‰MOOç®—æ³•ç›¸å½“æˆ–æ›´ä¼˜ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘è®­ç»ƒæ—¶é—´ï¼ˆå¦‚QM-9æ•°æ®é›†ä¸Šè®­ç»ƒæ—¶é—´å‡å°‘50%ä»¥ä¸Šï¼‰ã€‚\",\n  \"algorithm_details\": \"### âš™ï¸ ç®—æ³•/æ–¹æ¡ˆè¯¦è§£\\n\\n> **æ ¸å¿ƒæ€æƒ³ (Core Idea)**\\n> *   PSMGDåŸºäºŽåŠ¨æ€æƒé‡åœ¨çŸ­é—´éš”å†…å˜åŒ–å°çš„è§‚å¯Ÿï¼Œå‘¨æœŸæ€§è®¡ç®—æƒé‡å¹¶é‡å¤ä½¿ç”¨ï¼Œå‡å°‘è®¡ç®—å¼€é”€ã€‚\\n> *   è®¾è®¡å“²å­¦æ˜¯é€šè¿‡æ‡’æƒ°æ›´æ–°ç­–ç•¥å¹³è¡¡è®¡ç®—æ•ˆçŽ‡å’Œä¼˜åŒ–æ•ˆæžœã€‚\\n\\n> **åˆ›æ–°ç‚¹ (Innovations)**\\n> *   **ä¸Žå…ˆå‰å·¥ä½œçš„å¯¹æ¯”ï¼š** çŽ°æœ‰æ¢¯åº¦æ“çºµæ–¹æ³•ï¼ˆå¦‚MGDAï¼‰éœ€æ¯æ¬¡è¿­ä»£æ±‚è§£é¢å¤–ä¼˜åŒ–é—®é¢˜ï¼Œè®¡ç®—æˆæœ¬é«˜ã€‚\\n> *   **æœ¬æ–‡çš„æ”¹è¿›ï¼š** PSMGDé€šè¿‡å‘¨æœŸæ€§æ›´æ–°æƒé‡ï¼ˆæ¯Ræ¬¡è¿­ä»£ï¼‰ï¼Œé¿å…é¢‘ç¹æ±‚è§£ä¼˜åŒ–é—®é¢˜ï¼Œæ˜¾è‘—é™ä½Žè®¡ç®—è´Ÿæ‹…ã€‚\\n\\n> **å…·ä½“å®žçŽ°æ­¥éª¤ (Implementation Steps)**\\n> 1.  åˆå§‹åŒ–æ¨¡åž‹å‚æ•°å’Œå­¦ä¹ çŽ‡ã€‚\\n> 2.  æ¯Ræ¬¡è¿­ä»£è®¡ç®—åŠ¨æ€æƒé‡ï¼š\\n>      *   æ±‚è§£ä¼˜åŒ–é—®é¢˜ç¡®å®šæƒé‡ï¼ˆå…¬å¼2ï¼‰ã€‚\\n>      *   åº”ç”¨åŠ¨é‡ç¨³å®šæƒé‡ï¼ˆÎ»â‚œ = Î±â‚œÎ»â‚œâ‚‹áµ£ + (1-Î±â‚œ)Î»Ì‚â‚œ*ï¼‰ã€‚\\n> 3.  éžæƒé‡æ›´æ–°è¿­ä»£ä¸­é‡å¤ä½¿ç”¨çŽ°æœ‰æƒé‡ï¼ˆÎ»â‚œ = Î»â‚œâ‚‹â‚ï¼‰ã€‚\\n> 4.  ä½¿ç”¨åŠ æƒæ¢¯åº¦ï¼ˆdâ‚œ = Î»â‚œáµ€âˆ‡F(xâ‚œ,Î¾â‚œ)ï¼‰æ›´æ–°æ¨¡åž‹å‚æ•°ï¼ˆxâ‚œâ‚Šâ‚ = xâ‚œ - Î·â‚œdâ‚œï¼‰ã€‚\\n\\n> **æ¡ˆä¾‹è§£æž (Case Study)**\\n> *   è®ºæ–‡ä»¥Fonsecaé—®é¢˜ä¸ºä¾‹ï¼Œå±•ç¤ºPSMGDåœ¨äºŒç»´å†³ç­–å˜é‡ä¸Šçš„ä¼˜åŒ–è½¨è¿¹ï¼Œç”Ÿæˆåˆ†å¸ƒå‡åŒ€çš„Paretoè§£ï¼ˆå›¾1ï¼‰ã€‚\",\n  \"comparative_analysis\": \"### ðŸ“Š å¯¹æ¯”å®žéªŒåˆ†æž\\n\\n> **åŸºçº¿æ¨¡åž‹ (Baselines)**\\n> *   çº¿æ€§æ ‡é‡åŒ–ï¼ˆLSï¼‰ã€MGDAã€PCGRADã€CAGRADã€IMTL-Gã€NASHMTLã€FAMOç­‰14ç§MOOç®—æ³•ã€‚\\n\\n> **æ€§èƒ½å¯¹æ¯” (Performance Comparison)**\\n> *   **åœ¨è®­ç»ƒæ—¶é—´ä¸Šï¼š** PSMGDåœ¨QM-9æ•°æ®é›†ä¸Šå¹³å‡æ¯è½®è®­ç»ƒæ—¶é—´ä¸º2.23åˆ†é’Ÿï¼Œæ˜¾è‘—ä¼˜äºŽMGDAï¼ˆ12.94åˆ†é’Ÿï¼‰å’ŒPCGRADï¼ˆ6.15åˆ†é’Ÿï¼‰ã€‚ä¸Žæœ€å¿«åŸºçº¿FAMOï¼ˆ1.96åˆ†é’Ÿï¼‰ç›¸æ¯”ï¼ŒPSMGDåœ¨è¾¾åˆ°ç›¸åŒæµ‹è¯•æŸå¤±ï¼ˆ<75ï¼‰æ—¶æ€»è®­ç»ƒæ—¶é—´æ›´çŸ­ï¼ˆ109.27åˆ†é’Ÿ vs 131.32åˆ†é’Ÿï¼‰ã€‚\\n> *   **åœ¨æ¨¡åž‹æ€§èƒ½ä¸Šï¼š** PSMGDåœ¨NYU-v2æ•°æ®é›†ä¸Šå¹³å‡æŽ’åï¼ˆMRï¼‰ä¸º6.11ï¼Œä¼˜äºŽMGDAï¼ˆ8.11ï¼‰å’ŒPCGRADï¼ˆ10.67ï¼‰ï¼Œä¸Žæœ€ä½³åŸºçº¿NASHMTLï¼ˆ4.22ï¼‰æŽ¥è¿‘ã€‚\\n> *   **åœ¨æ”¶æ•›é€ŸçŽ‡ä¸Šï¼š** PSMGDåœ¨å¼ºå‡¸å‡½æ•°ä¸Šè¾¾åˆ°O(1/T)çš„æ”¶æ•›é€ŸçŽ‡ï¼Œä¸ŽçŽ°æœ‰æœ€ä¼˜æ–¹æ³•ç›¸å½“ï¼Œä½†è®¡ç®—å¤æ‚åº¦æ›´ä½Žï¼ˆBPå¤æ‚åº¦ä¸ºO(1/Îµ)ï¼‰ã€‚\",\n  \"keywords\": \"### ðŸ”‘ å…³é”®è¯\\n\\n*   å¤šç›®æ ‡ä¼˜åŒ– (Multi-Objective Optimization, MOO)\\n*   æ¢¯åº¦æ“çºµ (Gradient Manipulation, N/A)\\n*   å‘¨æœŸæ€§éšæœºå¤šæ¢¯åº¦ä¸‹é™ (Periodic Stochastic Multi-Gradient Descent, PSMGD)\\n*   åå‘ä¼ æ’­å¤æ‚åº¦ (Backpropagation Complexity, BP Complexity)\\n*   æœºå™¨å­¦ä¹  (Machine Learning, ML)\\n*   å¤šä»»åŠ¡å­¦ä¹  (Multi-Task Learning, MTL)\\n*   æ”¶æ•›é€ŸçŽ‡ (Convergence Rate, N/A)\"\n}\n```"
}