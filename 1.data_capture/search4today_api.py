import streamlit as st
import os
import re
import glob
import json
import random
import time
import requests
from datetime import datetime, timedelta
from pathlib import Path
from zipfile import ZipFile
from io import BytesIO
import concurrent.futures
# from transformers import AutoTokenizer # æš‚æ—¶ç”¨ä¸åˆ°
# from bs4 import BeautifulSoup # æš‚æ—¶ç”¨ä¸åˆ°

# =================================================================
# 0. æ¨¡å—å¯¼å…¥ä¸å ä½ç¬¦
# =================================================================
# --- å‡è®¾çš„å¤–éƒ¨æ¨¡å— ---
# æˆ‘ä»¬å…ˆç”¨å ä½ç¬¦å‡½æ•°æ›¿ä»£ï¼Œä»¥ç¡®ä¿UIå¯ä»¥ç‹¬ç«‹è¿è¡Œ
def summary_func(arxiv_id):
    """å ä½ç¬¦ï¼šæ¨¡æ‹Ÿè°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆæ€»ç»“"""
    st.toast(f"æ­£åœ¨ä¸º {arxiv_id} ç”Ÿæˆæ€»ç»“...")
    time.sleep(2) # æ¨¡æ‹Ÿè€—æ—¶
    # æ¨¡æ‹Ÿè¿”å›ä¸€ä¸ªåŒ…å«Markdownçš„JSONå­—ç¬¦ä¸²
    summary_data = {
        "ä¸€å¥è¯æ€»ç»“": "### ğŸš€ ä¸€å¥è¯æ€»ç»“\n\nè¿™æ˜¯ä¸€ç¯‡å…³äº Transformer æ¨¡å‹çš„å¼€åˆ›æ€§è®ºæ–‡ï¼Œæå‡ºäº†'Attention Is All You Need'çš„æ ¸å¿ƒæ€æƒ³ã€‚",
        "æ ¸å¿ƒè´¡çŒ®": "### ğŸ¯ æ ¸å¿ƒè´¡çŒ®\n\n- **è‡ªæ³¨æ„åŠ›æœºåˆ¶**: å®Œå…¨æ›¿ä»£äº†å¾ªç¯å’Œå·ç§¯ç»“æ„ã€‚\n- **ä½ç½®ç¼–ç **: è§£å†³äº†åºåˆ—ä¸­è¯è¯­çš„ä½ç½®ä¿¡æ¯é—®é¢˜ã€‚\n- **å¤šå¤´æ³¨æ„åŠ›**: å…è®¸æ¨¡å‹åœ¨ä¸åŒè¡¨ç¤ºå­ç©ºé—´ä¸­å…±åŒå…³æ³¨æ¥è‡ªä¸åŒä½ç½®çš„ä¿¡æ¯ã€‚"
    }
    return json.dumps(summary_data, ensure_ascii=False, indent=4)

def translate_text(text_to_translate):
    """å ä½ç¬¦ï¼šæ¨¡æ‹Ÿè°ƒç”¨ç¿»è¯‘API"""
    st.toast(f"æ­£åœ¨ç¿»è¯‘é€‰å®šæ–‡æœ¬...")
    time.sleep(1) # æ¨¡æ‹Ÿè€—æ—¶
    return f"ã€ç¿»è¯‘ç»“æœã€‘\n\n{text_to_translate}"

def rag_query(query, paper_content):
    """å ä½ç¬¦ï¼šæ¨¡æ‹Ÿè°ƒç”¨RAGè¿›è¡Œé—®ç­”"""
    st.toast(f"æ­£åœ¨åŸºäºåŸæ–‡å†…å®¹å›ç­”é—®é¢˜: {query}")
    time.sleep(2) # æ¨¡æ‹Ÿè€—æ—¶
    return f"å…³äºæ‚¨çš„é—®é¢˜ â€œ{query}â€ï¼Œæ ¹æ®è®ºæ–‡å†…å®¹ï¼Œç­”æ¡ˆæ˜¯... (è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„RAGå›ç­”)ã€‚"


# =================================================================
# 1. å…¨å±€é…ç½®å’Œåˆå§‹åŒ–
# =================================================================
st.set_page_config(page_title="PaperSAGE - ArXivè®ºæ–‡åŠ©æ‰‹", layout="wide")

# --- è·¯å¾„é…ç½® ---
BASE_PATH = Path("/home/zhangping/jrz-test/PaperSAGE/1.data_capture") 
DB_ROOT_PATH = BASE_PATH / "database"

# --- API å’Œæ¨¡å‹é…ç½® ---
# è­¦å‘Šï¼šè¯·å‹¿å°†æ•æ„Ÿä¿¡æ¯ç¡¬ç¼–ç ã€‚å»ºè®®ä½¿ç”¨st.secretsæˆ–ç¯å¢ƒå˜é‡ã€‚
API_TOKEN = 'Bearer eyJ0eXBlIjoiSldUIiwiYWxnIjoiSFM1MTIifQ.eyJqdGkiOiI2MjQwMzE5MCIsInJvbCI6IlJPTEVfUkVHSVNURVIiLCJpc3MiOiJPcGVuWExhYiIsImlhdCI6MTc1Mjg0Mzc4NCwiY2xpZW50SWQiOiJsa3pkeDU3bnZ5MjJqa3BxOXgydyIsInBob25lIjoiIiwib3BlbklkIjpudWxsLCJ1dWlkIjoiMzhkZDYzMjAtNzQ3Ny00ZjhjLTgwNTYtMWE0NjliNWUyZDc4IiwiZW1haWwiOiIiLCJleHAiOjE3NTQwNTMzODR9.2MBgX_jgoq6Z6XZLcMoi7YLZuJdQ_Yb2GXRh8SA0KglP0LWQZjUOTsv-xpgrIjCNGf9nBrsyRtg9CmUjIt6g0Q'
BASE_URL = 'https://mineru.net/api/v4'
POLLING_INTERVAL = 10

# --- ArXiv CS åˆ†ç±» ---
ARXIV_CATEGORIES = {
    "è®¡ç®—æœºç§‘å­¦ (CS)": [
        "cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.NE", "cs.RO",
        "cs.CR", "cs.DS", "cs.IT", "cs.SE"
    ]
    # å¯ä»¥æ·»åŠ æ›´å¤šé¡¶çº§åˆ†ç±»
}


# =================================================================
# 2. åç«¯åŠŸèƒ½å‡½æ•° (ä¸ä¹‹å‰ç‰ˆæœ¬åŸºæœ¬ç›¸åŒ)
# =================================================================

# --- æ­¥éª¤ 1: ä¸‹è½½PDF ---
def download_arxiv_pdf(arxiv_url, save_dir):
    """ä»ArXiv URLä¸‹è½½PDFæ–‡ä»¶ã€‚"""
    try:
        if not re.match(r'https?://arxiv\.org/abs/\d+\.\d+', arxiv_url):
            return None, "URLæ ¼å¼ä¸æ­£ç¡®ï¼Œåº”ä¸º 'https://arxiv.org/abs/...' æ ¼å¼ã€‚"
        
        pdf_url = arxiv_url.replace('/abs/', '/pdf/')
        arxiv_id = arxiv_url.split('/')[-1]
        pdf_filename = f"{arxiv_id}.pdf"
        save_path = save_dir / pdf_filename

        response = requests.get(pdf_url, stream=True, timeout=60)
        response.raise_for_status()

        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        return save_path, None
    except requests.exceptions.RequestException as e:
        return None, f"ä¸‹è½½PDFå¤±è´¥: {e}"
    except Exception as e:
        return None, f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}"

# --- æ­¥éª¤ 2: PDFè½¬Markdown (å•æ–‡ä»¶ç‰ˆæœ¬) ---
def convert_single_pdf_to_md(pdf_path, output_folder):
    """ä½¿ç”¨APIå°†å•ä¸ªPDFè½¬æ¢ä¸ºMarkdownã€‚"""
    api_headers = {'Content-Type': 'application/json', 'Authorization': API_TOKEN}
    filename = pdf_path.name
    
    try:
        response = requests.post(f'{BASE_URL}/file-urls/batch', headers=api_headers, json={"files": [{"name": filename}]})
        response.raise_for_status()
        result = response.json()
        if result.get("code") != 0: return None, f"è·å–ä¸Šä¼ URLå¤±è´¥: {result.get('msg')}"
        
        batch_id = result["data"]["batch_id"]
        upload_url = result["data"]["file_urls"][0]

        with open(pdf_path, 'rb') as f:
            res_upload = requests.put(upload_url, data=f)
            if res_upload.status_code != 200: return None, f"ä¸Šä¼ å¤±è´¥, çŠ¶æ€ç : {res_upload.status_code}"

        query_url = f"{BASE_URL}/extract-results/batch/{batch_id}"
        start_time = time.time()
        while time.time() - start_time < 300: # 5åˆ†é’Ÿè¶…æ—¶
            res_query = requests.get(query_url, headers=api_headers)
            if res_query.status_code == 200:
                result = res_query.json()
                if result.get("code") == 0 and result.get("data", {}).get("extract_result"):
                    task = result["data"]["extract_result"][0]
                    if task.get("state") == "done":
                        zip_url = task.get("full_zip_url")
                        zip_response = requests.get(zip_url, timeout=180)
                        zip_response.raise_for_status()
                        
                        with ZipFile(BytesIO(zip_response.content)) as z:
                            md_name_in_zip = next((item for item in z.namelist() if item.lower().endswith('.md')), None)
                            if md_name_in_zip:
                                md_content = z.read(md_name_in_zip).decode('utf-8')
                                md_path = output_folder / f"{pdf_path.stem}.md"
                                with open(md_path, 'w', encoding='utf-8') as f_out:
                                    f_out.write(md_content)
                                return md_path, None
                            else:
                                return None, "ç»“æœå‹ç¼©åŒ…ä¸­æœªæ‰¾åˆ°.mdæ–‡ä»¶ã€‚"
                    elif task.get("state") == "failed":
                        return None, f"APIå¤„ç†å¤±è´¥: {task.get('error_msg', 'æœªçŸ¥é”™è¯¯')}"
            time.sleep(POLLING_INTERVAL)
        return None, "è½®è¯¢ç»“æœè¶…æ—¶ã€‚"
    except Exception as e:
        return None, f"PDFåˆ°MDè½¬æ¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}"

# --- æ­¥éª¤ 3: æ¸…ç†Markdownæ–‡ä»¶ (å•æ–‡ä»¶ç‰ˆæœ¬) ---
def process_single_md_file(md_path, pdf_to_delete_path):
    """æ¸…ç†å•ä¸ªMDæ–‡ä»¶å¹¶åˆ é™¤å¯¹åº”çš„PDFã€‚"""
    try:
        with open(md_path, 'r', encoding='utf-8') as f:
            paper_content = f.read()

        sections_to_remove = ["Acknowledgement", "Acknowledgements", "Reference", "References", "Acknowledgment", "Acknowledgments",
                              "ACKNOWLEDGEMENT", "ACKNOWLEDGEMENTS", "REFERENCE", "REFERENCES", "ACKNOWLEDGMENT", "ACKNOWLEDGMENTS"]
        
        cut_off_pos = len(paper_content)
        for title in sections_to_remove:
            pattern_str = f"^#{{1,3}}\\s*(\\d+\\.?\\s+)?{re.escape(title)}\\b"
            match = re.search(pattern_str, paper_content, re.IGNORECASE | re.MULTILINE)
            if match and match.start() < cut_off_pos:
                cut_off_pos = match.start()
        
        cleaned_paper = paper_content[:cut_off_pos].strip()

        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(cleaned_paper)
        
        if pdf_to_delete_path.exists():
            os.remove(pdf_to_delete_path)
        
        return md_path, None
    except Exception as e:
        return None, f"æ¸…ç†Markdownæ–‡ä»¶æ—¶å‡ºé”™: {e}"

# --- æ­¥éª¤ 4: æŠ“å–å…ƒæ•°æ®å¹¶ç”Ÿæˆæ€»ç»“ (å•æ–‡ä»¶ç‰ˆæœ¬) ---
def create_paper_json(arxiv_id, md_path, db_save_path):
    """æŠ“å–å…ƒæ•°æ®ï¼Œæ•´åˆMDå†…å®¹å’Œæ€»ç»“ï¼Œç”Ÿæˆæœ€ç»ˆçš„JSONæ–‡ä»¶ã€‚"""
    try:
        arxiv_url = f"https://arxiv.org/abs/{arxiv_id}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        api_url = f'https://api.semanticscholar.org/graph/v1/paper/arXiv:{arxiv_id}'
        api_params = {'fields': 'citationCount,influentialCitationCount,authors.name,authors.affiliations,venue,publicationVenue,publicationDate,fieldsOfStudy,title'}
        
        api_response = requests.get(api_url, headers=headers, params=api_params, timeout=15)
        api_response.raise_for_status()
        api_data = api_response.json()
        
        authors_list = [author.get('name', 'æœªçŸ¥ä½œè€…') for author in api_data.get('authors', [])]
        institutions = sorted(list(set(aff for author in api_data.get('authors', []) for aff in author.get('affiliations', []) if aff and aff.strip())))

        paper_info = {
            'link': arxiv_url,
            'pdf_link': arxiv_url.replace('/abs/', '/pdf/'),
            'title': api_data.get('title', 'æœªçŸ¥æ ‡é¢˜'),
            'authors': authors_list,
            'institutions': institutions or ["æœªæ‰¾åˆ°æœºæ„ä¿¡æ¯"],
            'publication_date': api_data.get('publicationDate'),
            'venue': api_data.get('venue') or (api_data.get('publicationVenue') or {}).get('name') or "æœªæ‰¾åˆ°",
            'fields_of_study': api_data.get('fieldsOfStudy', []),
            'citation_count': api_data.get('citationCount', 0),
            'influential_citation_count': api_data.get('influentialCitationCount', 0),
        }

        with open(md_path, 'r', encoding='utf-8') as f:
            paper_info['paper_content'] = f.read()

        # è°ƒç”¨æˆ‘ä»¬æ¨¡æ‹Ÿçš„æ€»ç»“å‡½æ•°
        paper_info['summary'] = summary_func(arxiv_id) 

        # æ ¹æ®ä½ çš„æ–°è¦æ±‚ï¼Œæ•°æ®åº“è·¯å¾„ç°åœ¨åŒ…å«æ—¥æœŸå’Œåˆ†ç±»
        # è¿™é‡Œæˆ‘ä»¬å‡è®¾å®ƒæ¥è‡ªä¸€ä¸ªé€šç”¨å¤„ç†æµç¨‹ï¼Œå…ˆä¸åŠ åˆ†ç±»
        db_save_path.parent.mkdir(parents=True, exist_ok=True)
        with open(db_save_path, 'w', encoding='utf-8') as f:
            json.dump(paper_info, f, ensure_ascii=False, indent=4)
            
        return db_save_path, paper_info, None

    except Exception as e:
        return None, None, f"åˆ›å»ºJSONæ–‡ä»¶æ—¶å‡ºé”™: {e}"

# =================================================================
# 3. Streamlit UI ç•Œé¢å’Œé€»è¾‘
# =================================================================

# --- è¾…åŠ©UIå‡½æ•° ---
def display_paper_summary(paper_data, container=st):
    """
    åœ¨æŒ‡å®šå®¹å™¨ï¼ˆå¦‚st.expanderï¼‰ä¸­ä»¥ç¾åŒ–æ ¼å¼å±•ç¤ºå•ç¯‡è®ºæ–‡çš„æ‘˜è¦å’Œå…ƒæ•°æ®ã€‚
    - åŒ…å«ä¸€ä¸ªé†’ç›®çš„æ ‡é¢˜ã€‚
    - ä¿®å¤äº†å­—æ®µé”™è¯¯ï¼Œå¹¶ç¾åŒ–äº†é“¾æ¥æ˜¾ç¤ºã€‚
    - ã€æ–°åŠŸèƒ½ã€‘è‡ªåŠ¨ä¿®å¤å¹¶æ¸²æŸ“LaTeXå…¬å¼ã€‚
    - ã€æ–°åŠŸèƒ½ã€‘ç¾åŒ–æ‘˜è¦çš„å±•ç¤ºç»“æ„ã€‚
    """
    if not paper_data or not paper_data.get('title'):
        return

    # --- å…ƒæ•°æ®å±•ç¤ºéƒ¨åˆ† ---
    container.markdown(f"### ğŸ“„ {paper_data.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
    
    arxiv_id = paper_data.get('link', 'N/A').split('/')[-1]

    col1, col2 = container.columns([2, 1]) 

    with col1:
        container.markdown(f"**âœï¸ ä½œè€…:** {', '.join(paper_data.get('authors', ['æœªæ‰¾åˆ°ä½œè€…ä¿¡æ¯']))}")
        institutions = paper_data.get('institutions', ["æœªæ‰¾åˆ°æœºæ„ä¿¡æ¯"])
        container.markdown(f"**ğŸ¢ æœºæ„:** {', '.join(institutions)}")
        container.markdown(f"**ğŸ—“ï¸ å‘è¡¨æ—¥æœŸ:** {paper_data.get('publication_date', 'æœªæ‰¾åˆ°å‘è¡¨æ—¥æœŸ')}")
    
    with col2:
        container.markdown(f"**ğŸ†” ArXiv ID:** `{arxiv_id}`")
        container.markdown(f"**ğŸ“ˆ å¼•ç”¨æ•°:** {paper_data.get('citation_count', 'æœªæ‰¾åˆ°å¼•ç”¨é‡')} | **é«˜å½±å“åŠ›å¼•ç”¨:** {paper_data.get('influential_citation_count', 'æœªæ‰¾åˆ°é«˜å½±å“åŠ›å¼•ç”¨é‡')}")
        
        arxiv_link = paper_data.get('link')
        pdf_link = paper_data.get('pdf_link')
        
        if arxiv_link:
            container.markdown(f"**ğŸ”— ArXivé¡µé¢:** [ç‚¹å‡»è·³è½¬]({arxiv_link})")
        if pdf_link:
            container.markdown(f"**ğŸ“¥ PDFä¸‹è½½:** [ç›´æ¥ä¸‹è½½]({pdf_link})")
            
    container.divider()
    container.subheader("è®ºæ–‡æ€»ç»“")

    summary_content = paper_data.get('summary')
    if summary_content:
        try:
            # æ­¥éª¤ 1: å°è¯•è§£æJSONå­—ç¬¦ä¸²
            summary_content_fixed_escape = summary_content.replace("\\", "\\\\") 
            summary_dict = json.loads(summary_content_fixed_escape)
            
            if isinstance(summary_dict, dict):
                # æ­¥éª¤ 2: éå†æ‘˜è¦çš„æ¯ä¸ªéƒ¨åˆ†ï¼Œå¹¶è¿›è¡Œç¾åŒ–æ¸²æŸ“
                for key, value in summary_dict.items():

                    # æ­¥éª¤ 3: æ™ºèƒ½ä¿®å¤å¹¶æ¸²æŸ“å…¬å¼
                    # æ­£åˆ™è¡¨è¾¾å¼ï¼ŒæŸ¥æ‰¾è¢«åœ†æ‹¬å·åŒ…è£¹ã€ä¸”å†…éƒ¨å«æœ‰'\'çš„LaTeXå…¬å¼
                    # è¿™å¯ä»¥é¿å…é”™è¯¯åœ°å°†æ™®é€šæ‹¬å·å†…å®¹ (like this) è½¬æ¢ä¸ºå…¬å¼
                    latex_pattern = r'\(([^)]*\\.*?)\)'
                    # å°†æ‰¾åˆ°çš„ (...) æ ¼å¼æ›¿æ¢ä¸º $...$ æ ¼å¼
                    # r'$\1$' ä¸­çš„ \1 ä»£è¡¨æ­£åˆ™è¡¨è¾¾å¼ä¸­ç¬¬ä¸€ä¸ªæ‹¬å· (...) æ•è·çš„å†…å®¹
                    value_with_fixed_latex = re.sub(latex_pattern, r'$\1$', value)

                    # æ­¥éª¤ 4: å¤„ç†æ¢è¡Œç¬¦ï¼Œä½¿å…¶åœ¨Markdownä¸­ç”Ÿæ•ˆ
                    # å°† '\n' æ›¿æ¢ä¸º '  \n' (ä¸¤ä¸ªç©ºæ ¼+æ¢è¡Œç¬¦) æ¥åˆ›å»ºç¡¬æ¢è¡Œ
                    final_value = value_with_fixed_latex.replace("\\n", "  \n")
                    
                    # ä½¿ç”¨å¸¦è¾¹æ¡†çš„å®¹å™¨æ¥å±•ç¤ºå†…å®¹ï¼Œå¢åŠ ç¾æ„Ÿ
                    with container.container(border=True):
                        st.markdown(final_value, unsafe_allow_html=True)

            else:
                # å¦‚æœè§£æå‡ºæ¥ä¸æ˜¯å­—å…¸ï¼Œåˆ™æŒ‰åŸæ ·æ˜¾ç¤º
                container.write(summary_content)
        
        except (json.JSONDecodeError, TypeError):
            # å¦‚æœsummaryä¸æ˜¯æœ‰æ•ˆçš„JSONå­—ç¬¦ä¸²ï¼Œåˆ™ç›´æ¥å°†å…¶ä½œä¸ºæ™®é€šæ–‡æœ¬æ˜¾ç¤º
            # åŒæ ·åœ¨è¿™é‡Œå°è¯•ä¿®å¤å…¬å¼
            latex_pattern = r'\(([^)]*\\.*?)\)'
            fixed_content = re.sub(latex_pattern, r'$\1$', summary_content)
            container.markdown(fixed_content)
            
    else:
        container.info("æ­¤è®ºæ–‡æš‚æ— æ€»ç»“ä¿¡æ¯ã€‚")

def display_paper_card(paper_data):
    """
    ç”¨ st.expander å±•ç¤ºä¸€ç¯‡è®ºæ–‡çš„å¡ç‰‡ï¼ŒåŒ…å«ä¸€ä¸ªæŒ‰é’®ç”¨äºè·³è½¬åˆ°è¯¦æƒ…é¡µã€‚
    """
    if not paper_data or not paper_data.get('title'):
        return

    expander_title = paper_data.get('title', 'æœªçŸ¥æ ‡é¢˜')
    
    with st.expander(f"**{expander_title}**", expanded=False):
        # ç‚¹å‡»åï¼Œè®¾ç½® session_state ä»¥åˆ‡æ¢åˆ°è¯¦æƒ…é¡µè§†å›¾
        if st.button("æ·±å…¥é˜…è¯»ä¸å¯¹è¯", key=f"detail_{paper_data['link']}"):
            st.session_state.current_view = 'detail_view'
            st.session_state.active_paper_data = paper_data
            # æ¸…ç©ºä¸Šä¸€ç¯‡è®ºæ–‡çš„èŠå¤©è®°å½•
            if 'chat_messages' in st.session_state:
                st.session_state.chat_messages.clear()
            st.rerun()
        
        display_paper_summary(paper_data, container=st)


# --- é¡µé¢æ¸²æŸ“å‡½æ•° ---

def render_main_page():
    """æ¸²æŸ“ä¸»æ¬¢è¿é¡µé¢"""
    st.title("ğŸš€ æ¬¢è¿ä½¿ç”¨ PaperSAGE")
    st.markdown("è¯·ä»å·¦ä¾§ä¾§è¾¹æ é€‰æ‹©ä¸€é¡¹åŠŸèƒ½å¼€å§‹ã€‚")
    st.image("https://images.unsplash.com/photo-1581091226825-a6a2a5aee158?q=80&w=2070", caption="æ¢ç´¢ç§‘å­¦çš„å‰æ²¿")


def render_smart_summary_page():
    """æ¸²æŸ“â€œè®ºæ–‡æ€»ç»“â€é¡µé¢"""
    st.header("âœï¸ è®ºæ–‡æ€»ç»“ç”Ÿæˆ")
    st.markdown("è¾“å…¥ä¸€ç¯‡ArXivè®ºæ–‡çš„é“¾æ¥ï¼Œæˆ‘ä»¬å°†ä¸ºæ‚¨ä¸‹è½½ã€è§£æå¹¶ç”Ÿæˆä¸€ä»½è®ºæ–‡æ€»ç»“ã€‚")
    
    arxiv_url = st.text_input("è¯·è¾“å…¥ArXivè®ºæ–‡é“¾æ¥", key="arxiv_url_input")

    if st.button("å¼€å§‹å¤„ç†", type="primary"):
        if not arxiv_url:
            st.error("è¯·è¾“å…¥æœ‰æ•ˆçš„ArXivé“¾æ¥ã€‚")
        elif 'è¯·' in API_TOKEN or len(API_TOKEN) < 20:
            st.error("é”™è¯¯ï¼šè¯·å…ˆåœ¨è„šæœ¬ä¸­é…ç½®æ‚¨çš„æœ‰æ•ˆ API_TOKENã€‚")
        else:
            today_str = datetime.now().strftime("%Y%m%d")
            origin_dir = BASE_PATH / "origin_papers" / today_str
            transferred_dir = BASE_PATH / "transferred_papers" / today_str
            db_dir = DB_ROOT_PATH / today_str  # æ³¨æ„è·¯å¾„å˜åŒ–
            
            origin_dir.mkdir(parents=True, exist_ok=True)
            transferred_dir.mkdir(parents=True, exist_ok=True)
            
            arxiv_id = arxiv_url.split('/')[-1]
            final_json_path = db_dir / f"{arxiv_id}.json"

            with st.status("æ­£åœ¨å¤„ç†è®ºæ–‡...", expanded=True) as status:
                st.write("1/5: æ­£åœ¨ä¸‹è½½ PDF...")
                pdf_path, error = download_arxiv_pdf(arxiv_url, origin_dir)
                if error:
                    status.update(label=f"é”™è¯¯: {error}", state="error"); return

                st.write("2/5: æ­£åœ¨è½¬æ¢ PDF ä¸º Markdown...")
                md_path, error = convert_single_pdf_to_md(pdf_path, transferred_dir)
                if error:
                    status.update(label=f"é”™è¯¯: {error}", state="error"); return

                st.write("3/5: æ­£åœ¨æ¸…ç† Markdown å†…å®¹...")
                cleaned_md_path, error = process_single_md_file(md_path, pdf_path)
                if error:
                    status.update(label=f"é”™è¯¯: {error}", state="error"); return

                st.write("4/5: æ­£åœ¨æŠ“å–å…ƒæ•°æ®å¹¶ç”Ÿæˆæ€»ç»“...")
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å‡è®¾å¤„ç†çš„è®ºæ–‡æ²¡æœ‰é¢„å…ˆåˆ†ç±»ï¼Œæ‰€ä»¥ä¿å­˜åœ¨æ—¥æœŸç›®å½•ä¸‹
                json_path, final_data, error = create_paper_json(arxiv_id, cleaned_md_path, final_json_path)
                if error:
                    status.update(label=f"é”™è¯¯: {error}", state="error"); return

                st.write(f"5/5: æœ€ç»ˆ JSON æ–‡ä»¶å·²ä¿å­˜åˆ°: {json_path}")
                status.update(label="ğŸ‰ å¤„ç†å®Œæˆï¼", state="complete")

            st.subheader("å¤„ç†ç»“æœé¢„è§ˆ")
            if final_data:
                display_paper_card(final_data)


def render_daily_recommendation_page():
    """æ¸²æŸ“â€œæ¯æ—¥æ¨èâ€é¡µé¢"""
    st.header("ğŸ“… æ¯æ—¥æ¨è")
    st.markdown("é€‰æ‹©æ‚¨æ„Ÿå…´è¶£çš„é¢†åŸŸï¼Œæˆ‘ä»¬å°†ä¸ºæ‚¨æ¨èä»Šå¤©è¯¥é¢†åŸŸçš„æœ€æ–°è®ºæ–‡ã€‚")
    
    # å‡è®¾ä½ çš„æ•°æ®åº“ç»“æ„æ˜¯ /database/YYYYMMDD/cs.AI/*.json
    today_str = datetime.now().strftime("%Y%m%d")
    # ä¸ºäº†æ¼”ç¤ºï¼Œå¦‚æœä»Šå¤©æ²¡æœ‰ï¼Œå°±ç”¨å‰ä¸€å¤©çš„
    date_to_check = DB_ROOT_PATH / today_str
    if not date_to_check.exists():
        yesterday_str = (datetime.now() - timedelta(days=1)).strftime("%Y%m%d")
        date_to_check = DB_ROOT_PATH / yesterday_str
        if date_to_check.exists():
            st.info(f"æœªæ‰¾åˆ°ä»Šæ—¥({today_str})æ•°æ®ï¼Œæ˜¾ç¤ºæ˜¨æ—¥({yesterday_str})æ¨èã€‚")
        else:
            st.warning(f"æ•°æ®åº“ä¸­æœªæ‰¾åˆ°è¿‘æœŸè®ºæ–‡ ({today_str} æˆ– {yesterday_str})ã€‚è¯·å…ˆå¤„ç†ä¸€äº›è®ºæ–‡ã€‚")
            return

    # è®©ç”¨æˆ·é€‰æ‹©åˆ†ç±»
    top_category = st.selectbox("é€‰æ‹©ä¸€ä¸ªå¤§å­¦ç§‘", list(ARXIV_CATEGORIES.keys()))
    sub_category = st.selectbox("é€‰æ‹©ä¸€ä¸ªå­é¢†åŸŸ", ARXIV_CATEGORIES[top_category])

    category_path = date_to_check / sub_category
    if not category_path.exists() or not any(category_path.iterdir()):
        st.info(f"åœ¨ {date_to_check.name}/{sub_category} ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡ã€‚")
        return

    # ä»åˆ†ç±»æ–‡ä»¶å¤¹åŠ è½½è®ºæ–‡
    if 'daily_papers' not in st.session_state or st.session_state.get('current_category') != sub_category:
        st.session_state.all_daily_papers = list(category_path.glob("*.json"))
        st.session_state.current_category = sub_category
        st.session_state.daily_papers = random.sample(
            st.session_state.all_daily_papers, 
            min(10, len(st.session_state.all_daily_papers))
        )
    
    if st.button("ğŸ”„ æ¢ä¸€æ‰¹"):
        st.session_state.daily_papers = random.sample(
            st.session_state.all_daily_papers,
            min(10, len(st.session_state.all_daily_papers))
        )
        st.rerun()

    st.markdown(f"--- \n ### ä¸ºæ‚¨ä» **{sub_category}** é¢†åŸŸæ¨èäº† **{len(st.session_state.daily_papers)}** ç¯‡è®ºæ–‡")

    selected_papers = {}
    for paper_path in st.session_state.daily_papers:
        try:
            with open(paper_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                is_selected = st.checkbox(f"**{data.get('title', 'æ— æ ‡é¢˜')}**", key=f"select_{paper_path.name}")
                if is_selected:
                    selected_papers[paper_path.name] = data
        except Exception as e:
            st.error(f"åŠ è½½æ–‡ä»¶ {paper_path.name} å‡ºé”™: {e}")
    
    if st.button("æŸ¥çœ‹é€‰ä¸­è®ºæ–‡çš„æ€»ç»“", type="primary"):
        if not selected_papers:
            st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ç¯‡è®ºæ–‡ã€‚")
        else:
            st.subheader("æ‰€é€‰è®ºæ–‡è¯¦æƒ…")
            for name, data in selected_papers.items():
                display_paper_card(data)


def render_guess_you_like_page():
    """æ¸²æŸ“â€œçŒœä½ å–œæ¬¢â€é¡µé¢ (ç›®å‰ä¸ºéšæœºæ¨è)"""
    st.header("â¤ï¸ çŒœä½ å–œæ¬¢")
    st.markdown("æ ¹æ®æˆ‘ä»¬çš„æ•°æ®åº“ï¼Œä¸ºæ‚¨éšæœºæ¨èä¸€äº›å¯èƒ½æ„Ÿå…´è¶£çš„è®ºæ–‡ã€‚")
    
    # æœç´¢æ¡† (åŠŸèƒ½å¾…å®ç°)
    query = st.text_input("è¾“å…¥å…³é”®è¯è¿›è¡Œæœç´¢ï¼ˆåŠŸèƒ½å¼€å‘ä¸­...ï¼‰")

    if not DB_ROOT_PATH.exists() or not any(DB_ROOT_PATH.glob("**/*.json")):
        st.warning(f"æ•°æ®åº“ '{DB_ROOT_PATH}' ä¸ºç©ºã€‚è¯·å…ˆå¤„ç†ä¸€äº›è®ºæ–‡ã€‚")
        return

    if 'all_db_papers' not in st.session_state:
        st.session_state.all_db_papers = list(DB_ROOT_PATH.glob("**/*.json"))
    
    if 'recommended_papers' not in st.session_state:
        st.session_state.recommended_papers = random.sample(
            st.session_state.all_db_papers, 
            min(5, len(st.session_state.all_db_papers))
        )

    col1, col2 = st.columns([3, 1])
    with col1:
        if st.button("ğŸ” æœç´¢/æ¢ä¸€æ‰¹", type="primary"):
            if query:
                st.info(f"æœç´¢åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­ï¼Œå°†ä¸ºæ‚¨éšæœºæ¨èã€‚æ‚¨æœç´¢äº†ï¼š'{query}'")
            st.session_state.recommended_papers = random.sample(
                st.session_state.all_db_papers,
                min(5, len(st.session_state.all_db_papers))
            )
            st.rerun()

    if st.session_state.recommended_papers:
        for paper_path in st.session_state.recommended_papers:
            try:
                with open(paper_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    display_paper_card(data)
            except Exception as e:
                st.error(f"åŠ è½½æˆ–æ˜¾ç¤ºæ–‡ä»¶ {paper_path.name} å‡ºé”™: {e}")


def render_detail_view():
    """æ¸²æŸ“è®ºæ–‡è¯¦æƒ…å¯¹è¯é¡µé¢"""
    if 'active_paper_data' not in st.session_state:
        st.error("æ— æ³•åŠ è½½è®ºæ–‡æ•°æ®ï¼Œè¯·è¿”å›ä¸»é¡µé‡è¯•ã€‚")
        if st.button("è¿”å›"):
            st.session_state.current_view = "main_page"
            st.rerun()
        return

    paper_data = st.session_state.active_paper_data
    st.title(paper_data.get('title', 'è®ºæ–‡è¯¦æƒ…'))
    
    if st.button("â¬…ï¸ è¿”å›åˆ—è¡¨"):
        # æ ¹æ®ç”¨æˆ·ä»å“ªé‡Œè¿›å…¥è¯¦æƒ…é¡µï¼Œå†³å®šè¿”å›åˆ°å“ªä¸ªé¡µé¢
        # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ç»Ÿä¸€è¿”å›åˆ°â€œçŒœä½ å–œæ¬¢â€
        st.session_state.current_view = 'guess_you_like' 
        del st.session_state['active_paper_data']
        if 'chat_messages' in st.session_state:
            del st.session_state['chat_messages']
        st.rerun()

    st.markdown(f"é˜…è¯»åŸæ–‡ä¸AIå¯¹è¯ï¼š**[{paper_data.get('title')}]({paper_data.get('link')})**")
    st.divider()

    col1, col2 = st.columns([1, 1]) # å·¦å³ç­‰å®½

    # å·¦ä¾§ï¼šMarkdownåŸæ–‡å’Œå¯ç‚¹å‡»çš„æ ‡é¢˜
    with col1:
        st.header("ğŸ“„ è®ºæ–‡åŸæ–‡")
        content = paper_data.get('paper_content', 'æ— åŸæ–‡å†…å®¹ã€‚')
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ‰€æœ‰Markdownæ ‡é¢˜
        header_pattern = re.compile(r"(^#+\s+.*)", re.MULTILINE)
        parts = header_pattern.split(content)
        
        # æ˜¾ç¤ºéæ ‡é¢˜éƒ¨åˆ†
        st.markdown(parts[0], unsafe_allow_html=True)
        
        # æ˜¾ç¤ºæ ‡é¢˜å’Œå…¶åçš„å†…å®¹
        for i in range(1, len(parts), 2):
            header = parts[i]
            section_content = parts[i+1]
            
            # åˆ›å»ºä¸€ä¸ªå¯ç‚¹å‡»çš„æ ‡é¢˜æŒ‰é’®
            if st.button(header, use_container_width=True):
                # ç‚¹å‡»åï¼Œè°ƒç”¨ç¿»è¯‘å‡½æ•°å¹¶å°†ç»“æœæ·»åŠ åˆ°èŠå¤©è®°å½•
                translated_text = translate_text(header + '\n' + section_content.split('\n\n')[0]) # ä»…ç¿»è¯‘æ ‡é¢˜å’Œç¬¬ä¸€æ®µ
                if 'chat_messages' not in st.session_state:
                    st.session_state.chat_messages = []
                st.session_state.chat_messages.append({"role": "user", "content": f"è¯·ç¿»è¯‘è¿™ä¸ªéƒ¨åˆ†: {header}"})
                st.session_state.chat_messages.append({"role": "assistant", "content": translated_text})

            st.markdown(section_content, unsafe_allow_html=True)

    # å³ä¾§ï¼šå¯¹è¯æ¡†
    with col2:
        st.header("ğŸ’¬ ä¸è®ºæ–‡å¯¹è¯")
        
        # åˆå§‹åŒ–èŠå¤©è®°å½•
        if "chat_messages" not in st.session_state:
            st.session_state.chat_messages = []
        
        # æ˜¾ç¤ºå†å²æ¶ˆæ¯
        for message in st.session_state.chat_messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        
        # æ¥æ”¶ç”¨æˆ·è¾“å…¥
        if prompt := st.chat_input("å°±è¿™ç¯‡è®ºæ–‡è¿›è¡Œæé—®..."):
            # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°å†å²è®°å½•å¹¶æ˜¾ç¤º
            st.session_state.chat_messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            
            # ç”Ÿæˆå¹¶æ˜¾ç¤ºAIçš„å›ç­”
            with st.chat_message("assistant"):
                response = rag_query(prompt, content) # è°ƒç”¨RAGå‡½æ•°
                st.markdown(response)
            
            # å°†AIå›ç­”æ·»åŠ åˆ°å†å²è®°å½•
            st.session_state.chat_messages.append({"role": "assistant", "content": response})

# =================================================================
# 4. ä¸»åº”ç”¨å…¥å£
# =================================================================

# --- åˆå§‹åŒ– Session State ---
if 'current_view' not in st.session_state:
    st.session_state.current_view = 'main_page' # é»˜è®¤æ˜¾ç¤ºä¸»é¡µ

# --- ä¾§è¾¹æ å¯¼èˆª ---
with st.sidebar:
    st.title("ğŸ“š PaperSAGE å¯¼èˆª")
    
    # ä½¿ç”¨ on_change å›è°ƒæ¥æ¸…é™¤ç‰¹å®šçŠ¶æ€ï¼Œé˜²æ­¢é¡µé¢åˆ‡æ¢æ—¶æ•°æ®æ··ä¹±
    def change_page():
        # å½“æˆ‘ä»¬ç¦»å¼€è¯¦æƒ…é¡µæ—¶ï¼Œæ¸…é™¤æ´»åŠ¨è®ºæ–‡æ•°æ®
        if st.session_state.current_view == 'detail_view':
             if 'active_paper_data' in st.session_state:
                del st.session_state['active_paper_data']
             if 'chat_messages' in st.session_state:
                del st.session_state['chat_messages']
        # å°† radio çš„é€‰æ‹©åŒæ­¥åˆ° current_view
        st.session_state.current_view = st.session_state.page_selection

    page = st.radio(
        "é€‰æ‹©åŠŸèƒ½",
        options=['main_page', 'smart_summary', 'daily_recommendation', 'guess_you_like'],
        format_func=lambda x: {
            'main_page': 'ğŸ  ä¸»é¡µ',
            'smart_summary': 'âœï¸ è®ºæ–‡æ€»ç»“',
            'daily_recommendation': 'ğŸ“… æ¯æ—¥æ¨è',
            'guess_you_like': 'â¤ï¸ çŒœä½ å–œæ¬¢'
        }.get(x, x),
        key='page_selection',
        on_change=change_page
    )

    st.sidebar.divider()
    st.sidebar.info("PaperSAGE - æ‚¨çš„æ™ºèƒ½è®ºæ–‡é˜…è¯»åŠ©æ‰‹ã€‚")


# --- é¡µé¢è°ƒåº¦å™¨ ---
if st.session_state.current_view == 'main_page':
    render_main_page()
elif st.session_state.current_view == 'smart_summary':
    render_smart_summary_page()
elif st.session_state.current_view == 'daily_recommendation':
    render_daily_recommendation_page()
elif st.session_state.current_view == 'guess_you_like':
    render_guess_you_like_page()
elif st.session_state.current_view == 'detail_view':
    render_detail_view()
else:
    render_main_page() # é»˜è®¤æˆ–å‡ºé”™æ—¶è¿”å›ä¸»é¡µ
