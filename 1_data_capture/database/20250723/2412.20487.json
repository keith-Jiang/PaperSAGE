{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.20487",
    "link": "https://arxiv.org/abs/2412.20487",
    "pdf_link": "https://arxiv.org/pdf/2412.20487.pdf",
    "title": "Multimodal Variational Autoencoder: a Barycentric View",
    "authors": [
        "Peijie Qiu",
        "Wenhui Zhu",
        "Sayantan Kumar",
        "Xiwen Chen",
        "Xiaotong Sun",
        "Jin Yang",
        "Abolfazl Razi",
        "Yalin Wang",
        "Aristeidis Sotiras"
    ],
    "categories": [
        "cs.LG",
        "cs.CV",
        "cs.IT"
    ],
    "publication_date": "2024-12-29",
    "venue": "arXiv.org",
    "fields_of_study": [
        "Computer Science",
        "Mathematics"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Washington University in St. Louis",
        "Arizona State University",
        "Clemson University",
        "University of Arkansas"
    ],
    "paper_content": "# Multimodal Variational Autoencoder: A Barycentric View\n\nPeijie $\\mathbf { Q } \\mathbf { i } \\mathbf { u } ^ { 1 }$ , Wenhui $\\mathbf { Z } \\mathbf { h } \\mathbf { u } ^ { 2 }$ , Sayantan Kumar1, Xiwen Chen3, Jin Yang1, Xiaotong $\\mathbf { S u n ^ { 4 } }$ , Abolfazl Razi3, Yalin Wang2, Aristeidis Sotiras1\n\n1 Washington University in St. Louis 2 Arizona State University 3 Clemson University 4 University of Arkansas peijie.qiu, sayantan.kumar, yang.jin, aristeidis.sotiras $@$ wustl.edu wzhu59, ylwang @asu.edu, xiwenc, arazi @clemson.edu, $\\mathbf { \\boldsymbol { x } } \\mathbf { \\boldsymbol { s } } 0 1 8 @$ uark.edu\n\n# Abstract\n\nMultiple signal modalities, such as vision and sounds, are naturally present in real-world phenomena. Recently, there has been growing interest in learning generative models, in particular variational autoencoder (VAE), for multimodal representation learning especially in the case of missing modalities. The primary goal of these models is to learn a modalityinvariant and modality-specific representation that characterizes information across multiple modalities. Previous attempts at multimodal VAEs approach this mainly through the lens of experts, aggregating unimodal inference distributions with a product of experts (PoE), a mixture of experts (MoE), or a combination of both. In this paper, we provide an alternative generic and theoretical formulation of multimodal VAE through the lens of barycenter. We first show that PoE and MoE are specific instances of barycenters, derived by minimizing the asymmetric weighted KL divergence to unimodal inference distributions. Our novel formulation extends these two barycenters to a more flexible choice by considering different types of divergences. In particular, we explore the Wasserstein barycenter defined by the 2-Wasserstein distance, which better preserves the geometry of unimodal distributions by capturing both modality-specific and modalityinvariant representations compared to KL divergence. Empirical studies on three multimodal benchmarks demonstrated the effectiveness of the proposed method.\n\n# Introduction\n\nMultiple data types are naturally present together to characterize the same underlying phenomena in the real world. Multimodal representation learning is thus of interest across various fields, including computer vision, natural language processing, and the biomedical domain. However, understanding and interrelating different modalities is a challenging task due to the laboriousness of human annotations and the absence of certain modalities in practice. These two factors pose a significant challenge to the application of unimodal and discriminative (supervised) representation learning methods to the multimodal case (see e.g., Karpathy and Fei-Fei 2015; Pham et al. 2019; Lin et al. 2023).\n\nTherefore, we focus on the generative models for representation learning, which are typically considered as unsupervised, such as generative adversarial networks (GANs; Goodfellow et al. 2014) and variational autoencoders (VAEs; Kingma and Welling 2013). In particular, we focus on VAEs for multimodal representation learning since VAEs are graphical probabilistic models capable of learning an explicit latent distribution, which has the potential to directly learn the joint distributions of multiple modalities (Suzuki, Nakayama, and Matsuo 2016; Baltrusˇaitis, Ahuja, and Morency 2018). Despite their nice probabilistic properties and the success in unimodal applications, the direct translation of VAEs to the multimodal case (e.g., feeding the multimodal data to VAEs) is challenging, as they struggle with handling missing modalities and performing cross-modal generations. Therefore, the design of multimodal VAEs seeks to form a modality-invariant and modality-specific latent representation by learning a joint latent distribution (so-called joint posterior) to aggregate the information from different modalities (Ngiam et al. 2011; Suzuki, Nakayama, and Matsuo 2016; Baltrusˇaitis, Ahuja, and Morency 2018). The modality-specific and modalityinvariant formulation naturally enables a cross-modal generation (Shi et al. 2019). In addition, it can also handle missing modalities by directly sampling the learned joint posterior.\n\nThe core objective of multimodal VAEs then revolves around how to approximate the joint posterior by aggregating the unimodal posterior, also known as unimodal inference distribution in VAEs. This typically involves finding a proper aggregation function. However, such aggregation functions are challenging to identify due to the intractability of the true joint posterior. Previous explorations of multimodal VAEs addressed this challenge mainly through the lens of experts in statistics by aggregating unimodal inference distributions with a product of experts (PoE; Wu and Goodman 2018), a mixture of experts (MoE; Shi et al. 2019), or a combination of both (MoPoE; Sutter, Daunhawer, and Vogt 2021). Although empirical studies have shown their success for multimodal VAEs, theoretical analysis of their properties is still insufficient.\n\nIn this paper, we provide a theoretical view of previous multimodal VAEs in a unified way through the lens of barycenter. The barycentric distribution is the mean distribution of a set of distributions, defined by minimizing the weighted sum of divergences to these distributions. Interestingly, we discovered that the distributions aggregated by PoE and MoE are barycenters by optimizing the reverse and forward Kullback-Leibler (KL) divergence, respectively. This directly provides an information-theoretic view of PoE and MoE, which reveals their intrinsic properties: PoE is zero-forcing (i.e., pushing the joint posterior biased towards certain modalities), while MoE is masscovering (i.e., balancing all modalities). However, the KL divergence does not define a metric space for probability measures, as it is asymmetric and unbounded. This motivates us to explore other divergence measures that are defined in metric space. In particular, we explored the Wasserstein barycenter (Agueh and Carlier 2011) by optimizing the squared 2-Wasserstein distance, as it preserves the geometry of unimodal inference distributions in a geodesic space (whereas KL divergence focuses on pointwise differences). Leveraging the intricate geometry of the Wasserstein distance (Peyre´, Cuturi et al. 2019), the Wasserstein barycenter serves as the Fre´chet means (see e.g., Grove and Karcher 1973) within the space of probability measures.\n\nIn summary, our contributions are threefold: i) We introduce a novel and unified formulation for multimodal VAEs, where the aggregation of unimodal inference distributions is framed as solving the barycenter problem that minimizes certain divergence measures. This approach offers a theoretical framework to analyze intrinsic properties and enables a more flexible selection of aggregation functions for multimodal VAEs. ii) We propose $w B$ -VAE, a novel multimodal VAE for representation learning that leverages the Wasserstein barycenter to aggregate unimodal inference distributions. iii) Experiments on three benchmark datasets demonstrated the effectiveness of the proposed method compared to other state-of-the-art methods.\n\n# Background and Related Work\n\n# Multimodal VAEs\n\nPrior multimodal VAEs can be roughly divided into two main categories: coordinated models and joint models. The former only learns the inference distributions from a single modality, while the latter learns the joint inference distributions across all modalities (Baltrusˇaitis, Ahuja, and Morency 2018; Suzuki and Matsuo 2022). Accordingly, coordinated models (Higgins et al. 2017; Schonfeld et al. 2019; Korthals et al. 2019) strive to generate consistent inference results across all modalities. Although they can perform crossmodal generation, they may not effectively handle missing modalities as in joint models (Wu and Goodman 2018; Shi et al. 2019; Sutter, Daunhawer, and Vogt 2020). This is because they do not model the joint inference distribution of all modalities as in joint models.\n\nHere, we focus on joint models that can be applied to a wider spectrum of applications. Although there are some joint models that can handle missing modalities via a surrogate unimodal inference model (Vedantam et al. 2017; Korthals et al. 2019), they typically face scalability issues. Hence, we consider joint models that can directly learn the joint inference distributions by aggregating unimodal inference distributions through an aggregation function. Following this vein, Wu and Goodman (2018) proposed an PoE\n\nVAE (a.k.a., MVAE) by aggregating the unimodal distributions with a product of experts. Despite resulting in a sharper joint distribution, PoE-VAE is prone to focus on certain modalities while neglecting others. To mitigate this issue, Shi et al. (2019) proposed an MoE-VAE (a.k.a., MMVAE) by leveraging a mixture of experts. However, MoEVAE does not produce a joint distribution that is sharper than any other expert: the precision of the joint inference distribution may not increase as the number of modalities increases. To take advantage of both PoE and MoE, Sutter, Daunhawer, and Vogt (2021) proposed a generalized MoPoE-VAE, which first applies PoE and then MoE to all possible subsets of modalities. However, the previous attempts at joint models are limited to the perspective of experts in statistics.\n\nAlthough there are other multimodal VAEs (Palumbo, Daunhawer, and Vogt 2023; Hirt et al. 2024; Yuan et al. 2024), their focus is not on new aggregation functions. Instead, they are considered variants of PoE-VAE and MoEVAE. In this paper, we provide a unified framework for aggregation functions from a barycentric view. In contrast to previous works that combined unimodal distribution aggregation with model parameter optimization (Wu and Goodman 2018; Shi et al. 2019; Sutter, Daunhawer, and Vogt 2020, 2021), our barycentric formulation decouples these two steps. This enables a more flexible choice of barycenters for aggregating unimodal inference distributions (e.g., the Wasserstein barycenter, which we explore in this paper).\n\n# Optimal Transport and Wasserstein distance\n\nOptimal transport (OT) seeks to find a transport map to move the mass from one distribution to another while minimizing the transport cost. Here, we consider Kantorovich’s dual OT formulation (Kantorovich 1942) instead of Monge’s primal formulation (Monge 1781), as Monge’s formulation is not symmetric. $P \\sim { \\overline { { \\mathcal { P } ( \\chi ) } } }$ and $Q \\sim { \\bar { \\mathcal { P } } } ( \\mathcal { Y } )$ , with $\\mathcal { P } ( \\mathcal { X } )$ and $\\mathcal { P } ( \\mathcal { V } )$ being the respective sets of probability distributions on them, Kantorovich’s OT formulation is defined as\n\n$$\n\\operatorname* { i n f } _ { \\pi \\in \\prod ( P , Q ) } \\int _ { \\mathcal { X } \\times \\mathcal { Y } } c ( x , y ) d \\pi ( x , y ) ,\n$$\n\nwhere $\\boldsymbol { c } : \\boldsymbol { \\mathcal { X } } \\times \\boldsymbol { \\mathcal { Y } }$ is a cost function. The infimum is taken over the set of all transport plans $\\pi \\in \\prod ( P , Q )$ , i.e., joint distributions on $\\mathcal { X } \\times \\mathcal { Y }$ with marginals $P$ and $Q$ .\n\nThe $p$ -Wasserstein distance is then the $p$ -th root of the infimum of Kantorovich’s OT formulation for a cost function $c ( x , y ) = | x - y | ^ { p }$ :\n\n$$\n\\mathcal W _ { p } ( P , Q ) = \\operatorname* { i n f } _ { \\pi \\in \\prod ( P , Q ) } \\left( \\int _ { \\mathcal X \\times \\mathcal Y } | x - y | ^ { p } d \\pi ( x , y ) \\right) ^ { 1 / p } ,\n$$\n\nwith $p = 1$ being an earth mover’s distance that is commonly used in many generative adversarial networks (see e.g., Arjovsky, Chintala, and Bottou 2017; Gulrajani et al. 2017; Miyato et al. 2018). In contrast, we focus on the 2- Wasserstein distance for deriving the Wasserstein barycenter in this paper, as its quadratic form allows for an analytic solution in the case of Gaussian distributions. For two Gaussian distributions $\\mathcal { N } ( \\mu _ { 1 } , \\Sigma _ { 1 } )$ and $\\mathcal { N } ( \\mu _ { 2 } , \\Sigma _ { 2 } )$ , the squared\n\n![](images/231969e1db9b040067042dbc995fb18189652c19450401690771f0e12d5c91cc.jpg)  \nFigure 1: The overview of a multimodal VAE that takes $M$ modalities ${ \\cal X } _ { 1 : M } ~ = ~ \\{ { \\pmb x } _ { j } \\} _ { j = 1 } ^ { M }$ as input and outputs tThe rmecuoltnismtroudcatledVAinEpcut smisotdsaloift $\\tilde { \\cal X } _ { 1 : M } = \\{ \\tilde { \\pmb { x } } _ { j } \\} _ { j = 1 } ^ { M }$ $M$ $\\{ q _ { \\phi _ { j } } ( \\boldsymbol { z } | \\boldsymbol { x } _ { j } ) \\} _ { j = 1 } ^ { M }$ and decoders $\\{ p _ { \\theta _ { j } } ( \\pmb { x } _ { j } | \\pmb { z } ) \\} _ { j = 1 } ^ { M }$ .\n\n2-Wasserstein distance between them is solved analytically (see e.g., Knott and Smith 1984; Givens and Shortt 1984):\n\n$$\n\\begin{array} { r l } & { \\mathcal { W } _ { 2 } ^ { 2 } ( \\mathcal { N } ( \\pmb { \\mu } _ { 1 } , \\pmb { \\Sigma } _ { 1 } ) , \\mathcal { N } ( \\pmb { \\mu } _ { 2 } , \\pmb { \\Sigma } _ { 2 } ) ) = | \\pmb { \\mu } _ { 1 } - \\pmb { \\mu } _ { 2 } | _ { 2 } ^ { 2 } + } \\\\ & { \\qquad \\mathrm { T r } ( \\pmb { \\Sigma } _ { 1 } + \\pmb { \\Sigma } _ { 2 } - 2 ( \\pmb { \\Sigma } _ { 1 } ^ { 1 / 2 } \\pmb { \\Sigma } _ { 2 } \\pmb { \\Sigma } _ { 1 } ^ { 1 / 2 } ) ^ { 1 / 2 } ) . } \\end{array}\n$$\n\n# Methods\n\n# Multimodal VAE: an Expert View\n\nWithout loss of generality, we consider a dataset {X1(:i)M }iN= containing $N$ number of independent and identically distributed (i.i.d.) samples, each of which consists of $M$ modalities: $\\pmb { X } _ { 1 : M } ^ { ( i ) } = \\{ \\pmb { x } _ { 1 } ^ { ( i ) } , \\cdots , \\pmb { x } _ { M } ^ { ( i ) } \\}$ . Assuming the multimodal data can be generated by some random process involving a joint latent variable $z$ , the objective of a multimodal VAE is to maximize the log-likelihood of data over all $M$ modalities, given i.i.d. condition:\n\n$$\n\\begin{array} { r } { \\log p _ { \\theta } ( \\mathbf { { X } } _ { 1 : M } ^ { ( i ) } ) = D _ { \\mathrm { K L } } ( q _ { \\phi } ( z | \\mathbf { { X } } _ { 1 : M } ^ { ( i ) } ) | | p _ { \\theta } ( z | \\mathbf { { X } } _ { 1 : M } ^ { ( i ) } ) ) } \\\\ { + \\mathcal { L } ( \\theta , \\phi ; \\mathbf { { X } } _ { 1 : M } ^ { ( i ) } ) , } \\end{array}\n$$\n\nwhere ${ q _ { \\phi } ( \\boldsymbol { z } | \\boldsymbol { X } _ { 1 : M } ^ { ( i ) } ) }$ is the approximate posterior parameterized by deep neural networks (i.e., the probabilistic encoders in VAEs), as the true posterior is intractable in practice. Since the KL divergence of the approximate from the true posterior (i.e., first RHS term in Eq. (2)) is non-negative, we instead maximize the evidence lower bound (ELBO) $\\mathcal { L } ( \\theta , \\psi ; X _ { 1 : M } ^ { ( i ) } )$ as follows:\n\n$$\n\\begin{array} { r } { \\mathcal { L } ( \\theta , \\phi ; \\mathbf { X } _ { 1 : M } ^ { ( i ) } ) = \\mathbb { E } _ { q _ { \\phi } ( z | \\mathbf { X } _ { 1 : M } ^ { ( i ) } ) } [ \\log p _ { \\theta } ( \\mathbf { X } _ { 1 : M } ^ { ( i ) } | z ) ] } \\\\ { - D _ { \\mathrm { K L } } ( q _ { \\phi } ( z | \\mathbf { X } _ { 1 : M } ^ { ( i ) } ) | | p _ { \\theta } ( z ) ) , } \\end{array}\n$$\n\nwhere $\\{ q _ { \\phi _ { m } } ( z | X ) \\} _ { m = 1 } ^ { M }$ and $\\{ p _ { \\theta _ { m } } ( { \\pmb X } | z ) \\} _ { m = 1 } ^ { M }$ are the $M$ probabilistic encoders and decoders, respectively. For notation brevity, we will omit the sample index $( i )$ hereafter. An overview of the multimodal VAE is shown in Fig. 1. However, in a multimodal scenario, maximizing the above\n\nELBO objective requires the knowledge of the true joint posterior $\\bar { p } _ { \\theta } ( z | X _ { 1 : M } )$ , which is unknown in practice. To tackle this issue, previous explorations of multimodal VAEs approximate the true joint posterior by aggregating the unimodal inference distributions with a proper function $f _ { \\mathrm { a g g r } } ( \\cdot )$ : $\\tilde { q } ( z | X _ { 1 : M } ) = f _ { \\mathrm { a g g r } } \\bigl ( \\{ q _ { \\phi _ { m } } \\} _ { m = 1 } ^ { M } \\bigr )$ , where $\\tilde { q } \\big ( \\boldsymbol { z } | \\boldsymbol { X } _ { 1 : M } \\big )$ denotes the approximate joint posterior. Some popular choices of $f _ { \\mathrm { a g g r } } ( \\cdot )$ are PoE ( $\\mathrm { W u }$ and Goodman 2018), MoE (Shi et al. 2019), or a combination of both (MoPoE; Sutter, Daunhawer, and Vogt 2021). Formally, the approximate joint posterior by PoE and MoE can be summarized as\n\n$$\n\\tilde { q } ( z | \\boldsymbol { X } _ { 1 : M } ) = \\left\\{ \\begin{array} { l l } { \\frac { 1 } { Z } \\displaystyle \\prod _ { m = 1 } ^ { M } q _ { \\phi _ { m } } ( z | \\boldsymbol { x } _ { m } ) , \\mathrm { P o E } , } \\\\ { \\frac { 1 } { M } \\displaystyle \\sum _ { m = 1 } ^ { M } q _ { \\phi _ { m } } ( z | \\boldsymbol { x } _ { m } ) , \\mathrm { M o E } , } \\end{array} \\right.\n$$\n\nwhere $Z$ is the normalizer function that ensures the approximate posterior by PoE is a valid probability measure.\n\n# Multimodal VAE: a Barycentric View\n\nThe barycenter of distribution is defined as a central distribution of a set of distributions that minimizes the sum of divergences to all other distributions in the set. For a set of probability distributions $\\{ P _ { 1 } , \\cdots , P _ { M } \\}$ with associated weights $\\{ \\lambda _ { 1 } , \\cdot \\cdot \\cdot , \\lambda _ { M } \\}$ , the barycenter minimizes the weighted sum of some divergences $d ( \\cdot , \\cdot )$ from the barycenter distribution $P _ { B }$ to each of the given distributions:\n\n$$\nP _ { \\cal B } = \\underset { \\cal P } { \\arg \\operatorname* { m i n } } \\sum _ { m = 1 } ^ { M } \\lambda _ { m } d ( P _ { m } , \\boldsymbol { P } ) , \\quad \\sum _ { m = 1 } ^ { M } \\lambda _ { m } = 1 .\n$$\n\nLemma 1. In the context of multimodal $V A E ,$ , we seek to find a barycenter $\\tilde { q } ( \\boldsymbol { z } | \\boldsymbol { X } _ { 1 : M } )$ that can aggregate the unimodal inference distrib|utions $\\{ q _ { \\phi _ { m } } ( \\boldsymbol { z } | \\mathbf { x } _ { m } ) \\} _ { m = 1 } ^ { M ^ { \\bar { \\mathbf { \\alpha } } } }$ to approximate the true joint posterior $p _ { \\theta } \\big ( z | X _ { 1 : M } \\big )$ :\n\n$$\n\\tilde { q } = \\arg \\operatorname* { m i n } _ { q } \\sum _ { m = 1 } ^ { M } \\lambda _ { m } d \\left( q _ { \\phi _ { m } } , q \\right) , \\sum _ { m = 1 } ^ { M } \\lambda _ { m } = 1 .\n$$\n\nNote that, for notation brevity, we abbreviate $q _ { \\phi _ { m } } ( \\boldsymbol { z } | \\boldsymbol { x } _ { m } )$ and $\\tilde { q } ( \\boldsymbol { z } | \\boldsymbol { X } _ { 1 : M } )$ as $q _ { \\phi _ { m } }$ and $\\tilde { q }$ , respectively. Instead of directly minimizing the divergence between $q _ { \\phi } \\big ( \\boldsymbol { z } | \\boldsymbol { X } _ { 1 : M } \\big )$ and $p _ { \\theta } ( z )$ over trainable parameters $\\boldsymbol { \\phi } = \\{ \\phi _ { 1 } , \\cdot \\cdot \\cdot , \\stackrel { . } { \\phi _ { M } } \\}$ as formulated in Eq. (3) and prior multimodal VAEs (Wu and Goodman 2018; Shi et al. 2019; Sutter, Daunhawer, and Vogt 2020, 2021), Lemma 1 suggests that this involves a bilevel optimization. For the lower-level optimization (i.e., Eq. (4)), we determine a barycenter $\\tilde { q } \\big ( z | \\bar { \\boldsymbol { X } } _ { 1 : M } \\big )$ , which is equivalent to applying an aggregation function $f _ { \\mathrm { a g g r } }$ to combine the unimodal inference distributions. We then push $\\tilde { q } \\big ( \\boldsymbol { z } | \\boldsymbol { X } _ { 1 : M } \\big )$ towards $p _ { \\theta } ( z )$ by minimizing their divergence over| trainable parameters $\\circled { \\phi } \\doteq \\{ \\phi _ { m } \\} _ { m = 1 } ^ { M }$ (upper-level optimization; Eq. (3)). At first glance, this formulation is counterintuitive, as it complicates the formulation and optimization, whereas in-depth analysis reveals its theoretically intriguing properties.\n\nProposition 1. For any divergence measure $d ( q _ { \\phi _ { m } } , \\cdot )$ that is convex on $q _ { \\phi _ { m } }$ , the resulting barycenter by minimizing\n\n![](images/4a80aa129f8448f748ed147b79085eeac51c60cca7c532fdd70f6528b2701642.jpg)  \nFigure 2: Comparison of methods for aggregating the unimodal inference distributions $( \\{ q _ { \\phi _ { j } } \\} _ { j = 1 } ^ { M } )$ to approximate the joint posterior $( \\tilde { q } _ { \\phi } )$ : (a) PoE, (b) MoE, and (c) the proposed Wasserstein barycenter. In this illustrative example, we use two 1- dimensional Gaussian modalities ( $M = 2$ ) for a proof of concept.\n\nEq. (4) guarantees a valid ELBO on the marginal loglikelihood $p _ { \\theta } ( { \\pmb X } _ { 1 : M } )$ and a scalable inference. This is because of Jensen’s inequality:\n\n$$\nd \\left( \\sum _ { m = 1 } ^ { M } \\lambda _ { m } q _ { \\phi _ { m } } , q \\right) \\leq \\sum _ { m = 1 } ^ { M } \\lambda _ { m } d ( q _ { \\phi _ { m } } , q )\n$$\n\nFor a complete proof of Proposition 1, please see Appendix A.1. The LHS in Eq. (5) defines a scalable inference, as the naive implementation on the RHS requires 2M inference networks to handle arbitrary combination of input modalities. Although Proposition 1 has been considered in some prior works from different perspectives (Shi et al. 2019; Sutter, Daunhawer, and Vogt 2021), they are limited to the case of KL divergence (see Theorem 1). In contrast, our barycentric view extends them to a more general case whenever $d ( q _ { \\phi _ { m } } , q )$ is convex to $q _ { \\phi _ { m } }$ , which enables it to analyze the properties of a more flexible choice of divergence measures (e.g., $f$ -divergence, 2-Wasserstein distance, Gromov-Wasserstein distance, etc).\n\nTheorem 1. Considering $K L$ divergence $D _ { K L } ( \\cdot | | \\cdot )$ as the divergence measure $d ( \\cdot , \\cdot )$ , PoE and MoE are the barycenters yielded by optimizing the reverse and forward $K L$ divergence, respectively:\n\n$$\n\\begin{array} { l } { { \\displaystyle { \\tilde { q } } _ { P o E } = \\arg \\operatorname* { m i n } _ { q } \\frac { 1 } { Z } \\sum _ { m = 1 } ^ { M } D _ { K L } ^ { r e v e r s e } ( q | | q _ { \\phi _ { m } } ) } , } \\\\ { { \\displaystyle { \\tilde { q } } _ { M o E } = \\arg \\operatorname* { m i n } _ { q } \\frac { 1 } { M } \\sum _ { m = 1 } ^ { M } D _ { K L } ^ { f o r w a r d } ( q _ { \\phi _ { m } } | | q ) } . } \\end{array}\n$$\n\nThe proof of Theorem 1 is in Appendix A.2. In information theory, it is customary to define KL divergence as relative entropy (due to its asymmetry), with the form used in PoE and MoE in Theorem 1 being the exclusive (reverse) and inclusive (forward) KL divergence (Cover 1999; Murphy 2012). Theorem 1 immediately provides an information-theoretic view of PoE and MoE: they are two variants resulting from the inherent asymmetry of KL divergence. this provides us with an information-theoretic tool to analyze the properties of PoE and MoE in multimodal VAE.\n\nRemark 1. PoE is zero-forcing, encouraging $\\tilde { q } ( \\boldsymbol { z } | \\boldsymbol { X } _ { 1 : M } )$ to be zero where $q _ { \\phi _ { m } } ( \\boldsymbol { z } | \\boldsymbol { x } _ { m } )$ is zero, which makes it biased towards certain modalities. In contrast, MoE is masscovering, ensuring that there is mass under $\\tilde { q } ( \\boldsymbol { z } | \\boldsymbol { X } _ { 1 : M } )$ wherever there is mass under $q _ { \\phi _ { m } } ( \\boldsymbol { z } | \\mathbf { x } _ { m } )$ .\n\nRemark 1 is due to the intrinsic properties of forward and reverse KL divergence (Minka et al. 2005; Turner and Sahani 2011). Though it is well known that PoE results in a sharper distribution that concentrates on one of the modalities, whereas MoE does not produce a distribution sharper than any individual expert due to the nature of the mixture, Remark 1 provides an information-theoretic interpretation. We demonstrate this by considering an example with two modalities, as shown in Fig. 2. When there is zero mass under $q _ { \\phi _ { 1 } }$ and nonzero mass under $\\tilde { q } _ { \\mathrm { P o E } }$ , the reverse KL divergence1is almost infinity: $D _ { \\mathrm { K L } } ^ { \\mathrm { r e v e r s e } } ( \\tilde { q } _ { \\mathrm { P o E } } | | q _ { \\phi _ { 1 } } ) \\to \\infty$ which pushes $\\tilde { q } _ { \\mathrm { P o E } }$ toward $q _ { \\phi _ { 2 } }$ (see Fig. 2a). In contrast, since the forward KL divergence penalizes $\\log q _ { \\phi _ { m } } ( \\boldsymbol { z } | \\boldsymbol { x } _ { m } ) -$ $\\log \\tilde { q } ( z | X _ { 1 : M } )$ , it ensures that $\\tilde { q }$ has mass covered wherever this is mass under $q _ { \\phi _ { m } }$ (see Fig. 2b).\n\nHowever, the forward and reverse KL divergence does not define a metric space for probability measures because it is asymmetric and unbounded. One notable example is that solving Eq. (4) does not guarantee a valid probability measure in the case of PoE (see Appendix A.2). This motivates us to find a barycenter defined in the probability metric space. Below, we explore the barycenter defined in the 2-Wasserstein space, known as the Wasserstein barycenter.\n\n# Multimodal VAE from Wasserstein Barycenter\n\nHere, we provide a roadmap to derive the proposed Wasserstein barycenter VAE ( $\\ w B$ -VAE) for multimodal representation learning. Following the convention in Eq. (4), Wasserstein barycenter $( \\mathcal { W B } )$ is defined by minimizing the sum of the squared 2-Wasserstein distance $\\mathcal { W } _ { 2 } ^ { 2 } ( \\cdot , \\cdot )$ . Since the 2- Wasserstein distance is symmetric, the order of distributions in $\\mathcal { W } _ { 2 } ( \\cdot , \\cdot )$ does not matter. In the context of multimodal VAE, the approximate posterior resulting from optimizing the squared 2-Wasserstein distance is\n\n$$\n\\tilde { q } \\ d \\nu \\ d _ { \\nu } \\ d _ { \\mathcal { W } } = \\underset { q } { \\arg \\operatorname* { m i n } } \\sum _ { m = 1 } ^ { M } \\lambda _ { m } \\mathcal { W } _ { 2 } ^ { 2 } ( q _ { \\phi _ { m } } , q ) , \\sum _ { m = 1 } ^ { M } \\lambda _ { m } = 1 .\n$$\n\nUnlike the KL divergence used in the case of PoE and MoE, which focuses on pointwise differences, the 2-Wasserstein distance better preserves the geometry of the unimodal inference distributions. Accordingly, interpolating in the Wasserstein space (i.e., a geodesic space) can have a meaningful transition from unimodal distributions to the joint posterior, especially when the unimodal distributions have different shapes or supports (Ambrosio, Gigli, and Savare´ 2008). Therefore, different choices of weights associated with unimodal distributions (i.e., $\\{ \\lambda _ { 1 } , \\cdot \\cdot \\cdot , \\lambda _ { M } \\} )$ ) may lead to a joint posterior that maintains diverse shapes and structures of unimodal distributions. However, in the context of multimodal VAEs, it is challenging to determine $\\{ \\lambda _ { 1 } , \\cdot \\cdot \\cdot , \\lambda _ { M } \\}$ , as we only have the marginal unimodal distributions. Similar to the case of PoE and MoE, it is typically safe to set $\\lambda _ { m } = 1 / M$ , m.\n\nBures-Wasserstein barycenter. Wasserstein barycenter typically incurs the significant computational cost associated with the 2-Wasserstein distance. However, in the case of Gaussian distributions, as are typically assumed in VAEs, the Gaussian Wasserstein barycenter (i.e., the so-called Bures-Wasserstein barycenter (Agueh and Carlier 2011)) can be obtained by solving a fixed-point equation (Knott and Smith 1994; Agueh and Carlier 2011).\n\nConsidering the unimodal inference distributions $\\{ q _ { \\phi _ { m } } \\} _ { m = 1 } ^ { M }$ are $d$ -dimensional multivariate Gaussian $\\{ \\mathcal { N } ( \\pmb { \\mu _ { m } } , \\pmb { \\Sigma _ { m } } ) \\} _ { m = 1 } ^ { M }$ , with $\\pmb { \\mu _ { m } } ~ \\in ~ \\mathbb { R } ^ { d }$ and $\\pmb { \\Sigma } _ { m } \\in \\mathbb { R } ^ { d \\times d }$ being the associated mean and covariance of $q _ { \\phi _ { m } }$ , the resulting Bures-Wasserstein barycenter turns out to be Gaussian-distributed, i.e., $\\tilde { q } _ { \\mathcal { W B } } ( z | \\boldsymbol { X } _ { 1 : M } ) \\sim \\mathcal { N } ( \\tilde { \\pmb { u } } , \\tilde { \\pmb { \\Sigma } } )$ :\n\n$$\n\\tilde { \\pmb { \\mu } } = \\sum _ { m = 1 } ^ { M } \\lambda _ { m } \\pmb { \\mu _ { m } } , \\tilde { \\pmb { \\Sigma } } = \\sum _ { m = 1 } ^ { M } \\lambda _ { m } ( \\tilde { \\pmb { \\Sigma } } ^ { 1 / 2 } \\pmb { \\Sigma _ { m } } \\tilde { \\pmb { \\Sigma } } ^ { 1 / 2 } ) ^ { 1 / 2 } ,\n$$\n\nwhere the covariance $\\tilde { \\Sigma }$ is obtained by solving the fixpoint iteration. However, Eq. (6) can be further simplified by considering $q _ { \\phi _ { m } } ( \\boldsymbol { z } | \\boldsymbol { x } _ { m } )$ an isotropic Gaussian with a diagonal covariance $\\mathcal { N } ( \\mu _ { m } , \\sigma _ { m } ^ { 2 } I )$ with $\\pmb { \\mu } _ { m } , \\pmb { \\sigma } _ { m } \\in \\mathbb { R } ^ { d }$ and $\\check { I ^ { \\mathrm { * } } } \\in \\mathbb { R } ^ { d \\times d }$ . This is typically assumed in most VAEs (Kingma and Welling 2013).\n\nRemark 2. In the isotropic Gaussian case, Eq. (6) can be solved analytically dimension by dimension:\n\n$$\n\\tilde { \\pmb { \\mu } } = \\sum _ { m = 1 } ^ { M } \\lambda _ { m } \\pmb { \\mu _ { m } } , \\tilde { \\pmb { \\sigma } } = \\sum _ { m = 1 } ^ { M } \\lambda _ { m } \\pmb { \\sigma _ { m } } .\n$$\n\nRemark 2 is because the optimal transport map from one Gaussian to another is a linear map (Knott and Smith 1994; Agueh and Carlier 2011), with which the squared 2- Wasserstein distance can be solved analytically (for details, please see Appendix A.3). As suggested by Lemma 1, the Bures-Wasserstein barycenter can be viewed as minimizing the 2-Wasserstein distance to a mixture of distributions.\n\nMixture of Wasserstein barycenter. The approximate joint distribution derived from solving the Wasserstein barycenter strikes a balance between zero-forcing (bias) and mass-covering (variance), resulting in a distribution that is sharper than half of the unimodal inference distributions (see Fig. 2c). However, there is an inherent trade-off between zero-forcing and mass-covering (Murphy 2012). Similar to MoPoE-VAE (Sutter, Daunhawer, and Vogt 2021), we consider a variant of $\\scriptstyle { \\mathcal { W B } }$ -VAE by constructing a mixture of Wasserstein barycenter, termed $\\mathcal { M } \\mathcal { W } \\mathcal { B }$ -VAE.\n\nRemark 3. The mixture of Wasserstein barycenter with unimodal inference distributions is still a barycenter. Considering the powerset of $M$ modalities $\\mathcal { P } _ { M } ( { \\mathbf { \\bar { X } } } )$ , which consists of $2 ^ { \\hat { M } }$ different combinations, the mixture of Wasserstein barycenter is given as\n\n$$\n\\tilde { q } _ { \\mathcal M } \\mathcal { W } \\boldsymbol { B } = \\arg \\operatorname* { m i n } _ { \\boldsymbol { q } } \\sum _ { \\boldsymbol { X } _ { k } \\in \\mathcal { P } _ { M } ( \\boldsymbol { X } ) } \\lambda _ { k } D _ { K L } ( \\tilde { q } _ { \\mathcal { W B } } | | \\boldsymbol { q } )\n$$\n\n$$\ns u b j e c t t o \\quad \\tilde { q } _ { \\mathcal { W B } } = \\mathop { \\arg \\operatorname* { m i n } } _ { \\boldsymbol { q } } \\sum _ { \\mathbf { x } _ { j } \\in { \\cal X } _ { k } } \\lambda _ { j } \\mathcal { W } _ { 2 } ^ { 2 } ( q _ { \\phi _ { j } } , q )\n$$\n\nThough this is a bilevel optimization problem, the solution is analytical since both the lower-level and upper-level optimization problems can be solved analytically. The solution is also optimal due to the convexity of both forward KL divergence and 2-Wasserstein distance. By applying the same mechanism, we can also derive MoPoE (Sutter, Daunhawer, and Vogt 2021) as a barycenter, whereas the solution is not guaranteed to be optimal since the solution to the lower-level (PoE) case is not a global optimum in general.\n\n# Experiments\n\nDataset. We conducted comparative experiments on three multimodal benchmark datasets: i) PolyMNIST with five simplified modalities, ii) the trimodal MNIST-SVHNTEXT, and iii) the challenging bimodal CelebA dataset. PolyMNIST was generated by combining each MNIST digit (LeCun and Cortes 2010) with $2 8 \\times 2 8$ random crops from five distinct background images, as described in (Sutter, Daunhawer, and Vogt 2021). The MNIST-SVHN-TEXT dataset was introduced by (Sutter, Daunhawer, and Vogt 2020), which consists of three modalities: MNIST digit (LeCun and Cortes 2010), text, and SVHN (Netzer et al. 2011). The MNIST digit and text are two clean modalities, whereas SVHN is comprised of noisy images. Folliwing (Sutter, Daunhawer, and Vogt 2021), 20 triples were generated per set using a many-to-many mapping. The bimodal CelebA includes human face images as well as text describing the face attributes (Liu et al. 2015). This dataset is challenging because the text modality focuses on the attributes present in a face image. If an attribute is absent, it is omitted from the corresponding text (Sutter, Daunhawer, and Vogt 2020).\n\nBaseline methods. We compared the proposed method to three state-of-the-art multimodal VAEs, including PoEVAE (Wu and Goodman 2018), MoE-VAE (Shi et al. 2019), and MoPoE-VAE (Sutter, Daunhawer, and Vogt 2021).\n\nEvaluation metric. Following previous literature in Wu and Goodman (2018); Shi et al. (2019); Sutter, Daunhawer, and Vogt (2021), several tasks were conducted to evaluate the performance of the multimodal VAEs. First, a linear classifier was used to assess the quality of the learned latent representations. Second, the coherence of generated samples was evaluated using pre-trained classifiers. Third, the approximate joint posterior was measured by calculating the loglikelihoods on the test set.\n\n<html><body><table><tr><td>del</td><td>M</td><td>S</td><td>T</td><td>M,S</td><td></td><td>M,T</td><td>S,T</td><td>M,S,T</td><td>Avg.</td></tr><tr><td>E-VAE E-VAE PoE-VAE</td><td>0.90±0.01 0.95±0.01 0.95±0.01</td><td>0.44±0.01 0.79±0.05 0.80±0.03</td><td>0.85±0.10 0.99±0.01 0.99±0.01</td><td></td><td>0.89±0.01 0.87±0.03 0.97±0.01</td><td>0.97±0.02 0.93±0.03 0.98±0.01</td><td>0.81±0.09 0.84±0.04 0.99±0.01</td><td>0.96±0.02 0.86±0.03 0.98±0.01</td><td>0.83 0.89 0.95</td></tr><tr><td>3-VAE WB-VAE</td><td>0.91±0.03 0.97±0.00</td><td>0.44±0.02 0.83±0.01</td><td>1.00±0.00 1.00±0.00</td><td></td><td>0.89±0.00 0.99±0.00</td><td>0.99±0.02 1.00±0.00</td><td>0.99±0.01 1.00±0.00</td><td>0.99±0.00 1.00±0.00</td><td>0.89 0.97*</td></tr><tr><td colspan=\"16\">(a)Linear classification accuracy of latent representations</td></tr><tr><td colspan=\"2\">Model</td><td colspan=\"7\">M S T S T S,T M M,T M S M.S</td><td>Avg.</td></tr><tr><td>MoE-VAE MoPoE-VAE</td><td colspan=\"2\">PoE-VAE 0.24 0.75</td><td>0.20 0.32 0.99 0.87</td><td>0.43 0.31</td><td>T 0.30 0.75 0.30</td><td>0.28 0.30 0.96</td><td>0.17 0.76</td><td>0.29 0.84</td><td>0.32 0.68</td></tr><tr><td>WB-VAE</td><td></td><td>0.74 0.99 0.12 0.51</td><td>0.94 0.57</td><td>0.36 0.28</td><td>0.34 0.37 0.39 0.53</td><td>0.96 0.52</td><td>0.76 0.18</td><td>0.93 0.71 0.57 0.41</td><td></td></tr><tr><td>(b) Conditional generation coherence</td><td>MWB-VAE</td><td>0.82</td><td>1.00 0.99</td><td>0.36</td><td>0.35</td><td>0.39 0.97</td><td>0.84</td><td>0.99 0.75*</td><td></td></tr><tr><td colspan=\"10\"></td></tr><tr><td>[odel</td><td colspan=\"4\">X X|xm</td><td>X|xT</td><td colspan=\"2\">X|xM,xs</td><td>X|xM,xT</td><td>X|xs,xT</td></tr><tr><td>oE-VAE</td><td colspan=\"4\">-1790±3.3 -2090±3.8</td><td>X|xs -1895±0.2 -2133±6.9</td><td>-1825±2.6</td><td></td><td>-2050±2.6</td><td>-1855±0.3</td></tr><tr><td>IoE-VAE [oPoE-VAE</td><td colspan=\"4\">-1941±5.7 -1987±1.5</td><td>-1857±12 -2018±1.6 -1858±6.2</td><td colspan=\"2\">-1912±7.3</td><td>-2002±1.2</td><td>-1925±7.7</td></tr><tr><td>VB-VAE</td><td colspan=\"4\">-1819±5.7 -1991±2.9</td><td>-2024±2.6</td><td colspan=\"2\">-1822±5.0</td><td>-1987±3.1</td><td>-1850±5.8</td></tr><tr><td>AWB-VAE</td><td colspan=\"4\">-1785±7.4 -2072±13 -1890±1.7 -2000±1.4</td><td>-2126±12 -1856±3.4 -2036±0.4</td><td colspan=\"2\">-1814±7.5 -1825±1.6</td><td>-2033±7.1 -1988±1.4</td><td>-1856±4.7 -1853±2.2</td></tr></table></body></html>\n\n(c) Log-likelihoods of the joint generative model (ranking based on first three decimals)\n\nTable 1: Quantitative results on MNIST-SVHN-TEXT in terms of (a) linear classification accuracy, (b) conditional generation coherence, and (c) log-likelihoods of the joint generative model. We evaluated all possible combinations of modalities $X _ { k }$ . We reported the means $\\pm$ standard deviations) over 5 runs, where the best performance is highlighted with bold. The abbreviations of different modalities in this table are as follows: M: MNIST; S: SVHN; T: Text.\n\n![](images/eb4b1f51728d289b1f2268219ceeb66080ddd0af48953f95514cdd68a8cc687d.jpg)  \nFigure 3: Quantitative results on PolyMNIST as a function of the number of input modalities, averaged over all subsets of modalities of the respective size. Left: Linear classification accuracy of digits given the latent representation. Center: Coherence of conditionally generated samples that do not include input modalities. Right: Log-Likelihood of all generated modalities.\n\nImplementation details. For a fair comparison, we followed the experimental settings in previous literature (Shi et al. 2019; Sutter, Daunhawer, and Vogt 2021). In particular, we employed the same network architecture as in (Shi et al. 2019; Sutter, Daunhawer, and Vogt 2021). For more implementation details (e.g., hyperparameter configurations), we kindly direct the readers to Appendix B. All experiments were performed on a Nvidia-A100 GPU with 40G memory.\n\n# Results\n\nMNIST-SVHN-TEXT results. As shown in Tables 1, the proposed $\\mathcal { M } \\mathcal { W } \\mathcal { B }$ -VAE demonstrated superior performance compared to other state-of-the-art multimodal VAEs in terms of the quality of learned latent representations and generation coherence. In addition, our $\\scriptstyle { \\mathcal { W B } }$ -VAE outperformed PoE-VAE regarding the linear classification accuracy using the learned latent representations and was on par with PoE-VAE regarding generation coherence. Although there is an inherent trade-off between generation coherence and log-likelihood, the log-likelihood of our $\\scriptstyle { \\mathcal { W B } }$ -VAE and $\\mathcal { M } \\mathcal { W } \\mathcal { B }$ -VAE were on par with the other state-of-the-art methods. This suggests that the proposed method can approximate the joint posterior well.\n\nPolyMNIST results. The PolyMNIST dataset is unique in that it contains more than three modalities, enabling us to explore how different methods perform as the number of input modalities increases (see Fig. 3). Notably, the proposed $\\scriptstyle { \\mathcal { W B } }$ -VAE and $\\mathcal { M } \\mathcal { W } \\mathcal { B }$ -VAE showed an approximately linear relationship between all the performance metrics and the number of input modalities. This was particularly true for the linear classification task, where the performance of other\n\n5 o’clock Brown hair, Bangs, Black hair, Blond hair, Chubby, Attractive, big nose, 5 o’clock High shadow, heavy brown hair, double chin, heavy double heavy black hair, shadow, cheekbone male,bags makeup, high high high makeup, high chin, makeup, bushy attractive, s, no beard, under eyes, cheekbones, cheekbones, cheekbones cheekbones, eyeglasses, narrow eyebrows, black hair, oval face, bangs,black mouth slightly male,mouth , mouth no beard, gray hair, eyes, no high eyeglasses, pointy hair,bushy open, no slightly open, slightly pointy nose, male, no beard, cheekbones, male, nose, eyebrows, beard, pointy no beard, open, no smiling, beard, pointy nose, male, mouth sideburns, smiling, mustache, nose, smiling oval face, beard,smilin wavy hair, receding wearing slightly open, straight hair wearing narrow eyes, smiling g, wavy hair, wearing hairline earrings, smiling earrings straight hair young lipstick young\n\n![](images/d76af84c203ed3a7835cd8098ab1e3e2b75ff077760e07eaf16911995729e9cb.jpg)  \nFigure 4: Conditionally generated images given the text on top of each column on bimodal CelebA using $\\mathcal { M } \\mathcal { W } \\mathcal { B } .$ -VAE\n\nTable 2: Classification accuracy based on latent representation and conditionally generated coherence on the bimodal CelebA dataset. We report the mean average precision over all attributes (I: Image; T: Text; Joint: I and T).   \n\n<html><body><table><tr><td></td><td colspan=\"3\">Latent Representation</td><td colspan=\"2\">Generation</td><td></td></tr><tr><td>Model</td><td>I</td><td>T</td><td>Joint</td><td>I→T</td><td>T→I</td><td>Avg.</td></tr><tr><td>PoE-VAE</td><td>0.30</td><td>0.31</td><td>0.32</td><td>0.26</td><td>0.33</td><td>0.30</td></tr><tr><td>MoE-VAE</td><td>0.35</td><td>0.38</td><td>0.35</td><td>0.14</td><td>0.41</td><td>0.33</td></tr><tr><td>MoPoE-VAE</td><td>0.40</td><td>0.39</td><td>0.39</td><td>0.15</td><td>0.43</td><td>0.35</td></tr><tr><td>WB-VAE</td><td>0.34</td><td>0.38</td><td>0.40</td><td>0.29</td><td>0.40</td><td>0.36</td></tr><tr><td>MWB-VAE</td><td>0.37</td><td>0.44</td><td>0.44</td><td>0.34</td><td>0.43</td><td>0.40*</td></tr></table></body></html>\n\nbaseline methods was typically saturated after reaching a certain number of modalities (e.g., $M > 3$ in Fig. 3 Left). As a consequence, $\\boldsymbol { \\mathcal { W B } }$ -VAE and $\\mathcal { M } \\mathcal { W } \\mathcal { B }$ -VAE showed superior performance in terms of linear classification accuracy compared to all baseline methods, particularly when the number of input modalities increases. Similar trends were also observed in the conditional generation task (Fig. 3 Center), where the generation coherence of $w B$ -VAE increased as the number of input modalities increased. Although $\\begin{array} { r } { { \\small \\mathcal { W } \\ o B } } \\end{array}$ - VAE outperformed PoE-VAE, it did not surpass MoE-VAE, but it struck the balance between them, as there is an inherent trade-off between mass-covering and zero-forcing. As a consequence, -VAE can easily outperform MoE-VAE and achieve similar performance as MoPoE-VAE in the conditional generation task. As suggested by Sutter, Daunhawer, and Vogt (2021), there is a trade-off between generation coherence and the log-likelihood. Consequently, the PoE-VAE achieved the highest log-likelihood. Although $\\boldsymbol { \\mathcal { W B } }$ -VAE and $\\mathcal { M } \\mathcal { W } \\mathcal { B }$ -VAE did not surpass PoE-VAE in log-likelihood, their log-likelihoods were on par with MoPoE-VAE.\n\nCelebA results. As shown in Table 2, the proposed WBVAE outperformed PoE-VAE as well as competed favorably and even better than MoE-VAE in both latent representation and generation on the challenging bimodal CelebA dataset. Likewise, $\\mathcal { M } \\mathcal { W } \\mathcal { B }$ -VAE outperformed MoPoE-VAE in most scenarios, with the exception of latent representation classification when using image as the input modality. As consistent with the trends observed in the previous two datasets, the latent representation classification accuracy of -VAE increased as more modalities were present, similar to PoE-VAE. In contrast, the classification accuracy of MoE-VAE decreased when more modalities were given. Remarkably, both $\\begin{array} { r } { \\begin{array} { r } { \\scriptstyle \\mathcal { W } B } \\end{array} } \\end{array}$ -VAE and $\\mathcal { M } \\mathcal { W } \\mathcal { B }$ -VAE achieved good performance for the most challenging image-to-text generation task, outperforming the second-best method by $1 1 . 5 \\%$ and $3 0 . 8 \\%$ , respectively. $\\mathcal { M } \\mathcal { W } \\mathcal { B }$ -VAE also achieved good performance in text-to-image conditional generation (see Fig. 4), where $\\mathcal { M } \\mathcal { W } \\mathcal { B }$ -VAE learned good representations of different attributes well (e.g., ”smiling,” ”hairstyles,” etc).\n\n# Conclusion\n\nIn this work, we introduced a barycentric perspective on previous multimodal VAEs, offering a theoretical and unified formulation. This approach allows for explorations of various aggregation functions in the regime of multimodal VAEs. Leveraging this barycentric formulation, we proposed a -VAE, which uses the Wasserstein barycenter as an aggregation function that better preserves the geometry of unimodal distributions. Experimental results showed the effectiveness of the proposed $\\scriptstyle { \\mathcal { W B } }$ -VAE when compared to other state-of-the-art multimodal VAEs. We hope our new perspective will stimulate the exploration of other aggregation functions for multimodal VAEs in future work.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了多模态变分自编码器（Multimodal Variational Autoencoder, MVAE）在缺失模态情况下如何有效学习模态不变和模态特定的表示的问题。\\n> *   该问题的重要性在于，现实世界中的多模态数据（如视觉和声音）常常存在模态缺失，而现有方法（如PoE和MoE）在聚合单模态推理分布时存在偏差或覆盖不足的问题。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种基于重心（barycenter）的统一理论框架，将多模态VAE的聚合函数视为最小化单模态推理分布与联合后验分布之间的某种散度的重心问题。\\n> *   特别地，论文探索了基于2-Wasserstein距离的Wasserstein重心（Wasserstein barycenter），以更好地保留单模态分布的几何结构。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **贡献1：** 提出了一种统一的理论框架，将多模态VAE的聚合问题形式化为重心问题，揭示了PoE和MoE分别是反向和正向KL散度的重心。\\n> *   **贡献2：** 提出了基于Wasserstein重心的$\\\\mathcal{WB}$-VAE，在三个多模态基准数据集（PolyMNIST、MNIST-SVHN-TEXT、CelebA）上验证了其有效性。\\n> *   **贡献3：** 在MNIST-SVHN-TEXT数据集上，$\\\\mathcal{MWB}$-VAE在潜在表示分类任务中达到了97%的准确率，显著优于PoE-VAE（95%）和MoE-VAE（89%）。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   论文的核心思想是将多模态VAE的聚合问题形式化为重心问题，即寻找一个中心分布（重心），使其与单模态推理分布之间的加权散度和最小。\\n> *   这种方法的有效性在于，重心问题提供了一个统一的视角来分析不同聚合函数（如PoE和MoE）的性质，并允许更灵活的散度选择（如Wasserstein距离）。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 先前工作（如PoE-VAE和MoE-VAE）主要从专家（experts）的角度出发，通过PoE或MoE聚合单模态推理分布，但缺乏理论分析。\\n> *   **本文的改进：** 本文从重心角度提供了一个统一的理论框架，揭示了PoE和MoE分别是反向和正向KL散度的重心，并提出了基于Wasserstein重心的$\\\\mathcal{WB}$-VAE，更好地保留了单模态分布的几何结构。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **单模态推理分布：** 对每个模态$m$，学习一个单模态推理分布$q_{\\\\phi_m}(z|x_m)$，通常假设为各向同性高斯分布。\\n> 2.  **重心优化：** 通过最小化加权散度和（如KL散度或Wasserstein距离）来求解重心分布$\\\\tilde{q}(z|X_{1:M})$。\\n> 3.  **Wasserstein重心：** 对于高斯分布，Wasserstein重心（Bures-Wasserstein barycenter）可以通过固定点迭代求解，其均值和协方差分别为：\\n>     $$\\\\tilde{\\\\mu} = \\\\sum_{m=1}^M \\\\lambda_m \\\\mu_m, \\\\quad \\\\tilde{\\\\Sigma} = \\\\sum_{m=1}^M \\\\lambda_m (\\\\tilde{\\\\Sigma}^{1/2} \\\\Sigma_m \\\\tilde{\\\\Sigma}^{1/2})^{1/2}.$$\\n> 4.  **混合Wasserstein重心：** 进一步提出$\\\\mathcal{MWB}$-VAE，通过混合所有可能的子集模态的Wasserstein重心来平衡零强迫（zero-forcing）和质量覆盖（mass-covering）。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   PoE-VAE（Wu and Goodman 2018）\\n> *   MoE-VAE（Shi et al. 2019）\\n> *   MoPoE-VAE（Sutter, Daunhawer, and Vogt 2021）\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在潜在表示分类准确率上：** 在MNIST-SVHN-TEXT数据集上，$\\\\mathcal{MWB}$-VAE达到了97%的准确率，显著优于PoE-VAE（95%）和MoE-VAE（89%）。与表现最佳的基线（PoE-VAE）相比，提升了2个百分点。\\n> *   **在条件生成一致性上：** 在PolyMNIST数据集上，$\\\\mathcal{WB}$-VAE的生成一致性随着输入模态数量的增加而线性提升，优于PoE-VAE和MoE-VAE。\\n> *   **在联合生成模型的似然上：** 在CelebA数据集上，$\\\\mathcal{MWB}$-VAE在图像到文本生成任务中达到了40%的平均精度，优于MoPoE-VAE（35%）和MoE-VAE（33%）。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   多模态学习 (Multimodal Learning, N/A)\\n*   变分自编码器 (Variational Autoencoder, VAE)\\n*   Wasserstein重心 (Wasserstein Barycenter, N/A)\\n*   模态不变表示 (Modality-Invariant Representation, N/A)\\n*   模态特定表示 (Modality-Specific Representation, N/A)\\n*   生成模型 (Generative Model, N/A)\\n*   多模态生成 (Multimodal Generation, N/A)\"\n}\n```"
}