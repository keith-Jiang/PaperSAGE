{
    "source": "Semantic Scholar",
    "arxiv_id": "2502.08211",
    "link": "https://arxiv.org/abs/2502.08211",
    "pdf_link": "https://arxiv.org/pdf/2502.08211.pdf",
    "title": "Quality over Quantity: Boosting Data Efficiency Through Ensembled Multimodal Data Curation",
    "authors": [
        "Jinda Xu",
        "Yuhao Song",
        "Daming Wang",
        "Weiwei Zhao",
        "Minghua Chen",
        "Kangliang Chen",
        "Qinya Li"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "2025-02-12",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "institutions": [
        "Shanghai Key Laboratory of Scalable Computing and Systems",
        "Shanghai Jiao Tong University",
        "HAOMO.AI Technology Co., Ltd."
    ],
    "paper_content": "# Quality over Quantity: Boosting Data Efficiency Through Ensembled Multimodal Data Curation\n\nJinda $\\mathbf { X } \\mathbf { u } ^ { 1 * }$ , Yuhao $\\mathbf { S o n g ^ { 2 * } }$ , Daming Wang2, Weiwei Zhao1, Minghua Chen2, Kangliang Chen2†, Qinya $\\mathbf { L i } ^ { \\sharp }$\n\n1Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University 2HAOMO.AI Technology Co., Ltd. daidan2502 $@$ sjtu.edu.cn, {songyuhao, wangdaming}@haomo.ai, zhaoweiwei $@$ sjtu.edu.cn, {chenminghua, chenkangliang} $@$ haomo.ai, qinyali $@$ sjtu.edu.cn\n\n# Abstract\n\nIn an era overwhelmed by vast amounts of data, the effective curation of web-crawl datasets is essential for optimizing model performance. This paper tackles the challenges associated with the unstructured and heterogeneous nature of such datasets. Traditional heuristic curation methods often inadequately capture complex features, resulting in biases and the exclusion of relevant data. We introduce an advanced, learning-driven approach, Ensemble Curation Of DAta ThroUgh Multimodal Operators (EcoDatum), incorporating a novel quality-guided deduplication method to ensure balanced feature distributions. EcoDatum strategically integrates various unimodal and multimodal data curation operators within a weak supervision ensemble framework, utilizing automated optimization to score each data point effectively. EcoDatum, which significantly improves the data curation quality and efficiency, outperforms existing state-of-theart (SOTA) techniques, ranked $1 ^ { \\mathrm { s t } }$ on the DataComp leaderboard, with an average performance score of 0.182 across 38 diverse evaluation datasets. This represents a $28 \\%$ improvement over the DataComp baseline method, demonstrating its effectiveness in improving dataset curation and model training efficiency.\n\nEcoDatum EcoDatum Score:0.997 Score:0.988 A yung ldy rgigae Boxer dg a siars Score:0.530 Score:0.612 You can do these image 2 of Stratton all at home! Wood Windowpane Wall Decor EcoDatum EcoDatum Humor in het Frans IMG_8822 Hahahals Museum 3\n\n# Introduction\n\nThe vast amount of data presents opportunities for training advanced deep learning models, but it also introduces significant noise and irrelevant information, which can hinder model effectiveness. In both academia and industry, the need for robust data curation techniques to extract meaningful signals from extensive digital information has become a pressing concern. In web-crawled datasets, data curation is a multi-faceted task involving various stages and methodologies. The core objective is to identify and retain high-quality samples while discarding noise or mitigating the impact of irrelevant data (Hoffmann et al. 2022). This process is crucial for optimizing model performance in the deep learning framework.\n\nWeb-crawled data is inherently unstructured, diverse, and constantly evolving, making it essential to develop adaptive curation methods capable of handling such complexity(Hoffmann et al. 2022). Traditionally, data curation methods have relied heavily on heuristic filtering approaches based on manually defined content attributes, such as image resolution, graphic geometry, textual length, and linguistic complexity. While these heuristic methods provide a basic means of recognizing low-quality samples, they fail to adequately capture the subtle features of web-crawled data and may introduce biases or overlook relevant information (Mindermann et al. 2022; Maini, Yaghini, and Papernot 2021). To address these limitations, researchers are increasingly adopting automated curation methods that leverage deep learning techniques, including natural language processing, computer vision and cross-modal representation learning, to achieve a balance of quality and quantity.(Schuhmann et al. 2021; Brown 2020; Torkashvand, Jameii, and Reza 2023).\n\nThis research proposes a data curation framework called EcoDatum to address the aforementioned issues. Specifically, we implement a range of efficient data curation strategies as operators, to enhance the data curation process and to achieve cross-modal data alignment at various levels of granularity, as detailed in Figure 1. However, a simple combination of these operators may introduce bias and lead to insufficient utilization of their individual strengths. To fully capture their synergies, we develop a weak supervision ensemble framework that integrates these operators, achieving a synergistic effect. Furthermore, to enhance the integration efficiency of unimodal and multimodal operators, EcoDatum introduces an automated optimization approach. This is achieved by tuning the weak supervision integration module using a composite metric and a tiny-labeled dataset.\n\nAs a novel weak supervision-based framework for multimodal data curation, EcoDatum achieves state-of-the-art performance on the DataComp data filtering track(Gadre et al. 2024). The visual language model trained on curated data demonstrates outstanding results over 38 downstream tasks, highlighting its strong generalizability. Extended experiments demonstrate the effectiveness of this research in understanding various operators in cross-modal data management, offering insights for future work.\n\nOur main contributions are as follows:\n\n1. We propose an auto-optimized ensemble framework, EcoDatum, which integrates techniques to enhance data quality and curate multimodal data, ensuring aligned, high-quality inputs for visual language pretraining.   \n2. We introduce a search-based optimization algorithm for weak supervision labeling function tuning that enhances the curation process and boosts the system’s robustness.   \n3. EcoDatum surpasses existing state-of-the-art techniques in the DataComp benchmark over 38 downstream tasks and ranks $1 ^ { \\mathrm { s t } }$ on the leaderboard1.\n\n# Related Work\n\n# Data Curation for Web-crawled Datasets\n\nRecent research underscores the critical role of data curation in enhancing model performance with large-scale image-text datasets. Various studies focus on improving dataset quality through curation methods, such as enhancing the descriptiveness and cross-modal feature alignment of imagetext pairs, and reducing redundancy (Radenovic et al. 2023; Nguyen et al. 2024; Abbas et al. 2023).\n\nIn a broader context, DataComp is a benchmark designed to evaluate the performance of multimodal models on largescale, real-world datasets(Gadre et al. 2023). Recent advancements in the DataComp benchmark highlight notable progress in data curation techniques. Yokoo et al.(Yokoo et al. 2023) advanced data filtering using image-text similarity and caption modification, achieving notable progress in the Filtering and Bring Your Own Data (BYOD) Tracks. Yu et al.(Yu et al. 2023) evaluated data selection criteria’s impact on model performance, while Chen et al.(Chen et al. 2024) introduced DataJuicer for managing large-scale datasets. Nguyen et al.(Nguyen et al. 2024) enhanced image captioning for better modality alignment, and Maini et al.(Maini et al. 2023) presented T-MARS for improved visual representation by bypassing text feature learning. Additionally, significant contributions have been made in the areas of synthetic data, contrastive learning, image-text alignment, and few-shot learning(Chen et al. 2020; Radford et al. 2021; Jia et al. 2021; Li et al. 2022; Alayrac et al. 2022).\n\nDespite the significant advancements in data curation achieved by Radenovic et al.(Radenovic et al. 2023) and Nguyen et al.(Nguyen et al. 2024) through enhancing data relevance, existing automated filtering methods may still exclude valuable but less conventional data points or introduce biases by focusing too narrowly on specific aspects, potentially overlooking broader contextual information.\n\n# Ensemble Learning\n\nEnsemble learning, which combines multiple models to improve performance and generalization, has long been a foundational approach in machine learning. Classic methods such as Bagging(Breiman 1996) and Boosting (Freund and Schapire 1997) initially demonstrated how model aggregation could reduce variance and improve accuracy.\n\nAfterwards, ensemble learning has been increasingly applied to specialized tasks. Zimek, Schubert, & Kriegel (Zimek, Schubert, and Kriegel 2012) highlighted how ensemble methods enhance outlier detection by aggregating results from multiple models. Beluch et al.(Beluch et al. 2018) demonstrated that ensemble-based uncertainty sampling can significantly improve efficiency in active learning. Rasmus et al. (Rasmus et al. 2015) demonstrated that ensemble techniques enhance semi-supervised learning by effectively utilizing both labeled and unlabeled data. Song et al.(Song et al. 2022) used ensemble methods to improve data quality by detecting and filtering noisy or mislabeled data.\n\nThese advancements illustrate how ensemble techniques refine data preprocessing and improve model inputs.\n\n# Method\n\n# Overview\n\nAs illustrated in Figure 2, EcoDatum enhances the pretraining effectiveness of multimodal models like CLIP(Radford et al. 2021) by strategically selecting high-quality subset $\\hat { S }$ from the original dataset $S$ . This targeted data curation improves the model’s zero-shot performance on diverse tasks. The framework utilizes an ensemble of specialized data operators for comprehensive quality assessment, which addresses various dimensions including image filtering, text analysis, and cross-modal alignment at multiple granular levels. Automated optimization enables the weak supervision system to generate quality scores for data samples, thus minimizing manual input and enhancing the precision of threshold settings. Consequently, EcoDatum streamlines the data curation process and significantly elevates the quality, ensuring the dataset meets the rigorous requirements for model training.\n\nRAW Dataset EcoDatum Modality Curation Operators Ensemble Module n Score:0.891 (12.8M Image-Text Pairs) Unimodal--Text-based Operators 0.336 0.216 A person walking 1 Language Id. Operator ICC Model 0.998 0.521 their dog on the beach ® Quality-guided Operators LF1（x) LF2(x） 一 Deduplication Unimodal -- Image-based Operators Inf. Results Labeling heytA gg Trext Dedupliaian Ae co A pair of zebra standing Multimodal-- Local Alignment Operator Weak Supervision PEIGOER htheringiefmokeo GroundingDINO Model Label Matrix LabelModel IMG.9123 fight Interior wildfires.\n\n# Quality Guided Deduplication\n\nTo improve our dataset’s diversity and distribution, we employ a quality-guided deduplication process that removes redundant text-image pairs. This approach uses perceptual hashing (Farid 2021) to generate hash codes, identifying duplicates based on visual and textual content. Subsequently, the CLIP model assesses the semantic coherence of each duplicate group, allowing us to retain text-image pairs with the highest CLIP scores, as shown in Figure 3. This selective retention enhances the dataset by preserving the most relevant and semantically rich examples, minimizing redundancy while maintaining quality and diversity.\n\n![](images/bf79a59979fe7908305686b4b96c7767a3be6970d9699552d5856c31a5a9dc0e.jpg)  \nFigure 2: Overview of the EcoDatum Framework. EcoDatum utilizes quality-guided deduplication along with an ensemble of unimodal and multimodal data curation operators, that strategically curate multimodal datasets. This integrated approach systematically scores each data point, ensuring optimal quality and alignment for effective visual-language pretraining.   \nFigure 3: Quality Guided Deduplication retains the samples with better cross-modal alignment in duplicate groups to enhance the overall quality and achieve optimal data distributions.\n\n# Unimodal and Multimodal Curation Operators\n\nEcoDatum enhances the quality of multimodal datasets by implementing rigorous unimodal and multimodal curation operators. The unimodal curation operators systematically filter out low-quality visuals and evaluate textual data for concreteness and relevance using both language identification and Image Caption Concreteness (ICC) metric (Yanuka et al. 2024). Multimodal curation integrates these approaches with advanced alignment techniques, employing models like GroundingDINO, an advanced open-set object detector (Liu et al. 2023) for precise local feature alignment and the CLIP model, for global semantic coherence. Together, these strategies ensure the curated dataset is of high quality, with well-aligned multimodal content.\n\nUnimodal Curation Operators. For images, the specific heuristic operators filter out blurred and low-quality visuals. For texts, the FastText (Joulin et al. 2016) model identifies the language and the ICC metric evaluates the relevance and clarity of textual data using a pre-trained autoencoder.\n\nImage-based quality filtering. Low-quality images can severely impact the learning of visual semantics. Our unimodal operators, based on heuristic rules, enhance dataset quality by filtering out images with detrimental attributes. The Geometric Operator targets images with non-standard aspect ratios that distort geometric relationships and compromise visual integrity when resized. Additionally, the DataComp dataset contains many intentionally blurred images to meet privacy standards, which reduces the visual detail crucial for effective model training. The Blurry Operator identifies and removes these excessively blurred images, ensuring that the curated dataset retains high visual quality.\n\nText-based caption assessment. We leverage the FastText model to identify and remove captions in rare languages, enhancing the linguistic consistency of our dataset. Additionally, we use the ICC metric, developed by a pre-trained autoencoder, to independently assess and filter captions. EcoDatum ensures the dataset retains only concrete and relevant captions, directly corresponding to their images.\n\nMultimodal Curation Operators. EcoDatum enhances multimodal data curation by integrating both global and local image-text features, as shown in Figure 4. We employ GroundingDINO for precise local feature alignment, ensuring detailed correspondence between text and images at the object level. Additionally, we utilize the CLIP model, augmented with innovative adaptations, to maintain global semantic coherence throughout the dataset.\n\nLocal Cross-Modal Feature Alignment. We utilize GroundingDINO for the precise alignment of text descriptions with corresponding visual content. It integrates and analyzes text and visual data, effectively identifying relevant phrases in captions and accurately localizing associated visual elements within images, ensuring precise text-to-image alignment without prompt modification.\n\nTo quantitatively assess the alignment between text and images, we develop a metric based on the count of bounding\n\nGroundingDINO Original Image Detection Results Correponding Image Sausage pizza being cut with traditonal scissors Description on table being cut by woman Reklamni fotografie CLIP Image CLIPText Encoder Vertical Flipped / Horizontal Flipped Encoder /Original Image Cosine Similarity Score\n\nboxes with confidence scores exceeding a predefined threshold, as shown in Eq (1). This metric serves to highlight the degree of correspondence between textual descriptions and visual representations. A higher count of accurate detections indicates richer, more detailed scenes, signifying that these data points are of higher value for training and subsequent applications. Data points that do not meet this threshold can be effectively filtered out, including those where the described objects do not visually correspond to the images or where the textual descriptions are insufficiently specific. This ensures our dataset excludes mismatches and generalities, retaining only high-quality, relevant multimodal content.\n\n$$\nC o u n t _ { \\mathrm { G r o u n d i n g D I N O } } = \\sum _ { i = 1 } ^ { n } \\left\\{ \\mathbf { x _ { i } } > \\mathbf { t } \\right\\}\n$$\n\nwhere $x _ { i }$ represents the confidence score of the $i$ -th detected object, $n$ represents the total number of objects detected, and $t$ represents the predefined threshold.\n\nThis operator enhances the ability to curate multimodal data effectively, ensuring that the dataset maintains the most relevant and accurately aligned text-image pairs locally.\n\nGlobal Cross-Modal Feature Alignment. In this module, EcoDatum utilizes the CLIP model, celebrated for its ability to assess the global semantic similarity between text descriptions and their visual counterparts. However, the effectiveness of the CLIP-Score can be compromised when images contain textual content that overlaps with captions. This issue is observed in $40 \\%$ of the LAION dataset(Schuhmann et al. 2022) and $20 \\%$ of the Datacomp dataset (Maini et al. 2023). To mitigate this, we implement an innovative adaptation known as Flip-CLIP, which includes Horizontal-CLIP (H-CLIP) and Vertical-CLIP (V-CLIP) techniques, inspired by (Yu et al. 2023). Before computing the CLIP scores, images are flipped horizontally or vertically, reducing the model’s bias towards text-based features and enabling more equitable evaluations of purely visual elements. The development of Flip-CLIP is motivated by the observation that OCR tasks often disproportionately influence the standard\n\nCLIP score, especially when the image-text is overlapped.\n\nBy integrating both CLIP-Score and Flip-CLIP-Score, we foster the model’s ability to learn from visual content independently of textual influences, thereby enhancing EcoDatum’s capability to process and understand global visual features without excessive bias towards textual elements.\n\n# Modality Operators Ensemble\n\nGiven the vast volume of data and the high cost associated with obtaining high-quality labeled data, the availability of reliable labels is often limited. EcoDatum introduces a weak supervision labeling system that allows the efficient generation of quality-indicated labels at scale, mitigating the challenges of data scarcity and ensuring a more robust data quality assessment. In this study, data curation is abstracted as a data quality discrimination task, aiming to identify “highquality” data. This ensemble-based system further enhances the capabilities of the data operators described above.\n\nSpecifically, EcoDatum employs a weak supervision ensemble model called LabelModel (Ratner et al. 2017; Bach et al. 2019) into the scope of data curation research, which integrates signal sources abstracted from unimodal and multimodal operators for data quality evaluation. This integration balances the limitations of individual operators and significantly reduces their erroneous impacts.\n\nEach operator serves as an independent weak supervision signal source, assessing data quality from its unique dimension. The integration approach in this work uses LabelModel to combine multiple operators, automatically inferring a data quality score for each data sample by modeling the accuracy and relationships of these operators.\n\nThis process begins by matching each operator with corresponding labeling functions (LFs) (Ratner et al. 2017), which converts the operator’s inferred score $s$ of the data sample $x _ { i }$ into weak supervision label $L$ , as shown in Eq (2). The LFs compute operators’ inference results with the mean value $b$ and the standard deviation $\\beta$ of the decision boundary to transform continuous scores into discrete labels. These labels are then aggregated to form a comprehensive weak supervision label matrix. In this context, weak supervision labeling with “Abstain” addresses situations where LFs face unclear features or inapplicable rules. Allowing the LabelModel to abstain from assigning labels in these cases prevents the generation of incorrect labels. This approach enhances the LabelModel’s ability to integrate diverse LFs by learning transformed matrix, particularly when they exhibit different biases and error patterns, thereby increasing the model’s robustness when handling heterogeneous data.\n\n$$\nL _ { x _ { i } j } = \\left\\{ \\begin{array} { l l } { 1 , } & { \\mathrm { i f ~ } s _ { x _ { i } j } \\geq b _ { j } + \\beta _ { j } \\quad \\mathrm { ( S e l e c t e d ) } } \\\\ { 0 , } & { \\mathrm { i f ~ } s _ { x _ { i } j } \\leq b _ { j } - \\beta _ { j } \\quad \\mathrm { ( F i l t e r e d ) } } \\\\ { - 1 , } & { \\mathrm { i f ~ } b _ { j } - \\beta _ { j } < s _ { x _ { i } j } < b _ { j } + \\beta _ { j } \\quad \\mathrm { ( A b s t a i n ) } } \\end{array} \\right.\n$$\n\nThe LabelModel learns the transformed weak supervision label matrix $L _ { M }$ , estimating the weight $w _ { j }$ for each LF. These weights are used to combine the outputs of all LFs, ultimately generating a score for each data sample, which determines whether it is retained or filtered out. This\n\nLFs Combination_i LabelModel Metric $_ 1 = 1 . 4 3$ for LFs Combination_ LF1 Transformations, Evaluation Metric $_ 2 = 1 . 9 8$ for LFs Combination_2 $L F _ { 2 }$ LF3 Metric3 $= 2 . 4 1$ for LFs Combination_ Raw Data Operators' Weak Supervision Label Matrix Metric $_ 4 = 0 . 7 8$ for LFs Combination_ Parallel Score Matrix LFn Iteratively Updating Candidate LFs Tiny-Labeled Evaluation Dataset (\\~10w Image-Text Pairs with Labels) Rank by Metrics globalFiltered,Selected,Abstain=, def $\\mathsf { L F } _ { 1 } ( \\mathsf { x } , \\mathsf { B } _ { 1 } , \\mathsf { B e } \\mathsf { t } \\mathsf { a } _ { 1 } ) ;$ return Seleted def LFf $x > = B _ { n } + B e + a _ { n } \\cdot$ return Selected Abcycl wit wheel Man.png elif $x < = B _ { 1 }$ -Betal:return Filtered elif $x < = B _ { n } - B e + a _ { n } ;$ return Filtered A bathroom with O W.G. 96 Best LFs Combination' else: return Abstain else: return Abstain baby blue wall Metric $= 2 . 4 1$ Candidate LFs Collected From Operators Score Distribution Statistics\n\napproach enhances the comprehensiveness and robustness of data quality evaluation, which ultimately allows the LabelModel to score all raw data and reflect quality.\n\n# Search-based Optimization\n\nA novel search-based optimization method is introduced to enhance the design of LFs, improving the generation of a more accurate weak supervision label matrix for LabelModel modeling, as shown in Figure 5. This method addresses the challenge of converting operator-derived scores into labels by automatically optimizing LFs, reducing the need for manual experimentation. To further optimize the performance of the ensemble, EcoDatum proposes a composite metric that integrates the LabelModel’s data quality assessment capability with the attributes of LFs combination from the transformation steps, enabling a refined weak supervision label matrix. This approach enhances the LabelModel’s ability to analyze operator interrelations and importance, producing quality scores for data samples that closely approximate the ideal.\n\nThe evaluation stage automatically constructs a small labeled dataset containing “clean” and “noisy” samples. Clean data, labeled “1”, are sourced from the COCO dataset(Lin et al. 2014), while “noisy” samples, labeled $\\ \" 0 \\cdot \\bf { \\dot { \\sigma } }$ , are randomly sampled from the DataComp dataset to introduce both unimodal and multimodal noise and include added cross-modal noise through image-text pair exchanges. This setup tests the LabelModel’s ability to differentiate data quality via the F1-tiny scores in Eq (3). Importantly, this dataset is only used for assessing the LabelModel’s performance and does not contribute to training the model or optimizing Eq (3) coefficients, ensuring unbiased validation of the LF effectiveness.\n\nTo evaluate the data quality discrimination capacity of the LabelModel after learning generated weak supervision label matrics with different combinations of LFs, this research develops a specialized composite metric, shown in Eq (3), which combines classification metrics against ground\n\n1: Input: 2: Raw dataset Draw, Tiny-labeled dataset Dtiny, Operator $O p$ , Evaluation metrics $M$ , Candidate LF Combinations $L F s C o m b s$ , 3: Output: 4: Optimal LF sComb 5: Initialize $M ^ { * }$ 6: for each $L F s \\in L F s C o m b s \\{$ do 7: Convert weak supervision label matrix $L _ { M }$ from $L F s$ 8: Train LabelModel on $L _ { M }$ 9: Predict and evaluate LabelModel on Dtiny 10: if $M ( L F s ) > M ^ { * }$ then 11: Update ${ \\cal M } ^ { * } \\gets { \\cal M } ( L F s )$ and $L F s ^ { * } \\gets L F s$ 12: end if 13: end for 14: return ${ \\ L F s ^ { * } }$\n\ntruth and further incorporates the attributes of each operators’ LFs, specifically measuring the fOverlap, $f _ { \\mathrm { C o n f l i c t } }$ , and fCoverage. Here, they respectively indicate the frequency of agreement among LFs, the extent of disagreements, and the proportion of data labeled by at least one function.\n\n$$\nM = \\alpha _ { 1 } \\cdot F 1 _ { \\mathrm { t i n y } } + \\alpha _ { 2 } \\cdot f _ { \\mathrm { O v e r l a p } } - \\alpha _ { 3 } \\cdot f _ { \\mathrm { C o n f i c t } } + \\alpha _ { 4 } \\cdot f _ { \\mathrm { C o v e r a g e } }\n$$\n\nHere, $\\alpha _ { 1 } , \\alpha _ { 2 } , \\alpha _ { 3 } , \\alpha _ { 4 }$ are coefficients that are determined through a few rounds of experiments. These coefficients are tuned to optimize the balance between classification performance on the tiny labeled dataset and the contributions from overlap, conflict, and coverage metrics within the weak supervision labeling framework.\n\n$$\nf _ { \\mathrm { O v e r l a p } } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } \\mathbb { I } \\left( \\sum _ { j = 1 } ^ { m } \\mathbb { I } ( L F _ { j } ( x _ { i } ) \\neq 0 ) > 1 \\right)\n$$\n\n$$\nf _ { \\mathrm { C o n f i c t } } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } \\mathbb { I } \\left( \\exists j _ { 1 } \\neq j _ { 2 } , L F _ { j _ { 1 } } ( x _ { i } ) \\neq L F _ { j _ { 2 } } ( x _ { i } ) \\neq 0 \\right)\n$$\n\n$$\nf _ { \\mathrm { C o v e r a g e } } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } \\mathbb { I } \\left( \\sum _ { j = 1 } ^ { m } \\mathbb { I } ( L F _ { j } ( x _ { i } ) \\neq 0 ) \\ge 1 \\right)\n$$\n\nThese metrics capture the model’s effectiveness in integrating weak supervision signals. For example, even if the model achieves a high F1-score $F 1 _ { t i n y }$ on a tiny labeled dataset, significant conflicts between LFs or low coverage might still lead to instability in real-world applications. By optimizing these composite metrics, we can enhance the model’s generalization ability across different datasets.\n\nThe overall optimization process involves repeatedly constructing the LabelModel across candidate LFs combinations and using the results of the aforementioned composite metrics to identify the optimal LabelModel leaned by each weak supervision label matrix, as shown in Algorithm 1. This optimized model is then applied to the final data curation task. By employing the optimized LabelModel, the overall framework can maximize the robustness of data quality assessment, balance the limitations of individual operators, and ultimately enhance overall data efficiency.\n\n# Experiments\n\n# Dataset and Benchmark\n\nThe DataComp benchmark uniquely emphasizes data curation over model development. Unlike typical machine learning competitions that seek the best model with a fixed dataset, DataComp challenges participants to curate optimal datasets using fixed training code. This highlights the crucial role of high-quality, well-curated data in enhancing model performance. We choose the small-scale filtering track to validate the proposed framework EcoDatum, we curate a subset from a vast pool of 12.8 million image-text pairs from Common Crawl, adhering to the competition’s constraints of fixed training parameters and computational budgets. Our objective is to efficiently filter this dataset, ensuring consistency in training iterations regardless of dataset size.\n\nThe effectiveness of our curated dataset is evaluated across 38 diverse datasets, including ImageNet, 6 ImageNet distribution shift datasets, 12 datasets from the Visual Task Adaptation Benchmark, three retrieval datasets, and several others(Gadre et al. 2024). This extensive range of evaluation datasets tested the generalizability and robustness of EcoDatum, providing a comprehensive assessment of their impact on model training across various real-world scenarios.\n\n# Implementation Details\n\nFor the local cross-modal curation operator, we employ the GroundingDINO-based model(Liu et al. 2025) with SwinLarge as the image backbone and BERT-Base (Devlin 2018)\n\nfor encoding text, setting confidence thresholds at 0.1 to retain more potentially feature-aligned data. In global crossmodal curation, we use the CLIP-ViT-Large-14 architecture(Radford et al. 2021). In determining final data volume, we conducted extensive experiments and reviewed related works, concluding that approximately the top $40 \\%$ samples by the EcoDatum quality score after deduplication (around 3.5M) provide the best balance between quality and quantity. Experiments utilized 8 NVIDIA A100 GPUs, The training and evaluation process required 2.5 hours. Data curation for the 12.8 million dataset involved approximately 10 hours.\n\n# Result Analysis\n\nExisting Baselines. Several SOTA methods have previously set benchmarks in data filtering. LAION and CLIP Score utilize the CLIP model to refine datasets, while Datacomp Filtering employs heuristic unimodal operators for targeted data refinement (Gadre et al. 2023). The HYPerbolic Entailment (HYPE) Filtering technique(Kim et al. 2024) enhances data quality by integrating unimodal specificity with crossmodal alignment. LINE’s strategy leverages large models for web data curation (Yokoo et al. 2023). The TextMasking and Re-Scoring (T-MARS) method corrects imbalances where textual features overpower visual ones (Maini et al. 2023), and the University of Wisconsin-Madison’s (WS) approach utilizes an ensemble of object detection methods to optimize data filtering (Huang et al. 2024).\n\nPerformance Comparison. Building upon these foundations, EcoDatum enhances both efficiency and model training outcomes. As outlined in Table 1, using only $3 . 5 \\ \\mathrm { m i l } \\cdot$ - lion data pairs from the original 12.8 million, EcoDatum achieved the highest average score of 0.182. This surpasses the performance of established methods like T-MARS and WS, both of which scored 0.180 across 38 diverse evaluation datasets. This curation strategy not only reduces computational overhead by $72 \\%$ but also significantly improves data quality. EcoDatum exceeds the “No Filtering” baseline score of 0.132 and the Datacomp Basic filtering score of 0.142 by $28 \\%$ . The integration of advanced methodologies like our optimized LabelModel for labeling functions tuning further refines the data curation process, setting new benchmarks in multimodal applications. The empirical results robustly validate our hypothesis that smaller, wellcurated datasets can outperform larger, unfiltered datasets, underscoring the effectiveness of EcoDatum. Moreover, additional experiments show that EcoDatum consistently improves performance and scales effectively with increasing dataset size.\n\nIn this study, we introduce a composite metric designed to automatically optimize the generation of labeling functions (LFs), thereby facilitating the creation of a more accurate weak supervision label matrix. This optimization directly enhances the learning efficiency of the LabelModel, significantly improving its ability to assess data quality. To validate the effectiveness of this composite metric, we conducted a rigorous experimental case study. The process involved documenting a systematic search to identify the most effective LF combinations and repeatedly evaluating their impact on the average performance across a diverse set of 38 benchmark tasks. The results, depicted in Figure 6, demonstrate a consistent positive correlation between the composite metric scores and the model’s performance, affirming the metric’s utility in refining the data curation process.\n\n<html><body><table><tr><td></td><td>No Filtering</td><td>LAION</td><td>Datacomp</td><td>CLIP</td><td>HYPE</td><td>LINE</td><td>T-MARS</td><td>WS</td><td>Ours</td></tr><tr><td>Dataset Size</td><td>12.8M</td><td>1.3M</td><td>3M</td><td>3.8M</td><td>2.3M</td><td>4.5M</td><td>2.3M</td><td>4.1M</td><td>3.5M</td></tr><tr><td>Avg. Perf.</td><td>0.132</td><td>0.133</td><td>0.142</td><td>0.173</td><td>0.176</td><td>0.177</td><td>0.180</td><td>0.180</td><td>0.182</td></tr></table></body></html>\n\nTable 1: Performance comparison between our method, the Datacomp baseline, and other participants’ approaches.\n\n# Ablation Study\n\nThis experiment conducts a systematic evaluation of data filtering techniques to assess impacts on the performance of the deep learning model, as detailed in Table 2. The “No Filtering” condition acts as the control group.“Random Deduplication” utilizes a stochastic method to eliminate duplicates, indicating that even indiscriminate reductions can improve model performance by balancing feature distribution.\n\nTable 2: Performance comparison of different data curation and ensemble techniques over 38 downtasks.   \nComposite Metric vs.Avg.Perf. Over 38 Tasks   \n\n<html><body><table><tr><td>Methods</td><td>Dataset Size Avg.Perf.</td></tr><tr><td>No Filtering 12.8M</td><td>0.132</td></tr><tr><td>Random Dedup.</td><td>8.8M 0.145</td></tr><tr><td>Quality-Guided Dedup.(QGD)</td><td>8.8M 0.147</td></tr><tr><td>QGD+Ens.(Uni.)</td><td>3.5M 0.154</td></tr><tr><td>QGD+Ens.(Mul.)</td><td>3.5M 0.164</td></tr><tr><td>QGD+Ens.(Uni.&Global-Mul.)</td><td>3.5M 0.168</td></tr><tr><td>QGD+Ens.(Uni.&Local-Mul.)</td><td>3.5M 0.155</td></tr><tr><td>Best Perf.</td><td></td></tr><tr><td>QGD+Ens.(Uni.&Mul.)</td><td>3.5M 0.182</td></tr></table></body></html>\n\nThe introduction of QGD achieves a $1 . 4 \\%$ improvement over the random method with the same dataset size. Incorporating a unimodal operators’ ensemble within the QGD framework results in a $4 . 8 \\%$ improvement, while a multimodal operators’ ensemble leads to a more substantial $9 . 5 \\%$ enhancement. These results highlight the efficacy of both unimodal and multimodal operator ensembles in data curation. By integrating QGD with both unimodal and multimodal ensembles, the combined approach outperforms all others, showing a $4 5 . 4 \\%$ improvement in performance compared to the “No Filtering” baseline. These experiments illustrate that EcoDatum strategically integrates advanced deduplication techniques and sophisticated ensemble frameworks to markedly elevate data quality, optimizing the pretraining process for multimodal models.\n\nWe conduct another ablation study to assess the individual contributions of data processing operators in data curation. By applying each operator independently and incrementally adding them, we explored their impact on downstream tasks. This approach allowed us to identify the most effective combinations of operators, significantly streamlining the optimization process. Through meticulous integration and refinement of labeling function (LF) constructions, we determined the most efficient operator combinations, thereby enhancing the accuracy and efficacy of our data curation methods. This conclusion suggests a strategic approach when dealing with massive web data and limited computational resources: focusing on alignment techniques can lead to more efficient data filtering. Such a focus can improve the generalization performance of multimodal models. Potentially, this experiment could pave the way for future research, indicating that more advanced image-text matching techniques might result in even better multimodal curation outcomes.\n\n![](images/79793228fc4eb9f08d1a13997f1ebb034efcff2c1e4dd97a1bc0a3f7b1039a9f.jpg)  \nFigure 6: Composite Metric Validation with Repeated Experimental Downtasks Evaluations. The positive correlation indicates its capability to guide the tuning of the process.\n\n# Conclusion and Future Work\n\nThe volume of web-crawled datasets is rapidly expanding, and training multimodal models with such data are increasingly prevalent. This paper addresses the challenge of variable sample quality in web-crawled datasets by introducing a novel data curation framework, EcoDatum, designed to select high-quality data. EcoDatum begins with quality-guided deduplication to preprocess the data, followed by the integration of unimodal and multimodal operators into a weak supervision ensemble model, LabelModel, and have employed a search-based optimization method to refine the labeling matrix within LabelModel. Our experiments demonstrate robust performance across all evaluated tasks, securing a $1 ^ { \\mathrm { s t } }$ place ranking in the small-scale track of the DataComp benchmark. While this study validated EcoDatum on a small dataset, future work will extend the evaluation to larger datasets. This expansion will further test the scalability of EcoDatum, aiming to solidify its effectiveness and efficiency in enhancing the training of multimodal models with diverse, large-scale web-crawled data.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了网络爬取数据集中样本质量参差不齐的问题，这些数据集通常是非结构化、异构且包含大量噪声的。传统基于启发式的数据筛选方法难以捕捉复杂特征，容易引入偏见或遗漏相关信息。\\n> *   该问题在深度学习的多模态模型训练中至关重要，高质量的数据筛选能显著提升模型性能，减少计算资源浪费。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种名为EcoDatum的自动化数据筛选框架，通过集成多模态数据筛选算子（包括单模态和多模态算子）并结合弱监督集成学习，实现了高效的数据质量评估与筛选。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **贡献1：** 提出了EcoDatum框架，集成多种数据筛选算子，通过弱监督集成学习生成数据质量评分。\\n>   *   **效果：** 在DataComp基准测试中排名第一，平均性能得分为0.182，比基线方法提升28%。\\n> *   **贡献2：** 提出了一种基于搜索的优化方法，自动优化弱监督标签函数的生成。\\n>   *   **效果：** 显著提升了数据筛选的准确性和鲁棒性。\\n> *   **贡献3：** 引入了质量引导的去重方法，提升数据多样性和分布平衡。\\n>   *   **效果：** 在相同数据量下，比随机去重方法性能提升1.4%。\\n> *   **贡献4：** 提出了Flip-CLIP技术，减少文本特征对CLIP评分的影响。\\n>   *   **效果：** 提升了全局语义一致性评估的准确性。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   EcoDatum的核心思想是通过集成多种数据筛选算子（单模态和多模态）并结合弱监督集成学习，生成数据质量评分。其设计哲学是通过自动化优化和集成学习，减少人工干预，提升数据筛选的效率和准确性。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 传统方法依赖启发式规则或单一模态的筛选，难以捕捉复杂特征且容易引入偏见。\\n> *   **本文的改进：** EcoDatum通过集成多模态算子和弱监督学习，实现了更全面的数据质量评估。此外，通过搜索优化方法自动生成标签函数，减少了人工调参的需求。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **质量引导去重：** 使用感知哈希和CLIP模型对重复数据组进行语义一致性评估，保留得分最高的样本。\\n> 2.  **单模态筛选算子：** 对图像和文本分别进行筛选，例如过滤低质量图像（模糊、非标准比例）和低质量文本（罕见语言、不相关描述）。\\n> 3.  **多模态筛选算子：** 使用GroundingDINO进行局部特征对齐，CLIP模型进行全局语义一致性评估。\\n> 4.  **弱监督集成学习：** 将算子输出转换为标签函数，通过LabelModel生成最终数据质量评分。\\n> 5.  **搜索优化：** 使用复合指标优化标签函数的生成，提升筛选准确性。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   LAION、CLIP Score、Datacomp Filtering、HYPE Filtering、LINE、T-MARS、WS。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在平均性能得分上：** EcoDatum在38个下游任务上的平均得分为0.182，显著优于基线模型Datacomp Filtering（0.142）和T-MARS（0.180）。与表现最佳的基线相比，提升了0.002（约1.1%）。\\n> *   **在数据量效率上：** EcoDatum仅使用3.5M数据（原始数据量为12.8M），性能优于使用更大数据量的基线（如LINE使用4.5M数据，得分为0.177）。\\n> *   **在计算效率上：** EcoDatum减少了72%的计算开销，同时性能提升显著。\\n> *   **在消融实验中：** 质量引导去重（QGD）比随机去重性能提升1.4%，集成单模态和多模态算子后性能提升45.4%。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   数据筛选 (Data Curation, N/A)\\n*   弱监督学习 (Weak Supervision Learning, WSL)\\n*   多模态学习 (Multimodal Learning, N/A)\\n*   质量引导去重 (Quality-Guided Deduplication, QGD)\\n*   集成学习 (Ensemble Learning, N/A)\\n*   自动化优化 (Automated Optimization, N/A)\\n*   数据效率 (Data Efficiency, N/A)\\n*   视觉语言模型 (Vision-Language Model, VLM)\\n*   翻转CLIP (Flip-CLIP, N/A)\"\n}\n```"
}