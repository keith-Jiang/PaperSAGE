{
    "source": "Semantic Scholar",
    "arxiv_id": "2502.08211",
    "link": "https://arxiv.org/abs/2502.08211",
    "pdf_link": "https://arxiv.org/pdf/2502.08211.pdf",
    "title": "Quality over Quantity: Boosting Data Efficiency Through Ensembled Multimodal Data Curation",
    "authors": [
        "Jinda Xu",
        "Yuhao Song",
        "Daming Wang",
        "Weiwei Zhao",
        "Minghua Chen",
        "Kangliang Chen",
        "Qinya Li"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "2025-02-12",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "institutions": [
        "Shanghai Key Laboratory of Scalable Computing and Systems",
        "Shanghai Jiao Tong University",
        "HAOMO.AI Technology Co., Ltd."
    ],
    "paper_content": "# Quality over Quantity: Boosting Data Efficiency Through Ensembled Multimodal Data Curation\n\nJinda $\\mathbf { X } \\mathbf { u } ^ { 1 * }$ , Yuhao $\\mathbf { S o n g ^ { 2 * } }$ , Daming Wang2, Weiwei Zhao1, Minghua Chen2, Kangliang Chen2â€ , Qinya $\\mathbf { L i } ^ { \\sharp }$\n\n1Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University 2HAOMO.AI Technology Co., Ltd. daidan2502 $@$ sjtu.edu.cn, {songyuhao, wangdaming}@haomo.ai, zhaoweiwei $@$ sjtu.edu.cn, {chenminghua, chenkangliang} $@$ haomo.ai, qinyali $@$ sjtu.edu.cn\n\n# Abstract\n\nIn an era overwhelmed by vast amounts of data, the effective curation of web-crawl datasets is essential for optimizing model performance. This paper tackles the challenges associated with the unstructured and heterogeneous nature of such datasets. Traditional heuristic curation methods often inadequately capture complex features, resulting in biases and the exclusion of relevant data. We introduce an advanced, learning-driven approach, Ensemble Curation Of DAta ThroUgh Multimodal Operators (EcoDatum), incorporating a novel quality-guided deduplication method to ensure balanced feature distributions. EcoDatum strategically integrates various unimodal and multimodal data curation operators within a weak supervision ensemble framework, utilizing automated optimization to score each data point effectively. EcoDatum, which significantly improves the data curation quality and efficiency, outperforms existing state-of-theart (SOTA) techniques, ranked $1 ^ { \\mathrm { s t } }$ on the DataComp leaderboard, with an average performance score of 0.182 across 38 diverse evaluation datasets. This represents a $28 \\%$ improvement over the DataComp baseline method, demonstrating its effectiveness in improving dataset curation and model training efficiency.\n\nEcoDatum EcoDatum Score:0.997 Score:0.988 A yung ldy rgigae Boxer dg a siars Score:0.530 Score:0.612 You can do these image 2 of Stratton all at home! Wood Windowpane Wall Decor EcoDatum EcoDatum Humor in het Frans IMG_8822 Hahahals Museum 3\n\n# Introduction\n\nThe vast amount of data presents opportunities for training advanced deep learning models, but it also introduces significant noise and irrelevant information, which can hinder model effectiveness. In both academia and industry, the need for robust data curation techniques to extract meaningful signals from extensive digital information has become a pressing concern. In web-crawled datasets, data curation is a multi-faceted task involving various stages and methodologies. The core objective is to identify and retain high-quality samples while discarding noise or mitigating the impact of irrelevant data (Hoffmann et al. 2022). This process is crucial for optimizing model performance in the deep learning framework.\n\nWeb-crawled data is inherently unstructured, diverse, and constantly evolving, making it essential to develop adaptive curation methods capable of handling such complexity(Hoffmann et al. 2022). Traditionally, data curation methods have relied heavily on heuristic filtering approaches based on manually defined content attributes, such as image resolution, graphic geometry, textual length, and linguistic complexity. While these heuristic methods provide a basic means of recognizing low-quality samples, they fail to adequately capture the subtle features of web-crawled data and may introduce biases or overlook relevant information (Mindermann et al. 2022; Maini, Yaghini, and Papernot 2021). To address these limitations, researchers are increasingly adopting automated curation methods that leverage deep learning techniques, including natural language processing, computer vision and cross-modal representation learning, to achieve a balance of quality and quantity.(Schuhmann et al. 2021; Brown 2020; Torkashvand, Jameii, and Reza 2023).\n\nThis research proposes a data curation framework called EcoDatum to address the aforementioned issues. Specifically, we implement a range of efficient data curation strategies as operators, to enhance the data curation process and to achieve cross-modal data alignment at various levels of granularity, as detailed in Figure 1. However, a simple combination of these operators may introduce bias and lead to insufficient utilization of their individual strengths. To fully capture their synergies, we develop a weak supervision ensemble framework that integrates these operators, achieving a synergistic effect. Furthermore, to enhance the integration efficiency of unimodal and multimodal operators, EcoDatum introduces an automated optimization approach. This is achieved by tuning the weak supervision integration module using a composite metric and a tiny-labeled dataset.\n\nAs a novel weak supervision-based framework for multimodal data curation, EcoDatum achieves state-of-the-art performance on the DataComp data filtering track(Gadre et al. 2024). The visual language model trained on curated data demonstrates outstanding results over 38 downstream tasks, highlighting its strong generalizability. Extended experiments demonstrate the effectiveness of this research in understanding various operators in cross-modal data management, offering insights for future work.\n\nOur main contributions are as follows:\n\n1. We propose an auto-optimized ensemble framework, EcoDatum, which integrates techniques to enhance data quality and curate multimodal data, ensuring aligned, high-quality inputs for visual language pretraining.   \n2. We introduce a search-based optimization algorithm for weak supervision labeling function tuning that enhances the curation process and boosts the systemâ€™s robustness.   \n3. EcoDatum surpasses existing state-of-the-art techniques in the DataComp benchmark over 38 downstream tasks and ranks $1 ^ { \\mathrm { s t } }$ on the leaderboard1.\n\n# Related Work\n\n# Data Curation for Web-crawled Datasets\n\nRecent research underscores the critical role of data curation in enhancing model performance with large-scale image-text datasets. Various studies focus on improving dataset quality through curation methods, such as enhancing the descriptiveness and cross-modal feature alignment of imagetext pairs, and reducing redundancy (Radenovic et al. 2023; Nguyen et al. 2024; Abbas et al. 2023).\n\nIn a broader context, DataComp is a benchmark designed to evaluate the performance of multimodal models on largescale, real-world datasets(Gadre et al. 2023). Recent advancements in the DataComp benchmark highlight notable progress in data curation techniques. Yokoo et al.(Yokoo et al. 2023) advanced data filtering using image-text similarity and caption modification, achieving notable progress in the Filtering and Bring Your Own Data (BYOD) Tracks. Yu et al.(Yu et al. 2023) evaluated data selection criteriaâ€™s impact on model performance, while Chen et al.(Chen et al. 2024) introduced DataJuicer for managing large-scale datasets. Nguyen et al.(Nguyen et al. 2024) enhanced image captioning for better modality alignment, and Maini et al.(Maini et al. 2023) presented T-MARS for improved visual representation by bypassing text feature learning. Additionally, significant contributions have been made in the areas of synthetic data, contrastive learning, image-text alignment, and few-shot learning(Chen et al. 2020; Radford et al. 2021; Jia et al. 2021; Li et al. 2022; Alayrac et al. 2022).\n\nDespite the significant advancements in data curation achieved by Radenovic et al.(Radenovic et al. 2023) and Nguyen et al.(Nguyen et al. 2024) through enhancing data relevance, existing automated filtering methods may still exclude valuable but less conventional data points or introduce biases by focusing too narrowly on specific aspects, potentially overlooking broader contextual information.\n\n# Ensemble Learning\n\nEnsemble learning, which combines multiple models to improve performance and generalization, has long been a foundational approach in machine learning. Classic methods such as Bagging(Breiman 1996) and Boosting (Freund and Schapire 1997) initially demonstrated how model aggregation could reduce variance and improve accuracy.\n\nAfterwards, ensemble learning has been increasingly applied to specialized tasks. Zimek, Schubert, & Kriegel (Zimek, Schubert, and Kriegel 2012) highlighted how ensemble methods enhance outlier detection by aggregating results from multiple models. Beluch et al.(Beluch et al. 2018) demonstrated that ensemble-based uncertainty sampling can significantly improve efficiency in active learning. Rasmus et al. (Rasmus et al. 2015) demonstrated that ensemble techniques enhance semi-supervised learning by effectively utilizing both labeled and unlabeled data. Song et al.(Song et al. 2022) used ensemble methods to improve data quality by detecting and filtering noisy or mislabeled data.\n\nThese advancements illustrate how ensemble techniques refine data preprocessing and improve model inputs.\n\n# Method\n\n# Overview\n\nAs illustrated in Figure 2, EcoDatum enhances the pretraining effectiveness of multimodal models like CLIP(Radford et al. 2021) by strategically selecting high-quality subset $\\hat { S }$ from the original dataset $S$ . This targeted data curation improves the modelâ€™s zero-shot performance on diverse tasks. The framework utilizes an ensemble of specialized data operators for comprehensive quality assessment, which addresses various dimensions including image filtering, text analysis, and cross-modal alignment at multiple granular levels. Automated optimization enables the weak supervision system to generate quality scores for data samples, thus minimizing manual input and enhancing the precision of threshold settings. Consequently, EcoDatum streamlines the data curation process and significantly elevates the quality, ensuring the dataset meets the rigorous requirements for model training.\n\nRAW Dataset EcoDatum Modality Curation Operators Ensemble Module n Score:0.891 (12.8M Image-Text Pairs) Unimodal--Text-based Operators 0.336 0.216 A person walking 1 Language Id. Operator ICC Model 0.998 0.521 their dog on the beach Â® Quality-guided Operators LF1ï¼ˆx) LF2(xï¼‰ ä¸€ Deduplication Unimodal -- Image-based Operators Inf. Results Labeling heytA gg Trext Dedupliaian Ae co A pair of zebra standing Multimodal-- Local Alignment Operator Weak Supervision PEIGOER htheringiefmokeo GroundingDINO Model Label Matrix LabelModel IMG.9123 fight Interior wildfires.\n\n# Quality Guided Deduplication\n\nTo improve our datasetâ€™s diversity and distribution, we employ a quality-guided deduplication process that removes redundant text-image pairs. This approach uses perceptual hashing (Farid 2021) to generate hash codes, identifying duplicates based on visual and textual content. Subsequently, the CLIP model assesses the semantic coherence of each duplicate group, allowing us to retain text-image pairs with the highest CLIP scores, as shown in Figure 3. This selective retention enhances the dataset by preserving the most relevant and semantically rich examples, minimizing redundancy while maintaining quality and diversity.\n\n![](images/bf79a59979fe7908305686b4b96c7767a3be6970d9699552d5856c31a5a9dc0e.jpg)  \nFigure 2: Overview of the EcoDatum Framework. EcoDatum utilizes quality-guided deduplication along with an ensemble of unimodal and multimodal data curation operators, that strategically curate multimodal datasets. This integrated approach systematically scores each data point, ensuring optimal quality and alignment for effective visual-language pretraining.   \nFigure 3: Quality Guided Deduplication retains the samples with better cross-modal alignment in duplicate groups to enhance the overall quality and achieve optimal data distributions.\n\n# Unimodal and Multimodal Curation Operators\n\nEcoDatum enhances the quality of multimodal datasets by implementing rigorous unimodal and multimodal curation operators. The unimodal curation operators systematically filter out low-quality visuals and evaluate textual data for concreteness and relevance using both language identification and Image Caption Concreteness (ICC) metric (Yanuka et al. 2024). Multimodal curation integrates these approaches with advanced alignment techniques, employing models like GroundingDINO, an advanced open-set object detector (Liu et al. 2023) for precise local feature alignment and the CLIP model, for global semantic coherence. Together, these strategies ensure the curated dataset is of high quality, with well-aligned multimodal content.\n\nUnimodal Curation Operators. For images, the specific heuristic operators filter out blurred and low-quality visuals. For texts, the FastText (Joulin et al. 2016) model identifies the language and the ICC metric evaluates the relevance and clarity of textual data using a pre-trained autoencoder.\n\nImage-based quality filtering. Low-quality images can severely impact the learning of visual semantics. Our unimodal operators, based on heuristic rules, enhance dataset quality by filtering out images with detrimental attributes. The Geometric Operator targets images with non-standard aspect ratios that distort geometric relationships and compromise visual integrity when resized. Additionally, the DataComp dataset contains many intentionally blurred images to meet privacy standards, which reduces the visual detail crucial for effective model training. The Blurry Operator identifies and removes these excessively blurred images, ensuring that the curated dataset retains high visual quality.\n\nText-based caption assessment. We leverage the FastText model to identify and remove captions in rare languages, enhancing the linguistic consistency of our dataset. Additionally, we use the ICC metric, developed by a pre-trained autoencoder, to independently assess and filter captions. EcoDatum ensures the dataset retains only concrete and relevant captions, directly corresponding to their images.\n\nMultimodal Curation Operators. EcoDatum enhances multimodal data curation by integrating both global and local image-text features, as shown in Figure 4. We employ GroundingDINO for precise local feature alignment, ensuring detailed correspondence between text and images at the object level. Additionally, we utilize the CLIP model, augmented with innovative adaptations, to maintain global semantic coherence throughout the dataset.\n\nLocal Cross-Modal Feature Alignment. We utilize GroundingDINO for the precise alignment of text descriptions with corresponding visual content. It integrates and analyzes text and visual data, effectively identifying relevant phrases in captions and accurately localizing associated visual elements within images, ensuring precise text-to-image alignment without prompt modification.\n\nTo quantitatively assess the alignment between text and images, we develop a metric based on the count of bounding\n\nGroundingDINO Original Image Detection Results Correponding Image Sausage pizza being cut with traditonal scissors Description on table being cut by woman Reklamni fotografie CLIP Image CLIPText Encoder Vertical Flipped / Horizontal Flipped Encoder /Original Image Cosine Similarity Score\n\nboxes with confidence scores exceeding a predefined threshold, as shown in Eq (1). This metric serves to highlight the degree of correspondence between textual descriptions and visual representations. A higher count of accurate detections indicates richer, more detailed scenes, signifying that these data points are of higher value for training and subsequent applications. Data points that do not meet this threshold can be effectively filtered out, including those where the described objects do not visually correspond to the images or where the textual descriptions are insufficiently specific. This ensures our dataset excludes mismatches and generalities, retaining only high-quality, relevant multimodal content.\n\n$$\nC o u n t _ { \\mathrm { G r o u n d i n g D I N O } } = \\sum _ { i = 1 } ^ { n } \\left\\{ \\mathbf { x _ { i } } > \\mathbf { t } \\right\\}\n$$\n\nwhere $x _ { i }$ represents the confidence score of the $i$ -th detected object, $n$ represents the total number of objects detected, and $t$ represents the predefined threshold.\n\nThis operator enhances the ability to curate multimodal data effectively, ensuring that the dataset maintains the most relevant and accurately aligned text-image pairs locally.\n\nGlobal Cross-Modal Feature Alignment. In this module, EcoDatum utilizes the CLIP model, celebrated for its ability to assess the global semantic similarity between text descriptions and their visual counterparts. However, the effectiveness of the CLIP-Score can be compromised when images contain textual content that overlaps with captions. This issue is observed in $40 \\%$ of the LAION dataset(Schuhmann et al. 2022) and $20 \\%$ of the Datacomp dataset (Maini et al. 2023). To mitigate this, we implement an innovative adaptation known as Flip-CLIP, which includes Horizontal-CLIP (H-CLIP) and Vertical-CLIP (V-CLIP) techniques, inspired by (Yu et al. 2023). Before computing the CLIP scores, images are flipped horizontally or vertically, reducing the modelâ€™s bias towards text-based features and enabling more equitable evaluations of purely visual elements. The development of Flip-CLIP is motivated by the observation that OCR tasks often disproportionately influence the standard\n\nCLIP score, especially when the image-text is overlapped.\n\nBy integrating both CLIP-Score and Flip-CLIP-Score, we foster the modelâ€™s ability to learn from visual content independently of textual influences, thereby enhancing EcoDatumâ€™s capability to process and understand global visual features without excessive bias towards textual elements.\n\n# Modality Operators Ensemble\n\nGiven the vast volume of data and the high cost associated with obtaining high-quality labeled data, the availability of reliable labels is often limited. EcoDatum introduces a weak supervision labeling system that allows the efficient generation of quality-indicated labels at scale, mitigating the challenges of data scarcity and ensuring a more robust data quality assessment. In this study, data curation is abstracted as a data quality discrimination task, aiming to identify â€œhighqualityâ€ data. This ensemble-based system further enhances the capabilities of the data operators described above.\n\nSpecifically, EcoDatum employs a weak supervision ensemble model called LabelModel (Ratner et al. 2017; Bach et al. 2019) into the scope of data curation research, which integrates signal sources abstracted from unimodal and multimodal operators for data quality evaluation. This integration balances the limitations of individual operators and significantly reduces their erroneous impacts.\n\nEach operator serves as an independent weak supervision signal source, assessing data quality from its unique dimension. The integration approach in this work uses LabelModel to combine multiple operators, automatically inferring a data quality score for each data sample by modeling the accuracy and relationships of these operators.\n\nThis process begins by matching each operator with corresponding labeling functions (LFs) (Ratner et al. 2017), which converts the operatorâ€™s inferred score $s$ of the data sample $x _ { i }$ into weak supervision label $L$ , as shown in Eq (2). The LFs compute operatorsâ€™ inference results with the mean value $b$ and the standard deviation $\\beta$ of the decision boundary to transform continuous scores into discrete labels. These labels are then aggregated to form a comprehensive weak supervision label matrix. In this context, weak supervision labeling with â€œAbstainâ€ addresses situations where LFs face unclear features or inapplicable rules. Allowing the LabelModel to abstain from assigning labels in these cases prevents the generation of incorrect labels. This approach enhances the LabelModelâ€™s ability to integrate diverse LFs by learning transformed matrix, particularly when they exhibit different biases and error patterns, thereby increasing the modelâ€™s robustness when handling heterogeneous data.\n\n$$\nL _ { x _ { i } j } = \\left\\{ \\begin{array} { l l } { 1 , } & { \\mathrm { i f ~ } s _ { x _ { i } j } \\geq b _ { j } + \\beta _ { j } \\quad \\mathrm { ( S e l e c t e d ) } } \\\\ { 0 , } & { \\mathrm { i f ~ } s _ { x _ { i } j } \\leq b _ { j } - \\beta _ { j } \\quad \\mathrm { ( F i l t e r e d ) } } \\\\ { - 1 , } & { \\mathrm { i f ~ } b _ { j } - \\beta _ { j } < s _ { x _ { i } j } < b _ { j } + \\beta _ { j } \\quad \\mathrm { ( A b s t a i n ) } } \\end{array} \\right.\n$$\n\nThe LabelModel learns the transformed weak supervision label matrix $L _ { M }$ , estimating the weight $w _ { j }$ for each LF. These weights are used to combine the outputs of all LFs, ultimately generating a score for each data sample, which determines whether it is retained or filtered out. This\n\nLFs Combination_i LabelModel Metric $_ 1 = 1 . 4 3$ for LFs Combination_ LF1 Transformations, Evaluation Metric $_ 2 = 1 . 9 8$ for LFs Combination_2 $L F _ { 2 }$ LF3 Metric3 $= 2 . 4 1$ for LFs Combination_ Raw Data Operators' Weak Supervision Label Matrix Metric $_ 4 = 0 . 7 8$ for LFs Combination_ Parallel Score Matrix LFn Iteratively Updating Candidate LFs Tiny-Labeled Evaluation Dataset (\\~10w Image-Text Pairs with Labels) Rank by Metrics globalFiltered,Selected,Abstain=, def $\\mathsf { L F } _ { 1 } ( \\mathsf { x } , \\mathsf { B } _ { 1 } , \\mathsf { B e } \\mathsf { t } \\mathsf { a } _ { 1 } ) ;$ return Seleted def LFf $x > = B _ { n } + B e + a _ { n } \\cdot$ return Selected Abcycl wit wheel Man.png elif $x < = B _ { 1 }$ -Betal:return Filtered elif $x < = B _ { n } - B e + a _ { n } ;$ return Filtered A bathroom with O W.G. 96 Best LFs Combination' else: return Abstain else: return Abstain baby blue wall Metric $= 2 . 4 1$ Candidate LFs Collected From Operators Score Distribution Statistics\n\napproach enhances the comprehensiveness and robustness of data quality evaluation, which ultimately allows the LabelModel to score all raw data and reflect quality.\n\n# Search-based Optimization\n\nA novel search-based optimization method is introduced to enhance the design of LFs, improving the generation of a more accurate weak supervision label matrix for LabelModel modeling, as shown in Figure 5. This method addresses the challenge of converting operator-derived scores into labels by automatically optimizing LFs, reducing the need for manual experimentation. To further optimize the performance of the ensemble, EcoDatum proposes a composite metric that integrates the LabelModelâ€™s data quality assessment capability with the attributes of LFs combination from the transformation steps, enabling a refined weak supervision label matrix. This approach enhances the LabelModelâ€™s ability to analyze operator interrelations and importance, producing quality scores for data samples that closely approximate the ideal.\n\nThe evaluation stage automatically constructs a small labeled dataset containing â€œcleanâ€ and â€œnoisyâ€ samples. Clean data, labeled â€œ1â€, are sourced from the COCO dataset(Lin et al. 2014), while â€œnoisyâ€ samples, labeled $\\ \" 0 \\cdot \\bf { \\dot { \\sigma } }$ , are randomly sampled from the DataComp dataset to introduce both unimodal and multimodal noise and include added cross-modal noise through image-text pair exchanges. This setup tests the LabelModelâ€™s ability to differentiate data quality via the F1-tiny scores in Eq (3). Importantly, this dataset is only used for assessing the LabelModelâ€™s performance and does not contribute to training the model or optimizing Eq (3) coefficients, ensuring unbiased validation of the LF effectiveness.\n\nTo evaluate the data quality discrimination capacity of the LabelModel after learning generated weak supervision label matrics with different combinations of LFs, this research develops a specialized composite metric, shown in Eq (3), which combines classification metrics against ground\n\n1: Input: 2: Raw dataset Draw, Tiny-labeled dataset Dtiny, Operator $O p$ , Evaluation metrics $M$ , Candidate LF Combinations $L F s C o m b s$ , 3: Output: 4: Optimal LF sComb 5: Initialize $M ^ { * }$ 6: for each $L F s \\in L F s C o m b s \\{$ do 7: Convert weak supervision label matrix $L _ { M }$ from $L F s$ 8: Train LabelModel on $L _ { M }$ 9: Predict and evaluate LabelModel on Dtiny 10: if $M ( L F s ) > M ^ { * }$ then 11: Update ${ \\cal M } ^ { * } \\gets { \\cal M } ( L F s )$ and $L F s ^ { * } \\gets L F s$ 12: end if 13: end for 14: return ${ \\ L F s ^ { * } }$\n\ntruth and further incorporates the attributes of each operatorsâ€™ LFs, specifically measuring the fOverlap, $f _ { \\mathrm { C o n f l i c t } }$ , and fCoverage. Here, they respectively indicate the frequency of agreement among LFs, the extent of disagreements, and the proportion of data labeled by at least one function.\n\n$$\nM = \\alpha _ { 1 } \\cdot F 1 _ { \\mathrm { t i n y } } + \\alpha _ { 2 } \\cdot f _ { \\mathrm { O v e r l a p } } - \\alpha _ { 3 } \\cdot f _ { \\mathrm { C o n f i c t } } + \\alpha _ { 4 } \\cdot f _ { \\mathrm { C o v e r a g e } }\n$$\n\nHere, $\\alpha _ { 1 } , \\alpha _ { 2 } , \\alpha _ { 3 } , \\alpha _ { 4 }$ are coefficients that are determined through a few rounds of experiments. These coefficients are tuned to optimize the balance between classification performance on the tiny labeled dataset and the contributions from overlap, conflict, and coverage metrics within the weak supervision labeling framework.\n\n$$\nf _ { \\mathrm { O v e r l a p } } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } \\mathbb { I } \\left( \\sum _ { j = 1 } ^ { m } \\mathbb { I } ( L F _ { j } ( x _ { i } ) \\neq 0 ) > 1 \\right)\n$$\n\n$$\nf _ { \\mathrm { C o n f i c t } } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } \\mathbb { I } \\left( \\exists j _ { 1 } \\neq j _ { 2 } , L F _ { j _ { 1 } } ( x _ { i } ) \\neq L F _ { j _ { 2 } } ( x _ { i } ) \\neq 0 \\right)\n$$\n\n$$\nf _ { \\mathrm { C o v e r a g e } } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } \\mathbb { I } \\left( \\sum _ { j = 1 } ^ { m } \\mathbb { I } ( L F _ { j } ( x _ { i } ) \\neq 0 ) \\ge 1 \\right)\n$$\n\nThese metrics capture the modelâ€™s effectiveness in integrating weak supervision signals. For example, even if the model achieves a high F1-score $F 1 _ { t i n y }$ on a tiny labeled dataset, significant conflicts between LFs or low coverage might still lead to instability in real-world applications. By optimizing these composite metrics, we can enhance the modelâ€™s generalization ability across different datasets.\n\nThe overall optimization process involves repeatedly constructing the LabelModel across candidate LFs combinations and using the results of the aforementioned composite metrics to identify the optimal LabelModel leaned by each weak supervision label matrix, as shown in Algorithm 1. This optimized model is then applied to the final data curation task. By employing the optimized LabelModel, the overall framework can maximize the robustness of data quality assessment, balance the limitations of individual operators, and ultimately enhance overall data efficiency.\n\n# Experiments\n\n# Dataset and Benchmark\n\nThe DataComp benchmark uniquely emphasizes data curation over model development. Unlike typical machine learning competitions that seek the best model with a fixed dataset, DataComp challenges participants to curate optimal datasets using fixed training code. This highlights the crucial role of high-quality, well-curated data in enhancing model performance. We choose the small-scale filtering track to validate the proposed framework EcoDatum, we curate a subset from a vast pool of 12.8 million image-text pairs from Common Crawl, adhering to the competitionâ€™s constraints of fixed training parameters and computational budgets. Our objective is to efficiently filter this dataset, ensuring consistency in training iterations regardless of dataset size.\n\nThe effectiveness of our curated dataset is evaluated across 38 diverse datasets, including ImageNet, 6 ImageNet distribution shift datasets, 12 datasets from the Visual Task Adaptation Benchmark, three retrieval datasets, and several others(Gadre et al. 2024). This extensive range of evaluation datasets tested the generalizability and robustness of EcoDatum, providing a comprehensive assessment of their impact on model training across various real-world scenarios.\n\n# Implementation Details\n\nFor the local cross-modal curation operator, we employ the GroundingDINO-based model(Liu et al. 2025) with SwinLarge as the image backbone and BERT-Base (Devlin 2018)\n\nfor encoding text, setting confidence thresholds at 0.1 to retain more potentially feature-aligned data. In global crossmodal curation, we use the CLIP-ViT-Large-14 architecture(Radford et al. 2021). In determining final data volume, we conducted extensive experiments and reviewed related works, concluding that approximately the top $40 \\%$ samples by the EcoDatum quality score after deduplication (around 3.5M) provide the best balance between quality and quantity. Experiments utilized 8 NVIDIA A100 GPUs, The training and evaluation process required 2.5 hours. Data curation for the 12.8 million dataset involved approximately 10 hours.\n\n# Result Analysis\n\nExisting Baselines. Several SOTA methods have previously set benchmarks in data filtering. LAION and CLIP Score utilize the CLIP model to refine datasets, while Datacomp Filtering employs heuristic unimodal operators for targeted data refinement (Gadre et al. 2023). The HYPerbolic Entailment (HYPE) Filtering technique(Kim et al. 2024) enhances data quality by integrating unimodal specificity with crossmodal alignment. LINEâ€™s strategy leverages large models for web data curation (Yokoo et al. 2023). The TextMasking and Re-Scoring (T-MARS) method corrects imbalances where textual features overpower visual ones (Maini et al. 2023), and the University of Wisconsin-Madisonâ€™s (WS) approach utilizes an ensemble of object detection methods to optimize data filtering (Huang et al. 2024).\n\nPerformance Comparison. Building upon these foundations, EcoDatum enhances both efficiency and model training outcomes. As outlined in Table 1, using only $3 . 5 \\ \\mathrm { m i l } \\cdot$ - lion data pairs from the original 12.8 million, EcoDatum achieved the highest average score of 0.182. This surpasses the performance of established methods like T-MARS and WS, both of which scored 0.180 across 38 diverse evaluation datasets. This curation strategy not only reduces computational overhead by $72 \\%$ but also significantly improves data quality. EcoDatum exceeds the â€œNo Filteringâ€ baseline score of 0.132 and the Datacomp Basic filtering score of 0.142 by $28 \\%$ . The integration of advanced methodologies like our optimized LabelModel for labeling functions tuning further refines the data curation process, setting new benchmarks in multimodal applications. The empirical results robustly validate our hypothesis that smaller, wellcurated datasets can outperform larger, unfiltered datasets, underscoring the effectiveness of EcoDatum. Moreover, additional experiments show that EcoDatum consistently improves performance and scales effectively with increasing dataset size.\n\nIn this study, we introduce a composite metric designed to automatically optimize the generation of labeling functions (LFs), thereby facilitating the creation of a more accurate weak supervision label matrix. This optimization directly enhances the learning efficiency of the LabelModel, significantly improving its ability to assess data quality. To validate the effectiveness of this composite metric, we conducted a rigorous experimental case study. The process involved documenting a systematic search to identify the most effective LF combinations and repeatedly evaluating their impact on the average performance across a diverse set of 38 benchmark tasks. The results, depicted in Figure 6, demonstrate a consistent positive correlation between the composite metric scores and the modelâ€™s performance, affirming the metricâ€™s utility in refining the data curation process.\n\n<html><body><table><tr><td></td><td>No Filtering</td><td>LAION</td><td>Datacomp</td><td>CLIP</td><td>HYPE</td><td>LINE</td><td>T-MARS</td><td>WS</td><td>Ours</td></tr><tr><td>Dataset Size</td><td>12.8M</td><td>1.3M</td><td>3M</td><td>3.8M</td><td>2.3M</td><td>4.5M</td><td>2.3M</td><td>4.1M</td><td>3.5M</td></tr><tr><td>Avg. Perf.</td><td>0.132</td><td>0.133</td><td>0.142</td><td>0.173</td><td>0.176</td><td>0.177</td><td>0.180</td><td>0.180</td><td>0.182</td></tr></table></body></html>\n\nTable 1: Performance comparison between our method, the Datacomp baseline, and other participantsâ€™ approaches.\n\n# Ablation Study\n\nThis experiment conducts a systematic evaluation of data filtering techniques to assess impacts on the performance of the deep learning model, as detailed in Table 2. The â€œNo Filteringâ€ condition acts as the control group.â€œRandom Deduplicationâ€ utilizes a stochastic method to eliminate duplicates, indicating that even indiscriminate reductions can improve model performance by balancing feature distribution.\n\nTable 2: Performance comparison of different data curation and ensemble techniques over 38 downtasks.   \nComposite Metric vs.Avg.Perf. Over 38 Tasks   \n\n<html><body><table><tr><td>Methods</td><td>Dataset Size Avg.Perf.</td></tr><tr><td>No Filtering 12.8M</td><td>0.132</td></tr><tr><td>Random Dedup.</td><td>8.8M 0.145</td></tr><tr><td>Quality-Guided Dedup.(QGD)</td><td>8.8M 0.147</td></tr><tr><td>QGD+Ens.(Uni.)</td><td>3.5M 0.154</td></tr><tr><td>QGD+Ens.(Mul.)</td><td>3.5M 0.164</td></tr><tr><td>QGD+Ens.(Uni.&Global-Mul.)</td><td>3.5M 0.168</td></tr><tr><td>QGD+Ens.(Uni.&Local-Mul.)</td><td>3.5M 0.155</td></tr><tr><td>Best Perf.</td><td></td></tr><tr><td>QGD+Ens.(Uni.&Mul.)</td><td>3.5M 0.182</td></tr></table></body></html>\n\nThe introduction of QGD achieves a $1 . 4 \\%$ improvement over the random method with the same dataset size. Incorporating a unimodal operatorsâ€™ ensemble within the QGD framework results in a $4 . 8 \\%$ improvement, while a multimodal operatorsâ€™ ensemble leads to a more substantial $9 . 5 \\%$ enhancement. These results highlight the efficacy of both unimodal and multimodal operator ensembles in data curation. By integrating QGD with both unimodal and multimodal ensembles, the combined approach outperforms all others, showing a $4 5 . 4 \\%$ improvement in performance compared to the â€œNo Filteringâ€ baseline. These experiments illustrate that EcoDatum strategically integrates advanced deduplication techniques and sophisticated ensemble frameworks to markedly elevate data quality, optimizing the pretraining process for multimodal models.\n\nWe conduct another ablation study to assess the individual contributions of data processing operators in data curation. By applying each operator independently and incrementally adding them, we explored their impact on downstream tasks. This approach allowed us to identify the most effective combinations of operators, significantly streamlining the optimization process. Through meticulous integration and refinement of labeling function (LF) constructions, we determined the most efficient operator combinations, thereby enhancing the accuracy and efficacy of our data curation methods. This conclusion suggests a strategic approach when dealing with massive web data and limited computational resources: focusing on alignment techniques can lead to more efficient data filtering. Such a focus can improve the generalization performance of multimodal models. Potentially, this experiment could pave the way for future research, indicating that more advanced image-text matching techniques might result in even better multimodal curation outcomes.\n\n![](images/79793228fc4eb9f08d1a13997f1ebb034efcff2c1e4dd97a1bc0a3f7b1039a9f.jpg)  \nFigure 6: Composite Metric Validation with Repeated Experimental Downtasks Evaluations. The positive correlation indicates its capability to guide the tuning of the process.\n\n# Conclusion and Future Work\n\nThe volume of web-crawled datasets is rapidly expanding, and training multimodal models with such data are increasingly prevalent. This paper addresses the challenge of variable sample quality in web-crawled datasets by introducing a novel data curation framework, EcoDatum, designed to select high-quality data. EcoDatum begins with quality-guided deduplication to preprocess the data, followed by the integration of unimodal and multimodal operators into a weak supervision ensemble model, LabelModel, and have employed a search-based optimization method to refine the labeling matrix within LabelModel. Our experiments demonstrate robust performance across all evaluated tasks, securing a $1 ^ { \\mathrm { s t } }$ place ranking in the small-scale track of the DataComp benchmark. While this study validated EcoDatum on a small dataset, future work will extend the evaluation to larger datasets. This expansion will further test the scalability of EcoDatum, aiming to solidify its effectiveness and efficiency in enhancing the training of multimodal models with diverse, large-scale web-crawled data.",
    "summary": "```json\n{\n  \"core_summary\": \"### ğŸ¯ æ ¸å¿ƒæ¦‚è¦\\n\\n> **é—®é¢˜å®šä¹‰ (Problem Definition)**\\n> *   è®ºæ–‡è§£å†³äº†ç½‘ç»œçˆ¬å–æ•°æ®é›†ä¸­æ ·æœ¬è´¨é‡å‚å·®ä¸é½çš„é—®é¢˜ï¼Œè¿™äº›æ•°æ®é›†é€šå¸¸æ˜¯éç»“æ„åŒ–ã€å¼‚æ„ä¸”åŒ…å«å¤§é‡å™ªå£°çš„ã€‚ä¼ ç»ŸåŸºäºå¯å‘å¼çš„æ•°æ®ç­›é€‰æ–¹æ³•éš¾ä»¥æ•æ‰å¤æ‚ç‰¹å¾ï¼Œå®¹æ˜“å¼•å…¥åè§æˆ–é—æ¼ç›¸å…³ä¿¡æ¯ã€‚\\n> *   è¯¥é—®é¢˜åœ¨æ·±åº¦å­¦ä¹ çš„å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒä¸­è‡³å…³é‡è¦ï¼Œé«˜è´¨é‡çš„æ•°æ®ç­›é€‰èƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ï¼Œå‡å°‘è®¡ç®—èµ„æºæµªè´¹ã€‚\\n\\n> **æ–¹æ³•æ¦‚è¿° (Method Overview)**\\n> *   è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºEcoDatumçš„è‡ªåŠ¨åŒ–æ•°æ®ç­›é€‰æ¡†æ¶ï¼Œé€šè¿‡é›†æˆå¤šæ¨¡æ€æ•°æ®ç­›é€‰ç®—å­ï¼ˆåŒ…æ‹¬å•æ¨¡æ€å’Œå¤šæ¨¡æ€ç®—å­ï¼‰å¹¶ç»“åˆå¼±ç›‘ç£é›†æˆå­¦ä¹ ï¼Œå®ç°äº†é«˜æ•ˆçš„æ•°æ®è´¨é‡è¯„ä¼°ä¸ç­›é€‰ã€‚\\n\\n> **ä¸»è¦è´¡çŒ®ä¸æ•ˆæœ (Contributions & Results)**\\n> *   **è´¡çŒ®1ï¼š** æå‡ºäº†EcoDatumæ¡†æ¶ï¼Œé›†æˆå¤šç§æ•°æ®ç­›é€‰ç®—å­ï¼Œé€šè¿‡å¼±ç›‘ç£é›†æˆå­¦ä¹ ç”Ÿæˆæ•°æ®è´¨é‡è¯„åˆ†ã€‚\\n>   *   **æ•ˆæœï¼š** åœ¨DataCompåŸºå‡†æµ‹è¯•ä¸­æ’åç¬¬ä¸€ï¼Œå¹³å‡æ€§èƒ½å¾—åˆ†ä¸º0.182ï¼Œæ¯”åŸºçº¿æ–¹æ³•æå‡28%ã€‚\\n> *   **è´¡çŒ®2ï¼š** æå‡ºäº†ä¸€ç§åŸºäºæœç´¢çš„ä¼˜åŒ–æ–¹æ³•ï¼Œè‡ªåŠ¨ä¼˜åŒ–å¼±ç›‘ç£æ ‡ç­¾å‡½æ•°çš„ç”Ÿæˆã€‚\\n>   *   **æ•ˆæœï¼š** æ˜¾è‘—æå‡äº†æ•°æ®ç­›é€‰çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚\\n> *   **è´¡çŒ®3ï¼š** å¼•å…¥äº†è´¨é‡å¼•å¯¼çš„å»é‡æ–¹æ³•ï¼Œæå‡æ•°æ®å¤šæ ·æ€§å’Œåˆ†å¸ƒå¹³è¡¡ã€‚\\n>   *   **æ•ˆæœï¼š** åœ¨ç›¸åŒæ•°æ®é‡ä¸‹ï¼Œæ¯”éšæœºå»é‡æ–¹æ³•æ€§èƒ½æå‡1.4%ã€‚\\n> *   **è´¡çŒ®4ï¼š** æå‡ºäº†Flip-CLIPæŠ€æœ¯ï¼Œå‡å°‘æ–‡æœ¬ç‰¹å¾å¯¹CLIPè¯„åˆ†çš„å½±å“ã€‚\\n>   *   **æ•ˆæœï¼š** æå‡äº†å…¨å±€è¯­ä¹‰ä¸€è‡´æ€§è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚\",\n  \"algorithm_details\": \"### âš™ï¸ ç®—æ³•/æ–¹æ¡ˆè¯¦è§£\\n\\n> **æ ¸å¿ƒæ€æƒ³ (Core Idea)**\\n> *   EcoDatumçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡é›†æˆå¤šç§æ•°æ®ç­›é€‰ç®—å­ï¼ˆå•æ¨¡æ€å’Œå¤šæ¨¡æ€ï¼‰å¹¶ç»“åˆå¼±ç›‘ç£é›†æˆå­¦ä¹ ï¼Œç”Ÿæˆæ•°æ®è´¨é‡è¯„åˆ†ã€‚å…¶è®¾è®¡å“²å­¦æ˜¯é€šè¿‡è‡ªåŠ¨åŒ–ä¼˜åŒ–å’Œé›†æˆå­¦ä¹ ï¼Œå‡å°‘äººå·¥å¹²é¢„ï¼Œæå‡æ•°æ®ç­›é€‰çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚\\n\\n> **åˆ›æ–°ç‚¹ (Innovations)**\\n> *   **ä¸å…ˆå‰å·¥ä½œçš„å¯¹æ¯”ï¼š** ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å¯å‘å¼è§„åˆ™æˆ–å•ä¸€æ¨¡æ€çš„ç­›é€‰ï¼Œéš¾ä»¥æ•æ‰å¤æ‚ç‰¹å¾ä¸”å®¹æ˜“å¼•å…¥åè§ã€‚\\n> *   **æœ¬æ–‡çš„æ”¹è¿›ï¼š** EcoDatumé€šè¿‡é›†æˆå¤šæ¨¡æ€ç®—å­å’Œå¼±ç›‘ç£å­¦ä¹ ï¼Œå®ç°äº†æ›´å…¨é¢çš„æ•°æ®è´¨é‡è¯„ä¼°ã€‚æ­¤å¤–ï¼Œé€šè¿‡æœç´¢ä¼˜åŒ–æ–¹æ³•è‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾å‡½æ•°ï¼Œå‡å°‘äº†äººå·¥è°ƒå‚çš„éœ€æ±‚ã€‚\\n\\n> **å…·ä½“å®ç°æ­¥éª¤ (Implementation Steps)**\\n> 1.  **è´¨é‡å¼•å¯¼å»é‡ï¼š** ä½¿ç”¨æ„ŸçŸ¥å“ˆå¸Œå’ŒCLIPæ¨¡å‹å¯¹é‡å¤æ•°æ®ç»„è¿›è¡Œè¯­ä¹‰ä¸€è‡´æ€§è¯„ä¼°ï¼Œä¿ç•™å¾—åˆ†æœ€é«˜çš„æ ·æœ¬ã€‚\\n> 2.  **å•æ¨¡æ€ç­›é€‰ç®—å­ï¼š** å¯¹å›¾åƒå’Œæ–‡æœ¬åˆ†åˆ«è¿›è¡Œç­›é€‰ï¼Œä¾‹å¦‚è¿‡æ»¤ä½è´¨é‡å›¾åƒï¼ˆæ¨¡ç³Šã€éæ ‡å‡†æ¯”ä¾‹ï¼‰å’Œä½è´¨é‡æ–‡æœ¬ï¼ˆç½•è§è¯­è¨€ã€ä¸ç›¸å…³æè¿°ï¼‰ã€‚\\n> 3.  **å¤šæ¨¡æ€ç­›é€‰ç®—å­ï¼š** ä½¿ç”¨GroundingDINOè¿›è¡Œå±€éƒ¨ç‰¹å¾å¯¹é½ï¼ŒCLIPæ¨¡å‹è¿›è¡Œå…¨å±€è¯­ä¹‰ä¸€è‡´æ€§è¯„ä¼°ã€‚\\n> 4.  **å¼±ç›‘ç£é›†æˆå­¦ä¹ ï¼š** å°†ç®—å­è¾“å‡ºè½¬æ¢ä¸ºæ ‡ç­¾å‡½æ•°ï¼Œé€šè¿‡LabelModelç”Ÿæˆæœ€ç»ˆæ•°æ®è´¨é‡è¯„åˆ†ã€‚\\n> 5.  **æœç´¢ä¼˜åŒ–ï¼š** ä½¿ç”¨å¤åˆæŒ‡æ ‡ä¼˜åŒ–æ ‡ç­¾å‡½æ•°çš„ç”Ÿæˆï¼Œæå‡ç­›é€‰å‡†ç¡®æ€§ã€‚\\n\\n> **æ¡ˆä¾‹è§£æ (Case Study)**\\n> *   è®ºæ–‡æœªæ˜ç¡®æä¾›æ­¤éƒ¨åˆ†ä¿¡æ¯ã€‚\",\n  \"comparative_analysis\": \"### ğŸ“Š å¯¹æ¯”å®éªŒåˆ†æ\\n\\n> **åŸºçº¿æ¨¡å‹ (Baselines)**\\n> *   LAIONã€CLIP Scoreã€Datacomp Filteringã€HYPE Filteringã€LINEã€T-MARSã€WSã€‚\\n\\n> **æ€§èƒ½å¯¹æ¯” (Performance Comparison)**\\n> *   **åœ¨å¹³å‡æ€§èƒ½å¾—åˆ†ä¸Šï¼š** EcoDatumåœ¨38ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å¹³å‡å¾—åˆ†ä¸º0.182ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹Datacomp Filteringï¼ˆ0.142ï¼‰å’ŒT-MARSï¼ˆ0.180ï¼‰ã€‚ä¸è¡¨ç°æœ€ä½³çš„åŸºçº¿ç›¸æ¯”ï¼Œæå‡äº†0.002ï¼ˆçº¦1.1%ï¼‰ã€‚\\n> *   **åœ¨æ•°æ®é‡æ•ˆç‡ä¸Šï¼š** EcoDatumä»…ä½¿ç”¨3.5Mæ•°æ®ï¼ˆåŸå§‹æ•°æ®é‡ä¸º12.8Mï¼‰ï¼Œæ€§èƒ½ä¼˜äºä½¿ç”¨æ›´å¤§æ•°æ®é‡çš„åŸºçº¿ï¼ˆå¦‚LINEä½¿ç”¨4.5Mæ•°æ®ï¼Œå¾—åˆ†ä¸º0.177ï¼‰ã€‚\\n> *   **åœ¨è®¡ç®—æ•ˆç‡ä¸Šï¼š** EcoDatumå‡å°‘äº†72%çš„è®¡ç®—å¼€é”€ï¼ŒåŒæ—¶æ€§èƒ½æå‡æ˜¾è‘—ã€‚\\n> *   **åœ¨æ¶ˆèå®éªŒä¸­ï¼š** è´¨é‡å¼•å¯¼å»é‡ï¼ˆQGDï¼‰æ¯”éšæœºå»é‡æ€§èƒ½æå‡1.4%ï¼Œé›†æˆå•æ¨¡æ€å’Œå¤šæ¨¡æ€ç®—å­åæ€§èƒ½æå‡45.4%ã€‚\",\n  \"keywords\": \"### ğŸ”‘ å…³é”®è¯\\n\\n*   æ•°æ®ç­›é€‰ (Data Curation, N/A)\\n*   å¼±ç›‘ç£å­¦ä¹  (Weak Supervision Learning, WSL)\\n*   å¤šæ¨¡æ€å­¦ä¹  (Multimodal Learning, N/A)\\n*   è´¨é‡å¼•å¯¼å»é‡ (Quality-Guided Deduplication, QGD)\\n*   é›†æˆå­¦ä¹  (Ensemble Learning, N/A)\\n*   è‡ªåŠ¨åŒ–ä¼˜åŒ– (Automated Optimization, N/A)\\n*   æ•°æ®æ•ˆç‡ (Data Efficiency, N/A)\\n*   è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Model, VLM)\\n*   ç¿»è½¬CLIP (Flip-CLIP, N/A)\"\n}\n```"
}