{
    "source": "Semantic Scholar",
    "arxiv_id": "2502.10967",
    "link": "https://arxiv.org/abs/2502.10967",
    "pdf_link": "https://arxiv.org/pdf/2502.10967.pdf",
    "title": "Open-Set Cross-Network Node Classification via Unknown-Excluded Adversarial Graph Domain Alignment",
    "authors": [
        "Xiao Shen",
        "Zhihao Chen",
        "Shirui Pan",
        "Shuang Zhou",
        "Laurence T. Yang",
        "Xi Zhou"
    ],
    "categories": [
        "cs.SI"
    ],
    "publication_date": "2025-02-16",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "institutions": [
        "Hainan University",
        "Griffith University",
        "The Hong Kong Polytechnic University",
        "Zhengzhou University",
        "St. Francis Xavier University"
    ],
    "paper_content": "# Open-Set Cross-Network Node Classification via Unknown-Excluded Adversarial Graph Domain Alignment\n\nXiao Shen1, Zhihao Chen1, Shirui Pan2, Shuang Zhou3, Laurence T. Yang4, 5, and Xi Zhou1\\*\n\n1Hainan University 2Griffith University 3The Hong Kong Polytechnic University 4Zhengzhou University\n\n5St. Francis Xavier University\n\nxshen@hainanu.edu.cn, zhchen@hainanu.edu.cn, s.pan@griffith.edu.au, shuang.zhou@connect.polyu.hk, ltyang@ieee.org, xzhou $@$ hainanu.edu.cn\n\n# Abstract\n\nExisting cross-network node classification methods are mainly proposed for closed-set setting, where the source network and the target network share exactly the same label space. Such a setting is restricted in real-world applications, since the target network might contain additional classes that are not present in the source. In this work, we study a more realistic open-set cross-network node classification (OCNNC) problem, where the target network contains all the known classes in the source and further contains several target-private classes unseen in the source. Borrowing the concept from open-set domain adaptation, all target-private classes are defined as an additional ‚Äúunknown‚Äù class. To address the challenging O-CNNC problem, we propose an unknownexcluded adversarial graph domain alignment (UAGA) model with a separate-adapt training strategy. Firstly, UAGA roughly separates known classes from unknown class, by training a graph neural network encoder and a neighborhoodaggregation node classifier in an adversarial framework. Then, unknown-excluded adversarial domain alignment is customized to align only target nodes from known classes with the source, while pushing target nodes from unknown class far away from the source, by assigning positive and negative domain adaptation coefficient to known class nodes and unknown class nodes. Extensive experiments on real-world datasets demonstrate significant outperformance of the proposed UAGA over state-of-the-art methods on O-CNNC.\n\nCode ‚Äîhttps://github.com/3480430977/UAGA\n\n# Introduction\n\nCross-network node classification (CNNC) (Shen et al. 2021) aims to leverage the knowledge obtained from a source network with rich labels to facilitate the classification\n\nËá™ 0 Class 1 D ? Ëá™ DGormapaihn ? ? ? Class 32 ÈÖç È©¨ Adaptation Unknown Class D ? ‚ñ° ? D Labeled ? ? Unlabeled Source Network Target Network Attributes\n\nof nodes in a target network that lacks labels. Existing CNNC methods (Shen et al. 2020; Zhang et al. 2021; Huang, $\\mathrm { { X u } }$ , and Wang 2022; Dai et al. 2023; Qiao et al. 2023; Shao et al. 2024; Shen et al. 2024) are mainly under the closedset assumption that the source and target networks share an identical group of node classes. However, it is hard to guarantee such a closed-set setting in real-world CNNC applications, since we cannot determine whether the source and target networks share the same label space if no target labels are available.\n\nTo break through the restriction of the closed-set assumption, this work studies a more realistic open-set cross-network node classification (O-CNNC) problem, where the target network contains not only all the known classes in the source network, but also an additional ‚Äúunknown‚Äù class (Saito et al. 2018) covering all the target-private classes unseen in the source. Figure 1 illustrates the O-CNNC problem the goal is to classify target nodes from known classes into the corresponding seen classes, and recognize target nodes belonging to target-private classes as ‚Äúunknown‚Äù.\n\nTo effectively address the O-CNNC problem, one need to solve two challenges in essence: 1) Since the target network is without any labels, we do not know which target nodes belong to unknown class. Thus, how to generate a boundary between known classes and unknown class is the first obstacle in O-CNNC. 2) The domain discrepancy would impede a model trained on the source network to be directly applied to a new target network. Moreover, due to the absence of unknown class in the source, directly matching the whole distribution between the source and target networks like existing closed-set CNNC literature would be risky, since the misalignment between target unknown class and source known classes could cause negative transfer and make it more difficult to detect unknown class. Thus, how to align the target distribution with the source while excluding unknown class is the key to success in O-CNNC.\n\nWe propose a novel framework, named unknown-excluded adversarial graph domain alignment (UAGA), to address the challenging O-CNNC problem. The proposed UAGA is trained with a separate-adapt strategy, which roughly separates unknown class prior to unknown-excluded adversarial domain alignment. In separation stage, UAGA learns a rough boundary between unknown class and known classes, by training an attention-based graph neural network (GNN) encoder and a $( K { + } 1 )$ -class neighborhoodaggregation node classifier in an adversarial framework. In adaptation stage, pseudo-label would be assigned to a target node, only if its clustering and classification prediction reach an agreement. Such refined target pseudo-labels would be taken as the supervised signals to iteratively retrain the model in a self-training manner, which yields a refined boundary to separate unknown class from known classes progressively. Moreover, instead of matching the whole distribution between the source and target networks, we propose to conduct unknown-excluded adversarial domain alignment to explicitly exclude unknown class from crossnetwork distribution matching. The conventional adversarial domain adaptation method (Ganin et al. 2016) always assigns positive domain adaptation coefficient to all samples across domains in the gradient reversal layer (GRL). In contrast, we innovatively propose to assign negative (positive) domain adaptation coefficient to nodes belonging to unknown (known) classes. On one hand, the positive domain adaptation coefficient guides the GNN encoder and domain discriminator to compete against each other, so as to learn network-invariant embeddings for known classes. On the other hand, the negative domain adaptation coefficient guides the GNN encoder and domain discriminator to be trained in the same direction to make the embeddings of target unknown class very distinguishable from the source embeddings. As a result, the proposed UAGA aligns the target distribution to the source only for known classes while excluding unknown class. The contributions of this work are summarized as follows:\n\nProblem. We study a novel O-CNNC problem, and provide theoretical analysis of homophily and open-set domain adaptation (OSDA) w.r.t. O-CNNC.\n\nAlgorithm. We propose a novel separate-adapt framework named UAGA to address O-CNNC, which initially learns a rough boundary to separate unknown from known by adversarial learning, followed by unknown-excluded adversarial domain alignment. To our knowledge, UAGA is the first work to employ negative domain adaptation coefficient to exclude unknown class samples from adversarial domain adaptation. Empowered by such design, UAGA can reduce domain discrepancy only for known classes, while pushing target unknown class far away from the source to avoid negative transfer.\n\nExperiment. Extensive experimental results demonstrate the superiority of the proposed UAGA over state-ofthe-art baselines by a large margin.\n\n# Related Work\n\nOpen-set Node Classification rejects unlabeled nodes not belonging to any known classes as an ‚Äúunknown‚Äù class. OpenWGL (Wu, Pan, and Zhu 2020) performs uncertaintyaware node representation learning and separates unknown samples from known ones via a threshold based on the model‚Äôs confidence scores. OODGAT (Song and Wang 2022) introduces an attention mechanism to explicitly distinguish inliers from outliers during feature propagation process of GNN. G2Pxy (Zhang et al. 2023) follows an inductive learning setting where the information about unknown class is unavailable during model training. These open-set node classification methods are all designed for a single-network scenario. In contrast, our work studies open-set node classification in a cross-network scenario, where the source and target networks inherently exhibit diverse data distributions. Without addressing the domain discrepancy for known classes across networks, the single-network-based open-set node classification methods might exhibit sub-optimal performance in the challenging O-CNNC task.\n\nOpen-set Domain Adaptation allows target domain to contain new classes not present in the source. OSBP (Saito et al. 2018) is the most representative OSDA method which trains a classifier and a feature generator by adversarial learning, where the classifier is trained to construct a boundary between known class and unknown class samples, while the generator is trained to make target samples far away from the boundary. STA (Liu et al. 2019) progressively separates the samples of unknown class and known classes, and conducts weighted domain adaptation to only match the distribution of known classes across domains. OMEGA (Ru et al. 2023) designs unknown-aware target clustering to form tight clusters in target domain and generates specific thresholds for each target sample by moving-threshold estimation. Existing OSDA methods are mostly developed for computer vision (CV) field with the assumption of independent and identically distributed (i.i.d.) samples within each domain.\n\nHowever, the graph-structured data obviously violate the i.i.d. assumption, due to complex network connections between nodes (Shen et al. 2020; Wu et al. 2020; Dai et al. 2023). Thus, directly applying existing OSDA methods to address the O-CNNC problem might fail to obtain satisfactory performance.\n\nCross-network Node Classification. CDNE (Shen et al. 2021) is the pioneering CNNC method which employs stacked auto-encoders to reconstruct the proximity matrix of each network. ACDNE (Shen et al. 2020) leverages dual feature extractors to separately learn self-representations from neighbor-representations. AdaGCN (Dai et al. 2023) integrates graph convolution network (GCN) with Wasserstein distance guided adversarial domain adaptation. UDAGCN (Wu et al. 2020) adopts dual GCNs to capture local and global consistency. DMGNN (Shen et al. 2023) devises label-aware propagation scheme to promote intraclass propagation while avoiding inter-class propagation. DGDA (Cai et al. 2024) applies variational graph auto-encoders to disentangle the semantic latent variables, domain latent variables, and random latent variables. A2GNN (Liu et al. 2024) removes propagation layers in source graph and stacks multiple propagation layers in target graph. The aforementioned CNNC methods are all designed upon the closed-set setting, i.e., the source and target networks share exactly the same label space. To reduce domain discrepancy, they adopt either statistical matching (Shen et al. 2021; Wu, He, and Ainsworth 2023) or adversarial learning (Shen et al. 2020; Wu et al. 2020; Dai et al. 2023) techniques to align the whole distribution of the target network with the source network. However, such whole distribution matching is not supposed to perform well in O-CNNC, since aligning target unknown samples with the source would cause negative transfer (Saito et al. 2018).\n\nVery recently, some work investigate the CNNC problem without the closed-set restriction. UDANE (Chen et al. 2023) applies an entropy regularization on target nodes to enforce a separation between unknown class and known classes. SDA (Wang et al. 2024) separates target unknown class from known classes by neighbor center clustering.\n\n# Preliminaries\n\nLet $\\mathcal { G } ^ { s } = ( \\mathcal { V } ^ { s } , \\mathcal { E } ^ { s } , A ^ { s } , X ^ { s } , Y ^ { s } )$ denote a fully labeled source network with a set of nodes $\\mathcal { V } ^ { s }$ and a set of edges $\\mathcal { E } ^ { s }$ , where $A ^ { s } \\in \\{ 0 , 1 \\} ^ { n ^ { s } \\times n ^ { s } }$ , $\\ b { X } ^ { s } \\in \\mathbb { R } ^ { n ^ { s } \\times \\omega }$ and ${ \\cal Y } ^ { s } \\in \\{ 0 , 1 \\bar { \\} } ^ { s _ { \\times { K } } }$ are the adjacency matrix, node attribute matrix and node label matrix of $\\mathcal { G } ^ { s }$ , $\\mathcal { n } ^ { s }$ and $\\omega$ are the number of nodes and attributes, and $K$ is the number of known classes in $\\mathcal { G } ^ { s }$ . Let $\\mathcal { G } ^ { t } =$ $( \\mathcal V ^ { t } , \\mathcal { E } ^ { t } , A ^ { t } , X ^ { t } )$ denote an unlabeled target network with a node set $\\mathcal { V } ^ { t }$ and an edge set $\\mathcal { E } ^ { t }$ , where $\\breve { A ^ { t } } \\in \\{ 0 , 1 \\} ^ { n ^ { t } \\times n ^ { t } }$ and ùëøùë° ‚àà ‚ÑùùìÉùë°√óùúî are the adjacency matrix and node attribute matrix of $\\mathcal { G } ^ { t }$ , and $\\boldsymbol { \\mathscr { n } } ^ { t }$ is the number of nodes in $\\boldsymbol { \\mathcal { G } ^ { t } }$ .\n\nDefinition 1. Open-set Cross-network Node Classification (O-CNNC). Given a fully labeled $\\mathcal { G } ^ { s }$ and a completely unlabeled $\\mathcal { G } ^ { t }$ following two distinct distributions $\\mathbb { P } ^ { s }$ and $\\mathbb { Q } ^ { t }$ . Let $\\mathcal { Y } ^ { s }$ and $\\mathcal { Y } ^ { t o }$ denote the original label space of $\\mathcal { G } ^ { s }$ and $\\mathcal { G } ^ { t }$ , where $\\mathcal { Y } ^ { t o } = \\mathcal { Y } ^ { s } \\cup \\mathcal { Y } ^ { u }$ contains 1) all the known classes in $\\mathcal { Y } ^ { s } = \\{ 1 , \\cdots , K \\}$ , $K \\geq 2$ , and 2) some extra new classes $\\mathcal { Y } ^ { u } \\ ( | \\mathcal { Y } ^ { u } | \\geq 1 )$ unseen in $\\mathcal { Y } ^ { s }$ . Such target-private classes $\\mathcal { Y } ^ { u }$ are all represented by the $( K + 1 )$ -th ‚Äúunknown‚Äù class, since we know nothing about these classes. Accordingly, one can obtain a new label space $\\mathcal { Y } ^ { t } = \\{ 1 , \\cdots , K , K +$ 1}. The goal of O-CNNC is to learn an optimal classifier ùíΩ: $\\mathcal { G } ^ { t } \\to \\mathcal { Y } ^ { t }$ such that 1) the target nodes whose labels belonging to $\\mathcal { Y } ^ { s }$ are classified into one of the first $K$ known classes, and 2) the target nodes whose labels belonging to $\\mathcal { Y } ^ { u }$ are recognized as the $( K { + } 1 )$ -th ‚Äúunknown‚Äù class.\n\nDefinition 2. Homophily Ratio (Pei et al. 2020). Node homophily ratio $\\mathbb { h }$ is the average fraction of neighbors having the same class-label with each central node in graph $\\mathcal { G }$ :\n\n$$\n\\begin{array} { r } { \\mathbb { h } = \\frac { 1 } { | \\mathcal { V } | } \\sum _ { v _ { i } \\in \\mathcal { V } } \\frac { \\{ j \\vert j \\in \\mathcal { N } _ { i } \\wedge y _ { i } = y _ { j } \\} } { | \\mathcal { N } _ { i } | } } \\end{array}\n$$\n\nwhere $\\mathcal { N } _ { i } = \\left\\{ j \\middle | j \\neq i , A _ { i , j } = 1 \\right\\}$ is a set of first-order neighbors of $v _ { i } , y _ { i }$ and $y _ { j }$ are the class-label of $\\boldsymbol { v } _ { i }$ and $v _ { j }$ . A large ratio ùïô implies that $\\mathcal { G }$ is more homophilic w.r.t. the node labels, i.e., connected nodes more tend to share the same classlabel (McPherson, Smith-Lovin, and Cook 2001).\n\nTheorem 1. Homophilic w.r.t. $\\pmb { K } + \\pmb { 1 }$ Classes in OCNNC. Given a target graph $\\mathcal { G } ^ { t }$ with original label space $\\mathcal { Y } ^ { t o } = \\mathcal { Y } ^ { s } \\cup \\mathcal { Y } ^ { u }$ , $\\mathcal { Y } ^ { s } = \\{ 1 , \\cdots , K \\}$ . Assume that there exists a mapping $\\emptyset \\colon \\mathcal { Y } ^ { t o }  \\mathcal { Y } ^ { t } = \\{ 1 , \\cdots , K , K + 1 \\}$ , where all classes in $\\mathcal { Y } ^ { u }$ are represented by a new class $K + 1$ . If $\\boldsymbol { \\mathcal { G } ^ { t } }$ is homophilic w.r.t. $\\mathcal { Y } ^ { t o }$ , then $\\mathcal { G } ^ { t }$ is also homophilic w.r.t. $\\mathcal { Y } ^ { t }$ .\n\nProof: According to Definition 2, the homophily ratio of $\\mathcal { G } ^ { t }$ w.r.t. its original label space $\\mathcal { Y } ^ { t o }$ is measured as:\n\n$$\n\\begin{array} { r } { \\mathbb { h } ^ { \\boldsymbol { y } ^ { t o } } = \\frac { 1 } { | \\boldsymbol { \\nu } ^ { t } | } \\sum _ { \\boldsymbol { v } _ { i } \\in \\mathcal { V } ^ { t } } \\frac { \\left| \\mathcal { N } _ { i } ^ { 1 } \\right| } { \\left| \\mathcal { N } _ { i } \\right| } } \\end{array}\n$$\n\nwhere $\\mathcal { N } _ { i }$ can be divided into two disjoint subsets: 1) $\\mathcal { N } _ { i } ^ { 1 } =$ $\\{ j \\vert j \\in \\mathcal { N } _ { i } \\wedge y _ { i } ^ { t o } = y _ { j } ^ { t o } \\}$ is a set of intra-class neighbors sharing the same class-label with $\\textit { v } _ { i } , \\textit { } 2 )$ $\\mathcal { N } _ { i } ^ { 2 } =$ $\\{ j \\bar { | } j \\in \\mathcal { N } _ { i } \\land y _ { i } ^ { t o } \\neq y _ { j } ^ { t o } \\}$ is a set of inter-class neighbors having different class-labels with $\\boldsymbol { v } _ { i }$ , and $y _ { i } ^ { t o } , y _ { j } ^ { t o } \\in \\mathcal { Y } ^ { t o }$ are the original labels of $v _ { i }$ and $v _ { j }$ .\n\nLet $\\emptyset ( y _ { i } ^ { t o } ) , \\ell \\bigl ( y _ { j } ^ { t o } \\bigr ) \\in \\left\\{ 1 , \\cdots , K , K + 1 \\right\\}$ denote new labels of $v _ { i }$ and $v _ { j }$ after mapping $\\ell \\colon \\mathcal { Y } ^ { t o } \\to \\mathcal { Y } ^ { t }$ . The homophily ratio of $\\mathcal { G } ^ { t }$ w.r.t. new label space $\\mathcal { Y } ^ { t }$ can be derived as:\n\n$$\n\\begin{array} { r } { \\ln ^ { \\mathcal { Y } ^ { t } } = \\frac { 1 } { | \\mathcal { V } ^ { t } | } \\sum _ { v _ { i } \\in \\mathcal { V } ^ { t } } \\frac { | \\mathcal { N } _ { i } ^ { 1 } | + | \\left\\{ j \\right\\} j \\in \\mathcal { N } _ { i } ^ { 2 } \\wedge \\beta ( y _ { i } ^ { t o } ) = \\beta \\left( y _ { j } ^ { t o } \\right) \\big \\} } { | \\mathcal { N } _ { i } | } } \\end{array}\n$$\n\nSince we have $| \\{ j | j \\in \\mathcal { N } _ { i } ^ { 2 } \\land \\ell ( y _ { i } ^ { t o } ) = \\ell \\big ( y _ { j } ^ { t o } \\big ) \\} | \\geq 0$ for any $v _ { j }$ , we can derive that $\\mathrm { \\bar { h } } ^ { \\mathcal { Y } ^ { t } } \\geq \\mathrm { \\bar { h } } ^ { \\mathcal { Y } ^ { t o } }$ . Therefore, if $\\boldsymbol { \\mathcal { G } ^ { \\mathrm { t } } }$ is homophilic w.r.t. original label space $\\mathcal { Y } ^ { t o }$ , then $\\mathcal { G } ^ { \\mathrm { t } }$ should be no less homophilic w.r.t. new label space $\\mathcal { Y } ^ { t }$ containing $K$ known classes and the $( K + 1 )$ -th ‚Äúunknown‚Äù class.\n\nRemark: According to Theorem 1, for target graph with homophily in O-CNNC, nodes no matter belonging to known classes or ‚Äúunknown‚Äù class, tend to connect with others sharing the same class-label. Such connection patterns in graph have been shown to be informative for both known classification and unknown detection (Song and Wang 2022; Wu et al. 2023; Zhou et al. 2023). This naturally motivates us to adopt a GNN encoder and a $( K { + } 1 )$ -class node classifier to aggregate neighborhood information for joint known classification and unknown detection in O-CNNC.\n\n![](images/ef761ad0a47e32301a4d61b9438961360b36bf941b9b8df755cc7cc885ff3ad1.jpg)  \nFigure 2: Model architecture of UAGA with a separate-adapt framework. In separation stage, a GNN encoder and a node classifier are trained in an adversarial manner to roughly separate unknown from known. In adaptation stage, positive and negative domain adaptation coefficients are leveraged to explicitly exclude unknown class from adversarial domain alignment.\n\nTheorem 2. OSDA Generalization Bounds (Fang et al. 2020). Let $\\mathbb { P } ^ { s }$ and $\\mathbb { Q } ^ { t }$ denote the distribution of the source and target domains respectively, $\\pi _ { K + 1 } ^ { t }$ denote the class-prior probability of target unknown class. Given a hypothesis space $\\mathcal { H }$ with a mild condition that the constant value function $K + 1 \\in { \\mathcal { H } }$ , for $\\forall \\mathcal { h } \\in \\mathcal { H }$ , the target risk $R _ { t } ( \\mathscr { h } )$ is bounded by: $\\begin{array} { r } { \\frac { R _ { t } ( \\hbar ) } { 1 - \\pi _ { K + 1 } ^ { t } } \\leq R _ { s } ( \\hbar ) + d i s c \\big ( \\mathbb { Q } _ { X | Y \\leq K } ^ { t } , \\mathbb { P } _ { X } ^ { s } \\big ) + \\frac { \\pi _ { K + 1 } ^ { t } } { 1 - \\pi _ { K + 1 } ^ { t } } R _ { t , K + 1 } ( \\hbar ) + \\varLambda ( 4 } \\end{array}$ where $R _ { s } ( \\hbar )$ is the source risk, $d i s c \\big ( \\mathbb { Q } _ { X | Y \\leq K } ^ { t } , \\mathbb { P } _ { X } ^ { s } \\big )$ is the domain discrepancy upon the $K$ known classes shared by the source and target domains, $( \\pi _ { K + 1 } ^ { t } / ( 1 - \\pi _ { K + 1 } ^ { t } ) ) R _ { t , K + 1 } ( \\hbar )$ is the open-set risk in target domain that can be interpreted as the mis-classification rate of unknown samples, and $\\varLambda$ is the shared error of the joint optimal hypothesis for the source and target domains.\n\nRemark: Note that O-CNNC can be seen as applying OSDA to node classification task across graphs $\\mathcal { G } ^ { s }$ and $\\mathcal { G } ^ { t }$ . According to Theorem 2, in order to reduce target risk in OCNNC, one should 1) minimize the classification error on $\\mathcal { G } ^ { s } , 2 )$ match the domain distribution between $\\mathcal { G } ^ { s }$ and $\\mathcal { G } ^ { t }$ only for known classes, and 3) reduce the open-set risk on $\\mathcal { G } ^ { t }$ (i.e., accurately identify ‚Äúunknown‚Äù class nodes).\n\n# The Proposed UAGA Model\n\nIn this section, we go through the details of the proposed UAGA to address O-CNNC. The framework of UAGA with a separate-adapt strategy is illustrated in Figure 2.\n\n# Adversarial Boundary Separation for Known and Unknown\n\nIn separation stage, a GNN encoder $f _ { G }$ and a $( K { + } 1 )$ -class node classifier $f _ { C }$ are trained in an adversarial manner to learn a rough boundary between unknown class and known classes. The proposed UAGA resorts to graph attention network (GAT) (Veliƒçkoviƒá et al. 2018) to construct the GNN encoder $f _ { G }$ for node embedding learning, as:\n\n$$\n\\begin{array} { r } { \\pmb { h } _ { i } = \\mathrm { R e L U } \\big ( \\sum _ { j \\in \\mathcal { N } _ { i } \\cup \\{ i \\} } \\tilde { A } _ { i , j } \\pmb { W } ^ { G } \\pmb { x } _ { j } \\big ) } \\end{array}\n$$\n\nwhere $\\tilde { A } _ { i , j }$ is adaptive edge weight of $\\left( v _ { i } , v _ { j } \\right)$ learned by GAT, $\\pmb { x } _ { i } \\in \\mathbb { R } ^ { \\omega }$ is input node attribute vector of $v _ { i }$ , $\\pmb { W } ^ { G }$ is learnable parameter, and $\\pmb { h } _ { i }$ is node embedding vector of $v _ { i }$ .\n\nThe conventional OSDA methods (Saito et al. 2018; Feng et al. 2019; Shermin et al. 2020) typically construct a $( K + 1 )$ -class task classifier by a multi-layer perceptron (MLP), to jointly deal with known classification and unknown detection. However, adopting an MLP-based classifier in O-CNNC fails to capture the homophily between connected nodes for graph data, since MLP considers each sample independently. According to Theorem 1, in O-CNNC, for a graph homophilic w.r.t. its original label space, it should be also homophilic w.r.t. the new label space containing $K$ known classes and $( K + 1 )$ -th ‚Äúunknown‚Äù class. Motivated by this, the proposed UAGA constructs a $( K +$ 1)-class neighborhood-aggregation node classifier $f _ { C }$ by a graph attention layer (Veliƒçkoviƒá et al. 2018) to adaptively aggregate the logits among the neighborhood for all $\\textstyle K + 1$ dimensions, as:\n\n$$\n\\begin{array} { r } { \\pmb { \\mathcal { \\hat { y } } } _ { i } = \\mathrm { S o f t m a x } \\big ( \\sum _ { j \\in \\mathcal { N } _ { i } \\cup \\{ i \\} } \\tilde { A } _ { i , j } \\pmb { W } ^ { c } \\pmb { h } _ { j } \\big ) } \\end{array}\n$$\n\nwhere $\\widehat { \\pmb { y } } _ { i } \\in \\mathbb { R } ^ { K + 1 }$ is the predicted label probability vector of $v _ { i }$ after neighborhood aggregation, and $\\pmb { W } ^ { c }$ is learnable parameter. The source node classification loss is defined as:\n\n$$\n\\begin{array} { r } { \\mathcal { L } _ { s } = - \\frac { 1 } { n ^ { s } } \\sum _ { i = 1 } ^ { n ^ { s } } \\sum _ { k = 1 } ^ { K } Y _ { i , k } ^ { s } \\log \\widehat { Y } _ { i , k } ^ { s } } \\end{array}\n$$\n\nwhere $Y _ { i , k } ^ { s } = 1$ if the ground-truth class-label of $v _ { i } ^ { s }$ is $k$ ; otherwise, $Y _ { i , k } ^ { s } = 0 . \\hat { Y } _ { i , k } ^ { s }$ is the probability of $v _ { i } ^ { s }$ belonging to class $k$ predicted by $f _ { C }$ in Eq. (6). Minimizing $\\mathcal { L } _ { s }$ guides label-discriminative node embeddings to separate different known classes, consequently reducing the source risk.\n\nAdversarial Training of GNN Encoder and Node Classifier. According to Theorem 2, to reduce the open-set risk on $\\mathcal { G } ^ { t }$ which also partly bounds the target risk of OCNNC, we need to recognize target nodes belonging to ‚Äúunknown‚Äù class. To this end, we follow the representative OSDA method OSBP (Saito et al. 2018) to construct a rough boundary between known classes and unknown class by adversarial learning. On one hand, $f _ { C }$ is trained to output unknown probability $\\mu$ for each target node $v _ { i } ^ { t }$ , i.e., $\\hat { Y } _ { i , K + 1 } ^ { t } = \\mu$ where $0 < \\mu < 1$ . On the other hand, $f _ { G }$ is trained to maximize the error of ùëìùê∂ by making ùëåÃÇùëñ,ùë°ùêæ+1 very different from ùúá, via two options: 1) increasing ùëåÃÇùëñ,ùë°ùêæ+1 to be much larger than $\\mu$ and then reject $v _ { i } ^ { t }$ as unknown; or 2) decreasing $\\hat { Y } _ { i , K + 1 } ^ { t }$ to be much smaller than $\\mu$ so as to classify $\\ v _ { i } ^ { t }$ into one of known classes. A binary cross-entropy loss is adopted to define the unknown classification loss, as:\n\n$$\n\\begin{array} { r } { \\mathcal { L } _ { u } = - \\frac { 1 } { n ^ { t } } { \\sum _ { i = 1 } ^ { n ^ { t } } \\mu \\log \\widehat { Y } } _ { i , K + 1 } ^ { t } + ( 1 - \\mu ) \\log \\left( 1 - \\widehat { Y } _ { i , K + 1 } ^ { t } \\right) } \\end{array}\n$$\n\nThe adversarial training of $f _ { G }$ and $f _ { C }$ can be achieved via optimizing the following objectives:\n\n$$\n\\operatorname* { m i n } _ { \\theta _ { C } } \\{ \\mathcal { L } _ { s } + \\mathcal { L } _ { u } \\} , \\operatorname* { m i n } _ { \\theta _ { G } } \\{ \\mathcal { L } _ { s } - \\mathcal { L } _ { u } \\}\n$$\n\nwhere $\\theta _ { G }$ and $\\theta _ { C }$ are the learnable parameters of $f _ { G }$ and $f _ { C }$ . In Eq. (9), $\\theta _ { C }$ is trained to minimize $\\mathcal { L } _ { u }$ so as to construct a rough boundary between known and unknown, whereas $\\theta _ { G }$ is trained to maximize $\\mathcal { L } _ { u }$ to push target nodes far away from boundary. To update $\\theta _ { G }$ and $\\theta _ { C }$ simultaneously, we follow (Saito et al. 2018) to insert a GRL (Ganin et al. 2016) to flip the sign of the gradient during back-propagation.\n\n# Unknown-excluded Adversarial Domain Alignment\n\nAfter rough separation, we go into adaptation stage to align the target distribution to the source only for known classes while explicitly excluding unknown class.\n\nWhile in O-CNNC, the target network is completely unlabeled, we do not know which target nodes belong to unknown class. Thus, we propose to assign pseudo-labels to target nodes beforehand. Firstly, we employ K-means algorithm to cluster all target nodes into $K + 1$ clusters, by taking the embeddings learned by $f _ { G }$ as inputs. For the first $K$ clusters corresponding to known classes, the initial centroid of each $k$ -th cluster is simply computed as the average over the source embeddings of class $k$ , i.e., $\\mathbb { C } _ { k } =$ $\\textstyle \\sum _ { i = 1 } ^ { n ^ { s } } Y _ { i , k } ^ { s } \\pmb { h } _ { i } ^ { s } \\big / \\sum _ { i = 1 } ^ { n ^ { s } } Y _ { i , k } ^ { s }$ . However, the last $( K + 1 )$ -th cluster corresponds to ‚Äúunknown‚Äù class, which is not present in $\\mathcal { G } ^ { s }$ . Therefore, we pick out top $R$ target nodes with the highest predicted unknown probability to form a pseudo-unknown set $\\mathcal { U } = \\left\\{ v _ { i } ^ { t } \\big | \\hat { Y } _ { i , K + 1 } ^ { t } \\right.$ is top $R$ highest}. Then, the average embedding of $\\mathcal { U }$ is adopted to compute the initial centroid of the $( K + 1 )$ -th cluster, i.e., $\\mathbb { C } _ { K + 1 } = \\sum _ { v _ { i } ^ { t } \\in \\mathcal { U } } \\pmb { h } _ { i } ^ { t } / R$ .\n\nGiven $K + 1$ initial cluster centroids, each target node would be assigned to its closest centroid, consequently obtaining a cluster-label matrix $\\widehat { \\mathbf { Y } } ^ { t ( c l u ) } \\in \\{ 0 , 1 \\} ^ { n ^ { t } \\times ( \\hat { \\boldsymbol { K } } + 1 ) }$ . Raw cluster-labels might contain noise. To obtain more accurate pseudo-labels, we propose to only assign a pseudo-label to a target node $\\ v _ { i } ^ { t }$ , if and only if its cluster-label and classlabel predicted by $f _ { C }$ reach an agreement:\n\n$$\n\\bar { \\bar { Y } } _ { i , k } ^ { t } = \\left\\{ \\begin{array} { l l } { 1 , \\mathrm { i f } \\hat { Y } _ { i , k } ^ { t ( c l u ) } = 1 \\land \\mathrm { a r g m a x } \\hat { Y } _ { i , j } ^ { t } = k } \\\\ { \\qquad \\quad 0 , \\mathrm { o t h e r w i s e } } \\end{array} \\right.\n$$\n\nBesides, we further assign pseudo-unknown label to top $R$ target nodes in the pseudo-unknown set $\\mathcal { U }$ , if they have not been assigned with any pseudo-labels by Eq. (10).Such confident pseudo-labeled target nodes are employed to iteratively re-train $f _ { G }$ and $f _ { C }$ in a self-training manner (Chen, Weinberger, and Blitzer 2011; Shen, Mao, and Chung 2020), by minimizing the following target node classification loss:\n\n$$\n\\begin{array} { r } { \\mathcal { L } _ { t } = - \\frac { 1 } { n _ { l } ^ { t } } \\sum _ { i = 1 } ^ { n _ { l } ^ { t } } \\sum _ { k = 1 } ^ { K + 1 } \\bar { \\bar { Y } } _ { i , k } ^ { t } \\log \\hat { Y } _ { i , k } ^ { t } } \\end{array}\n$$\n\nwhere $\\mathcal { n } _ { l } ^ { t }$ is the number of target nodes assigned with pseudo-labels. Minimizing $\\mathcal { L } _ { t }$ exploits the supervision from $\\mathcal { G } ^ { t }$ to yield a refined boundary to separate $K { + } 1$ classes, which is conductive to reducing the open-set risk on $\\boldsymbol { \\mathcal { G } ^ { t } }$ .\n\nAccording to Theorem 2, to succeed in O-CNNC, it is essential to match the domain distribution between $\\mathcal { G } ^ { s }$ and $\\mathcal { G } ^ { t }$ only for known classes. Note that the OSBP method (Saito et al. 2018) adopted in previous separation stage does not utilize any domain information during adversarial learning (Shermin et al. 2020), thus it cannot explicitly reduce the domain discrepancy for known classes. To remedy this, we further employ a domain discriminator $f _ { D }$ to compete against $f _ { G }$ , following conventional closed-set CNNC methods (Shen et al. 2020; $\\mathrm { w } _ { \\mathrm { u } }$ et al. 2020). The domain discriminator $f _ { D }$ is constructed by an MLP taking node embeddings learned by $f _ { G }$ as inputs and outputs $\\hat { d } _ { i } = f _ { D } ( \\pmb { h } _ { i } ; \\theta _ { D } ) ^ { \\top }$ , where $\\hat { d } _ { i }$ is the predicted probability of $v _ { i }$ coming from $\\mathcal { G } ^ { t }$ . The domain classification loss is defined as:\n\n$$\n\\begin{array} { r } { \\mathcal { L } _ { d } = - \\frac { 1 } { n ^ { s } + n ^ { t } } \\sum _ { i = 1 } ^ { n ^ { s } + n ^ { t } } d _ { i } \\log \\hat { d } _ { i } + ( 1 - d _ { i } ) \\log ( 1 - \\hat { d } _ { i } ) } \\end{array}\n$$\n\nThen, $f _ { D }$ and $f _ { G }$ are trained in an adversarial manner by optimizing the following minimax objective:\n\n$$\n\\operatorname* { m i n } _ { \\theta _ { G } , \\theta _ { C } } \\left\\{ \\mathcal { L } _ { s } + \\beta \\mathcal { L } _ { t } + \\lambda \\operatorname* { m a x } _ { \\theta _ { D } } \\ ( - \\mathcal { L } _ { d } \\right\\}\n$$\n\nwhere $\\beta$ and $\\lambda$ are trade-off parameters to balance the effects of different losses. To simultaneously update the learnable parameters of $f _ { G }$ and $f _ { D }$ , a GRL (Ganin et al. 2016) is inserted between them during back-propagation to reverse the gradient of $\\mathscr { L } _ { d }$ w.r.t. $\\theta _ { G }$ and multiply it by a domain adaptation coefficient $\\lambda$ .\n\nPositive and Negative Domain Adaptation Coefficient. It is worth noting that in the conventional GRL-based adversarial domain adaptation method (Ganin et al. 2016), the domain adaptation coefficient $\\lambda$ is always set to a positive value for all samples across domains. We argue that such a setting should be problematic in O-CNNC, since setting a positive $\\lambda$ for all samples would align the entire target network with the source network without excluding unknown class, consequently causing negative transfer. To remedy this, we propose to assign positive $\\lambda$ to nodes belonging to known classes, while negative $\\lambda$ to nodes coming from unknown class, as:\n\n$$\n\\lambda = \\left\\{ \\begin{array} { c } { 1 , \\mathrm { i f } v _ { i } \\in \\mathcal { V } ^ { s } \\vee \\left( v _ { i } \\in \\mathcal { V } ^ { t } \\wedge \\bar { \\bar { Y } } _ { i , K + 1 } ^ { t } = 0 \\right) } \\\\ { - 1 , \\mathrm { i f } v _ { i } \\in \\mathcal { V } ^ { t } \\wedge \\bar { \\bar { Y } } _ { i , K + 1 } ^ { t } = 1 } \\end{array} \\right.\n$$\n\nFor simplicity, here we just assign fixed $1 / { - } 1$ as the positive/negative value of $\\lambda$ . It is flexible to assign adaptive positive/negative values to $\\lambda$ , which we leave as the future work. On one hand, by assigning positive $\\lambda$ to the source nodes and the target nodes with pseudo-known labels, $f _ { G }$ and $f _ { D }$ would be trained to compete against each other to yield network-invariant embeddings for known classes, like conventional closed-set domain adaptation method (Ganin et al. 2016). On the other hand, assigning negative $\\lambda$ to the target nodes with pseudo-unknown label would train both $f _ { G }$ and $f _ { D }$ in the same direction to minimize $\\mathscr { L } _ { d }$ , thus making the embeddings of target unknown class very distinguishable from the source embeddings. As a result, UAGA can reduce the domain discrepancy only for the shared known classes, while pushing target unknown class far away from the source to effectively reduce the open-set risk on $\\mathcal { G } ^ { t }$ , which is conductive to reducing the target risk of O-CNNC, according to Theorem 2.\n\nAlgorithm Description. Algorithm 1 shows the training process of UAGA in a mini-batch strategy. In separation stage (Line 2-9), the GNN encoder $f _ { G }$ and the $( K { + } 1 )$ -class neighborhood aggregation node classifier $f _ { C }$ are trained in an adversarial manner via optimizing Eq. (9), so as to learn a rough boundary between unknown and known classes. In adaptation stage (Line 10-22), the GNN encoder $f _ { G }$ , node classifier $f _ { C }$ and domain discriminator $f _ { D }$ are trained via optimizing the overall minimax objective in Eq. (13). The target nodes would be assigned with pseudo-labels, according to the prediction results of both unsupervised clustering and supervised classification (Line 11-14). On one hand, such pseudo-labeled target nodes are employed to compute the target node classification loss $\\mathcal { L } _ { t }$ to iteratively re-train $f _ { G }$ and $f _ { C }$ . On the other hand, such pseudo-labels are leveraged to assign positive/negative domain adaptation coefficient $\\lambda$ to known/unknown class nodes. Thanks to unknown-excluded adversarial domain alignment and iterative self-training, a refined node classification boundary can be progressively learned. After training convergence or reaching the maximum training epochs of adaptation stage, the optimized parameters of $f _ { G }$ and $f _ { C }$ would be employed to generate node embeddings and predict target labels (Line 23-24).\n\nComplexity Analysis of UAGA. Both $f _ { G }$ and $f _ { C }$ are constructed by a single graph attention layer. The time complexity of $f _ { G }$ is $O \\big ( ( | \\mathcal { V } ^ { s } | + | \\mathcal { V } ^ { t } | ) \\omega \\mathbb { d } + ( | \\mathcal { E } ^ { s } | + | \\mathcal { E } ^ { t } | ) \\mathbb { d } \\big )$ ,\n\n# Algorithm 1 UAGA\n\nInput: Fully labeled source network $\\mathcal { G } ^ { s } = ( A ^ { s } , X ^ { s } , Y ^ { s } )$   \n\n<html><body><table><tr><td colspan=\"2\">and unlabeled target network Gt = (At,Xt).</td></tr><tr><td>1</td><td>Initialize parameters 0g,0c,0D.</td></tr><tr><td>2</td><td>while not max epoch of separation stage do</td></tr><tr><td>3</td><td>while not max iteration do</td></tr><tr><td>4</td><td>Sample minibatch of B source nodes from</td></tr><tr><td>5</td><td>G and minibatch of B target nodes from Gt Learn embeddings by fg in Eq. (5).</td></tr><tr><td>6</td><td>Calculate Ls and Lu in Eq. (7) and Eq. (8).</td></tr><tr><td>7</td><td>Update 0g and 0c in Eq. (9).</td></tr><tr><td>8</td><td>end while</td></tr><tr><td>9</td><td>end while</td></tr><tr><td>10</td><td>while not max epoch of adaptation stage do</td></tr><tr><td>11</td><td>Compute initial centroids of first K clusters</td></tr><tr><td></td><td>based onsource embeddings {CkJK=1:</td></tr><tr><td>12</td><td>Compute initial centroid of (K +1)-th cluster Ck+1 based on average embeddings of pseudo- unknown set U.</td></tr><tr><td>13</td><td>Apply K-means to obtain target cluster-labels Yt(clu).</td></tr><tr><td>14</td><td>Consider both cluster-labels and class-labels predicted by fc to obtain refined pseudo-labels</td></tr><tr><td>15</td><td>Yt in Eq. (10). Assign positive or negative Œª in Eq. (14).</td></tr><tr><td>16</td><td>while not max iteration do</td></tr><tr><td>17</td><td>Sample minibatch of B source nodes from G$ and minibatch of B target nodes from Gt.</td></tr><tr><td>18</td><td>Learn embeddings by fg in Eq. (5).</td></tr><tr><td>19</td><td>Calculate Ls,Lt and Ld in Eq. (7), Eq. (11)</td></tr><tr><td>20</td><td>and Eq. (12). Update 0g,0c,0D in Eq. (13).</td></tr><tr><td>21</td><td>end while</td></tr><tr><td>22</td><td>end while</td></tr><tr><td>23</td><td>Apply optimized 0* to generate embeddings of</td></tr><tr><td></td><td>G$ and gt in Eq. (5).</td></tr><tr><td>24</td><td>Apply optimized 0* to predict target labels in Eq. (6).</td></tr></table></body></html>\n\nOutput: Predicted node labels of target network ${ \\widehat { \\pmb { Y } } } ^ { t }$ .\n\nwhere $| \\mathcal { V } ^ { s } |$ and $| \\mathcal { V } ^ { t } |$ are the number of nodes in $\\mathcal { G } ^ { s }$ and $\\mathcal { G } ^ { t }$ respectively, $| \\mathcal { E } ^ { s } |$ and $| \\mathcal { E } ^ { t } |$ are the number of edges in $\\mathcal { G } ^ { s }$ and $\\mathcal { G } ^ { t }$ respectively, $\\omega$ is the number of node attributes, and $\\mathbb { d }$ is the number of embedding dimensions. The time complexity of $f _ { C }$ is $O \\big ( ( | \\mathcal { V } ^ { s } | + | \\mathcal { V } ^ { t } | ) \\mathrm { d } ( K + 1 ) + ( | \\mathcal { E } ^ { s } | +$ $\\begin{array} { r } { | \\mathcal { E } ^ { t } | ) ( K + 1 ) \\big ) } \\end{array}$ , where $K$ is the number of known classes. The time complexity of K-means clustering is $O \\big ( ( K +$ $1 ) \\mathbb { d } | \\mathcal { V } ^ { t } | \\big )$ . The time complexity of $f _ { D }$ constructed by an MLP is linear to the number of nodes across networks. Hence, the overall time complexity of UAGA is linear to number of nodes and edges in $\\mathcal { G } ^ { s }$ and $\\mathcal { G } ^ { t }$ .\n\n# Experiments\n\nDatasets. Previous benchmark datasets for closed-set CNNC (Shen et al. 2021) only contain 5 node classes, which limits possible splits of known and unknown classes (i.e. various openness) in the open-set setting. To remedy this, we construct new benchmark datasets to contain more node classes for O-CNNC, i.e., Citation-v1 (C), DBLP-v4 (D) and ACM-v8 (A). They are real-world paper citation networks extracted from ArnetMiner with the papers published in different periods, i.e., between years 1997 and 2003, between years 2004 and 2011, and between years 2012 and 2015, respectively. Each node represents a paper and each edge denotes the citation relation between two papers. There are no common nodes between any two networks. The citation networks were modeled as undirected networks in our experiments. The keywords extracted from the paper title were utilized as node attributes. Each paper belongs to one of the following nine categories according to its research topics, including ‚ÄúArtificial Intelligence‚Äù, ‚ÄúHuman-computer Interaction‚Äù, ‚ÄúInformation Security‚Äù, ‚ÄúData Mining‚Äù, ‚ÄúComputer Architecture‚Äù, ‚ÄúMultimedia‚Äù, ‚ÄúComputer Theory‚Äù, ‚ÄúComputer Network‚Äù, and ‚ÄúSoftware Engineering‚Äù. Six O-CNNC tasks can be conducted among three networks, by selecting one as the source and another as the target, i.e., $\\mathrm { C } \\mathrm { \\to D }$ , $\\mathrm { C } {  } \\mathrm { A }$ , $\\mathrm { D \\mathrm { \\to } C }$ , $\\mathrm { D \\mathrm { \\to A } }$ , $\\mathrm { A { \\to } C }$ and $\\mathrm { A } {  } \\mathrm { D }$ . For each OCNNC task, we chose the first $K$ classes as known classes, while all the remaining $9 - K$ classes were re-labeled as the $( K + 1 )$ -th ‚Äúunknown‚Äù class, following the common setting in OSDA (Liu et al. 2019). The openness (Liu et al. 2019) is defined as the proportion of target-private classes in all original target classes, i.e., $\\mathcal { O } = | \\mathcal { Y } ^ { u } | / | \\mathcal { Y } ^ { t o } | = ( 9 - K ) / 9$ .\n\nBaselines. The proposed UAGA is competed against 9 baselines in four categories: 1) Open-set Domain Adaptation: OSBP (Saito et al. 2018) and OMEGA (Ru et al. 2023), 2) Open-set Node Classification: OODGAT (Song and Wang 2022) and $\\mathbf { G } ^ { 2 } \\mathbf { P } \\mathbf { x } \\mathbf { y }$ (Zhang et al. 2023), 3) Closed-set Cross-network Node Classification: UDAGCN (Wu et al. 2020), AdaGCN (Dai et al. 2023) and SGDA (Qiao et al. 2023), 4) Open-set Cross-network Node Classification: SDA (Wang et al. 2024) and UDANE (Chen et al. 2023).\n\nImplementation Details. All experiments were conducted on a single Tesla A40 GPU with 48GB memory. The proposed UAGA was implemented by PyTorch 1.7.1 (Paszke et al. 2019) and Deep Graph Library 0.7.2 (Wang et al. 2019). UAGA was trained by the Adam optimizer with learning rate of 1e-3. The batch size $\\mathbb { B }$ was set to 2048. The number of training epochs of separation stage and adaptation stage were set as 30 and 200 respectively. The number of layers of the GNN encoder $f _ { G }$ was set to 1. The number of attention heads in GNN encoder $f _ { G }$ and node classifier $f _ { C }$ were set to 8 and 2 respectively. The number of embedding dimensions of each head $\\mathbb { d }$ in the GNN encoder $f _ { G }$ was set to 32. The weight of target node classification loss $\\mathcal { L } _ { t }$ (i.e.\n\nTable 1. Statistics of the experimental datasets.   \n\n<html><body><table><tr><td>Datasets</td><td># Nodes</td><td>#Edges</td><td># Attributes</td><td>#Labels</td></tr><tr><td>Citation-v1</td><td>9737</td><td>14054</td><td rowspan=\"3\">7786</td><td rowspan=\"3\">9</td></tr><tr><td>DBLP-v4</td><td>8653</td><td>12967</td></tr><tr><td>ACM-v8</td><td>8806</td><td>17661</td></tr></table></body></html>\n\n$\\beta$ ) was set to 0.1. The unknown threshold $\\mu$ in rough sepa ration stage was set to 0.5 following OSBP (Saito et al. 2018). The number of top $R$ target nodes to form pseudounknown set $\\mathcal { U }$ was selected from $\\{ 2 0 0 0 , 3 0 0 0 , 4 0 0 0 \\}$ .\n\nEvaluation Metrics. We adopt three widely used metrics in OSDA literature to evaluate the O-CNNC performance. ${ \\mathrm { O S ^ { * } } }$ (Saito et al. 2018) is the accuracy averaged over all known classes. OS (Saito et al. 2018) is the average accuracy over all classes. HS (Fu et al. 2020) is the harmonic mean of the instance-level accuracy on known classes and unknown class. We also adopt the Area Under the Receiver Operating Characteristic curve (AUC) to evaluate the detection of unknown class, following (Song and Wang 2022; Zhou et al. 2022). For each O-CNNC task, we run each method with 5 random initializations and report the average results and standard deviations.\n\n# O-CNNC Results\n\nTable 2 shows O-CNNC results under the openness $\\mathcal { O } =$ $4 / 9$ , i.e., the number of target-private classes is 4 and the number of known classes is 5. We have key observations as follows:\n\n1) The OSDA baselines, i.e., OSBP and OMEGA perform poorly in O-CNNC. This is because they are developed based on the i.i.d. assumption, which ignores the connections between nodes in graph. However, considering the connection patterns between nodes has been proved to be essential for both known classification and unknown detection on graph data (Song and Wang 2022).\n\n2) The open-set node classification baselines, i.e., OODGAT and $\\mathrm { G } ^ { 2 } \\mathrm { P x y }$ , are good at unknown detection (high AUC) while rather weak in known classification (low ${ \\mathrm { O S ^ { * } } }$ ) in OCNNC. This is because they are developed for a single-network scenario, failing to mitigate the domain discrepancy for known classes across networks.\n\n3) The closed-set CNNC baselines (i.e., UDAGCN, AdaGCN, and SGDA) are good at known classification (high ${ \\mathrm { O S ^ { * } } }$ ) while still weak in unknown detection (low AUC). We believe the reason behind is that the domain adaptation component in the closed-set CNNC methods tends to match the whole distribution between the source and target networks, that is, the unknown class from target network would also be aligned with the source, which inevitably causes negative transfer for unknown detection.\n\n4) The state-of-the-art O-CNNC baselines (i.e. UDANE and SDA) significantly outperform other baselines. Moreover, the proposed UAGA beats UDANE and SDA by a large margin across six O-CNNC tasks. The reason behind our outperformance might be two-fold. On one hand, to learn node embeddings, UDANE and SDA adopt GCN and dualGCN, while UAGA adopts GAT as the GNN encoder. The latest literature on open-set node classification (Huang, Wang, and Fang 2022; Song and Wang 2022) have revealed that the attention-based model like GAT can achieve better performance on unknown detection than GCN. This is because GCN treats all neighbors equally during neighborhood aggregation, while GAT adaptively assigns small attention weight to neighbors from different distributions. On the other hand, to determine whether a target node belongs to one of the known classes or the ‚Äúunknown‚Äù class, UDANE and SDA employ a thresholding method based on a $K$ -class classifier which makes prediction over known classes. However, finding an optimal threshold to separate unknown class from known classes is hard and time-consuming (Zhang et al. 2023). Instead, our UAGA constructs a $( K + 1 )$ -class classifier by adding an extra class to represent the ‚Äúunknown‚Äù class, which eliminates the difficulty of tuning the threshold.\n\n<html><body><table><tr><td></td><td>Task Metric</td><td>OSBP OMEGA</td><td>OODGAT</td><td>G¬≤Pxy</td><td>UDAGCN AdaGCN</td><td>SGDA</td><td>UDANE SDA</td><td>UAGA</td></tr><tr><td rowspan=\"4\"></td><td>OS</td><td>44.91¬±0.42 37.96¬±1.8036.49¬±0.39 37.88¬±8.70</td><td></td><td></td><td>55.08¬±7.33 51.17¬±1.63 49.57¬±5.28</td><td></td><td>57.09¬±0.36 54.38¬±0.94 69.21¬±0.17</td><td></td></tr><tr><td>OS*</td><td>52.36¬±0.44 30.38¬±2.62</td><td>27.02¬±0.46 36.52¬±9.89</td><td></td><td>63.14¬±10.24 58.63¬±1.81 56.68¬±6.42</td><td></td><td>61.78¬±0.59 58.94¬±1.27 74.00¬±0.15</td><td></td></tr><tr><td>C‚ÜíD AUC</td><td>48.78¬±1.07 60.18¬±1.24</td><td></td><td>70.36¬±0.44 58.33¬±1.45</td><td>54.01¬±9.98 54.87¬±1.49 52.46¬±3.87</td><td></td><td>55.90¬±0.46 61.45¬±2.30 73.72¬±1.03</td><td></td></tr><tr><td>HS</td><td>13.59¬±0.56 47.06¬±1.55</td><td>51.73¬±0.9036.40¬±17.96</td><td></td><td>21.97¬±12.23 22.78¬±2.71 22.74¬±1.82</td><td></td><td></td><td>43.85¬±0.78 41.95¬±6.02 56.41¬±0.49</td></tr><tr><td rowspan=\"4\">C‚ÜíA</td><td>OS</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>41.98¬±1.07 29.35¬±1.28</td><td></td><td>28.25¬±0.55 39.83¬±6.30</td><td>44.33¬±13.68 45.96¬±0.77 45.80¬±4.89</td><td></td><td>54.76¬±0.69 54.93¬±1.87 69.50¬±0.39</td><td></td></tr><tr><td>OS*</td><td>45.33¬±0.46 25.07¬±2.47</td><td></td><td>18.76¬±0.80 40.02¬±7.60</td><td>48.41¬±22.00 51.13¬±1.53 52.32¬±6.26</td><td></td><td>58.03¬±0.85 59.35¬±2.89 72.48¬±0.47</td><td></td></tr><tr><td>AUC</td><td>63.17¬±0.57 54.89¬±0.55</td><td></td><td>55.45¬±0.75 60.46¬±1.86</td><td>56.49¬±6.21 61.69¬±1.33 50.39¬±3.26</td><td></td><td></td><td>62.13¬±0.60 66.88¬±1.95 83.41¬±0.26</td></tr><tr><td rowspan=\"4\">D‚ÜíC</td><td>HS</td><td>33.51¬±1.04 38.65¬±0.71</td><td></td><td>35.39¬±1.25 32.59¬±16.39</td><td>16.58¬±16.22 29.46¬±4.22 21.19¬±2.62</td><td></td><td>46.53¬±0.24 41.36¬±9.43 61.39¬±0.60</td><td></td></tr><tr><td>OS</td><td>46.04¬±0.43 38.17¬±0.91</td><td></td><td>39.07¬±0.36 44.86¬±0.81</td><td>46.31¬±14.87 55.42¬±0.64 50.74¬±4.15</td><td></td><td></td><td>63.65¬±0.25 58.65¬±0.86 69.12¬±0.36</td></tr><tr><td>OS*</td><td>51.63¬±0.60 31.48¬±3.39</td><td></td><td>29.89¬±0.56 43.98¬±2.05</td><td>51.17¬±19.21 61.41¬±1.03 57.50¬±5.13</td><td></td><td>68.08¬±0.22 63.01¬±1.53 74.76¬±0.21</td><td></td></tr><tr><td></td><td>AUC 52.83¬±1.27 62.28¬±3.29</td><td></td><td>70.50¬±0.12 62.02¬±0.33</td><td>61.00¬±4.41 57.66¬±2.16 57.96¬±2.47</td><td></td><td></td><td>66.84¬±0.57 69.58¬±0.58 72.61¬±0.90</td></tr><tr><td rowspan=\"4\">D‚ÜíA</td><td>HS</td><td>27.02¬±1.61 43.77¬±1.43</td><td></td><td>48.51¬±0.37 48.36¬±2.43</td><td>24.44¬±12.39 36.00¬±2.78 26.02¬±6.06</td><td></td><td>50.89¬±0.60 46.97¬±3.41 51.69¬±1.03</td><td></td></tr><tr><td>OS</td><td>43.71¬±1.48 33.96¬±0.56</td><td></td><td>27.62¬±0.29 42.73¬±0.91</td><td>48.81¬±6.31 55.09¬±0.45 53.70¬±4.33</td><td></td><td></td><td>60.22¬±1.00 55.95¬±1.03 64.83¬±1.16</td></tr><tr><td>OS*</td><td>49.16¬±2.97 27.39¬±1.73</td><td></td><td>15.74¬±0.36 41.13¬±1.79</td><td>57.74¬±7.25 60.10¬±0.69 63.02¬±4.24</td><td></td><td></td><td>63.41¬±0.96 60.41¬±1.20 69.17¬±0.82</td></tr><tr><td>AUC</td><td>57.18¬±2.61 59.95¬±0.92</td><td></td><td>61.46¬±0.26 61.15¬±0.11</td><td>56.32¬±4.53 65.21¬±1.16 55.75¬±3.97</td><td></td><td></td><td>69.25¬±1.28 70.90¬±0.39 78.33¬±2.61</td></tr><tr><td rowspan=\"4\">A‚ÜíC</td><td>HS</td><td>23.98¬±7.07 43.65¬±0.97</td><td></td><td>37.40¬±0.60 47.39¬±0.90</td><td>6.96¬±8.93 40.50¬±2.2410.93¬±14.1351.54¬±1.23 43.85¬±0.87 51.90¬±2.54</td><td></td><td></td><td></td></tr><tr><td>OS</td><td>46.20¬±0.93 34.41¬±0.48</td><td></td><td>31.43¬±0.51 44.92¬±4.29</td><td>50.08¬±14.52 59.39¬±0.54 53.32¬±1.99</td><td></td><td></td><td>65.75¬±0.30 60.52¬±0.92 71.78¬±0.42</td></tr><tr><td>OS*</td><td>51.50¬±2.56 32.20¬±0.57</td><td>19.90¬±0.68 43.01¬±7.95</td><td></td><td>54.27¬±20.73 66.50¬±0.77 58.66¬±2.13</td><td></td><td></td><td>69.78¬±0.3965.44¬±2.34 77.58¬±0.44</td></tr><tr><td></td><td>AUC 52.15¬±1.62 56.35¬±2.19</td><td></td><td>65.32¬±0.27 60.80¬±0.45</td><td>57.94¬±6.23 59.40¬±0.88 52.73¬±4.28</td><td></td><td></td><td>66.37¬±0.84 65.95¬±1.40 76.26¬±0.62</td></tr><tr><td rowspan=\"4\">A‚ÜíD</td><td>HS</td><td>27.02¬±7.8339.75¬±1.2030.56¬±0.83 45.88¬±4.11</td><td></td><td></td><td>28.73¬±14.17 34.97¬±1.64 35.61¬±5.53</td><td></td><td>54.03¬±0.8545.42¬±4.76 54.20¬±0.40</td><td></td></tr><tr><td>OS</td><td>44.47¬±1.08 37.11¬±1.12</td><td></td><td>30.79¬±0.12 47.12¬±2.97</td><td>52.65¬±6.02 55.56¬±0.74 46.99¬±4.09</td><td></td><td></td><td>58.42¬±0.6055.71¬±0.85 66.40¬±0.34</td></tr><tr><td>OS*</td><td>49.80¬±1.65 34.67¬±3.84</td><td>18.61¬±0.13 47.70¬±6.01</td><td></td><td>60.02¬±8.86 62.48¬±1.15 51.02¬±4.78</td><td></td><td>62.36¬±0.38 60.14¬±2.10 70.95¬±0.38</td><td></td></tr><tr><td>AUC</td><td>52.24¬±1.44 56.12¬±4.78</td><td></td><td>71.67¬±0.58 59.17¬±0.17</td><td>54.95¬±3.48 56.62¬±0.95 51.34¬±2.14</td><td></td><td></td><td>62.07¬±3.81 64.16¬±1.27 71.87¬±1.40</td></tr><tr><td rowspan=\"2\"></td><td>HS</td><td>26.71¬±2.16 40.95¬±1.0935.89¬±0.73 44.83¬±7.29</td><td></td><td></td><td>20.62¬±16.18 31.73¬±3.49 36.11¬±2.5247.92¬±2.45 43.85¬±4.57 53.93¬±0.42</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>\n\nTable 2: O-CNNC results on six tasks when openness is $\\scriptstyle { \\mathcal { O } } = 4 / 9$ . The best scores among all methods are shown in boldface.\n\nTable 3. Model variants of UAGA.   \n\n<html><body><table><tr><td rowspan=\"2\">Model Variants</td><td colspan=\"4\">C‚ÜíD</td><td colspan=\"4\">C‚ÜíA</td></tr><tr><td>OS</td><td>OS*</td><td>AUC</td><td>HS</td><td>OS</td><td>OS*</td><td>AUC</td><td>HS</td></tr><tr><td>UAGA</td><td>69.2</td><td>74.0</td><td>73.7</td><td>56.4</td><td>69.5</td><td>72.5</td><td>83.4</td><td>61.4</td></tr><tr><td>w/o separation</td><td>64.3</td><td>68.9</td><td>69.2</td><td>52.2</td><td>46.8</td><td>50.8</td><td>64.9</td><td>36.4</td></tr><tr><td>w/o adaptation</td><td>55.0</td><td>60.5</td><td>67.9</td><td>39.1</td><td>50.5</td><td>52.7</td><td>75.2</td><td>47.1</td></tr><tr><td>w/o=-1</td><td>66.7</td><td>74.1</td><td>72.5</td><td>42.8</td><td>64.4</td><td>72.6</td><td>80.9</td><td>35.1</td></tr><tr><td>Replace fc byMLP</td><td>59.6</td><td>71.5</td><td>63.2</td><td>0.03</td><td>67.8</td><td>70.8</td><td>81.2</td><td>58.9</td></tr><tr><td>w/o self-training</td><td>58.2</td><td>62.9</td><td>63.9</td><td>44.9</td><td>54.8</td><td>57.5</td><td>73.1</td><td>48.7</td></tr></table></body></html>\n\nVarious Openness. We investigate the O-CNNC performance under various openness. As shown in Figure 3, the increase of openness generally leads to an increase of ${ \\mathrm { O S ^ { * } } }$ while a decrease of AUC. This is because a larger openness implies more target-private classes and less known classes, which naturally eases known classification while increasing the difficulty of unknown detection. Moreover, one can see that the proposed UAGA consistently achieves better overall performance than all baselines under various openness. This reflects that UAGA allows openness-robust O-CNNC for both known classification and unknown detection.\n\nAblation Study. We investigate the contributions of different components in the proposed UAGA and report the results in Table 3. Removing separation stage results in significantly worse performance. This is because without learning a rough separation boundary between unknown and known beforehand, it would fail to obtain pseudo-labels to guide the following adaptation stage. Without adaptation stage, the performance also drops considerably. This shows that only rough separation is not enough, while further conducting unknown-excluded adversarial domain alignment with self-training is indispensable for UAGA. Without setting $\\lambda = - 1$ in the GRL-based adversarial domain adaptation leads to worse performance. This confirms that assigning negative domain adaptation coefficient to target unknownclass nodes to explicitly exclude them from adversarial domain alignment is a crucial component of UAGA. Replacing the neighborhood-aggregation node classifier $f _ { C }$ by an MLP presents inferior performance. This is consistent with Theorem 1, i.e., $\\mathcal { G } ^ { t }$ is homophilic w.r.t. $K + 1$ classes. Thus, aggregating label prediction among neighborhood for all $\\textstyle K + 1$ dimensions can indeed boost both known classification and unknown detection. Without self-training significantly degrades the performance. This verifies that exploiting confident pseudo-labeled target nodes to iteratively re-train the model indeed yields a refined boundary.\n\n![](images/ce5899d1ef8656029911cdabc4024be2c53de0f001f4b54a36db1b7d84d87b4c.jpg)  \nFigure 3: Performance of O-CNNC under various openness on the representative task $\\mathrm { D } {  } \\mathrm { C }$ .\n\n![](images/2ac974ee904a1ef78d33ad31b8c0a7c597caea9b271d5b7264e5ebd630849cee.jpg)  \nFigure 4: Visualization of cross-network embeddings on the representative task $\\mathrm { C } {  } \\mathrm { A }$ . Grey indicates ‚Äúunknown‚Äù class and other colors indicate different known classes.\n\n![](images/bcad7703efad541642bd78352954495d48bf8568d7b5d97e4025e2cc0f4a7319.jpg)  \nFigure 5. Parameter sensitivity of UAGA on task $\\mathrm { C } \\mathrm { \\to D }$ .\n\nVisualization. The cross-network node embeddings learned by representative methods were visualized by t-SNE (Van der Maaten and Hinton 2008) in Figure 4. We can observe that AdaGCN forms clear clusters between different known classes, but fails to detect unknown class. OODGAT fails to align the same known class of nodes across networks. UDANE and SDA can separate unknown class from known classes to some extent, however, the boundaries between different classes are not clear enough. The proposed UAGA yields the best visualization for both known classes and unknown class, where the target embeddings of known classes are aligned to the source accurately, while the embeddings of unknown class are separated far apart.\n\nParameter Sensitivity. As shown in Figure 5, UAGA achieves the best performance when the number of embedding dimensions $\\mathbb { d }$ is 32. Deeper GNN encoder leads to worse performance of UAGA. The performance of UAGA is sensitive to the weight of target node classification loss $\\beta$ and it is suggested to set $\\beta = 0 . 1$ . The hyper-parameter $R$ is the number of target nodes selected to construct pseudo-unknown set, and $R { = } 2 0 0 0$ achieves the best overall results.\n\n# Conclusion\n\nThis work studies a novel O-CNNC problem, which allows target network to contain private classes unseen in the source. A novel framework named UAGA with a separateadapt training strategy is proposed to address O-CNNC. Firstly, a rough boundary between known classes and unknown class is constructed, by training an attention-based GNN encoder and a $( K { + } 1 )$ -class neighborhood-aggregation node classifier in adversarial learning. Then, negative domain adaptation coefficient is assigned to target nodes likely to belong to unknown class during adversarial domain adaptation. As a result, UAGA only aligns target nodes from known classes with the source, while pushing target nodes from unknown class far away from the source to avoid negative transfer. Extensive experiments on real-world datasets demonstrate that the proposed UAGA significantly outperforms state-of-the-art methods by a large margin in the challenging O-CNNC problem.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáÁ†îÁ©∂ÂºÄÊîæÈõÜË∑®ÁΩëÁªúËäÇÁÇπÂàÜÁ±ªÔºàOpen-set Cross-Network Node Classification, O-CNNCÔºâÈóÆÈ¢òÔºåÁõÆÊ†áÁΩëÁªúÂåÖÂê´Ê∫êÁΩëÁªúÂ∑≤Áü•Á±ªÂà´ÂíåÊú™ËßÅÁöÑÁßÅÊúâÁ±ªÂà´ÔºàÁªüÁß∞‚ÄúÊú™Áü•‚ÄùÁ±ªÔºâ„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éÁé∞ÂÆûÂú∫ÊôØ‰∏≠Ê∫êÂíåÁõÆÊ†áÁΩëÁªúÊ†áÁ≠æÁ©∫Èó¥Èöæ‰ª•ÂÆåÂÖ®‰∏ÄËá¥ÔºåÁé∞ÊúâÈó≠ÈõÜÊñπÊ≥ïÔºàÂ¶ÇUDAGCNÔºâÂõ†Ë¥üËøÅÁßªÂ§±ÊïàÔºåËÄåÂçïÁΩëÁªúÂºÄÊîæÈõÜÊñπÊ≥ïÔºàÂ¶ÇOODGATÔºâÊó†Ê≥ïÂ§ÑÁêÜË∑®ÁΩëÁªúÂüüÂÅèÁßª„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ÊèêÂá∫Êú™Áü•ÊéíÈô§ÂØπÊäóÂõæÂüüÂØπÈΩêÔºàUnknown-Excluded Adversarial Graph Domain Alignment, UAGAÔºâÊ°ÜÊû∂ÔºåÈááÁî®ÂàÜÁ¶ª-ÈÄÇÂ∫îÁ≠ñÁï•ÔºöÂÖàÈÄöËøáÂØπÊäóÂ≠¶‰π†ÂàÜÁ¶ªÊú™Áü•Á±ª‰∏éÂ∑≤Áü•Á±ªÔºåÂÜçÈÄöËøáÊ≠£Ë¥üÂüüÈÄÇÂ∫îÁ≥ªÊï∞ÔºàŒª=1/-1ÔºâÂÆûÁé∞‰ªÖÂ∑≤Áü•Á±ªÁöÑÂüüÂØπÈΩê„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **ÈóÆÈ¢òÂÆö‰πâ‰∏éÁêÜËÆ∫ÂàÜÊûê**ÔºöÈ¶ñÊ¨°ÂΩ¢ÂºèÂåñO-CNNCÈóÆÈ¢òÔºåËØÅÊòéÁõÆÊ†áÂõæÂú®ÂêåÈÖçÊÄß‰∏ãÂØπK+1Á±ª‰ªç‰øùÊåÅÂêåÈÖçÊÄßÔºàTheorem 1ÔºâÔºåÂπ∂‰ªéÂºÄÊîæÈõÜÂüüÈÄÇÂ∫îÁêÜËÆ∫Êé®ÂØºÈ£éÈô©‰∏äÁïåÔºàTheorem 2Ôºâ„ÄÇ\\n> *   **ÁÆóÊ≥ïÂàõÊñ∞**ÔºöÊèêÂá∫Ë¥üÂüüÈÄÇÂ∫îÁ≥ªÊï∞Êú∫Âà∂ÔºàÂºè14ÔºâÔºåÈ¶ñÊ¨°Âú®ÂØπÊäóÂüüÂØπÈΩê‰∏≠ÊòæÂºèÊéíÈô§Êú™Áü•Á±ªÔºåÂÆûÈ™åÊòæÁ§∫AUCÊèêÂçá11.3%ÔºàC‚ÜíA‰ªªÂä°Ëææ83.4%Ôºâ„ÄÇ\\n> *   **Â∑•Á®ãË¥°ÁåÆ**ÔºöÊûÑÂª∫Âê´9Á±ªÁöÑÊñ∞Âü∫ÂáÜÊï∞ÊçÆÈõÜÔºàÂ¶ÇACM-v8ÔºâÔºåÂú®6‰∏™‰ªªÂä°‰∏äOS*Âπ≥ÂùáÊèêÂçá12.8%ÔºàC‚ÜíD‰ªªÂä°Ëææ74.0%Ôºâ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   Ê†∏ÂøÉÂéüÁêÜÂü∫‰∫éÂàÜÁ¶ª-ÈÄÇÂ∫î‰∏§Èò∂ÊÆµÔºö\\n>     1. **ÂàÜÁ¶ªÈò∂ÊÆµ**ÔºöÂà©Áî®ÂØπÊäóÂ≠¶‰π†ÔºàÂºè9ÔºâËÆ≠ÁªÉGATÁºñÁ†ÅÂô®ÔºàÂºè5ÔºâÂíå(K+1)Á±ªÈÇªÂüüËÅöÂêàÂàÜÁ±ªÂô®ÔºàÂºè6ÔºâÔºåÈÄöËøáÊúÄÂ§ßÂåñÂàÜÁ±ªÂô®‰∏éÁºñÁ†ÅÂô®ÁöÑÊú™Áü•ÂØπÊäóÊçüÂ§±ÔºàÂºè8ÔºâÊûÑÂª∫Á≤óÁï•ËæπÁïå„ÄÇ\\n>     2. **ÈÄÇÂ∫îÈò∂ÊÆµ**ÔºöÂü∫‰∫éTheorem 2ÔºåÈÄöËøáÂä®ÊÄÅÂàÜÈÖçÂüüÈÄÇÂ∫îÁ≥ªÊï∞ÔºàŒª=1/-1ÔºâÂÆûÁé∞Êú™Áü•ÊéíÈô§ÁöÑÂØπÊäóÂØπÈΩêÔºàÂºè14ÔºâÔºåÂêåÊó∂Áî®Ëá™ËÆ≠ÁªÉÔºàÂºè11ÔºâËø≠‰ª£‰ºòÂåñËæπÁïå„ÄÇ\\n> *   ËÆæËÆ°Âì≤Â≠¶ÔºöÂà©Áî®ÂõæÂêåÈÖçÊÄßÔºàTheorem 1ÔºâÂ¢ûÂº∫Êú™Áü•Ê£ÄÊµãÔºåÈÄöËøáË¥üÁ≥ªÊï∞ÈÅøÂÖçÊú™Áü•Á±ªË¥üËøÅÁßª„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **ÂÖàÂâçÂ∑•‰ΩúÂ±ÄÈôê**Ôºö\\n>     - Èó≠ÈõÜÊñπÊ≥ïÔºàÂ¶ÇAdaGCNÔºâÁõ¥Êé•ÂØπÈΩêÊï¥‰∏™ÁõÆÊ†áÂàÜÂ∏ÉÔºåÂØºËá¥Êú™Áü•Á±ªË¥üËøÅÁßªÔºàË°®2‰∏≠AUC‰ªÖ54.87%Ôºâ„ÄÇ\\n>     - ÂçïÁΩëÁªúÊñπÊ≥ïÔºàÂ¶ÇG¬≤PxyÔºâÂøΩÁï•Ë∑®ÁΩëÁªúÂüüÂÅèÁßªÔºàC‚ÜíA‰ªªÂä°OS*‰ªÖ18.76%Ôºâ„ÄÇ\\n> *   **Êú¨ÊñáÊîπËøõ**Ôºö\\n>     - ÊèêÂá∫(K+1)Á±ªÈÇªÂüüËÅöÂêàÂàÜÁ±ªÂô®ÔºàÂºè6ÔºâÔºåÂà©Áî®ÂêåÈÖçÊÄßÊèêÂçáÊú™Áü•Ê£ÄÊµãÔºàTheorem 1Ôºâ„ÄÇ\\n>     - ËÆæËÆ°Ë¥üÂüüÈÄÇÂ∫îÁ≥ªÊï∞ÔºàÂºè14ÔºâÔºåÂÆûÈ™åÊòæÁ§∫HSÊèêÂçá9.5%ÔºàD‚ÜíC‰ªªÂä°Ëææ51.69%Ôºâ„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1. **ÂàÜÁ¶ªÈò∂ÊÆµ**Ôºö\\n>    - Áî®GATÁºñÁ†ÅÂô®ÔºàÂºè5ÔºâÁîüÊàêËäÇÁÇπÂµåÂÖ•ÔºåËæìÂÖ•(K+1)Á±ªÂàÜÁ±ªÂô®ÔºàÂºè6Ôºâ„ÄÇ\\n>    - ÊúÄÂ∞èÂåñÊ∫êÂàÜÁ±ªÊçüÂ§±ÔºàÂºè7ÔºâÂíåÊú™Áü•ÂØπÊäóÊçüÂ§±ÔºàÂºè8ÔºâÔºåÈÄöËøáGRLÂÆûÁé∞ÂØπÊäóËÆ≠ÁªÉÔºàÂºè9Ôºâ„ÄÇ\\n> 2. **ÈÄÇÂ∫îÈò∂ÊÆµ**Ôºö\\n>    - Âü∫‰∫éK-meansÂíåÂàÜÁ±ªÁΩÆ‰ø°Â∫¶ÁîüÊàê‰º™Ê†áÁ≠æÔºàÂºè10ÔºâÔºåÁ≠õÈÄâÁΩÆ‰ø°Ê†∑Êú¨„ÄÇ\\n>    - ÂüüÂà§Âà´Âô®ÈÄöËøáÂ∏¶GRLÁöÑÊçüÂ§±ÔºàÂºè12ÔºâÂØπÊäóËÆ≠ÁªÉÔºåÂä®ÊÄÅÂàÜÈÖçŒªÔºàÂºè14Ôºâ„ÄÇ\\n>    - Ëá™ËÆ≠ÁªÉËø≠‰ª£‰ºòÂåñËæπÁïåÔºàÂºè11ÔºâÔºåÊúÄÁªàËæìÂá∫ÁõÆÊ†áÊ†áÁ≠æÔºàÂºè6Ôºâ„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   Âõæ4ÂèØËßÜÂåñÊòæÁ§∫ÔºöUAGAÂú®C‚ÜíA‰ªªÂä°‰∏≠ÔºåÊú™Áü•Á±ªÔºàÁÅ∞Ëâ≤Ôºâ‰∏éÂ∑≤Áü•Á±ªÂàÜÁ¶ªÊòéÊòæÔºå‰∏îÁõ∏ÂêåÂ∑≤Áü•Á±ªÁöÑÊ∫ê/ÁõÆÊ†áËäÇÁÇπÔºàÂêåÈ¢úËâ≤ÔºâÂàÜÂ∏ÉÂØπÈΩê„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   ÂºÄÊîæÈõÜÂüüÈÄÇÂ∫îÔºöOSBP„ÄÅOMEGA\\n> *   ÂºÄÊîæÈõÜËäÇÁÇπÂàÜÁ±ªÔºöOODGAT„ÄÅG¬≤Pxy\\n> *   Â∞ÅÈó≠ÈõÜË∑®ÁΩëÁªúÂàÜÁ±ªÔºöUDAGCN„ÄÅAdaGCN„ÄÅSGDA\\n> *   ÂºÄÊîæÈõÜË∑®ÁΩëÁªúÂàÜÁ±ªÔºöUDANE„ÄÅSDA\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®OS*ÔºàÂ∑≤Áü•Á±ªÂáÜÁ°ÆÁéáÔºâ‰∏ä**ÔºöUAGAÂú®C‚ÜíD‰ªªÂä°Ëææ74.0%ÔºåÊòæËëó‰ºò‰∫éUDANEÔºà61.78%ÔºâÂíåSDAÔºà58.94%ÔºâÔºåÁõ∏ÂØπÊèêÂçá12.2Âíå15.1‰∏™ÁôæÂàÜÁÇπÔºàË°®2Ôºâ„ÄÇ\\n> *   **Âú®AUCÔºàÊú™Áü•Ê£ÄÊµãÔºâ‰∏ä**ÔºöUAGAÂú®C‚ÜíA‰ªªÂä°Ëææ83.4%ÔºåËøúË∂ÖOODGATÔºà55.45%ÔºâÂíåOMEGAÔºà60.18%ÔºâÔºåÊèêÂçá23.2‰∏™ÁôæÂàÜÁÇπÔºàË°®2Ôºâ„ÄÇ\\n> *   **Âú®HSÔºàË∞ÉÂíåÂùáÂÄºÔºâ‰∏ä**ÔºöUAGAÂú®D‚ÜíC‰ªªÂä°Ëææ51.69%ÔºåÊØîUDANEÔºà50.89%ÔºâÊõ¥ÂùáË°°ÔºåËØÅÊòéÂÖ∂ËÉΩÂêåÊó∂‰ºòÂåñÂ∑≤Áü•ÂàÜÁ±ªÂíåÊú™Áü•Ê£ÄÊµãÔºàË°®2Ôºâ„ÄÇ\\n> *   **ÂºÄÊîæÂ∫¶È≤ÅÊ£íÊÄß**ÔºöÂΩìÂºÄÊîæÂ∫¶‰ªé2/9Â¢ûËá≥6/9Êó∂ÔºåUAGAÁöÑOS*Ê≥¢Âä®‰ªÖ¬±3.2%ÔºåËÄåÂü∫Á∫øÊ≥¢Âä®Ëææ¬±15.7%ÔºàÂõæ3Ôºâ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   ÂºÄÊîæÈõÜË∑®ÁΩëÁªúËäÇÁÇπÂàÜÁ±ª (Open-set Cross-Network Node Classification, O-CNNC)\\n*   ÂõæÊ≥®ÊÑèÂäõÁΩëÁªú (Graph Attention Network, GAT)\\n*   ÂØπÊäóÂüüÈÄÇÂ∫î (Adversarial Domain Adaptation, ADA)\\n*   Ë¥üÂüüÈÄÇÂ∫îÁ≥ªÊï∞ (Negative Domain Adaptation Coefficient, N/A)\\n*   ÂêåÈÖçÊÄß (Homophily, N/A)\\n*   Ëá™ËÆ≠ÁªÉ (Self-training, N/A)\\n*   ‰º™Ê†áÁ≠æÁîüÊàê (Pseudo-label Generation, N/A)\\n*   Ê¢ØÂ∫¶ÂèçËΩ¨Â±Ç (Gradient Reversal Layer, GRL)\"\n}\n```"
}