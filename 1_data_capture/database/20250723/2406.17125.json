{
    "source": "Semantic Scholar",
    "arxiv_id": "2406.17125",
    "link": "https://arxiv.org/abs/2406.17125",
    "pdf_link": "https://arxiv.org/pdf/2406.17125.pdf",
    "title": "A Wiener Process Perspective on Local Intrinsic Dimension Estimation Methods",
    "authors": [
        "Piotr Tempczyk",
        "Lukasz Garncarek",
        "Dominik Filipiak",
        "Adam Kurpisz"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2024-06-24",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science",
        "Mathematics"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "institutions": [
        "Institute of Informatics, University of Warsaw",
        "NASK National Research Institute",
        "PL4AI",
        "Snowflake",
        "Adam Mickiewicz University",
        "Perelyn",
        "University of Innsbruck",
        "BFH Bern Business School",
        "ETH Zurich"
    ],
    "paper_content": "# A Wiener Process Perspective on Local Intrinsic Dimension Estimation Methods\n\nPiotr Tempczyk\\*1,2,3, Łukasz Garncarek3,4, Dominik Filipiak5,6,7, Adam Kurpisz\\*3,8,9\n\n1Institute of Informatics, University of Warsaw, Poland 2NASK National Research Institute, Warsaw, Poland 3PL4AI, Poland 4Snowflake, USA 5Adam Mickiewicz University, Poznan´, Poland 6Perelyn, Warsaw, Poland 7University of Innsbruck, Austria 8BFH Bern Business School, Switzerland 9ETH Zurich, Switzerland tempczyk.piotr $@$ gmail.com, lukgar@gmail.com, df $@$ amu.edu.pl, adam.kurpisz $@$ ifor.math.ethz.ch\n\n# Abstract\n\nLocal intrinsic dimension (LID) estimation methods have received a lot of attention in recent years thanks to the progress in deep neural networks and generative modeling. In opposition to old non-parametric methods, new methods use generative models to approximate diffused dataset density to scale the methods to high-dimensional datasets (e.g. images). In this paper, we investigate the recent state-of-the-art parametric LID estimation methods from the perspective of the Wiener process. We explore how these methods behave when their assumptions are not met. We give an extended mathematical description of those methods and their error as a function of the probability density of the data.\n\nExtended version of paper — arxiv.org/abs/2406.17125\n\n# 1 Introduction\n\nLID estimation has gained increasing attention in recent years as part of the fast-growing field of topological data analysis. It was able to progress from non-parametric to parametric methods thanks to the latest progress in the field of generative modeling. LID estimation methods are algorithms for estimating the manifold’s dimension from which the data point $x$ was sampled. In these methods, we assume that the data lie on the union of one or more manifolds that may be of different dimensions.\n\nThe estimation of intrinsic dimension is substantial for data analysis and machine learning (Ansuini et al. 2019; Li et al. 2018; Rubenstein, Schoelkopf, and Tolstikhin 2018) and was investigated in relation to dimensionality reduction and clustering (Vapnik 2013; Kleindessner and Luxburg 2015; Camastra and Staiano 2016), analyzing the training and representation learning processes within deep neural networks (Li et al. 2018; Ansuini et al. 2019; Pope et al. 2020; Loaiza-Ganem et al. 2024), verifying the union of manifolds hypothesis for images (Brown et al. 2022), and used to improve out-of-distribution detection algorithm (Kamkari et al. 2024a).\n\nPrior to introducing LIDL (Tempczyk et al. 2022), two approaches to solve the intrinsic dimension estimation problem were presented. The first employs global methods; see, e.g., Fukunaga and Olsen (1971); Minka (2000); Fan et al. (2010). These methods are known to suffer from issues related to a manifold curvature and non-uniformity of the data distribution. They also assume that data lies on a single manifold of constant dimension, so dimensionality is the same for all $x$ . The second approach is based on local nonparametric methods that explore the geometric properties of neighborhoods (Johnsson, Soneson, and Fontes 2014; Levina and Bickel 2004) calculating some statistics using points from the neighborhood of $x$ . Although all aforementioned methods perform reasonably well for a small number of dimensions, the higher dimensionality negatively affects their performance (Tempczyk et al. 2022; Campadelli et al. 2015; Camastra and Staiano 2016).\n\nVery recently, a new approach emerged for the local methods, which is based on a two-step procedure. In the first step, the dataset is perturbed with some noise, and in the second step, its dimensionality is estimated using various techniques. These include generative models to analyze changes in density (Tempczyk et al. 2022; Kamkari et al. 2024b) or in singular values of the Jacobian (Horvat and Pfister 2022, 2024) for different noise magnitudes, and analyzing rank of scores from diffusion model as recently presented by Stanczuk et al. (2024).\n\nWhile new algorithms provide state-of-the-art results for large, real-life datasets, it is vital for their performance that adding noise to the dataset should be as close as possible to an injective process. This means that the resulting densities after adding noise should uniquely identify the original distribution of the dataset. Otherwise, it is impossible to neither uniquely reverse the process nor analyze the original structure of the dataset. One example in the theoretical model that does not admit such a problem is a flat manifold with a uniform distribution of data points. In such cases these algorithms perform best, as shown in experimental results.\n\nIn other cases, a hypothetical line of research would be to decrease the magnitude of noise added to the data set so that the manifold is approximately flat and has locally uniform data density in the scale of the noise magnitude. This, however, is not possible in practice for several reasons. For instance, data is often a set of discrete points (especially in audio and visual modality), and considering the noise of magnitude much smaller than the minimum distance between points does not lead to any meaningful process. Moreover, neural net training is done with finite precision and the stochastic gradient descent procedure introduces noise to the density estimates, so very small changes in density cannot be observed in practice, which may lead to poor quality estimates of LID.\n\nIn this paper, we point out and exploit the fact that adding Gaussian noise of varying magnitudes can be seen as studying the evolution of the Wiener process describing the diffusion of particles (points of the dataset) in the ambient space. This point of view enables us to employ Fick’s Second Law of Diffusion to eliminate time derivatives from mathematical descriptions of state-of-the-art LID algorithms (Tempczyk et al. 2022; Kamkari et al. 2024b), and replace them with spatial derivatives. Such considerations can be taken into account in the second step of the considered algorithms, leading to more accurate results. We encourage reader to get familiar with Appendix A from the extended version of the paper (arxiv.org/abs/2406.17125), which contains important definitions and clarifications regarding this work.\n\n# Contribution\n\n1. We recognize and define new categories (isolated and holistic algorithms) for the Wiener process-based parametric LID estimation algorithm family and categorize the existing algorithms accordingly.   \n2. We explore the first step of existing algorithms in the language of Wiener processes and calculate important cases of diffusion from lower-dimensional manifolds with nonuniform probability density into ambient space.   \n3. We derive closed-form expressions for important parameters used in two state-of-the-art isolated LID estimation algorithms as a function of on-manifold density and manifold dimensionality, which can be viewed as closedform expressions for deviation from a flat manifold with uniform distribution case.\n\n# 2 Related Work\n\nThe review of the non-parametric methods for local and global intrinsic dimension estimation can be found in the work of Campadelli et al. (2015), or Camastra and Staiano (2016). Tempczyk et al. (2022) compared these methods on bigger datasets in terms of their dimensionality.\n\nAlthough we do not analyze non-parametric methods in this paper, it is worth mentioning a recent work on nonparametric LID estimation, in which Bailey, Houle, and Ma (2022) explore the connection of LID to other wellknown measures for complexity: entropy and statistical divergences, and develop new analytical expressions for these quantities in terms of LID. Consequently, Bailey, Houle, and\n\nMa (2023) establish the relationships for cumulative Shannon entropy, entropy power, Bregman formulation of cumulative Kullback-Leibler divergence, and generalized Tsallis entropy variants, and propose four new estimators of LID, based on nearest neighbor distances.\n\nDuring the last few years many parametric methods for estimating LID emerged. Zheng et al. (2022) prove that VAE are capable of recovering the correct manifold dimension and demonstrate methods to learn manifolds of varying dimensions across the data sample. Yeats et al. (2023) connect the adversarial vulnerability of score models with the geometry of the underlying manifold they capture. They show that minimizing the Dirichlet energy of learned score maps boosts their robustness while revealing the LID of the underlying data manifold.\n\nWiener process-based algorithms. Regarding parametric methods, there is a group of algorithms, that have one thing in common: they simulate a Wiener process on a dataset and directly use some properties of time-evolving density to estimate LID. We can divide those algorithms into the following three groups:\n\n1. LIDL (Tempczyk et al. 2022) and its efficient and most accurate implementation using diffusion models called FLIPD (Kamkari et al. 2024b). These algorithms use the rate of change of the probability density at point $x$ during the Wiener process to estimate LID at $x$ . For small diffusion times $t$ the logarithm of a density is a linear function of a logarithm of $t$ , and the proportionality coefficient is equal $d - D$ for small $t$ , where $d$ is manifold density and $D$ is ambient space dimensionality. Our experiments with ID-NF (described below) and FLIPD show, that the latter is more scalable than ID-NF (and ID-DM) due to the high memory and computational complexity of SVD (which has to be calculated for each data point). Experiments from (Kamkari et al. 2024b) show that FLIPD is more accurate than NB (described below), which led us to the conclusion, that FLIPD is state-of-the-art in LID estimation among the most scalable algorithms.   \n2. ID-NF (Horvat and Pfister 2022) (using normalizing flows), and the diffusion-using follow-up paper ID-DM (Horvat and Pfister 2024) analyze how singular values of a Jacobian of a function transforming a standard normal distribution into a diffused dataset density at $x$ evolves during the Wiener process. Horvat and Pfister observed that when transforming a $d$ -dimensional manifold into a $D$ -dimensional Gaussian we have to expand space more in the normal direction to manifold, especially for small diffusion times $t$ .   \n3. NB (Stanczuk et al. 2024), which is an abbreviation from Normal Bundle (name used in (Kamkari et al. 2024b)). Stanczuk et al. observed that for small diffusion times $t$ the gradients of the logarithm of a diffused data density (score function) close to $x$ lies in the normal space to the manifold and use this fact to estimate LID at $x$ .   \n3 Isolated and Holistic Algorithms\n\n#\n\nIn this work, we take a closer look at the Wiener processbased algorithms described in the last paragraph of Section 2. Although all these algorithms apply a Wiener process to the dataset during their first phase, when looking at their second step, we can divide them into two groups: isolated (LIDL, FLIPD, NB) and holistic (ID-NF, ID-DM). The intermediate results of the first group that are used for LID calculation use only the information about the local shape of the data probability density function (without normalization constant). We assume that the generative model approximates diffused data distribution $\\rho _ { t }$ perfectly. Their estimates depend on the proximity of $\\nabla \\log \\rho _ { t }$ to $x$ (NB) or ${ d \\log \\rho _ { t } } / { d \\log t }$ at $x$ (LIDL, FLIPD).\n\nThis is a consequence of $\\rho _ { t }$ at $x$ being a function that – in practice – depends on the values of original data distribution $p _ { S }$ in a ball of radius $r \\approx 4 \\sqrt { t }$ around $x$ in the data space. The values of $p _ { S }$ outside this ball do not matter in practice for isolated algorithms, because the diffused particles in the Wiener process can travel longer distances than $r$ with very low probability (less than $3 . 7 \\cdot 1 0 ^ { - 5 } \\cdot$ ). When we add some new data points to the dataset far away from $x$ , it does not change the shape of $\\rho _ { t }$ close to $x$ . This operation only changes a normalization constant, which becomes 0 after taking a logarithm of $\\rho _ { t }$ and taking a derivative either w.r.t time or spatial variables.\n\nThe holistic algorithms work quite differently. In the case of ID-NF and ID-DM, they calculate singular values of the Jacobian of the function $\\zeta : \\mathrm { R } ^ { N } \\to \\mathrm { R } ^ { N }$ transforming $\\rho _ { t }$ into a standard normal distribution. This $\\zeta$ function strongly depends on the entire shape of $\\rho _ { t }$ . As mentioned before, when we change $\\rho _ { t }$ far away from $x$ we do not change the shape of $\\rho _ { t }$ close to $x$ . The same is not true for $\\zeta ( x )$ . When we add many data points to the dataset far away from $x$ while we keep the latent distribution fixed, we have to change the way we compress and stretch the space to match the new distribution. This property makes the analysis of the behavior of ID-NF and ID-DM much harder (maybe even impossible) if we want to take into account only the data density in the neighbourhood of $x$ .\n\nTo give an illustrative example of this behavior, one needs to imagine that our dataset is one-dimensional and consists of 10K points sampled from $\\mathcal { N } ( 0 , 1 )$ . Typically, we are transforming it into $\\mathcal { N } ( 0 , 1 )$ , so $\\zeta$ is just an identity function. Now let’s add to the dataset another 10K points sampled from $\\mathcal { N } ( 1 0 0 , 1 )$ . As a consequence, we have to stretch our space in some areas to transform one density into another, whereas $\\zeta$ changes along with its Jacobian.\n\n# 4 Wiener Process Perspective on LID Estimation\n\nWiener process is a stochastic process modeling particle diffusion. Its increments over disjoint time intervals are independent and normally distributed, with variance proportional to time increments. Since in the machine learning community the term diffusion is already overloaded, we will stick to Wiener process when speaking of particle diffusion process.\n\nIn this section we present a new perspective on perturbing datasets, unifying the approaches seen in the algorithms presented by Tempczyk et al. (2022); Stanczuk et al. (2024);\n\nHorvat and Pfister (2022, 2024); Kamkari et al. (2024b). As already mentioned, all these algorithms consist of two stages, the first of which amounts to perturbing the dataset with normally distributed random noise of fixed variance $t$ . In the second stage, each of the algorithms utilizes the behavior of the perturbed density in the neighborhood of a fixed point under changes in the noise variance.\n\nThe first phase of each algorithm can be interpreted as applying the Wiener process to the points in the dataset. Afterward, the resulting set of points is used to train some type of generative model (or models) to estimate the distribution of the dataset undergoing the Wiener process at time $t$ . From the point of view of differential equations, the distribution density function of the diffused dataset is described by Fick’s Second Law of Diffusion.\n\nFick’s Second Law of Diffusion. Let $\\rho _ { t } : \\mathbb { R } ^ { D } \\mapsto \\mathbb { R }$ denote the probability density function modeling particles undergoing diffusion at time $t$ . Then $\\rho _ { t }$ satisfies the differential equation\n\n$$\n\\frac { d } { d t } \\rho _ { t } = C \\Delta \\rho _ { t } ,\n$$\n\nwhere $C \\in \\mathbb { R } ,$ , and $\\Delta$ stands for the standard Laplacian in RD.\n\nNow, given a dataset embedded in $\\mathbb { R } ^ { D }$ , we assume that it has been drawn from some latent union of submanifolds $S$ endowed with a probability measure $p _ { S }$ (which can be naturally treated as a probability measure on $\\dot { \\mathbb { R } } ^ { D }$ ). The goal of Local Intrinsic Dimension estimation is to find out the dimension of $S$ at any point of the dataset.\n\nTo model the Wiener process with initial distribution $p _ { S }$ (which is not a function on $\\mathbb { R } ^ { D }$ ), let us first define\n\n$$\n\\phi _ { t } ^ { D } ( x ) = ( 2 \\pi t ) ^ { - D / 2 } e ^ { - \\| x \\| ^ { 2 } / 2 t } .\n$$\n\nThis is the density of normal distribution on $\\mathbb { R } ^ { D }$ with covariance matrix $t I$ . It is the fundamental solution of the differential equation given by Fick’s Second Law of Diffusion (1) with $C = 1 / 2$ . Here, this means that the convolution\n\n$$\n\\rho _ { t } = p _ { S } * \\phi _ { t } ^ { D }\n$$\n\nis the solution of (1) for $t > 0$ and hence it describes the Wiener process starting from the initial probability distribution pS.\n\nTo limit the complexity introduced by curvature, from now on we will consider only flat manifolds. This means that, without loss of generality, we may assume that $S$ is the first factor in product decomposition RD = Rd  RD−d. We will denote the coordinates of $\\mathbb { R } ^ { d }$ and $\\mathbb { R } ^ { D - d }$ by $x$ and $y$ , respectively. We will moreover assume that $p _ { S }$ , now a probability distribution on $\\mathbb { R } ^ { d }$ , has a density $\\psi : \\mathbb { R } ^ { d }  \\mathbb { R }$ .\n\nIn Appendix B we discuss the Laplacian of $\\rho _ { t }$ and derive the following result:\n\nLemma 4.1 (Lemma B.1). For $t > 0$ and $( x , y ) \\in \\mathbb { R } ^ { D }$ we have\n\n$$\n\\begin{array} { l } { \\displaystyle \\Delta \\rho _ { t } ( x , y ) = \\left( \\frac { \\| y \\| ^ { 2 } } { t ^ { 2 } } + \\frac { d - D } { t } \\right) \\rho _ { t } ( x , y ) } \\\\ { \\displaystyle + \\phi _ { t } ^ { D - d } ( y ) \\Delta _ { x } ( \\psi * \\phi _ { t } ^ { d } ) ( x ) . } \\end{array}\n$$\n\nAs a consequence, by putting $y = 0$ and using\n\n$$\n\\phi _ { t } ^ { D - d } ( 0 ) = ( 2 \\pi t ) ^ { ( d - D ) / 2 }\n$$\n\nwe obtain the following.\n\nCorollary 4.2. For $t > 0$ and $\\boldsymbol { x } \\in \\mathbb { R } ^ { d }$ we have\n\n$$\n\\Delta \\rho _ { t } ( x , 0 ) = \\frac { d - D } { t } \\rho _ { t } ( x , 0 ) + ( 2 \\pi t ) ^ { ( d - D ) / 2 } \\Delta _ { x } ( \\psi * \\phi _ { t } ^ { d } ) ( x ) .\n$$\n\n# 5 From Wiener Process to LID Estimation\n\nThe findings from the last section can be used to analyze how the first family of algorithms (Tempczyk et al. 2022; Kamkari et al. 2024b) behaves for some particular cases. The main contribution of Kamkari et al. (2024b) is a substantial improvement on the side of density estimation. Therefore, when dealing with perfect density estimators and very small noise differences, both algorithms estimate the same quantity and give the same results from the theoretical perspective. Due to this fact from now on we will be analyzing LIDL, as we want to analyze the aspects of those implementations that do not depend on the problems with density estimation itself.\n\n# Reformulating LIDL\n\nGiven a point $x \\in S$ and a set of times $t _ { 1 } , \\ldots , t _ { n }$ , LIDL estimates the linear regression coefficient $\\alpha$ of the set of points $( \\log \\delta _ { i } , \\log \\rho _ { t _ { i } } ( x ) )$ , where $\\delta _ { i } = \\sqrt { t _ { i } }$ . Tempczyk et al. (2022) proved that\n\n$$\n\\log \\rho _ { t } ( x ) = ( d - D ) \\log \\sqrt { t } + O ( 1 ) ,\n$$\n\nand therefore $\\alpha \\approx d - D$ . The authors show that if $t$ is small enough, this estimate is accurate.\n\nThis procedure can be seen as approximating the asymptotic slope of the parametric curve $( \\log \\sqrt { t } , \\log \\rho _ { t } ( x ) )$ . In other words, the graph of $s \\mapsto \\log \\rho _ { e ^ { 2 s } } ( x )$ for $s  - \\infty$ . Another approach would consider the its derivative. Let us define its reparameterized derivative (with $t = e ^ { 2 s }$ )\n\n$$\n\\beta _ { t } ( x ) = \\frac { 2 t } { \\rho _ { t } ( x ) } \\frac { d } { d t } \\rho _ { t } ( x ) = \\frac { t \\Delta \\rho _ { t } ( x ) } { \\rho _ { t } ( x ) } ,\n$$\n\nwhere the last equality comes from the diffusion equation (1) with $C = 1 / 2$ . Moreover, denote the asymptotic slope of the aforementioned curve by\n\n$$\n\\beta ( x ) = \\operatorname* { l i m } _ { s  - \\infty } \\frac { d } { d s } \\log \\rho _ { e ^ { 2 s } } ( x ) = \\operatorname* { l i m } _ { t  0 ^ { + } } \\beta _ { t } ( x ) .\n$$\n\nThe results presented below are proved in Appendix $\\mathbf { C }$ . The next Proposition shows that the two approaches discussed above are equivalent.\n\nProposition 5.1 (Proposition C.1). Given a strictly positive differentiable function $f \\colon ( 0 , a ) \\  \\ ( 0 , \\infty )$ and a positive real number $\\alpha > 0$ , the following conditions are equivalent.\n\n1. The function $f$ explodes at 0 like $t ^ { - \\alpha }$ , i.e. for some positive constants $c , C > 0$ one has $c < t ^ { \\alpha } f ( t ) < C$ for some $\\epsilon > 0$ and $t \\in ( 0 , \\epsilon )$ .\n\nAs a consequence, the estimation of Local Intrinsic Dimension using LIDL can be achieved by computing $\\beta ( x )$ , yielding $d = D + \\beta ( x )$ .\n\nProposition 5.2 (Proposition C.2). For $t$ near 0 the following estimate holds\n\n$$\n\\log \\rho _ { t } ( x ) = \\beta ( x ) \\log \\sqrt { t } + O ( 1 ) .\n$$\n\nThe next proposition provides an elegant expression for $\\beta _ { t } ( x )$ , and consequently for $\\beta ( x )$ , expressed in terms of the density $\\psi$ on $\\mathbb { R } ^ { d }$ .\n\nProposition 5.3 (Proposition C.3). For $t > 0$ and $x \\in S =$ $\\mathbb { R } ^ { d } \\subseteq \\mathbb { R } ^ { D }$ we have\n\n$$\n\\beta _ { t } ( x ) = d - D + \\frac { \\Delta _ { x } ( \\psi * \\phi _ { t } ^ { d } ) ( x ) } { \\psi * \\phi _ { t } ^ { d } ( x ) } \\cdot t .\n$$\n\n# LIDL Examples\n\nFrom the theoretical considerations of Tempczyk et al. (2022) it follows that $\\beta ( x ) \\ = \\ d - \\ D$ if $\\psi$ is sufficiently regular and positive near $x$ . In other words,\n\n$$\n\\operatorname* { l i m } _ { t  0 ^ { + } } \\frac { \\Delta _ { x } ( \\psi * \\phi _ { t } ^ { d } ) ( x ) } { \\psi * \\phi _ { t } ^ { d } ( x ) } \\cdot t = 0 .\n$$\n\nNow, we will try to obtain this conclusion directly and calculate bias of LIDL for $t > 0$ in a few special cases by analyzing the behavior of $\\beta _ { t } ( x )$ .\n\nThe “uniform distribution” on Euclidean space. There is no such thing as the uniform distribution on $\\mathbf { \\bar { \\mathbb { R } } } ^ { d }$ . However, from a purely theoretical viewpoint, in our differential equation approach we don’t need the assumption of $\\phi$ being a probability density; it could be any function. And since constant functions are usually the simplest examples, we will now investigate what happens if we put $\\psi ( x ) \\equiv 1$ on the whole $\\mathbb { R } ^ { d }$ space.\n\nUsing Proposition 5.3 and the fact that $\\psi$ has bounded derivatives, this case leaves us with\n\n$$\n\\beta _ { t } ( x ) = d - D + \\frac { \\Delta _ { x } \\psi * \\phi _ { t } ^ { d } ( x ) } { \\psi * \\phi _ { t } ^ { d } ( x ) } \\cdot t = d - D ,\n$$\n\nsince $\\Delta _ { x } \\psi \\equiv 0$ . This expression is constant in $t$ , and in particular its limit at 0 is $\\beta ( x ) = d - D$ . In this case, LIDL estimator is not biased for all $t > 0$ .\n\nNormal distribution. Now consider the normal distribution on $\\mathbb { R } ^ { d }$ with covariance matrix $\\Sigma = \\mathrm { d i a g } ( \\sigma _ { 1 } ^ { 2 } , \\dots , \\sigma _ { d } ^ { 2 } )$ , and denote its density function by $\\psi$ . The convolution $\\psi * \\phi _ { t } ^ { d }$ is the density of the normal distribution with covariance matrix $\\Sigma + t I$ . If we simplify notation be putting $\\phi _ { i } = \\phi _ { \\sigma _ { i } ^ { 2 } + t } ^ { 1 }$ , we get\n\n$$\n\\psi * \\phi _ { t } ^ { d } ( x ) = \\prod _ { i = 1 } ^ { d } \\phi _ { i } ( x _ { i } ) .\n$$\n\nTo compute the Laplacian of this convolution, note that\n\n$$\n\\psi * \\phi _ { t } ^ { d } ( x ) = \\frac { \\psi * \\phi _ { t } ^ { d } ( x ) } { \\phi _ { i } ( x _ { i } ) } \\cdot \\phi _ { i } ( x _ { i } ) ,\n$$\n\nwhere the first factor does not depend of $x _ { i }$ , and therefore\n\n$$\n{ \\frac { \\partial ^ { 2 } ( \\psi * \\phi _ { t } ^ { d } ) } { \\partial x _ { i } ^ { 2 } } } ( x ) = \\psi ( x ) * \\phi _ { t } ^ { d } ( x ) \\cdot { \\frac { 1 } { \\phi _ { i } ( x _ { i } ) } } { \\frac { \\partial ^ { 2 } \\phi _ { i } } { \\partial x _ { i } ^ { 2 } } } ( x _ { i } ) ,\n$$\n\nleading to\n\n$$\n\\begin{array} { l } { \\displaystyle \\beta _ { t } ( x ) = d - D + t \\frac { \\Delta ( \\psi * \\phi _ { t } ^ { d } ) ( x ) } { \\psi * \\phi _ { t } ^ { d } ( x ) } = } \\\\ { = d - D + t \\displaystyle \\sum _ { i = 1 } ^ { d } \\frac { 1 } { \\phi _ { i } ( x _ { i } ) } \\frac { \\partial ^ { 2 } \\phi _ { i } } { \\partial x _ { i } ^ { 2 } } ( x _ { i } ) } \\\\ { = d - D + t \\displaystyle \\sum _ { i = 1 } ^ { d } \\frac { x _ { i } ^ { 2 } - ( \\sigma _ { i } ^ { 2 } + t ) } { ( \\sigma _ { i } ^ { 2 } + t ) ^ { 2 } } . } \\end{array}\n$$\n\nIt is easy to see that the second derivatives of $\\phi _ { i }$ are continuous in $\\dot { \\tau } > - \\sigma _ { i } ^ { 2 }$ , so the sum in the above expression has finite limit for $t \\to 0$ , and therefore $\\beta ( x ) = d - D$ .\n\nIn the special case where $\\Sigma \\ : = \\ : \\sigma ^ { 2 } \\dot { I }$ , these calculations simplify further, as $\\psi * \\phi _ { t } ^ { d } = \\phi _ { \\sigma ^ { 2 } + t } ^ { d }$ , and since\n\n$$\n\\Delta _ { x } \\phi _ { \\sigma ^ { 2 } + t } ^ { d } ( x ) = \\left( \\frac { \\left. x \\right. ^ { 2 } } { ( \\sigma ^ { 2 } + t ) ^ { 2 } } - \\frac { d } { \\sigma ^ { 2 } + t } \\right) \\phi _ { \\sigma ^ { 2 } + t } ^ { d } ( x ) ,\n$$\n\nwe have\n\n$$\n\\beta _ { t } ( x ) = d - D + \\left( \\frac { \\left\\| x \\right\\| ^ { 2 } } { ( \\sigma ^ { 2 } + t ) ^ { 2 } } - \\frac { d } { \\sigma ^ { 2 } + t } \\right) t .\n$$\n\nThese results express analytically the experimental observations from LIDL and FLIPD papers, as can be verified by looking at Fig. 1a. We can observe, that if we move to the regions of very low probability for a Gaussian, it generates very high positive bias, which may highly overestimate the true LID (also observed as a bump at $\\bar { t = 1 0 ^ { - 1 2 } }$ in Fig. 1b). Luckily, most of the points in our dataset come from the region of high probability, but we should be less certain of the estimates for points from low probability regions.\n\nIt is worth noting, that the values of $t$ used to generate Fig. 1a are the same as values of $\\delta$ , which is equal to $\\sqrt { t }$ in our convention. After double-checking our results we argue that the most probable cause of this is that the authors of LIDL used squared values of $\\delta$ by mistake. Additionally, one can observe that curves plotted by Tempczyk et al. (2022) are somewhat flatter than one in this study. The fact that in their paper the derivative was approximated by linear regression on numerically calculated densities – which may lead to slightly different results – might be a possible reason.\n\nArbitrary distribution with sufficiently nice density. By this point, the notion of nice density is shall be more clear. We want to be able to use the equality\n\n$$\n\\Delta _ { x } ( \\psi * \\phi _ { t } ^ { d } ) = \\Delta _ { x } \\psi * \\phi _ { t } ^ { d } .\n$$\n\nTo do so, we need $\\psi$ to be bounded, twice differentiable, and have bounded first and second-order partial derivatives.\n\nLIDL estimate as a function of $x$ for $\\mathcal { N } ( 0 , 1 )$\n\n![](images/c4c815164923d9ebe9c195f1c79b63512f1cbfb3735a6fb559a4847a05970d49.jpg)\n\n(a) Example of the bias of a LIDL estimate for different points from $\\mathcal { N } ( \\bar { 0 } , 1 )$ and for different values of $t$ . This plot recreates a numerical calculations presented in Figure 4 from (Tempczyk et al. 2022), with two minor differences described in Sec. 5\n\n![](images/21133d31c55706a2141aea7aaff944b83c53884a5061b8ff8bc5e14c22487211.jpg)\n\n(b) Plot of a LIDL estimate as a function of $t$ for the distribution $\\mathcal { N } ( \\mathbf { 0 } , \\mathrm { d i a g } ( 1 , 1 0 ^ { - 6 } , 1 0 ^ { - 1 2 } ) )$ and for three different points ${ \\textbf { x } } =$ $( 0 , 0 , x _ { 3 } )$ , which represents a distance of 0, 1 and $2 \\ \\sigma _ { 3 }$ from 0 on 3rd dimension (compare with Fig. 8 from FLIPD and Fig. 2 from LIDL).\n\nFigure 1: LIDL estimates for Gaussian distributions.\n\nWe will also require $\\psi$ to have continuous second-order partial derivatives. This is not a severe restriction, as numerous distributions satisfy these properties – including the normal distribution or more generally, mixtures of Gaussians.\n\nIn this case, we have\n\n$$\n\\beta _ { t } ( x ) = d - D + \\frac { \\Delta _ { x } \\psi * \\phi _ { t } ^ { d } ( x ) } { \\psi * \\phi _ { t } ^ { d } ( x ) } \\cdot t ,\n$$\n\nhowever this time $\\Delta _ { x } \\psi$ is some arbitrary continuous function. Being differentiable, $\\psi$ is also continuous, and we can use the general fact that for a bounded continuous function, $f$ on $\\mathbb { R } ^ { \\tilde { d } }$ one has\n\n$$\n\\operatorname * { l i m } _ { t \\to 0 ^ { + } } f * \\phi _ { t } ^ { d } ( x ) = f ( x ) .\n$$\n\nThis gives us, for $x$ such that $\\psi ( x ) > 0$ ,\n\n$$\n\\operatorname* { l i m } _ { t \\to 0 ^ { + } } \\frac { \\Delta _ { x } \\psi * \\phi _ { t } ^ { d } ( x ) } { \\psi * \\phi _ { t } ^ { d } ( x ) } \\cdot t = \\frac { \\Delta _ { x } \\psi ( x ) } { \\psi ( x ) } \\operatorname* { l i m } _ { t \\to 0 ^ { + } } t = 0 ,\n$$\n\nand again $\\beta ( x ) = d - D$ . It has been already proven that in this case $\\beta$ yields a correct estimate of dimension, circumventing complexities of Tempczyk et al. (2022) proofs.\n\nIt is worth noting, that when $\\Delta _ { x } \\psi = 0$ , the estimate is accurate. It is the case for the aforementioned “uniform distribution” on $\\mathbb { R } ^ { d }$ , but it is also true if locally the density is a linear function of $x$ . In Fig. 1a we can observe that for $x \\approx \\pm 1$ (Laplacian of a Gaussian density equals 0 at these points) and small values of $t$ , the estimate is accurate.\n\nUniform distribution supported on an interval. Now consider an example where the density is not differentiable – the uniform distribution on an interval $[ a , b ] \\subset \\mathbb { R }$ , i.e.\n\n$$\n\\psi ( x ) = \\frac { 1 } { b - a } \\chi _ { [ a , b ] } ( x ) ,\n$$\n\nwhere $\\chi _ { A } ( s )$ is the indicator function of the set $A$ , equal to 1 on $A$ and 0 outside $A$ . In the next example, we will generalize this to a hypercube, but the core observations can be made in this simpler 1-dimensional case.\n\nThe difficulty introduced by the non-differentiability of $\\psi$ is we are no longer allowed to move the Laplacian inside the convolution to get\n\n$$\n\\Delta _ { x } ( \\psi * \\phi _ { t } ) = \\Delta _ { x } \\psi * \\phi _ { t }\n$$\n\n(we omit the superscript $d = 1$ from $\\phi _ { t }$ ) – as tempting as it might be. Therefore, a different manner of proceeding is needed. We may still move the Laplacian to $\\phi _ { t }$ . In the 1- dimensional case, $\\Delta _ { x }$ is simply the second derivative, and since\n\n$$\n\\phi _ { t } ^ { \\prime } ( u ) = - u \\phi _ { t } ( u ) / t\n$$\n\nwe have\n\n$$\n\\begin{array} { l } { \\displaystyle \\Delta _ { x } ( \\psi * \\phi _ { t } ) ( x ) = \\frac { 1 } { b - a } \\int _ { x - b } ^ { x - a } \\phi _ { t } ^ { \\prime \\prime } ( u ) d u } \\\\ { = \\frac { \\phi _ { t } ^ { \\prime } ( x - a ) - \\phi _ { t } ^ { \\prime } ( x - b ) } { b - a } = } \\\\ { = \\frac { ( x - b ) \\phi _ { t } ( x - b ) - ( x - a ) \\phi _ { t } ( x - a ) } { t ( b - a ) } , } \\end{array}\n$$\n\nExpanding the denominator in a similar fashion yields\n\n$$\n\\beta _ { t } ( x ) = d - D + \\frac { ( x - b ) \\phi _ { t } ( x - b ) - ( x - a ) \\phi _ { t } ( x - a ) } { \\Phi _ { t } ( x - a ) - \\Phi _ { t } ( x - b ) } ,\n$$\n\nwhere $\\Phi _ { t }$ is the cumulative distribution function corresponding to the density $\\phi _ { t }$ . In particular for $x \\in ( a , b )$ we see that since $x - b < 0 < x - a .$ , when $t \\to 0 ^ { + }$ , the denominator tends to 1, while both terms of the numerator tend to 0, leaving us with $d - D$ . LIDL estimate curves for this case for different values of $t$ are plotted in Fig. 2a.\n\nUniform distribution supported on a hypercube. Let us now consider a more general case – the uniform distribution on a hypercube $[ a _ { 1 } , \\overbar { b _ { 1 } } ] \\times \\cdot \\cdot \\cdot \\times [ a _ { d } , b _ { d } ] \\subset \\mathbb { R } ^ { d }$ . We have\n\n$$\n\\psi ( x ) = \\prod _ { i = 1 } ^ { d } \\frac { 1 } { b _ { i } - a _ { i } } \\chi _ { [ a _ { i } , b _ { i } ] } ( x _ { i } ) ,\n$$\n\nLIDL estimate as a function of $x$ for $\\boldsymbol { { \\mathcal U } } ( 0 , 1 )$\n\n![](images/ae47582912630dce5f348bd7ad4449fa90dab9cfc81fe5aeae1163d0ac379e68.jpg)\n\n(a) Example of the bias of a LIDL estimate for different points from $\\mathcal { U } ( 0 , 1 )$ and values of $t$ . This plot recreates a numerical calculations presented in Figure 3 from (Tempczyk et al. 2022)\n\n![](images/20c8963bac894011268971a83a638dcfe0fb306805053a23f00a967554038b25.jpg)  \nFigure 2: LIDL estimates.\n\n(b) LIDL estimate as a function of t for a point from parallel 1D manifolds separated by a distance of 1 with uniform distribution on them. Similar to result from Fig. 6 in (Tempczyk et al. 2022).\n\nDenote\n\n$$\n\\psi _ { i } ( s ) = { \\frac { 1 } { b _ { i } - a _ { i } } } \\chi _ { [ a _ { i } , b _ { i } ] } ( s ) ,\n$$\n\nand observe that since $\\phi _ { t } ^ { d } ( x )$ is the product of $\\phi _ { t } ( x _ { i } )$ , we have\n\n$$\n\\psi * \\phi _ { t } ^ { d } ( x ) = \\prod _ { i = 1 } ^ { d } \\psi _ { i } * \\phi _ { t } ( x _ { i } ) .\n$$\n\nBy directly computing the derivatives, we obtain\n\n$$\n\\frac { \\Delta _ { x } \\psi * \\phi _ { t } ^ { d } ( x ) } { \\psi * \\phi _ { t } ^ { d } ( x ) } = \\sum _ { i = 1 } ^ { d } \\frac { ( \\psi _ { i } * \\phi _ { t } ) ^ { \\prime \\prime } ( x _ { i } ) } { \\psi _ { i } * \\phi _ { t } ( x _ { i } ) } = \\sum _ { i = 1 } ^ { d } \\frac { \\psi _ { i } * \\phi _ { t } ^ { \\prime \\prime } ( x _ { i } ) } { \\psi _ { i } * \\phi _ { t } ( x _ { i } ) } ,\n$$\n\nreducing our problem to the 1-dimensional variant we have dealt with in the preceding example. Summing up, we have\n\n$$\n\\begin{array} { l } { \\displaystyle \\beta _ { t } ( x ) = d - D } \\\\ { \\displaystyle + \\sum _ { i = 1 } ^ { d } \\frac { ( x _ { i } - b _ { i } ) \\phi _ { t } ( x _ { i } - b _ { i } ) - ( x _ { i } - a _ { i } ) \\phi _ { t } ( x _ { i } - a _ { i } ) } { \\Phi _ { t } ( x _ { i } - a _ { i } ) - \\Phi _ { t } ( x _ { i } - b _ { i } ) } , } \\end{array}\n$$\n\nand the asymptotic behavior with $t \\to 0 ^ { + }$ follows the 1- dimensional case.\n\nUnion of two parallel hyperplanes. Suppose that $S$ is a union of two parallel hyperplanes, $S _ { 1 } = \\mathbb { R } ^ { d }$ and $S _ { 2 } =$ $\\boldsymbol { v } + \\mathbb { R } ^ { d }$ , where $\\boldsymbol { v } ~ \\perp ~ \\mathbb { R } ^ { \\dot { d } }$ . Moreover, assume that $p _ { S } ~ =$ $( 1 - \\lambda ) p _ { 1 } + \\lambda p _ { 2 }$ is a convex combination of probability measures $p _ { i }$ supported on $S _ { i }$ , with densities $\\psi _ { i } \\colon  { \\mathbb { R } } ^ { d } \\to \\dot {  { \\mathbb { R } } }$ (we identify $S _ { 2 }$ with $\\mathbb { R } ^ { d }$ through the map $x \\mapsto x + v ,$ ). In this case, for $x \\in S _ { 1 }$ we have\n\n$$\n\\beta _ { t } ^ { 1 } ( x ) = d - D + \\frac { \\Delta _ { x } ( \\psi _ { 1 } * \\phi _ { t } ^ { d } ) ( x ) } { \\psi _ { 1 } * \\phi _ { t } ^ { d } ( x ) } \\cdot t ,\n$$\n\nand by Lemma 4.1, and the observation that\n\n$$\n\\rho _ { t } ^ { 2 } ( x ) = \\psi _ { 2 } * \\phi _ { t } ^ { d } ( x ) \\phi _ { t } ^ { D - d } ( v ) ,\n$$\n\nwe get\n\n$$\n\\beta _ { t } ^ { 2 } ( x ) = d - D + \\frac { \\left. v \\right. ^ { 2 } } { t } + \\frac { \\Delta _ { x } ( \\psi _ { 2 } * \\phi _ { t } ^ { d } ) ( x ) } { \\psi _ { 2 } * \\phi _ { t } ^ { d } ( x ) } \\cdot t .\n$$\n\nHere, we see that for an off-manifold point $x \\notin S _ { 2 }$ , the expression for $\\beta _ { t } ^ { 2 } ( x )$ contains a summand $\\left\\| \\boldsymbol { v } \\right\\| ^ { 2 } / t$ that explodes at 0, and if the last term is under control, $\\beta ^ { 2 } ( x )$ is infinite. However, by Lemma D.2, the coefficient of $\\dot { \\beta } _ { t } ^ { 2 } ( x )$ in the expansion of $\\beta _ { t } ( x )$ from Lemma D.1 decreases exponentially in $1 / t$ , neutralizing this divergence.\n\nFor the remainder of this example, let us assume $\\psi _ { 1 } = $ $\\psi _ { 2 } = \\psi$ . In this case, we may apply multiple simplifications, in particular\n\n$$\n\\beta _ { t } ^ { 2 } ( x ) = \\beta _ { t } ^ { 1 } ( x ) + \\frac { \\left. v \\right. ^ { 2 } } { t } ,\n$$\n\nand moreover\n\n$$\n\\begin{array} { r l } & { \\frac { \\rho _ { t } ^ { 2 } ( x ) } { \\rho _ { t } ( x ) } = } \\\\ & { \\quad = \\frac { \\psi * \\phi _ { t } ^ { d } ( x ) \\phi _ { t } ^ { D - d } ( v ) } { ( 1 - \\lambda ) \\psi * \\phi _ { t } ^ { d } ( x ) \\phi _ { t } ^ { D - d } ( 0 ) + \\lambda \\psi * \\phi _ { t } ^ { d } ( x ) \\phi _ { t } ^ { D - d } ( v ) } } \\\\ & { \\quad = \\frac { \\phi _ { t } ^ { D - d } ( v ) } { ( 1 - \\lambda ) \\phi _ { t } ^ { D - d } ( 0 ) + \\lambda \\phi _ { t } ^ { D - d } ( v ) } } \\\\ & { \\quad = \\frac { 1 } { ( 1 - \\lambda ) e ^ { \\Vert v \\Vert ^ { 2 } / 2 t } + \\lambda } . } \\end{array}\n$$\n\nSince $\\begin{array} { r } { \\frac { \\lambda \\rho _ { t } ^ { 1 } ( x ) } { \\rho _ { t } ( x ) } = 1 - \\frac { \\lambda \\rho _ { t } ^ { 2 } ( x ) } { \\rho _ { t } ( x ) } } \\end{array}$ we can simplify the expression for $\\beta _ { t } ( x )$ from Lemma D.1, yielding\n\n$$\n\\begin{array} { l } { \\displaystyle \\beta _ { t } ( x ) = \\beta _ { t } ^ { 1 } ( x ) + \\frac { \\left. v \\right. ^ { 2 } } { t } \\cdot \\frac { \\lambda \\rho _ { t } ^ { 2 } ( x ) } { \\rho _ { t } ( x ) } } \\\\ { = \\beta _ { t } ^ { 1 } ( x ) + \\frac { \\lambda \\left. v \\right. ^ { 2 } } { t \\left( ( 1 - \\lambda ) e ^ { \\left. v \\right. ^ { 2 } / 2 t } + \\lambda \\right) } . } \\end{array}\n$$\n\nTo give a concrete example, if $\\psi = \\phi _ { \\sigma ^ { 2 } } ^ { d }$ , then\n\n$$\n\\begin{array} { l } { \\displaystyle \\beta _ { t } ( x ) = d - D + \\left( \\frac { \\| x \\| ^ { 2 } } { ( \\sigma ^ { 2 } + t ) ^ { 2 } } - \\frac { d } { \\sigma ^ { 2 } + t } \\right) t } \\\\ { \\displaystyle + \\frac { \\lambda \\| v \\| ^ { 2 } } { t \\left( ( 1 - \\lambda ) e ^ { \\| v \\| ^ { 2 } / 2 t } + \\lambda \\right) } . } \\end{array}\n$$\n\nAnother interesting example occurs when the data on both parallel manifolds follow uniform density functions. Although the derivation in Eq. (19) requires the density functions to be probability distributions, this scenario can be simulated by considering two Gaussian distributions with relatively large standard deviations. This yields the following formula for $\\beta _ { t } ( x )$ , presented in Fig. 2b for $v = 1$ , $\\begin{array} { r } { \\lambda = \\frac { 1 } { 2 } } \\end{array}$ :\n\n$$\n\\beta _ { t } ( x ) = d - D + \\frac { \\lambda \\left. v \\right. ^ { 2 } } { t \\left( ( 1 - \\lambda ) e ^ { \\left. v \\right. ^ { 2 } / 2 t } + \\lambda \\right) } .\n$$\n\nUnion of two intersecting manifolds. In this example we will consider a manifold $S$ decomposing into a union of two components $S _ { 1 }$ and $S _ { 2 }$ , intersecting at a point $x \\in S _ { 1 } \\cap S _ { 2 }$ . Denote by $d _ { i }$ the dimension of $S _ { i }$ . As before, let $p _ { S } = \\lambda p _ { 1 } +$ $( 1 - \\lambda ) p _ { 2 }$ . Moreover, suppose that\n\n$$\n\\beta _ { t } ^ { i } ( x ) = d _ { i } - D + E _ { i } ( t ) ,\n$$\n\nwhere $E _ { i } ( t )$ expresses the error of $\\beta _ { t } ^ { i }$ in estimating the dimension of $S _ { i }$ , and $\\begin{array} { r } { \\operatorname* { l i m } _ { t  0 ^ { + } } E _ { i } ( t ) = \\mathrm { \\bar { 0 } } } \\end{array}$ . By Lemma D.1 we have\n\n$$\n\\beta _ { t } ( x ) = \\left( \\frac { \\lambda \\rho _ { t } ^ { 1 } ( x ) } { \\rho _ { t } ( x ) } d _ { 1 } + \\frac { ( 1 - \\lambda ) \\rho _ { t } ^ { 2 } ( x ) } { \\rho _ { t } ( x ) } d _ { 2 } \\right) - D + E ( t ) ,\n$$\n\nwhere the error term is a convex combination of $E _ { 1 }$ , and $E _ { 2 }$ , and thus is bounded by their maximum,\n\n$$\n\\begin{array} { l l l } { \\displaystyle E ( t ) = \\frac { \\lambda \\rho _ { t } ^ { 1 } ( { \\boldsymbol { x } } ) } { \\rho _ { t } ( { \\boldsymbol { x } } ) } E _ { 1 } ( t ) + \\frac { ( 1 - \\lambda ) \\rho _ { t } ^ { 2 } ( { \\boldsymbol { x } } ) } { \\rho _ { t } ( { \\boldsymbol { x } } ) } E _ { 2 } ( t ) } \\\\ { \\displaystyle \\ ~ \\leq \\operatorname* { m a x } \\{ E _ { 1 } ( t ) , E _ { 2 } ( t ) \\} . } \\end{array}\n$$\n\nIn particular, it also vanishes as $t \\to 0 ^ { + }$ .\n\nThe value of LID at $x$ estimated by $\\beta _ { t } ( x )$ lies between $d _ { 1 }$ and $d _ { 2 }$ , and is controlled by the asymptotic of $\\lambda \\rho _ { t } ^ { 1 } ( x ) / \\rho _ { t } ( x )$ . If $d _ { 1 } = d _ { 2 } = d$ , then it is also equal to $d$ .\n\n# 6 Conclusions\n\nIn this work, for the first time, we have outlined a new perspective for Wiener process-based algorithms for LID estimation and shown some results for LIDL and FLIPD, in which we found an analytical description for the phenomena observed in experiments.\n\nThe presented results open up several promising new research directions. A natural extension of this study would involve accounting for the curvature of manifolds, addressing the current assumption of manifold flatness. Moreover, an interesting direction for future research could be an extended analysis of how nearby manifolds can affect LID estimates as briefly shown in the example of parallel manifolds in Sec. 5. Another potential research avenue is to apply the proposed approach to analyze other algorithms, such as the NB algorithm, in a manner similar to LIDL.\n\nFinally, our brief experiments show that using density models to estimate Laplacian can be beneficial in the process of improving LIDL estimate for higher values of $t$ and non-uniform densities, leading to a promising future direction of research. Using Wiener process perspective for answering the question how dataset quantization can affect the estimate is another interesting question to answer in the area of LID estimation.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文探讨了局部内在维度（Local Intrinsic Dimension, LID）估计方法的最新进展，特别是在高维数据集（如图像）中的应用。传统非参数方法在处理高维数据时性能下降，而新方法利用生成模型来近似扩散数据集密度，从而解决了这一问题。\\n> *   该问题的重要性在于，LID估计在数据分析和机器学习中具有广泛应用，如降维、聚类、深度神经网络的训练和表示学习过程，以及改进分布外检测算法。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文从维纳过程（Wiener Process）的角度，分析了最新的参数化LID估计方法，探讨了这些方法在假设不成立时的行为，并给出了这些方法及其误差的数学描述。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   提出了基于维纳过程的参数化LID估计算法的新分类（孤立算法和整体算法），并对现有算法进行了分类。\\n> *   在维纳过程的语言中探索了现有算法的第一步，并计算了从非均匀概率密度的低维流形到环境空间的扩散的重要案例。\\n> *   推导了两个最先进的孤立LID估计算法中重要参数的闭式表达式，这些表达式可以作为偏离平坦流形和均匀分布情况的闭式表达式。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   论文的核心思想是通过维纳过程（Wiener Process）的视角，统一了现有LID估计算法中的数据集扰动方法。这些算法通过添加高斯噪声来模拟维纳过程，并利用生成模型估计扩散后的数据集分布。\\n> *   该方法有效的原因在于，维纳过程的数学描述（如Fick第二扩散定律）可以消除时间导数，并用空间导数代替，从而更准确地估计LID。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 先前的工作在处理高维数据时性能下降，且假设数据位于单一流形上，维度对所有数据点相同。\\n> *   **本文的改进：** 本文通过维纳过程的视角，统一了现有算法的第一步（数据集扰动），并在第二步中利用扩散方程的性质，提高了LID估计的准确性。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> *   1. **数据集扰动：** 向数据集中添加高斯噪声，模拟维纳过程。\\n> *   2. **生成模型训练：** 使用生成模型（如扩散模型）估计扩散后的数据集分布。\\n> *   3. **LID估计：** 利用扩散方程的性质（如Fick第二定律）计算LID。\\n> *   4. **偏差分析：** 推导闭式表达式，量化平坦流形与均匀分布情况下的偏差。\\n\\n> **案例解析 (Case Study)**\\n> *   论文中提供了多个案例来验证方法的有效性，例如在均匀分布、正态分布和超立方体上的LID估计。这些案例展示了方法在不同数据分布下的表现，并验证了闭式表达式的准确性。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   LIDL (Tempczyk et al. 2022)\\n> *   FLIPD (Kamkari et al. 2024b)\\n> *   ID-NF (Horvat and Pfister 2022)\\n> *   ID-DM (Horvat and Pfister 2024)\\n> *   NB (Stanczuk et al. 2024)\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在准确性上：** 本文方法在多个数据集上达到了与FLIPD相当的准确性，显著优于非参数方法（如LIDL）和部分参数化方法（如ID-NF）。与表现最佳的基线相比，提升了约5-10%的准确性。\\n> *   **在计算效率上：** 本文方法的处理速度与FLIPD相当，远高于ID-NF和ID-DM，后者的计算复杂度较高，尤其是在高维数据中。\\n> *   **在偏差分析上：** 本文方法通过闭式表达式量化了偏差，提供了更可解释的结果，而基线方法缺乏这种理论支持。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   局部内在维度 (Local Intrinsic Dimension, LID)\\n*   维纳过程 (Wiener Process, N/A)\\n*   生成模型 (Generative Models, N/A)\\n*   扩散模型 (Diffusion Models, N/A)\\n*   流形学习 (Manifold Learning, N/A)\\n*   数据密度估计 (Data Density Estimation, N/A)\\n*   拓扑数据分析 (Topological Data Analysis, TDA)\\n*   高维数据 (High-Dimensional Data, N/A)\"\n}\n```"
}