{
    "source": "Semantic Scholar",
    "arxiv_id": "2410.19796",
    "link": "https://arxiv.org/abs/2410.19796",
    "pdf_link": "https://arxiv.org/pdf/2410.19796.pdf",
    "title": "Feature Clipping for Uncertainty Calibration",
    "authors": [
        "Linwei Tao",
        "Minjing Dong",
        "Chang Xu"
    ],
    "categories": [
        "cs.CV",
        "cs.LG"
    ],
    "publication_date": "2024-10-16",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 2,
    "influential_citation_count": 0,
    "institutions": [
        "University of Sydney",
        "City University of Hong Kong"
    ],
    "paper_content": "# Feature Clipping for Uncertainty Calibration\n\nLinwei Tao1, Minjing Dong2, Chang $\\mathbf { X } \\mathbf { u } ^ { 1 }$\n\n1University of Sydney 2City University of Hong Kong linwei.tao@sydney.edu.au, minjdong@cityu.edu.hk, c.xu@sydney.edu.au\n\n# Abstract\n\nDeep neural networks (DNNs) have achieved significant success across various tasks, but ensuring reliable uncertainty estimates, known as model calibration, is crucial for their safe and effective deployment. Modern DNNs often suffer from overconfidence, leading to miscalibration. We propose a novel post-hoc calibration method called feature clipping (FC) to address this issue. FC involves clipping feature values to a specified threshold, effectively increasing entropy in high calibration error samples while maintaining the information in low calibration error samples. This process reduces the overconfidence in predictions, improving the overall calibration of the model. Our extensive experiments on datasets such as CIFAR-10, CIFAR-100, and ImageNet, and models including CNNs and transformers, demonstrate that FC consistently enhances calibration performance. Additionally, we provide a theoretical analysis that validates the effectiveness of our method. As the first calibration technique based on feature modification, feature clipping offers a novel approach to improving model calibration, showing significant improvements over both post-hoc and train-time calibration methods and pioneering a new avenue for feature-based model calibration.\n\nCode — https://github.com/Linwei94/AAAI2025-FC.git\n\n# Introduction\n\nWhile deep neural networks achieve significant improvements across various tasks, model calibration—ensuring a model provides reliable uncertainty estimates—is as important as achieving high prediction accuracy. Accurate and reliable uncertainty estimation is vital for many safety-critical downstream tasks, such as autonomous driving (Feng et al. 2019) and medical diagnosis (Chen et al. 2018). However, recent studies (Guo et al. 2017) have found that most modern neural networks struggle to accurately reflect the actual probabilities of their predictions through their confidence scores. Thus, improving model calibration techniques is essential to enhance the reliability of these models.\n\nEfforts to address this issue can be divided into two streams: train-time calibration and post-hoc calibration. The first stream is train-time calibration, which includes training frameworks (Tao et al. 2023a; Liu et al. 2023), data augmentation (Wang et al. 2023; Zhang et al. 2022; Hendrycks et al. 2019), and regularization techniques like label smoothing (Mu¨ ller et al. 2019; Liu et al. 2022) and entropy regularizers (Pereyra et al. 2017). Training losses such as dual focal loss (Tao et al. 2023b) and focal loss (Mukhoti et al. 2020) are also notable methods. The second stream is post-hoc calibration methods, which are applied to trained models and modify the output probability. Representative works include Isotonic Regression (Zadrozny and Elkan 2002), Histogram Binning (Zadrozny and Elkan 2001), and Temperature Scaling (TS) (Guo et al. 2017). Among these, TS is widely accepted due to its simplicity and good performance. Many subsequent works (Frenkel et al. 2021; Xiong et al. 2023; Yang et al. 2024; Tomani et al. 2022) propose improved versions of TS, often making the temperature factor adaptive according to different criteria.\n\nGuo et al. (2017) identified overconfidence as a major cause of miscalibration in most modern neural networks. Adding a maximum-entropy penalty effectively increases prediction uncertainty, thereby mitigating overconfidence issues. Many calibration methods can be summarized as using entropy regularization in various forms. Pereyra et al. (2017) apply a maximum entropy penalty uniformly to all samples. Similarly, Label Smoothing (Mu¨ller et al. 2019) can be transformed into a form of entropy penalty, while Focal Loss (Mukhoti et al. 2020) can be viewed as the upper bound of a form with negative entropy, effectively adding a maximum entropy regularizer. TS often uses a temperature parameter larger than 1, resulting in a smoother probability distribution with higher entropy.\n\nSince the features extracted by neural networks are direct representations of data, a possible way to mimic this maximum-entropy penalty effect is by applying information loss directly to the features, thereby increasing entropy. To explore this idea, we begin by comparing high calibration error samples with low calibration error samples. Accurately obtaining per-sample calibration error is nontrivial, so we choose wrongly predicted samples with high confidence (greater than 0.95) as high calibration error (HCE) samples and correctly predicted samples with high confidence (greater than 0.95) as low calibration error (LCE) samples. We randomly select 100 feature units from the feature of these samples and plot the average unit value in Figure 1.\n\n![](images/f891a81a8c641444313033c39ffaceccb92adc3ef86edb93cd71f583c138f0f6.jpg)  \nFigure 1: Average feature value of samples with high or low calibration error. We randomly select 100 feature units out of 2048 units. The high/low calibration error samples are selected as the wrongly/correctly predicted samples with confidence larger than 0.95. High calibration error samples shows a obvious tendency of higher feature value in around $30 \\%$ feature. We provide a comparison of full 2048 feature units in Appendix, which shows similar pattern.\n\n![](images/8026b5901e2e0e5e91a2f0af518b8f5c8963e445da7c701ead3d264547c0e03e.jpg)  \nFigure 2: Histogram of feature values for HCE and LCE samples. The thicker tail of the HCE distribution indicates a larger variance $\\sigma ^ { 2 }$ compared to the LCE distribution. These experiments were conducted using ResNet-50 on the CIFAR-10 dataset.\n\nWe observe that the feature value of HCE samples is much higher than that of LCE samples in some units. A potential solution is to clip the feature values, making values larger than a threshold $c$ equal to $c$ . This might help reduce the abnormally large feature values, increasing the entropy of HCE samples. For example, in Figure 1, we propose feature clipping at 0.15 to increase the entropy of HCE samples while retaining the information of LCE samples. This removes significant information from HCE samples, making them more uncertain, while maintaining as much information as possible from LCE samples. We also plot the histogram of feature of both HCE samples and LCE samples to examine the feature distribution of both samples as shown in Fig. 2. We observe that HCE samples exhibit a thicker tail, indicating a larger variance compared to LCE samples. These patterns suggest notable differences in features between HCE and LCE samples. Therefore, it is worthwhile to conduct a deeper study on how to calibrate models based on these features.\n\nMotivated by these observation, we propose a simple and effective post-hoc calibration method called feature clipping (FC), which clips the feature value to a hyperparameter $c$ , optimized on the validation set to minimize negative log likelihood (NLL), similar to temperature scaling. We also provide a solid theoretical analysis to prove the effectiveness of FC. To the best of our knowledge, we are the first to propose a calibration method based on feature modification. We conduct extensive experiments on a wide range of datasets, including CIFAR-10, CIFAR-100, and ImageNet, and models, including CNNs and transformers. Our method shows consistent improvement. Furthermore, since we are the first to perform calibration on features, our method is orthogonal to previous calibration methods. Extensive experiments demonstrate that FC can enhance calibration performance over both previous post-hoc and train-time calibration methods. Overall, we make the following contributions:\n\n• We propose a simple and effective calibration method called feature clipping, which achieves SOTA calibration performance across multiple models and datasets. • We provide a solid theoretical analysis to prove the effectiveness of feature clipping by showing feature clipping increases more entropy on HCE samples. • We are the first to propose calibration based on features, initiating a new avenue for feature-based calibration. Our method serves as a strong baseline for this emerging area.\n\n# Related Works\n\nDeep neural networks have long been a focus of calibration research (Guo et al. 2017), with extensive studies examining their calibration properties (Minderer et al. 2021; Wang, Feng, and Zhang 2021; Tao et al. 2023c). Numerous calibration methods have been proposed, generally divided into two categories: train-time calibration and post-hoc calibration.\n\nTrain-Time Calibration Train-time calibration aims to improve a model’s calibration performance during training. A notable example is focal loss (Mukhoti et al. 2020), with subsequent works such as dual focal loss (Tao et al. 2023b) focusing on both the highest and second-highest probabilities. Adaptive focal loss (Ghosh, Schaaf, and Gormley 2022) modifies hyperparameters for different sample groups based on prior training knowledge. These focal loss-based methods can be transformed into an upper bound of negative entropy, thereby performing an entropy penalty during training. Similarly, label smoothing (Mu¨ller et al. 2019) can also be transformed into a form of entropy penalty.\n\nPost-Hoc Calibration Post-hoc calibration is resourceefficient and can be easily applied to pretrained models without altering their weights, preserving the model’s accuracy and robustness. A common technique is temperature scaling (TS), which adjusts the output probability distribution’s sharpness via a temperature parameter optimized to minimize negative log likelihood (NLL) on a validation set. TS typically uses larger temperature parameters for CNN models, reducing probability distribution sharpness and acting as an uniform maximum-entropy regularizer. Many subsequent methods aim to improve TS by applying adaptive temperature parameters, treating samples differently for a more effective maximum-entropy regularizer. For example, CTS (Frenkel et al. 2021) adapts temperature based on class labels, while PTS (Tomani et al. 2022) proposes learnable temperature parameters using a neural network. Recent methods like Proximity-based TS (Xiong et al. 2023) and Group Calibration (Yang et al. 2024) adjust temperature based on features, aiming for more precise entropy penalties.\n\nCalibration Using Features Although feature representation is a crucial aspect of deep neural networks and is well-studied in robustness literature (Ilyas et al. 2019), it is underutilized in calibration literature. Pioneering works such as (Xiong et al. 2023; Yang et al. 2024) have explored using features to group similar samples to achieve multicalibration (He´bert-Johnson et al. 2018). However, they do not perform calibration based on feature modification.\n\n# Methodology\n\nProblem Formulation In a classification task, let $\\chi$ be the input space and $y$ be the label space. The classifier $f$ maps an input to a probability distribution $\\hat { p } _ { [ 1 , 2 , . . . , K ] } \\ \\in \\ [ 0 , 1 ] ^ { K }$ over $K$ classes. The confidence of a prediction is defined as the largest probability, $\\operatorname* { m a x } ( \\hat { p } _ { i } )$ . For simplicity, we use $\\hat { p }$ to represent confidence in the following discussion.\n\nA network is perfectly calibrated if the predicted confidence $\\hat { p }$ accurately represents the true probability of the classification being correct. Formally, a perfectly calibrated network satisfies ${ \\bar { \\mathbb { P } } } ( { \\hat { y } } ~ = ~ y | { \\hat { p } } ~ = ~ { \\bar { p } } ) ~ { \\mathrm { ~ = ~ } } ~ p$ for all $p \\in$ [0, 1] (Guo et al. 2017), where $\\hat { y }$ is the predicted label and $y$ is the ground truth label. Given the confidence score and the probability of correctness, the Expected Calibration $E r$ - ror (ECE) is defined as $\\mathbb { E } _ { \\hat { p } } [ | \\mathbb { P } ( \\hat { y } = \\bar { y } | \\hat { p } ) - \\hat { p } | ]$ . In practice, since the calibration error cannot be exactly derived from finite samples, an approximation of ECE is introduced (Guo et al. 2017). Specifically, samples are grouped into $M$ bins $\\{ B _ { m } \\} _ { m = 1 } ^ { M }$ based on their confidence scores, where $B _ { m }$ contains samples with confidence scores $\\hat { p } _ { i } \\ \\in \\ \\left[ \\frac { m - 1 } { M } , \\frac { m } { M } \\right)$ For each bin , the average confidence is computed as $\\begin{array} { r c l } { C _ { m } } & { = } & { \\frac { 1 } { \\left| B _ { m } \\right| } \\sum _ { i \\in B _ { m } } \\hat { p } _ { i } } \\end{array}$ and the bin accuracy as $A _ { m } \\ =$ B1m i Bm 1(yˆi = yi), where 1 is the indicator function. The ECE is then approximated as the expected absolute difference between bin accuracy and average confidence:\n\n$$\n\\mathrm { E C E } \\approx \\sum _ { m = 1 } ^ { M } \\frac { \\left| B _ { m } \\right| } { N } \\left| A _ { m } - C _ { m } \\right| ,\n$$\n\nwhere $N$ is the total number of samples. Besides this estimated ECE, there are variants like Adaptive ECE (Krishnan and Tickoo 2020), which groups samples into bins with equal sample sizes, and Classwise ECE (Kull et al. 2019), which computes ECE over $K$ classes.\n\nFeature Clipping We propose feature clipping (FC), a simple and effective post-hoc calibration method designed to reduce overconfidence problem in deep neural networks. The key idea is to clip the feature values to a specified threshold, thereby increasing entropy in HCE samples while preserving the information in LCE samples. This approach helps mitigate overconfidence issues in HCE samples and improves overall model calibration. Given feature values $x$ , we apply feature clipping as follows:\n\n$$\n\\tilde { x } = \\operatorname* { m a x } ( \\operatorname* { m i n } ( x , c ) , - c )\n$$\n\nwhere $c$ is a positive hyperparameter optimized on a validation set to minimize negative log likelihood (NLL).\n\n# Theoretical Evidence\n\nIn this section, we present theoretical evidence to explain the effectiveness of feature clipping by analyzing the information loss in features of HCE and LCE samples. Our aim is to demonstrate that after feature clipping, HCE samples, characterized by larger variance, experience greater information loss compared to LCE samples, which have smaller variance. Consequently, we perform the entropy penalty differently to HCE and LCE samples and make HCE samples more uncertain.\n\nEntropy of original feature We consider the case where the output are all postive values after ReLU activation function. Consider the feature vector $\\textbf { x } : = ~ \\{ x _ { 1 } , . . . , x _ { n } \\}$ extracted from a sample, which is normally the output of penultimate layer of a neural network. Suppose the feature value $X$ follows a rectified normal distribution (Socci, Lee, and Seung 1997), which is a mixture distribution with both discrete variables and continuous variables. To calculate the entropy for this mixture distribution1 (Politis 1991), first, we treat the continuous variables as the truncated normal distribution (Burkardt 2014).\n\nFor a standard truncated normal distribution, suppose $X$ has a normal distribution with mean $\\mu = 0$ and variance $\\sigma ^ { 2 }$ and lies within the interval $( a , b )$ . The probability density function (PDF) of truncated normal distribution is given by:\n\n$$\nf ( x ; \\mu , \\sigma , a , b ) = { \\frac { 1 } { \\sigma } } { \\frac { \\phi \\left( { \\frac { x - \\mu } { \\sigma } } \\right) } { \\Phi \\left( { \\frac { b - \\mu } { \\sigma } } \\right) - \\Phi \\left( { \\frac { a - \\mu } { \\sigma } } \\right) } }\n$$\n\nand by $f = 0$ otherwise. Here, $\\begin{array} { r } { \\phi ( \\xi ) = \\frac { 1 } { \\sqrt { 2 \\pi } } \\exp \\left( - \\frac { 1 } { 2 } \\xi ^ { 2 } \\right) } \\end{array}$ is the probability density function of the standard normal distribution and $\\Phi ( \\cdot )$ is its cumulative distribution function $\\begin{array} { r } { \\Phi ( x ) = \\frac { 1 } { 2 } \\left( 1 + \\mathrm { e r f } \\left( \\frac { x } { \\sqrt { 2 } } \\right) \\right) } \\end{array}$ and $\\begin{array} { r } { \\operatorname { e r f } ( x ) = \\frac { 2 } { \\sqrt { \\pi } } \\int _ { 0 } ^ { x } e ^ { - t ^ { 2 } } d t } \\end{array}$ is the error function. By definition, if $b = \\infty$ , then $\\begin{array} { r } { \\Phi \\left( \\frac { b - \\mu } { \\sigma } \\right) = } \\end{array}$ 1. The entropy of truncated normal distribution is given by:\n\n$$\n\\begin{array} { l } { { H _ { c } ( x ; \\mu , \\sigma , a , b ) = \\displaystyle - \\int _ { a } ^ { b } f ( x ) \\log f ( x ) d x } } \\\\ { { \\displaystyle ~ = \\log ( \\sqrt { 2 \\pi e \\sigma Z } ) + \\frac { \\alpha \\phi ( \\alpha ) - \\beta \\phi ( \\beta ) } { 2 Z } } } \\end{array}\n$$\n\nwhere $\\begin{array} { r } { \\alpha = \\frac { a - \\mu } { \\sigma } , \\quad \\beta = \\frac { b - \\mu } { \\sigma } } \\end{array}$ b−µ and Z = Φ(β) Φ(α).\n\nThus, the PDF of the continuous variables of the mixture distribution is $f ( x ; 0 , \\sigma , 0 , + \\infty )$ and the corresponding differential entropy is $H _ { c } ( x ; 0 , \\sigma , 0 , + \\infty )$ . The probability mass function (PMF) for discrete variables in the mixture distribution is given by:\n\n$$\np ( x ) = { \\left\\{ \\begin{array} { l l } { 1 0 0 \\% } & { { \\mathrm { i f ~ } } x = 0 } \\\\ { 0 } & { { \\mathrm { o t h e r w i s e } } } \\end{array} \\right. } ,\n$$\n\nand the corresponding Shannon entropy is $H _ { d } ( x ) = 0$ .\n\nAssume the input of ReLU layer follows Gaussian distribution with mean at 0, we can derive that feature $x$ with probability $q = 0 . 5$ to be discrete variables and $1 - q$ to be continuous variables. According to the entropy calculation of mixture distribution (Politis 1991), the entropy of original feature $x$ is given by:\n\n$$\n\\begin{array} { l } { { H ( x ) = - q \\log q - ( 1 - q ) \\log ( 1 - q ) } } \\\\ { { \\ ~ + q H _ { d } ( x ) + ( 1 - q ) H _ { c } ( x ; 0 , \\sigma , 0 , + \\infty ) } } \\\\ { { \\ ~ = - \\log ( \\displaystyle \\frac 1 2 ) - \\displaystyle \\frac 1 2 H _ { c } ( x ; 0 , \\sigma , 0 , + \\infty ) } } \\\\ { { \\ ~ = - \\log ( \\displaystyle \\frac 1 2 ) - \\displaystyle \\frac 1 2 \\log ( \\sqrt { \\pi e \\sigma } ) } } \\end{array}\n$$\n\nEntropy of clipped feature Similarly, the clipped feature $\\tilde { x }$ follows mixture distribution with discrete variables and continuous variables, where the PDF of the continuous variables is $f ( \\tilde { x } ; 0 , \\sigma , 0 , c )$ and the corresponding differential entropy is $H _ { c } ( x ; 0 , \\sigma , 0 , c )$ The PMF for discrete variables is given by:\n\n$$\np ( \\tilde { x } ) = \\left\\{ \\begin{array} { l l } { \\frac { \\Phi ( 0 ) } { \\Phi ( 0 ) + ( 1 - \\Phi \\left( \\frac { c } { \\sigma } \\right) ) } } & { \\mathrm { i f } \\tilde { x } = 0 } \\\\ { \\frac { 1 - \\Phi \\left( \\frac { c } { \\sigma } \\right) } { \\Phi ( 0 ) + ( 1 - \\Phi \\left( \\frac { c } { \\sigma } \\right) ) } } & { \\mathrm { i f } \\tilde { x } = c } \\\\ { 0 } & { \\mathrm { o t h e r w i s e } } \\end{array} \\right. ,\n$$\n\nand the corresponding entropy is\n\n$$\n\\begin{array} { l } { { H _ { d } ( \\tilde { x } ) = - \\sum p ( \\tilde { x } ) \\log ( p ( \\tilde { x } ) ) } } \\\\ { { \\ = - \\frac { 0 . 5 } { 1 . 5 - \\Phi \\left( \\frac { c } { \\sigma } \\right) } \\log \\left( \\frac { 0 . 5 } { 1 . 5 - \\Phi \\left( \\frac { c } { \\sigma } \\right) } \\right) } } \\\\ { { \\ - \\frac { 1 - \\Phi \\left( \\frac { c } { \\sigma } \\right) } { 1 . 5 - \\Phi \\left( \\frac { c } { \\sigma } \\right) } \\log \\left( \\frac { 1 - \\Phi \\left( \\frac { c } { \\sigma } \\right) } { 1 . 5 - \\Phi \\left( \\frac { c } { \\sigma } \\right) } \\right) } } \\end{array}\n$$\n\nSince $\\tilde { x }$ with probability $\\begin{array} { r } { \\tilde { q } = \\Phi ( 0 ) + ( 1 - \\Phi \\left( \\frac { c } { \\sigma } \\right) ) } \\end{array}$ to be discrete variables and $1 - \\tilde { q }$ to be continuous variables, similarly, the entropy of clipped feature $\\tilde { x }$ can be derived as following\n\nform according to (Politis 1991),\n\n$$\n\\begin{array} { l } { { H ( \\tilde { x } ) = - \\tilde { q } \\mathrm { i } \\log \\tilde { q } - ( 1 - \\tilde { q } ) \\mathrm { l o g } ( 1 - \\tilde { q } ) } } \\\\ { { \\ } } \\\\ { { \\ } } \\\\ { { \\ } } \\\\ { { \\displaystyle \\quad + \\tilde { q } H _ { a } ( \\tilde { x } ) + ( 1 - \\tilde { q } ) H _ { c } ( \\tilde { x } ; 0 , \\sigma , 0 , c ) } } \\\\ { { \\ } } \\\\ { { \\ } } \\\\ { { \\displaystyle \\quad = - \\left( 1 . 5 - \\Phi \\left( \\frac { c } { \\sigma } \\right) \\right) \\log \\left( 1 . 5 - \\Phi \\left( \\frac { c } { \\sigma } \\right) \\right) } } \\\\ { { \\ } } \\\\ { { \\ } } \\\\ { { \\displaystyle \\quad - \\left( \\Phi \\left( \\frac { c } { \\sigma } \\right) - 0 . 5 \\right) \\log \\left( \\Phi \\left( \\frac { c } { \\sigma } \\right) - 0 . 5 \\right) } } \\\\ { { \\ } } \\\\ { { \\ } } \\\\ { { \\displaystyle - 0 . 5 \\log \\left( \\frac { 0 . 5 } { 1 . 5 - \\Phi \\left( \\frac { c } { \\sigma } \\right) } \\right) } } \\\\ { { \\ } } \\\\ { { \\ } } \\\\ { { \\displaystyle - \\left( 1 - \\Phi \\left( \\frac { c } { \\sigma } \\right) \\right) \\log \\left( \\frac { 1 - \\Phi \\left( \\frac { c } { \\sigma } \\right) } { 1 . 5 - \\Phi \\left( \\frac { c } { \\sigma } \\right) } \\right) } } \\\\ { { \\ } } \\\\ { { \\ } } \\\\ { { \\displaystyle + \\left( \\Phi \\left( \\frac { c } { \\sigma } \\right) - 0 . 5 \\right) H _ { c } ( \\tilde { x } ; 0 , \\sigma , 0 , c ) } } \\end{array}\n$$\n\nTable 1: Entropy values calculated on Softmax probability before and after clipping, and their differences for HCE and LCE samples. FC makes HCE samples more uncertain. The experiment is conducted on ResNet-50 on CIFAR-10.   \n\n<html><body><table><tr><td></td><td>Hsm(X)</td><td>Hsm(X)</td><td>△Hsm</td></tr><tr><td>HCE</td><td>0.0824</td><td>0.5723</td><td>0.4908</td></tr><tr><td>LCE</td><td>0.0032</td><td>0.1525</td><td>0.1493</td></tr></table></body></html>\n\nEntropy Difference Then, the Shannon entropy difference between features before and after clipping is given by\n\n$$\n\\begin{array} { l } { \\displaystyle \\Delta H = - \\left( \\Phi \\left( \\frac { c } { \\sigma } \\right) - 0 . 5 \\right) \\log \\left( \\Phi \\left( \\frac { c } { \\sigma } \\right) - 0 . 5 \\right) } \\\\ { \\displaystyle \\qquad + 0 . 5 \\log \\left( 0 . 5 \\right) } \\\\ { \\displaystyle \\qquad - \\left( 1 - \\Phi \\left( \\frac { c } { \\sigma } \\right) \\right) \\log \\left( 1 - \\Phi \\left( \\frac { c } { \\sigma } \\right) \\right) } \\\\ { \\displaystyle \\qquad + \\left( \\Phi \\left( \\frac { c } { \\sigma } \\right) - 0 . 5 \\right) H _ { c } ( \\tilde { x } ; 0 , \\sigma , 0 , c ) } \\\\ { \\displaystyle \\qquad + \\frac { 1 } { 2 } \\log ( \\sqrt { \\pi \\epsilon \\sigma } ) } \\end{array}\n$$\n\nand $\\Delta H$ is determined by the clipping threshold $c$ and $\\sigma$ . We adopt the empirical result that $\\sigma _ { H C E } > \\sigma _ { L C E }$ , as discussed in previous section. Thus, we can derive the theorem,\n\nTheorem 1. High calibration error samples suffer larger entropy difference compared to low calibration error samples after feature clipping.\n\n$$\n\\Delta H _ { L C E } < \\Delta H _ { H C E }\n$$\n\nThe detailed proof of Theorem 1 is given in Appendix. To verify our conclusion, we further calculate the entropy difference at Softmax layer, which is consistent with our observation. Specifically, we numerically calculate the entropy based on Softmax probability before and after feature clipping. As shown in Table 1, both entropy of HCE samples $H _ { \\mathrm { { s m } } } ^ { \\mathrm { { \\tilde { H } C E } } } ( X )$ and entropy of LCE samples $H _ { \\mathrm { { s m } } } ^ { \\mathrm { { L C E } } } ( X )$ are close to zero before clipping. However, after feature clipping, the entropy of HCE samples $H _ { \\mathrm { { s m } } } ^ { \\mathrm { { H C E } } } ( \\tilde { X } )$ become much larger than entropy of LCE samples $H _ { \\mathrm { { s m } } } ^ { \\mathrm { { L C E } } } ( \\tilde { X } )$ .\n\nTable $2 { : } \\mathbf { E C E } { \\downarrow }$ before and after after feature clipping. ECE is measured as a percentage, with lower values indicating better calibration. ECE is evaluated for different post hoc calibration methods, both before (base) and after ( $^ +$ ours) feature clipping. The results are calculated with number of bins set as 15. The optimal $c$ is determined on the validation set, included in brackets.   \n\n<html><body><table><tr><td>Dataset</td><td>Model</td><td colspan=\"2\">Original Feature</td><td colspan=\"2\">TS</td><td colspan=\"2\">ETS</td><td colspan=\"2\">PTS</td><td colspan=\"2\">CTS</td><td colspan=\"2\">GC</td></tr><tr><td></td><td></td><td>base</td><td>+ours(c)</td><td>base</td><td>(Guo et al. 2017) +ours</td><td>base</td><td>(Zhang et al.2020) +ours</td><td>base</td><td>(Tomani et al. 2022) +ours</td><td>base</td><td>(Frenkel et al. 2021) +ours</td><td>base</td><td>(Yang et al. 2024) +ours</td></tr><tr><td rowspan=\"3\">CIFAR-10</td><td>ResNet-50</td><td>4.34</td><td>1.10(0.23) </td><td>1.39</td><td></td><td>1.37</td><td></td><td></td><td>1.25 </td><td>1.46</td><td>1.25 √</td><td>0.97</td><td>0.49</td></tr><tr><td>ResNet-110</td><td>4.41</td><td>0.96(0.23) </td><td>0.98</td><td>1.22 0.94√</td><td>0.98</td><td>1.22 0.94√</td><td>1.36 0.95</td><td>0.90 √</td><td>1.13</td><td>0.90 √</td><td>1.24</td><td>1.78 </td></tr><tr><td>DenseNet-121</td><td>4.51</td><td>1.05(0.45) </td><td>1.41</td><td>1.11 √</td><td>1.40</td><td>1.12 √</td><td>1.38</td><td>1.14√</td><td>1.44</td><td>1.14√</td><td>1.27</td><td>2.51 </td></tr><tr><td rowspan=\"3\">CIFAR-100</td><td>ResNet-50</td><td>17.52</td><td>3.98(0.60)</td><td>5.72</td><td>4.26</td><td>5.68</td><td>4.29</td><td>5.64</td><td>4.37</td><td>6.03</td><td>4.37</td><td>3.43</td><td>1.70 √</td></tr><tr><td>ResNet-110</td><td>19.06</td><td>4.40(0.61) √</td><td>5.12</td><td>4.81√</td><td>5.10</td><td>4.81√</td><td>5.05</td><td>4.98</td><td>5.43</td><td>4.98</td><td>2.71</td><td>3.45 </td></tr><tr><td>DenseNet-121</td><td>20.99</td><td>3.28(1.40)</td><td>5.15</td><td>3.92</td><td>5.09</td><td>3.95√</td><td>5.06</td><td>4.04√</td><td>4.87</td><td>4.04√</td><td>2.84</td><td>1.75 √</td></tr><tr><td rowspan=\"4\">ImageNet</td><td>ResNet-50</td><td>3.69</td><td>1.74(2.06) </td><td>2.08</td><td>1.64√</td><td>2.08</td><td>1.65√</td><td>2.11</td><td>1.63√</td><td>3.05</td><td>1.63√</td><td>1.30</td><td>1.00 √</td></tr><tr><td>DenseNet-121</td><td>6.66</td><td>3.08(3.45)</td><td>1.65</td><td>1.19 √</td><td>1.65</td><td>1.20√</td><td>1.61</td><td>1.20√</td><td>2.21</td><td>1.20√</td><td>2.67</td><td>0.63√</td></tr><tr><td>Wide-Resnet-50</td><td>5.52</td><td>2.52(3.05) </td><td>3.01</td><td>2.21</td><td>3.01</td><td>2.20√</td><td>3.00</td><td>2.18√</td><td>4.31</td><td>2.18√</td><td>3.01</td><td>0.88 </td></tr><tr><td>MobileNet-V2</td><td>2.72</td><td>1.36(1.73)</td><td>1.92</td><td>1.41 √</td><td>1.92</td><td>1.41√</td><td>1.93</td><td>1.44V</td><td>2.34</td><td>1.44√</td><td>1.81</td><td>0.50 √</td></tr></table></body></html>\n\n$$\n\\Delta H _ { \\mathrm { s m } } ^ { \\mathrm { L C E } } \\ll \\Delta H _ { \\mathrm { s m } } ^ { \\mathrm { H C E } } .\n$$\n\nIn other words, feature clipping successfully differentiated the handling of HCE and LCE samples, increase more entropy in HCE samples compared to LCE samples and make HCE samples more uncertain.\n\n# Experiments\n\n# Experiment Setup\n\nModels and Datasets We evaluate our methods on various deep neural networks (DNNs), including ResNet (He et al. 2016), Wide-ResNet (Zagoruyko and Komodakis 2016), DenseNet (Huang et al. 2017), MobileNet (Howard et al. 2017), and ViT (Dosovitskiy et al. 2020), using the CIFAR10, CIFAR-100 (Krizhevsky, Hinton et al. 2009), and ImageNet-1K (Deng et al. 2009) datasets to assess the effectiveness of feature clipping. Pre-trained weights for post hoc calibration evaluation are provided by PyTorch.torchvision. Pre-trained weights trained by other train-time calibration methods are provided by Mukhoti et al. (2020).\n\nMetrics We use the Expected Calibration Error (ECE) and accuracy as our primary metrics for evaluation. Additionally, we incorporate Adaptive ECE, a variant of ECE, which groups samples into bins of equal sizes to provide a balanced evaluation of calibration performance. For both ECE and Adaptive ECE, we use bin size at 15. We also measure the influence of calibration methods on prediction accuracy.\n\nComparison methods We compare our methods with several popular and state-of-the-art (SOTA) approaches. For post-hoc methods, we evaluate the widely used temperature scaling (TS) and other subsequent methods such as ETS (Zhang et al. 2020), PTS (Tomani et al. 2022), CTS (Frenkel et al. 2021), and a recently proposed SOTA calibration method called Group Calibration (Yang et al.\n\n2024). For all TS-based methods, we determine the temperature by tuning the hyperparameter on the validation set to minimize the Negative Log Likelihood (NLL). To maintain consistency with TS, we also determine the optimal clipping threshold $c$ on the validation set by minimizing the NLL. For training-time calibration methods, we include training with Brier loss (Brier 1950), label smoothing (Mu¨ ller et al. 2019) with a smoothing factor of 0.05, FLSD-53 (Mukhoti et al. 2020) using the same $\\gamma$ scheduling scheme as in (Mukhoti et al. 2020), and Dual Focal Loss (Tao et al. 2023b). Detailed settings are following the settings in (Mukhoti et al. 2020).\n\n# Calibration Performance\n\nTo evaluate the performance, we assess feature clipping on both post-hoc methods and train-time calibration. We also find that post-hoc calibration methods hardly improve calibration performance on ViT and provide an empirical analysis to support this finding.\n\nCompare with Post-Hoc Calibration Methods We compare the post-hoc calibration performance across multiple datasets and models, as shown in Table 2. FC consistently improves over the original features. With similar computational overhead and simplicity, FC outperforms TS in most cases, as seen when comparing columns 2 and 3. When combined with other post-hoc calibration methods, FC achieves state-of-the-art results. While Group Calibration also shows competitive results, it requires training an additional neural network based on features, resulting in higher computational overhead. Additionally, feature clipping is not compatible with Group Calibration in some cases, likely because Group Calibration separates groups based on features, while FC clips features, reducing information and making them less separable. Notably, in several instances, FC alone achieves the best performance, highlighting the potential of feature-based calibration. The simplicity of FC as a baseline method suggests significant opportunities for enhancement and optimization in future work. This demonstrates that even straightforward approaches like FC can yield substantial improvements, paving the way for more sophisticated featurebased calibration techniques. We also evaluate feature clipping using Adaptive ECE, a balanced version of ECE, with the results presented in the Appendix. FC demonstrates competitive results in this evaluation as well.\n\nTable 3: ECE Calibration performance on Vision Transformer. Feature Clipping provides little but consistent improvement on Vision Transformer. Experiments are conducted on ViT-L-16 on ImageNet.   \n\n<html><body><table><tr><td></td><td>Vanilla</td><td>TS</td><td>ETS</td><td>PTS</td><td>CTS</td></tr><tr><td>w/o FC</td><td>5.24</td><td>5.73</td><td>5.73</td><td>5.73</td><td>6.07</td></tr><tr><td>w/FC</td><td>5.04</td><td>5.59</td><td>5.60</td><td>5.60</td><td>5.60</td></tr></table></body></html>\n\n![](images/8bf53627d2ec152343eb437263907fcc58ad0570f0f6111a17141ed416d7b599.jpg)  \nFigure 3: Average absolute feature value of samples with high or low calibration error on Vision Transformer. We randomly select 50 feature units out of 2048 units. The high/low calibration error samples are selected as the wrongly/- correctly predicted samples with confidence larger than 0.8.\n\nTable 4: Number of high confidence samples in ImageNet test set. The total number of samples is 50,000.   \n\n<html><body><table><tr><td rowspan=\"2\">Confidence</td><td colspan=\"2\">ResNet-50</td><td colspan=\"2\">ViT-L-16</td></tr><tr><td>Correct</td><td>Wrong</td><td>Correct</td><td>Wrong</td></tr><tr><td>>0.80</td><td>5921</td><td>471</td><td>6431</td><td>455</td></tr><tr><td>>0.90</td><td>5221</td><td>271</td><td>3429</td><td>92</td></tr><tr><td>>0.95</td><td>4526</td><td>161</td><td>44</td><td>0</td></tr><tr><td>>0.99</td><td>3173</td><td>53</td><td>0</td><td>0</td></tr></table></body></html>\n\nPerformance on Vision Transformer Although Vision Transformers do not end with a ReLU layer, the difference between HCE samples and LCE samples still exists, indicating that feature clipping can significantly influence HCE samples. As shown in Figure 3, we take the mean of the absolute value of features for better visualization. The HCE samples show higher average feature values than LCE samples. However, the improvement is not as pronounced compared to CNN models. We show the ECE performance of ViT-L-16 in Table 3. To investigate the reason, we count the number of overconfident samples, as shown in Table 4. The number of samples with confidence larger than 0.8 is similar for both CNN and ViT. However, for samples with confidence greater than 0.95, CNN has significantly more samples than ViT. When examining samples with confidence greater than 0.99, CNN still has many samples, while ViT has none within this confidence range. This indicates that transformers face far fewer overconfidence issues compared to CNN models. Theoretically, clipping feature values results in a loss of information, increasing entropy and mitigating overconfidence problems. Since transformers exhibit fewer overconfidence problems compared to CNNs, our method has less impact on transformer-based models compared to CNNs. However, the difference in feature values among samples still exists, indicating significant potential for future improvements in transformer models.\n\nCompare with Train-time Calibration Methods We also compare the effectiveness of feature clipping when applied on top of various train-time calibration methods. Feature clipping consistently demonstrates improvement across all these train-time calibration methods and different models, as shown in Table 5. On simpler datasets like CIFAR10, models trained with “maximum-entropy penalty” methods such as focal loss and label smoothing adequately address the overconfidence issue. These methods effectively mitigate the overconfidence problem, leaving little room for additional improvement through feature clipping. However, when applied to more complex datasets like CIFAR-100, these training losses may not entirely resolve the overconfidence problem, providing an opportunity for feature clipping to further alleviate this issue and enhance calibration. Feature clipping’s ability to improve calibration in such scenarios underscores its potential as a valuable addition to existing training-time calibration methods.\n\n# Ablation Study\n\nFeature clipping is a straightforward method that causes samples to lose information. We are interested in understanding how this loss of information affects various aspects of model performance. Therefore, we study its influence on accuracy, how hyperparameter $c$ affect performance, and its performance when applied to different layers.\n\nDoes Feature Clipping Affect Accuracy? Although post-hoc methods do not change the model weights and can maintain prediction performance by keeping the original features, we are still interested in how the optimal clipping value affects accuracy. In Table 6, we compare the prediction accuracy of different train-time calibration methods with our feature clipping method. The baseline column indicates the model trained with cross-entropy loss using the original features, while the FC column shows the results of applying our feature clipping on the baseline. All models are trained with the same training recipe, which is included in the Appendix. We observe that feature clipping does not significantly affect accuracy. Despite reducing the information contained in the feature representation, FC sometimes even improves accuracy. On the other hand, some train-time methods, such as Brier loss, can negatively impact accuracy in most cases. This suggests that while these methods aim to improve calibration, they may inadvertently reduce the model’s ability to generalize, thereby lowering prediction accuracy. The detailed comparison of accuracy across different methods and datasets illustrates that our feature clipping method maintains competitive performance.\n\nTable $5 \\colon { \\bf E C E \\bot }$ before and after after feature clipping. ECE is measured as a percentage, with lower values indicating better calibration. ECE is evaluated for different train-time calibration methods, both before (base) and after $^ +$ ours) feature clipping. The results are calculated with number of bins set as 15. The optimal $c$ is determined on the validation set, included in brackets.   \n\n<html><body><table><tr><td>Dataset</td><td>Model</td><td colspan=\"2\">Cross Entropy</td><td colspan=\"2\">Brier Loss</td><td colspan=\"2\">LS-0.05</td><td colspan=\"2\">FLSD-53</td><td colspan=\"2\">Dual Focal Loss</td></tr><tr><td></td><td></td><td>base</td><td>+ours</td><td>base</td><td>(Brier 1950) +ours</td><td>base</td><td>(Muller et al. 2019) +ours</td><td>base</td><td>(Mukhoti et al. 2020) +ours</td><td>base</td><td>(Tao et al.2023b) +ours</td></tr><tr><td></td><td>ResNet-50</td><td>4.34</td><td>1.10(0.23) </td><td>1.80</td><td>1.49(0.98)</td><td>2.97</td><td>2.97(1.18)</td><td>1.55</td><td>1.50(0.75)</td><td>0.46</td><td>0.45(0.80)</td></tr><tr><td>CIFAR-10</td><td>ResNet-110</td><td>4.41</td><td>0.96(0.23) </td><td>2.57</td><td>2.34(1.15) </td><td>2.09</td><td>2.09(1.19) </td><td>1.88</td><td>1.28(0.51) </td><td>0.98</td><td>0.98(0.55) </td></tr><tr><td></td><td>DenseNet-121</td><td>4.51</td><td>1.05(0.45)</td><td>1.52</td><td>1.52(2.69)</td><td>1.87</td><td>1.87(2.05)</td><td>1.23</td><td>1.20(1.76)</td><td>0.57</td><td>0.57(1.96)</td></tr><tr><td></td><td>Wide-Resnet-26</td><td>3.24</td><td>1.35(0.28)</td><td>1.24</td><td>1.24(2.08)</td><td>4.25</td><td>4.25(1.74)</td><td>1.58</td><td>1.58(2.20) </td><td>0.81</td><td>0.81(2.12) </td></tr><tr><td></td><td>ResNet-50</td><td>17.52</td><td>3.98(0.60)</td><td>6.57</td><td>3.96(2.11) </td><td>7.82</td><td>7.82(3.67)</td><td>4.49</td><td>3.83(2.17)</td><td>1.08</td><td>1.01(2.23) </td></tr><tr><td></td><td>ResNet-110</td><td>19.06</td><td>4.40(0.61) </td><td>7.87</td><td>4.05(1.83)</td><td>11.04</td><td>6.83(1.03)</td><td>8.55</td><td>5.12(1.50)</td><td>2.90</td><td>2.53(1.51) </td></tr><tr><td>CIFAR-100</td><td>DenseNet-121</td><td>20.99</td><td>3.28(1.40) </td><td>5.22</td><td>3.50(4.11) </td><td>12.87</td><td>3.06(1.89) </td><td>3.70</td><td>2.96(4.02) </td><td>1.81</td><td>1.53(3.52)</td></tr><tr><td></td><td>Wide-Resnet-26</td><td>15.34</td><td>4.38(0.98)</td><td>4.34</td><td>3.11(2.24) </td><td>4.88</td><td>4.88(3.10)</td><td>3.02</td><td>1.90(2.47) </td><td>1.79</td><td>1.18(2.30) </td></tr></table></body></html>\n\nTable 6: Accuracy for different train-time methods and feature clipping. Feature clipping does not impact prediction accuracy performance.   \n\n<html><body><table><tr><td>Dataset</td><td>Model</td><td>Base</td><td>Brier</td><td>LS</td><td>Focal</td><td>FC</td></tr><tr><td rowspan=\"3\">CIFAR-10</td><td>ResNet-50</td><td>95.05</td><td>95.0</td><td>94.71</td><td>95.02</td><td>94.93</td></tr><tr><td>ResNet-110</td><td>95.11</td><td>94.52</td><td>94.48</td><td>94.58</td><td>95.01</td></tr><tr><td>DenseNet-121</td><td>95.0</td><td>94.89</td><td>94.91</td><td>94.54</td><td>95.13</td></tr><tr><td rowspan=\"3\">CIFAR-100</td><td>ResNet-50</td><td>76.7</td><td>76.61</td><td>76.57</td><td>76.78</td><td>76.74</td></tr><tr><td>ResNet-110</td><td>77.27</td><td>74.9</td><td>76.57</td><td>77.49</td><td>77.06</td></tr><tr><td>DenseNet-121</td><td>75.48</td><td>76.25</td><td>75.95</td><td>77.33</td><td>75.52</td></tr></table></body></html>\n\n![](images/f718b70421442248c399afdc05321b5a9985455f7c626174f388449c05ecf234.jpg)  \nFigure 4: Feature clipping at different value. Points to the right bottom cornor indicate better performance. The experiment is conducted on ResNet-50 on CIFAR-10.\n\nHow does clip threshold affect performance? To test how the clipping threshold influences performance, we clip the features of a ResNet-50 trained with cross-entropy loss on CIFAR-10 using different clipping thresholds. We plot the resulting performance in terms of ECE and accuracy, as shown in Figure 4. The red star indicates the performance of the original features. Generally, within a certain range (between 0.15 and 0.35 in this case), feature clipping does not significantly affect model accuracy. However, feature clipping can substantially influence calibration performance. For instance, a clipping value of 0.15 (point at the top left corner) achieves similar accuracy to the baseline but results in much worse calibration performance, with an ECE exceeding $2 5 \\%$ . With the optimal clipping value, the model can achieve an ECE as low as 1.10, as shown in Table 2. We believe the reason feature clipping has a larger influence on calibration is that excessive clipping may significantly increase entropy, affecting correct predictions with high confidence. As a result, the model faces underconfidence, leading to a large ECE.\n\n# Conclusion\n\nIn conclusion, our proposed feature clipping method demonstrates substantial improvements in model calibration across various datasets and models. FC effectively reduces overconfidence in predictions, enhancing calibration performance while maintaining accuracy. Despite its simplicity, FC achieves state-of-the-art calibration performance and provides a solid foundation for future research on featurebased calibration. However, there are several limitations and opportunities for improvement. The performance on transformer models, for instance, can be further improved. Future work should focus on developing more sophisticated methods, such as starting with a better threshold and conducting faster hyperparameter tuning. Additionally, exploring ways to find optimal clipping values using feature statistics and employing smoothed or adaptive thresholds instead of fixed ones are promising directions. These enhancements will potentially lead to even better calibration. Furthermore, investigating the impact of feature clipping on different neural network architectures and understanding its effects on various types of data can provide deeper insights. Our method serves as a strong baseline for feature-based calibration, and we believe that future developments can build upon this foundation to achieve even greater calibration improvements.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决的核心问题是深度神经网络（DNNs）在预测时存在的过度自信（overconfidence）问题，导致模型校准（model calibration）不佳。这一问题在安全关键应用（如自动驾驶和医疗诊断）中尤为重要，因为不可靠的不确定性估计可能导致严重后果。\\n> *   现有方法（如温度缩放，Temperature Scaling）虽然有效，但主要关注输出概率的调整，而忽略了特征层面的校准潜力。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种新颖的后处理校准方法——特征裁剪（Feature Clipping, FC），通过裁剪特征值到一个阈值，增加高校准误差样本的熵，同时保留低校准误差样本的信息，从而减少预测的过度自信。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **创新贡献点1：** 首次提出基于特征修改的校准方法，填补了特征层面校准的研究空白。\\n>       *   关键数据：在CIFAR-10数据集上，FC将ECE（Expected Calibration Error）从4.34%降低到1.10%。\\n>   *   **创新贡献点2：** 提供了理论分析，证明FC能够显著增加高校准误差样本的熵，从而改善校准性能。\\n>       *   关键数据：理论分析和实验验证表明，FC对高校准误差样本的熵增加量（ΔH）显著高于低校准误差样本。\\n>   *   **创新贡献点3：** FC与现有校准方法（如温度缩放）正交，可以结合使用以进一步提升性能。\\n>       *   关键数据：在ImageNet数据集上，FC结合温度缩放将ECE从2.08%降低到1.64%。\\n>   *   **创新贡献点4：** 在多个数据集（CIFAR-10, CIFAR-100, ImageNet）和模型（CNNs, transformers）上验证了FC的普适性和有效性。\\n>       *   关键数据：在CIFAR-100上，FC将ECE从17.52%降低到3.98%。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   FC的核心思想是通过裁剪特征值来减少高校准误差样本的过度自信。高校准误差样本通常具有较大的特征值方差，裁剪这些值可以增加熵，使预测更加不确定。\\n> *   设计哲学：特征层面的信息损失可以更直接地影响模型的校准性能，而不仅仅是调整输出概率。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 现有校准方法（如温度缩放）仅调整输出概率，忽略了特征层面的校准潜力。\\n> *   **本文的改进：** FC首次将校准问题引入特征层面，通过裁剪特征值直接调整模型的内部表示，从而更有效地解决过度自信问题。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.   **特征选择：** 从模型的倒数第二层提取特征向量。\\n> 2.   **特征裁剪：** 对特征值应用裁剪操作，公式为：\\n>      $$\\n>      \\\\tilde { x } = \\\\operatorname* { m a x } ( \\\\operatorname* { m i n } ( x , c ) , - c )\\n>      $$\\n>      其中，$c$ 是通过验证集优化的超参数。\\n> 3.   **优化阈值：** 使用验证集优化裁剪阈值 $c$，以最小化负对数似然（NLL）。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   温度缩放（Temperature Scaling, TS）\\n> *   增强温度缩放（Enhanced Temperature Scaling, ETS）\\n> *   可学习温度缩放（Learnable Temperature Scaling, PTS）\\n> *   类别温度缩放（Classwise Temperature Scaling, CTS）\\n> *   组校准（Group Calibration, GC）\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在ECE（Expected Calibration Error）上：** 本文方法在CIFAR-10数据集上达到了 **1.10%**，显著优于基线模型TS（1.39%）和ETS（1.37%）。与表现最佳的基线相比，提升了0.29个百分点。\\n> *   **在ECE上：** 本文方法在CIFAR-100数据集上达到了 **3.98%**，显著优于基线模型TS（5.72%）和ETS（5.68%）。与表现最佳的基线相比，提升了1.74个百分点。\\n> *   **在ECE上：** 本文方法在ImageNet数据集上达到了 **1.74%**，显著优于基线模型TS（2.08%）和ETS（2.08%）。与表现最佳的基线相比，提升了0.34个百分点。\\n> *   **在模型兼容性上：** FC能够与现有的后处理校准方法（如TS、ETS等）结合使用，进一步提升校准性能。例如，在CIFAR-100上，FC与TS结合后的ECE为4.26%，优于单独使用TS的5.72%。\\n> *   **在计算效率上：** FC的计算开销与TS相当，但校准效果更优，尤其在复杂数据集（如ImageNet）上表现突出。\\n> *   **在Transformer模型上：** FC在ViT-L-16模型上将ECE从5.24%降低到5.04%，虽然提升幅度较小，但仍展示了其普适性。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   模型校准 (Model Calibration, MC)\\n*   特征裁剪 (Feature Clipping, FC)\\n*   深度神经网络 (Deep Neural Network, DNN)\\n*   后处理校准 (Post-hoc Calibration, N/A)\\n*   熵增加 (Entropy Increase, N/A)\\n*   过度自信 (Overconfidence, N/A)\\n*   不确定性估计 (Uncertainty Estimation, N/A)\\n*   预期校准误差 (Expected Calibration Error, ECE)\"\n}\n```"
}