{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.13439",
    "link": "https://arxiv.org/abs/2412.13439",
    "pdf_link": "https://arxiv.org/pdf/2412.13439.pdf",
    "title": "Rare Event Detection in Imbalanced Multi-Class Datasets Using an Optimal MIP-Based Ensemble Weighting Approach",
    "authors": [
        "Georgios Tertytchny",
        "Georgios L. Stavrinides",
        "Maria K. Michael"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2024-12-18",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "KIOS Research and Innovation Center of Excellence",
        "Department of Electrical and Computer Engineering, University of Cyprus",
        "University of Cyprus"
    ],
    "paper_content": "# Rare Event Detection in Imbalanced Multi-Class Datasets Using an Optimal MIP-Based Ensemble Weighting Approach\n\nGeorgios Tertytchny1,2\\*‚Ä†, Georgios L. Stavrinides1‚Ä†, Maria K. Michael1,2\n\n1KIOS Research and Innovation Center of Excellence 2Department of Electrical and Computer Engineering, University of Cyprus {tertytchny.georgios, stavrinides.georgios, mmichael}@ucy.ac.cy\n\n# Abstract\n\nTo address the challenges of imbalanced multi-class datasets typically used for rare event detection in critical cyberphysical systems, we propose an optimal, efficient, and adaptable mixed integer programming (MIP) ensemble weighting scheme. Our approach leverages the diverse capabilities of the classifier ensemble on a granular per class basis, while optimizing the weights of classifier-class pairs using elastic net regularization for improved robustness and generalization. Additionally, it seamlessly and optimally selects a predefined number of classifiers from a given set. We evaluate and compare our MIP-based method against six wellestablished weighting schemes, using representative datasets and suitable metrics, under various ensemble sizes. The experimental results reveal that MIP outperforms all existing approaches, achieving an improvement in balanced accuracy ranging from $0 . 9 9 \\%$ to $7 . 3 1 \\%$ , with an overall average of $4 . 5 3 \\%$ across all datasets and ensemble sizes. Furthermore, it attains an overall average increase of $4 . 6 3 \\%$ , $4 . 6 0 \\%$ , and $4 . 6 1 \\%$ in macro-averaged precision, recall, and F1-score, respectively, while maintaining computational efficiency.\n\n# Code ‚Äî https://github.com/gterty019/MIPENS Extended version ‚Äî https://arxiv.org/abs/2412.13439\n\n# Introduction\n\nRare event detection in cyber-physical systems (CPS) is vital for maintaining the safety and reliability of critical infrastructures, such as water distribution networks and power grids (Taheri et al. 2024; Tertytchny, Nicolaou, and Michael 2020). Critical CPS often generate imbalanced multi-class datasets where some classes have significantly fewer instances than others, due to the infrequent occurrence of abnormal events (e.g., faults or attacks) compared to normal activities (Yin et al. 2020). Class imbalance poses several challenges, such as biased model performance towards the majority classes, poor generalization to minority classes, and thus increased difficulty in detecting rare events (Shyalika, Wickramarachchi, and Sheth 2023). These issues are further amplified by the critical nature of CPS, where misclassifying low-probability but high-impact events may lead to safety risks or system failures. Consequently, addressing these challenges is crucial for effective rare event detection.\n\nEnsemble learning approaches, especially weighted voting techniques, are commonly employed to handle imbalanced multi-class datasets (Khan, Chaudhari, and Chandra 2024). The aim of such methods is to improve overall performance and robustness, by leveraging the strengths of diverse classification algorithms. Specifically, a weighted voting ensemble model combines the predictions of multiple classifiers, using different weights based on their predictive performance. Higher weights are assigned to better-performing classifiers, either at an overall classifier level or on a per class basis (Dogan and Birant 2019). However, existing weighting schemes often fail to optimally balance the contribution of each classifier, especially in the presence of a highly skewed class distribution. Moreover, since smaller ensembles are typically less computationally demanding, and given that rare event detection in CPS is often performed at the network edge on resource-constrained devices, adaptable approaches are required to seamlessly and optimally select a predefined number of classifiers from a larger set while assigning their weights (Danso et al. 2022).\n\nAiming to fill this gap, we propose an optimal, adaptable, and efficient mixed integer programming (MIP) weighting scheme for weighted voting ensembles, suitable for rare event detection in imbalanced multi-class datasets. Our approach optimally assigns weights to classifiers per class based on their performance, leveraging their individual capabilities while selecting the predefined number of classifiers from a given set. To prevent overfitting and enhance generalization in weight assignment, we incorporate into our method the elastic net regularization technique (Zou and Hastie 2005), applying it beyond its traditional use in the training and validation phase. The proposed weighting scheme aims to enhance performance across all classes while ensuring computational efficiency, effectively addressing the limitations of existing approaches. Our method optimizes weights based on empirical accuracy results, avoiding simplifying assumptions typically required by theoretical analysis, such as independent and identically distributed instances, which may not hold in realistic scenarios involving class imbalance and diverse classifier behavior (Opitz and Maclin 1999).\n\nOur main contributions are summarized as follows:\n\n‚Ä¢ We propose an optimal MIP-based ensemble weighting approach for imbalanced multi-class datasets to improve rare event detection in critical CPS. Our weighting scheme optimizes classifier weights on a per class basis, leveraging the diverse strengths of each classifier in a granular manner.   \n‚Ä¢ The proposed method seamlessly and optimally selects a predefined number of classifiers from a given set while calculating their weights. Moreover, it utilizes elastic net regularization to improve generalization and robustness in the assignment of classifier weights, while maintaining computational efficiency.   \n‚Ä¢ We demonstrate the improved performance, adaptability, and efficiency achieved by our approach through comparative evaluation against six well-established weighting schemes, using representative datasets and appropriate metrics under different ensemble sizes.\n\nTo our knowledge, in contrast to our approach, none of the existing ensemble weighting schemes are intrinsically adaptable to varied ensemble sizes and capable of optimally assigning classifier weights for imbalanced multiclass datasets, while attaining generalization and efficiency.\n\n# Related Work\n\nWeighted voting ensemble methods typically apply weights per classifier or per classifier and class, based on classifier performance in the training and validation phase (Maheshwari et al. 2022). Among the fundamental techniques, UW-PC (uniform weights per classifier) and UW-PCC (uniform weights per classifier and class) assign equal weights to all classifiers and classifier-class pairs, respectively, and commonly serve as baselines in comparative studies (Dong et al. 2020). On the other hand, WA-PC (weighted average based on normalized accuracy per classifier) and WAPCC (weighted average based on normalized accuracy per classifier and class) apply weights based on the normalized accuracy of each classifier or classifier-class pair, respectively (Sagi and Rokach 2018). Both techniques assign greater weights to classifiers or classifier-class pairs that exhibit better performance. DE (differential evolution) is an evolutionary algorithm that determines classifier weights in an iterative refining manner (Storn and Price 1997; Zhang et al. 2014, 2024). In contrast, BMA (Bayesian model averaging) assigns weights to classifier-class pairs based on their Bayesian inference posterior probabilities, which reflect the likelihood that each classifier is accurate given the observed data (Raftery, Madigan, and Hoeting 1997; Wang et al. 2022).\n\nHowever, these weighting schemes do not optimally assign classifier weights, are not easily adaptable to a varied number of classifiers, and fail to address highly uneven class distributions while achieving generalization. In contrast, our MIP approach attains generalization in weight calculation through elastic net regularization, which is a regularized regression technique. It linearly combines the widely used L1 and L2 penalties of the lasso and ridge methods, respectively, to leverage the advantages of both strategies (Zou and\n\n1. Training & Validation ‰∏Ä Ëûç Ëûç Mean Validation Set  of  Classifiers Accuracy Matrix V= [Uijlnxm Training/ Validation Set 80% Stratified k-Fold Cross-Validation Test Set 20% Dataset (Set  of  Classes) MIP - Elastic Net ÂÅàËá≥‚Üê Ensemble Model Inference of  Classifiers Weight Matrix Select  Classifiers 3. Test & Evaluation W= [wij]nxm 2. Weight Calculation\n\nHastie 2005). This dual technique can lead to a weight assignment that is both sparse (less likely to overfit) and robust (less sensitive to training data noise) (Giesen et al. 2019).\n\nPrevious studies have explored different perspectives and strategies to address the challenges of imbalanced datasets. A comprehensive review of the problem, potential solutions, and applicable evaluation metrics are examined in (He and Garcia 2009). A general evaluation framework is proposed in (Khan, Chaudhari, and Chandra 2024), employing data augmentation and ensemble techniques by analyzing binary datasets with varied class imbalance. In (Lango and Stefanowski 2022), the impact of multi-class imbalanced data difficulty factors (such as class overlapping and skewed distribution) on classifier performance is examined. On the other hand, two approaches integrating sampling, data space improvement, ensemble, and self-paced learning are introduced in (Liu et al. 2020) and (Yin et al. 2020) to handle class imbalance. However, both methods are tailored to binary classification. Overall, previous studies lack an optimal and adaptable technique for classifier weight assignment to enhance ensemble performance in imbalanced multi-class datasets, while achieving generalization and computational efficiency across varied ensemble sizes.\n\n# Proposed Weighting Scheme\n\nTo incorporate the proposed weighting approach into the ensemble model, we employ the methodology outlined in Figure 1. Specifically, given a set of $n$ classifiers $\\mathcal { C } ~ = ~ \\bar { \\{ C _ { 1 } , C _ { 2 } , \\bar { ~ . ~ . ~ . ~ } , C _ { n } \\} }$ , a set of $m$ classes $\\varepsilon \\ =$ $\\{ E _ { 1 } , E _ { 2 } , \\dots , E _ { m } \\}$ , and a predefined ensemble size $K$ , our methodology consists of three phases:\n\n1. Training & Validation: The input dataset corresponding to $\\mathcal { E }$ is used to train each classifier in $\\mathcal { C }$ , using stratified $k$ -fold cross-validation to ensure a similar class distribution in each fold. This yields the mean validation accuracy matrix $\\mathbf { V } = [ v _ { i j } ] _ { n \\times m }$ , where $v _ { i j }$ denotes the mean\n\nvalidation accuracy of classifier $C _ { i }$ for class $E _ { j }$ across all folds. It is noted that $\\mathcal { E } _ { \\mathrm { N } } \\cup \\mathcal { E } _ { \\mathrm { A } } = \\mathcal { E }$ and $\\mathcal { E } _ { \\mathrm { N } } \\overset { \\cdot } { \\cap } \\mathcal { E } _ { \\mathrm { A } } = \\varnothing$ , where $\\mathcal { E } _ { \\mathrm { N } } ~ \\subset ~ \\mathcal { E }$ and $\\mathcal { E } _ { \\mathrm { A } } ~ \\subset ~ \\mathcal { E }$ represent the normal and abnormal (e.g., faults/attacks) classes, respectively, in $\\mathcal { E }$ .   \n2. Weight Calculation: We use $\\mathbf { V }$ to determine the weights for each classifier-class pair. The proposed approach utilizes MIP and elastic net regularization to select the predefined number of classifiers $K$ from the initial set $\\mathcal { C }$ , while optimally calculating their weights per class. This results in a weight matrix $\\mathbf { W } = [ w _ { i j } ] _ { n \\times m }$ , where $w _ { i j }$ denotes the weight of classifier $C _ { i }$ for class $E _ { j }$ . If a classifier $C _ { i }$ is not selected, then $w _ { i j } = 0 \\forall E _ { j } \\in \\mathcal { \\bar { E } }$ .   \n3. Test & Evaluation: The calculated weights in $\\mathbf { W }$ are assigned to the selected $K$ classifiers to evaluate the performance of the resulting ensemble model.\n\nThe main problem addressed‚Äîdetermining the optimal class-based weight assignment for classifiers in a weighted voting ensemble model‚Äîcorresponds to phase 2.\n\n# Problem Formulation\n\nTo solve the examined problem in phase 2, we formulate it as a MIP model by defining its decision variables, objective function, and constraints.\n\nDecision Variables. The following variables are used to select $K$ classifiers and to determine their optimal weights:\n\n‚Ä¢ $x _ { i }$ : binary variable corresponding to classifier $C _ { i }$ , such that $x _ { i } = 1$ if $C _ { i }$ is selected, and $x _ { i } = 0$ otherwise $\\mathbf { X } =$ $[ x _ { i } ] _ { n \\times 1 } )$ . ‚Ä¢ $w _ { i j }$ : non-negative continuous variable representing the $( i , j )$ -th element of weight matrix $\\mathbf { W }$ , denoting the weight of classifier-class pair $( C _ { i } , E _ { j } )$ .\n\nObjective. We aim to maximize the overall classification accuracy of the ensemble while ensuring generalization, i.e.:\n\n$$\n\\begin{array} { r l r } {  { \\sum _ { \\mathbf { X } , \\mathbf { W } } ^ { \\mathrm { m a x } } [ \\frac { 1 } { m } \\sum _ { C _ { i } \\in \\mathcal { C } } \\sum _ { E _ { j } \\in \\mathcal { E } } w _ { i j } v _ { i j }  } } \\\\ & { } & { \\quad -  \\lambda ( \\alpha \\sum _ { C _ { i } \\in \\mathcal { C } } \\sum _ { E _ { j } \\in \\mathcal { E } } w _ { i j } + \\frac { 1 - \\alpha } { 2 } \\sum _ { C _ { i } \\in \\mathcal { C } } \\sum _ { E _ { j } \\in \\mathcal { E } } w _ { i j } ^ { 2 } ) ] } \\end{array}\n$$\n\nsubject to the constraints defined in the next subsection.\n\nIn (1), the overall accuracy is denoted by the first term, where $v _ { i j }$ represents the $( i , j )$ -th element of mean validation accuracy matrix $\\mathbf { V }$ , denoting the mean accuracy of classifier $C _ { i }$ for class $E _ { j }$ in the training and validation phase (phase 1). To prevent the ensemble model from overly relying on specific classifier-class combinations based on the accuracy attained in phase 1, and thus achieve a more balanced and generalizable model, we incorporate into our objective the elastic net regularization technique, represented by the second term in (1). The L1 and L2 penalties are denoted by $\\begin{array} { r } { \\sum _ { C _ { i } \\in \\mathcal { C } } \\sum _ { E _ { j } \\in \\mathcal { E } } w _ { i j } } \\end{array}$ and $\\begin{array} { r } { \\sum _ { C _ { i } \\in \\mathcal { C } } \\bar { \\sum _ { E _ { j } \\in \\mathcal { E } } w _ { i j } ^ { 2 } } } \\end{array}$ , respectively. The L1 penalty promote  sparsity by driving some weights to zero, while the L2 penalty results in smaller and more evenly distributed weights. Consequently, the combination of both penalties leads to a sparse and robust weight assignment. In (1), $\\lambda$ is the regularization parameter that controls the strength of the overall penalty applied to the objective function $( \\lambda \\geq 0 )$ ). On the other hand, $\\alpha$ is the mixing parameter that balances the contribution of the L1 and L2 penalties in the elastic net regularization $( 0 \\leq \\alpha \\leq 1 )$ ).\n\nConstraints. Our objective function (1) is solved subject to the following constraints:\n\n$$\nx _ { i } \\in \\{ 0 , 1 \\} , \\forall C _ { i } \\in \\mathcal { C } ,\n$$\n\n$$\nw _ { i j } \\geq 0 , \\forall C _ { i } \\in \\mathcal { C } , \\forall E _ { j } \\in \\mathcal { E } ,\n$$\n\n$$\n\\sum _ { C _ { i } \\in \\mathcal { C } } x _ { i } = K ,\n$$\n\n$$\n\\sum _ { C _ { i } \\in \\mathcal { C } } w _ { i j } = 1 , \\forall E _ { j } \\in \\mathcal { E } ,\n$$\n\n$$\n\\sum _ { E _ { j } \\in \\mathcal { E } } w _ { i j } \\leq m x _ { i } , \\forall C _ { i } \\in \\mathcal { C } ,\n$$\n\n$$\n\\sum _ { E _ { j } \\in \\mathcal { E } } w _ { i j } + M ( 1 - x _ { i } ) \\geq \\epsilon , \\forall C _ { i } \\in \\mathcal { C } ,\n$$\n\n$$\n\\sum _ { C _ { i } \\in \\mathcal { C } } w _ { i j } v _ { i j } \\ge \\frac { 1 } { n } \\sum _ { C _ { i } \\in \\mathcal { C } } v _ { i j } + \\epsilon , \\forall E _ { j } \\in \\mathcal { E } ,\n$$\n\n$$\n\\frac { 1 } { m } \\sum _ { C _ { i } \\in \\mathcal { C } } \\sum _ { E _ { j } \\in \\mathcal { E } } w _ { i j } v _ { i j } \\ge \\frac { 1 } { n m } \\sum _ { C _ { i } \\in \\mathcal { C } } \\sum _ { E _ { j } \\in \\mathcal { E } } v _ { i j } + \\epsilon .\n$$\n\nConstraints (2) and (3) ensure the binary nature and nonnegativity of variables $x _ { i }$ and $w _ { i j }$ , respectively. Constraint (4) enforces the selection of the predefined number of classifiers $K$ . On the other hand, constraint (5) guarantees that for each class, the sum of weights of all classifiers is equal to one. If a classifier is not selected, (6) enforces all of its weights across all classes to be equal to zero (otherwise, the sum of its weights is bounded by the total number of classes $m$ ). In contrast, constraint (7) ensures that if a classifier is selected, at least one of its weights across all classes is greater than zero. In (7), $\\epsilon$ is a positive constant, sufficiently smaller than the variables and parameters in the MIP model. It is used to convert (7) to a non-strict inequality, as strict inequalities are not supported in MIP.\n\nOn the other hand, $M$ is a positive constant, sufficiently larger than the variables and parameters in the MIP model. It is used to express the conditional aspect of (7) in linear form. Specifically, the constraint becomes relevant $\\begin{array} { r } { \\big ( \\sum _ { E _ { j } \\in \\mathcal { E } } w _ { i j } \\ \\ge \\ \\epsilon \\big ) } \\end{array}$ only if $x _ { i } = 1$ . Otherwise, if $x _ { i } = 0$ the constraint becomes irrelevant $\\begin{array} { r } { \\big ( \\sum _ { E _ { j } \\in \\mathcal { E } } w _ { i j } + M \\ge \\epsilon \\big ) } \\end{array}$ , as it is always true. Constraint (8) ensures that the selected weights per classifier and class are such that the resulting weighted average accuracy for each class is greater than the average accuracy for the particular class when uniform weights are utilized across all classifiers. Similarly, (9) guarantees that the overall weighted average accuracy of the ensemble is greater than the overall average accuracy in the case where all weights of classifier-class pairs are uniform. In (8) and (9), $\\epsilon$ is utilized in the same manner as in (7).\n\nThe time complexity of MIP models depends on both the problem size and the solver used. MIP solvers typically employ proprietary algorithms with undisclosed implementation details, and thus their time complexity cannot be easily derived. However, the computational cost of the proposed MIP approach can be considered proportional to the problem size $n m$ , where $n$ (number of classifiers) is small in practice (Dogan and Birant 2019). Consequently, the computational cost grows approximately linearly with respect to $m$ (number of classes). In CPS rare event detection, $m$ is typically small (Vrachimis et al. 2018), as the aim is to distinguish between normal and rare abnormal states rather than to conduct a fine-grained classification. It is noted that our MIP model is independent of the dataset size. These factors ensure that the runtime of our MIP method remains reasonable, as demonstrated in our experimental results.\n\n# Experimental Framework\n\nWe evaluated and compared the performance and efficiency of the proposed weighting scheme against six wellestablished approaches. The comparative evaluation was conducted using appropriate metrics and four relevant imbalanced multi-class datasets for rare event detection in critical CPS, under varied ensemble sizes.\n\n# Experimental Setup\n\nThe three phases of our methodology were implemented using a server running CentOS 7.9, equipped with an Intel Xeon Gold 6240 processor $\\boldsymbol { @ } \\ 2 . 6 \\boldsymbol { \\mathrm { G H z } }$ and $4 0 0 \\mathrm { G B }$ of RAM. The software environment included Python 3.12 with scipy 1.14.0, scikit-learn 1.5.1, NumPy 2.0, pandas 2.2.2, and Gurobi Optimizer 11.0.3 with its Python interface gurobipy. We utilized the Gurobi solver in phase 2 to solve our MIP-formulated model, as it is widely used to optimally solve similar optimization problems (Gurobi, LLC 2024). We split each examined dataset into $80 \\%$ for training and validation, used in phase 1, and $20 \\%$ for the test set, used in phase 3. In phase 1, we employed stratified $k$ -fold crossvalidation with $k = 5$ (McTavish et al. 2022). To fine-tune parameters $\\lambda$ and $\\alpha$ in (1), we incremented/decremented each parameter, terminating adjustments when performance began to deteriorate. This fine-tuning resulted in the following values for $( \\lambda , \\alpha )$ for each of the four examined datasets: (0.95, 0.85), (0.96, 0.80), (1.00, 0.82), and (0.95, 0.86), respectively.\n\n# Existing Weighting Schemes\n\nWe compared our MIP approach against the six widely used weighting methods discussed in related work: UW-PC, UWPCC, WA-PC, WA-PCC, DE, and BMA. In our experiments, UW-PC and UW-PCC served as baselines. Without loss of generality, we used the mean validation accuracy matrix $\\mathbf { V }$ as the likelihood matrix in BMA (Zhou 2021).\n\n# Classifiers\n\nWe considered a set of $n = 8$ commonly used classifiers ${ \\mathcal { C } } = \\{ { \\mathrm { M L R } } ,$ , J48, JRIP, REPTree, MLP, SVM, GNB, $\\mathbf { I B k } \\}$ . These classifiers were selected for their complementary strengths in multi-class classification, as they encompass a diverse range of approaches: probabilistic (GNB), rulebased (JRIP), tree-based (J48, REPTree), function-based (MLR, MLP, SVM), and instance-based (IBk) (Hastie, Tibshirani, and Friedman 2009). All classifiers are available in the scikit-learn 1.5.1 Python library. In our experiments, we used ensemble sizes ranging from $K = 2$ to $K = 8$ to investigate a comprehensive set of scenarios.\n\nTable 1: Class distribution in datasets D1‚ÄìD4 $( N _ { j } \\in \\mathcal { E } _ { \\mathrm { N } }$ and $A _ { j } , F _ { j } \\in \\mathcal { E } _ { \\mathrm { A } } )$ .   \n\n<html><body><table><tr><td>Ej ClassDescription</td><td>#Instances</td><td>%</td></tr><tr><td colspan=\"3\">DatasetD1:LeakDB (p = 6059.97)</td></tr><tr><td>N1</td><td>Normal-No Leak/No Pressure</td><td>17520 3.14</td></tr><tr><td></td><td>N2Normal-NoLeak/HighPressure</td><td>448 438 80.38</td></tr><tr><td></td><td>N3Normal -No Leak/Low Pressure</td><td>88154 15.80</td></tr><tr><td>F1</td><td>Abrupt-LargeLeak/High Pressure</td><td>1721 0.31</td></tr><tr><td>F2</td><td>Incipient-LargeLeak/HighPressure</td><td>1181 0.21</td></tr><tr><td>F3</td><td>Incipient-Large Leak/Low Pressure</td><td>812 0.15</td></tr><tr><td>F4</td><td>Incipient- SmallLeak/LowPressure</td><td>74 0.01</td></tr><tr><td>Total</td><td>557900</td><td>100.00</td></tr><tr><td colspan=\"3\">Dataset D2: NSL-KDD (p = 1222.64)</td></tr><tr><td>N1 Normal</td><td>13 449</td><td>53.39</td></tr><tr><td>AiProbe</td><td>2289</td><td>9.09</td></tr><tr><td>A2 Denial of Service (DoS)</td><td>9 234</td><td>36.65</td></tr><tr><td>A3User to Root (U2R)</td><td>11</td><td>0.04</td></tr><tr><td>A4 Remote to Local (R2L)</td><td></td><td>0.83</td></tr><tr><td>Total</td><td>209 25192</td><td>100.00</td></tr><tr><td colspan=\"3\">DatasetD3:SG-MITM(p = 5.91)</td></tr><tr><td>N1Normal</td><td></td><td>455 65.28</td></tr><tr><td>AMITMon Router-Controller</td><td></td><td>84 12.05</td></tr><tr><td>A2MITMon Controller</td><td></td><td>77 11.05</td></tr><tr><td>A3MITMon Router</td><td></td><td>81 11.62</td></tr><tr><td>Total</td><td>697</td><td>100.00</td></tr><tr><td colspan=\"3\">DatasetD4: CIC-IDS2017(p= 2.13)</td></tr><tr><td>N1 Normal</td><td>273097</td><td>34.52</td></tr><tr><td></td><td>A1Distributed Denial of Service (DDoS) 128027</td><td>16.18</td></tr><tr><td></td><td>A2 Denial of Service Hulk (DoS Hulk) 231073</td><td>29.20</td></tr><tr><td>A3 PortScan</td><td>158 930</td><td>20.10</td></tr><tr><td>Total</td><td>791127</td><td>100.00</td></tr></table></body></html>\n\n# Datasets\n\nWe experimented with four representative datasets for rare event detection in critical CPS, featuring multiple classes with varied imbalance: [D1] LeakDB (Vrachimis et al. 2018), [D2] NSL-KDD (Tavallaee et al. 2009), [D3] SG-MITM (Elrawy et al. 2023), and [D4] CIC-IDS2017 (Sharafaldin, Lashkari, and Ghorbani 2018). Benchmarks D2 and D4 pertain to intrusion detection in simulated network environments, resembling real-world network traffic captures. They are widely used to investigate anomaly detection in CPS (Zhou et al. 2020; Danso et al. 2022). On the other hand, D1 involves leakages (faults) in a simulated real-world water distribution network under varying conditions, while D3 focuses on man-in-the-middle (MITM) attacks targeting client-server protocols in the private area network (PAN) of a smart grid CPS, represented by an experimental testbed. Table 1 shows the description, number of instances, and the percentage distribution of each normal/abnormal class in datasets D1‚ÄìD4. Furthermore, it showcases the imbalance ratio $\\rho$ for each dataset, which is defined as the ratio of the number of instances $\\mu$ in the majority class to the number of instances $\\nu$ in the minority class (Wang et al. 2021). Considering that a dataset is highly imbalanced when $\\mu \\gg \\nu$ , D1 and D2 are highly imbalanced, whereas D3 and D4 are moderately imbalanced (Liu et al. 2020).\n\n![](images/d66b4c760c745192c761cd7b7b692f72c0e20735b82555317977386299c372a2.jpg)\n\nFigure 2: Improvement achieved in balanced accuracy by proposed MIP approach over other weighting schemes (D1).   \nTable 2: Improvement attained in macro-averaged precision, recall, and F1-score by proposed MIP approach over other weighting schemes (D1).   \n\n<html><body><table><tr><td></td><td>K</td><td>2</td><td>3</td><td>4 4.93</td><td>5 5.34</td><td>6 5.47</td><td>7 5.59</td><td>8 5.88</td></tr><tr><td></td><td>UW-PC UW-PCC WA-PC WA-PCC DE BMA Avg</td><td>4.06 3.83 3.71 3.48 2.90 3.48 3.58</td><td>4.59 4.23 4.11 3.76 3.17 3.88 3.96</td><td>4.45 4.57 4.33 3.51 4.45 4.37</td><td>4.84 4.96 4.72 3.74 4.96 4.76</td><td>4.97 4.97 4.85 3.74 4.85 4.81</td><td>5.21 5.09 4.96 3.86 5.09 4.97</td><td>5.62 5.49 5.10 4.09 5.23 5.24</td></tr><tr><td></td><td>Keeer</td><td>UW-PC UW-PCC WA-PC WA-PCC</td><td>4.02 3.80 3.68</td><td>4.48 4.25 4.13</td><td>5.04 4.45 4.57</td><td>5.47 4.89 5.01</td><td>5.57 5.11 5.11</td><td>5.67 5.78 5.32 5.54 5.20 5.43</td></tr><tr><td></td><td>DE BMA Avg UW-PC</td><td>3.46 2.90 3.46 3.55 3.99</td><td>3.79 3.21 3.90 3.96 4.54</td><td>4.34 3.53 4.45 4.40 4.99</td><td>4.77 3.74 5.01 4.82 5.41</td><td>4.87 3.84 4.99 4.92 5.53</td><td>4.97 3.83 5.09 5.01</td><td>5.08 4.04 5.19 5.18</td></tr><tr><td></td><td>UW-PCC WA-PC WA-PCC DE BMA Avg</td><td>3.76 3.64 3.41 2.96 3.41</td><td>4.19 4.07 3.72 3.14 3.83</td><td>4.40 4.51 4.28 3.46 4.40</td><td>4.81 4.93 4.69 3.74 4.93</td><td>5.05 5.05 4.81 3.85 4.93</td><td>5.64 5.28 5.15 4.91 3.85 5.03</td><td>5.72 5.47 5.47 5.10 4.02 5.23</td></tr></table></body></html>\n\n# Evaluation Metrics\n\nTo provide a comprehensive assessment of our approach against the examined weighting schemes, we employed in test and evaluation phase (phase 3) performance metrics well-suited for imbalanced multi-class datasets. Specifically, balanced accuracy served as our primary metric due to its ability to account for class imbalance by averaging the recall obtained for each class. Additionally, we utilized macroaveraged precision, recall, and F1-score as secondary metrics to ensure an unbiased evaluation across all classes, thus providing a holistic comparative analysis (Grandini, Bagli, and Visani 2020; Lango and Stefanowski 2022).\n\n![](images/8ae444aae9d0afaf55852bceb2cee5b07c5326e26270a15c676e13d4cde095f6.jpg)\n\nFigure 3: Improvement achieved in balanced accuracy by proposed MIP approach over other weighting schemes (D2).   \nTable 3: Improvement attained in macro-averaged precision, recall, and F1-score by proposed MIP approach over other weighting schemes (D2).   \n\n<html><body><table><tr><td colspan=\"2\"></td><td>K</td><td>2 3</td><td>4</td><td>5 4.87</td><td>6 4.97</td><td>7 5.06</td><td>8</td></tr><tr><td></td><td></td><td>UW-PC UW-PCC WA-PC WA-PCC DE BMA</td><td>3.25 3.03 2.92 2.69 1.06 1.60</td><td>3.84 3.62 3.51 3.29 1.46 2.10</td><td>4.37 4.04 3.82 3.60 1.77</td><td>4.76 4.43 4.21 2.17</td><td>4.97 4.42 4.09 2.16</td><td>4.95 4.51 4.18 2.15</td><td>5.04 4.93 4.49 4.17 2.15 3.63 4.07</td></tr><tr><td rowspan=\"7\"></td><td>Keeer</td><td>Avg UW-PC UW-PCC</td><td>2.43 3.23 2.99</td><td>2.97 3.79 3.56</td><td>2.62 3.37 4.36 4.12</td><td>3.34 3.96 4.81 4.69</td><td>3.44 4.01 4.80 4.80</td><td>3.64 4.08 5.02 5.13</td></tr><tr><td></td><td>WA-PC WA-PCC DE BMA</td><td>2.87 2.63 1.01 1.59</td><td>3.44 3.20 1.46 2.15</td><td>3.89 3.65 1.79 2.71</td><td>4.45 4.21 2.23 3.27</td><td>4.32 4.09 2.12 3.38</td><td>4.90 5.01 4.54 4.65 4.19 4.30 2.22 2.33 3.72 3.83</td></tr><tr><td></td><td>Avg UW-PC UW-PCC WA-PC</td><td>2.39 3.24 3.01 2.90</td><td>2.93 3.88 3.65 3.54</td><td>3.42 4.43 4.20 3.97</td><td>3.94 4.85 4.74 4.39</td><td>3.92 4.84 4.84 4.38</td><td>4.10 4.21 5.16 5.15 5.04 5.03 4.59 4.58</td></tr><tr><td></td><td>WA-PCC DE BMA Avg</td><td>2.78 1.09 1.65 2.45</td><td>3.31 1.52 2.18 3.01</td><td>3.74 1.84 2.72 3.48</td><td>4.16 2.15 3.26 3.93</td><td>4.04 2.03 3.36 3.92</td><td>4.24 4.23 2.24 2.24 3.79 3.78 4.18 4.17</td></tr></table></body></html>\n\n# Results & Discussion\n\nFigures 2‚Äì5 demonstrate the percentage improvement achieved in balanced accuracy (primary metric) by the proposed MIP weighting scheme over all existing approaches for datasets D1‚ÄìD4, respectively, for $K = 2$ to $K = 8$ classifiers selected from set $\\mathcal { C }$ . Tables 2‚Äì5 showcase the percentage improvement attained with respect to the secondary metrics (macro-averaged precision, recall, and F1-score) in each case. The average improvement per dataset is also shown in Table 6. Notably, MIP consistently outperformed the other techniques across all evaluation metrics, ensemble sizes, and datasets, demonstrating superior generalization and robustness. Specifically, MIP yielded an increase in balanced accuracy ranging from $0 . 9 9 \\%$ to $7 . 3 1 \\%$ , with an overall average of $4 . 5 3 \\%$ across all datasets and ensemble sizes. Moreover, it attained an overall average increase of $4 . 6 3 \\%$ , $4 . 6 0 \\%$ , and $4 . 6 1 \\%$ in macro-averaged precision, recall, and F1-score, respectively. In addition to these metrics, MIP also outperformed all other schemes with respect to the macro-averaged area under the precision-recall curve (AUPRC), as detailed in the provided supplementary material.\n\n![](images/74ec62f08ecf7aef7ea0aae32c8d92a5eeb69ff66ca86225bc0fcd9ca6a5967e.jpg)\n\nFigure 4: Improvement achieved in balanced accuracy by proposed MIP approach over other weighting schemes (D3).   \nTable 4: Improvement attained in macro-averaged precision, recall, and F1-score by proposed MIP approach over other weighting schemes (D3).   \n\n<html><body><table><tr><td></td><td>K</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7 6.88</td><td>8 6.99</td></tr><tr><td></td><td>UW-PC UW-PCC WA-PC WA-PCC DE BMA</td><td>5.02 4.89 4.64 4.27 3.29 3.90</td><td>5.83 5.70 5.44 5.06 4.18 4.81 5.17</td><td>6.48 6.35 5.83 5.45 4.31 5.19</td><td>6.46 6.33 5.82 5.31 3.93 5.05</td><td>6.93 6.80 6.28 5.77 4.51 5.52</td><td>6.63 6.37 5.86 4.61 5.73</td><td>6.72 6.46 5.80 4.51</td></tr><tr><td></td><td>Keeer</td><td>Avg UW-PC UW-PCC WA-PC</td><td>4.34 5.02 5.84 4.89 5.71 4.64</td><td>5.60 6.42 6.29</td><td>5.48 6.41 6.28 5.78</td><td>5.97 6.97 6.85 6.35</td><td>6.01 6.82 6.57 6.19</td><td>5.80 6.05 6.92 6.67 6.42</td></tr><tr><td></td><td>WA-PCC DE BMA Avg</td><td>4.27 3.29 3.90 4.34</td><td>5.34 5.09 4.10 4.84 5.15</td><td>5.92 5.42 4.31 5.17 5.59</td><td>5.28 3.94 5.04 5.46</td><td>5.85 4.50 5.60 6.02</td><td>5.82 4.60 5.70 5.95</td><td>5.80 4.47 5.68 5.99</td></tr><tr><td></td><td>UW-PC UW-PCC WA-PC WA-PCC DE BMA</td><td>5.02 4.89 4.64 4.27 3.29 3.90</td><td>5.90 5.77 5.39 5.01 4.14</td><td>6.51 6.39 5.88 5.50 4.25</td><td>6.50 6.37 5.73 5.36 4.00</td><td>6.83 6.70 6.32 5.81 4.45</td><td>6.91 6.66 6.28 5.90 4.55</td><td>6.90 6.64 6.39 5.75 4.37</td></tr></table></body></html>\n\nFurthermore, it can be observed that the improvement provided in all performance metrics by our approach over the other schemes generally increased as the ensemble size $K$ increased. This reveals that MIP benefits more from a larger number of classifiers compared to the other methods. By optimally assigning weights to classifier-class pairs, our approach enhances the ability of the ensemble to leverage the diverse capabilities of each classifier, leading to a better performance as the number of classifiers grows. On the other hand, the performance improvement was lower for the highly imbalanced datasets D1 and D2 (which had a higher imbalance ratio), compared to the moderately imbalanced datasets D3 and D4 (which had a lower imbalance ratio). This highlights the negative impact of class imbalance on the ensemble performance under all weighting schemes. However, as demonstrated in Table 6, the performance gains for the highly imbalanced datasets were still significant and comparable to those observed for the moderately imbalanced datasets. The effectiveness and robustness of our method under different cases of class imbalance are further demonstrated through the ablation studies included in the supplementary material. As reported by the Gurobi solver, the number of variables and constraints required for each dataset by our MIP approach are shown in Table 6.\n\n![](images/6391f096e7f33140b8a0f91fdb891f7d31baeb6506a5d45e4e73922af1290070.jpg)\n\nFigure 5: Improvement achieved in balanced accuracy by proposed MIP approach over other weighting schemes (D4).   \nTable 5: Improvement attained in macro-averaged precision, recall, and F1-score by proposed MIP approach over other weighting schemes (D4).   \n\n<html><body><table><tr><td colspan=\"2\"></td><td>K</td><td>2 3</td><td>4</td><td>5 5.84</td><td>6 6.54</td><td>7 8 7.32 6.89</td></tr><tr><td></td><td></td><td>UW-PC UW-PCC WA-PC WA-PCC DE BMA</td><td>1.68 4.74 5.81 4.62 5.45 4.13 5.20 3.76 4.12 2.91 4.72 3.40</td><td>5.13 5.13 4.49 3.61 3.36 3.86</td><td>5.71 4.96 3.74 3.74 4.47</td><td>6.41 5.29 3.94 4.19 5.04</td><td>7.05 6.64 5.75 6.38 4.22 5.88 4.59 4.65</td></tr><tr><td></td><td>eeer</td><td>Avg UW-PC UW-PCC WA-PC</td><td>4.50 3.93 4.38 4.82 4.14 4.70</td><td>4.26 5.22 5.22</td><td>4.74 5.82 5.69</td><td>5.24 6.67 6.54</td><td>5.62 5.76 5.76 6.03 7.31 6.87 7.08 6.63</td></tr><tr><td></td><td>WA-PCC DE BMA Avg</td><td>3.78 3.54 2.37 2.95 3.53</td><td>4.10 3.75 2.82 3.40</td><td>4.52 3.72 3.38 3.95</td><td>4.86 3.80 3.80 4.50</td><td>5.33 5.70 4.04 4.24 4.27 4.57 5.10 5.59</td><td>6.39 5.90 4.60 5.78</td></tr><tr><td></td><td>UW-PC UW-PCC WA-PC WA-PCC DE</td><td>3.06 4.97 4.61 4.37</td><td>3.93 4.67 4.55 4.06 3.70</td><td>4.33 5.13 5.13 4.40 3.68</td><td>4.75 5.71 5.59 4.86</td><td>5.32 6.67 6.54 5.31</td><td>5.75 6.03 7.23 6.88 7.10 6.64 5.63 6.26</td></tr></table></body></html>\n\n# Weight Assignment Example\n\nTo demonstrate the effective utilization of the strengths of each classifier in $\\mathcal { C }$ by our method when assigning weights to classifier-class pairs, we consider the mean validation accuracy matrix $\\mathbf { V }$ (given in Table 7) as obtained for dataset\n\n<html><body><table><tr><td rowspan=\"3\">8</td><td rowspan=\"3\">p</td><td rowspan=\"3\">#varr.</td><td colspan=\"4\">Avre.Inon Re (al)</td></tr><tr><td>Bal. Acc.</td><td></td><td></td><td>F1-score</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>D1</td><td>6059.97</td><td>64/32</td><td>4.45</td><td>4.53</td><td>4.55</td><td>4.51</td></tr><tr><td>D2</td><td>1222.64</td><td>48/28</td><td>3.54</td><td>3.55</td><td>3.56</td><td>3.59</td></tr><tr><td>D3 D4</td><td>5.91 2.13</td><td>40/26 40/26</td><td>5.42 4.72</td><td>5.52 4.92</td><td>5.50</td><td>5.51</td></tr><tr><td colspan=\"2\">Overall</td><td></td><td>4.53</td><td>4.63</td><td>4.81 4.60</td><td>4.83 4.61</td></tr></table></body></html>\n\nTable 6: Average improvement achieved in all performance metrics by proposed MIP approach over other weighting schemes across all ensemble sizes for datasets D1‚ÄìD4.\n\nD2 and ensemble size $K = 8$ . Although in the specific example classifier SVM (highlighted in gray) was not among the best-performing classifiers on average, it performed well for classes $A _ { 2 }$ and $A _ { 4 }$ (despite $A _ { 4 }$ being among the minority classes in D2). Table 8 shows the weights assigned to each class for SVM by each weighting approach. It can be observed that MIP (highlighted in gray) assigned relatively high weights for classes $A _ { 2 }$ and $A _ { 4 }$ where SVM performed well, in contrast to all other methods, which assigned significantly lower weights. On the other hand, MIP assigned low weights to the classes where SVM did not perform well. Evidently, MIP leveraged the capabilities of SVM more effectively than all other approaches, even though SVM was among the lower-performing classifiers on average.\n\n# Computational Efficiency\n\nIn addition to the significant performance gains provided by our method, MIP demonstrated computational efficiency in calculating the weights for $K$ classifiers from a set of $n$ , compared to the other weighting schemes, despite the NPhard nature of the examined problem. MIP seamlessly selects $K$ classifiers while optimally calculating their weights in a single run. In contrast, the other examined approaches calculate the weights for each possible classifier combination individually (i.e., for a total of $\\textstyle { \\binom { n } { K } }$ combinations) before eventually selecting the $K$ best-performing classifiers. Figure 6 illustrates the speedup achieved in weight calculation by MIP over the other methods, for dataset D2, $K \\ = \\ \\{ 3 , 5 , 7 \\}$ , and $n ~ = ~ \\{ 8 , 1 6 , 2 4 \\}$ . It can be observed that the speedup provided by MIP increased significantly as $n$ grew, especially for larger ensemble sizes, highlighting its practicality compared to the other techniques. Even for smaller $K$ and $n$ , MIP was considerably more efficient than the other weighting schemes. For example, for $K = 3$ and $n = 8$ , MIP required $0 . 3 5 \\mathrm { s }$ to calculate the classifier weights, whereas the other approaches required 1 to $3 . 1 2 \\mathrm { s }$ .\n\n# Conclusion\n\nWe proposed an optimal, adaptable, and efficient MIPbased ensemble weighting scheme for imbalanced multiclass datasets to improve rare event detection in critical CPS. Our method optimizes classifier weights on a granular per class basis, while seamlessly and optimally selecting a predefined number of classifiers from a given set. Furthermore, it employs elastic net regularization to enhance robustness and generalization in weight assignment, while leveraging the strengths of each classifier. We compared our approach against six widely used weighting schemes, considering relevant datasets with multiple classes and varied imbalance ratios, under a wide range of ensemble sizes. The experimental results demonstrate the superior performance and efficiency of our technique over all existing methods, attaining an increase in balanced accuracy ranging from $0 . 9 9 \\%$ to $7 . 3 1 \\%$ , with an overall average of $4 . 5 3 \\%$ across all datasets and ensemble sizes. Moreover, it achieved an overall average improvement of $4 . 6 3 \\%$ , $4 . 6 0 \\%$ , and $4 . 6 1 \\%$ in macro-averaged precision, recall, and F1-score, respectively, while also exhibiting computational efficiency.\n\n![](images/5990c3184a4254ecbca1b69c74d0cea6762c10e24db0d15d155ea5f5df0696b6.jpg)  \nFigure 6: Speedup attained by proposed MIP approach over other schemes in calculating the weights of $\\bar { K ^ { \\prime } } = \\{ 3 , 5 , 7 \\}$ classifiers from a set of $n = \\bar { \\{ 8 , 1 6 , 2 \\bar { 4 } \\} }$ , for dataset D2.\n\nTable 7: Mean validation accuracy matrix $\\mathbf { V }$ for dataset D2 and ensemble size $K = 8$ .   \n\n<html><body><table><tr><td rowspan=\"2\">C</td><td colspan=\"3\">Ej</td><td rowspan=\"2\">Avg</td></tr><tr><td>N1 A1</td><td>A2</td><td>A3 A4</td></tr><tr><td>MLR</td><td>0.96</td><td>0.92 0.86</td><td>0.99 0.95 0.90</td><td>0.94</td></tr><tr><td>J48 JRIP</td><td>0.89 0.90</td><td>0.78 0.85 0.74 0.78</td><td>0.90 0.89 0.96</td><td>0.86 0.85</td></tr><tr><td>REPTree</td><td>0.76 0.86</td><td>0.80</td><td>0.98 0.73</td><td>0.83</td></tr><tr><td>MLP</td><td>0.90 0.92</td><td>0.81</td><td>0.71 0.79</td><td>0.83</td></tr><tr><td>SVM</td><td>0.76 0.73</td><td>0.89</td><td>0.76 0.94</td><td>0.82</td></tr><tr><td>GNB</td><td>0.90 0.90 0.72</td><td>0.85 0.81</td><td>0.71 0.73 0.74 0.71</td><td>0.80 0.76</td></tr></table></body></html>\n\nTable 8: Weights assigned to the SVM classifier (shown per class) by each weighting approach, for dataset D2 and ensemble size $K = 8$ .   \n\n<html><body><table><tr><td rowspan=\"2\">Weighting Scheme UW-PC</td><td>Ej</td></tr><tr><td>N1 A1 A2 A3 A4 0.13 0.13 0.13 0.13 0.13</td></tr><tr><td>UW-PCC WA-PC WA-PCC DE</td><td>0.03 0.03 0.03 0.03 0.03 0.11 0.11 0.11 0.11 0.11 0.02 0.02 0.03 0.02 0.03 0.12 0.12 0.12 0.12 0.12</td></tr><tr><td>BMA</td><td>0.02 0.02 0.03 0.02 0.02</td></tr><tr><td>MIP</td><td>0.00 0.03 0.20 0.04 0.23</td></tr></table></body></html>",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáÈíàÂØπÂÖ≥ÈîÆ‰ø°ÊÅØÁâ©ÁêÜÁ≥ªÁªüÔºàCPSÔºâ‰∏≠ÁΩïËßÅ‰∫ã‰ª∂Ê£ÄÊµãÁöÑÁ±ªÂà´‰∏çÂπ≥Ë°°Â§öÂàÜÁ±ªÊï∞ÊçÆÈõÜÈóÆÈ¢òÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßç‰ºòÂåñÁöÑÊ∑∑ÂêàÊï¥Êï∞ËßÑÂàíÔºàMIPÔºâÈõÜÊàêÂä†ÊùÉÊñπÊ°à„ÄÇÁé∞ÊúâÂä†ÊùÉÊñπÊ°àÊó†Ê≥ïÂú®È´òÂ∫¶‰∏çÂπ≥Ë°°ÁöÑÁ±ªÂà´ÂàÜÂ∏É‰∏ãÊúÄ‰ºòÂàÜÈÖçÂàÜÁ±ªÂô®ÊùÉÈáçÔºå‰∏îÁº∫‰πèÂØπ‰∏çÂêåÈõÜÊàêËßÑÊ®°ÁöÑÈÄÇÂ∫îÊÄß„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÂú®Â¶Ç‰æõÊ∞¥ÁΩëÁªúÂíåÁîµÁΩëÁ≠âÂÖ≥ÈîÆÂü∫Á°ÄËÆæÊñΩÁöÑÂÆâÂÖ®ÊÄßÂíåÂèØÈù†ÊÄßÁª¥Êä§‰∏≠ÂÖ∑ÊúâÈáçË¶Å‰ª∑ÂÄºÔºåÂõ†‰∏∫ËØØÂàÜÁ±ª‰ΩéÊ¶ÇÁéá‰ΩÜÈ´òÂΩ±ÂìçÁöÑ‰∫ã‰ª∂ÂèØËÉΩÂØºËá¥ÂÆâÂÖ®È£éÈô©ÊàñÁ≥ªÁªüÊïÖÈöú„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ÊèêÂá∫‰∏ÄÁßçÂü∫‰∫éMIPÂíåÂºπÊÄßÁΩëÁªúÊ≠£ÂàôÂåñÁöÑÈõÜÊàêÂä†ÊùÉÊñπÊ°àÔºåÈÄöËøá‰ºòÂåñÂàÜÁ±ªÂô®-Á±ªÂà´ÁöÑÊùÉÈáçÂàÜÈÖçÔºåÂêåÊó∂Ëá™Âä®ÈÄâÊã©È¢ÑÂÆö‰πâÊï∞ÈáèÁöÑÂàÜÁ±ªÂô®„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ1Ôºö** ÊèêÂá∫È¶ñ‰∏™ÂèØËá™ÈÄÇÂ∫îÈõÜÊàêËßÑÊ®°ÁöÑMIPÂä†ÊùÉÊñπÊ°àÔºåÂú®Âõõ‰∏™Êï∞ÊçÆÈõÜ‰∏äÂπ≥ÂùáÊèêÂçáÂπ≥Ë°°ÂáÜÁ°ÆÁéá4.53%ÔºàÊúÄÈ´ò7.31%Ôºâ„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ2Ôºö** ÂºïÂÖ•ÂºπÊÄßÁΩëÁªúÊ≠£ÂàôÂåñÔºàŒª=0.95-1.00, Œ±=0.80-0.86ÔºâÔºå‰ΩøÂÆèÂπ≥ÂùáF1-scoreÊèêÂçá4.61%„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ3Ôºö** ËÆ°ÁÆóÊïàÁéá‰ºòÂåñÔºåÁõ∏ÊØîÂü∫Á∫øÊñπÊ≥ïÂä†ÈÄü3.12ÂÄçÔºàn=8Êó∂Ôºâ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   ÈÄöËøáÊ∑∑ÂêàÊï¥Êï∞ËßÑÂàíÂêåÊó∂Ëß£ÂÜ≥ÂàÜÁ±ªÂô®ÈÄâÊã©ÂíåÊùÉÈáç‰ºòÂåñÈóÆÈ¢òÔºåÂºπÊÄßÁΩëÁªúÊ≠£ÂàôÂåñÔºàL1+L2ÔºâÂπ≥Ë°°Á®ÄÁñèÊÄß‰∏éÈ≤ÅÊ£íÊÄß„ÄÇËÆæËÆ°Âì≤Â≠¶Âú®‰∫éÔºö‰∏çÂêåÂàÜÁ±ªÂô®ÂØπ‰∏çÂêåÁ±ªÂà´ÁöÑËØÜÂà´ËÉΩÂäõÂ≠òÂú®‰∫íË°•ÊÄßÔºåÈúÄÁ≤æÁªÜÂåñÂà©Áî®„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **ÂÖàÂâçÂ±ÄÈôêÔºö** ‰º†ÁªüÊñπÊ≥ïÔºàÂ¶ÇUW-PC„ÄÅDEÔºâ‰ªÖÊï¥‰ΩìËØÑ‰º∞ÂàÜÁ±ªÂô®ÊÄßËÉΩÔºå‰∏îÂõ∫ÂÆöÈõÜÊàêËßÑÊ®°„ÄÇ\\n> *   **Êú¨ÊñáÊîπËøõÔºö** \\n>     1.  **ÁªìÊûÑÂàõÊñ∞Ôºö** ÊûÑÂª∫ÂàÜÁ±ªÂô®-Á±ªÂà´ÊùÉÈáçÁü©ÈòµW=[w_ij]_{n√óm}\\n>     2.  **ÁÆóÊ≥ïÂàõÊñ∞Ôºö** ÁõÆÊ†áÂáΩÊï∞ËûçÂêàÈ™åËØÅÂáÜÁ°ÆÁéáÔºàV_ijÔºâÂíåÊ≠£ÂàôÂåñÈ°πÔºàÂÖ¨Âºè1Ôºâ\\n>     3.  **Á≠ñÁï•ÂàõÊñ∞Ôºö** ÈÄöËøáÁ∫¶Êùü(4)-(9)‰øùËØÅÈõÜÊàêËßÑÊ®°ÂèØÊéß‰∏éÊÄßËÉΩ‰∏ãÈôê\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.  **ËÆ≠ÁªÉÈ™åËØÅÈò∂ÊÆµÔºö** ÈÄöËøáÂàÜÂ±Ç5Êäò‰∫§ÂèâÈ™åËØÅÁîüÊàêÂπ≥ÂùáÈ™åËØÅÂáÜÁ°ÆÁéáÁü©ÈòµV\\n> 2.  **ÊùÉÈáçËÆ°ÁÆóÈò∂ÊÆµÔºö** Ê±ÇËß£MIPÈóÆÈ¢òÔºàÂÖ¨Âºè1ÔºâÔºåÁõÆÊ†áÂáΩÊï∞‰∏∫Ôºö\\n>     ```math\\n>     \\\\max_{\\\\mathbf{X},\\\\mathbf{W}} \\\\left[ \\\\frac{1}{m}\\\\sum_{i,j}w_{ij}v_{ij} - \\\\lambda\\\\left(\\\\alpha\\\\sum_{i,j}|w_{ij}| + \\\\frac{1-\\\\alpha}{2}\\\\sum_{i,j}w_{ij}^2\\\\right) \\\\right]\\n>     ```\\n> 3.  **ÊµãËØïÈò∂ÊÆµÔºö** Áî®‰ºòÂåñÂêéÁöÑÊùÉÈáçÁü©ÈòµWËøõË°åÈõÜÊàêÈ¢ÑÊµã\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   UW-PCÔºàÁªü‰∏ÄÂàÜÁ±ªÂô®ÊùÉÈáçÔºâ\\n> *   UW-PCCÔºàÁªü‰∏ÄÂàÜÁ±ªÂô®-Á±ªÂà´ÊùÉÈáçÔºâ\\n> *   WA-PCÔºàÂü∫‰∫éÂàÜÁ±ªÂô®ÂáÜÁ°ÆÁéáÂä†ÊùÉÔºâ\\n> *   WA-PCCÔºàÂü∫‰∫éÂàÜÁ±ªÂô®-Á±ªÂà´ÂáÜÁ°ÆÁéáÂä†ÊùÉÔºâ\\n> *   DEÔºàÂ∑ÆÂàÜËøõÂåñÁÆóÊ≥ïÔºâ\\n> *   BMAÔºàË¥ùÂè∂ÊñØÊ®°ÂûãÂπ≥ÂùáÔºâ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®Âπ≥Ë°°ÂáÜÁ°ÆÁéá‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®LeakDBÊï∞ÊçÆÈõÜÔºàœÅ=6059.97Ôºâ‰∏äËææÂà∞Âπ≥ÂùáÊèêÂçá4.45%ÔºåÊòæËëó‰ºò‰∫éÊúÄ‰Ω≥Âü∫Á∫øBMAÔºà3.48%Ôºâ„ÄÇÂú®NSL-KDDÊï∞ÊçÆÈõÜÔºàœÅ=1222.64Ôºâ‰∏äÊúÄÈ´òÊèêÂçá7.31%„ÄÇ\\n> *   **Âú®ÂÆèÂπ≥ÂùáF1-score‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®SG-MITMÊï∞ÊçÆÈõÜÔºàœÅ=5.91Ôºâ‰∏äÂèñÂæó5.51%ÊèêÂçáÔºåËøúË∂ÖDEÔºà4.31%ÔºâÂíåWA-PCCÔºà5.45%Ôºâ„ÄÇ\\n> *   **Âú®ËÆ°ÁÆóÊïàÁéá‰∏äÔºö** ÂΩìn=24Êó∂ÔºåMIPÊØîDEÂø´8.9ÂÄçÔºàK=7ÔºâÔºåÂêåÊó∂‰øùÊåÅAUPRCÊèêÂçá4.83%„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   Á±ªÂà´‰∏çÂπ≥Ë°°Â≠¶‰π† (Imbalanced Learning, N/A)\\n*   Ê∑∑ÂêàÊï¥Êï∞ËßÑÂàí (Mixed Integer Programming, MIP)\\n*   ÈõÜÊàêÂ≠¶‰π† (Ensemble Learning, N/A)\\n*   ÂºπÊÄßÁΩëÁªúÊ≠£ÂàôÂåñ (Elastic Net Regularization, N/A)\\n*   ÁΩïËßÅ‰∫ã‰ª∂Ê£ÄÊµã (Rare Event Detection, N/A)\\n*   ‰ø°ÊÅØÁâ©ÁêÜÁ≥ªÁªü (Cyber-Physical Systems, CPS)\\n*   Âä†ÊùÉÊäïÁ•® (Weighted Voting, N/A)\"\n}\n```"
}