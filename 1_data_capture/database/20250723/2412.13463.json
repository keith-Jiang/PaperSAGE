{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.13463",
    "link": "https://arxiv.org/abs/2412.13463",
    "pdf_link": "https://arxiv.org/pdf/2412.13463.pdf",
    "title": "FlexPose: Pose Distribution Adaptation with Limited Guidance",
    "authors": [
        "Zixiao Wang",
        "Junwu Weng",
        "Mengyuan Liu",
        "Bei Yu"
    ],
    "categories": [
        "cs.CV",
        "cs.AI"
    ],
    "publication_date": "2024-12-18",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "The Chinese University of Hong Kong",
        "ByteDance Inc.",
        "Peking University"
    ],
    "paper_content": "# FlexPose: Pose Distribution Adaptation with Limited Guidance\n\nZixiao Wang1, Junwu Weng2\\*, Mengyuan Liu3, Bei $\\mathbf { Y } \\mathbf { u } ^ { 1 * }$\n\n1The Chinese University of Hong Kong 2ByteDance Inc. 3Peking University zxwang22@cse.cuhk.edu.hk, we0001wu@e.ntu.edu.sg, nkliuyifang $@$ gmail.com, byu@cse.cuhk.edu.hk\n\n# Abstract\n\nNumerous well-annotated human key-point datasets are publicly available to date. However, annotating human poses for newly collected images is still a costly and time-consuming progress. Pose distributions from different datasets share similar pose hinge-structure priors with different geometric transformations, such as pivot orientation, joint rotation, and bone length ratio. The difference between Pose distributions is essentially the difference between the transformation distributions. Inspired by this fact, we propose a method to calibrate a pre-trained pose generator in which the pose prior has already been learned to an adapted one following a new pose distribution. We treat the representation of human pose joint coordinates as skeleton image and transfer a pre-trained pose annotation generator with only a few annotation guidance. By fine-tuning a limited number of linear layers that closely related to the pose transformation, the adapted generator is able to produce any number of pose annotations that are similar to the target poses. We evaluate our proposed method, FlexPose, on several cross-dataset settings both qualitatively and quantitatively, which demonstrates that our approach achieves stateof-the-art performance compared to the existing generativemodel-based transfer learning methods when given limited annotation guidance.\n\n# 1 Introduction\n\nDeep neural networks are data-hungry and rely on large-scale datasets with high-quality human annotations for training. However, the process of annotating these datasets can be expensive and time-consuming, particularly when dense annotation are required, as is often the case in pose estimation tasks (Wang and Zhang 2022; He et al. 2017). To overcome this challenge, AI-aided labeling methods have become increasingly popular, where a pre-trained model‚Äôs prediction serves as a reference to reduce human workload. However, when there is a domain shift (Luo et al. 2019), where the distribution of the training dataset and test dataset are not aligned not only on the input image domain but on the pose annotation domain as well, the accuracy of the model can significantly decline.\n\nConsiderable efforts have been devoted to tackling this issue. Among them, domain adaptation (DA) (Daum¬¥e III\n\nCommon Prior Common Prior Transformation FlexPose Transformation Dataset Annotation 1 ‰ª∑ T Â±±4‰ª∑ Pose-basedPainting Animation Design Weakly Supervision Source Pose Distribution Target Pose Distribution\n\n2009; Csurka 2017) introduces knowledge from existing annotated datasets to a target dataset and is verified effective on several computer vision tasks (Cao et al. 2019; Inoue et al. 2018). However, things become different in the humanrelated dataset, e.g., human pose (Ionescu et al. 2014) and human face (Wu et al. 2018). As the source human appearance is usually required in DA-related methods for input image domain adaptation, they may import unexpected data distribution bias (Buolamwini and Gebru 2018), e.g., gender or color, from the source. Besides, the direct exposure of private portraits may raise the privacy issue.\n\nOn the other hand, it is commonly observed that different human poses share a similar hinge-structure prior. Typically, poses in a target dataset can be transferred from poses of a pre-collected source set by applying geometric transformations on for example pivot orientation, joint rotation, and bone length ratio. Therefore, adapting the pose distribution only can be a viable option in the case of human-related datasets. Pose Domain Adaptation (PDA) avoids the direct use of human appearance images, effectively addressing the aforementioned issues. Motivated by this observation, we propose FlexPose (shown in Figure 1), a method that transfers the source pose distribution to a target distribution with limited pose annotation guidance. After pose distribution transfer by FlexPose, each input image can be matched with the most related generated pose in estimated pose distribution by utilizing a matching algorithm (Jakab et al. 2020) for weakly supervised pose estimation and pose annotation. Besides, the generated poses can also be utilized in plenty of downstream tasks such as pose-conditioned image generation (Zhang and Agrawala 2023).\n\nIn FlexPose, we treat pose annotations as skeleton images to well align the annotations with their RGB appearance correspondences, and to improve the learnability of pose prior as the skeleton images well preserve the spatial structure of joint connection on image plane. We first learn the pose prior and fit the empirical distribution from a source human pose dataset by a multi-layer generative model. Thereafter, specific layers of the generative model are calibrated by inserting learnable lightweight linear modules to transfer the source distribution to the target domain. Considering that only a limited number of poses are given, we introduce three regularizations to avoid the collapse of the transfer solution. By generating credible pose interpolations with Pose-mixup regularization and by strictly limiting the complexity of the transfer module with linear and sparse regularization, we minimize the requirement of sample amount but maximize the sample diversity in FlexPose. FlexPose is computationefficient. It operates on the pose domain, and hence the training convergence is much faster than methods in domain adaptation, which focuses adaptation on both the pose and image domains together. FlexPose is also data-efficient. We only need limited pose annotations from the target dataset to finetune the transfer modules. Extensive experiments on three pose-based tasks, i.e., human pose annotation, human face landmarks annotation, and pose-conditional human image generation, demonstrate that FlexPose outperforms baselines by a large margin both quantitatively and qualitatively. Our contributions can be summarized as follows:\n\n‚Ä¢ We propose to treat the task of Pose Domain Adaptation as the transfer of skeleton image generator and demonstrate that a target pose distribution can be well approximated from a well-learned pose prior.   \n‚Ä¢ We introduce FlexPose, a PDA framework that employs three regularizations to efficiently transfer a pose distribution to a target one by utilizing a limited number of guiding poses with low computation and storage costs.   \n‚Ä¢ Extensive experimental results on three pose-related tasks show that FlexPose achieves remarkable improvement over existing methods.\n\n# 2 Related Works\n\nDeep Generative Model for Image Generation. Deep generative models such as GAN, Variational AutoEncoder (VAE), and Diffusion models achieve great success in realistic/artificial image generating and natural image distribution modeling. Recently proposed generative models such as StyleGAN (Karras, Laine, and Aila 2019), DDPM (Ho, Jain, and Abbeel 2020), NVAE (Vahdat and Kautz 2020) introduce new mechanisms, new architecture, and new regularizations into image generation. VAEs (Kingma and Welling 2014) learn to maximize the variational lower bound of likelihood. Diffusion probabilistic models (Sohl-Dickstein et al. 2015)\n\nsynthesize images by a denoising procedure. GANs (Goodfellow et al. 2014) are trained in an adversarial manner to learn how to generate realistic images. Among them, Karras et al. (Karras, Laine, and Aila 2019) proposed an architecture StyleGAN that can learn a hierarchical decoupled style code and controls image synthesis. Our method is based on generators with multi-layer architecture and leverages StyleGAN as the backbone. We are inspired by the recent works (Zhu et al. 2016; Yin et al. 2022), which manipulate the latent code in the generative model to edit the output images. These works motivate us to transfer the pose distribution to the target domain by transferring style codes with few-shot guidance.\n\nTransfer Learning for Generative Models. The literature on transfer learning has been extensively studied in recent years (Oquab et al. 2014; Long et al. 2015; Ganin and Lempitsky 2015). Transfer learning learns to transfer the knowledge from a large-scale source dataset to a small target dataset to enhance model performance on the target dataset. The methodology of transfer learning is also treated as a pretraining technique. It is utilized to accelerate the learning on the target dataset. (Wang et al. 2018) finetunes a pre-trained GAN on a target dataset to get better performance. (Noguchi and Harada 2019) transfers knowledge from a large dataset to a small dataset by re-computing batch statistics. Existing methods focus on either the image domain or the neural language processing domain (Shin, Hwang, and Sung 2016). For these methods, hundreds of training samples are still required. Compared with these approaches, we focus on pose domain adaptation, and our method only requires few-shot guidance for transferring. After LoRA (Hu et al. 2021) is widely used in Large Language Model finetuning, the researchers in Content Generation are inspired to introduce or extend this technique in generative model (Mou et al. 2024). Compared with the light-weight but global model finetuning in LoRA, FlexPose only focuses on calibrating specific layers with semantics of pose geometric transformation locally to satisfy the linear and sparse finetuning requirements.\n\nHuman Pose Estimation. 2D Human pose estimation is a task that predicts the 2D pose from a single image. Fullysupervised methods (Andriluka, Roth, and Schiele 2009; Bai and Wang 2019; Belagiannis and Zisserman 2017) utilize large-scale annotated datasets such as COCO (Lin et al. 2014), Human3.6M (Ionescu et al. 2014) and 3DHP (Mehta et al. 2017) for model training. Weakly-supervised (Kanazawa et al. 2018; Gecer et al. 2019; Geng, Cao, and Tulyakov 2019; Wang et al. 2023) and unsupervised (Shu et al. 2018; Jakab et al. 2018) methods such as KeypointGAN (Jakab et al. 2020) have been proposed to reduce the dependence on the expensive pose annotation. These methods require supervised post-training or additional prior knowledge to generate meaningful landmarks, which can serve as a distance measurement between the provided prior knowledge and the target distribution. To match poses generated by FlexPose with unlabelled images in the target dataset, we employ an unsupervised method (Jakab et al. 2020) in addition to supervision from adversarial training. This matching procedure serves as an evaluation method for FlexPose and is further detailed in Section 4. Recently, test-time adaptation (Li et al. 2021; Cui et al. 2023; Hu et al. 2024) has proven to be an effective way to deal with domain shift in pose estimation. It utilizes selfsupervised learning during inference to adapt model to the input human appearance distribution. Compared with FlexPose which focuses on PDA, Test-time adaptation tackle the issues in input image domain shift.\n\n![](images/66c74430b9bf91bf47590e2839c5d9f30b917e046525ce0c24d2b0f1258ff26c.jpg)  \nFigure 2: An illustration of the FlexPose framework for pose distribution adaptation. There are three main steps in our framework: $\\textcircled{1}$ We train a skeleton image generator to learn the pose prior from the source pose distribution; $\\textcircled{2}$ The source generator is transferred to a target generator with limited target pose guidance to achieve pose distribution adaptation; $\\textcircled{3}$ We utilize the target generator to generate target pose annotations for downstream tasks.\n\n# 3 Method\n\nGiven a limited number of 2D pose annotations set $T =$ $\\left\\{ \\pmb { y } _ { t } | \\pmb { y } _ { t } \\in \\mathbb { R } ^ { M \\times 2 } \\right\\}$ of a newly collected human pose images, FlexPose aims to estimate the whole distribution $\\mathcal { D } _ { t }$ which pose annotation $T$ belongs to, and generate any number of new pose annotations that follow the distribution. $M$ here is the number of joints in each pose. This task setting is challenging. However, we believe that with the prior from sufficient off-the-shelf annotations $S = \\left\\{ \\pmb { y } _ { s } | \\pmb { y } _ { s } \\overset { * } { \\in } \\mathbb { R } ^ { M \\times 2 } \\right\\}$ , the distribution $\\mathcal { D } _ { t }$ can be estimated and well shaped. In this paper, we transfer the distribution $\\mathcal { D } _ { s }$ estimated from $S$ to the target pose domain to estimate the target distribution $\\mathcal { D } _ { t }$ by considering the guidance from 2D pose annotations set $T$\n\n# 3.1 Overview\n\nAs illustrated in Figure 2, our framework consists of three phases: $\\textcircled{1}$ Generic Pose Distribution Estimation. We learn a generator $g _ { s } ( \\cdot )$ on the pose set $S$ to estimate the pose distribution $\\mathcal { D } _ { s }$ . The generator takes a latent code $z$ as input and outputs a skeleton $\\hat { \\pmb x } _ { s }$ , i.e. $\\hat { \\pmb { x } } _ { s } = g _ { s } ( \\pmb { z } )$ . We take the distribution of generated $\\hat { \\pmb x } _ { s }$ to mimic that of the generic pose $\\pmb { x } _ { s }$ . Here, $\\scriptstyle { \\mathbf { { \\boldsymbol { x } } } }$ is the corresponding skeleton image of an annotation $_ y$ as shown in the left part of Figure 2. $\\textcircled{2}$ Pose Distribution Adaptation. Given the limited target annotation set $T$ , we transfer $g _ { s } ( \\cdot )$ to fit the pose distribution $\\mathcal { D } _ { t }$ and learn a new generator $g _ { t } ( \\cdot )$ of the target pose domain. Considering the limited knowledge acquired from target pose annotation $T$ , we introduce three regularizations, Linear, Sparse and Pose-mixup, to avoid reaching a collapse solution. $\\textcircled{3}$ Target\n\nPose Sampling. The transferred generator $g _ { t } ( \\cdot )$ can flexibly synthesize any number of fake pose annotations by randomly sampling in the latent space. This generated annotation set $\\hat { T }$ will be treated as an extension of given annotations set $T$ in the downstream tasks, e.g., Keypoints Annotation and Poseconditional Human Image Generation, since poses within both of them follow the distribution $\\mathcal { D } _ { t }$ .\n\n# 3.2 Generic Pose Distribution Estimation\n\nDeep generative models have been widely verified that they have a rich capacity to well approximate image distributions when given sufficient training data. Motivated by the success of these generative models (Karras, Laine, and Aila 2019) on natural/artificial image generation, we treat 2D pose annotations ${ \\pmb y } _ { s } , { \\pmb y } _ { t } \\in \\mathbb { R } ^ { M \\times 2 }$ as skeleton images $\\pmb { x } _ { s } , \\hat { \\pmb { x } } _ { t } \\in \\mathbb { R } ^ { C \\times W \\times H }$ and ext nd an image generator to generate 2D pose annotations by synthesizing corresponding skeleton images. As shown in the left part of Figure 2, the transformation from the 2D keypoints to the skeleton images can be implemented by functions $\\alpha ( \\cdot )$ , namely $\\pmb { x } = \\alpha ( \\pmb { y } )$ , where $\\alpha ( \\cdot )$ simply draws keypoints from $_ y$ and connects them with straight lines on a blank figure. The visual effect is similar to the stick man. To achieve precise semantic alignment with the appearance correspondence, each bone in the skeleton image is assigned a unique color. Therefore, $C$ of each skeleton image is set as three (RGB channels). Compared with Black&White, the colorful embedding brings marginal improvement in the quality of generated skeletons.\n\nA generator can be formulated as a mapping function $g ( \\cdot )$ which gets a latent code $z$ and outputs a skeleton image $\\scriptstyle { \\mathbf { { \\boldsymbol { x } } } }$ . The probability distribution of skeleton images hence is estimated by $p ( { \\pmb x } ) = p ( { \\pmb z } ) p _ { g } ( { \\pmb x } | { \\pmb z } )$ . We assume that the pose distributions of different datasets share similar pose prior, and their distributions can transfer to one another by geometric transformations. Based on this assumption, we further factorize the generator $g ( \\cdot )$ as $g = \\phi \\circ \\delta$ . Therefore, the source skeleton image generator can be formulated as\n\n![](images/59bf46d7910ba91212b7a8cfc0e88520c3ffaea691993e3cc6cfec0d668ce815.jpg)  \nFigure 3: An illustration of the generator decomposition. We use $\\tau ( \\cdot )$ to adjust the source generator for pose distribution adaptation.\n\n$$\np ( \\hat { \\pmb x } _ { s } ) = p ( z ) p _ { g } ^ { s } ( \\hat { \\pmb x } _ { s } | z ) = p ( z ) p _ { \\delta } ^ { s } ( \\pmb { h } _ { s } | z ) p _ { \\phi } ( \\hat { \\pmb x } _ { s } | \\pmb { h } _ { s } ) ,\n$$\n\nin which $\\phi ( \\cdot )$ preserves the learned pose prior and $\\delta ( \\cdot )$ records the mapping from the learned prior $\\boldsymbol { h }$ to the skeleton image $\\pmb { x } _ { s }$ of a certain pose domain. Similarly, the distribution of target domain can be formulated as $p ( \\hat { \\pmb x } _ { t } ) =$ $p ( z ) ~ p _ { \\delta } ^ { t } ( h _ { t } | z ) ~ \\overline { { p _ { \\phi } ( \\hat { x } _ { t } | h _ { t } ) } }$ . With the prior sharing assumption, the pose distribution adaptation aims at transferring pretrained conditional probability $p _ { \\delta } ^ { s } ( h _ { s } | z )$ to $p _ { \\delta } ^ { t } ( h _ { t } | z )$ with guidance from the pose annotation set $T$ :\n\n$$\n\\begin{array} { r } { p _ { \\delta } ^ { s } ( \\pmb { h } _ { s } | z ) \\overset { T } { \\longrightarrow } p _ { \\delta } ^ { t } ( \\pmb { h } _ { t } | z ) . } \\end{array}\n$$\n\nConsidering the ability of StyleGAN in separating highlevel attributes and in the interpolation between these attributes, we utilize StyleGAN network architecture to disentangle the pose prior and the transformation of the source skeleton image generator,\n\n$$\ng _ { s } = \\phi \\circ \\delta _ { s } = \\phi \\circ ( A \\circ f ) _ { s } ,\n$$\n\nwhere $f ( \\cdot )$ is a non-linear mapping that takes random noise as input and outputs a random vector. $A ( \\cdot )$ is a learned affine transformation and can be treated as a block diagonal matrix with $L$ blocks, where $L$ is the number of layers. The output of $A ( \\cdot )$ is the style code to modulate the synthesis network $\\phi ( \\cdot )$ by adaptive instance normalization. Due to the ability of StyleGAN in style control, we can directly adapt the distribution of source skeleton image to the target domain by adjusting the style code.\n\n# 3.3 Pose Distribution Adaptation\n\nAs illustrated in Figure 3, to transfer $p _ { \\delta } ^ { s } ( h _ { s } | z )$ to $p _ { \\delta } ^ { t } ( h _ { t } | z )$ , we adjust the style code by introducing a transfer function $\\tau ( \\cdot )$ at the output of $\\delta ( \\cdot )$ , and therefore the target domain generator is defined as\n\n$$\ng _ { t } = \\phi \\circ \\delta _ { t } = \\phi \\circ ( \\tau \\circ \\delta _ { s } ) .\n$$\n\nTo learn the transfer function $\\tau$ , we first randomly sample $| T |$ latent codes $z$ (one for each pose in $T$ ) from the latent\n\nspace, and require the generator maps each code to corresponding skeletons in $T$ . This transferring procedure can be achieved by minimizing the following perceptual loss,\n\n$$\n\\operatorname* { m i n } _ { \\theta _ { \\tau } } \\mathcal { L } _ { s  t } = \\operatorname* { m i n } _ { \\theta _ { \\tau } } \\sum \\| \\Gamma \\big ( g _ { t } ( z ) ; \\theta _ { \\tau } \\big ) - \\Gamma \\big ( \\pmb { x } \\big ) \\| _ { 2 } ^ { 2 } ,\n$$\n\nwhere $\\theta _ { \\tau }$ is the parameter of $\\tau ( \\cdot )$ , $\\Gamma$ is a pre-trained feature extractor, $z$ is from the set of sampled latent codes, and $\\scriptstyle { \\mathbf { { \\boldsymbol { x } } } }$ is the skeleton image drawn from the pose annotation set $T$ .\n\nHowever, the problem is we only have few-shot guidance $T$ from the target domain distribution. Given a data-starving deep learning model, the guidance is insufficient to reach a satisfactory solution. For that reason, we introduce three regularizations to alleviate the data-insufficient issue.\n\nLinear $\\&$ Sparse Regularization. Compared with finetuning the whole transformation function $\\delta _ { s }$ to reach $\\delta _ { t }$ , only adjusting the affine transformation from $A _ { s }$ to $A _ { t }$ , i.e. $A _ { t } = \\tau \\circ A _ { s }$ , can efficiently shrink the searching space of transfer solution, and therefore avoid overfitting. Meanwhile, the recent GAN inversion technique shows that the layer-wise style code in StyleGAN leads to the hierarchical disentanglement of local and global attributes, which aligns well with our motivation of adapting pose distribution by considering the global geometric transformation between poses. We thus adjust the source affine transformation $A _ { s }$ from the perspective of layer level, and limit the number of to-be-adjusted layers as small as possible. Considering the form of the affine transformation $A$ and the layer decoupling characteristics of StyleGAN, we empirically define the transfer function $\\tau ( \\cdot )$ as a block diagonal matrix,\n\n$$\n\\tau \\triangleq \\operatorname { d i a g } ( I , . . . , I , U _ { l _ { 0 } } , I , . . . , I , U _ { l _ { 1 } } , I , . . . , I ) ,\n$$\n\nwhere only a limited number of block is defined by $\\pmb { U }$ , i.e. $l _ { 0 }$ and $l _ { 1 }$ in this case, to follow the sparse regularization. We experimentally find that the earlier layers are most related to the geometric transformation. And we only learn those layers in our experiments. Meanwhile, other blocks are set as identity matrix $\\boldsymbol { \\mathit { I } }$ . We investigated how the choice of layer $l$ affects the transformation procedure in Section 4.4.\n\nPose-mixup Regularization. Most poses interpolated between two real poses physically exist, and their convex combinations build the real-world pose distribution. Inspired by the mixup regularization (Zhang et al. 2017) on images, we therefore extend it to 2D pose annotations and propose the Pose-mixup to enrich the guidance set. The main difference between mixup and Pose-mixup is that the mixup works on image space and the Pose-mixup works on keypoint space. Given that the mixup on skeleton space may lead to unreasonable results, Pose-mixup regularizes the neural network to learn the simple linear behavior in-between 2D poses and thus prevents the model from generating unrealistic human pose annotations. By mixing up the corresponding joints of any two 2D poses with mixup ratio $\\lambda \\in [ 0 , 1 ]$ , the extended annotation set $T ^ { * }$ from $T$ is then defined as,\n\n$$\nT ^ { * } = \\left\\{ y ^ { * } \\mid y ^ { * } = \\lambda y _ { i } + \\left( 1 - \\lambda \\right) y _ { j } , \\ y _ { i } , y _ { j } \\in T \\right\\} .\n$$\n\n![](images/5b059c85c918ec28c25589f8e081208427b568111640c2c00380d716f8cd1aee.jpg)  \nFigure 4: Illustration of pose distribution transformation. (a) Reconstruction loss with different choice of layer l. (b) Visualization of pose adaptation. The left and middle of each row are generated from the same random noise. The middle aims to mimic the right. (c) t-SNE visualization of human poses before and after adaptation. We visualize the pose distribution $( \\mathbb { R } ^ { M \\times 2 } )$ in a two-dimensional space.\n\nTable 1: The results of distribution distance measurement.   \n\n<html><body><table><tr><td>Method</td><td>Target</td><td>Source</td><td>MMD¬≤ (‚Üì)</td><td>FD (‚Üì)</td></tr><tr><td>FreezeD</td><td rowspan=\"3\">H3.6M</td><td>COCO</td><td>0.081</td><td>3.77</td></tr><tr><td>AdaGAN</td><td>COCO</td><td>0.052</td><td>2.67</td></tr><tr><td>LoRA FlexPose</td><td>COCO COCO</td><td>0.035 0.029</td><td>1.36 0.80</td></tr></table></body></html>\n\n# 3.4 Target Pose Sampling\n\nOnce the transferred generator $g _ { t } ( \\cdot )$ is obtained, we can generate theoretically as many target skeleton images $\\tilde { \\pmb { x } } _ { t }$ as possible by randomly sampling latent codes in the estimated target distribution $\\mathcal { D } _ { t }$ . Unfortunately, the generated target skeleton images are not perfect and may bring in artifacts which mislead the training of a neural network. To address this issue, we utilize $\\beta ( \\cdot )$ to filter out the random noise. $\\beta ( \\cdot )$ is a neural network regressor pre-trained on $S$ and acts as a tight information bottleneck that preserves skeleton information and ignores random noise. Following the generation of the fake skeleton images $\\tilde { \\pmb { x } } _ { t }$ from $g _ { t } ( \\cdot )$ , we extract the coordinates of interpretable 2D keypoints $\\hat { T } = \\{ \\hat { y } _ { t } \\}$ from it, e.g., hands, by applying $\\hat { \\pmb y } _ { t } = \\beta ( \\tilde { \\pmb x } _ { t } )$ . Thereafter, we can get a clean generated skeleton $\\hat { \\pmb x } _ { t }$ by a re-render process,\n\n$$\n\\hat { \\pmb { x } } _ { t } = \\alpha ( \\hat { \\pmb { y } } _ { t } ) = \\alpha \\big ( \\beta ( \\tilde { \\pmb { x } } _ { t } ) \\big ) .\n$$\n\nThese generated skeleton images and the corresponding generated 2D pose annotations can be further utilized to assist any pose-related down-stream tasks.\n\n# 4 Experiments\n\nIn this section, we first evaluate the distribution similarity between transferred distribution and target distribution via two standard metrics. Then, we show how FlexPose can improve the performance of existing unsupervised landmark detection algorithms and benefit unlabelled human pose dataset annotation. At last, we extensively discussed how each part of FlexPose works.\n\n# 4.1 Pose Distribution Transformation\n\nIn this subsection, we conduct a transformation experiment between COCO (Lin et al. 2014) and Human3.6M (Ionescu et al. 2014) to show how FlexPose works.\n\nExperiment Setting. We train a StyleGAN (Karras, Laine, and Aila 2019) using the skeleton images from the source datasets to estimate the distribution of source human pose. And then we transform the estimated distribution to the target one according to several samples from target dataset. For source dataset COCO, we only keep the annotated people instances with full pose annotations to construct a training set of 32k samples. The training of StyleGAN follows standard protocol in the original work. In the transformation phase, we only use 30 annotations from the target dataset Human3.6M (two for each class). The size of interpolated pose set $( | T ^ { * } | )$ is set as 1000. We experimented with changing different layers and found that setting $\\scriptstyle { l = 3 }$ , i.e., transferring the third coarsest layer, usually gets the lowest reconstruction loss in Equation (5) as shown in Figure 4a. So, we set $\\scriptstyle { l = 3 }$ in all experiments. A detailed setting and deeper analysis can be found in appendix. When adaptation phase ends, we sample new poses from generator and treat them as Adapted COCO. Evaluation $\\&$ Visualization. Qualitatively, we show the visual result of pose transformation in Figure 4b. For each row, we show one skeleton (Left) that was randomly sampled from the generator before transformation, one skeleton (Middle) that was sampled from the transformed generator by using the same latent noise as the Left, and one skeleton (Right) in the few-shot annotation set $T$ from target dataset. We can see that the Left and the Middle generated from the same random noise are visually quite different, and the Middle is more similar to the Right.\n\nIn Figure 4c, we plot the t-SNE embedding of the poses generated by FlexPose (Adapted COCO), comparing it with the embedding of poses from the source (COCO) and target (Human3.6M) dataset. As can be seen, the embedding of poses from the source and target dataset are separated, and the distribution of generated poses significantly overlaps with the target ones. We also noted that the pose distributions are ‚Äòmismatched‚Äô in the upper right region. Considering that only two shots are utilized as guidance for each class during the transformation, such a mismatch is reasonable.\n\nTable 2: Results on human pose annotation task. S-H3.6M and $_ { \\mathrm { H 3 . 6 M } }$ are short for Simplified Human3.6M dataset and Human3.6M dataset respectively. The threshold of PCK is $10 \\%$ for $\\mathrm { S - H } 3 . 6 \\mathrm { M }$ and $20 \\%$ for $_ { \\mathrm { H 3 . 6 M } }$ in this table.   \n\n<html><body><table><tr><td>Method</td><td>Target</td><td>Source</td><td>MSE(‚ÜìÔºâPCK(‚Üë)</td></tr><tr><td>Baseline FreezeD AdaGAN LoRA FlexPose</td><td>H3.6M</td><td>COCO 17.86 COCo 20.60 COCO 14.88 COCO 13.85 COCO 13.19</td><td>0.015 0.081 0.395 0.430 0.585 0.685</td></tr><tr><td>Baseline FreezeD AdaGAN LoRA FlexPose Baseline</td><td>COCO COCo COCo COCo COCO 3DHP</td><td>5.47 7.63 5.36 5.02 3.79 12.66</td><td>0.003 0.455 0.512 0.770 0.000</td></tr><tr><td>AdaGAN FreezeD LoRA FlexPose Baseline</td><td>S-H3.6M</td><td>3DHP 7.23 3DHP 6.28 3DHP 6.15 3DHP 5.98 SURREAL</td><td>0.215 0.206 0.314 0.467 0.000</td></tr><tr><td>FreezeD AdaGAN LoRA FlexPose</td><td></td><td>11.18 SURREAL 11.38 SURREAL 6.63 SURREAL 6.52 SURREAL 6.47</td><td>0.006 0.228 0.337 0.499</td></tr></table></body></html>\n\nQuantitatively, we measure the similarity between the transferred distribution and target distribution using the Fr¬¥echet distance (FD), which follows from the Wassersteinbased definition of FID (Heusel et al. 2017) without the application of the pre-trained Inception network. We also measure the square of Maximum Mean Discrepancy (MMD) to provide more insights. The measurements are conducted on the keypoint coordinates space.\n\nWe compare our method with three strong competitors. AdaGAN (Noguchi and Harada 2019), FreezeD (Mo, Cho, and Shin 2020) and LoRA (Hu et al. 2021) (rank $r { = } 8$ ). Both AdaGAN and FreezeD suggest finetuning-based strategies with regularization. LoRA is the most related work to our FlexPose, introducing low-rank regularization to the generative model. For all methods, we generate $5 0 \\mathrm { k }$ samples from the transferred generative model and compare them with all samples in the target dataset Human3.6M. In Table 1, experiment results suggest that FlexPose gives superior performance on both MMD and FD evaluation, indicating the transferred distribution shares more similar characteristics to the target distribution. The finding remains the same as that of the observation on qualitative evaluation.\n\n# 4.2 Unlabelled Human Pose Dataset Annotation\n\nTo further evaluate the quality of transformed pose quantitatively and show potential downstream application of FlexPose, we show how can we annotated unlabelled humanrelated dataset with the help of FlexPose.\n\nTable 3: Results on human face annotation task.   \n\n<html><body><table><tr><td>Method</td><td>Target</td><td>Source</td><td>MSE (‚Üì)</td><td>PCK (‚Üë)</td></tr><tr><td>Baseline</td><td></td><td>300-VW</td><td>18.78</td><td>0.679</td></tr><tr><td>AdaGAN FreezeD</td><td></td><td>300-VW</td><td>11.95</td><td>0.785</td></tr><tr><td></td><td>WFLW</td><td>300-VW</td><td>11.66</td><td>0.779</td></tr><tr><td>LoRA</td><td></td><td>300-VW</td><td>11.77</td><td>0.760</td></tr><tr><td>FlexPose</td><td></td><td>300-VW</td><td>11.64</td><td>0.766</td></tr></table></body></html>\n\n![](images/13aa9a07e7a5e018021bcc46612b763f7bfc29e45193abe1bb1c95b80009a26a.jpg)  \nFigure 5: Visualization of human face landmark annotation on WFLW dataset. Upper Row: landmarks given by matching algorithm. Bottom Row: landmarks in the upper row with their corresponding human faces.\n\nPose-Image Matching Algorithm. In dataset annotation tasks, our goal is to assign each image in the target dataset to the most closely related pose in the estimated target distribution $\\mathcal { D } _ { t }$ . Existing self-supervised human pose detection methods (Jakab et al. 2018; Lorenz et al. 2019; Thewlis, Bilen, and Vedaldi 2017) are usually constrained by the high relevance between model prediction and input image. Among them, KeypointGAN (Jakab et al. 2020) can match unpaired images and annotations by forcing the distribution of detector predictions to align with the existing poses. We train KeypointGAN by using human images from target dataset and generated pose set $\\hat { T }$ . Once the training process is completed, the model prediction on samples can be treated as the best-matched annotations in given distribution.\n\nSource Datasets. Apart from COCO, we also use MPI-INF3DHP (3DHP) (Mehta et al. 2017) which contains more than 1.8 million human pose annotations from eight subjects and covers eight complex exercise activities. SURREAL (Varol et al. 2017) is a synthetic dataset containing more than six million frames of people in motion.\n\nTarget Datasets. The large-scale dataset Human3.6M (Ionescu et al. 2014) has 3.6 million samples. The Simplified Human3.6M dataset (Zhang et al. 2018) contains 800k training and around 90k testing images. We use all human images in the target dataset and randomly select several guide poses. Evaluation Metrics. Since dataset annotation shares similar targets with 2D landmark detection. We report 2D landmark detection performance for evaluation. Two standard evaluation metrics are considered to compare our method with baselines. The MSE column reports a mean square error in pixels overall pre-defined common joints. The Percentage of Correct Key-points $( \\mathrm { P C K } { - \\rho } )$ is used as an accuracy metric that measures if the distance between the predicted keypoint and the true joint is within a certain threshold $\\rho$ .\n\nPerformance Comparisons. We feed the generated skeleton images $\\hat { \\pmb x } _ { t }$ and RGB human images from target dataset into KeypointGAN to evaluate the effectiveness of each pose transformation algorithm. As a baseline, we train the detector on each target dataset by directly using the pose annotations set $S$ from the source dataset, which we denote as Baseline in the comparison and can be roughly treated as the worst case. We also employ three strong competitors, AdaGAN, FreezeD, and LoRA, for comparison.\n\nTable 4: Ablation study on human pose annotation. The target dataset is Simplified-Human3.6M for all experiments. C, D, S are short for COCO, 3DHP, SURREAL dataset.   \n\n<html><body><table><tr><td># Source Layer Mixup</td><td>Linear Shots MSE PCK</td></tr><tr><td>1 C 3</td><td>12 3.79 0.77 0.78</td></tr><tr><td>C</td><td>‚àö ‚àö 1,3 ‚àö ‚àö</td></tr><tr><td>2 3 C 3,5 ‚àö</td><td>12 3.82 12 4.02 ‚àö</td></tr><tr><td>4 C ALL ‚àö</td><td>0.61 ‚àö 12 4.50 0.66</td></tr><tr><td>5 3</td><td>12 5.98 0.44</td></tr><tr><td>D 6 D ALL √ó ‚àö</td><td>‚àö ‚àö 12 9.82 0.01</td></tr><tr><td>7 D ALL</td><td>√ó √ó 12 12.32 0.00</td></tr><tr><td>8 C 3</td><td>‚àö ‚àö 12 3.79 0.77</td></tr><tr><td>9 C 3</td><td>‚àö ‚àö 24 3.80 0.75</td></tr><tr><td>10 C 3 ‚àö ‚àö</td><td>48 3.73 0.70</td></tr><tr><td>11 D 3 ‚àö</td><td>‚àö 12 5.98 0.47</td></tr><tr><td>12 DC 3 ‚àö</td><td>12 5.28 0.59 ‚àö 12 5.19 0.59</td></tr><tr><td>13 DCS 3 ‚àö</td><td></td></tr></table></body></html>\n\nQuantitatively, we compare their performance with FlexPose on human pose estimation in Table 2. As shown in Table 2, the Baseline has much lower performance on the target dataset as the pose annotations are from different datasets, especially when some of them have a distinct pose distribution from that of the target dataset, e.g., when 3DHP or SURREAL is the source dataset and Simplified Human3.6M is the target one. FlexPose gets better results on all settings under both metrics. FlexPose largely reduces the performance gap when the pose distribution of the source dataset is very different from that of the target distribution, e.g., MSE $1 2 . 7 \\substack {  6 . 0 }$ and PCK10 $0 . 0 0 {  } 0 . 4 7$ when adaptation occurs from 3DHP to Simplified Human3.6M. The results show that FlexPose is effective at generating similar poses with the target dataset, even with less to only two poses per class in the target dataset.\n\n# 4.3 Unlabelled Human Face Dataset Annotation\n\nIntroducing FlexPose to human face landmarks transfer is straightforward since both the human pose and the human face consist of a set of pre-determined keypoints.\n\nDatasets. WFLW (Wu et al. 2018) has 10 thousand samples with 98 facial landmarks, where 7.5 thousand for training and 2.5 thousand for testing. 300-VW (Sagonas et al. 2013) consists of 300 Videos in the wild and contains ${ \\sim } 9 5$ thousand annotated human faces in the training set. We treat 300-VW as the source dataset and only use its annotations for training StyleGAN. And few-shot annotations in the target dataset WFLW are utilized for transformation. We only keep the shared 68 facial landmarks in two datasets.\n\nExperiments Settings and Results. The evaluation metrics and the experiment protocols are the same as that in the human pose. The size of the few-shot guidance set from target dataset is set as 30. We report the evaluation results on the validation set of WFLW in Table 3. FlexPose still outperforms the baseline by a large margin (MSE $1 8 . 7 8 $ 11.64 and PCK $0 . 6 7 9  0 . 7 6 6 )$ . Given that the human face can be treated as a rigid body approximately and are easier to transfer, FlexPose achieves comparable performance with previous SOTA methods, AdaGAN, FreezeD and LoRA.\n\nWe show the detected human face landmarks in Figure 5. The human face detector trained with generated face landmarks can handle human faces in different directions well.\n\n# 4.4 Ablation Study & Parameter Sensitivity\n\nIn Table 4, ablation studies are conducted on:\n\nEffect of Regularization. We remove part of proposed regularization from our FlexPose, and the results are #1-#7. From #1-#4, we gradually relax the sparsity regularization by allowing more blocks in the diagonal matrix $\\tau$ not to be an identity matrix $\\boldsymbol { \\mathit { I } }$ . The performance only drops by an acceptable level thanks to the Linear and Mixup regularization. Furthermore, in $\\# 5 . \\# 7$ , we further relax the Mixup and Linear regularization, which significantly hurt the quality of generated images and lower the model accuracy in downstream tasks.\n\nNumber of Shots from Target Dataset. Under the setting of $\\mathrm { C O C O } { \\to } \\mathrm { S } { \\cdot } \\mathrm { H } 3 . 6 \\mathrm { M }$ , we increase the number of shots from 12 to 48 and found that the performance of the pose detector has no obvious difference. The results can be found in #8-#10. An explanation is that the increment of few-shot samples from the target dataset brings a limited gain of information compared with the strong prior trained on large-scale datasets. Few samples are enough for target distribution localization. Choice of Layers l. In previous experiments, we empirically choose $\\scriptstyle { l = 3 }$ in Equation (4) for all experiments. We found that the choice of $l$ is not strictly fixed. We have also tried a composition of multi-layer, and the results can be found in $\\# 2 \\# 4$ . The result in $\\# 4$ shows the necessity of sparse regularization. We leave the best choice of $l$ to future work. Multi-source Datasets. To study the effect of the setting where the source annotations are from different datasets, we use the union of different source datasets to train the generic generator. The result indicates that the increasing diversity on the source dataset $\\# 1 1 \\to \\# 1 2 \\to \\# 1 3$ ) brings better results on the target dataset. By utilizing FlexPose, the performance of downstream task models can benefit from collecting a more diverse pose dataset, which is much easier compared with collecting a realistic human dataset with accurate landmarks. However, the result of $\\# 1 3$ is still worse than that of #1, which indicates the trade-off between diversity and similarity to the target dataset when choosing the source.\n\n# 5 Conclusion\n\nWe aim to transfer knowledge in the pose domain and propose an effective method named FlexPose. Our approach allows us to adapt an existing pose distribution to a different target one by using a few poses from the target dataset and generating theoretically infinite poses following the target distribution. FlexPose can be used on several pose-related works. In future work, we hope to extend our method to a more generic pose domain adaptation approach.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáËß£ÂÜ≥ÁöÑÊ†∏ÂøÉÈóÆÈ¢òÊòØÂ¶Ç‰ΩïÂú®Êñ∞Êî∂ÈõÜÁöÑÂõæÂÉèÊï∞ÊçÆ‰∏äÈ´òÊïà‰∏î‰ΩéÊàêÊú¨Âú∞Ê†áÊ≥®‰∫∫‰ΩìÂßøÊÄÅÔºåÂÖãÊúçÁé∞ÊúâÊñπÊ≥ïÂú®Ë∑®Êï∞ÊçÆÈõÜÂßøÂäøÂàÜÂ∏ÉÈÄÇÂ∫îÊó∂ÁöÑÈ´òÊ†áÊ≥®ÊàêÊú¨ÂíåÈöêÁßÅÊ≥ÑÈú≤È£éÈô©„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éÔºåÈ´òË¥®ÈáèÁöÑÂßøÊÄÅÊ†áÊ≥®ÂØπ‰∫éËÆ≠ÁªÉÊ∑±Â∫¶Á•ûÁªèÁΩëÁªúËá≥ÂÖ≥ÈáçË¶ÅÔºåÂ∞§ÂÖ∂Âú®ÂßøÊÄÅ‰º∞ËÆ°„ÄÅ‰∫∫ËÑ∏ÂÖ≥ÈîÆÁÇπÊ†áÊ≥®Á≠â‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÂÖ∑ÊúâÂÖ≥ÈîÆ‰ª∑ÂÄº„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ÊèêÂá∫FlexPoseÊ°ÜÊû∂ÔºåÈÄöËøáË∞ÉÊï¥È¢ÑËÆ≠ÁªÉÁîüÊàêÂô®ÁöÑÈ£éÊ†º‰ª£Á†ÅÔºàstyle codeÔºâÔºåÂ∞ÜÊ∫êÂßøÂäøÂàÜÂ∏ÉËΩ¨ÁßªÂà∞ÁõÆÊ†áÂàÜÂ∏ÉÔºå‰ªÖÈúÄÂ∞ëÈáèÁõÆÊ†áÂßøÂäøÊ†áÊ≥®‰Ωú‰∏∫ÊåáÂØº„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **Ë¥°ÁåÆ1Ôºö** Â∞ÜÂßøÊÄÅÂüüÈÄÇÂ∫î‰ªªÂä°Âª∫Ê®°‰∏∫È™®Êû∂ÂõæÂÉèÁîüÊàêÂô®ÁöÑËΩ¨ÁßªÔºåËØÅÊòéÁõÆÊ†áÂßøÊÄÅÂàÜÂ∏ÉÂèØ‰ª•‰ªéÂ≠¶‰π†Âà∞ÁöÑÂßøÊÄÅÂÖàÈ™å‰∏≠Ëøë‰ººÂæóÂà∞„ÄÇ\\n> *   **Ë¥°ÁåÆ2Ôºö** ÂºïÂÖ•‰∏âÁßçÊ≠£ÂàôÂåñÔºàÁ∫øÊÄß„ÄÅÁ®ÄÁñèÂíåÂßøÊÄÅÊ∑∑ÂêàÔºâ‰ª•È´òÊïàËΩ¨ÁßªÂßøÊÄÅÂàÜÂ∏ÉÔºåËÆ°ÁÆóÂíåÂ≠òÂÇ®ÊàêÊú¨‰Ωé„ÄÇ\\n> *   **Ë¥°ÁåÆ3Ôºö** Âú®Ë∑®Êï∞ÊçÆÈõÜËÆæÁΩÆ‰∏ãÔºåFlexPoseÂú®ÁîüÊàêÊ®°ÂûãÂü∫Á°ÄÁöÑËøÅÁßªÂ≠¶‰π†ÊñπÊ≥ï‰∏≠ËææÂà∞ÊúÄÂÖàËøõÊÄßËÉΩÔºåMMD¬≤ÊåáÊ†á‰ªéÂü∫Á∫øÁöÑ0.081ÈôçËá≥0.029ÔºåFDÊåáÊ†á‰ªé3.77ÈôçËá≥0.80„ÄÇ\\n> *   **Ë¥°ÁåÆ4Ôºö** Âú®Human3.6MÊï∞ÊçÆÈõÜ‰∏äÁöÑPCK@20ËææÂà∞0.685ÔºåÊØîÂü∫Á∫øÔºà0.015ÔºâÊèêÂçá45.7‰∏™ÁôæÂàÜÁÇπ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   Ê†∏ÂøÉÂéüÁêÜÔºö‰∏çÂêåÊï∞ÊçÆÈõÜÁöÑÂßøÊÄÅÂàÜÂ∏ÉÂÖ±‰∫´Áõ∏‰ººÁöÑÈì∞ÈìæÁªìÊûÑÂÖàÈ™åÔºåÂÖ∂Â∑ÆÂºÇÊú¨Ë¥®‰∏äÊòØÂá†‰ΩïÂèòÊç¢ÔºàÂ¶ÇÊû¢ËΩ¥ÊñπÂêë„ÄÅÂÖ≥ËäÇÊóãËΩ¨„ÄÅÈ™®È™ºÈïøÂ∫¶ÊØîÔºâÂàÜÂ∏ÉÁöÑÂ∑ÆÂºÇ„ÄÇÈÄöËøáË∞ÉÊï¥ÁîüÊàêÂô®ÁöÑÈ£éÊ†º‰ª£Á†ÅÔºåÂèØ‰ª•Ê†°ÂáÜËøô‰∫õÂèòÊç¢ÂàÜÂ∏É„ÄÇ\\n> *   ËÆæËÆ°Âì≤Â≠¶ÔºöÂ∞ÜÂßøÊÄÅÊ†áÊ≥®ËßÜ‰∏∫È™®Êû∂ÂõæÂÉèÔºåÂà©Áî®StyleGANÁöÑÂ±ÇÊ¨°Ëß£ËÄ¶ÁâπÊÄßÔºå‰ªÖÂæÆË∞É‰∏éÂá†‰ΩïÂèòÊç¢Áõ∏ÂÖ≥ÁöÑÁâπÂÆöÂ±ÇÔºåÂÆûÁé∞È´òÊïàÂàÜÂ∏ÉÈÄÇÂ∫î„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **ÂÖàÂâçÂ∑•‰ΩúÂ±ÄÈôêÔºö** ‰º†ÁªüÂüüÈÄÇÂ∫îÊñπÊ≥ïÈúÄË¶ÅÊ∫êÂüü‰∫∫Á±ªÂ§ñËßÇÂõæÂÉèÔºåÂèØËÉΩÂºïÂÖ•Êï∞ÊçÆÂàÜÂ∏ÉÂÅèÂ∑ÆÊàñÈöêÁßÅÈóÆÈ¢ò„ÄÇ\\n> *   **Êú¨ÊñáÊîπËøõÔºö** 1) ‰ªÖÊìç‰ΩúÂßøÊÄÅÂüüÔºåÈÅøÂÖçÂ§ñËßÇÂõæÂÉè‰ΩøÁî®Ôºõ2) ÈÄöËøáÁ®ÄÁñèÁ∫øÊÄßÊ®°ÂùóË∞ÉÊï¥ÁâπÂÆöÂ±ÇÔºåÂáèÂ∞ëËøáÊãüÂêàÔºõ3) ÂºïÂÖ•ÂßøÊÄÅÊ∑∑ÂêàÊ≠£ÂàôÂåñÂ¢ûÂº∫Ê†∑Êú¨Â§öÊ†∑ÊÄß„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.  **ÈÄöÁî®ÂßøÊÄÅÂàÜÂ∏É‰º∞ËÆ°Ôºö** ‰ΩøÁî®Ê∫êÊï∞ÊçÆÈõÜËÆ≠ÁªÉStyleGANÁîüÊàêÈ™®Êû∂ÂõæÂÉèÔºåÂ≠¶‰π†ÂßøÊÄÅÂÖàÈ™å„ÄÇ\\n> 2.  **ÂßøÊÄÅÂàÜÂ∏ÉÈÄÇÂ∫îÔºö** ÈÄöËøáÊèíÂÖ•ÂèØÂ≠¶‰π†ÁöÑËΩªÈáèÁ∫ßÁ∫øÊÄßÊ®°ÂùóÔºàœÑÂáΩÊï∞ÔºâË∞ÉÊï¥È£éÊ†º‰ª£Á†ÅÔºåÊúÄÂ∞èÂåñÁõÆÊ†áÈ™®Êû∂ÂõæÂÉèÁöÑÊÑüÁü•ÊçüÂ§±„ÄÇ\\n> 3.  **ÁõÆÊ†áÂßøÊÄÅÈááÊ†∑Ôºö** ‰ªéÈÄÇÂ∫îÂêéÁöÑÁîüÊàêÂô®‰∏≠ÈöèÊú∫ÈááÊ†∑ÔºåÁîüÊàêÁõÆÊ†áÂßøÊÄÅÊ†áÊ≥®Áî®‰∫é‰∏ãÊ∏∏‰ªªÂä°„ÄÇ\\n> *   **ÂÖ≥ÈîÆÂÖ¨ÂºèÔºö** ÁõÆÊ†áÁîüÊàêÂô®ÂÆö‰πâ‰∏∫ $g_t = \\\\phi \\\\circ \\\\delta_t = \\\\phi \\\\circ (\\\\tau \\\\circ \\\\delta_s)$ÔºåÂÖ∂‰∏≠œÑÊòØÂùóÂØπËßíÁü©ÈòµÔºå‰ªÖË∞ÉÊï¥ÁâπÂÆöÂ±Ç„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   FreezeD„ÄÅAdaGAN„ÄÅLoRA\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®MMD¬≤ÊåáÊ†á‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®Human3.6MÊï∞ÊçÆÈõÜ‰∏äËææÂà∞‰∫Ü **0.029**ÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãFreezeD (0.081) ÂíåAdaGAN (0.052)„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü0.023„ÄÇ\\n> *   **Âú®Fr¬¥echetË∑ùÁ¶ªÔºàFDÔºâ‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÁöÑFD‰∏∫ **0.80**ÔºåËøú‰Ωé‰∫éFreezeD (3.77) ÂíåAdaGAN (2.67)ÔºåË°®ÊòéÁîüÊàêÁöÑÂßøÊÄÅÂàÜÂ∏ÉÊõ¥Êé•ËøëÁõÆÊ†áÂàÜÂ∏É„ÄÇ\\n> *   **Âú®ÂßøÊÄÅÊ†áÊ≥®‰ªªÂä°‰∏äÔºö** ‰ΩøÁî®FlexPoseÁîüÊàêÁöÑÊ†áÊ≥®ËÆ≠ÁªÉÊ®°ÂûãÔºåÂú®Human3.6MÊï∞ÊçÆÈõÜ‰∏äÁöÑPCK@20ËææÂà∞ **0.685**ÔºåÊØîÂü∫Á∫øÔºà0.015ÔºâÊèêÂçá45.7‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®‰∫∫ËÑ∏Ê†áÊ≥®‰ªªÂä°‰∏äÔºö** Âú®WFLWÊï∞ÊçÆÈõÜ‰∏äÔºåFlexPoseÁöÑMSE‰∏∫11.64ÔºåPCK‰∏∫0.766Ôºå‰ºò‰∫éÂü∫Á∫øÔºàMSE 18.78ÔºåPCK 0.679Ôºâ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   ÂßøÊÄÅÂüüÈÄÇÂ∫î (Pose Domain Adaptation, PDA)\\n*   È™®Êû∂ÂõæÂÉèÁîüÊàê (Skeleton Image Generation, N/A)\\n*   È£éÊ†º‰ª£Á†ÅË∞ÉÊï¥ (Style Code Adaptation, N/A)\\n*   Â∞ëÊ†∑Êú¨Â≠¶‰π† (Few-shot Learning, FSL)\\n*   ÁîüÊàêÂØπÊäóÁΩëÁªú (Generative Adversarial Network, GAN)\\n*   Âº±ÁõëÁù£ÂßøÊÄÅ‰º∞ËÆ° (Weakly-supervised Pose Estimation, N/A)\\n*   Âá†‰ΩïÂèòÊç¢ (Geometric Transformation, N/A)\\n*   Ê≠£ÂàôÂåñ (Regularization, N/A)\"\n}\n```"
}