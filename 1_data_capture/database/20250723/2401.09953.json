{
    "source": "Semantic Scholar",
    "arxiv_id": "2401.09953",
    "link": "https://arxiv.org/abs/2401.09953",
    "pdf_link": "https://arxiv.org/pdf/2401.09953.pdf",
    "title": "Through the Dual-Prism: A Spectral Perspective on Graph Data Augmentation for Graph Classification",
    "authors": [
        "Yutong Xia",
        "Runpeng Yu",
        "Yuxuan Liang",
        "Xavier Bresson",
        "Xinchao Wang",
        "Roger Zimmermann"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2024-01-18",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 4,
    "influential_citation_count": 0,
    "institutions": [
        "National University of Singapore",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ],
    "paper_content": "# Through the Dual-Prism: A Spectral Perspective on Graph Data Augmentation for Graph Classifications\n\nYutong $\\mathbf { X _ { i a } ^ { * } }$ , Runpeng $\\mathbf { Y } \\mathbf { u } ^ { 1 \\ast }$ , Yuxuan Liang2â€ , Xavier Bresson1, Xinchao Wang1â€ , Roger Zimmermann1\n\n1National University of Singapore 2The Hong Kong University of Science and Technology (Guangzhou) {yutong.xia,r.yu}@u.nus.edu; yuxliang $@$ outlook.com; $\\{$ {xaviercs,xinchao,dcsrz}@nus.edu.sg\n\n# Abstract\n\nGraph Neural Networks have become the preferred tool to process graph data, with their efficacy being boosted through graph data augmentation techniques. Despite the evolution of augmentation methods, issues like graph property distortions and restricted structural changes persist. This leads to the question: Is it possible to develop more property-conserving and structure-sensitive augmentation methods? Through a spectral lens, we investigate the interplay between graph properties, their augmentation, and their spectral behavior, and observe that keeping the low-frequency eigenvalues unchanged can preserve the critical properties at a large scale when generating augmented graphs. These observations inform our introduction of the Dual-Prism (DP) augmentation methods, including DP-Noise and DP-Mask, which retain essential graph properties while diversifying augmented graphs. Extensive experiments validate the efficiency of our approach, providing a new and promising direction for graph data augmentation.\n\n# Code & Appendix â€” https://github.com/yu-rp/DualPrism\n\n# Introduction\n\nGraph structures, modeling complex systems through nodes and edges, are ubiquitous across various domains, including social networks (Newman, Watts, and Strogatz 2002), bioinformatics (Yi et al. 2022), and transportation systems (Jin et al. 2023a). Graph Neural Networks (GNNs) (Kipf and Welling 2016a) elegantly handle this relational information for tasks like accurate predictions. Their capabilities are further enhanced by graph data augmentation techniques, which artificially diversify the dataset via strategic manipulations, thereby improving the performance and generalization of GNNs. Graph data augmentation has progressed from early random topological modifications, e.g. DropEdge (Rong et al. 2019) and DropNode (Feng et al. 2020), to sophisticated learning-centric approaches like InfoMin (Suresh et al. 2021). Furthermore, techniques inspired by image augmentationâ€™s mixup principle (Zhang et al. 2017) have emerged as prominent contenders (Verma et al. 2019; Wang et al. 2021; Guo and Mao 2021).\n\nThough promising, these augmentation methods are challenged by three key issues as follows. (1) Graph Property Distortion. Before the era of deep learning, graph properties, e.g., graph connectivity and diameter, served as vital features for classification for decades (Childs et al. 2009). While now they seem to be ignored, many aforementioned contemporary augmentation methods appear to sidestep this tradition and overlook the graph properties. For instance, an example graph from the IMDB-BINARY dataset (Morris et al. 2020) and its augmented graph via DropEdge are illustrated in Figures 1a and 1b, respectively. The polar plot in Figure 1e shows the properties of these graphs, where each axis represents a distinct property. It is evident that DropEdge significantly alters the original graphâ€™s properties, as indicated by the stark difference between the shapes of the orange (original) and blue (augmented) pentagons. (2) Limited Structural Impact. The majority of existing methods localized alterations do not capture the broader relationships and structures within the graph, limiting their utility. Consider a social network graph, where removing an edge affects just the immediate node and does little to alter the overall community structure. We thus ask: Can we design more property-retentive and structure-aware data augmentation techniques for GNNs?\n\nThrough the Dual-Prism: A Spectral Lens. Graph data augmentation involves altering components of an original graph. These modifications, in turn, lead to changes in the graphâ€™s spectral frequencies (Ortega et al. 2018). Recent research highlighted the importance of the graph spectrum: it can reveal critical graph properties, e.g., connectivity and radius (Chung 1997; Lee, Gharan, and Trevisan 2014). Additionally, it also provides a holistic summary of a graphâ€™s intrinsic structure (Chang et al. 2021), providing a global view for graph topology alterations. Building on this foundation, a pivotal question arises: Could the spectral domain be the stage for structure-aware and property-retentive augmentation efforts? Drawing inspiration from dual prismsâ€”which filter and reconstruct light based on spectral elementsâ€”can we design a polarizer to shed new light on this challenge? With this in mind, we use spectral graph theory, aiming to answer the following questions: 1) Can a spectral approach to graph data augmentation preserve essential graph properties effectively? 2) How does spectral-based augmentation impact broader graph structures? 3) How does spectralbased augmentation compare to existing methods in enhancing the efficiency of GNNs for graph classification?\n\n![](images/ed36ea7ae67e11aea2477f8d96c0b771d0bb74d83349354acc583ff9eaaaab11.jpg)  \nFigure 1: Visualization of (a) a graph from IMDB-B dataset and its augmented graphs via (b) DropEdge (Rong et al. 2019), (c) DP-Noise (ours), and (d) DP-Mask (ours). Dashed line: Dropped edge. Red line: Added edge. (e) Five properties of these graphs. $r$ : radius. $d$ : diameter. conn.: connectivity. ASPL: average shortest path length. #peri: number of periphery. Ori.: Original. D.E.: DropEdge. DP-N: DP-Noise. DP-M: DP-Mask. (f) The eigenvalues of these graphs.\n\nWe begin with an empirical exploration, where we aim to understand the interplay between topological modifications and their spectral responses. Our insights reveal that changes in graph properties mainly manifest in low-frequency components. Armed with this, we unveil our Dual-Prism (DP) augmentation strategies, DP-Noise and DP-Mask, by only changing the high-frequency part of the spectrum of graphs. Figures 1c and 1d provide a visualization of the augmented graphs via our proposed methods, i.e., DP-Noise and DPMask. As shown in Figure 1e, compared with DropEdge, our approaches skillfully maintain the inherent properties of the original graph, differing only slightly in the ASPL. Note that although we solely present one example underscoring our methodâ€™s capability, its robustness is consistently evident across all scenarios.\n\nIn addition to the properties, we further explore the spectrum comparison, shown in Figure 1f. Compared with DropEdge, the spectrum shifts caused by our methods are noticeably smaller. Interestingly, despite our approachesâ€™ relative stability in the spectral domain, they induce substantial changes in the spatial space (i.e., notable edge modifications). This spectral stability helps retain the core properties, while the spatial variations ensure a rich diversity in augmented graphs. Conversely, DropEdge, despite only causing certain edge changes, disrupts the spectrum and essential graph properties significantly. Simply put, our methods skillfully maintain graph properties while also diversifying augmented graphs. In the Experiments section, we evaluate the efficacy of our methods on graph classification, across diverse settings: supervised, semi-supervised, unsupervised, and transfer learning on various real-world datasets.\n\nContributions. Our main contributions are outlined as follows. (1) Prism â€“ Bridging Spatial and Spectral Domains: We introduce a spectral lens to shed light on spatial graph data augmentation, aiming to better understand the spectral behavior of graph modifications and their interplay with inherent graph properties. (2) Polarizer â€“ Innovative Augmentation Method: We propose the globallyaware and property-retentive augmentation methods, DualPrism (DP), including DP-Noise and DP-Mask. Our methods are able to preserve inherent graph properties while simultaneously enhancing the diversity of augmented graphs. (3) New Light â€“ Extensive Evaluations: We conduct comprehensive experiments spanning supervised, semi-supervised, unsupervised, and transfer learning paradigms on 21 realworld datasets. The experimental results demonstrate that our proposed methods can achieve state-of-art performance on the majority of datasets.\n\n# Related Work\n\nData Augmentations for GNNs. Graph data augmentation refers to the process of modifying a graph to enhance or diversify the information contained within, which can be used to bolster the training dataset for better generalization or model variations in real-world networks (Ding et al. 2022; Zhao et al. 2022). Early methods are grounded in random modification to the graph topology. Techniques like DropEdge (Rong et al. 2019), DropNode (Feng et al. 2020), and random subgraph sampling (You et al. 2020) introduce stochastic perturbations in the graph structure. In addition to random modification, there is a wave of methods utilizing more sophisticated, learning-based strategies to generate augmented graphs (Suresh et al. 2021). Another research line is inspired by the efficiency of mixup (Zhang et al. 2017) in image augmentation, blending node features or entire subgraphs to create hybrid graph structures (Verma et al. 2019; Wang et al. 2021; Guo and Mao 2021; Han et al. 2022; Park, Shim, and Yang 2022; Ling et al. 2023). However, while the above techniques have advanced the field of graph data augmentation, challenges remain, especially in preserving broader structural changes and graph semantics.\n\nSpectrum and GNNs. Spectral graph theory (Chung 1997) has significantly influenced GNNs(Ortega et al. 2018; Wu et al. 2019; Dong et al. 2020; Bo et al. 2021; Chang et al. 2021; Yang et al. 2022), evolving from defining early GNN convolutions using the Laplacian spectrum (Hammond, Vandergheynst, and Gribonval 2011; Defferrard, Bresson, and Vandergheynst 2016) to enhancing scalability (Nt and Maehara 2019). This approach is pivotal in various domains, including graph contrastive learning (GCL) (Liu et al. 2022; Lin, Chen, and Wang 2022; Yang et al. 2023; Chen, Lei, and Wei 2024), adversarial attacks (Entezari et al. 2020; Chang et al. 2021), and multivariate time series (Cao et al. 2020; Jin et al. 2023b). Zooming into the GCL domain, where data augmentation plays a pivotal role, (Liu et al. 2022)\n\n![](images/562070b5729442153e47f2093d02d076dd3ad97170e46b55859ac5c8486c11c7.jpg)  \nFigure 2: (a) A toy graph $\\mathcal { G }$ consisting of eight nodes. (b) Absolute variation in eigenvalues of $\\mathcal { G }$ when adding an edge at diverse positions. The red and blue rectangles represent when adding the corresponding edges in $\\mathcal { G }$ and the change of the eigenvalues. (c) A real-world case in the REDD-B dataset - when dropping $20 \\%$ and $50 \\%$ , the high frequency is more vulnerable.\n\nintroduced the general rule of effective augmented graphs in GCL via a spectral perspective; GCL-SPAN (Lin, Chen, and Wang 2022) presented a novel augmentation method for GCL, focusing on the invariance of graph representation in the spectral domain; GASSER (Yang et al. 2023) uses a selective perturbation across various frequency bands to maintain homophily ratios while introducing perturbations to optimize task-relevant features; and PolyGCL (Chen, Lei, and Wei 2024) demonstrates the spectral filters can be effectively utilized for contrastive learning on graphs. Though promising, these methods do not fully address the intricacies of maintaining inherent graph properties during augmentation.\n\n# A Spectral Lens on Graph Data Augmentations\n\nPreliminaries. An undirected graph $\\mathcal { G }$ is represented as $\\mathcal { G } = ( V , E )$ where $V$ is the set of nodes with $| V | = N$ and $E \\subseteq V \\times V$ is the set of edges. Let $A \\in \\dot { \\mathbb { R } } ^ { N \\times N }$ be the adjacency matrix of $\\mathcal { G }$ , with elements $a _ { i j } = 1$ if there is an edge between nodes $\\mathbf { \\chi } _ { i }$ and $j$ , and $a _ { i j } = 0$ otherwise. Let $D \\in \\mathbb { R } ^ { N \\times N }$ be the degree matrix, which is a diagonal matrix with elements $\\begin{array} { r } { d _ { i i } ^ { \\bf { \\bar { \\alpha } } } = \\sum _ { j } a _ { i j } } \\end{array}$ , representing the degree of node $i$ . The Laplacian matrix of $\\mathcal { G }$ is denoted as $L { \\bf \\bar { \\Psi } } = D - A \\in \\mathbb { R } ^ { N \\times N }$ . The eigen-decoGmposition of $L$ is denoted as $U \\Lambda U ^ { \\top }$ , where $\\boldsymbol { \\Lambda } = d i a g ( \\lambda _ { 1 } , \\ldots , \\lambda _ { N } )$ and $U = [ u _ { 1 } ^ { \\top } , \\ldots , u _ { N } ^ { \\top } ] \\in \\mathbb { R } ^ { N \\times N }$ . For graph $\\mathcal { G }$ , $L$ has $n$ nonnegative real eigenvalues $0 \\le \\lambda _ { 1 } \\le \\lambda _ { 2 } \\le . . . \\le \\lambda _ { N }$ . Specifically, the low-frequency components refer to the eigenvalues closer to 0, and the high-frequency components refer to the relatively larger eigenvalues.\n\n# Spectral Analysis Insights\n\nFrom a spectral view, we conduct a thorough empirical study to understand the interplay between graph properties, graph topology alterations in the spatial domain, and their corresponding impacts in the spectral domain. The findings from our analysis include four crucial aspects as detailed below.\n\nObs 1. The position of the edge flip influences the magnitude of spectral changes. In Figures 2a and 2b, we explore how adding different edges to a toy graph affects its eigenvalues. For instance, the addition of the edge $1  3$ (shown as the red line), which connects two proximate nodes, primarily impacts the high-frequency component $\\lambda _ { 6 }$ (the red rectangle). In contrast, when adding edge $2 {  } 6$ (the blue line) between two distant nodes, the low-frequency component $\\lambda _ { 1 }$ exhibits the most noticeable change (the blue rectangle). These variations in the spectrum underscore the significance of the edge-flipping position within the graphâ€™s overall topology, consistent with findings by (Entezari et al. 2020; Chang et al. 2021). Such spectral changes affect the graphâ€™s inherent structural features, leading to decreased performance on tasks that rely on graph properties.\n\nObs 2. Low-frequency components display greater resilience to edge alterations. Building on Obs 1, we further investigate the different responses of high- and lowfrequency components to topology alterations using a realworld graph. We apply DropEdge (Rong et al. 2019) for augmentation by first randomly dropping $2 0 \\%$ and $5 0 \\%$ of edges and then computing the corresponding eigenvalues, as depicted in Figure 2c. It can be observed that, under random edge removal, low-frequency components exhibit greater robustness compared to their high-frequency counterparts.\n\nObs 3. Graph properties are crucial for graph classification. Certain fundamental properties of graphs, e.g., diameter and radius, are critical for a variety of downstream tasks, including graph classification (Feragen et al. 2013). In Figures 3a and 3b, we present the distributions of two key graph properties â€“ diameter $d$ and radius r â€“ across the two classes in the REDD-M12 dataset. The different variations in these distributions emphasize their critical role in graph classification. Nevertheless, arbitrary modifications to the graphâ€™s topology, e.g., random-manner-based augmentation techniques, could potentially distort these vital properties, illustrated in Figures 1b and 1e.\n\nObs 4. Specific low-frequency eigenvalues are closely tied to crucial graph properties. From Obs 3, a question is raised: Can we retain the integrity of these essential graph properties during augmentation? To explore this, we turn our attention back to the toy graph in Figure 2a and examine the evolution of its properties and eigenvalues in response to single-edge flips. In Figures 3c and 3d, we chart the graphâ€™s average shortest path length (denoted by blue dots), diameter $d$ (green dots) against the reciprocal of its second smallest eigenvalue $1 / \\lambda _ { 1 }$ (red dots)\\*. Our observations reveal a notable correlation between the alterations in $d$ and $1 / \\lambda _ { 1 }$ .\n\n![](images/8449a2f92850079bb27c51f9c16765bb1867cab0e890122e379ab3a7e7069c6a.jpg)  \nFigure 3: (a) Diameter and (b) radius distributions of different classes in REDD-M12. When (c) adding or (d) removing an edge, variation of the spectral domain $\\Delta L _ { 2 }$ , $1 / \\lambda _ { 1 }$ of $\\mathcal { G } ^ { \\prime }$ , ASPL and diameter $d$ of $\\mathcal { G } ^ { \\prime }$ .\n\nFurther, we investigate correlations among overall spectral shifts, graph properties, and specific eigenvalues. Consistent with established methods (Lin, Chen, and Wang 2022; Wang et al. 2022; Wills and Meyer 2020), we adopt the Frobenius distance to quantify the overall spectral variations by computing the $L _ { 2 }$ distance between the spectrum of $\\mathcal { G }$ and augmented graph $\\mathcal { G } ^ { \\prime }$ . Notably, this spectral shift does not directly correspond with the changes in properties or eigenvalues. This suggests that by maintaining critical eigenvalues, primarily the low-frequency components, we can inject relatively large spectral changes without affecting essential graph properties. This observation thus leads to our proposition: preserving key eigenvalues while modifying others enables the generation of augmented graphs that uphold foundational properties, instead of only focusing on the overall spectral shifts.\n\n# Methodology\n\nDrawing inspiration from how prisms decompose and reconstruct light and how a polarizer selectively filters light (Figure 4a), we design our own â€œpolarizerâ€, i.e., the Dual-Prism (DP) method for graph data augmentation (Figure 4b).\n\n# Proposed Augmentation Methods\n\nThe proposed DP methods, including $D P$ -Noise and $D P$ - Mask, obtain the augmented graphs by directly changing the spectrum of graphs. A step-by-step breakdown is delineated in Algorithm 1. The DP method starts by extracting the Laplacian Matrix $L$ of a graph $\\mathcal { G }$ by $L = D - A$ and then computes its eigen-decomposition $\\dot { \\boldsymbol { L } } = \\boldsymbol { U } \\boldsymbol { \\Lambda } \\boldsymbol { U } ^ { \\top }$ . Based on the frequency ratio $r _ { f }$ , $N _ { a }$ eigenvalues are selected for augmentation, where $N _ { a } = N \\times r _ { f }$ . Note that since we only target high-frequency eigenvalues, we arrange eigenvalues in increasing order and only focus on the last $\\textstyle N _ { a }$ eigenvalues. Then, a binary mask $M$ is formed based on the augmentation ratio $r _ { a }$ to sample the eigenvalues to make a change. Depending on the chosen augmentation type $T$ , either noise is infused to the sampled eigenvalues, modulated by $\\sigma$ and\n\n$M$ (from Line 6 to 8), or the eigenvalues are adjusted using the mask $M$ directly (from Line 9 to 10). Finally, we reconstruct the new Laplacian $\\hat { L }$ based on the updated eigenvalues $\\hat { \\Lambda }$ . Given the Laplacian matrix is $L = D - A$ , where $D$ is a diagonal matrix, an updated adjacency matrix $\\hat { A }$ can be derived by eliminating self-loops. Lastly, we obtain the augmented graph $\\hat { \\mathcal G }$ with its original labels and features retained. Selection of $L$ . Note that we adopt the Laplacian matrix L instead of the normalized Laplacian matrix Lnorm $I - D ^ { - 1 / 2 } A D ^ { - 1 / 2 }$ . The rationale behind this choice is that reconstructing the adjacency matrix using $L _ { \\mathrm { { n o r m } } }$ necessitates solving a system quadratic equation, where the number of unknown parameters equals the number of nodes in the graph. The computational complexity of this solution is more than $O ( N ^ { 3 } )$ . Even if we approximate it as a quadratic optimization problem, making it solvable with a gradientbased optimizer, the computational overhead introduced by solving such an optimization problem for each graph to be augmented is prohibitively high.\n\n# Empirical Evidence\n\nNext, we verify our methods on the improvement of the performance for graph classification via experimental analysis. Diverse roles of eigenvalues. We begin by masking selected eigenvalues to generate graphs for $20 \\%$ of REDDITBINARY. The training loss when masking the eigenvalues $\\lambda _ { 1 } , \\lambda _ { 2 }$ and $\\lambda _ { 5 }$ is shown in Figure 5a. These curves suggest that individual eigenvalues contribute differently to the training process. Specifically, masking $\\lambda _ { 1 }$ results in a notably unstable training loss for the initial 500 epochs, evidenced by the expansive blue-shaded region. For $\\lambda _ { 2 }$ , while the shaded regionâ€™s extent is smaller, the curve exhibits noticeable fluctuations, particularly around epoch 200. Conversely, when masking $\\lambda _ { 5 }$ , the training appears more stable, with the green curve showing relative steadiness and a reduced shaded area. These demonstrate the various significance of eigenvalues: $\\lambda _ { 1 }$ and $\\lambda _ { 2 }$ include more crucial structural and property details of the graph compared to $\\lambda _ { 5 }$ . As a result, they deserve to be prioritized for preservation during augmentation.\n\nDifferent importance of high- and low-frequency parts. We then conduct experiments on group-level eigenvalues, i.e., the high- and low-frequency ones, to gain a broader view of the influence exerted by varying frequency eigenvalues. We introduce noise to eigenvalues across various hyperparameter combinations. Concretely, we use the standard deviation $\\sigma$ to determine the magnitude of the noise. The frequency ratio $r _ { f }$ dictates the number of eigenvalues $\\textstyle N _ { a }$ we might change, while the augmentation probability $r _ { a }$ specifies the final eigenvalues sampled for modification. The eigenvalues are arranged in ascending order. The setting of â€™Lowâ€™ means that we select candidates from the first $N _ { a }$ eigenvalues, while â€™Highâ€™ denotes selection from the last $N _ { a }$ eigenvalues. As shown in Figure 5b, the orange lines consistently outperform the green lines across all three plots, indicating that the performance associated with perturbing highfrequency eigenvalues consistently exceeds that of their lowfrequency counterparts. Moreover, as the frequency ratio $\\boldsymbol { r } _ { f }$ increases, the accuracy of the â€™Lowâ€™ scenario remains rela\n\nSpSepcetcrtruum Prism Polarizer Prism Polarizer IIncreasingfrferequqeunecnycy C ã€‚ O A D U â†‘ Light Target â–¡ Altered add noise ä¸‰ GOriagpihnaGl Î› mask æ›² AGurgampehnted Light Prism Prism Eigen-Decomp. Aug on high-freq. Recover L (a) Dual Prism with Polarizer (b) Dual Prism (DP) Graph Data Augmentation Method\n\n. Low High 0.6 Mask 1 0.75 0.734 Mask 2 0.734 Mask 5 0.2 0.730 0.72 0 200 400 600 800 1 2 0.25 0.50 0.75 0.5 1.0 Epoch Standard Deviation Frequency Ratio rf Augmentation Probability $r _ { a }$ (a) Diverse roles of selected eigenvalues (b) Relative significance of high- and low-frequency eigenvalues\n\n# Algorithm 1: Dual-Prism Augmentation\n\ntively stable and low. Contrastingly, for the â€™Highâ€™ scenario, a notable decline in accuracy begins once the ratio exceeds around $30 \\%$ . This suggests that the eigenvalues outside the top $30 \\%$ of the high-frequency range may start to include more critical information beneficial for graph classification tasks that should not be distorted.\n\n# Theoretical Backing and Insights\n\nThe eigenvalues of the Laplacian matrix provide significant insights into various graph properties (Chung 1997). Such insights have driven and backed our proposal to modify the high-frequency eigenvalues while preserving their lowfrequency counterparts. For example, the second-smallest eigenvalue $\\lambda _ { 1 }$ , often termed the Fiedler value, quantifies the graphâ€™s algebraic connectivity. A greater Fiedler value indicates a better-connected graph and it is greater than 0 if and only if the graph is connected. The number of times 0 appears as an eigenvalue is the number of connected components in the graph. In addition to the connectivity, the diameter of a graph is also highly related to the eigenvalues â€“ it can be upper and lower bounded from its spectrum (Chung 1997): $4 / n \\lambda _ { 1 } \\le d \\le 2 [ \\sqrt { 2 m / \\lambda _ { 1 } } \\log _ { 2 } n ]$ , where $n$ and $m$ denotes the number of nodes and the maximum degree of the graph. Besides these widely-used ones, other properties are also highly related to spectrum, e.g., graph diffusion distance (Hammond, Gur, and Johnson 2013). In essence, eigenvalues serve as powerful spectral signatures comprising a myriad of structural and functional aspects of graphs.\n\n# Experiments\n\nExperimental Setup. We evaluate our method for graph classification under four different settings, including supervised learning, semi-supervised learning, unsupervised learning, and transfer learning. We conduct our experiments on 21 real-world datasets across three different domains, including bio-informatics, molecule, and social network, from the TUDatasets (Morris et al. 2020), OGB (Hu et al. 2020a) and ZINC chemical molecule dataset (Hu et al. 2020b).\n\n# Supervised Learning\n\nPerformance. We first evaluate our proposed methods in the supervised learning setting. Following the prior works (Han et al. 2022; Ling et al. 2023), we use GIN and GCN as backbones for graph classification on eight different datasets.\n\n<html><body><table><tr><td></td><td>Dataset</td><td>IMDB-B</td><td>IMDB-M</td><td>REDD-B</td><td>REDD-M5</td><td>REDD-M12</td><td>PROTEINS</td><td>NCI1</td><td>ogbg-molhiv</td></tr><tr><td rowspan=\"8\">GCG</td><td>Vanilla</td><td>72.80Â±4.08</td><td>49.47Â±2.60</td><td>84.85Â±2.42</td><td>49.99Â±1.37</td><td>46.90Â±0.73</td><td>71.43Â±2.60</td><td>72.38Â±1.45</td><td>96.80Â±0.06</td></tr><tr><td>DropEdge</td><td>73.20Â±5.62</td><td>49.00Â±2.94</td><td>85.15Â±2.81</td><td>51.19Â±1.74</td><td>47.08Â±0.55</td><td>71.61Â±4.28</td><td>68.32Â±1.60</td><td>96.53Â±0.07</td></tr><tr><td>DropNode</td><td>73.80Â±5.71</td><td>50.00Â±4.85</td><td>83.65Â±3.63</td><td>47.71Â±1.75</td><td>47.93Â±0.64</td><td>72.69Â±3.55</td><td>70.73Â±2.02</td><td>96.52Â±0.14</td></tr><tr><td>Subgraph</td><td>70.90Â±5.07</td><td>49.80Â±3.43</td><td>68.41Â±2.57</td><td>47.31Â±5.23</td><td>47.49Â±0.93</td><td>67.93Â±3.24</td><td>65.05Â±4.36</td><td>96.49Â±0.17</td></tr><tr><td>M-Mixup</td><td>72.00Â±5.66</td><td>49.73Â±2.67</td><td>87.05Â±2.47</td><td>51.49Â±2.00</td><td>46.92Â±1.05</td><td>71.16Â±2.87</td><td>71.58Â±1.79</td><td>-</td></tr><tr><td>SubMix</td><td>72.30Â±4.75</td><td>49.73Â±2.88</td><td>85.15Â±2.37</td><td>52.87Â±2.19</td><td></td><td>72.42Â±2.43</td><td>71.65Â±1.58</td><td>-</td></tr><tr><td>G-Mixup</td><td>73.20Â±5.60</td><td>50.33Â±3.67</td><td>86.85Â±2.30</td><td>51.77Â±1.42</td><td>48.06Â±0.53</td><td>70.18Â±2.44</td><td>70.75Â±1.72</td><td>-</td></tr><tr><td>S-Mixup</td><td>74.40Â±5.44</td><td>50.73Â±3.66</td><td>89.30Â±2.69</td><td>53.29Â±1.97</td><td></td><td>73.05Â±2.81</td><td>75.47Â±1.49</td><td>96.70Â±0.20</td></tr><tr><td rowspan=\"10\"></td><td>DP-Noise</td><td>77.90Â±2.30 *</td><td>53.60Â±1.59**</td><td>84.60Â±7.61</td><td>53.42Â±1.36</td><td>48.47 Â±0.57</td><td>75.03Â±2.66</td><td>69.20Â±2.57</td><td>97.02Â±0.19**</td></tr><tr><td>DP-Mask</td><td>76.00Â±3.62</td><td>51.20Â±1.73</td><td>76.70Â±1.34</td><td>52.42Â±2.78</td><td>47.25Â±1.12</td><td>73.60Â±3.10</td><td>62.45Â±3.80</td><td>96.90Â±0.24*</td></tr><tr><td>Vanilla</td><td>71.30Â±4.36</td><td>48.80Â±2.54</td><td>89.15Â±2.47</td><td>53.17Â±2.26</td><td>50.23Â±0.83</td><td>68.28Â±2.47</td><td>79.08Â±2.12</td><td>96.54Â±0.11</td></tr><tr><td>DropEdge</td><td>70.50Â±3.80</td><td>48.73Â±4.08</td><td>87.45Â±3.91</td><td>54.11Â±1.94</td><td>49.77Â±0.76</td><td>68.01Â±3.22</td><td>76.47Â±2.34</td><td>96.48Â±0.09</td></tr><tr><td>DropNode</td><td>72.00Â±6.97</td><td>45.67Â±2.59</td><td>88.60Â±2.52</td><td>53.97Â±2.11</td><td>49.95Â±1.70</td><td>69.64Â±2.98</td><td>74.60Â±2.12</td><td>96.51Â±0.16</td></tr><tr><td>Subgraph</td><td>70.40Â±4.98</td><td>43.74Â±5.74</td><td>76.80Â±3.87</td><td>50.09Â±4.94</td><td>49.67Â±0.90</td><td>66.67Â±3.10</td><td>60.17Â±2.33</td><td>96.49Â±0.13</td></tr><tr><td>M-Mixup</td><td>72.00Â±5.14</td><td>48.67Â±5.32</td><td>87.70Â±2.50</td><td>52.85Â±1.03</td><td>49.81Â±0.80</td><td>68.65Â±3.76</td><td>79.85Â±1.88</td><td></td></tr><tr><td>SubMix</td><td>71.70Â±6.20</td><td>49.80Â±4.01</td><td>90.45Â±1.93</td><td>54.27Â±2.92</td><td></td><td>69.54Â±3.15</td><td>79.78Â±1.09</td><td></td></tr><tr><td>G-Mixup</td><td>72.40Â±5.64</td><td>49.93Â±2.82</td><td>90.20Â±2.84</td><td>54.33Â±1.99</td><td>50.50Â±0.41</td><td>64.69Â±3.60</td><td>78.20Â±1.58</td><td></td></tr><tr><td>S-Mixup</td><td>73.40Â±6.26</td><td>50.13Â±4.34</td><td>90.55Â±2.11</td><td>55.19Â±1.99</td><td></td><td></td><td></td><td>=</td></tr><tr><td></td><td>DP-Noise</td><td>78.40Â±1.82**</td><td>61.67Â±0.71**</td><td>93.42Â±1.41**</td><td>57.72Â±1.87**</td><td>53.70Â±1.16**</td><td>69.37Â±2.86 73.51Â±4.54**</td><td>80.02Â±2.45 90.56Â±5.78**</td><td>96.84Â±0.40 97.43Â±0.48**</td></tr><tr><td></td><td>DP-Mask</td><td>76.30Â±2.56</td><td>51.60Â±1.32</td><td>93.25Â±1.19**</td><td>56.50Â±0.80*</td><td>49.11Â±1.30</td><td>72.79Â±1.95**</td><td>80.30Â±2.01</td><td>96.98Â±0.29</td></tr></table></body></html>\n\nTable 1: Performance comparisons with GCN and GIN in the supervised learning setting. The best and second best results are highlighted with bold and underline, respectively. \\* and $^ { * * }$ denote the improvement over the second best baseline is statistically significant at level 0.1 and 0.05 (Newey and West 1987).\n\nVanilla G-mixup DP-Noise DP-Mask 1.4 0.80 NCI1 1.95 1.71 2.62 2.13 1.80 NCI1 1.32 0.88 2.56 1.80 1.51 1.02 0.8 2 PROTEINS 4.33 4.33 4.99 4.33 3.84 PROTEINS 4.33 1.65 2.76 3.84 3.44 CROELDLDA-DB 23.53074 202.94294 321.37359 312.254204 401.216965 CROELDLDA-DB 24.202509 313.1923 02.92459 104.612569 104.821536 3 0.6 0.65 REDD-M5 2.97 3.02 3.27 3.12 0.63 REDD-M5 1.96 1.47 1.87 0.63 1.15 + GITHUB 4.37 3.54 4.72 2.68 3.26 GITHUB 3.59 3.62 4.21 3.26 3.26 0.4 0.60 0 100 200 300 0 100 200 300 dropN PmaskN subgrgp-noise oisp-Mask dropN PmaskN assubgrap-Nois-Mask Epoch Epoch (a) Test Loss on IMDB-B (b) Test Accuracy on IMDB-B (c) DP-Noise (d) DP-Mask\n\nTable 2: Performance comparisons in the semi-supervised learning setting with $1 \\%$ label ratio.   \n\n<html><body><table><tr><td>Dataset</td><td>NCI1</td><td>COLLAB</td><td>GITHUB</td></tr><tr><td>Vallina</td><td>60.72Â±0.45</td><td>57.46Â±0.25</td><td>54.25Â±0.22</td></tr><tr><td>Aug.</td><td>60.49Â±0.46</td><td>58.40Â±0.97</td><td>56.36Â±0.42</td></tr><tr><td>GAE</td><td>61.63Â±0.84</td><td>63.20Â±0.67</td><td>59.44Â±0.44</td></tr><tr><td>Infomax</td><td>62.72Â±0.65</td><td>61.70Â±0.77</td><td>58.99Â±0.50</td></tr><tr><td>GraphCL</td><td>62.55Â±0.86</td><td>64.57Â±1.15</td><td>58.56Â±0.59</td></tr><tr><td>DP-Noise</td><td>63.43Â±1.39</td><td>65.94Â±3.13</td><td>60.06Â±2.72</td></tr><tr><td>DP-Mask</td><td>62.43Â±1.08</td><td>65.68Â±1.66*</td><td>59.70Â±0.53</td></tr></table></body></html>\n\nTable 1 shows the performance of DP methods compared with seven state-of-art (SOTA) baselines, including DropEdge(Rong et al. 2019), DropNode(Feng et al. 2020), Subgraph(You et al. 2020), M-Mixup (Verma et al. 2019), SubMix(Yoo, Shim, and Kang 2022), G-Mixup(Han et al. 2022) and S-Mixup (Ling et al. 2023). According to the results, DP-Noise consistently outperforms other existing methods across the majority of datasets, establishing its dominance in effectiveness. DP-Mask also shines, often securing a secondplace standing. GIN tends to obtain superior outcomes, especially when combined with DP-Noise, exemplified by its $6 1 . 6 7 \\%$ accuracy on IMDB-M. Note that on REDD-B, GIN achieves more satisfactory performance than GCN, which is a consistent pattern across baselines but becomes particularly pronounced with our methods. This phenomenon may be attributed to the intrinsic characteristics of GIN and GCN: GIN is known for its precision in capturing the complex structures of graphs (Xu et al. 2018), while GCN is characterized by its smoothing effect (Defferrard, Bresson, and Vandergheynst 2016). Our methodsâ€™ superiority in diversifying the graphsâ€™ structures naturally amplifies GINâ€™s strengths. In contrast, GCN may not be as adept at leveraging the enhancements offered by our techniques.\n\nGeneralization. Figures 6a and 6b display the test loss and accuracy curves for the IMDB-B dataset, comparing four distinct augmentation strategies: G-mixup, DP-Noise, DP-Mask, and a scenario without any augmentation (i.e., Vanilla). The consistently lower and more stable test loss curves for DP-Noise and DP-Mask is comparison to Vanilla and G-mixup. Concurrently, the accuracy achieved with DP\n\nTable 3: Performance comparisons in the semi-supervised learning setting with $10 \\%$ label ratio.   \n\n<html><body><table><tr><td>Dataset</td><td>NCI1</td><td>PROTEINS</td><td>DD</td><td>COLLAB</td><td>REDD-B</td><td>REDD-M5</td><td>GITHUB</td></tr><tr><td>Vallina</td><td>73.72Â±0.24</td><td>70.40Â±1.54</td><td>73.56Â±0.41</td><td>73.71Â±0.27</td><td>86.63Â±0.27</td><td>51.33Â±0.44</td><td>60.87Â±0.17</td></tr><tr><td>Aug.</td><td>73.59Â±0.32</td><td>70.29Â±0.64</td><td>74.30Â±0.81</td><td>74.19Â±0.13</td><td>87.74Â±0.39</td><td>52.01Â±0.20</td><td>60.91Â±0.32</td></tr><tr><td>GAE</td><td>74.36Â±0.24</td><td>70.51Â±0.17</td><td>74.54Â±0.68</td><td>75.09Â±0.19</td><td>87.69Â±0.40</td><td>53.58Â±0.13</td><td>63.89Â±0.52</td></tr><tr><td>Infomax</td><td>74.86Â±0.26</td><td>72.27Â±0.40</td><td>75.78Â±0.34</td><td>73.76Â±0.29</td><td>88.66Â±0.95</td><td>53.61Â±0.31</td><td>65.21Â±0.88</td></tr><tr><td>GraphCL</td><td>74.63Â±0.25</td><td>74.17Â±0.34</td><td>76.17Â±1.37</td><td>74.23Â±0.21</td><td>89.11Â±0.19</td><td>52.55Â±0.45</td><td>65.81Â±0.79</td></tr><tr><td>DP-Noise</td><td>75.30Â±0.58**</td><td>74.73Â± 1.01</td><td>76.91Â±0.81</td><td>77.05Â±0.82**</td><td>89.38Â±0.95</td><td>54.45Â±0.64**</td><td>65.59Â±0.88</td></tr><tr><td>DP-Mask</td><td>74.88Â±1.84</td><td>71.37Â±4.18</td><td>75.64Â±0.81</td><td>76.90Â±0.62**</td><td>88.62Â±0.63</td><td>52.80Â±0.59</td><td>64.95Â±1.03</td></tr></table></body></html>\n\nTable 4: Performance comparisons in the unsupervised learning.   \n\n<html><body><table><tr><td>Dataset</td><td>NCI1</td><td>PROTEINS</td><td>DD</td><td>MUTAG</td><td>REDD-B</td><td>REDD-M5</td><td>IMDB-B</td></tr><tr><td></td><td></td><td></td><td>--</td><td></td><td>71.48Â±0.41</td><td>36.68Â±042</td><td>55.26Â±1.54</td></tr><tr><td>gub2vevec</td><td>52.241.47</td><td>53.03Â±5.5</td><td></td><td>61.05Â±15.80</td><td></td><td></td><td></td></tr><tr><td>InfoGraph</td><td>76.20Â±1.06</td><td>74.44Â±0.31</td><td>75.23Â±0.39</td><td>89.01Â±1.13</td><td>82.50Â±1.42</td><td>53.46Â±1.03</td><td>73.03Â±0.87</td></tr><tr><td>GraphCL</td><td>77.87Â±0.41</td><td>74.39Â±0.45</td><td>78.62Â±0.40</td><td>86.80Â±1.34</td><td>89.53Â±0.84</td><td>55.99Â±0.28</td><td>71.14Â±0.44</td></tr><tr><td>MVGRL</td><td>68.68Â±0.42</td><td>74.02Â±0.32</td><td>75.20Â±0.55</td><td>89.24Â±1.31</td><td>81.20Â±0.69</td><td>51.87Â±0.65</td><td>71.84Â±0.78</td></tr><tr><td>AD-GCL</td><td>69.67Â±0.51</td><td>73.59Â±0.65</td><td>74.49Â±0.52</td><td>89.25Â±1.45</td><td>85.52Â±0.79</td><td>53.00Â±0.82</td><td>71.57Â±1.01</td></tr><tr><td>JOAO</td><td>72.99Â±0.75</td><td>71.25Â±0.85</td><td>66.91Â±1.75</td><td>85.20Â±1.64</td><td>78.35Â±1.38</td><td>45.57Â±2.86</td><td>71.60Â±0.86</td></tr><tr><td>GCL-SPAN</td><td>71.43Â±0.49</td><td>75.78Â±0.41</td><td>75.78Â±0.52</td><td>89.12Â±0.76</td><td>83.62Â±0.64</td><td>54.10Â±0.49</td><td>73.65Â±0.69</td></tr><tr><td>DP-Noise</td><td>79.69Â±0.70</td><td>74.60Â±0.43</td><td>78.59Â±0.23</td><td>87.63Â±1.98</td><td>90.90Â±0.32**</td><td>55.54Â±0.15</td><td>71.42Â±0.41</td></tr><tr><td>DP-Mask</td><td>79.47Â±0.22</td><td>74.70Â±0.29</td><td>79.97Â±1.09**</td><td>89.98Â±1.36</td><td>91.21Â±0.24**</td><td>55.92Â±0.49</td><td>71.78Â±0.37</td></tr></table></body></html>\n\nNoise and DP-Mask is higher, indicating the DP methodsâ€™ superior generalization and the capacity for enhancing model stability.\n\n# Semi-supervised Learning\n\nPerformance. We then evaluate our methods in a semisupervised setting comparing with five baselines, including training from scratch without and with augmentations (denoted as Vanilla and Aug.), GAE (Kipf and Welling 2016b), Informax (VelicË‡kovicÂ´ et al. 2018) and GraphCL(You et al. 2020). Tables 2 and 3 provide the performance comparison when utilizing $1 \\%$ and $10 \\%$ label ratios. At the more challenging $1 \\%$ label ratio, DP-Noise achieves SOTA results across all three datasets, and DP-Mask secures the secondbest performance on two out of the three datasets. As the label ratio increases to $10 \\%$ , DP-Noise maintains its efficacy, showing SOTA performance on six out of seven datasets.\n\nAugmentation Pairing Efficiency. To investigate optimal combinations that could potentially enhance performance, we evaluate the synergistic effects on accuracy gain $( \\% )$ when pairing DP-Noise and DP-Mask with different augmentation methods using the same setting in (You et al. 2020). From Figures 6c and 6d, overall, the diverse accuracy gains across datasets indicate that there is no one-size-fits-all â€œpartnerâ€ for DP-Noise and DP-Mask; the efficacy of each combination varies depending on the dataset. However, generally, both DP-Noise and DP-Mask exhibit enhanced performance when paired with dropN at a large degree.\n\n# Unsupervised Representation Learning\n\nWe next evaluate DP methods in unsupervised learning and compare them with eight baselines, including three representation learning methods (sub2vec(Adhikari et al. 2018), graph2vec(Narayanan et al. 2017) and InfoGraph(Sun et al. 2019)) and five GCL-based methods (GraphCL(You et al. 2020), MVGRL(Hassani and Khasahmadi 2020), ADGCL(Suresh et al. 2021), JOAO(You et al. 2021) and GCL\n\nSPAN(Lin, Chen, and Wang 2022)). Table 4 shows the performance of our methods in an unsupervised setting. From the results, DP methods surpass other baselines on five out of seven datasets. Notably, compared with another spectralbased method GCL-SPAN (Lin, Chen, and Wang 2022), our methods outperform it on most datasets, especially on the molecules dataset NCI1 (an increase of around $1 1 . 5 \\%$ accuracy). This superiority can be attributed to the fact that while GCL-SPAN is also spectral-based, it operates on graph modification in the spatial domain rather than directly manipulating the spectral domain like DP methods. Given the critical role that structural patterns in molecular data play on classification tasks, the enhanced performance underscores the efficiency of our direct spectral modifications in creating more effective and insightful augmented graphs.\n\n# Transfer Learning\n\nWe conduct transfer learning experiments on molecular property prediction in the manner of (Hu et al. 2020b). Specifically, we initially pre-train models on the extensive chemical molecule dataset ZINC (Sterling and Irwin 2015), then fine-tune the models on eight distinct datasets within a similar domain. Due to space constraints, the performance comparisons and analysis are provided in the Appendix.\n\n# Conclusion\n\nIn this study, we adopt a spectral perspective, bridging graph properties and spectral insights for property-retentive and globally-aware graph data augmentation. From this point, we propose a novel augmentation method DP, including DPNoise and DP-Mask. By focusing on different frequency components, our method skillfully preserves graph properties while ensuring diversity in augmented graphs. Extensive experiments validate the efficacy of our methods across four learning paradigms on the graph classification task.",
    "summary": "```json\n{\n  \"core_summary\": \"### ğŸ¯ æ ¸å¿ƒæ¦‚è¦\\n\\n> **é—®é¢˜å®šä¹‰ (Problem Definition)**\\n> *   è®ºæ–‡è§£å†³äº†å›¾æ•°æ®å¢å¼ºä¸­çš„ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ï¼š1) å›¾å±æ€§å¤±çœŸï¼ˆGraph Property Distortionï¼‰ï¼Œå³ç°æœ‰å¢å¼ºæ–¹æ³•ä¼šæ˜¾è‘—æ”¹å˜å›¾çš„å…³é”®å±æ€§ï¼ˆå¦‚è¿é€šæ€§ã€ç›´å¾„ç­‰ï¼‰ï¼›2) ç»“æ„å½±å“æœ‰é™ï¼ˆLimited Structural Impactï¼‰ï¼Œå³ç°æœ‰æ–¹æ³•ä»…èƒ½å±€éƒ¨ä¿®æ”¹å›¾ç»“æ„ï¼Œç¼ºä¹å…¨å±€è§†è§’ã€‚\\n> *   è¯¥é—®é¢˜åœ¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰çš„å›¾åˆ†ç±»ä»»åŠ¡ä¸­å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºä¿æŒå›¾çš„å…³é”®å±æ€§å¯¹äºåˆ†ç±»æ€§èƒ½è‡³å…³é‡è¦ï¼Œè€Œç°æœ‰æ–¹æ³•æ— æ³•åœ¨å¢å¼ºè¿‡ç¨‹ä¸­æœ‰æ•ˆä¿ç•™è¿™äº›å±æ€§ã€‚\\n\\n> **æ–¹æ³•æ¦‚è¿° (Method Overview)**\\n> *   è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè°±è§†è§’çš„å›¾æ•°æ®å¢å¼ºæ–¹æ³•Dual-Prismï¼ˆDPï¼‰ï¼ŒåŒ…æ‹¬DP-Noiseå’ŒDP-Maskï¼Œé€šè¿‡ä»…ä¿®æ”¹å›¾çš„é«˜é¢‘è°±æˆåˆ†æ¥ç”Ÿæˆå¢å¼ºå›¾ï¼ŒåŒæ—¶ä¿ç•™ä½é¢‘è°±æˆåˆ†ä»¥ç»´æŒå›¾çš„å…³é”®å±æ€§ã€‚\\n\\n> **ä¸»è¦è´¡çŒ®ä¸æ•ˆæœ (Contributions & Results)**\\n> *   **è´¡çŒ®1ï¼š** æå‡ºäº†ä¸€ç§æ–°çš„è°±è§†è§’ï¼Œæ­ç¤ºäº†å›¾å±æ€§ã€å›¾å¢å¼ºå’Œè°±è¡Œä¸ºä¹‹é—´çš„äº¤äº’å…³ç³»ï¼Œå‘ç°ä¿ç•™ä½é¢‘ç‰¹å¾å€¼å¯ä»¥å¤§è§„æ¨¡ä¿æŒå›¾çš„å…³é”®å±æ€§ã€‚\\n> *   **è´¡çŒ®2ï¼š** è®¾è®¡äº†DP-Noiseå’ŒDP-Maskä¸¤ç§å¢å¼ºæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¿ç•™å›¾å±æ€§çš„åŒæ—¶å¢åŠ å¢å¼ºå›¾çš„å¤šæ ·æ€§ã€‚\\n> *   **è´¡çŒ®3ï¼š** åœ¨21ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDPæ–¹æ³•åœ¨ç›‘ç£ã€åŠç›‘ç£ã€æ— ç›‘ç£å’Œè¿ç§»å­¦ä¹ åœºæ™¯ä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¾‹å¦‚åœ¨IMDB-Bæ•°æ®é›†ä¸Šï¼ŒDP-Noiseçš„å‡†ç¡®ç‡è¾¾åˆ°äº†77.9%ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚\",\n  \"algorithm_details\": \"### âš™ï¸ ç®—æ³•/æ–¹æ¡ˆè¯¦è§£\\n\\n> **æ ¸å¿ƒæ€æƒ³ (Core Idea)**\\n> *   è®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡è°±å›¾ç†è®ºï¼Œå°†å›¾æ•°æ®å¢å¼ºé—®é¢˜è½¬åŒ–ä¸ºå¯¹å›¾è°±çš„ä¿®æ”¹é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå›¾çš„æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ç‰¹å¾å€¼ï¼ˆè°±ï¼‰å¯ä»¥åæ˜ å›¾çš„ç»“æ„å’Œå±æ€§ï¼Œä½é¢‘ç‰¹å¾å€¼ä¸å›¾çš„å…³é”®å±æ€§ï¼ˆå¦‚è¿é€šæ€§ã€ç›´å¾„ï¼‰å¯†åˆ‡ç›¸å…³ï¼Œè€Œé«˜é¢‘ç‰¹å¾å€¼åˆ™ä¸å±€éƒ¨ç»“æ„ç›¸å…³ã€‚å› æ­¤ï¼Œé€šè¿‡ä»…ä¿®æ”¹é«˜é¢‘ç‰¹å¾å€¼ï¼Œå¯ä»¥åœ¨å¢å¼ºå›¾çš„åŒæ—¶ä¿ç•™å…¶å…³é”®å±æ€§ã€‚\\n\\n> **åˆ›æ–°ç‚¹ (Innovations)**\\n> *   **ä¸å…ˆå‰å·¥ä½œçš„å¯¹æ¯”ï¼š** ç°æœ‰æ–¹æ³•ï¼ˆå¦‚DropEdgeã€DropNodeï¼‰ç›´æ¥åœ¨ç©ºé—´åŸŸè¿›è¡Œéšæœºä¿®æ”¹ï¼Œç¼ºä¹å¯¹å›¾å±æ€§çš„å…¨å±€è€ƒè™‘ï¼Œå®¹æ˜“å¯¼è‡´å±æ€§å¤±çœŸã€‚\\n> *   **æœ¬æ–‡çš„æ”¹è¿›ï¼š** DPæ–¹æ³•åœ¨è°±åŸŸè¿›è¡Œæ“ä½œï¼Œé€šè¿‡é€‰æ‹©æ€§ä¿®æ”¹é«˜é¢‘ç‰¹å¾å€¼ï¼Œæ—¢ä¿ç•™äº†ä½é¢‘ç‰¹å¾å€¼å¯¹åº”çš„å›¾å±æ€§ï¼Œåˆé€šè¿‡é«˜é¢‘ç‰¹å¾å€¼çš„ä¿®æ”¹å¼•å…¥äº†å¤šæ ·æ€§ã€‚\\n\\n> **å…·ä½“å®ç°æ­¥éª¤ (Implementation Steps)**\\n> *   1. **æå–æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µï¼š** ç»™å®šå›¾$\\\\mathcal{G}$ï¼Œè®¡ç®—å…¶æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ$L = D - A$ï¼Œå…¶ä¸­$D$ä¸ºåº¦çŸ©é˜µï¼Œ$A$ä¸ºé‚»æ¥çŸ©é˜µã€‚\\n> *   2. **ç‰¹å¾åˆ†è§£ï¼š** å¯¹$L$è¿›è¡Œç‰¹å¾åˆ†è§£ï¼Œå¾—åˆ°ç‰¹å¾å€¼çŸ©é˜µ$\\\\Lambda$å’Œç‰¹å¾å‘é‡çŸ©é˜µ$U$ã€‚\\n> *   3. **é€‰æ‹©é«˜é¢‘ç‰¹å¾å€¼ï¼š** æ ¹æ®é¢‘ç‡æ¯”ä¾‹$r_f$ï¼Œé€‰æ‹©å$N_a = N \\\\times r_f$ä¸ªé«˜é¢‘ç‰¹å¾å€¼è¿›è¡Œä¿®æ”¹ã€‚\\n> *   4. **å¢å¼ºæ“ä½œï¼š** å¯¹äºDP-Noiseï¼Œå‘é€‰å®šçš„é«˜é¢‘ç‰¹å¾å€¼æ·»åŠ å™ªå£°ï¼›å¯¹äºDP-Maskï¼Œç›´æ¥å¯¹é€‰å®šçš„é«˜é¢‘ç‰¹å¾å€¼è¿›è¡Œæ©ç æ“ä½œã€‚\\n> *   5. **é‡æ„æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µï¼š** ä½¿ç”¨ä¿®æ”¹åçš„ç‰¹å¾å€¼çŸ©é˜µ$\\\\hat{\\\\Lambda}$å’ŒåŸå§‹ç‰¹å¾å‘é‡çŸ©é˜µ$U$é‡æ„æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ$\\\\hat{L} = U \\\\hat{\\\\Lambda} U^\\\\top$ã€‚\\n> *   6. **ç”Ÿæˆå¢å¼ºå›¾ï¼š** ä»$\\\\hat{L}$ä¸­å¯¼å‡ºæ–°çš„é‚»æ¥çŸ©é˜µ$\\\\hat{A}$ï¼Œå¹¶ä¿ç•™åŸå§‹å›¾çš„æ ‡ç­¾å’Œç‰¹å¾ã€‚\\n\\n> **æ¡ˆä¾‹è§£æ (Case Study)**\\n> *   è®ºæ–‡é€šè¿‡ä¸€ä¸ªIMDB-Bæ•°æ®é›†çš„æ¡ˆä¾‹å±•ç¤ºäº†DPæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚åŸå§‹å›¾ç»è¿‡DropEdgeå¢å¼ºåï¼Œå…¶å…³é”®å±æ€§ï¼ˆå¦‚ç›´å¾„ã€è¿é€šæ€§ï¼‰å‘ç”Ÿäº†æ˜¾è‘—å˜åŒ–ã€‚è€ŒDP-Noiseå’ŒDP-Maskç”Ÿæˆçš„å¢å¼ºå›¾åˆ™ä¿æŒäº†åŸå§‹å›¾çš„å…³é”®å±æ€§ã€‚\",\n  \"comparative_analysis\": \"### ğŸ“Š å¯¹æ¯”å®éªŒåˆ†æ\\n\\n> **åŸºçº¿æ¨¡å‹ (Baselines)**\\n> *   è®ºæ–‡å¯¹æ¯”äº†å¤šç§åŸºçº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬DropEdgeã€DropNodeã€Subgraphã€M-Mixupã€SubMixã€G-Mixupå’ŒS-Mixupã€‚\\n\\n> **æ€§èƒ½å¯¹æ¯” (Performance Comparison)**\\n> *   **åœ¨å‡†ç¡®ç‡ä¸Šï¼š** åœ¨IMDB-Bæ•°æ®é›†ä¸Šï¼ŒDP-Noiseçš„å‡†ç¡®ç‡è¾¾åˆ°77.9%ï¼Œæ˜¾è‘—ä¼˜äºè¡¨ç°æœ€ä½³çš„åŸºçº¿S-Mixupï¼ˆ74.4%ï¼‰ï¼Œæå‡äº†3.5ä¸ªç™¾åˆ†ç‚¹ã€‚\\n> *   **åœ¨ç¨³å®šæ€§ä¸Šï¼š** DP-Noiseå’ŒDP-Maskçš„è®­ç»ƒæŸå¤±æ›²çº¿æ¯”G-Mixupå’ŒVanillaæ›´ç¨³å®šï¼Œè¡¨æ˜å…¶å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚\\n> *   **åœ¨åŠç›‘ç£å­¦ä¹ ä¸Šï¼š** åœ¨1%æ ‡ç­¾æ¯”ä¾‹ä¸‹ï¼ŒDP-Noiseåœ¨NCI1æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º63.43%ï¼Œä¼˜äºåŸºçº¿GraphCLï¼ˆ62.55%ï¼‰å’ŒInfomaxï¼ˆ62.72%ï¼‰ã€‚\\n> *   **åœ¨æ— ç›‘ç£å­¦ä¹ ä¸Šï¼š** DP-Noiseåœ¨NCI1æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º79.69%ï¼Œä¼˜äºåŸºçº¿GraphCLï¼ˆ77.87%ï¼‰å’ŒGCL-SPANï¼ˆ71.43%ï¼‰ã€‚\",\n  \"keywords\": \"### ğŸ”‘ å…³é”®è¯\\n\\n*   å›¾æ•°æ®å¢å¼º (Graph Data Augmentation, GDA)\\n*   å›¾ç¥ç»ç½‘ç»œ (Graph Neural Network, GNN)\\n*   è°±å›¾ç†è®º (Spectral Graph Theory, N/A)\\n*   æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ (Laplacian Matrix, N/A)\\n*   ç›‘ç£å­¦ä¹  (Supervised Learning, SL)\\n*   åŠç›‘ç£å­¦ä¹  (Semi-Supervised Learning, SSL)\\n*   æ— ç›‘ç£å­¦ä¹  (Unsupervised Learning, UL)\\n*   è¿ç§»å­¦ä¹  (Transfer Learning, TL)\"\n}\n```"
}