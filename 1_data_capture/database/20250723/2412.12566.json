{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.12566",
    "link": "https://arxiv.org/abs/2412.12566",
    "pdf_link": "https://arxiv.org/pdf/2412.12566.pdf",
    "title": "ITP: Instance-Aware Test Pruning for Out-of-Distribution Detection",
    "authors": [
        "Haonan Xu",
        "Yang Yang"
    ],
    "categories": [
        "cs.CV"
    ],
    "publication_date": "2024-12-17",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 2,
    "influential_citation_count": 0,
    "institutions": [
        "Nanjing University of Science and Technology",
        "Pazhou Lab"
    ],
    "paper_content": "# ITP: Instance-Aware Test Pruning for Out-of-Distribution Detection\n\nHaonan ${ \\bf X } { \\bf u } ^ { 1 }$ , Yang Yang1,2\\*\n\n1Nanjing University of Science and Technology 2Pazhou Lab, Guangzhou {xhnxhn, yyang}@njust.edu.cn\n\n# Abstract\n\nOut-of-distribution (OOD) detection is crucial for ensuring the reliable deployment of deep models in real-world scenarios. Recently, from the perspective of over-parameterization, a series of methods leveraging weight sparsification techniques have shown promising performance. These methods typically focus on selecting important parameters for indistribution (ID) data to reduce the negative impact of redundant parameters on OOD detection. However, we empirically find that these selected parameters may behave overconfidently toward OOD data and hurt OOD detection. To address this issue, we propose a simple yet effective post-hoc method called Instance-aware Test Pruning (ITP), which performs OOD detection by considering both coarse-grained and finegrained levels of parameter pruning. Specifically, ITP first estimates the class-specific parameter contribution distribution by exploring the ID data. By using the contribution distribution, ITP conducts coarse-grained pruning to eliminate redundant parameters. More importantly, ITP further adopts a finegrained test pruning process based on the right-tailed Z-score test, which can adaptively remove instance-level overconfident parameters. Finally, ITP derives OOD scores from the pruned model to achieve more reliable predictions. Extensive experiments on widely adopted benchmarks verify the effectiveness of ITP, demonstrating its competitive performance.\n\n# 1 Introduction\n\nDeep neural networks (DNNs) have recently achieved remarkable success, driving significant progress in various fields, particularly in computer vision (Dosovitskiy et al. 2021; Yang et al. 2021, 2023a) and natural language processing (Radford et al. 2018; Achiam et al. 2023). However, when deployed in open-world environments, DNNs may fail by producing confident yet erroneous predictions for OOD data. Such unreliable behavior could lead to disastrous consequences, particularly in safety-critical fields like autonomous driving (Geiger, Lenz, and Urtasun 2012) and medical diagnosis (Litjens et al. 2017). Therefore, detecting and rejecting predictions on OOD inputs is crucial for ensuring the reliability of AI systems. This task, referred to as OOD detection, has gained widespread attention.\n\nParameter contribution distribution 1.0 ID: CIFAR-10 (airplane) 0.8 OOD: Textures Parameter #287 0.6 0.05 0.00 2 4 6 0.2 Overconfidence ■ 0.0 2 0 2 4 6 8 Contribution\n\nMany techniques have been developed for OOD detection to enhance the discrimination between ID and OOD data. The training-based methods (Malinin and Gales 2018; Monteiro et al. 2023; Ghosal, Sun, and Li 2024) necessitate model training or fine-tuning, whereas post-hoc methods (Bendale and Boult 2016; Sun et al. 2022; Wang et al. 2022; Yang et al. 2023b, 2024) can be applied directly to pre-trained models off-the-shelf, eliminating the need for retraining process. This paper primarily focuses on post-hoc methods, which are easy to use, low-cost, and generally applicable. Early post-hoc methods, such as MSP (Hendrycks and Gimpel 2017), Energy (Liu et al. 2020), and GradNorm (Huang, Geng, and Li 2021), focus on devising a suitable OOD scoring function based on model outputs or gradients to indicate the likelihood that a sample originates from the OOD distribution. However, these methods overlook the fact that DNNs are typically over-parameterized to fit complex data distributions. This design makes the model more susceptible to noise from redundant parameters (Sun and Li 2022), leading to the brittleness of OOD detection.\n\nTo address this issue, a series of post-hoc methods that use network adjustments to improve OOD scoring has emerged. Sparsification-based methods, represented by DICE (Sun and Li 2022) and LINe (Ahn, Park, and Kim 2023), provide effective solutions for model over-parameterization. Their key idea is to selectively use the weight parameters that are important for ID data to derive OOD scores, thereby reducing the noise interference caused by redundant parameters in OOD detection. However, as illustrated in Figure 1, our empirical findings reveal that these selected important parameters are not always beneficial for OOD detection. When processing OOD data, these parameters may contribute abnormally high to ID predictions, behaving overconfidently. Such overconfidence increases the risk of the model predicting OOD data as ID categories with high confidence. As a result, the derived OOD scores become unreliable, leading to confusion between ID and OOD data and hindering OOD detection. Therefore, addressing parameter-level overconfidence is crucial for better separating ID and OOD data.\n\nTargeting this important problem, we propose Instanceaware Test Pruning (ITP), a simple yet effective method for OOD detection that considers parameter pruning from both coarse-grained and fine-grained perspectives. Concretely, ITP first estimates the class-specific parameter contribution distribution by exploring the ID data. Then, by leveraging the contribution distribution, ITP considers the following two key points to improve OOD detection performance: (1) ITP conducts coarse-grained pruning to remove noise interference caused by redundant parameters based on DICE (Sun and Li 2022). (2) ITP adopts a fine-grained test pruning process based on the right-tailed Z-score test, which adaptively removes instance-level overconfident parameters to reduce the risk of the model making confident yet erroneous predictions. We further provide insightful justification of the working mechanism of ITP from the perspective of the OOD score distribution. As a result of ITP, we show that the OOD scores derived from the model are more reliable and become more separable between ID and OOD data. Moreover, by examining parameter behavior in the weight space, ITP operates orthogonally to activation-based OOD detection methods (e.g., ReAct (Sun, Guo, and Li 2021)), facilitating their integration to push ITP’s performance further.\n\n# 2 Related Work\n\nOOD detection aims to enable models to identify and reject predictions for OOD inputs, thereby ensuring the reliability of AI systems. We highlight three major lines of work.\n\nOOD Scoring Methods are designed to provide appropriate criteria for indicating the likelihood that an input sample is OOD. Distance-based (Lee et al. 2018; Huang et al. 2021; Sun et al. 2022) methods identify OOD data as being farther from the training set compared to ID data. Gradient-based methods (Huang, Geng, and Li 2021; Behpour et al. 2023) detect OOD inputs by utilizing information extracted from the gradient space. Output-based methods rely on model output logits to identify OOD data. MSP (Hendrycks and Gimpel 2017) directly uses the maximum SoftMax score to classify a test sample as either ID or OOD. ODIN (Liang, Li, and Srikant 2018) improves the MSP score by perturbing the input and applying temperature scaling to the logits. Energy score (Liu et al. 2020) uses the logsumexp of the output logits, which is consistent with input density and less susceptible to overconfidence problems. However, output-based methods are often disrupted by redundant or overconfident parameters, which can negatively impact OOD detection.\n\nSparsification-Based Methods perform OOD detection by pruning the weights of the model. DICE (Sun and Li 2022) proposes selectively using the most salient weights to derive the output for OOD detection. LINe (Ahn, Park, and Kim 2023) adopts the Shapley value (Shapley et al. 1953) for more precise pruning of redundant parameters and neurons, and it further considers the number of activated features by clipping activations. OPNP (Chen et al. 2023) prunes the parameters and neurons with exceptionally large or nearly zero sensitivities to mitigate over-fitting. These methods typically focus on selecting parameters that are important for ID prediction before testing for OOD detection. However, at test time, these selected parameters may exhibit overconfidence, which can impact the performance of OOD detection.\n\nActivation-Based Methods attempt to rectify activations to widen the gap between ID and OOD data. ReAct (Sun, Guo, and Li 2021) truncates activations above a pre-computed threshold to treat all activated features equally, thereby incorporating the number of activated features into consideration for OOD detection. VRA (Xu et al. 2023) zeros out anomalously low activations and truncates anomalously high activations. BATS (Zhu et al. 2022) proposes rectifying activations towards their typical set, while LAPS (He et al. 2024) improves BATS by considering channel-aware typical sets. These methods only examine anomalies at the activation level, whereas managing overconfidence anomalies at a more granular parameter level is important for more effective OOD detection.\n\n# 3 Proposed Method\n\n# 3.1 Preliminaries\n\nSetup. In this paper, we follow previous work (Yang et al. 2022) and focus on the setting of $K$ -way image classification. Let $\\mathcal { X }$ be the input space and $\\bar { \\mathcal { V } } \\bar { = } \\{ \\bar { 1 } , 2 , . . . , K \\}$ be the ID label space. Suppose that the training set $\\mathcal { D } =$ $\\{ ( \\mathbf { x } _ { i } , y _ { i } ) \\} _ { i = 1 } ^ { n }$ is drawn i.i.d from a joint distribution $\\mathcal { P } _ { \\mathcal { X } \\mathcal { Y } }$ defined over $\\boldsymbol { \\mathcal { X } } \\times \\boldsymbol { \\mathcal { Y } }$ . We denote $\\mathcal { P } _ { i n }$ as the marginal distribution of $\\mathcal { P } _ { \\mathcal { X } \\mathcal { Y } }$ on $\\chi$ , representing the ID distribution.\n\nLet $f$ be a model pre-trained from $\\mathcal { D }$ . For typical image classification architectures, $f$ first extracts a $D$ -dimensional penultimate feature representation $h ( \\mathbf { x } ) \\in \\mathbb { R } ^ { D }$ from an input $\\mathbf { x } \\in \\mathcal { X }$ . The last fully connected (FC) layer, parameterized by a weight matrix $\\dot { \\mathbf { W } } \\in \\mathbb { R } ^ { D \\times K }$ and a bias vector $\\mathbf { b } \\in \\mathbb { R } ^ { K }$ , then maps $h ( \\mathbf { x } )$ to the output vector $f ( \\mathbf { x } ) \\in \\mathbb { R } ^ { K }$ . Mathematically, the model output can be expressed as:\n\n$$\nf ( \\mathbf { x } ) = \\mathbf { W } ^ { \\top } h ( \\mathbf { x } ) + \\mathbf { b } .\n$$\n\nOut-of-distribution Detection. The goal of OOD detection is to determine whether a test input $\\mathbf { x }$ is from $\\mathcal { P } _ { i n }$ (ID) or not (OOD). In practice, the OOD detection task is often formu\n\n![](images/c1be637228527774c454490b338856fea183164987cd91484ea6182c6e10d93f.jpg)  \nFigure 2: Illustration of OOD detection using ITP. The overall procedure involves three main steps. (1) Training data are used to estimate the class-specific parameter contribution distribution for a pre-trained model. (2) Coarse-grained redundancy pruning applies a fixed pruning pattern to the model’s last layer to remove redundant parameters. (3) Fine-grained test pruning applies a customized pruning pattern to remove overconfident parameters for each test sample at test time. After applying ITP, the OOD scores derived from the model are better able to distinguish between ID and OOD data.\n\nlated as the following binary decision problem:\n\n$$\nG ( \\mathbf { x } ) = \\left\\{ \\begin{array} { l l } { \\mathrm { I D } , } & { \\mathrm { i f } \\ S ( \\mathbf { x } ) > \\tau , } \\\\ { \\mathrm { O O D } , } & { \\mathrm { i f } \\ S ( \\mathbf { x } ) \\le \\tau , } \\end{array} \\right.\n$$\n\nwhere $S ( \\cdot )$ represents the OOD scoring function, and $\\tau$ is a chosen threshold to ensure that the majority of $\\mathrm { I D }$ data are correctly classified (e.g., $9 5 \\%$ ). By convention, samples with higher OOD scores are heuristically classified as ID and vice versa. Given that the energy score (Liu et al. 2020) has been proven to be consistent with the input density and performs well, we mainly adopt the negative energy score as the OOD score, expressed as:\n\n$$\nS ( \\mathbf { x } ) = - E \\left( \\mathbf { x } \\right) = \\log \\sum _ { k = 1 } ^ { K } \\exp ( f _ { k } ( \\mathbf { x } ) ) ,\n$$\n\nwhere $E ( \\mathbf { x } )$ denotes the energy of $\\mathbf { x }$ , and $f _ { k } ( { \\bf x } )$ represents the $k$ -th output of the model.\n\n# 3.2 Parameter Contribution Distribution Estimation\n\nFigure 2 illustrates the overall procedure of our proposal. In this section, we first provide a detailed description of how to estimate the parameter contribution distribution, which will guide subsequent parameter pruning.\n\nDefining the Parameter Contribution. For a given input $\\mathbf { x }$ , the contribution of a specific parameter $\\pmb { \\theta } _ { i j }$ to the category $k$ is defined as the change in the $k$ -th output of the model by the presence or absence (setting $\\pmb { \\theta } _ { i j }$ to $0$ ) of the parameter $\\pmb { \\theta } _ { i j }$ , i.e.,\n\n$$\nc _ { k } ( \\mathbf { x } ; \\pmb { \\theta } _ { i j } ) = f _ { k } ( \\mathbf { x } ) - f _ { k } ( \\mathbf { x } ; \\pmb { \\theta } _ { i j } = 0 ) .\n$$\n\nPrevious studies (Zhu et al. 2022) highlight that the features extracted by early layers show similarities between ID and OOD data. In contrast, the later layers, particularly the penultimate layer, can extract more separable features. In this paper, we primarily focus on the last layer’s model parameters, which significantly impact OOD detection by processing separable features and directly influencing particular class outputs. Especially, the contributions of the last layer’s parameters $\\mathbf { W } _ { i j }$ can be expressed more simply using Equation 4 as follows (see Appendix for details):\n\n$$\nc _ { k } ( \\mathbf { x } ; \\mathbf { W } _ { i j } ) = { \\left\\{ \\begin{array} { l l } { \\mathbf { W } _ { i j } \\cdot h _ { i } ( \\mathbf { x } ) , } & { { \\mathrm { i f ~ } } k = j , } \\\\ { 0 , } & { { \\mathrm { i f ~ } } k \\neq j . } \\end{array} \\right. }\n$$\n\n# Estimating the Distribution of Parameter Contribution.\n\nThe distribution estimation relies on the assumption that the contribution of the last layer’s parameters approximately follows Gaussian distributions parameterized by $( \\mu , \\sigma )$ , as observed empirically (see Appendix). According to the Equation 5, the parameter $\\mathbf { W } _ { i j }$ is specifically associated with class $j$ . To minimize potential bias from including data from other classes, we estimate the contribution distribution of parameter $\\mathbf { W } _ { i j }$ using only the training data for class $j$ . Let $\\mathcal { D } _ { j }$ denote the set of data points belonging to class $j$ . The mean $\\mu _ { i j }$ and standard deviation $\\sigma _ { i j }$ of the contribution distribution for the parameter $\\mathbf { W } _ { i j }$ are estimated using the following\n\nclass-specific formulas:\n\n$$\n\\begin{array} { l } { \\displaystyle \\mu _ { i j } = \\frac { 1 } { | \\mathcal { D } _ { j } | } \\sum _ { x \\in \\mathcal { D } _ { j } } c _ { j } ( x ; \\mathbf { W } _ { i j } ) , } \\\\ { \\displaystyle \\sigma _ { i j } = \\left( \\frac { 1 } { | \\mathcal { D } _ { j } | - \\delta } \\sum _ { x \\in \\mathcal { D } _ { j } } \\left( c _ { j } ( x ; \\mathbf { W } _ { i j } ) - \\mu _ { i j } \\right) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } , } \\end{array}\n$$\n\nwhere $| \\mathcal { D } _ { j } |$ denotes the cardinality of the set $\\mathcal { D } _ { j }$ , and $\\delta$ represents the correction factor. To correct the bias in the estimation of the population standard deviation, we adopt Bessel’s correction by setting $\\delta$ to 1.\n\n# 3.3 Instance-Aware Test Pruning (ITP)\n\nIn this section, we introduce two parameter pruning strategies with different levels of granularity used in ITP for posthoc enhancement in OOD detection: coarse-grained redundancy pruning (Figure 2(2)) and fine-grained test pruning (Figure 2(3)). Detailed descriptions of each strategy are provided below.\n\nCoarse-Grained Redundancy Pruning (CRP) remove redundant parameters in the over-parameterized weight space of the model. CRP operates at a coarse-grained level by applying a uniform pruning pattern across all test samples. Specifically, CRP measures each parameter’s redundancy based on its average contribution to ID prediction. Parameters that fall within the lowest $p \\%$ of average contributions are deemed redundant and pruned. To implement this, we define a mask matrix $\\mathbf { M } ^ { \\mathrm { C R P } }$ for CRP, where the average contribution of a parameter is directly obtained from the mean $\\mu$ of its contribution distribution. The $( i , j )$ -th entry of the mask matrix $\\mathbf { M } _ { i j } ^ { \\mathrm { C R P } } \\in \\mathbf { M } ^ { \\mathrm { C R P } }$ is defined as follows:\n\n$$\n\\mathbf { M } _ { i j } ^ { \\mathrm { C R P } } = \\left\\{ \\begin{array} { l l } { 1 , } & { \\mathrm { i f } \\quad \\mu _ { i j } > \\Omega _ { p } , } \\\\ { 0 , } & { \\mathrm { i f } \\quad \\mu _ { i j } \\le \\Omega _ { p } , } \\end{array} \\right.\n$$\n\nwhere $\\Omega _ { p }$ represents the average contribution threshold at the lowest $p$ percentile. The model output after applying CRP can be expressed as follows:\n\n$$\nf ^ { \\mathrm { C R P } } ( \\mathbf { x } ) = \\left( \\mathbf { W } \\odot \\mathbf { M } ^ { \\mathrm { C R P } } \\right) ^ { \\top } h ( \\mathbf { x } ) + \\mathbf { b } ,\n$$\n\nwhere $\\odot$ denotes the element-wise multiplication. Through CRP, we remove noise interference from redundant parameters at a coarse-grained level in OOD detection by retaining only the parameters important for ID data, thereby enhancing the distinction between ID and OOD data.\n\nFine-Grained Test Pruning (FTP) prunes overconfident parameters with anomalously high contributions to ID prediction. FTP operates at a fine-grained level by customizing parameter pruning patterns for each test sample at test time. Specifically, FTP determines whether the parameter is overconfident by performing a right-tail test based on the $Z \\cdot$ - score. The $Z$ -score quantifies the deviation of a data point from the mean of the distribution, expressed as $( X - \\mu ) / \\sigma$ . In this context, $X$ represents the contribution being evaluated, while $\\mu$ and $\\sigma$ denote the mean and standard deviation of the contribution distribution, respectively. FTP can be framed as a single-sample hypothesis testing task:\n\n$$\n\\mathcal { H } _ { 0 } : \\frac { X - \\mu } { \\sigma } \\leq \\lambda , \\quad \\mathrm { v s . } \\quad \\mathcal { H } _ { 1 } : \\frac { X - \\mu } { \\sigma } > \\lambda ,\n$$\n\nID: ImageNet-1k ID: ImageNet-1k ID: ImageNet-1k OOD: iNaturalist OOD: iNaturalist OOD: iNaturalist   \nFPR95: 55.72% FPR95: 23.06% FPR95: 11.53% Energy CRP ITP (CRP + FTP) (a) ITP on iNaturalist benchmark ID: ImageNet-1k ID: ImageNet-1k ID: ImageNet-1k OOD: Textures OOD: Textures OOD: Textures   \nFPR95: 53.72% FPR95: 33.71% FPR95: $\\mathbf { 1 7 . 0 6 \\% }$ Energy CRP ITP (CRP + FTP) (b) ITP on Textures benchmark\n\nwhere the alternative hypothesis $\\mathcal { H } _ { 1 }$ implies that the parameter behaves overconfidently, and $\\lambda$ is the threshold, with $\\lambda > 0$ . In practice, we define ${ { \\bf { M } } ^ { \\mathrm { { F T P } } } } ( { \\bf { x } } )$ as the mask matrix customized for $\\mathbf { x }$ to perform FTP. The $( i , j )$ -th entry of the mask matrix $\\mathbf { M } _ { i j } ^ { \\mathrm { F T P } } ( \\bar { \\mathbf { x } } ) \\in \\mathbf { M } ^ { \\mathrm { F T P } } ( \\mathbf { x } )$ is defined as follows:\n\n$$\n\\mathbf { M } _ { i j } ^ { \\mathrm { F T P } } ( \\mathbf { x } ) = \\left\\{ \\begin{array} { l l } { 1 , } & { \\mathrm { i f } \\quad \\frac { c _ { j } ( \\mathbf { x } ; \\mathbf { W } _ { i j } ) - \\mu _ { i j } } { \\sigma _ { i j } } \\leq \\lambda , } \\\\ { 0 , } & { \\mathrm { i f } \\quad \\frac { c _ { j } ( \\mathbf { x } ; \\mathbf { W } _ { i j } ) - \\mu _ { i j } } { \\sigma _ { i j } } > \\lambda . } \\end{array} \\right.\n$$\n\nThe model output after applying FTP can be expressed as:\n\n$$\nf ^ { \\mathrm { F T P } } ( \\mathbf { x } ) = \\left( \\mathbf { W } \\odot \\mathbf { M } ^ { \\mathrm { F T P } } ( \\mathbf { x } ) \\right) ^ { \\top } h ( \\mathbf { x } ) + \\mathbf { b } .\n$$\n\nThrough FTP, we can adaptively prevent the abnormal increase in ID confidence caused by anomalously high contributions from overconfident parameters. This effectively reduces the risk of the model making confident yet erroneous predictions, thereby making ID data and OOD data more distinguishable.\n\nOverall Methods. Both CRP and FTP pruning strategies are designed to remove parameters that negatively impact OOD detection. CRP utilizes a fixed, coarse-grained pruning pattern across all test samples to reduce interference from noisy signals. FTP applies a customized, fine-grained pruning pattern to overconfident parameters for each test sample, mitigating the risk of overconfident predictions. ITP achieves both ways to improve OOD detection. As a result, the model outputs using ITP can be expressed as follows:\n\n$$\nf ^ { \\mathrm { I T P } } ( \\mathbf { x } ) = \\left( \\mathbf { W } \\odot \\mathbf { M } ^ { \\mathrm { C R P } } \\odot \\mathbf { M } ^ { \\mathrm { F T P } } ( \\mathbf { x } ) \\right) ^ { \\top } h ( \\mathbf { x } ) + \\mathbf { b } .\n$$\n\nSimilar to previous works (Ahn, Park, and $\\mathrm { K i m } \\ 2 0 2 3$ ), we can always use the original FC layer for prediction to preserve ID accuracy with negligible additional overhead.\n\n# 3.4 Insight Justification\n\nThe following remarks are provided to further explain how ITP widens the gap between ID and OOD data. Remark 1. CRP enhances the disparity between the left tail of the OOD score distributions for ID and OOD data. After employing CRP to prune redundant parameters, the logits reduction for the $k$ -th class is given by:\n\n$$\n\\Delta f _ { j } ( \\mathbf { x } ) = \\sum _ { d = 1 } ^ { D } ( 1 - \\mathbf { M } _ { d j } ^ { \\mathrm { C R P } } ) \\cdot c _ { j } ( \\mathbf { x } ; \\mathbf { W } _ { d j } ) .\n$$\n\nCRP eliminates parameters with the least average contribution to the $\\mathrm { I D }$ distribution. Hence, redundant parameters generally have higher contributions (manifesting as noise) to ID prediction for OOD data compared to ID data, i.e.,\n\n$$\n\\sum _ { \\mathbf { M } _ { d j } ^ { \\mathrm { C R P } } = 0 } c _ { j } ( \\mathbf { x } ^ { \\mathrm { O O D } } ; \\mathbf { W } _ { d j } ) > \\sum _ { \\mathbf { M } _ { d j } ^ { \\mathrm { C R P } } = 0 } c _ { j } ( \\mathbf { x } ^ { \\mathrm { I D } } ; \\mathbf { W } _ { d j } ) .\n$$\n\nTherefore, the reduction in logits for OOD data is greater than that for ID data $\\Delta f _ { j } ( \\mathbf { x } ^ { \\mathrm { { O O D } } } ) \\ > \\ \\Delta f _ { j } ( \\mathbf { x } ^ { \\mathrm { { I D } } } )$ . This improves the differentiation between the left tail of the energy score distributions for ID and OOD data, due to the positive correlation between energy scores and logits. This effect is empirically validated in Figure 3.\n\nRemark 2. FTP alleviates the overconfidence of OOD data at the right tail of the OOD score distribution. FTP removes the abnormally high contributions caused by overconfident parameters, thereby increasing the left skewness in the parameter contribution distribution, i.e.,\n\n$$\n\\begin{array} { r l } & { \\mathbb { E } _ { \\mathbf { x } } \\left[ \\left( \\frac { c _ { j } \\left( \\mathbf { x } ; \\mathbf { W } _ { i j } \\right) - \\mu _ { i j } } { \\sigma _ { i j } } \\right) ^ { 3 } \\cdot \\mathbf { M } _ { i j } ^ { \\mathrm { F T P } } ( \\mathbf { x } ) \\right] } \\\\ & { \\qquad < \\mathbb { E } _ { \\mathbf { x } } \\left[ \\left( \\frac { c _ { j } \\left( \\mathbf { x } ; \\mathbf { W } _ { i j } \\right) - \\mu _ { i j } } { \\sigma _ { i j } } \\right) ^ { 3 } \\right] . } \\end{array}\n$$\n\nSince parameter contributions directly determine the energy score, the left skewness of the energy score distribution will also increase accordingly. This helps reduce the risk of parameters being overly confident when handling OOD data. As a result, the overlap between the right tail of the OOD energy score distribution and the ID energy score distribution is diminished, as illustrated in Figure 3.\n\n# 4 Experiments\n\nIn this section, we first describe our experimental setup, then present the main results on multiple OOD detection benchmarks, followed by ablation studies and further analysis.\n\n# 4.1 Experimental Setup\n\nIn line with other OOD literature (Sun and Li 2022), we evaluate our methods both on the small-scale CIFAR benchmarks and the large-scale ImageNet benchmark1. Moreover, we provide a further evaluation of our proposal on the OpenOOD v1.5 benchmark (Zhang et al. 2023) in the appendix. We default to using the entire training set for estimating the parameter contribution distribution.\n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">CIFAR-10</td><td rowspan=\"2\">CIFAR-100</td></tr><tr><td>FPR95AUROCFPR95AUROC</td><td></td></tr><tr><td>MSP</td><td>√</td><td>个</td><td>√ 80.13</td></tr><tr><td>Energy</td><td>48.73 26.55</td><td>92.46</td><td>74.36 81.19</td></tr><tr><td>ODIN</td><td>24.57</td><td>94.57</td><td>68.45 58.14 84.49</td></tr><tr><td>ReAct</td><td>26.45</td><td>93.71 94.67</td><td>62.27 84.47</td></tr><tr><td>DICE</td><td>20.83</td><td>95.24</td><td>49.72 87.23</td></tr><tr><td>OPNP</td><td>22.07</td><td>95.14</td><td>51.79 87.20</td></tr><tr><td>LAPS</td><td>19.40</td><td></td><td>50.50 88.07</td></tr><tr><td>ITP (Ours)</td><td>16.72</td><td>96.10</td><td>35.03 91.39</td></tr><tr><td>DICE+ReAct</td><td>16.48</td><td>96.64</td><td>49.57 85.07</td></tr><tr><td>OPNP+ReAct</td><td></td><td>96.64</td><td></td></tr><tr><td>LINe (w/ ReAct)</td><td>18.46</td><td>96.35 42.98</td><td>88.55</td></tr><tr><td></td><td>14.72</td><td>96.99</td><td>35.67 88.67</td></tr><tr><td>ITP + ReAct (Ours)</td><td>14.50</td><td>97.13</td><td>30.13 91.91</td></tr></table></body></html>\n\nTable 1: OOD detection performance on CIFAR benchmarks with DenseNet-101 as the backbone. All values in the table are averaged over six OOD test datasets and are percentages. The best results are in bold. $\\uparrow$ indicates that larger values are better, while $\\downarrow$ indicates that smaller values are better. Detailed results for each OOD dataset are provided in the Appendix.\n\nCIFAR. We use CIFAR-10 and CIFAR-100 (Krizhevsky 2009) as ID datasets and consider six OOD datasets: SVHN (Netzer et al. 2011), Textures (Cimpoi et al. 2014), iSUN (Xu et al. 2015), LSUN-Resize (Yu et al. 2015), LSUN-Crop (Yu et al. 2015), and Places365 (Zhou et al. 2018). For consistency with previous work (Sun and Li 2022), we use the same model architecture and pre-trained weights, namely DenseNet-101 (Huang et al. 2017).\n\nImageNet. For the large-scale ImageNet experiments, we use the ImageNet-1k as the ID dataset and consider (subsets of) iNaturalist (Horn et al. 2018), Places (Zhou et al. 2018), SUN (Xiao et al. 2010), and Textures (Cimpoi et al. 2014) with non-overlapping categories from ImageNet-1k as OOD datasets. We adopt the widely used ResNet-50 (He et al. 2016) model architectures, and we obtain the pre-trained weights from the torchvision library.\n\nBaselines. We compare ITP with the most competitive OOD detection methods: MSP (Hendrycks and Gimpel 2017), Energy (Liu et al. 2020), ODIN (Liang, Li, and Srikant 2018), ReAct (Sun, Guo, and Li 2021), DICE (Sun and Li 2022), LINe (Ahn, Park, and Kim 2023), OPNP (Chen et al. 2023), and LAPS (He et al. 2024). Moreover, to align with standard sparsification practices, we also report the results of a comparison with ReAct (e.g., $\\mathrm { I T P + R e A c t } ,$ ). In particular, LINe integrates ReAct within its framework, which we refer to as “LINE (w/ ReAct)” in the table. All methods are post-hoc and can be directly applied to pre-trained models.\n\nEvaluation Metric. We adopt two threshold-free metrics for evaluation. FPR95: the false positive rate of OOD data at $9 5 \\%$ true positive rate of ID data. AUROC: the area under the receiver operating characteristic curve.\n\nTable 2: OOD detection performance on ImageNet with ResNet-50 as the backbone.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"8\">OOD Datasets</td><td rowspan=\"2\">Average</td></tr><tr><td colspan=\"2\">iNaturalist</td><td colspan=\"2\">SUN</td><td colspan=\"2\">Places</td><td colspan=\"2\">Textures</td></tr><tr><td></td><td>FPR95</td><td>AUROC</td><td>FPR95</td><td>AUROC</td><td>FPR95AUROC</td><td></td><td>FPR95 AUROC</td><td>FPR95</td><td>AUROC</td></tr><tr><td></td><td>√</td><td>个</td><td>√</td><td>个</td><td>√</td><td></td><td>√ 个</td><td>√</td><td>个</td></tr><tr><td>MSP</td><td>54.99</td><td>87.74</td><td>70.83</td><td>80.86</td><td>73.99</td><td>79.76 68.00</td><td>79.61</td><td>66.95</td><td>81.99</td></tr><tr><td>Energy</td><td>55.72</td><td>89.95</td><td>59.26</td><td>85.89</td><td>64.92</td><td>82.86 53.72</td><td>85.99</td><td>58.41</td><td>86.17</td></tr><tr><td>ODIN</td><td>47.66</td><td>89.66</td><td>60.15</td><td>84.59</td><td>67.89</td><td>81.78 50.23</td><td>85.62</td><td>56.48</td><td>85.41</td></tr><tr><td>ReAct</td><td>20.38</td><td>96.22</td><td>24.20</td><td>94.20</td><td>33.85</td><td>91.58</td><td>47.30 89.80</td><td>31.43</td><td>92.95</td></tr><tr><td>DICE</td><td>25.63</td><td>94.49</td><td>35.15</td><td>90.83</td><td>46.49</td><td>87.48</td><td>31.72 90.30</td><td>34.75</td><td>90.77</td></tr><tr><td>OPNP</td><td>18.89</td><td>96.03</td><td>18.50</td><td>95.62</td><td>30.14</td><td>93.46</td><td>36.17 91.70</td><td>25.93</td><td>94.20</td></tr><tr><td>LAPS</td><td>12.72</td><td>97.50</td><td>15.81</td><td>96.18</td><td>24.71</td><td>93.64</td><td>41.49 91.81</td><td>23.68</td><td>94.78</td></tr><tr><td>ITP (Ours)</td><td>11.53</td><td>97.83</td><td>25.82</td><td>93.58</td><td>35.63</td><td>90.75</td><td>17.06 96.03</td><td>22.51</td><td>94.55</td></tr><tr><td>DICE + ReAct</td><td>18.64</td><td>96.24</td><td>25.45</td><td>93.94 95.65</td><td>36.86 30.23</td><td>90.67 93.34</td><td>28.07 92.74</td><td>27.25</td><td>93.40</td></tr><tr><td>OPNP + ReAct</td><td>14.72</td><td>96.78</td><td>19.73</td><td></td><td>28.52</td><td>27.78</td><td>94.13 94.44</td><td>23.12</td><td>94.98</td></tr><tr><td>LINe (w/ ReAct)</td><td>12.26</td><td>97.56</td><td>19.48</td><td>95.26</td><td></td><td>92.85</td><td>22.54</td><td>20.70</td><td>95.03</td></tr><tr><td>ITP + ReAct (Ours)</td><td>9.78</td><td>98.02</td><td>22.82</td><td>94.47</td><td>30.87</td><td>92.03</td><td>18.09 95.98</td><td>20.39</td><td>95.13</td></tr></table></body></html>\n\nTable 3: Ablation study for our proposed method. All values are percentages and averaged over multiple OOD datasets.   \n\n<html><body><table><tr><td>Dataset</td><td>CRP</td><td>FTP</td><td>FPR95 √</td><td>AUROC 个</td></tr><tr><td>CIFAR-10 DenseNet-101</td><td>× √ × √</td><td>× × √ √</td><td>26.55 21.29 19.16 16.96</td><td>94.57 95.09 96.36 96.59</td></tr><tr><td>CIFAR-100 DenseNet-101</td><td>× √ × √</td><td>× × √ √</td><td>68.45 53.60 53.73 35.03 58.41</td><td>81.19 85.33 87.56 91.39</td></tr><tr><td>ImageNet-1k ResNet-50</td><td>× √ × √</td><td>× × √ √</td><td>33.96 51.38 22.51</td><td>86.17 91.26 88.68 94.55</td></tr></table></body></html>\n\n# 4.2 Main Results\n\nIn this section, we report the performance of our ITP on commonly used CIFAR benchmarks and the more realistic and challenging ImageNet benchmark. Baseline results are sourced from (Ahn, Park, and $\\mathrm { K i m } \\ 2 0 2 3$ ; Chen et al. 2023; He et al. 2024), with additional baselines (e.g., LAPS, OPNP, and $\\mathrm { O P N P + R e A c t }$ on CIFAR) reproduced by us.\n\nFor the CIFAR benchmarks, Table 1 lays out the performance of OOD detection on the CIFAR-10 and CIFAR-100, respectively. As we can see, the proposed ITP outperforms all baselines considered and achieves state-of-the-art performance. In CIFAR-100, ITP reduces FPR95 by $3 3 . 4 2 \\%$ compared to the energy baseline, showing the effectiveness of our proposal with the same OOD scoring function. Remarkably, $\\mathrm { I T P + R e A c t }$ outperforms the most competitive method LINe by $5 . 5 4 \\%$ in FPR95 and $3 . 2 4 \\%$ in AUROC, highlighting the importance of further fine-grained pruning of overconfident parameters.\n\nTable 4: Impact of varying hyperparameters on FPR95. We use ImageNet-1k as the ID dataset and ResNet-50 as the pretrained model. All values are percentages and averaged over four OOD datasets.   \n\n<html><body><table><tr><td>p=10</td><td>p=30</td><td>p= 50 p= 70</td></tr><tr><td>入=0.5 73.19</td><td>34.19 33.70</td><td>33.75 35.96</td></tr><tr><td>入=1.0 33.18</td><td>25.44 26.83</td><td>27.26 30.96</td></tr><tr><td>入=1.5 26.30</td><td>22.51 24.18</td><td>24.67 29.50</td></tr><tr><td>入=2.0 27.58</td><td>24.69 25.95</td><td>26.45 31.59</td></tr><tr><td>入=3.0 31.95</td><td>29.14 30.37</td><td>30.73 36.53</td></tr><tr><td>入= 5.0 35.90</td><td>33.00 34.14</td><td>34.52 40.23</td></tr></table></body></html>\n\nFor the large-scale ImageNet benchmark, Table 2 reports detailed performances for each OOD dataset and the average over the four datasets. Our proposed method, ITP, outperforms recent approaches such as DICE, OPNP, and LAPS, achieving the best performance among the baseline methods with an FPR95 of $2 2 . 5 1 \\%$ . Moreover, the combination of ITP and ReAct outperforms recent approaches $\\mathrm { D I C E } +$ ReAct, $\\mathrm { O P N P + R e A c t }$ , and LINe. The experimental results demonstrate that our ITP is state-of-the-art and effective for OOD detection on large-scale real-world datasets.\n\n# 4.3 Ablation Study\n\nAblation on proposed pruning strategies. To fully demonstrate the impact of different granularity pruning strategies in ITP, we conduct a comprehensive empirical analysis on CIFAR-10, CIFAR-100, and ImageNet-1k, and report the results in Table 3. As shown in the table, both FTP and CRP improve performance, and ITP further markedly boosts OOD detection performance by integrating these two coarse-grained and fine-grained pruning strategies. However, the improvement of FTP on ResNet-50 is less pronounced, likely due to its larger feature space (2048 dimensions) compared to DenseNet-101 (342 dimensions). This larger space allows noise to dominate and interfere with FTP.\n\n![](images/49eb52e1563403c84461c75d32b8b94624004bf9629ccba70af6414c84399dd8.jpg)  \nFigure 4: The FFPR95 and AUROC with different number of training samples on ImageNet benchmark. The results are averaged over five independent runs.\n\nTable 5: Comparison of preprocessing overhead. We assess the preprocessing overhead by averaging the preprocessing times measured across three runs on ImageNet-1k. “ITP (30)” denotes that only 30 images per class are used for ITP while maintaining the original performance (see Figure 4).   \n\n<html><body><table><tr><td>Method</td><td>Preprocessing Time (hours)</td><td>Additional Backpropagation</td><td>Batch Support</td></tr><tr><td>OPNP</td><td>8.7940</td><td></td><td>×</td></tr><tr><td>LINe</td><td>7.3096</td><td>√ √</td><td>×</td></tr><tr><td>ITP</td><td>0.2411</td><td>×</td><td>√</td></tr><tr><td>ITP (30)</td><td>0.0073</td><td>×</td><td>√</td></tr></table></body></html>\n\nThe significant improvement observed when FTP is applied after noise removal with CRP supports this explanation. The ablation study verifies the effectiveness of the two strategies and demonstrates that they mutually enhance and complement each other.\n\nEffect of the Hyperparameter. Table 4 shows the results of varying the percentile $p$ used for pruning redundant parameters and the threshold $\\lambda$ for identifying overconfident parameters. The optimal performance is observed at $p = 3 0$ and $\\lambda = 1 . 5$ , achieving an FPR95 of $2 2 . 5 1 \\%$ . Conversely, we notice that selecting excessively large values for $p$ and overly small values for $\\lambda$ can lead to the erroneous removal of critical parameters, thereby impairing OOD detection.\n\nEffect of the Amount of Training Samples. In Figure 4, we show the effect of utilizing different numbers of training samples to estimate the parameter contribution distribution. Remarkably, even with just two samples per class, ITP can significantly improve OOD detection performance, resulting in a drastic $2 9 . 4 8 \\%$ reduction in FPR95 compared to the energy baseline (without pruning). Furthermore, empirical evidence suggests that using only 30 samples per class can yield performance nearly equivalent to that achieved with the full dataset. Therefore, to reduce computational overhead, it is feasible to use a suitably sized subset of the training set (e.g., 30 samples per class) for distribution estimation, while still achieving comparable performance.\n\n<html><body><table><tr><td>Method</td><td>FPR95 √</td><td>AUROC 个</td></tr><tr><td>MSP</td><td>66.95</td><td>81.99</td></tr><tr><td>MSP+ITP</td><td>62.44</td><td>82.99</td></tr><tr><td>ODIN</td><td>56.48</td><td>85.41</td></tr><tr><td>ODIN+ITP</td><td>42.32</td><td>90.10</td></tr><tr><td>GradNorm</td><td>36.49</td><td>90.18</td></tr><tr><td>GradNorm+ITP</td><td>29.93</td><td>92.13</td></tr><tr><td>MLS</td><td>58.05</td><td>87.00</td></tr><tr><td>MLS+ITP</td><td>26.43</td><td>93.45</td></tr><tr><td>Energy</td><td>58.41</td><td>86.17</td></tr><tr><td>Energy+ITP</td><td>22.51</td><td>94.55</td></tr></table></body></html>\n\nTable 6: ITP on other OOD scores. We use ResNet-50 as the pre-trained model and ImageNet-1k as the ID dataset. The results are averaged over four OOD datasets.\n\n# 4.4 Further Analysis\n\nAnalysis of Preprocessing Overhead. Table 5 compares the preprocessing overhead of ITP with the most competitive weight sparsification methods: LINe and OPNP. In contrast to ITP, both LINe (Ahn, Park, and $\\mathrm { K i m } \\ 2 0 2 3$ ) and OPNP (Chen et al. 2023) require additional backpropagation to compute gradient information and lack support for batch processing. The results indicate that our proposal demonstrates a significant advantage in terms of preprocessing overhead. Notably, with only 30 samples per class, we can further substantially reduce the overhead while maintaining comparable performance (see Figure 4). Therefore, ITP is efficient and well-suited for real-world applications.\n\nCompatibility with Other OOD Scores. Table 6 presents the OOD detection performance of ITP using various OOD scoring methods, including MSP (Hendrycks and Gimpel 2017), ODIN (Liang, Li, and Srikant 2018), GradNorm (Huang, Geng, and Li 2021), MLS (Hendrycks et al. 2022), and Energy (Liu et al. 2020). Our ITP consistently improves FPR95 and AUROC across different OOD scores. In particular, ITP can effectively complement gradient-based methods such as GradNorm. These results indicate that the parameters our ITP selectively used are also applicable to other OOD scores and show strong compatibility.\n\n# 5 Conclusion\n\nIn this paper, we reveal that parameters important for ID data prediction are not always beneficial for OOD detection. To address this issue, we propose a parameter pruning method called ITP, which utilizes class-specific parameter contribution distributions for post-hoc OOD detection. ITP is based on two powerful pruning strategies: CRP performs coarsegrained pruning to remove redundant parameters, while FTP executes fine-grained pruning to eliminate overconfident parameters. Experimental results show that our ITP method significantly improves OOD detection performance and can be integrated with a wide range of other OOD scoring methods. We hope our work can raise more attention to the importance of test parameter pruning for OOD detection.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了深度神经网络（DNNs）在开放世界环境中对分布外（OOD）数据的检测不可靠问题。现有基于权重稀疏化的方法虽然能减少冗余参数对OOD检测的负面影响，但这些参数在处理OOD数据时可能表现出过度自信（overconfidence），从而损害检测性能。\\n> *   该问题的重要性在于，OOD检测的不可靠性可能导致AI系统在安全关键领域（如自动驾驶、医疗诊断）产生灾难性后果。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种名为“实例感知测试剪枝”（Instance-aware Test Pruning, ITP）的后处理方法，通过粗粒度和细粒度的参数剪枝策略，提升OOD检测的可靠性。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **创新贡献点1：** 提出粗粒度冗余剪枝（CRP），去除对ID数据贡献低的冗余参数，减少噪声干扰。在CIFAR-10上，FPR95从26.55%降至16.72%，AUROC从94.57%提升至96.10%。\\n> *   **创新贡献点2：** 提出细粒度测试剪枝（FTP），基于右尾Z检验自适应去除实例级过度自信参数。在ImageNet-1k上，FPR95从58.41%降至22.51%，AUROC从86.17%提升至94.55%。\\n> *   **创新贡献点3：** ITP与激活基方法（如ReAct）正交，可进一步集成提升性能。ITP+ReAct在CIFAR-100上FPR95降至30.13%，AUROC提升至91.91%。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   ITP的核心思想是通过参数剪枝策略减少OOD检测中的噪声干扰和过度自信问题。其设计哲学是基于参数贡献分布的统计特性，从粗粒度和细粒度两个层面优化模型行为。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 现有稀疏化方法（如DICE、LINe）仅关注ID数据的重要参数，但未解决这些参数在OOD数据上的过度自信问题。\\n> *   **本文的改进：** ITP通过CRP去除冗余参数，再通过FTP针对每个测试样本动态剪除过度自信参数，从而更可靠地区分ID和OOD数据。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **参数贡献分布估计：** 使用ID数据估计每个参数对特定类别的贡献分布（均值和标准差）。公式：\\n>     $$\\n>     \\\\mu_{ij} = \\\\frac{1}{|\\\\mathcal{D}_j|} \\\\sum_{x \\\\in \\\\mathcal{D}_j} c_j(x; \\\\mathbf{W}_{ij})\\n>     $$\\n> 2.  **粗粒度冗余剪枝（CRP）：** 移除贡献均值低于p百分位的参数。掩码矩阵公式：\\n>     $$\\n>     \\\\mathbf{M}_{ij}^{\\\\mathrm{CRP}} = \\\\begin{cases} 1, & \\\\text{if } \\\\mu_{ij} > \\\\Omega_p \\\\\\\\ 0, & \\\\text{otherwise} \\\\end{cases}\\n>     $$\\n> 3.  **细粒度测试剪枝（FTP）：** 对每个测试样本，通过右尾Z-score测试（阈值λ）移除过度自信参数。掩码矩阵公式：\\n>     $$\\n>     \\\\mathbf{M}_{ij}^{\\\\mathrm{FTP}}(\\\\mathbf{x}) = \\\\begin{cases} 1, & \\\\text{if } \\\\frac{c_j(\\\\mathbf{x}; \\\\mathbf{W}_{ij}) - \\\\mu_{ij}}{\\\\sigma_{ij}} \\\\leq \\\\lambda \\\\\\\\ 0, & \\\\text{otherwise} \\\\end{cases}\\n>     $$\\n> 4.  **OOD分数计算：** 使用剪枝后的模型输出计算能量分数：\\n>     $$\\n>     S(\\\\mathbf{x}) = -E(\\\\mathbf{x}) = \\\\log \\\\sum_{k=1}^K \\\\exp(f_k^{\\\\mathrm{ITP}}(\\\\mathbf{x}))\\n>     $$\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   基线模型包括MSP、Energy、ODIN、ReAct、DICE、LINe、OPNP和LAPS等OOD检测方法。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在FPR95上：** 本文方法在CIFAR-10上达到了16.72%，显著优于基线模型Energy（26.55%）和DICE（20.83%）。与表现最佳的基线LAPS（19.40%）相比，提升了2.68个百分点。\\n> *   **在AUROC上：** 本文方法在CIFAR-100上达到了91.39%，远高于基线模型Energy（81.19%）和DICE（87.23%），与表现最佳的基线LINe（88.55%）相比，提升了2.84个百分点。\\n> *   **在ImageNet-1k上：** 本文方法的平均FPR95为22.51%，优于基线模型DICE（34.75%）和OPNP（25.93%），与表现最佳的基线LAPS（23.68%）相当，但在AUROC上（94.55%）略优于后者（94.78%）。\\n> *   **在预处理开销上：** ITP的预处理时间为0.2411小时，远低于LINe（7.3096小时）和OPNP（8.7940小时），同时支持批量处理。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   分布外检测 (Out-of-Distribution Detection, OOD)\\n*   实例感知测试剪枝 (Instance-aware Test Pruning, ITP)\\n*   参数贡献分布 (Parameter Contribution Distribution, N/A)\\n*   粗粒度冗余剪枝 (Coarse-grained Redundancy Pruning, CRP)\\n*   细粒度测试剪枝 (Fine-grained Test Pruning, FTP)\\n*   深度神经网络 (Deep Neural Network, DNN)\\n*   后处理方法 (Post-hoc Method, N/A)\\n*   安全关键应用 (Safety-critical Applications, N/A)\"\n}\n```"
}