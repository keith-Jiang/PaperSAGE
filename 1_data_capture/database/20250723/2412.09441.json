{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.09441",
    "link": "https://arxiv.org/abs/2412.09441",
    "pdf_link": "https://arxiv.org/pdf/2412.09441.pdf",
    "title": "MOS: Model Surgery for Pre-Trained Model-Based Class-Incremental Learning",
    "authors": [
        "Hai-Long Sun",
        "Da-Wei Zhou",
        "Hanbin Zhao",
        "Le Gan",
        "De-Chuan Zhan",
        "Han-Jia Ye"
    ],
    "categories": [
        "cs.LG",
        "cs.CV"
    ],
    "publication_date": "2024-12-12",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 6,
    "influential_citation_count": 1,
    "institutions": [
        "Nanjing University",
        "National Key Laboratory for Novel Software Technology",
        "Zhejiang University"
    ],
    "paper_content": "# MOS: Model Surgery for Pre-Trained Model-Based Class-Incremental Learning\n\nHai-Long Sun1, 2, Da-Wei Zhou1, 2\\*, Hanbin Zhao3, Le Gan1, 2, De-Chuan Zhan1, 2, Han-Jia Ye1, 2\n\n1School of Artificial Intelligence, Nanjing University 2National Key Laboratory for Novel Software Technology, Nanjing University 3College of Computer Science and Technology, Zhejiang University sunhl, zhoudw, zhandc, yehj @lamda.nju.edu.cn, ganle $@$ nju.edu.cn, zhaohanbin $@$ zju.edu.cn\n\n# Abstract\n\nClass-Incremental Learning (CIL) requires models to continually acquire knowledge of new classes without forgetting old ones. Despite Pre-trained Models (PTMs) have shown excellent performance in CIL, catastrophic forgetting still occurs as the model learns new concepts. Existing work seeks to utilize lightweight components to adjust the PTM, while the forgetting phenomenon still comes from parameter and retrieval levels. Specifically, iterative updates of the model result in parameter drift, while mistakenly retrieving irrelevant modules leads to the mismatch during inference. To this end, we propose MOdel Surgery (MOS) to rescue the model from forgetting previous knowledge. By training task-specific adapters, we continually adjust the PTM to downstream tasks. To mitigate parameter-level forgetting, we present an adapter merging approach to learn task-specific adapters, which aims to bridge the gap between different components while reserve task-specific information. Besides, to address retrieval-level forgetting, we introduce a training-free self-refined adapter retrieval mechanism during inference, which leverages the model‚Äôs inherent ability for better adapter retrieval. By jointly rectifying the model with those steps, MOS can robustly resist catastrophic forgetting in the learning process. Extensive experiments on seven benchmark datasets validate MOS‚Äôs state-of-the-art performance.\n\nCode ‚Äî https://github.com/sun-hailong/AAAI25-MOS\n\n# Introduction\n\nIn recent years, deep learning has achieved significant results in many real-world applications (Deng et al. 2009; He et al. 2015; Cao et al. 2024b; Sun et al. 2024). While in the open world, data often appears in a streaming format (Golab and ¬®Ozsu 2003), requiring a machine learning paradigm capable of incrementally acquiring new class knowledge, which is denoted as Class-Incremental Learning (CIL) (Rebuffi et al. 2017; Zhou et al. 2024b). One of the significant challenges in CIL is catastrophic forgetting, where the model, after learning new classes incrementally, gradually loses its ability to recognize the old ones (French 1999). In response to this challenge, the field of CIL is evolving with the emergence of pre-trained models (PTMs). Unlike the traditional approach of ‚Äútraining from scratch‚Äù (Li and\n\nHoiem 2017; Zhou et al. 2023; Ye et al. 2019), contemporary CIL methods are increasingly leveraging PTMs, which are initially pre-trained on vast datasets using substantial resources (McDonnell et al. 2024; Jung et al. 2023). This pretraining process endows PTMs with robust generalization abilities. Consequently, designing an effective CIL method that leverages PTMs and resists catastrophic forgetting has garnered significant attention from researchers.\n\nDue to the generalization of PTMs, existing works often freeze the pre-trained weights and adapt to incremental tasks using additional lightweight modules (Hu et al. 2022; Chao et al. 2020; Ye, Lu, and Zhan 2022). For example, visual prompt tuning (Jia et al. 2022) customizes prompts to modify model behavior, facilitating adaptation to downstream tasks. Specifically, L2P (Wang et al. 2022c) designs a key-query matching strategy to retrieve instancespecific prompts from a prompt pool. Based on L2P, DualPrompt (Wang et al. 2022b) introduces expert prompts to encode task-specific information and explores the impact of prompt depth. Furthermore, CODA-Prompt (Smith et al. 2023) proposes an attention-based weighting method for prompts to enhance the efficacy of prompt retrieval.\n\nHowever, as the model learns new concepts, catastrophic forgetting still occurs. This forgetting phenomenon happens at both the parameter and retrieval levels. During the training stage, although many methods use lightweight components to adjust the PTM, iterative updates of these components will lead to parameter drift and trigger forgetting. Moreover, existing works devote to preventing conflicts between prompts or achieving orthogonal projection, which exacerbates parameter drift between new and old components. During inference, training multiple lightweight modules requires selecting the most relevant one, but the model may mistakenly retrieve the irrelevant modules, leading to the performance decay. This motivates us to question if it is possible to jointly rectify the model to resist catastrophic forgetting at both the parameter and retrieval levels?\n\nFacing the challenges at both the parameter and retrieval levels, our model should be able to effectively design mechanisms to overcome these issues. To address forgetting at the parameter level, the model needs to develop effective update methods that ensure the updated parameters remain discriminative for old data. To overcome forgetting at the retrieval level, the model requires efficient self-correction strategies to help utilize relevant information, assisting in the instance-specific retrieval of lightweight modules.\n\nTo this end, we propose MOdel Surgery (MOS) for pretrained model-based class-incremental learning to rescue the model from forgetting previous knowledge. This surgery is divided into the training and inference stages. To mitigate parameter-level forgetting, we present an adapter merging approach during training, which learns task-specific adapters while bridging gaps between components and retaining task-specific information. This strategy helps previously learned adapters aid in learning new tasks. To address retrieval-level forgetting, we introduce a training-free selfrefined adapter retrieval mechanism during inference, which leverages the model‚Äôs inherent ability for better adapter retrieval. This mechanism requires no additional training overhead, making the algorithm simple and efficient. Finally, to enable the model to balance the stability-plasticity dilemma, we present a model ensemble method that integrates the model‚Äôs capabilities across multiple phases. It not only ensures strong generalization but also allows the model to quickly recognize and update information. Experiments on seven benchmark datasets validate the effectiveness of MOS. Additionally, the visualization of the self-refined adapter retrieval mechanism indicates that MOS effectively learns adapter retrieval for various downstream tasks.\n\n# Related Work\n\nClass-Incremental Learning (CIL). It aims to enable models to acquire new classes knowledge while retaining previously learned information (Rebuffi et al. 2017). Existing works can be roughly categorized into several categories. Knowledge distillation-based methods (Li and Hoiem 2017; Rebuffi et al. 2017; Snell, Swersky, and Zemel 2017) establish a mapping between the former stage model and the current model, thereby aiding the latter in retaining characteristics from earlier updates during incremental learning (Hinton, Vinyals, and Dean 2015). Data rehearsal-based methods (Chaudhry et al. 2018; Liu et al. 2020; Zhao et al. 2021) select and replay crucial exemplars from old classes during training new ones to continuously revise former knowledge. Parameter regularization-based methods (Aljundi, Kelchtermans, and Tuytelaars 2019; Kirkpatrick et al. 2017) aim to predict and minimize the drift of key parameters by using regularization terms. Model rectification-based methods (Pham, Liu, and Steven 2022; Shi et al. 2022; Yu et al. 2020) focus on correcting the model‚Äôs inductive bias to ensure unbiased estimations. Model expansion-based methods (Chen and Chang 2023; Hu et al. 2023; Wang et al. 2022a; Yan, Xie, and He 2021) construct non-interfering subnetworks for each task. During inference, they are combined to form a larger feature map and train a classifier to effectively calibrate across all classes.\n\nPre-Trained Model-Based CIL. PTM-based CIL has emerged as a hot topic in the current CIL research area. With advances in pre-training techniques, numerous parameterefficient fine-tuning (PEFT) methods (Jia et al. 2022; Hu et al. 2022; Lian et al. 2022; Rebuffi, Bilen, and Vedaldi 2017; Cao et al. 2024a; Hu et al. 2024; Lu et al. 2024; Li et al. 2024; Wei et al. 2019) have been developed. These methods aim to improve model performance with minimal additional resources while freezing pre-trained weights. In this context, L2P (Wang et al. 2022c) introduces a prompt pool, selecting instance-specific prompts via a keyquery matching selection mechanism to guide the PTM‚Äôs response. DualPrompt (Wang et al. 2022b) extends L2P by designing G-Prompt and E-Prompt, which encode taskinvariant and task-specific instructions, respectively. CODAPrompt (Smith et al. 2023) innovates by developing decomposed prompts and combining them using an attention-based weighting method. DAP (Jung et al. 2023) extends prompt selection into prompt generation. SLCA (Zhang et al. 2023) reveals that fine-tuning a ViT backbone with a lower learning rate at the representation layer yields higher accuracy than prompt strategies. APER (Zhou et al. 2024a) explores various PEFT methods and shows that prototypical classifiers serve as a strong baseline, and RanPAC (McDonnell et al. 2024) further expands APER in random projection. EASE (Zhou et al. 2024c) concatenates the feature representations of multiple task-specific backbones.\n\n# Preliminaries\n\n# Class-Incremental Learning\n\nClass-incremental learning aims to acquire knowledge from continuously evolving data streams that introduce new classes while retaining knowledge of previous ones to build a unified classifier (Rebuffi et al. 2017). Consider a series of $B$ training stages, expressed as $\\{ \\mathcal { D } ^ { 1 } , \\mathcal { D } ^ { 2 } , \\cdot \\cdot \\cdot , \\mathcal { D } ^ { B } \\}$ , where $\\mathbf { \\mathcal { D } } ^ { b } = \\{ ( \\mathbf { \\breve { x } } _ { i } ^ { b } , y _ { i } ^ { \\breve { b } } ) \\} _ { i = 1 } ^ { n _ { b } }$ represents the $b$ -th incremental stage containing $n _ { b }$ instances. Correspondingly, the testing set is denoted as $\\{ \\mathcal { D } _ { t } ^ { 1 } , \\mathcal { D } _ { t } ^ { 2 } , \\cdot \\cdot \\cdot , \\mathcal { D } _ { t } ^ { B } \\}$ . Within this setting, each training instance $\\mathbf { x } _ { i } ^ { b } \\in \\mathbb { R } ^ { D }$ is associated with a class $y _ { i } \\in Y _ { b }$ . Here, $Y _ { b }$ defines the set of labels for task $b$ , and it is ensured that $Y _ { b } \\cap Y _ { b ^ { \\prime } } = \\emptyset$ for any $b \\neq b ^ { \\prime }$ . During $b$ -th training stage, the model is updated utilizing data exclusively from $\\mathcal { D } ^ { b }$ . In this paper, we follow the exemplar-free setting in (Wang et al. 2022c,b; Zhou et al. 2024a), which entails not using any historical exemplars from previous classes. Therefore, the model can only access data from $\\mathcal { D } ^ { b }$ for training during the $b$ -th stage. The effectiveness of the model is evaluated across all previously encountered classes, collectively represented as $\\mathcal { V } _ { b } = Y _ { 1 } \\cup \\dots \\cup Y _ { b }$ , after each CIL task. Specifically, we aim to find a model $f ( \\mathbf { x } ) : X \\to \\mathcal { V } _ { b }$ that minimizes empirical risk across all test datasets:\n\n$$\nf ^ { * } = \\underset { f \\in \\mathcal { H } } { \\mathrm { a r g m i n } } \\mathbb { E } _ { ( \\mathbf { x } , y ) \\sim \\mathcal { D } _ { t } ^ { 1 } \\cup \\cdots \\mathcal { D } _ { t } ^ { b } } \\mathbb { I } \\left( y \\neq f ( \\mathbf { x } ) \\right) ,\n$$\n\nwhere $\\mathcal { H }$ is the hypothesis space and $\\mathbb { I } ( \\cdot )$ denotes the indicator function. $\\mathcal { D } _ { t } ^ { b }$ represents the testing set of task $b$ . An effective CIL model satisfying Eq. 1 exhibits discriminative abilities across all classes. It achieves a balance between learning new classes and retaining information about old ones.\n\nFollowing the typical PTM-based CIL works (Wang et al. 2022c,b), we assume that a PTM (e.g., Vision Transformer (ViT) (Dosovitskiy et al. 2020)) is available as the initialization for $f ( \\mathbf { x } )$ . For clearer understanding, we decouple the PTM into two components: $f ( { \\bf x } ) = \\breve { W } ^ { \\top } \\phi ( { \\bf x } )$ , where $\\phi ( \\cdot ) : \\mathbb { R } ^ { D }  \\mathbb { R } ^ { d }$ is the feature extractor and $W \\in \\mathbb { R } ^ { d \\times | \\mathcal { V } _ { b } | }$ is the classifier. We denote the classifier for class $k$ as $\\pmb { w } _ { k }$ : $W = [ { \\pmb w } _ { 1 } , { \\pmb w } _ { 2 } , \\cdot \\cdot \\cdot , { \\pmb w } _ { | { \\pmb y } _ { b } | } ]$ . For a standard ViT, the initial encoding layer converts the image into a sequence of output features, denoted as $\\mathbf { x } _ { e } ~ \\in ~ \\mathbb { R } ^ { L \\times d }$ , where $L$ is the sequence length. We simplify this by treating the first token in ${ \\bf x } _ { e }$ to be the [CLS] token. The sequence $\\mathbf { x } _ { e }$ is then processed through subsequent layers, including multi-head selfattention and MLP, to produce the final embeddings. Finally, the embedded [CLS] token is considered as $\\phi ( \\mathbf { x } )$ .\n\n# Analysis of PTM-Based CIL\n\nLearning with PTMs. A representative work in PTM-based CIL is L2P (Wang et al. 2022c). They introduce a strategy of freezing the pre-trained weights and constructing a learnable prompt pool that can be shared across all tasks. This prompt pool is denoted as $\\textbf { P } = \\ \\{ P _ { 1 } , P _ { 2 } , \\cdot \\cdot \\cdot \\ , P _ { M } \\}$ , where $P _ { j } \\in \\mathbf { \\sigma } \\mathbf { \\widetilde { R } } ^ { L _ { p } \\times d }$ is a single prompt with token length $L _ { p }$ and the same embedding size $d$ as ${ \\bf x } _ { e }$ . $M$ is the size of the prompt pool. Each prompt in this pool corresponds to a specific key $\\{ ( \\pmb { k } _ { 1 } , P _ { 1 } ) , ( \\pmb { k } _ { 2 } , P _ { 2 } ) , \\cdots , ( \\pmb { k } _ { M } , P _ { M } ) \\bar  \\}$ , where $\\pmb { k } _ { i } \\in \\mathbb { R } ^ { d _ { k } }$ . First, they utilize a PTM without prompting (i.e., $\\phi ( \\cdot ) )$ to encode the features into the key‚Äôs embedding space and retrieve prompts with similar keys. During inference, given an input $\\mathbf { x }$ , the model employs $\\phi ( \\mathbf { x } )$ to look up the top-N keys by solving the objective in Eq. 2. This process retrieves the most relevant keys and their corresponding prompts from the prompt pool.\n\n$$\n{ \\bf K _ { x } } = \\underset { \\{ s _ { i } \\} _ { i = 1 } ^ { N } \\subseteq [ 1 , M ] } { \\mathrm { a r g m i n } } \\sum _ { i = 1 } ^ { N } \\gamma \\left( \\phi ( { \\bf x } ) , k _ { s _ { i } } \\right) ,\n$$\n\nwhere $\\mathbf { K }$ is the set of all keys and ${ \\bf K } _ { \\bf x }$ is the selected top- $\\mathbf { \\nabla \\cdot N }$ keys. $\\gamma ( \\cdot , \\cdot )$ denotes the cosine distance. Finally, L2P minimize the end-to-end training loss function:\n\n$$\n\\operatorname* { m i n } _ { \\mathbf { P } , \\mathbf { K } , \\phi } \\ell ( W ^ { \\top } \\phi ( \\mathbf { x } ; \\mathbf { P } ) , y ) + \\lambda \\sum _ { \\mathbf { K _ { x } } } \\gamma \\left( \\phi ( \\mathbf { x } ) , { k _ { s } } _ { i } \\right) ,\n$$\n\nwhere $\\ell ( \\cdot , \\cdot )$ is the cross-entropy loss that measures the discrepancy between prediction and ground truth. $\\lambda$ is a scalar to weight the loss. Optimizing Eq. 3 enhances the PTM‚Äôs ability to incorporate task-specific information, allowing it to adapt more effectively to evolving data instances.\n\nForgetting of parameter and retrieval levels. L2P continually updates prompts and retrieves instance-specific prompts to guide the PTM‚Äôs response. However, although the model learns new concepts, catastrophic forgetting still occurs at the parameter and retrieval levels. Specifically, Eq. 3 shows how L2P uses lightweight modules to adjust the PTM to downstream tasks. As the prompts are iteratively updated, they gradually adapt to the subsequent tasks, leading to parameter drift. On the other hand, training multiple lightweight modules requires selecting the most relevant one during inference, while the model may mistakenly retrieve the irrelevant modules, leading to the performance decay. The mistaken retrieval comes from three aspects: First, modules learned in previous tasks might be re-selected for new tasks, causing confusion between the retrieval of old and new modules. Besides, since the keys for subsequent tasks do not exist during current training, a gap may arise between the keys and the feature embeddings, leading to mistaken retrieval during inference. Therefore, it is essential to design a method to jointly rectify the model to resist catastrophic forgetting at both the parameter and retrieval levels.\n\n# MOS: Model Surgery for PTM-based CIL\n\nFacing the challenge of resisting catastrophic forgetting, we need a method to jointly rectify the model. The key idea of MOS is to design model surgery in two aspects, i.e., training stage surgery that mitigates parameter drift and testing stage surgery that retrieves better lightweight modules. Training stage surgery aims to use previously learned knowledge to improve performance on current tasks, allowing the model to adapt to new tasks more quickly. Testing stage surgery seeks to find a mechanism for better adapter retrieval without additional overhead. As a result, the model can benefit from continual lightweight module updates and effective retrieval ability without forgetting existing knowledge.\n\nWe first introduce the process of progressively merged adapters for mitigating parameter drift and then discuss the self-refined adapter retrieval mechanism. We summarize the inference function with pseudo-code in the last part.\n\n# Progressively Merged Adapters\n\nTo handle the parameter drift caused by the iterative updates of the model, we need to bridge the gap between different lightweight modules. In other words, as the model continually receives new data and tasks, it is crucial to effectively retain and utilize previously learned knowledge. This approach allows the model to transfer prior knowledge to new tasks and mitigates the parameter drift problem. In Eq. 3, the embedding of a given input $\\mathbf { x }$ is obtained using instance-specific prompts. During the incremental phase, a potential problem can emerge, i.e., iterative updates to existing prompts might cause them to better match new tasks, possibly resulting in forgetting older tasks.\n\nDue to the large prompt pool in the above methods, which exacerbates mistaken retrieval, we suggest mitigating this problem by using a smaller number of lightweight modules. In detail, by directly incorporating adapter tuning (Rebuffi, Bilen, and Vedaldi 2017) into the PTM to optimize a single adapter for encoding task-specific information, we achieve this goal through the application of this method. This enhanced integration allows facilitates a more effective assimilation of task-specific information. With this approach, we only need to optimize a collection of adapters to encode task-specific information. Denote that there are $L$ transformer blocks in the pre-trained model, each with a selfattention module and an MLP layer. We integrate an adapter into each layer‚Äôs MLP via residual connections. An adapter is a bottleneck module comprising a down-projection layer $W _ { d o w n } \\in \\mathbb { R } ^ { d \\times r }$ , a non-linear activation function ReLU, and an up-projection layer $W _ { u p } \\in \\mathbb { R } ^ { d \\times r }$ . The output formula of the MLP is formatted as follows:\n\n$$\n\\begin{array} { r } { \\mathbf { x } _ { o } = \\mathbf { M } \\mathbf { L } \\mathbf { P } ( \\mathbf { x } _ { i } ) + \\mathbf { R e L } \\mathbf { U } ( \\mathbf { x } _ { i } W _ { d o w n } ) W _ { u p } , } \\end{array}\n$$\n\nwhere $\\mathbf { x } _ { i }$ and $\\mathbf { x } _ { o }$ are the input and output of the MLP, respectively. Eq. 4 illustrates how to enhance the task information by adding residual connections of adapters to the original outputs. In the context of ViT and for a specific $i$ -th task, we define the set of adapters across all $L$ transformer blocks as $\\mathcal { A } _ { i }$ , representing task-specific adapters. Furthermore, we denote the output embedding of a given $\\mathbf { \\mathcal { A } } _ { i }$ , combined with the PTM, as $\\phi ( \\mathbf { x } ; A _ { i } )$ . Therefore, when a new task emerges, we freeze the weights of the PTM and focus solely on optimizing the adapters and the corresponding classifier $W$ :\n\n![](images/f6f776b71c7e4c44554f9d63f9f2f4b361e9e74adbd0a6b82fde15bd2b43ee58.jpg)  \nFigure 1: Illustration of MOS. Left: the training protocol of MOS. We use progressively merged adapters to incrementally adapt the PTM. Right: the self-refined adapter retrieval mechanism for the testing stage. We use the model‚Äôs own capabilities to correct errors caused by the mistaken retrieval problem.\n\n$$\n\\operatorname* { m i n } _ { A _ { i } , W } \\sum _ { \\left( \\mathbf { x } , y \\right) \\in \\mathcal { D } ^ { b } } \\ell \\left( W ^ { \\top } \\phi \\left( \\mathbf { x } ; \\mathcal { A } _ { i } \\right) , y \\right) .\n$$\n\nWe enable the incorporation of task-specific information into embeddings through adapters by optimizing Eq. 5, facilitating the learning of new tasks. In an ideal scenario, if the task ID of each test sample is known, we can easily select the corresponding task-specific adapter using this ID to achieve optimal results.\n\nHowever, in the CIL setting, obtaining such a task ID during the testing phase is forbidden. To address this challenge and mitigate parameter drift, we propose the training stage surgery which uses adapter merging strategy based on Exponential Moving Average (EMA) in Eq. 6. This approach allows subsequent adapters to retain some knowledge of their predecessors, ensuring satisfactory results even if an incorrect $\\mathcal { A }$ is selected.\n\n$$\n\\mathcal { A } _ { b } = ( 1 - \\alpha ) \\hat { \\mathcal { A } } _ { b } + \\frac { \\alpha } { b - 1 } \\sum _ { k = 1 } ^ { b - 1 } \\mathcal { A } _ { k } ,\n$$\n\nwhere $\\hat { \\mathcal { A } } _ { b }$ represents the set of adapters for the $b$ -th training stage and $\\boldsymbol { \\mathcal { A } } _ { b }$ is the final result after the EMA process. Specifically, given an adapter comprises $W _ { u p }$ and $W _ { d o w n }$ , we perform the merge process on both of them to facilitate the integration of adapters. When training a new $\\mathcal { A } _ { b }$ , all previously trained $\\boldsymbol { \\mathcal { A } } _ { k }$ are frozen, and the adapter merging process is executed following each backpropagation step.\n\nEffect of adapter merging strategy. Figure 1 (left) depicts this merging process. This strategy ensures that the training of the current adapter $\\boldsymbol { \\mathcal { A } } _ { b }$ does not interfere with the performance of already trained adapters, thereby preventing catastrophic forgetting. Moreover, it guarantees that each $\\mathcal { A }$ retains task-specific information while maintaining remaining well-aligned in the feature space, even if an incorrect $\\mathcal { A }$ is selected. In this way, we can mitigate parameter drift during iterative adapter updates. Moreover, because adapters are lightweight branches, they require significantly fewer parameters compared to fully fine-tuning the backbone. The parameter cost for saving these adapters is calculated as $( B \\times L \\times 2 d r )$ , where $B$ denotes the number of tasks, $L$ is the number of transformer blocks, and $2 d r$ signifies the parameter count of each adapter (i.e., linear projections).\n\n# Self-Refined Adapter Retrieval Mechanism\n\nAfter obtaining these task-specific adapters, we utilize a prototype-based classifier (Snell, Swersky, and Zemel 2017) for prediction. Specifically, after the training process of each incremental stage, we extract the class prototype of the $i$ -th class using adapter $\\mathcal { A } _ { b }$ :\n\n$$\np _ { i , b } = \\frac { 1 } { N } \\sum _ { j = 1 } ^ { | \\mathcal { D } ^ { b } | } \\mathbb { I } ( y _ { j } = i ) \\phi ( \\mathbf { x } _ { j } ; \\mathcal { A } _ { b } ) ,\n$$\n\nwhere $N$ is the instance number of class $i$ . Eq. 7 illustrates the constrution of classifier. During inference, we directly adopt the class prototype as the classifier weight, i.e., $w _ { i } =$ $\\pmb { p } _ { i }$ , and utilize a cosine classifier for classification:\n\n$$\nf ( \\mathbf { x } | \\mathbf { \\mathcal { A } } _ { i } ) = ( \\frac { W } { \\| W \\| _ { 2 } } ) ^ { \\top } ( \\frac { \\phi ( \\mathbf { x } ; \\mathbf { \\mathcal { A } } _ { i } ) } { \\| \\phi ( \\mathbf { x } ; \\mathbf { \\mathcal { A } } _ { i } ) \\| _ { 2 } } ) ,\n$$\n\nwhere $\\mathbf { \\mathcal { A } } _ { i }$ denotes the selected adapter for the input $\\mathbf { x }$\n\nEq. 2 illustrates how prompts are selected from the prompt pool. Subsequently, L2P integrates the selected prompt into the original PTM (i.e., $\\phi ( \\mathbf { \\bar { x } } ; P ) )$ to guide the model‚Äôs response. However, this approach heavily relies on the retrieval mechanism of key-query pairs. Mistakenly retrieving the irrelevant prompts often leads to performance decay. To address the retrieval-level issue, we design the testing stage surgery which uses self-refined adapter retrieval mechanism. It is an efficient and training-free method that enables the model to autonomously correct this problem, thereby improving adapter retrieval. This mechanism does not require any additional training overhead and is only used during the inference process, making the algorithm both simple and efficient.\n\nTable 1: Average and last performance comparison on seven datasets with ViT-B/16-IN21K as the backbone. ‚ÄòIN-R/A‚Äô stands for ‚ÄòImageNetR/A,‚Äô ‚ÄòObjNet‚Äô stands for ‚ÄòObjectNet,‚Äô and ‚ÄòOmniBench‚Äô stands for ‚ÄòOmniBenchmark.‚Äô We report all compared methods with their source code. The best performance is shown in bold. All methods are implemented without using exemplars.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">CIFAR B0 Ine5</td><td colspan=\"2\">CUB B0 Ine10</td><td colspan=\"2\">IN-R B0Ie20</td><td colspan=\"2\">IN-AB0 Ie20</td><td colspan=\"2\">ObjNet B0Ine10</td><td colspan=\"2\">OmniBench B0 Ine30</td><td colspan=\"2\">VTAB B0 Inc10</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Finetune</td><td>38.90</td><td>20.17</td><td>26.08</td><td>13.96</td><td>32.31</td><td>22.78</td><td>24.28</td><td>14.51</td><td>19.14</td><td>8.73</td><td>23.61</td><td>10.57</td><td>34.95</td><td>21.25</td></tr><tr><td>Finetune Adapter</td><td>60.51</td><td>49.32</td><td>66.84</td><td>52.99</td><td>58.17</td><td>52.39</td><td>45.41</td><td>41.10</td><td>50.22</td><td>35.95</td><td>62.32</td><td>50.53</td><td>48.91</td><td>45.12</td></tr><tr><td>LwF</td><td>46.29</td><td>41.07</td><td>48.97</td><td>32.03</td><td>45.72</td><td>34.17</td><td>37.75</td><td>26.84</td><td>33.01</td><td>20.65</td><td>47.14</td><td>33.95</td><td>40.48</td><td>27.54</td></tr><tr><td>L2P</td><td>85.94</td><td>79.93</td><td>67.05</td><td>56.25</td><td>75.46</td><td>69.77</td><td>49.39</td><td>41.71</td><td>63.78</td><td>52.19</td><td>73.36</td><td>64.69</td><td>77.11</td><td>77.10</td></tr><tr><td>DualPrompt</td><td>87.87</td><td>81.15</td><td>77.47</td><td>66.54</td><td>73.10</td><td>67.18</td><td>53.71</td><td>41.67</td><td>59.27</td><td>49.33</td><td>73.92</td><td>65.52</td><td>83.36</td><td>81.23</td></tr><tr><td>CODA-Prompt</td><td>89.11</td><td>81.96</td><td>84.00</td><td>73.37</td><td>77.97</td><td>72.27</td><td>53.54</td><td>42.73</td><td>66.07</td><td>53.29</td><td>77.03</td><td>68.09</td><td>83.90</td><td>83.02</td></tr><tr><td>SimpleCIL</td><td>87.57</td><td>81.26</td><td>92.20</td><td>86.73</td><td>61.26</td><td>54.55</td><td>59.77</td><td>48.91</td><td>65.45</td><td>53.59</td><td>79.34</td><td>73.15</td><td>85.99</td><td>84.38</td></tr><tr><td>APER+ Finetune</td><td>87.67</td><td>81.27</td><td>91.82</td><td>86.39</td><td>68.54</td><td>58.37</td><td>61.01</td><td>49.57</td><td>61.41</td><td>48.34</td><td>73.02</td><td>65.03</td><td>87.47</td><td>80.44</td></tr><tr><td>APER+ VPT-S</td><td>90.43</td><td>84.57</td><td>92.02</td><td>86.51</td><td>68.83</td><td>62.03</td><td>58.39</td><td>47.20</td><td>64.54</td><td>52.53</td><td>79.63</td><td>73.68</td><td>87.15</td><td>85.36</td></tr><tr><td>APER+ VPT-D</td><td>88.46</td><td>82.17</td><td>91.02</td><td>84.99</td><td>77.05</td><td>69.47</td><td>58.48</td><td>48.52</td><td>67.83</td><td>54.65</td><td>81.05</td><td>74.47</td><td>86.59</td><td>83.06</td></tr><tr><td>APER+ SSF</td><td>87.78</td><td>81.98</td><td>91.72</td><td>86.13</td><td>75.47</td><td>67.02</td><td>61.30</td><td>50.03</td><td>69.15</td><td>56.64</td><td>80.53</td><td>74.00</td><td>85.66</td><td>81.92</td></tr><tr><td>APER+ Adapter</td><td>90.65</td><td>85.15</td><td>92.21</td><td>86.73</td><td>75.82</td><td>67.95</td><td>60.47</td><td>49.37</td><td>67.18</td><td>55.24</td><td>80.75</td><td>74.37</td><td>85.95</td><td>84.35</td></tr><tr><td>SLCA</td><td>92.49</td><td>88.55</td><td>89.51</td><td>82.19</td><td>81.17</td><td>77.00</td><td>68.66</td><td>58.74</td><td>72.55</td><td>61.30</td><td>82.80</td><td>74.10</td><td>90.94</td><td>90.76</td></tr><tr><td>EASE</td><td>91.51</td><td>85.80</td><td>92.23</td><td>86.81</td><td>81.74</td><td>76.17</td><td>65.34</td><td>55.04</td><td>70.84</td><td>57.86</td><td>81.11</td><td>74.85</td><td>93.61</td><td>93.55</td></tr><tr><td>MOS</td><td>93.30</td><td>89.25</td><td>93.49</td><td>90.12</td><td>82.96</td><td>77.93</td><td>69.13</td><td> 59.12</td><td>74.69</td><td>63.62</td><td>85.91</td><td>80.05</td><td>92.62</td><td>92.79</td></tr></table></body></html>\n\nSince there is a gap between the PTM and downstream datasets, we first use an adapter to fine-tune the PTM on the first incremental task, denoting the model as $f ( \\mathbf { x } ; A _ { 1 } )$ . This process effectively bridges this gap and makes the model suitable as the initial selector. During inference, we utilize $f ( \\mathbf { x } ; A _ { 1 } )$ to obtain the embedding of each testing example and perform the initial retrieval of task-specific adapters. Specifically, given an input $\\mathbf { x }$ , we first obtain the prediction result $f ( \\mathbf { x } | \\mathbf { \\mathcal { A } } _ { 1 } )$ of the model through Eq. 8. Afterwards, we can easily infer its corresponding task ID $i$ :\n\n$$\ni = \\operatorname { a r g m a x } { \\big ( } f ( \\mathbf { x } | A _ { 1 } ) { \\big ) } \\operatorname { m o d } | Y _ { b } | ,\n$$\n\nwhere $Y _ { b }$ is the number of classes for each task. Building on this result, we introduce an iterative self-refined process. As defined in Eq. 8, this process primarily uses $f ( \\mathbf { x } ; \\mathcal { A } _ { i } )$ to obtain prediction and identify the task ID $j$ . Since each adapter is task-specific, we can determine whether to end the iteration by checking if $i = j$ . Specifically, through $f ( \\mathbf { x } | \\mathbf { \\mathcal { A } } _ { i } )$ , we can infer its corresponding task $\\operatorname { I D } j$ :\n\n$$\nj = \\operatorname { a r g m a x } \\left( f ( \\mathbf { x } | \\mathcal { A } _ { i } ) \\right) { \\bmod { } } | Y _ { b } | .\n$$\n\nFor example, in a scenario where each task comprises 10 classes, classes 0 through 9 are in the task 0, while classes 10 through 19 are in the task 1. Subsequently, if $i \\neq j$ , we replace $i$ with $j$ and repeat the process of Eq. 10 until $i = j$ , ensuring the self-consistency.\n\nEffect of self-refined adapter retrieval mechanism. Figure 1 (right) illustrates the self-refined process. Firstly, $\\phi ( \\mathbf { x } ; A _ { 1 } )$ with the prototype-based classifier bridges the gap between upstream and downstream datasets, enhancing the model‚Äôs ability to generalize to new classes. Hence, we use it as the initial selector to start the self-refined iteration. Moreover, this approach is training-free and does not incur any additional training costs, ensuring the algorithm‚Äôs efficiency. Due to the self-refined adapter retrieval mechanism allowing the model to verify the correctness of its initial predictions, we can easily check model consistency, thereby alleviating the aforementioned mistaken retrieval problem. By using this mechanism, MOS successfully corrects some images that were originally incorrectly predicted. The detailed visualization examples will be provided in experiments.\n\n# Multi-Stage Model Ensemble\n\nInspired by the Complementary Learning System of the human brain (McClelland, McNaughton, and O‚ÄôReilly 1995; Kumaran, Hassabis, and McClelland 2016), which suggests that the anterior cingulate circuit is responsible for rapid pattern recognition and unconscious memory, and the hippocampal circuit for deep processing and conscious memory. Therefore, we implement a two-stage model ensemble:\n\n$$\n\\begin{array} { r } { y ^ { * } = \\underset { y } { \\mathrm { a r g m a x } } ( \\underset { \\mathrm { P a r t 1 } } { \\underbrace { f ( \\mathbf { x } | \\mathcal { A } _ { 1 } ) } } + \\underset { \\mathrm { P a r t 2 } } { \\underbrace { f ( \\mathbf { x } | \\mathcal { A } _ { j } ) } } ) . } \\end{array}\n$$\n\nIn Eq. 11, Part 1, trained solely on the first incremental task, acts as a crucial bridge between the upstream and downstream datasets. It not only demonstrates strong generalization but also has the ability to quickly recognize and update information. In contrast, Part 2 employs progressively merged adapters and a self-refined adapter retrieval mechanism for deep processing and conscious memory.\n\nSummary of MOS. We initialize and train an adapter for each incremental task to encode the task-specific information, and employ the adapter merging strategy to mitigate parameter drift phenomenon. Subsequently, we extract prototypes from the current dataset for the current adapter to complete the classifier head. During inference, we utilize the training-free self-refined adapter retrieval mechanism to correct the mistaken retrieval of irrelevant adapters. Finally, we implement a two-stage model ensemble to select the maximum logit.\n\n95 1.29 6080100Accuracy (%) 3.6 80 1.62 1   \nL2P ADAM+SSF L2P ADAM+SSF 670 LD2uPal ADAM+SSF   \nDualPrompt ADAM+Adapter DualPrompt ADAM+Adapter Prompt ADAM+Adapter   \nSimpleCIL CODA-Prompt SimpleCIL CODA-Prompt SimpleCIL CODA-Prompt   \n70 ADAM+FViPneTt-uSne SELACSEA ADAM+VPT-D MOS 40 ADAM+FViPneTt-uSne SELACSEA ADAM+VPT-D MOS 40 ADAM+FViPneTt-uSne SELACSEA ADAM+VPT-D MOS   \n20 40 60 80 100 50 100 150 200 50 100 150 200 Number of Classes Number of Classes Number of Classes   \n(a) CIFAR B0 Inc20 (b) CUB B0 Inc10 (c) ImageNet-R B0 Inc10 100 90 1.34   \n80 5.03   \nDualPrompt L2P ADAM+SASdFapter 2.12 80 670 DualPrompt L2P ADAM+SASdFapter 780 90 LD2uPalPrompt ADAM+SASdFapter   \n40 SimpleCIL CODA-Prompt SimpleCIL CODA-Prompt SimpleCIL CODA-Prompt   \nADAM+Finetune SLCA 50 ADAM+Finetune SLCA ADAM+Finetune SLCA   \nADAM+VPT-S EASE ADAM+VPT-S EASE 60 ADAM+VPT-S EASE   \nADAM+VPT-D MOS ADAM+VPT-D MOS ADAM+VPT-D MOS 40 50 100 150 200 50 100 150 200 250 300 10 20 30 40 50 Number of Classes Number of Classes Number of Classes   \n(d) ObjectNet B0 Inc20 (e) Omnibenchmark B0 Inc30 (f) VTAB B0 Inc10\n\n# Experiments\n\nIn this section, we evaluate MOS on seven benchmark datasets and compare it to other SOTA methods to demonstrate its superiority. Moreover, we provide an ablation study and a visualized analysis to validate the robustness of MOS.\n\n# Implementation Details\n\nDataset: Since PTMs possess extensive knowledge regarding upstream tasks, we follow (Zhou et al. 2024a) to evaluate the performance on CIFAR100 (Krizhevsky, Hinton et al. 2009), CUB200 (Wah et al. 2011), ImageNetR (Hendrycks et al. 2021a), ImageNet-A (Hendrycks et al. 2021b), objectNet (Barbu et al. 2019), Omnibenchmark (Zhang et al. 2022), and VTAB (Zhai et al. 2019). These datasets represent typical CIL benchmarks and include out-of-distribution datasets that exhibit a significant domain gap with ImageNet (i.e., the pre-trained dataset). Specifically, There are 50 classes in VTAB, 100 classes in CIFAR100, 200 classes in CUB, ImageNet-R, ImageNet-A, ObjectNet, and 300 classes in OmniBenchmark. More details are reported in the supplementary.\n\nDataset split: Following the benchmark setting (Rebuff et al. 2017; Wang et al. 2022c), we utilize the notation ‚ÄòB-m Inc- $\\cdot n ^ { \\prime }$ to represent class splits, where $m$ indicates the number of classes in the initial task, and $n$ denotes the number of classes in each subsequent incremental task. $m = 0$ means the total classes are equally divided into each task. For a consistent and fair comparison, we randomly shuffle class orders using a random seed of 1993 before splitting the data.\n\nWe ensure consistency in the training and testing sets across all methods, following (Zhou et al. 2024a).\n\nTraining details: We use PyTorch (Paszke et al. 2019) and PILOT (Sun et al. 2023) to implement all models on NVIDIA RTX 4090 with the same network backbone. Since the wide range of PTMs are publicly accessible (Wightman 2019), we choose two representative models following (Wang et al. 2022b; Zhou et al. 2024a), denoted as ViTB/16-IN1K and ViT-B/16-IN21K. They are both initially pre-trained on ImageNet21K, while the former is further finetuned on ImageNet1K. In MOS, we set the batch size to 48 and train for 20 epochs using the SGD optimizer with momentum. The learning rate is initially set to 0.01 and follows a cosine annealing decay pattern. The projection dimension $r$ in the adapter is set to 16, and the EMA factor parameter $\\alpha$ is set to 0.1.\n\nComparison methods: We choose state-of-the-art PTMbased CIL methods for comparison, such as Finetune Adapter (Chen et al. 2022), L2P (Wang et al. 2022c), DualPrompt (Wang et al. 2022b), CODA-Prompt (Smith et al. 2023), SimpleCIL (Zhou et al. 2024a), APER (Zhou et al. 2024a), SLCA (Zhang et al. 2023), EASE (Zhou et al. 2024c). In addition, we compare MOS with traditional CIL methods modified by PTM, including LwF (Li and Hoiem 2017), FOSTER (Wang et al. 2022a), MEMO (Zhou et al. 2023), iCaRL (Rebuffi et al. 2017), DER (Yan, Xie, and He 2021). We report the baseline method, which sequentially finetunes the PTM, denoted as Finetune. All methods are implemented with the same PTM for a fair comparison.\n\n90 3.95 90 80 Accuracy (%) 70 85 80 60 L2P iCaRL DualPrompt MEMO 50 CAODDAAM-+PrFAoidnmaeptptutenre FDMOEOSRSTER 75 w/‚Ä¶SEenlsfe-RmebflienedAd‚Ä¶apter‚Ä¶Retrieval‚Ä¶Mechanism + Baseline 70 100 120 140 160 180 200 50 100 150 200 Number of Classes Number‚Ä¶of‚Ä¶Classes (a) ImageNet-R B100 Inc50 (b) Ablation study\n\nEvaluation protocol: Following the benchmark established by (Rebuffi et al. 2017), we denote the Top-1 accuracy after the $b$ -th stage as $\\boldsymbol { \\mathcal { A } } _ { b }$ . Moreover, we use $\\mathcal { A } _ { B }$ (the performance after the last stage) and A¬Ø = B1 bB=1 Ab (average performance along incremental stages) as measurements.\n\n# Benchmark Comparison\n\nIn this section, we compare MOS with other SOTA methods across seven datasets and various backbone weights. As detailed in Table 1, MOS shows the best performance across all seven benchmarks, significantly surpassing the SOTA methods, such as SLCA, EASE, and APER. Furthermore, we present an analysis of the incremental performance trend of different methods in Figure 2 with ViT-B/16-IN1K. Notably, MOS outperforms the runner-up method by $2 \\% \\sim 5 \\%$ on CUB, ObjectNet, and OmniBenchmark, as highlighted in the annotations at the end of each image.\n\nBeyond the B0 setting presented in Table 1 and Figure 2, we extend our experiments to a larger base setting. In Figure 3a, we compare MOS with several SOTA methods and traditional methods using the same PTM. Although traditional methods require storing exemplars to recover previous knowledge, MOS achieves SOTA performance in this setting as well.\n\n# Ablation Study\n\nIn this section, we conduct an ablation study by incrementally adding each component to evaluate their effectiveness within MOS. Specifically, we present this ablation study on ImageNet-R $\\mathrm { B } 0 \\ \\mathrm { I n c } 2 0$ setting. As depicted in Figure 3b, ‚ÄòBaseline‚Äô refers to the PTM integrated with $\\mathbf { \\mathcal { A } } _ { 1 }$ (i.e., $\\phi ( \\mathbf { x } | \\mathbf { \\mathcal { A } } _ { 1 } ) )$ . Since we aim to mitigate parameter drift and build task-specific adapters, we report ‚Äòw/ Adapter Merge‚Äô by only using Eq. 6. Due to the mistaken retrieval issue, we propose using the model‚Äôs inherent capabilities to correct errors. We report the performance of ‚Äòw/ Self-Refined Adapter Retrieval Mechanism‚Äô by using this technique along with the adapter merging strategy. As shown in the figure, both the adapter merging strategy and self-refined adapter retrieval mechanism significantly improve the performance, which indicates MOS has the ability to correct itself and alleviate the catastrophic forgetting. Finally, we adjust the logits using Eq. 11 to trade off stability and plasticity, denoted as ‚Äòw/ Ensemble‚Äô. Ablations verify that every component in MOS contributes to improving performance.\n\n![](images/84cd144c52c226316b236d1b0de60a758356a8de5a3262398944774180c6dc63.jpg)  \nFigure 3: Left: Experimental results with large base classes. All methods are based on the same PTM (ViT-B/16-IN1K). Right: Ablation study of different components in MOS. We find each component within MOS enhances the performance.   \nFigure 4: Visualizations of self-refined adapter retrieval mechanism on ImageNet-R. The original images are depicted in the first row, followed by the top-5 prediction probability before the selfrefined process, and the probabilities after refinement in the last row. The ground-truth class is highlighted with red boxes.\n\n# Visualizations\n\nIn this section, we discuss how the self-refined adapter retrieval mechanism works. To illustrate this, we present the visualization of prediction results before and after the selfrefined process and analyze their differences. We choose images from ImageNet-R and utilize the model trained under the $\\mathrm { B 0 \\ I n c { 2 0 } }$ setting. The results are shown in Figure 4. As shown in these figures, MOS is capable of rectifying incorrect predictions. This is evident even in the example below, where the initial top-5 class predictions do not include the ground truth, yet MOS accurately corrects this error. It demonstrates that the model, using its inherent capabilities, can select the most suitable adapter for the current sample. Hence, MOS can use this adapter to extract more suitable features, which aids in enhancing prediction accuracy. These visualizations reveal that the self-refined adapter retrieval mechanism can help to correct the outputs, thereby enhancing the attention of the ground-truth class.\n\n# Conclusion\n\nIncremental learning is an increasingly prominent paradigm in real-world systems. This paper proposes a novel model surgery (MOS) for PTM-based CIL to rescue the model from forgetting previous knowledge. Specifically, we introduce an adapter merging method to mitigate parameter drift and design a training-free self-refined adapter retrieval mechanism for better adapter retrieval during inference. Our approach balances the stability-plasticity dilemma by leveraging the model‚Äôs inherent capabilities, enhancing generalization and adaptability. Extensive experiments on seven benchmark datasets validate the effectiveness of MOS. In future work, we aim to explore further application scenarios, such as few-shot class-incremental learning.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáËß£ÂÜ≥‰∫ÜÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºàPTMÔºâÂú®Á±ªÂ¢ûÈáèÂ≠¶‰π†ÔºàCILÔºâ‰∏≠ÁöÑÁÅæÈöæÊÄßÈÅóÂøòÈóÆÈ¢ò„ÄÇÂÖ∑‰ΩìÊù•ËØ¥ÔºåÂΩìÊ®°ÂûãÂú®Â≠¶‰π†Êñ∞Á±ªÊó∂Ôºå‰ºöÈÄêÊ∏êÈÅóÂøòÊóßÁ±ªÁöÑÁü•ËØÜ„ÄÇËøô‰∏ÄÈóÆÈ¢òÂú®ÂºÄÊîæ‰∏ñÁïåÁöÑÊµÅÂºèÊï∞ÊçÆÂú∫ÊôØ‰∏≠Â∞§‰∏∫ÂÖ≥ÈîÆÔºåÂ¶ÇÂåªÁñóÂΩ±ÂÉèÂàÜÊûê„ÄÅËá™Âä®È©æÈ©∂Á≠âÈúÄË¶ÅÊåÅÁª≠Â≠¶‰π†ÁöÑÈ¢ÜÂüü„ÄÇ\\n> *   Áé∞ÊúâÊñπÊ≥ïËôΩÁÑ∂ÈÄöËøáËΩªÈáèÁ∫ßÁªÑ‰ª∂Ë∞ÉÊï¥PTMÔºå‰ΩÜ‰ªçÂ≠òÂú®ÂèÇÊï∞ÊºÇÁßªÂíåÊ£ÄÁ¥¢ÈîôËØØÁöÑÈóÆÈ¢òÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫‚ÄúÊ®°ÂûãÊâãÊúØ‚ÄùÔºàMOSÔºâÁöÑÊñπÊ≥ïÔºåÈÄöËøáËÆ≠ÁªÉ‰ªªÂä°ÁâπÂÆöÁöÑÈÄÇÈÖçÂô®ÔºàadaptersÔºâÂíåËá™‰ºòÂåñÁöÑÈÄÇÈÖçÂô®Ê£ÄÁ¥¢Êú∫Âà∂Ôºå‰ªéÂèÇÊï∞ÂíåÊ£ÄÁ¥¢‰∏§‰∏™Â±ÇÈù¢ÁºìËß£ÁÅæÈöæÊÄßÈÅóÂøò„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **Ë¥°ÁåÆ1Ôºö** ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊåáÊï∞ÁßªÂä®Âπ≥ÂùáÔºàEMAÔºâÁöÑÈÄÇÈÖçÂô®ÂêàÂπ∂Á≠ñÁï•ÔºåÊúâÊïàÁºìËß£ÂèÇÊï∞ÊºÇÁßªÈóÆÈ¢ò„ÄÇ\\n> *   **Ë¥°ÁåÆ2Ôºö** ËÆæËÆ°‰∫Ü‰∏ÄÁßçÊó†ÈúÄÈ¢ùÂ§ñËÆ≠ÁªÉÁöÑËá™‰ºòÂåñÈÄÇÈÖçÂô®Ê£ÄÁ¥¢Êú∫Âà∂ÔºåÊòæËëóÊèêÂçá‰∫ÜÈÄÇÈÖçÂô®Ê£ÄÁ¥¢ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ\\n> *   **Ë¥°ÁåÆ3Ôºö** Âú®‰∏É‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåMOSÂú®Âπ≥ÂùáÂáÜÁ°ÆÁéá‰∏äÊØîÁé∞ÊúâÊúÄ‰Ω≥ÊñπÊ≥ïÔºàÂ¶ÇSLCAÂíåEASEÔºâÊèêÂçá‰∫Ü2%~5%„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   MOSÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÈÄöËøáÈÄÇÈÖçÂô®ÂêàÂπ∂ÂíåËá™‰ºòÂåñÊ£ÄÁ¥¢Êú∫Âà∂ÔºåÂàÜÂà´Ëß£ÂÜ≥ÂèÇÊï∞ÊºÇÁßªÂíåÊ£ÄÁ¥¢ÈîôËØØÈóÆÈ¢ò„ÄÇÈÄÇÈÖçÂô®ÂêàÂπ∂Á°Æ‰øùÊñ∞‰ªªÂä°ÁöÑÁü•ËØÜ‰∏ç‰ºöË¶ÜÁõñÊóß‰ªªÂä°ÁöÑÁü•ËØÜÔºåËÄåËá™‰ºòÂåñÊ£ÄÁ¥¢Êú∫Âà∂ÂàôÂà©Áî®Ê®°ÂûãËá™Ë∫´ÁöÑËÉΩÂäõÁ∫†Ê≠£ÈîôËØØÁöÑÈÄÇÈÖçÂô®ÈÄâÊã©„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** Áé∞ÊúâÊñπÊ≥ïÔºàÂ¶ÇL2PÂíåDualPromptÔºâÈÄöËøáÊèêÁ§∫Ê±†Ôºàprompt poolÔºâË∞ÉÊï¥PTMÔºå‰ΩÜËø≠‰ª£Êõ¥Êñ∞‰ºöÂØºËá¥ÂèÇÊï∞ÊºÇÁßªÔºå‰∏îÊ£ÄÁ¥¢Êú∫Âà∂ÂÆπÊòìÈîôËØØÈÄâÊã©Êó†ÂÖ≥ÊèêÁ§∫„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** MOSÈÄöËøáÈÄÇÈÖçÂô®ÂêàÂπ∂Á≠ñÁï•ÔºàEMAÔºâ‰øùÁïôÊóß‰ªªÂä°Áü•ËØÜÔºåÂπ∂ÈÄöËøáËá™‰ºòÂåñÊ£ÄÁ¥¢Êú∫Âà∂Âä®ÊÄÅÁ∫†Ê≠£ÈÄÇÈÖçÂô®ÈÄâÊã©Ôºå‰ªéËÄåÊèêÂçáÊ®°ÂûãÊÄßËÉΩ„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.  **ÈÄÇÈÖçÂô®ÂêàÂπ∂Ôºö** ÂØπ‰∫éÊØè‰∏™Êñ∞‰ªªÂä°ÔºåËÆ≠ÁªÉ‰∏Ä‰∏™‰ªªÂä°ÁâπÂÆöÁöÑÈÄÇÈÖçÂô®ÔºåÂπ∂ÈÄöËøáEMAÂ∞ÜÂÖ∂‰∏éÊóß‰ªªÂä°ÈÄÇÈÖçÂô®ÂêàÂπ∂ÔºåÂÖ¨Âºè‰∏∫Ôºö\\n     $$\\\\mathcal{A}_b = (1-\\\\alpha)\\\\hat{\\\\mathcal{A}}_b + \\\\frac{\\\\alpha}{b-1}\\\\sum_{k=1}^{b-1}\\\\mathcal{A}_k$$\\n     ÂÖ∂‰∏≠Ôºå$\\\\alpha$‰∏∫EMAÂõ†Â≠ê„ÄÇ\\n> 2.  **Ëá™‰ºòÂåñÊ£ÄÁ¥¢Êú∫Âà∂Ôºö** Âú®Êé®ÁêÜÈò∂ÊÆµÔºåÈÄöËøáËø≠‰ª£È™åËØÅÈÄÇÈÖçÂô®ÈÄâÊã©ÁöÑÊ≠£Á°ÆÊÄßÔºåÁõ¥Âà∞Ê®°ÂûãÈ¢ÑÊµãÁªìÊûúËá™Ê¥Ω„ÄÇÂÖ∑‰ΩìÊµÅÁ®ã‰∏∫Ôºö\\n     - ‰ΩøÁî®ÂàùÂßãÈÄÇÈÖçÂô®$\\\\mathcal{A}_1$È¢ÑÊµã‰ªªÂä°ID $i$„ÄÇ\\n     - ‰ΩøÁî®ÈÄÇÈÖçÂô®$\\\\mathcal{A}_i$È¢ÑÊµã‰ªªÂä°ID $j$„ÄÇ\\n     - Ëã•$i \\\\neq j$ÔºåÂàôÁî®$j$ÊõøÊç¢$i$Âπ∂ÈáçÂ§ç‰∏äËø∞Ê≠•È™§ÔºåÁõ¥Âà∞$i = j$„ÄÇ\\n> 3.  **Â§öÈò∂ÊÆµÊ®°ÂûãÈõÜÊàêÔºö** ÁªìÂêàÂàùÂßãÈÄÇÈÖçÂô®Âíå‰ºòÂåñÂêéÁöÑÈÄÇÈÖçÂô®È¢ÑÊµãÁªìÊûúÔºåÈÄöËøáÊúÄÂ§ßÂåñÂØπÊï∞Ê¶ÇÁéáËæìÂá∫ÊúÄÁªàÈ¢ÑÊµã„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   Finetune„ÄÅFinetune Adapter„ÄÅL2P„ÄÅDualPrompt„ÄÅCODA-Prompt„ÄÅSimpleCIL„ÄÅAPER„ÄÅSLCA„ÄÅEASEÁ≠â„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®Âπ≥ÂùáÂáÜÁ°ÆÁéáÔºàAverage AccuracyÔºâ‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®CIFAR100Êï∞ÊçÆÈõÜ‰∏äËææÂà∞‰∫Ü93.30%ÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãSLCAÔºà92.49%ÔºâÂíåEASEÔºà91.51%Ôºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü0.81‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®ÊúÄÂêé‰∏ÄÈò∂ÊÆµÂáÜÁ°ÆÁéáÔºàLast AccuracyÔºâ‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®ImageNet-RÊï∞ÊçÆÈõÜ‰∏äËææÂà∞‰∫Ü77.93%ÔºåËøúÈ´ò‰∫éÂü∫Á∫øÊ®°ÂûãCODA-PromptÔºà72.27%ÔºâÂíåDualPromptÔºà67.18%Ôºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü5.66‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®ÂèÇÊï∞ÊïàÁéá‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ï‰ªÖÈúÄ‰øùÂ≠òÈÄÇÈÖçÂô®ÂèÇÊï∞Ôºà$(B \\\\times L \\\\times 2dr)$ÔºâÔºåËøú‰Ωé‰∫éÂÆåÂÖ®ÂæÆË∞ÉPTMÁöÑÂèÇÊï∞ÈáèÔºåÂêåÊó∂ÊÄßËÉΩ‰ºò‰∫éËΩªÈáèÁ∫ßÊñπÊ≥ïÔºàÂ¶ÇL2PÔºâ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   Á±ªÂ¢ûÈáèÂ≠¶‰π† (Class-Incremental Learning, CIL)\\n*   È¢ÑËÆ≠ÁªÉÊ®°Âûã (Pre-Trained Model, PTM)\\n*   ÁÅæÈöæÊÄßÈÅóÂøò (Catastrophic Forgetting, N/A)\\n*   ÈÄÇÈÖçÂô®ÂêàÂπ∂ (Adapter Merging, N/A)\\n*   Ëá™‰ºòÂåñÊ£ÄÁ¥¢ (Self-Refined Retrieval, N/A)\\n*   ÊåáÊï∞ÁßªÂä®Âπ≥Âùá (Exponential Moving Average, EMA)\\n*   Â§öÈò∂ÊÆµÈõÜÊàê (Multi-Stage Ensemble, N/A)\"\n}\n```"
}