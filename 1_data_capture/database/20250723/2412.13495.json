{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.13495",
    "link": "https://arxiv.org/abs/2412.13495",
    "pdf_link": "https://arxiv.org/pdf/2412.13495.pdf",
    "title": "Federated t-SNE and UMAP for Distributed Data Visualization",
    "authors": [
        "Dong Qiao",
        "Xinxian Ma",
        "Jicong Fan"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "2024-12-18",
    "venue": "arXiv.org",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "The Chinese University of Hong Kong, Shenzhen"
    ],
    "paper_content": "# Federated t-SNE and UMAP for Distributed Data Visualization\n\nDong Qiao\\*, Xinxian Ma\\*, Jicong Fan†\n\nSchool of Data Science, The Chinese University of Hong Kong, Shenzhen, China dongqiao@link.cuhk.edu.cn, xinxianma@link.cuhk.edu.cn, fanjicong@cuhk.edu.cn\n\n# Abstract\n\nHigh-dimensional data visualization is crucial in the big data era and these techniques such as t-SNE and UMAP have been widely used in science and engineering. Big data, however, is often distributed across multiple data centers and subject to security and privacy concerns, which leads to difficulties for the standard algorithms of t-SNE and UMAP. To tackle the challenge, this work proposes Fed-tSNE and Fed-UMAP, which provide high-dimensional data visualization under the framework of federated learning, without exchanging data across clients or sending data to the central server. The main idea of Fed-tSNE and Fed-UMAP is implicitly learning the distribution information of data in a manner of federated learning and then estimating the global distance matrix for t-SNE and UMAP. To further enhance the protection of data privacy, we propose Fed- $\\mathrm { t S N E } +$ and Fed-UMAP+. We also extend our idea to federated spectral clustering, yielding algorithms of clustering distributed data. In addition to these new algorithms, we offer theoretical guarantees of optimization convergence, distance and similarity estimation, and differential privacy. Experiments on multiple datasets demonstrate that, compared to the original algorithms, the accuracy drops of our federated algorithms are tiny.\n\nUMAP map the data points to a two- or three-dimensional space, exhibiting the intrinsic data distribution or pattern of the original high-dimensional data. Due to their superiority over other methods such as PCA (Jolliffe and Cadima 2016), Isomap (Tenenbaum, De Silva, and Langford 2000), and autoencoder (Hinton and Salakhutdinov 2006), they have been used for visualizing images, tabular data (Hao et al. 2021), text (Grootendorst 2022), and graphs (Wu, Zhang, and Fan 2023) in diverse fields and provide huge convenience for scientific research and engineering practice (Becht et al. 2019). Besides visualization, t-SNE and UMAP are also useful in clustering (Linderman and Steinerberger 2019) and outlier detection (Fu, Zhang, and Fan 2024). There are also a few variants of t-SNE (Yang et al. 2009; Carreira-Perpina´n 2010; Xie et al. 2011; Van Der Maaten 2014; Gisbrecht, Schulz, and Hammer 2015; Pezzotti et al. 2016; Linderman et al. 2019; Chatzimparmpas, Martins, and Kerren 2020; Sun, Han, and Fan 2023) and UMAP (Sainburg, McInnes, and Gentner 2021; Nolet et al. 2021). For instance, Van Der Maaten (2014) used tree-based algorithms to accelerate the implementation of t-SNE. Sainburg, McInnes, and Gentner (2021) proposed a parametric UMAP that can visualize new data without re-training the model.\n\n# 1 Introduction\n\nHigh-dimensional data are prevalent in science and engineering and their structures are often very complicated, which makes dimensionality reduction and data visualization appealing in knowledge discovery and decision-making (Jolliffe and Cadima 2016; Hinton and Salakhutdinov 2006; Van Der Maaten et al. 2009). In the past decades, many algorithms have been proposed for dimensionality and visualization (Pearson 1901; Fisher 1936; Sammon 1969; Baker 1977; Kohonen 1982; Scho¨lkopf, Smola, and Mu¨ller 1998; Roweis and Saul 2000; Tenenbaum, De Silva, and Langford 2000; Van der Maaten and Hinton 2008; Fan et al. 2018; McInnes et al. 2018). Perhaps, the most popular algorithms in recent years are the t-distributed stochastic neighbor embedding (t-SNE) developed by (Van der Maaten and Hinton 2008) and the Uniform Manifold Approximation and Projection (UMAP) proposed by (McInnes et al. 2018). T-SNE and\n\nIn many real cases such as mobile devices, IoT networks, medical records, and social media platforms, the high-dimensional data are distributed across multiple data centers and subject to security and privacy concerns (Dwork, Roth et al. 2014; McMahan et al. 2017; Kairouz et al. 2021; Qiao, Ding, and Fan 2024), which leads to difficulties for the standard algorithms of t-SNE and UMAP. Specifically, in tSNE and UMAP, we need to compute the pair-wise distance or similarity between all data points, meaning that different data centers or clients should share their data mutually or send their data to a common central server, which will leak data privacy and lose information security. To address this challenge, we propose federated t-SNE and federated UMAP in this work. Our main idea is implicitly learning the distribution information of data in a manner of federated learning and then estimating the global distance matrix for t-SNE and UMAP. The contribution of this work is summarized as follows:\n\n• We propose Fed-tSNE and Fed-UMAP that are able to visualize distributed data of high-dimension. • We further provide Fed-tSNE $^ +$ and Fed-UMAP $+$ to en\n\nhance privacy protection.\n\n• We extend our idea to federated spectral clustering for distributed data with privacy protection. • We provide theoretical guarantees such as reconstruction error bounds and differential privacy analysis.\n\n# 2 Related Work\n\nt-SNE t-SNE (Van der Maaten and Hinton 2008) aims to preserve the pair-wise similarities from high-dimension space $\\mathcal { P }$ to low-dimension space $\\mathcal { Q }$ . The pair-wise similarities are measured as the probability that two data points are neighbors mutually. Specifically, given high-dimensional data points $\\mathbf { x } _ { 1 } , \\mathbf { x } _ { 2 } , \\ldots , \\mathbf { x } _ { N }$ in $\\mathbb { R } ^ { \\bar { D } }$ , $\\mathbf { \\sigma } _ { \\mathrm { t - S N E } }$ computes the joint probability matrix $\\mathbf { P } \\in \\mathbb { R } ^ { N \\times N }$ , in which $p _ { i j } = 0$ if $i = j$ , and $\\begin{array} { r } { p _ { i j } = \\frac { p _ { i \\mid j } + p _ { j \\mid i } } { 2 N } } \\end{array}$ , if $i \\neq j$ , where\n\n$$\n\\begin{array} { r } { p _ { j | i } = \\frac { \\exp \\left( - \\| \\mathbf { x } _ { i } - \\mathbf { x } _ { j } \\| _ { 2 } ^ { 2 } / ( 2 \\tau _ { i } ^ { 2 } ) \\right) } { \\sum _ { \\ell \\in [ N ] \\setminus \\{ i \\} } \\exp \\left( - \\| \\mathbf { x } _ { i } - \\mathbf { x } _ { \\ell } \\| _ { 2 } ^ { 2 } / ( 2 \\tau _ { i } ^ { 2 } ) \\right) } . } \\end{array}\n$$\n\nIn (1), $\\tau _ { i }$ is the bandwidth of the Gaussian kernel. Suppose y1, y2, . . . , yN are the low-dimensional embeddings in Rd, where $d \\ll D$ , t-SNE constructs a probability matrix $\\mathbf { Q }$ by\n\n$$\n\\begin{array} { r } { q _ { i j } = \\frac { \\left( 1 + \\| \\mathbf { y } _ { i } - \\mathbf { y } _ { j } \\| _ { 2 } ^ { 2 } \\right) ^ { - 1 } } { \\sum _ { \\ell , s \\in [ N ] , \\ell \\neq s } \\left( 1 + \\| \\mathbf { y } _ { \\ell } - \\mathbf { y } _ { s } \\| _ { 2 } ^ { 2 } \\right) ^ { - 1 } } } \\end{array}\n$$\n\nwhere $i \\neq j$ . Then t-SNE obtains $\\mathbf { y } _ { 1 } , \\mathbf { y } _ { 2 } , \\ldots , \\mathbf { y } _ { N }$ by minimizing the Kullback-Leibler (KL) divergence\n\n$$\n\\underset { { \\bf y } _ { 1 } , . . . , { \\bf y } _ { N } } { \\mathrm { m i n i m i z e } } ~ \\sum _ { i \\ne j } p _ { i j } \\log \\frac { p _ { i j } } { q _ { i j } }\n$$\n\nUMAP UMAP (McInnes et al. 2018) is a little similar to tSNE. It starts by constructing a weighted k-NN graph in the high-dimensional space. The edge weights between points $\\mathbf { x } _ { i }$ and $\\mathbf { x } _ { j }$ are defined based on a fuzzy set membership, representing the probability that $\\mathbf { x } _ { j }$ is in the neighborhood of $\\mathbf { x } _ { i }$ . Specifically, the membership strength is computed using\n\n$$\n\\begin{array} { r } { \\mu _ { i | j } = \\exp \\big ( - \\| \\mathbf { x } _ { i } - \\mathbf { x } _ { j } \\| ^ { 2 } / \\sigma _ { i } \\big ) , } \\end{array}\n$$\n\nwhere $\\sigma _ { i }$ is a local scaling factor determined by the $\\mathbf { k }$ -NNs of $\\mathbf { x } _ { i }$ . The final membership strength is symmetrized as\n\n$$\n\\mu _ { i j } = \\mu _ { i | j } + \\mu _ { j | i } - \\mu _ { i | j } \\cdot \\mu _ { j | i }\n$$\n\nIn the low-dimensional space, the probability of two points being neighbors is modeled using a smooth, differentiable approximation to a fuzzy set membership function. The edge weights between points $\\mathbf { y } _ { i }$ and $\\mathbf { y } _ { j }$ are given by\n\n$$\n\\begin{array} { r } { \\mu _ { i j } ^ { \\prime } = \\frac { 1 } { 1 + a \\| \\mathbf { y } _ { i } - \\mathbf { y } _ { j } \\| ^ { 2 b } } } \\end{array}\n$$\n\nwhere $a$ and $b$ are hyperparameters typically set based on empirical data to control the spread of points in the lowdimensional space. UMAP minimizes the cross-entropy between the high-dimensional fuzzy simplicial set and the lowdimensional fuzzy simplicial set, i.e.,\n\n$$\n\\underset { \\mathbf { y } _ { 1 } , . . . , \\mathbf { y } _ { N } } { \\mathrm { m i n i m i z e } } \\sum _ { i \\neq j } \\mu _ { i j } \\log \\Big ( \\frac { \\mu _ { i j } } { \\mu _ { i j } ^ { \\prime } } \\Big ) + \\big ( 1 - \\mu _ { i j } \\big ) \\log \\Big ( \\frac { 1 - \\mu _ { i j } } { 1 - \\mu _ { i j } ^ { \\prime } } \\Big )\n$$\n\nDiscussion Studies about federated dimensionality reduction or data visualization are scarce in the literature. Grammenos et al. (2020) proposed a federated, asynchronous, and $( \\epsilon , \\delta )$ -differentially private algorithm for PCA in the memory-limited setting. Briguglio et al. (2023) developed a federated supervised PCA for supervised learning. NovoaParadela, Fontenla-Romero, and Guijarro-Berdin˜as (2023) proposed a privacy-preserving training algorithm for deep autoencoders. Different from PCA and autoencoders, in tSNE and UMAP, we need to compute the pair-wise distance or similarity between data points, which leads to significantly greater difficulty in developing federated learning algorithms. Saha et al. (2022) proposed a decentralized data stochastic neighbor embedding, dSNE. However, dSNE assumes that there is a shared subset of data among different clients, which may not hold in real applications.\n\n# 3 Federated Distribution Learning\n\n# 3.1 Framework\n\nSuppose data $\\mathbb { R } ^ { m \\times n _ { x } } \\ni X = \\{ X _ { p } \\} _ { p = 1 } ^ { P }$ are distributed at $P$ clients, where $\\pmb { X } _ { p } \\in \\mathbb { R } ^ { m \\times n _ { p } }$ belongs to client $p$ and $\\begin{array} { r } { \\sum _ { p = 1 } ^ { P } n _ { p } = n _ { x } } \\end{array}$ . To implement t-SNE and UMAP, we need to compute a matrix $D _ { X , X } \\in \\mathbb { R } ^ { n _ { x } \\times n _ { x } }$ of distances between all data pairs in $X$ , which requires data sharing between the clients and central server, leading to data or privacy leaks. We propose to find an estimate of the distance or similarity matrix without data sharing. To do this, we let the central server construct a set of intermediate data points denoted by $\\pmb { Y } = [ \\pmb { y } _ { 1 } , \\dots , \\pmb { y } _ { n _ { y } } ] \\in \\mathbb { R } ^ { m \\times n _ { y } }$ and then compute distance matrices $D _ { Y , Y }$ and $\\{ D _ { X _ { p } , Y } \\} _ { p = 1 } ^ { P }$ . These distance matrices can be used to construct an estimate $\\widehat { D } _ { X , X }$ of $D _ { X , X }$ by applying the Nytro¨m method (Williambs and Seeger 2001) (to be detailed later). However, the choice of $\\boldsymbol { Y }$ affects the accuracy of $\\widehat { D } _ { X , X }$ , further influencing the performance of t-SNE and UbMAP.\n\nSince Nytro¨m method (Williams and Seeger 2001) aims to estimate an entire matrix using its small sub-matrices, the sub-matrices should preserve the key information of the entire matrix, which means a good $Y = [ \\pmb { y } _ { 1 } , \\dots , \\pmb { y } _ { n _ { y } } ] \\in$ $\\mathbb { R } ^ { m \\times n _ { y } }$ should capture the distribution information of $X$ . Therefore, we propose to learn such a $\\boldsymbol { Y }$ adaptively from the $P$ clients via solving the following federated distribution learning (FedDL) framework:\n\n$$\n\\underset { \\pmb { Y } } { \\mathrm { m i n i m i z e } } \\ F ( \\pmb { Y } ) \\triangleq \\sum _ { p = 1 } ^ { P } \\omega _ { p } f _ { p } ( \\pmb { Y } )\n$$\n\nwhere $f _ { p }$ is the local objective function for each client, and $\\omega _ { 1 } , \\ldots , \\omega _ { P }$ are nonnegative weights for the clients. Without loss of generality, we set $\\omega _ { 1 } = \\cdot \\cdot \\cdot = \\omega _ { P } = 1 / P$ for convenience in the remaining context. In this work, we set $f _ { p }$ to be the Maximum Mean Discrepancy (MMD) (Gretton et al.\n\n2012) metric:\n\n$$\n\\begin{array} { r l } & { \\quad f _ { p } ( \\boldsymbol { Y } ) = \\boldsymbol { \\mathrm { M M D } } ( \\boldsymbol { X } _ { p } , \\boldsymbol { Y } ) } \\\\ & { = \\displaystyle \\frac { 1 } { n _ { p } ( n _ { p } - 1 ) } \\sum _ { i = 1 } ^ { n _ { p } } \\sum _ { j \\neq i } ^ { n _ { p } } k \\left( ( \\boldsymbol { X } _ { p } ) _ { : , i } , ( \\boldsymbol { X } _ { p } ) _ { : , j } \\right) } \\\\ & { \\quad - \\displaystyle \\frac { 2 } { n _ { p } n _ { y } } \\sum _ { i = 1 } ^ { n _ { p } } \\sum _ { j = 1 } ^ { n _ { y } } k \\left( ( \\boldsymbol { X } _ { p } ) _ { : , i } , ( \\boldsymbol { Y } ) _ { : , j } \\right) } \\\\ & { \\quad + \\displaystyle \\frac { 1 } { n _ { y } ( n _ { y } - 1 ) } \\sum _ { i = 1 } ^ { n _ { y } } \\sum _ { j \\neq i } ^ { n _ { y } } k \\left( ( \\boldsymbol { Y } ) _ { : , i } , ( \\boldsymbol { Y } ) _ { : , j } \\right) } \\end{array}\n$$\n\nor in the following compact form\n\n$$\n\\begin{array} { r l } & { ~ f _ { p } ( { \\pmb Y } ) = { \\mathrm { M M D } } ( X _ { p } , { \\pmb Y } ) } \\\\ & { = \\frac { 1 } { n _ { p } ( n _ { p } - 1 ) } \\left[ { \\pmb 1 } _ { n _ { p } } ^ { T } K _ { X _ { p } , { \\pmb X } _ { p } } { \\bf 1 } _ { n _ { p } } - n _ { p } \\right] - \\frac { 2 } { n _ { p } n _ { y } } { \\bf 1 } _ { n _ { p } } ^ { T } K _ { { \\pmb X } _ { p } , { \\pmb Y } } { \\bf 1 } _ { n _ { y } } } \\\\ & { ~ + \\frac { 1 } { n _ { y } ( n _ { y } - 1 ) } \\left[ { \\bf 1 } _ { n _ { y } } ^ { T } K _ { { \\pmb Y } , { \\pmb Y } } { \\bf 1 } _ { n _ { y } } - n _ { y } \\right] } \\end{array}\n$$\n\nwhere $k ( \\cdot , \\cdot )$ is a kernel function and $\\kappa .$ denotes the kernel matrix computed from two matrices. MMD is a distance metric between two distributions and (10) is actually an estimation of MMD with finite samples from two distributions. If we use the Gaussian kernel $\\bar { k } ( { \\pmb x } _ { i } , { \\pmb y } _ { j } ) = \\exp ( - \\gamma \\| { \\pmb x } _ { i } -$ $y _ { j } \\| ^ { 2 } )$ , MMD compares all-order statistics between two distributions. For any $\\pmb { X } \\in \\mathbb { R } ^ { m \\times n _ { x } }$ and $\\pmb { Y } \\in \\mathbb { R } ^ { m \\times n _ { y } }$ , we calculate the Gaussian kernel matrix as $K _ { X , Y } = \\exp ( - \\gamma D ^ { 2 } )$ , where $D ^ { 2 }$ is the squared pairwise distance matrix between $X$ and $\\boldsymbol { Y }$ , i.e., $\\begin{array} { r } { \\dot { D } ^ { 2 } \\ = \\ \\operatorname { D i a g } ( X ^ { T } X ) \\mathbf { 1 } _ { n _ { y } } ^ { T } \\ - \\ 2 X ^ { T } Y \\ + } \\end{array}$ $\\mathbf { 1 } _ { n _ { x } } { \\mathrm { D i a g } } ( { \\pmb Y } ^ { T } { \\pmb Y } ) ^ { T }$ .\n\nCombining (8) and (10), we have the following optimization problem of federated distribution learning\n\n$$\n\\underset { \\mathbf { Y } } { \\mathrm { m i n i m i z e } } \\ \\sum _ { p = 1 } ^ { P } \\omega _ { p } \\times \\mathbf { M M D } ( X _ { p } , \\mathbf { Y } )\n$$\n\nBy solving this problem, the central server or $\\boldsymbol { Y }$ equivalently can learn the distribution information of the data distributed on the $P$ clients. Based on such an $\\boldsymbol { Y }$ , we can estimate the distance or similarity matrix between all data points in $X$ , which will be detailed later.\n\n# 3.2 Optimization\n\nFor a client $p$ , we consider the corresponding local optimization problem\n\n$$\n{ \\underset { \\pmb { Y } } { \\mathrm { m i n i m i z e } } } \\ f _ { p } ( \\pmb { Y } )\n$$\n\nwhere $f _ { p } ( Y ) =  { \\mathrm { M M D } } ( X _ { p } , Y )$ . Due to the presence of kernel function, we have to use some numerical methods like gradient descent to update the decision variable $\\boldsymbol { Y }$ . The gradient of $f _ { p }$ at $\\boldsymbol { Y }$ is\n\n$$\n\\begin{array} { c } { { \\displaystyle \\nabla f _ { p } ( { \\pmb Y } ) = \\frac { - 4 \\gamma } { n _ { p } n _ { y } } \\left[ { \\pmb X } _ { p } { \\pmb K } _ { { \\pmb X } _ { p } , { \\pmb Y } } - { \\pmb Y } \\mathrm { D i a g } ( \\mathbf { 1 } _ { n _ { p } } ^ { T } { \\pmb K } _ { { \\pmb X } _ { p } , { \\pmb Y } } ) \\right] } } \\\\ { { \\displaystyle + \\frac { 4 \\gamma } { n _ { y } ( n _ { y } - 1 ) } \\left[ { \\pmb Y } { \\pmb K } _ { { \\pmb Y } , { \\pmb Y } } - { \\pmb Y } \\mathrm { D i a g } ( \\mathbf { 1 } _ { n _ { y } } ^ { T } { \\pmb K } _ { { \\pmb Y } , { \\pmb Y } } ) \\right] } } \\end{array}\n$$\n\nAlgorithm 1: Federated Distribution Learning\n\n1: Server broadcast an initial $\\mathbf { Y } ^ { 0 }$ to all clients.   \n2: for round $s = 1$ to $S$ do   \n3: Client side:   \n4: for client $p = 1$ to $P$ in parallel do   \n5: Set $\\bar { Y _ { p } ^ { s , 0 } } = Y ^ { s - 1 }$   \n6: Update local variable $Y _ { p } ^ { s }$ :   \n7: for $t = 1$ to $Q$ do   \n8: 1 $Y _ { p } ^ { s , t } = Y _ { p } ^ { s , t - 1 } - \\eta _ { s } \\nabla f _ { p } ( Y _ { p } ^ { s , t - 1 } )$   \n9: end for   \n10: Denote Y  = Y s,Q   \n11: Upload $Y _ { p } ^ { s }$ (resp., $\\nabla f _ { p } ( Y _ { p } ^ { s , t } ) )$ to the server.   \n12: end for   \n13: Server side: compute $\\begin{array} { r } { Y ^ { s } = \\frac { 1 } { P } \\sum _ { p = 1 } ^ { P } Y _ { p } ^ { s } } \\end{array}$ .   \n14: $\\begin{array} { r } { \\Big ( r e s p . , Y ^ { s } \\gets Y ^ { s - 1 } - \\eta _ { s } ^ { \\prime } \\times \\frac { 1 } { P } \\sum _ { p = 1 } ^ { P } \\nabla f _ { p } ( Y _ { p } ^ { s } ) \\Big ) } \\end{array}$   \n15: Broadcast $\\mathbf { \\nabla } \\mathbf { Y } ^ { s }$ to all clients.   \n16: end for   \nEnsure: $\\boldsymbol { Y }$\n\nTo make it more explicit, we outline the key steps of FedDL to demonstrate how the central server coordinates local models for learning global distribution in a federated way.\n\n• Step 1: The central server initializes a global $Y _ { g }$ before the learning cycle begins and broadcasts it to all participating local models.   \n• Step 2: The local clients copy the global $Y _ { g }$ as their uniform initial guess $Y _ { p }$ and compute the gradient $\\nabla f _ { p } ( { \\pmb Y } _ { p } )$ .   \n• Step 3: Each client $p$ sends its gradient $\\nabla f _ { p } ( { \\pmb Y } _ { p } )$ or the updated $\\boldsymbol { Y }$ , i.e.,\n\n$$\nY _ { p } \\gets Y _ { p } - \\eta \\nabla f _ { p } ( Y _ { p } )\n$$\n\nto the central server, where $\\eta$ is the step size and can be set as the reverse of the Lipschitz constant of gradient if possible.\n\n• Step 4: The central server updates the global $\\boldsymbol { Y }$ by averaging all posted $Y _ { p }$ , i.e.,\n\n$$\nY = \\frac { 1 } { P } \\sum _ { p = 1 } ^ { P } Y _ { p } ,\n$$\n\nor performing gradient descent with the average of all $\\nabla \\bar { f } _ { p } ( { \\pmb Y } _ { p } )$ , i.e.,\n\n$$\n\\boldsymbol { Y }  \\boldsymbol { Y } - \\boldsymbol { \\eta ^ { \\prime } } \\times \\frac { 1 } { P } \\sum _ { p = 1 } ^ { P } \\nabla f _ { p } ( Y _ { p } ) ,\n$$\n\nwhere $\\eta ^ { \\prime }$ is a step size.\n\n• Step 5: The central server broadcasts the newly aggregated communication variables so as to trigger the next local updates.\n\nThe optimization details are summarized in Algorithm 1. In the algorithm, for each client $p$ , the time complexity per iteration is $\\mathcal { O } ( m n _ { p } ^ { 2 } + m n _ { p } n _ { y } )$ and the space complexity is $\\mathcal { O } ( m n _ { p } + m n _ { y } + n _ { p } n _ { y } )$ .\n\nIn Algorithm 1, it is necessary to share some variables like the global distribution information $\\boldsymbol { Y }$ or the gradient $\\nabla f _ { p } ( { \\pmb Y } )$ for proceeding the process of training. This may result in data privacy leakage. Data or gradient perturbation by some special types of noise is a common way to enhance the security of federated algorithms. In Section 5, we present the theoretical guarantees of distance estimation and similarity estimation and analyze the properties of differential privacy in such two ways, respectively.\n\n# 3.3 Convergence Analysis\n\nSince we adopt MMD as our local objective function, they are all bounded below. Here, we give the convergence guarantee of Algorithm 1.\n\nTheorem 1. Assume the gradient of all local objective functions $\\{ f _ { p } \\} _ { p = 1 } ^ { P }$ are $L _ { p }$ -Lipschitz continuous, $L \\ =$ $\\scriptstyle \\sum _ { p = 1 } ^ { P } \\omega _ { p } L _ { p }$ with $\\begin{array} { r } { \\omega _ { p } = \\frac { n _ { p } } { n _ { x } } } \\end{array}$ , = PpP=L1 2ωpL2p , and ∥∇fp − $\\nabla f _ { p ^ { \\prime } } \\| _ { F } \\leq \\zeta$ for all $p , p ^ { \\prime }$ , the sequence $\\{ Y ^ { s , t } \\}$ generated by Algorithm $^ { l }$ with step size $1 / L$ satisfies\n\n$$\n\\begin{array} { r l } & { \\displaystyle \\frac { 1 } { S Q } \\sum _ { s = 1 } ^ { S } \\sum _ { t = 1 } ^ { Q } \\| { \\boldsymbol { Y } } ^ { s , t } - { \\boldsymbol { Y } } ^ { s , t - 1 } \\| _ { F } ^ { 2 } \\leq \\frac { 4 } { S Q L } [ F ( { \\boldsymbol { Y } } ^ { 0 } ) - F ( { \\boldsymbol { Y } } ^ { S } ) ] } \\\\ & { \\displaystyle + \\frac { 1 2 \\rho _ { L } \\zeta ^ { 2 } ( Q + 1 ) ( 2 Q + 1 ) } { L ^ { 2 } [ 1 - 3 ( Q - 1 ) ^ { 2 } ( \\rho _ { L } + \\frac { \\operatorname* { m a x } _ { p } L _ { p } ^ { 2 } } { L ^ { 2 } } ) ] } } \\end{array}\n$$\n\nThe proof can be found in Appendix F. It can be seen that when $S Q$ goes large enough, our algorithm converges to a finite value that is small provided that $\\zeta$ is small. Figure 2 in Section 6.1 will show the convergence of the optimization numerically.\n\n# 4 Applications of FedDL\n\n# 4.1 Federated tSNE and UMAP\n\nNystrom approximation is a technique that can approximate a positive semi-definite (PSD) matrix merely through a subset of its rows and columns (Williams and Seeger 2001). Consider a PSD matrix $S _ { + } ^ { n } \\ni H \\succeq 0$ that has a representation of block matrix\n\n$$\n\\mathbf { } S _ { + } ^ { n } \\ni \\mathbf { \\vec { H } } = \\left[ \\begin{array} { c c } { W } & { B ^ { T } } \\\\ { B } & { Z } \\end{array} \\right]\n$$\n\nwhere ${ \\pmb W } \\in \\mathcal { S } _ { + } ^ { c } , { \\pmb B } \\in \\mathbb { R } ^ { ( n - c ) \\times c }$ , and $Z \\in S _ { + } ^ { n - c }$ for which $c \\ll n$ . Specifically, suppose $z$ is unknown, we can approximate it using $W , B$ , and $B ^ { T }$ as\n\n$$\nZ \\approx B W _ { k } ^ { \\dagger } B ^ { T } \\triangleq \\widehat { Z }\n$$\n\nThis means we can approximate the incomplete $H$ by $\\widehat { H } =$ $[ W , B ^ { T } ; B , \\widehat { Z } ]$ . By Nystro¨m method, we can approximcate a distance or  ibmilarity matrix on large-scale dataset in a relatively low computational complexity. Some literature gives some useful upper bounds on Nystro¨m approximation in terms of Frobenius norm and spectral norm for different sampling techniques (Kumar, Mohri, and Talwalkar $2 0 0 9 { \\mathrm { b } }$ ; Drineas and Mahoney 2005; Zhang, Tsang, and Kwok 2008;\n\nKumar, Mohri, and Talwalkar $2 0 0 9 \\mathrm { a }$ ; Li, Kwok, and Lu 2010). Here, we present the upper bounds of Nystro¨m approximation in (Drineas and Mahoney 2005) for our subsequent derivation.\n\nTheorem 2 (Error bounds of Nystr¨om approximation). Given $\\pmb { X } = [ \\pmb { x } _ { 1 } , \\dots , \\pmb { x } _ { n } ] \\in \\mathbb { R } ^ { m \\times n }$ , let $\\widehat { H }$ be the rank- $k$ Nystrom approximation of $\\pmb { H }$ only throughc columns sampled uniformly at random without replacement from $H$ , and $\\pmb { H } _ { k }$ be the best rank- $k$ approximation of $H$ . Then, the following inequalities hold for any sample of size $c$ :\n\n$$\n\\begin{array} { r l } & { \\| \\pmb { H } - \\widehat { \\pmb { H } } \\| _ { 2 } \\leq \\| \\pmb { H } - \\pmb { H } _ { k } \\| _ { 2 } + \\frac { 2 n \\rho } { \\sqrt { c } } } \\\\ & { \\| \\pmb { H } - \\widehat { \\pmb { H } } \\| _ { F } \\leq \\| \\pmb { H } - \\pmb { H } _ { k } \\| _ { F } + \\rho \\left( \\frac { 6 4 k } { c } \\right) ^ { 1 / 4 } } \\end{array}\n$$\n\nwhere $\\rho = \\operatorname* { m a x } _ { i } H _ { i i }$ .\n\nWithout the retrieval of raw data from clients, we present federated tSNE (Fed-tSNE) and federated UMAP (FedUMAP) to visualize the high-dimensional data distributed across multiple regional centers. The main idea is to perform Algorithm 1 to learn a $\\boldsymbol { Y }$ and then each client $p$ posts the distance matrix $D _ { X _ { p } , Y } \\in \\mathbb { R } ^ { n _ { p } \\times n _ { y } }$ between $X _ { p }$ and $\\boldsymbol { Y }$ to the central server. Consequently, the central server assembles all $D _ { X _ { p } , Y }$ to form\n\n$$\n\\pmb { B } = [ \\pmb { D } _ { { \\pmb X } _ { 1 } , { \\pmb Y } } ^ { \\top } \\pmb { D } _ { { \\pmb X } _ { 2 } , { \\pmb Y } } ^ { \\top } \\cdot \\cdot \\cdot \\pmb { D } _ { { \\pmb X } _ { P } , { \\pmb Y } } ^ { \\top } ] ^ { \\top }\n$$\n\nand estimate $D _ { X , X }$ as\n\n$$\n\\widehat { D } _ { X , X } = B W _ { k } ^ { \\dagger } B ^ { \\top }\n$$\n\nwhere $W = D _ { Y , Y }$ ,bi.e., the distance matrix of $\\boldsymbol { Y }$ . Note that in the case that $\\dot { \\boldsymbol W }$ is singular, we can add an identity matrix to it, i.e., $W + \\lambda I$ , where $\\lambda > 0$ is a small constant. Finally, the central server implements either t-SNE or UMAP based on $D _ { X , X }$ . The steps are summarized into Algorithm 2.\n\n# Algorithm 2: Fed-tSNE and Fed-UMAP\n\nRequire: Distributed data $\\{ X _ { 1 } , X _ { 2 } , \\ldots , X _ { P } \\}$ at $P$ clients.   \n1: Perform Algorithm 1 to compute $\\boldsymbol { Y }$ .   \n2: Each client $p$ computes the distance matrix $D _ { X _ { p } , Y }$ and posts it to the central server.   \n3: The central server constructs $B$ using (21) and computes $\\widehat { D } _ { X , X }$ using (22).   \n4: The  ebntral server runs either t-SNE or UMAP on $\\widehat { D } _ { X , X }$ to obtain the low-dimensional embeddings $z$ .\n\n# Ensubre: $z$\n\nNote that sampling data points from clients like in classical Nystro¨m approximation is prohibitive in the federated settings. Thus, it motivates us to use FedDL to learn a useful set of fake points (i.e., landmarks) close enough to the data across the clients in terms of MMD.\n\n# 4.2 Federated Spectral Clustering\n\nNote that after running Algorithm 1, if each client post the kernel matrix $K _ { X _ { p } , Y }$ rather than the distance matrix $D _ { X _ { p } , Y }$ to the central server, the central server can construct a kernel or similarity matrix $\\widehat { K } _ { X , X }$ that is useful for spectral clustering. Thus we obta ncfederated spectral clustering, of which the steps are summarized into Algorithm 3.\n\n![](images/bf744d14775785e1c49265737c69d7307503c13e1f9dc24129a285e2d638e415.jpg)  \nFigure 1: Visualization of MNIST data using t-SNE, UMAP, and the proposed federated variants.\n\n# Algorithm 3: Fed-SpeClust\n\nRequire: Distributed data $\\{ X _ { 1 } , X _ { 2 } , \\ldots , X _ { P } \\}$ at $P$ clients.   \n1: Perform Algorithm 1 to compute $\\mathbf { \\Delta } _ { Y }$ .   \n2: Each client $p$ computes the kernel matrix $K _ { X _ { p } , Y }$ and posts it to the central server.   \n3: The central server constructs $\\begin{array} { r l r l } { C } & { { } } & { } & { { } = } \\end{array}$ $\\begin{array} { r l } { [ K _ { X _ { 1 } , Y } ^ { \\top } } & { { } K _ { X _ { 2 } , Y } ^ { \\top } \\quad \\cdot \\cdot \\quad K _ { X _ { P } , Y } ^ { \\top } ] ^ { \\top } } \\end{array}$ and computes $\\widehat { K } _ { X , X } = C W ^ { - 1 } C ^ { \\top }$ with $W = K _ { Y , Y }$ .   \n4: Tche central server runs spectral clustering on $\\widehat { K } _ { X , X }$ to obtain the clusters $\\mathcal { C } = \\{ \\mathcal { C } _ { 1 } , \\mathcal { C } _ { 2 } , \\ldots , \\mathcal { C } _ { c } \\}$ .\n\nEnsure: $\\mathcal { C }$\n\n# 5 FedDL with Differential Privacy 5.1 FedDL by Data Perturbation\n\nWe inject noise into the raw data in each client and then run FedDL to learn the global distribution information. Note that data perturbation is a one-shot operation before performing Algorithm 1. Specifically, the data $X$ is perturbed by a noise matrix $\\pmb { { \\cal E } } \\in \\mathbb { R } ^ { m \\times n _ { x } }$ to form the noisy data matrix $\\tilde { \\pmb X } =$ $X + E$ , where $e _ { i , j } \\sim \\mathcal { N } ( 0 , \\sigma ^ { 2 } )$ . Define $\\tilde { \\pmb { X } } = \\{ \\tilde { \\pmb { X } } _ { p } \\} _ { p = 1 } ^ { P }$ and we then perform Algorithm 1 on $\\tilde { X }$ to obtain $\\boldsymbol { Y }$ which gives the Nystro¨m approximation\n\n$$\n\\widehat { H } _ { \\tilde { X } , \\tilde { X } | Y } \\simeq B W _ { k } ^ { \\dagger } B ^ { T }\n$$\n\nwhere $B = { K } _ { \\tilde { X } , Y }$ (or $D _ { \\tilde { { X } } , Y } )$ , $W = K _ { Y , Y }$ (or $D _ { Y , Y } )$ .\n\nFollowing the logistics of existing literature, we give the upper bounds on the approximation error of Nystro¨m approximation involved with FedDL, where we focus only on the kernel matrix because it is more complex than the distance matrix.\n\nTheorem 3 (Error bound of Nystro¨m approximation with FedDL having data perturbation). Given $\\pmb { X } ~ = ~ \\{ \\pmb { X } _ { p } \\} _ { p = 1 } ^ { P }$ with $\\begin{array} { r c l } { \\pmb { { X } } _ { p } } & { \\in } & { \\mathbb { R } ^ { m \\times n _ { p } } } \\end{array}$ having $\\begin{array} { r c l } { \\sum _ { p = 1 } ^ { P } n _ { p } } & { = } & { n _ { x } } \\end{array}$ , $\\begin{array} { r l } { \\pmb { Y } } & { { } = } \\end{array}$ $[ { \\pmb y } _ { 1 } , \\dots , { \\pmb y } _ { n _ { y } } ] \\ \\in \\ \\mathbb { R } ^ { m \\times n _ { y } }$ , let $\\tilde { \\cal X } _ { a } ^ { x } \\ = \\ [ { \\cal Y } , \\tilde { \\cal X } ]$ be the augmented matrix, $C = K _ { \\tilde { X } _ { a } ^ { x } , Y }$ , $W = K _ { Y , Y }$ with $\\boldsymbol { W } _ { \\boldsymbol { k } } ^ { \\dagger }$ being the Moore-Penrose inverse of the best rank- $k$ approximation of $W$ , $\\xi _ { m } = \\sqrt { m + \\sqrt { 2 m t } } + 2 t$ , and $C o n d ( \\cdot )$ denote condition number of matrix. Denoting $\\widehat { \\pmb { H } } _ { \\tilde { \\pmb { X } } , \\tilde { \\pmb { X } } | \\pmb { Y } } = \\pmb { C } \\pmb { W } _ { k } ^ { \\dagger } \\pmb { C } ^ { T }$ , it holds with probability at least $1 - n ( n - 1 ) e ^ { - t }$ that\n\n$$\n\\begin{array} { r l } & { \\quad \\| \\widehat { H } _ { { \\tilde { X } } , { \\tilde { X } } | Y } - K _ { X , X } \\| _ { 2 } } \\\\ & { \\leq \\mathrm { C o n d } ( K _ { { \\tilde { X } } _ { a } ^ { x } , { \\tilde { X } } _ { a } ^ { x } } ) \\left( \\frac { | \\mathrm { M M D } ( \\tilde { X } , { Y } ) | } { n _ { x } + n _ { y } } + 1 \\right) + 2 n _ { x } } \\\\ & { \\quad + \\sqrt { 2 } n _ { x } \\gamma \\left[ \\sigma ^ { 2 } \\xi _ { m } ^ { 2 } + \\sqrt { 2 } \\| D _ { X , X } \\| _ { \\infty } \\sigma \\xi _ { m } \\right] } \\end{array}\n$$\n\nalternatively, it holds that\n\n$$\n\\begin{array} { r l } & { \\quad \\big \\| \\widehat { H } _ { { \\tilde { X } } , { \\tilde { X } } \\mid { Y } } - K _ { X , X } \\big \\| _ { F } } \\\\ & { \\leq \\sqrt { n _ { x } + n _ { y } - k } \\mathrm { C o n d } ( K _ { { \\tilde { X } } _ { a } ^ { x } , { \\tilde { X } } _ { a } ^ { x } } ) \\left( \\frac { | \\mathbf { M } \\mathbf { M } \\mathbf { D } ( { \\tilde { X } } , \\mathbf { Y } ) | } { n _ { x } + n _ { y } } + 1 \\right) } \\\\ & { + 2 k ^ { \\frac { 1 } { 4 } } n _ { x } \\sqrt { 1 + \\frac { n _ { y } } { n _ { x } } } + \\sqrt { 2 } n _ { x } \\gamma \\left[ \\sigma ^ { 2 } \\xi _ { m } ^ { 2 } + \\sqrt { 2 } \\big \\| D _ { X , X } \\big \\| _ { \\infty } \\sigma \\xi _ { m } \\right] } \\end{array}\n$$\n\nTheorem 4 (Differential privacy of FedDL with data perturbation). Assume $\\mathrm { m a x } _ { p , j } \\| ( \\boldsymbol { X } _ { p } ) _ { : , j } \\| _ { 2 } = \\tau _ { X }$ , FedDL with perturbed data given by Section 5.1 is $( \\varepsilon , \\delta ) - { \\dot { a } }$ ifferentially private if $\\dot { \\delta } \\geq 2 c \\tau _ { X } / \\varepsilon$ , where $c ^ { 2 } > 2 \\ln ( \\mathrm { 1 . 2 5 } / \\delta )$ .\n\n# 5.2 FedDL by Variable and Gradient Perturbation\n\nWe can also perturb the optimization variable $\\mathbf { \\Delta } _ { Y }$ or the gradient $\\nabla f _ { p } ( { \\pmb Y } _ { p } )$ by Gaussian noise in the training progression to improve the security of Algorithm 1. No matter which method we follow, the $\\boldsymbol { Y }$ obtained by the central server is noisy, i.e., $\\tilde { \\cal Y } = { \\cal Y } + { \\cal E }$ , where $\\scriptstyle { E }$ is drawn elementwise from ${ \\mathcal { N } } ( 0 , \\sigma ^ { 2 } )$ . Then, we do Nystrom approximation by\n\n$$\n\\widehat { \\pmb { H } } _ { X , X | \\tilde { Y } } \\simeq B W _ { k } ^ { \\dagger } B ^ { T }\n$$\n\nwhere $B = K _ { X , \\tilde { Y } }$ (or $D _ { X , { \\tilde { Y } } } )$ ), $W = K _ { \\tilde { Y } , \\tilde { Y } }$ (or $D _ { \\tilde { Y } , \\tilde { Y } } )$ .\n\nTheorem 5 (Error bound of Nystro¨m approximation with FedDL having gradient perturbation). With the same notations in Theorem $3$ , let $\\tilde { \\cal X } _ { a } ^ { y } \\ = \\ [ \\tilde { \\cal Y } , { \\cal X } ]$ be the augmented matrix. Then it holds that\n\n$$\n\\begin{array} { r l } & { \\quad \\left\\| \\widehat { H } _ { X , X | \\tilde { Y } } - K _ { X , X } \\right\\| _ { 2 } } \\\\ & { \\leq \\mathrm { C o n d } ( K _ { \\tilde { X } _ { a } ^ { y } , \\tilde { X } _ { a } ^ { y } } ) \\left( \\frac { \\left| \\mathrm { M M D } ( X , \\tilde { Y } ) \\right| } { n _ { x } + n _ { y } } + 1 \\right) + 2 n _ { x } } \\end{array}\n$$\n\nalternatively, it holds that\n\n$$\n\\begin{array} { r l } & { \\quad \\left\\| \\widehat { H } _ { X , X | \\tilde { Y } } - K _ { X , X } \\right\\| _ { F } } \\\\ & { \\leq \\sqrt { n _ { x } + n _ { y } - k } \\mathrm { C o n d } \\left( K _ { \\tilde { X } _ { a } ^ { y } , \\tilde { X } _ { a } ^ { y } } \\right) \\left( \\frac { | \\mathrm { M M D } ( X , \\tilde { Y } ) | } { n _ { x } + n _ { y } } + 1 \\right) } \\\\ & { \\quad + 2 k ^ { 1 / 4 } n _ { x } \\sqrt { 1 + \\frac { n _ { y } } { n _ { x } } } } \\end{array}\n$$\n\nNote that $\\mathrm { M M D } ( X , \\tilde { Y } ) \\leq \\mathrm { M M D } ( X , Y ) + \\mathrm { M M D } ( Y , \\tilde { Y } )$ is related to $\\sigma$ . A smaller $\\sigma$ leads to a lower estimation error (higher estimation accuracy) but weaker privacy protection. We can obtain a precise trade-off between accuracy and privacy by combining Theorem 5 with Theorem 6.\n\nTheorem 6 (Differential privacy of FedDL with gradient perturbation). Suppose $\\begin{array} { r l r } { \\operatorname* { m a x } _ { p , j } \\| ( \\boldsymbol { X } _ { p } ) _ { : , j } \\| _ { 2 } } & { { } = } & { \\tau _ { X } , } \\end{array}$ , $\\begin{array} { r } { \\operatorname* { m a x } _ { p , i , j } \\| ( \\mathbf { Y } _ { p } ) _ { : , i } - ( \\mathbf { \\boldsymbol { X } } _ { p } ) _ { : , j } \\| = \\mathrm { ~ \\widetilde { ~ } \\gamma ~ } } \\end{array}$ , $\\| { \\bf Y } _ { p } ^ { s } \\| _ { s p } \\leq \\tau _ { Y } \\forall s ,$ , let $\\{ \\nabla f _ { p } ( Y _ { p } ^ { s } ) \\} _ { p = 1 } ^ { P }$ for $\\begin{array} { r l r } { s } & { { } \\in } & { [ S ] } \\end{array}$ be the sequence that is perturbed by noise drawn from $\\mathcal { N } ( 0 , \\sigma ^ { 2 } )$ with variance $8 S \\Delta ^ { 2 } \\log ( e \\mathrm { ~  ~ \\xi ~ } + \\mathrm { ~  ~ \\xi ~ } ( \\varepsilon / \\delta ) ) / \\varepsilon ^ { 2 }$ where 8√n ynγτX {1 + 2γ(τX + τY ) (τX + Υ)}. Then, the Gaussian Mechanism that injects noise to $\\{ \\nabla f _ { p } ( Y _ { p } ^ { s } ) \\} _ { s = 1 } ^ { S }$ for $p \\in [ P ]$ is $( \\varepsilon , \\delta )$ −differentially private.\n\nNote that it is intuitively appropriate to choose a decreasing sequence of noise variance $\\lbrace \\dot { \\sigma } _ { s } ^ { 2 } \\rbrace _ { s = 1 } ^ { S }$ adapted to the gradient norm, which may make the algorithm converge well. In practice, we do not have to do this and can instead inject homoscedastic noise while incorporating a carefully chosen scaling factor into the step size of the gradient descent. By doing so, the differential privacy of our FedDL with gradient perturbation can be guaranteed by Theorem 6.\n\n# 5.3 Fed-tSNE $^ +$ and Fed-UMAP+\n\nBased on the above discussion, we propose the securityenhanced versions of Fed-tSNE and Fed-UMAP, denoted by Fed-tSNE+ and Fed-UMAP+, for which Algorithm 2 has noise injection in line 1 (Algorithm 1).\n\n# 6 Experiments\n\n# 6.1 Data Visualization\n\nWe applied the proposed Fed-tSNE and Fed-UMAP methods to the MNIST and Fashion-MNIST datasets, with $m _ { X } = 4 0 , 0 0 0$ , and set $n _ { Y } ~ = ~ 5 0 0$ . We designed the experiment with 10 clients, where IID (independent and identically distributed) refers to each client’s data being randomly sampled from the MNIST dataset, thus including all classes. In contrast, non-IID means that each client’s data contains only a single class. After reducing the data dimension to two, we visualized them. Figure 1 presents the results on MNIST, showing the data distribution under both\n\nIID and non-IID conditions. Additionally, we included results using $\\mathrm { F e d - t S N E + }$ and Fed-UMAP+, where noise $\\pmb { { \\cal E } }$ is introduced to the gradient $\\nabla f _ { p } ( Y _ { p } )$ . Each element of $\\scriptstyle { E }$ is drawn from $\\mathcal { N } ( 0 , \\mathrm { s d } ^ { 2 } ( \\nabla f _ { p } ( Y _ { p } ) ) )$ , where $\\operatorname { s d } ( \\nabla f _ { p } ( Y _ { p } ) )$ represents the standard deviation of $\\nabla f _ { p } ( { \\pmb Y _ { p } } )$ . Due to space limitations, the results on Fashion-MNIST are provided in Appendix (Figure 4). Based on the visualization results, our proposed methods perform very well in all settings, with only minor differences compared to the non-distributed results. They preserved nearly all the essential information and structure of the data. Tables 1 and 2 provide quantitative evaluations using the following metrics (detailed in Appendix A): CA (Classification Accuracy) with $\\mathbf { k }$ -NN, NPA (Neighbor Preservation Accuracy) with k-NN, NMI (Normalized Mutual Information) of $\\mathbf { k }$ -means, and SC (Silhouette Coefficient) of $\\mathbf { k }$ -means. It can be observed that the performance of our proposed method shows a slight decline in various metrics compared to the nondistributed results, which is unavoidable. However, the overall differences remain within an acceptable range. Notably, the method performs slightly better on distributed data when the distribution is IID compared to non-IID. Moreover, the performance of Fed-tSNE+ and Fed-UMAP+ with added noise to protect privacy is somewhat inferior to the performance without noise, which is expected, as the non-IID scenario and the introduction of noise both impact the accuracy of $\\boldsymbol { Y }$ ’s learning on whole $X$ , thereby affecting the final results.\n\nConvergence Analysis We also conducted experiments to test the convergence of our methods. In Figure 2, the relevant metrics reached convergence after approximately 50 epochs. Figure 3 provides a more intuitive demonstration that, with the increase in epochs, the learning of $\\boldsymbol { Y }$ significantly improves the final results of Fed-tSNE and Fed-UMAP, further confirming the feasibility of our method. (The full process visualization is included in Figure 5 of Appendix A.)\n\n0.5 0.95 0.34 0.90 0.85 0.75 Fed-tSNE 0.1 0.0 0.70 Fed-UMAP 0 1 10 50 100 200 500 0 1 10 50 100 200 500 epoch epoch 0.35 0.80 0.30 0.25 0.705   \n0.20 Fed-tSNE M   \n0.15 Fed-UMAP 0.65 0.60 Fed-tSNE 0.10 0.05 0.55 Fed-UMAP 0 1 10 50 100 200 500 0 1 10 50 100 200 500 epoch epoch\n\nIn addition, we also studied the impact of $n _ { y }$ and noise level $\\beta$ on NMI (Figures 6 and 7 in Appendix A). The noise level $\\beta$ controls the scale of noise, with each element of noise $\\scriptstyle { E }$ being drawn from $\\mathcal { N } ( 0 , \\beta ^ { 2 } \\mathrm { s d } ^ { 2 } ( \\nabla f _ { p } ( \\mathbf { Y } _ { p } ) ) )$ . We see, regardless of the method or conditions, that the larger the $\\boldsymbol { Y }$ volume or the smaller the noise level $\\beta$ (indicating a lower privacy protection requirement), the better the NMI results.\n\nTable 1: Performance (mean std) of dimensionality reduction on MNIST   \n\n<html><body><table><tr><td></td><td></td><td colspan=\"2\">ID</td><td colspan=\"2\">non-IID</td></tr><tr><td>Metric</td><td>tSNE</td><td>Fed-tSNE</td><td>Fed-tSNE+</td><td>Fed-tSNE</td><td>Fed-tSNE+</td></tr><tr><td>CA1-NN CA 10-NN CA 50-NN</td><td>0.9618±0.0015 0.9656±0.0017</td><td>0.9400±0.0017 0.9477±0.0017</td><td>0.9364±0.0020 0.9443±0.0012</td><td>0.9412±0.0021 0.9483±0.0019</td><td>0.9189±0.0030 0.9307±0.0026</td></tr><tr><td rowspan=\"5\">NPA 1-NN NPA 10-NN NPA 50-NN</td><td>0.9609±0.0015</td><td>0.9401±0.0022</td><td>0.9354±0.0022</td><td>0.9406±0.0020</td><td>0.9209±0.0035</td></tr><tr><td>0.4176±0.0016</td><td>0.2728±0.0022</td><td>0.2543±0.0016</td><td>0.2729±0.0022</td><td>0.1928±0.0019</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0.3905±0.0005</td><td>0.3373±0.0007</td><td>0.3263±0.0005</td><td>0.3375±0.0013</td><td>0.2827±0.0010</td></tr><tr><td>0.3441±0.0007 0.7747±0.0243</td><td>0.3301±0.0007</td><td>0.3258±0.0007</td><td>0.3305±0.0006</td><td>0.3030±0.0012</td></tr><tr><td>NMI SC</td><td>0.4226±0.0082</td><td>0.7534±0.0202 0.4407±0.0103</td><td>0.7471±0.0073 0.4478±0.0066</td><td>0.7399±0.0109 0.4321±0.0058</td><td>0.7025±0.0149 0.4441±0.0045</td></tr><tr><td>Metric</td><td>UMAP</td><td>Fed-UMAP</td><td>Fed-UMAP+</td><td>Fed-UMAP</td><td>Fed-UMAP+</td></tr><tr><td>CA 1-NN</td><td>0.9322±0.0053</td><td>0.9066±0.0031</td><td>0.9007±0.0034</td><td>0.9064±0.0026</td><td>0.8730±0.0041</td></tr><tr><td>CA 10-NN</td><td>0.9613±0.0048</td><td>0.9445±0.0018</td><td>0.9416±0.0023</td><td>0.9449±0.0022</td><td>0.9224±0.0036</td></tr><tr><td>CA 50-NN</td><td>0.9602±0.0049</td><td>0.9432±0.0020</td><td>0.9400±0.0025</td><td>0.9441±0.0022</td><td>0.9219±0.0037</td></tr><tr><td>NPA 1-NN</td><td>0.0308±0.0009</td><td>0.0293±0.0007</td><td>0.0277±0.0008</td><td>0.0298±0.0011</td><td>0.0218±0.0009</td></tr><tr><td>NPA 10-NN</td><td>0.1227±0.0010</td><td>0.1133±0.0008</td><td>0.1088±0.0009</td><td>0.1131±0.0012</td><td>0.0914±0.0006</td></tr><tr><td>NPA 50-NN</td><td>0.2226±0.0015</td><td>0.2099±0.0011</td><td>0.2053±0.0011</td><td>0.2095±0.0013</td><td>0.1860±0.0013</td></tr><tr><td>NMI</td><td>0.8285±0.0150</td><td>0.7844±0.0208</td><td>0.7812±0.0153</td><td>0.7919±0.0217</td><td>0.7368±0.0194</td></tr><tr><td>SC</td><td>0.6118±0.0207</td><td>0.5812±0.0261</td><td>0.5746±0.0229</td><td>0.5889±0.0248</td><td>0.5422±0.0173</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>\n\nTable 2: Performance (mean std) of dimensionality reduction on Fashion-MNIST   \n\n<html><body><table><tr><td></td><td></td><td colspan=\"2\">IID</td><td colspan=\"2\">non-IID</td></tr><tr><td>Metric</td><td>tSNE</td><td>Fed-tSNE</td><td>Fed-tSNE+</td><td>Fed-tSNE</td><td>Fed-tSNE+</td></tr><tr><td>CA 1-NN CA 10-NN</td><td>0.8112±0.0049</td><td>0.7473±0.0029</td><td>0.7198±0.0041</td><td>0.7453±0.0044</td><td>0.6669±0.0044</td></tr><tr><td></td><td>0.8260±0.0039</td><td>0.7892±0.0030</td><td>0.7706±0.0034 0.7631±0.0037</td><td>0.7898±0.0039 0.7760±0.0045</td><td>0.7280±0.0048 0.7280±0.0043</td></tr><tr><td>CA 50-NN NPA 1-NN</td><td>0.8064±0.0041</td><td>0.7754±0.0033</td><td>0.0718±0.0013</td><td></td><td></td></tr><tr><td></td><td>0.3518±0.0018</td><td>0.1251±0.0021</td><td></td><td>0.1275±0.0017</td><td>0.0274±0.0006</td></tr><tr><td>NPA 10-NN</td><td>0.3635±0.0007</td><td>0.2551±0.0010</td><td>0.1954±0.0011</td><td>0.2571±0.0011</td><td>0.1090±0.0010</td></tr><tr><td>NPA 50-NN</td><td>0.3710±0.0003</td><td>0.3363±0.0006</td><td>0.3004±0.0006</td><td>0.3369±0.0008</td><td>0.2204±0.0017</td></tr><tr><td>NMI</td><td>0.5787±0.0212</td><td>0.5780±0.0154</td><td>0.5733±0.0149</td><td>0.5778±0.0044</td><td>0.5162±0.0129</td></tr><tr><td>SC Metric</td><td>0.4049±0.0101</td><td>0.4382±0.0070</td><td>0.4638±0.0147</td><td>0.4389±0.0085</td><td>0.4564±0.0111</td></tr><tr><td>CA 1-NN</td><td>UMAP</td><td>Fed-UMAP</td><td>Fed-UMAP+</td><td>Fed-UMAP</td><td>Fed-UMAP+</td></tr><tr><td>CA 10-NN</td><td>0.7146±0.0029</td><td>0.6756±0.0036</td><td>0.6587±0.0055</td><td>0.6766±0.0043</td><td>0.6110±0.0037</td></tr><tr><td></td><td>0.7734±0.0039</td><td>0.7413±0.0045</td><td>0.7287±0.0041</td><td>0.7437±0.0030</td><td>0.6875±0.0041</td></tr><tr><td>CA 50-NN</td><td>0.7781±0.0039</td><td>0.7491±0.0052</td><td>0.7383±0.0039</td><td>0.7501±0.0040</td><td>0.7006±0.0033</td></tr><tr><td>NPA 1-NN NPA 10-NN</td><td>0.0356±0.0012</td><td>0.0218±0.0011</td><td>0.0156±0.0009</td><td>0.0223±0.0011</td><td>0.0071±0.0004</td></tr><tr><td>NPA 50-NN</td><td>0.1401±0.0013</td><td>0.1002±0.0015</td><td>0.0799±0.0012</td><td>0.1020±0.0010</td><td>0.0423±0.0007</td></tr><tr><td>NMI</td><td>0.2518±0.0018 0.6187±0.0127</td><td>0.2152±0.0028 0.5915±0.0112</td><td>0.1907±0.0018 0.5755±0.0090</td><td>0.2167±0.0022</td><td>0.1226±0.0015</td></tr><tr><td></td><td></td><td></td><td></td><td>0.5877±0.0181</td><td>0.5191±0.0132</td></tr><tr><td>SC</td><td>0.5304±0.0286</td><td>0.5448±0.0264</td><td>0.5476±0.0176</td><td>0.5338±0.0252</td><td>0.5322±0.0191</td></tr></table></body></html>\n\n<html><body><table><tr><td></td><td>Metric</td><td>k=5</td><td>k=10</td></tr><tr><td>Fed-tSNE</td><td>NMI NPA10-NN</td><td>0.741±0.011 0.336±0.001</td><td>0.740±0.011 0.338±0.001</td></tr><tr><td>Fed-tSNE+</td><td>NMI NPA 10-NN</td><td>0.709±0.026 0.286±0.001</td><td>0.702±0.015 0.283±0.001</td></tr></table></body></html>\n\nTable 3: Performance (mean±std) of Fed-tSNE and Fed-tSNE $+$ on non-IID data for different values of the number of clients $k$\n\nBesides, in the current non-IID case, each client has one class of data, which is the hardest setting (the distribution of clients is highly heterogeneous), while the IID case is the easiest setting. Other settings interpolate between these two extreme cases. To further investigate the impact of the number of clients $k$ , we conducted additional experiments on\n\nable 4: Performance (mean std) of Fed-tSNE and $\\mathrm { F e d - t S N E + }$ on IID data for different values of the number of clients $k$   \n\n<html><body><table><tr><td></td><td>Metric</td><td>k=5</td><td>k=10</td><td>k=20</td><td>k=50</td><td>k=100</td></tr><tr><td rowspan=\"2\">Fed-tSNE</td><td rowspan=\"2\">NMI NPA 10-NN</td><td rowspan=\"2\">0.747±0.016</td><td rowspan=\"2\">0.753±0.020 0.337±0.001</td><td rowspan=\"2\">0.745±0.009 0.337±0.001</td><td rowspan=\"2\">0.747±0.014 0.338±0.001</td><td rowspan=\"2\">0.749±0.015 0.338±0.001</td></tr><tr><td>0.337±0.001</td></tr><tr><td>Fed-tSNE+</td><td>NMI</td><td>0.740±0.013</td><td>0.747±0.007</td><td>0.742±0.019</td><td>0.741±0.018</td><td>0.741±0.012</td></tr><tr><td></td><td>NPA 10-NN</td><td>0.323±0.001</td><td>0.326±0.001</td><td>0.329±0.001</td><td>0.332±0.001</td><td>0.333±0.001</td></tr></table></body></html>\n\nTable 5: Performance (mean $\\pm$ std) of spectral clustering   \n\n<html><body><table><tr><td></td><td></td><td></td><td>IID</td><td></td><td>non-IID</td><td></td></tr><tr><td></td><td>Metric</td><td>SpeClust</td><td>Fed-SpeClust</td><td>Fed-SpeClust+</td><td>Fed-SpeClust</td><td>Fed-SpeClust+</td></tr><tr><td rowspan=\"2\">MNIST</td><td rowspan=\"2\">NMI</td><td>0.5415±0.0009</td><td>0.5240±0.0038</td><td>0.5220±0.0052</td><td>0.5235±0.0051</td><td>0.5025±0.0068</td></tr><tr><td>0.3837±0.0008</td><td>0.3815±0.0076</td><td>0.3807±0.0088</td><td>0.3806±0.1123</td><td>0.3829±0.0102</td></tr><tr><td rowspan=\"2\">COIL-20</td><td>ARI</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>NMI</td><td>0.6085±0.0012</td><td>0.8425±0.0218</td><td>0.4333±0.0173</td><td>0.8339±0.0216</td><td>0.4215±0.0163</td></tr><tr><td rowspan=\"2\">Mice-Protein</td><td>NMI</td><td>0.3241±0.0063</td><td>0.3233±0.0121</td><td>0.3220±0.0143</td><td>0.3222±0.0190</td><td>0.3198±0.0100</td></tr><tr><td>ARI</td><td>0.1837±0.0037</td><td>0.1827±0.0033</td><td>0.1802±0.0154</td><td>0.1809±0.0024</td><td>0.1783±0.0016</td></tr></table></body></html>\n\n![](images/8ffca057768a86c73d3834c86d419f3ff7f352eb35f1a8a35cc42911739e98a8.jpg)  \nFigure 3: Visualization of Fed-tSNE and Fed-UMAP Convergence from epoch 1 to 10 (MNIST)\n\nMNIST to explore the impact of client number. In the IID setting, the dataset is randomly divided among $k$ clients. In the non-IID setting, we focus on the extreme case of completely different distributions, adding $k = 5$ , with each client containing data from two distinct classes. The results, shown in Table 3 and 4, demonstrate that our proposed methods show stable performance across different values of $k$ .\n\n# 6.2 Clustering Performance\n\nWe utilized three datasets MNIST, COIL-20, and MiceProtein (detailed in Appendix) to evaluate the effectiveness of our Fed-SpeClust. The hyperparameters were adjusted accordingly and the corresponding results are presented in Table 5. In addition to the NMI metric used previously, we also employed the ARI (Adjusted Rand Index) metric, detailed in Appendix. We see that both NMI and ARI indicate that FedSpeClust achieves results comparable to the original spectral clustering, despite a slight decrease in performance, demonstrating the feasibility of our method.\n\n# 7 Conclusion\n\nThis work proposed FedDL and applied it to t-SNE and UMAP to visualize distributed data. The idea was also extended for spectral clustering to cluster distributed data. We provided theoretical guarantees such as differential privacy for the proposed federated learning algorithms. Experimental results demonstrated that the accuracies of our federated algorithms are close to the original algorithms.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   高维数据可视化在大数据时代至关重要，但数据通常分布在多个数据中心且受安全和隐私问题限制，导致标准算法（如t-SNE和UMAP）难以直接应用。\\n> *   该问题的重要性在于：现有的集中式方法需要共享数据或发送到中央服务器，这会泄露隐私并失去信息安全。\\n\\n> **方法概述 (Method Overview)**\\n> *   本文提出Fed-tSNE和Fed-UMAP，通过联邦学习框架学习数据的分布信息，并估计全局距离矩阵，从而实现高维数据的分布式可视化。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **贡献1：** 提出Fed-tSNE和Fed-UMAP，能够在联邦学习框架下可视化分布式高维数据，无需跨客户端交换数据或发送数据到中央服务器。\\n>   *   **效果：** 在MNIST和Fashion-MNIST数据集上，Fed-tSNE和Fed-UMAP的准确率仅比原始算法下降微小（如分类准确率下降约2-3个百分点）。\\n> *   **贡献2：** 提出Fed-tSNE+和Fed-UMAP+，通过数据扰动增强隐私保护。\\n>   *   **效果：** 在隐私保护增强的情况下，性能下降仍在可接受范围内（如NMI下降约0.02-0.03）。\\n> *   **贡献3：** 将方法扩展到联邦谱聚类（Fed-SpeClust），用于分布式数据的聚类。\\n>   *   **效果：** 在MNIST和COIL-20数据集上，Fed-SpeClust的聚类性能接近原始谱聚类（如NMI差异小于0.02）。\\n> *   **贡献4：** 提供理论保证，包括优化收敛性、距离和相似性估计以及差分隐私分析。\\n>   *   **效果：** 实验验证了理论分析的正确性，如优化收敛性和隐私保护的有效性。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   核心原理是通过联邦学习框架隐式学习数据的分布信息，并利用Nyström方法估计全局距离矩阵，从而避免直接共享原始数据。\\n> *   设计哲学是：通过中间数据点（landmarks）捕捉客户端数据的分布信息，从而在保护隐私的同时实现全局可视化。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 现有联邦降维方法（如PCA和自编码器）无法直接应用于t-SNE和UMAP，因为后者需要计算所有数据点对的相似性。\\n> *   **本文的改进：** 提出联邦分布学习（FedDL）框架，通过优化最大均值差异（MMD）来学习中间数据点的分布，从而估计全局距离矩阵。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **联邦分布学习（FedDL）：** 中央服务器初始化一组中间数据点Y，客户端通过MMD优化本地数据与Y的分布匹配。\\n> 2.  **距离矩阵估计：** 客户端计算本地数据与Y的距离矩阵，上传到中央服务器，服务器通过Nyström方法估计全局距离矩阵。\\n> 3.  **可视化或聚类：** 中央服务器基于估计的全局距离矩阵运行t-SNE、UMAP或谱聚类。\\n> *   **关键公式：** MMD的优化目标为：\\n>     $$\\n>     \\\\underset { \\\\pmb { Y } } { \\\\mathrm { m i n i m i z e } } \\\\ F ( \\\\pmb { Y } ) \\\\triangleq \\\\sum _ { p = 1 } ^ { P } \\\\omega _ { p } f _ { p } ( \\\\pmb { Y } )\\n>     $$\\n>     其中，$f_p(Y)$是客户端p的MMD损失函数。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   原始t-SNE和UMAP（集中式方法）。\\n> *   联邦PCA和联邦自编码器。\\n> *   分散式数据随机邻居嵌入（dSNE）。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在分类准确率（CA）上：** 在MNIST数据集上，Fed-tSNE的1-NN分类准确率为0.9400±0.0017，显著优于dSNE（未提供具体数值），但略低于原始t-SNE（0.9618±0.0015）。\\n> *   **在邻居保留准确率（NPA）上：** Fed-UMAP在10-NN上的NPA为0.1133±0.0008，与原始UMAP（0.1227±0.0010）接近，但优于联邦PCA（未提供具体数值）。\\n> *   **在归一化互信息（NMI）上：** Fed-SpeClust在MNIST数据集上的NMI为0.5240±0.0038，与原始谱聚类（0.5415±0.0009）性能相当。\\n> *   **在隐私保护上：** Fed-tSNE+和Fed-UMAP+通过噪声注入实现差分隐私，性能下降可控（如NMI下降约0.02）。\\n> *   **在收敛性上：** 实验表明，FedDL在50轮左右达到收敛，验证了理论分析的优化收敛性。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   联邦学习 (Federated Learning, FL)\\n*   高维数据可视化 (High-dimensional Data Visualization, N/A)\\n*   t分布随机邻居嵌入 (t-Distributed Stochastic Neighbor Embedding, t-SNE)\\n*   均匀流形近似与投影 (Uniform Manifold Approximation and Projection, UMAP)\\n*   联邦分布学习 (Federated Distribution Learning, FedDL)\\n*   隐私保护 (Privacy Protection, N/A)\\n*   谱聚类 (Spectral Clustering, N/A)\\n*   Nyström方法 (Nyström Method, N/A)\\n*   差分隐私 (Differential Privacy, DP)\"\n}\n```"
}