{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.11253",
    "link": "https://arxiv.org/abs/2412.11253",
    "pdf_link": "https://arxiv.org/pdf/2412.11253.pdf",
    "title": "Are Expressive Models Truly Necessary for Offline RL?",
    "authors": [
        "Guan Wang",
        "Haoyi Niu",
        "Jianxiong Li",
        "Li Jiang",
        "Jianming Hu",
        "Xianyuan Zhan"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "2024-12-15",
    "venue": "arXiv.org",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Tsinghua University",
        "McGill University",
        "Shanghai AI Laboratory",
        "Beijing Academy of Artificial Intelligence"
    ],
    "paper_content": "# Are Expressive Models Truly Necessary for Offline RL?\n\nGuan Wang\\*1, Haoyi $\\mathbf { N i u } ^ { * 1 }$ , Jianxiong $\\mathbf { L i } ^ { 1 }$ , Li Jiang2, Jianming $\\mathbf { H } \\mathbf { u } ^ { 1 \\dagger }$ , Xianyuan Zhan1,3,4‚Ä†\n\n1Tsinghua University 2McGill University 3Shanghai AI Laboratory 4Beijing Academy of Artificial Intelligence imonenext@gmail.com, nhy22@mails,hujm $@$ mail,zhanxianyuan ${ \\mathfrak { Q } } \\mathrm { a i r } \\}$ .tsinghua.edu.cn\n\n# Abstract\n\nAmong various branches of offline reinforcement learning (RL) methods, goal-conditioned supervised learning (GCSL) has gained increasing popularity as it formulates the offline RL problem as a sequential modeling task, therefore bypassing the notoriously difficult credit assignment challenge of value learning in conventional RL paradigm. Sequential modeling, however, requires capturing accurate dynamics across long horizons in trajectory data to ensure reasonable policy performance. To meet this requirement, leveraging large, expressive models has become a popular choice in recent literature, which, however, comes at the cost of significantly increased computation and inference latency. Contradictory yet promising, we reveal that lightweight models as simple as shallow 2-layer MLPs, can also enjoy accurate dynamics consistency and significantly reduced sequential modeling errors against large expressive models by adopting a simple recursive planning scheme: recursively planning coarse-grained future sub-goals based on current and target information, and then executes the action with a goal-conditioned policy learned from data relabeled with these sub-goal ground truths. We term our method as Recursive Skip-Step Planning (RSP). Simple yet effective, RSP enjoys great efficiency improvements thanks to its lightweight structure, and substantially outperforms existing methods, reaching new SOTA performances on the D4RL benchmark, especially in multi-stage long-horizon tasks.\n\nCode ‚Äî https://github.com/imoneoi/RSP JAX\n\n# Introduction\n\nOffline reinforcement learning (RL) has emerged as a promising approach to solving many real-world tasks using logged experiences without extra costly or unsafe interactions with environments (Fujimoto, Meger, and Precup 2019; Levine et al. 2020; Zhan et al. 2022). Unfortunately, the absence of online interaction also poses the counterfactual reasoning challenge in out-of-distribution (OOD) regions, causing catastrophic failures in the conventional RL paradigm due to extrapolation error accumulation during value function learning using temporal difference (TD) updates (Fujimoto, Meger, and Precup 2019; Kumar et al. 2019; Li et al. 2023).\n\n20406080Normalized Return (%) RSP (N=4) TT RSSP ((N=14)) TT TAP TAP IRQvLS IQL RvS DT DT   \n102 103 104 105 10‚àí1 101 103 105 Training Time (s) Inference Latency (ms) w/ expressive models w/o expressive models\n\nTo remedy this, Goal-Conditioned Supervised Learning (GCSL) has gained much attention as an alternative paradigm since it reformulates offline RL problems as sequential modeling tasks, where policy extraction can be learned in a supervised manner, thus bypassing problematic value update in conventional TD learning. However, this comes with the price of much higher requirements of accurately describing the data dynamics in sequential modeling, which naturally favors more expressive models. Recent advances in highly expressive models have yielded remarkable achievements across a diverse range of domains such as computer vision (Liu et al. 2021) and natural language processing (Vaswani et al. 2017), which have also been extended to offline RL to accurately capture policy distributions (Wang, Hunt, and Zhou 2023; Hu et al. 2023a; Hansen-Estruch et al. 2023; Ada, Oztop, and Ugur 2024; Wang et al. 2023; Chen et al. 2023), or reduce the accumulative compounding errors in sequential data modeling (Chen et al. 2021; Janner, Li, and Levine 2021; Janner et al. 2022; Ajay et al. 2022; Villaflor et al. 2022; Jiang et al. 2023; Mazoure et al. 2023; Niu et al. 2024). Although they sometimes provide encouraging improvements over previous approaches, they also suffer from noticeable performance deterioration when they fail to accurately represent the behavior policy and/or environment dynamics in long-horizon tasks.\n\nMoreover, the integration of expressive models in offline RL inevitably increases both the computational load and inference latency. This poses a significant challenge for many real-world applications that are latency-sensitive or resourceconstrained. Besides, methods that employ such expressive models are notoriously data-hungry to train, making them impractical for scenarios with expensive samples (Zhan et al. 2022; Cheng et al. 2024). In Fig. 1, we compare key metrics of recent offline RL methods, including policy performance, training time, and inference latency. The use of large expressive models in prior offline RL methods such as DT (Chen et al. 2021), TAP (Zhang et al. 2022), TT (Janner, Li, and Levine 2021), and Diffuser (Janner et al. 2022) sometimes results in marginal performance gains. However, this incremental improvement is offset by the exponentially increased computational load and inference latency, particularly when compared to models that do not employ such expressive models. This prompts us to ask the question:\n\n# Are expressive models truly necessary for offline RL?\n\nWhile prior attempts introduce highly expressive models to combat the accumulated compounding error in long-sequence modeling, these methods still operate in a fine-grained (stepby-step) manner (Chen et al. 2021; Wang, Hunt, and Zhou 2023; Janner et al. 2022), which is inherently tied to the temporal structure of the environment and is susceptible to rapidly accumulated approximation errors over longer horizons (Levine et al. 2020). In this study, we provide a novel perspective to avoid step-wise accumulated compounding error in sequential modeling while only relying on a lightweight model architecture (as simple as 2-layer MLPs). The core of our method is the Recursive Skip-step Planning (RSP) scheme, which employs an iterative two-phase approach to solve tasks: it recursively plans skip-step future sub-goals conditioned on the current and target information, and then executes a goal-conditioned policy based on these coarse-grained predictions. During the recursive planning phase, RSP bypasses step-wise sub-goal prediction and focuses more on longer-horizon outcomes through recursively skipping steps. In essence, RSP can generate long-horizon sub-goals with just a few planning steps, which uses exponentially fewer steps than what is required by previous fine-grained sequence modeling methods, therefore smartly bypassing the long-horizon compounding error issue while enjoying great computational efficiency.\n\nRSP can be easily implemented using simple two-layer shallow MLPs for recursive skip-step dynamics models and the goal-conditioned policy. The entire learning process can be conducted in a supervised learning fashion, eliminating the need for complex stabilization tricks or delicate hyperparameter tuning in value learning. Notably, RSP not only exhibits great training efficiency but also enjoys very low inference complexity, while it achieves comparable or even superior performance against existing offline RL algorithms on D4RL benchmark (Fu et al. 2020), including those equipped with expressive models. This advantage is particularly evident in complex tasks such as AntMaze and Kitchen, demonstrating the effectiveness of the long-horizon modeling capability by adopting our recursive skip-step planning scheme.\n\n# Related Work\n\nModel-free Offline RL. Most prior methods incorporate pessimism during training to alleviate the distributional shift problem (Levine et al. 2020). One solution is leveraging various behavior regularizations to constrain the learned policies close to the behavior policy in offline dataset (Fujimoto, Meger, and Precup 2019; Wu, Tucker, and Nachum 2019; Kumar et al. 2019; Fujimoto and Gu 2021; Li et al. 2023). Some other works introduce pessimism during policy evaluation by assigning low confidences or low values for the value functions in out-of-distribution (OOD) regions (Kumar et al. 2020; Kostrikov et al. 2021; Lyu et al. 2022; An et al. 2021; Bai et al. 2021; Niu et al. 2022). In-sample learning methods (Kostrikov, Nair, and Levine 2022; Xu et al. 2023, 2022; Garg et al. 2023; Xiao et al. 2023; Wang et al. 2024; Zheng et al. 2024) have recently emerged as an alternative, optimizing on samples exclusively from the offline dataset and thus eliminating any OOD value queries. Moreover, an alternative in-sample learning approach performs RL tasks via supervised learning by conditioning other available sources, also called Reinforcement Learning via Supervised Learning) (RvS) (Kumar, Peng, and Levine 2019; Schmidhuber 2019; Emmons et al. 2022; Feng et al. 2022). These approaches are super stable and easy to tune compared to other methods.\n\nExisting methods, however, rely on shallow MLPs to model their actors, performing well on simple tasks but falling short on more complex long-horizon planning tasks, such as AntMaze and Kitchen tasks. To combat this, one popular direction is to increase policy capacity by employing highly expressive models to accurately approximate the actor, which obtains certain degrees of performance gains (Wang, Hunt, and Zhou 2023; Hansen-Estruch et al. 2023; Ada, Oztop, and Ugur 2024; Chen et al. 2023; Lu et al. 2023). However, this marginal improvement comes at the cost of extra model complexity and exponentially increased computational burden, inevitably limiting their applications.\n\nSequential Modeling for Offline RL. Different from traditional TD methods, this avenue treats offline RL problems as general sequence modeling tasks, with the motivation to generate a sequence of actions or trajectories that attain high rewards. In this formulation, shallow feedforward models are typically believed to suffer from a limited modeling horizon due to accumulated modeling errors caused by deficient model capacities (Janner et al. 2019; Amos et al. 2021; Zhan, Zhu, and $\\mathrm { X u } \\ 2 0 2 2 ,$ ). To combat this, recent innovations in sequential modeling techniques shift towards achieving precise long-horizon modeling through the use of high-capacity sequence model architectures such as Transformer (Chen et al. 2021; Janner, Li, and Levine 2021; Jiang et al. 2023; Wang et al. 2022; Konan, Seraj, and Gombolay 2023; Hu et al. 2023b; Wu, Wang, and Hamaya 2024; Villaflor et al. 2022) and diffusion models (Janner et al. 2022; Ajay et al. 2022; Niu et al. 2024). However, Fig. 1 manifests that the adoption of expressive models imposes a large amount of computational load and complexity. In this study, RSP adopts a recursive coarse-grained paradigm for sub-goal prediction solely with shallow 2-layer MLPs, achieving exceptional results while bypassing training inefficiency and high inference latency issues led by expressive models.\n\n# Preliminaries\n\nIn this paper, we consider the standard Markov Decision Process (MDP), denoted by $\\mathcal { M } = ( S , \\mathcal { A } , T , \\rho , r , \\gamma )$ , which is characterized by states $s \\in { \\mathcal { S } }$ , actions $a \\in { \\mathcal { A } }$ , transition probabilities $T ( s _ { t + 1 } | a _ { t } , s _ { t } )$ , initial state distribution $\\rho ( s _ { 0 } )$ , reward function $r$ and the discount factor $\\gamma \\in ( 0 , 1 )$ . In each episode, the agent is given a reward function $r ( s _ { t } , a _ { t } )$ and embodies with a policy $\\pi ( a \\mid s )$ to interact with the environment. Let $\\tau : = ( s _ { 0 } , a _ { 0 } , s _ { 1 } , a _ { 1 } , \\cdot \\cdot \\cdot )$ denote an infinite length trajectory. We employ $\\pi ( \\tau )$ to represent the probability of sampling trajectory $\\tau$ from the policy $\\pi ( a \\mid s )$ . The primary goal is to optimize the expected cumulative discounted rewards incurred along a trajectory, where the objective and the corresponding Q-functions are expressed as:\n\n$$\n\\begin{array} { r l } & { \\displaystyle \\operatorname* { m a x } _ { \\pi } \\mathbb { E } _ { \\pi ( \\tau ) } \\Big [ \\displaystyle \\sum _ { t = 0 } ^ { \\infty } \\gamma ^ { t } r ( s _ { t } , a _ { t } ) \\Big ] , } \\\\ & { \\displaystyle Q ^ { \\pi } ( s , a ) : = \\mathbb { E } _ { \\pi ( \\tau ) } \\Big [ \\displaystyle \\sum _ { t = 0 } ^ { \\infty } \\gamma ^ { t } r ( s _ { t } , a _ { t } ) \\Big \\vert _ { a _ { 0 } = a } ^ { s _ { 0 } = s } \\Big ] . } \\end{array}\n$$\n\nWe are interested in the offline setting in this work and the objective of offline RL is in line with traditional RL, but with infeasible additional online interactions. In the offline setting, a fixed dataset is given by $\\mathcal { D } : = \\{ \\tau _ { 1 } , \\tau _ { 2 } , \\tau _ { 3 } , \\cdot \\cdot \\cdot \\}$ , which comprises multiple trajectories gathered from unknown behavior policies, denoted with $\\beta ( a | s )$ in this work.\n\nRL via Goal-Conditioned Supervised Learning   \n\n<html><body><table><tr><td>Algorithm1: General Procedure of GCSL</td><td></td></tr><tr><td>1:Input: Offline dataset D</td><td></td></tr><tr><td></td><td>2:RELABEL(T),‚àÄT ~ D</td></tr><tr><td></td><td>3:Sample transitions (s,a,e) ‚àà D</td></tr><tr><td></td><td>4:Update œÄ(a |s,e) with Eqn. (2)</td></tr><tr><td>5: return œÄ(a | s,e)</td><td></td></tr></table></body></html>\n\nReformulating RL as GCSL, previous works (Schmidhuber 2019; Emmons et al. 2022) typically employ hindsight relabeling to perform conditional imitation learning. The core idea underlying this approach is that any trajectory can be considered a successful demonstration of reaching certain goals, i.e. future states or accumulating rewards within the same trajectory. GCSL demonstrates significant potential in the offline setting, largely due to the availability of that goal information (Kumar, Peng, and Levine 2019; Chen et al. 2021; Emmons et al. 2022; Feng et al. 2022).\n\nThe general procedure of GCSL is simple and straightforward, as shown in 1, consisting of two main steps. In the first step, GCSL relabels the dataset by adding an additional goal information $e$ to each state-action pair and forms a stateaction-goal tuple, i.e., $( s , a , e )$ . The goal can be the target state, cumulative return, or language description (Lynch et al. 2020; Ghosh et al. 2021; Kumar, Peng, and Levine 2019; Chen et al. 2021; Feng et al. 2022), if applicable. For the state $s _ { t }$ , the goal $\\boldsymbol { e } _ { t }$ is randomly sampled from the future steps from the current step $t$ along a valid trajectory in the dataset. The key idea behind this resampling is to consider any future goal $\\boldsymbol { e } _ { t }$ is reachable by taking action $a _ { t }$ and state $s _ { t }$ . In the second step, GCSL learns a goal-conditioned policy within the relabeled dataset via maximum log-likelihood:\n\n![](images/420cfb7a22137ddc3b2e2001a983b713a0c6fceef7d97a4b950670197511e42e.jpg)  \nFigure 2: The illustration of Recursive Skip-Step Planning\n\n$$\n\\arg \\operatorname* { m a x } _ { \\pi } \\mathbb { E } _ { ( s , a , e ) \\sim \\mathcal { D } } \\left[ \\log \\pi _ { \\phi } ( a \\mid s , e ) \\right] .\n$$\n\n# Recursive Skip-Step Planning Why Non-recursive Planning Might Struggle?\n\nFine-grained planning using single-step dynamics $f ( s _ { t + 1 } | s _ { t } , g )$ often faces significant challenges due to the rapid accumulation of step-wise errors in sequential modeling, led by such a limited planning horizon. Although expressive models like Transformers (Chen et al. 2021; Janner, Li, and Levine 2021) and Diffusers (Janner et al. 2022; Ajay et al. 2022) aim to mitigate these issues by modeling long-term dependencies, their complex architectures significantly increase computational load. This approach ultimately struggles to balance prediction accuracy with efficiency, remaining constrained within the limitations of fine-grained sequential modeling.\n\nTo overcome this, coarse-grained planning uses skipstep dynamics, predicting intermediate sub-goals via $f ( s _ { t + k } | s _ { t } , g )$ with an extended planning horizon $k$ . However, setting an appropriate skip-step horizon $k$ is non-trivial: too short a horizon fails to alleviate error accumulation as in fine-grained planning, while too long a horizon weakens the learning signal, compromising policy performance in long sequential tasks. Thus, a delicate balance must be achieved between the prediction stability of the sub-goal $s _ { t + k }$ and the control precision of the policy $\\pi ( a _ { t } | s _ { t } , s _ { t + k } , g )$ . Non-recursive planning may struggle to select sub-goal information that provides both adequate long-term guidance from the final goal and sufficient supervision for current action.\n\n# Coarse-grained Planning in a Recursive Way\n\nThe dilemma for non-recursive planning suggests using a higher-level dynamics model to predict distant sub-goals $f ^ { \\prime } ( s _ { t + k ^ { \\prime } } | s _ { t } , g )$ and conditioning intermediate predictions on the current-level dynamics $f ( s _ { t + k } | s _ { t } , s _ { t + k ^ { \\prime } } , \\bar { g } )$ . This hierarchical approach stabilizes sub-goal predictions $\\hat { s } _ { t + k }$ in the same way that sub-goals enhance action extraction $\\hat { a } _ { t }$ . By recursively introducing next-level sub-goal prediction with more distant horizons, sub-goal approximation error is reduced at each level. As these sub-goals approach the final target, their error also inherently diminishes. Planning in such a recursive manner thus effectively mitigates error accumulation in long-horizon tasks.\n\nIn the RSP framework, each current state $s _ { t }$ in the dataset is first relabeled with a fixed-horizon future target state $s _ { t + k }$ as the sub-goal, resulting in an augmented dataset $\\mathcal { D } = \\{ ( s _ { t } , a _ { t } , s _ { t + k } , g ) \\}$ . This allows us to revise the optimization objective of goal-conditioned policies in Equation 2 to focus on goal-conditioned sub-goal prediction:\n\n$$\n\\hat { f } = \\underset { f } { \\arg \\operatorname* { m a x } } \\ \\mathbb { E } _ { ( s _ { t } , s _ { t + k } , g ) \\sim \\mathcal { D } } \\left[ \\log f ( s _ { t + k } | s _ { t } , g ) \\right] .\n$$\n\nAs explained in the previous section, planning with a learned coarse-grained dynamics model $\\hat { f }$ is insufficient for balancing the minimization of action and sub-goal prediction errors. To further reduce the sub-goal prediction error, we extend it to a recursive skip-step prediction strategy, as depicted in Fig. 2, predicting high-level sub-goals to stabilize the lower-level sub-goal predictions in sequential modeling. To standardize the recursive process, we set $K ^ { ( n ) } : = \\bar { t } + K / 2 ^ { n } , n \\in$ $[ 0 , \\cdots , N - 1 ]$ where $N$ is the recursion depth, $K$ is the skip step of the highest-level sub-goal, and $s _ { K ( n ) }$ are subgoal ground truths. The highest-level sub-goal skips $K$ steps while the lowest-level sub-goal skips $k \\stackrel { \\smile } { = } K / 2 ^ { \\sp { - } N - 1 }$ steps from the current step. For $N$ -level recursion, we need to relabel the dataset $\\mathcal { D } = \\{ ( s _ { t } , a _ { t } , g ) \\}$ with additional $N$ skip-step sub-goal ground truths:\n\n$$\n\\mathcal { D } \\gets \\left\\{ \\left( a _ { t } , \\kappa _ { t } ^ { ( N ) } : = \\left( s _ { t } , s _ { K ^ { ( N - 1 ) } } , \\cdot \\cdot \\cdot , s _ { K ^ { ( 0 ) } } , g \\right) \\right) \\right\\}\n$$\n\nw Œ∫t(0) here $\\boldsymbol \\kappa _ { t } ^ { ( 0 ) } = ( s _ { t } , g )$ ; $\\forall n \\in [ 1 , \\ldots , N ]$ , $\\kappa _ { t } ^ { ( n ) } = s _ { K ^ { ( n - 1 ) } } \\cup$ $\\kappa _ { t } ^ { ( n - 1 ) }$ contains all the information required by the $n$ -th level dynamics model to fit the sub-goal ground truth $s _ { K ^ { ( n - 1 ) } }$ . For brevity, we omit $t$ and refer to $\\kappa ^ { ( n ) }$ in the following descriptions. With all the definitions above, the highest(first)- level coarse-grained dynamics model could be denoted by $\\hat { f } _ { 0 } \\big ( s _ { K ^ { ( 0 ) } } | s _ { t } , g \\big )$ . Subsequently, we can learn more coarsegrained dynamics models to push the limits of the benefits of coarse-grained planning, as depicted in Fig. 2. In terms of the $n$ -th dynamics model, the recursion solution naturally extends to the optimization objective in Eq. 5:\n\n$$\n\\operatorname* { m a x } _ { f _ { n } } \\mathbb { E } _ { \\kappa ^ { ( n ) } \\sim \\mathcal { D } } \\left[ \\log f _ { n } \\big ( s _ { K ^ { ( n - 1 ) } } \\big | \\kappa ^ { ( n - 1 ) } \\big ) \\right] , \\forall n \\in [ 1 , \\dots , N ] .\n$$\n\nwhere the current level sub-goal generation is conditioned on all the sub-goals predicted with higher-level dynamics for stability, which proves a significant design in our experiments.\n\nDuring the evaluation process, we can formulate the recursive sub-goal planning as:\n\n$$\n\\widehat { s _ { K ^ { ( n - 1 ) } } } = \\underset { s _ { K } ( n - 1 ) } { \\arg \\operatorname* { m a x } } \\ \\hat { f } _ { n } \\big ( s _ { K ^ { ( n - 1 ) } } \\big | \\widehat { \\kappa ^ { ( n - 1 ) } } \\big ) , \\forall n \\in [ 1 , \\cdot \\cdot \\cdot , N ]\n$$\n\n1: Input: Dataset $\\mathcal { D }$ , Recursion depth $N$ , Fixed planning step $K$   \n2: Relabel Dataset with Eq. 4 $D$ Step 1   \n3: // Training ‚ñ∑ Step 2   \n4: for every training step do   \n5: Sample transitions $\\left( s _ { t } , a _ { t } , s _ { K ^ { \\left( 0 \\right) } } , \\cdot \\cdot \\cdot , s _ { K ^ { \\left( N - 1 \\right) } } , g \\right) \\in \\mathcal { D }$   \n6: for $n = 1 , \\cdots , N$ do   \n7: Update the $n$ -th dynamics model ${ \\hat { f } } _ { n }$ by Eq. 5   \n8: end for   \n9: Update $\\pi _ { \\phi }$ by Eq. 7   \n10: end for   \n11: // Evaluation ‚ñ∑ Step 3   \n12: Get initial state $s$ , set done as False   \n13: while not done do   \n14: Get action $a$ from Eq. 8   \n15: Roll out $a$ and get $( \\bar { s ^ { \\prime } } , r , \\mathsf { d o n e } )$   \n16: Set $s = s ^ { \\prime }$   \n17: end while\n\nwhere $\\widehat { { \\kappa } ^ { ( 0 ) } } = { \\kappa } ^ { ( 0 ) } = ( s _ { t } , g )$ ; $\\forall n \\ \\in \\ [ 1 , \\ldots , N ]$ , $\\widehat { \\kappa ^ { ( n ) } } =$ ${ \\widehat { s _ { K ^ { ( n - 1 ) } } } } \\cup { \\widehat { \\kappa ^ { ( n - 1 ) } } }$ . Eventually, we get the predicted s b-dgoals and leverage $\\widehat { \\kappa ^ { ( N ) } }$ for policy extraction.\n\n# Policy Extraction\n\nDifferent from other planning methods, the coarse-grained planning approach employed by RSP does not explicitly integrate the corresponding policy into the learning process, necessitating a separate policy extraction step. To maintain the simplicity and efficiency of RSP, we extract the policy by maximizing the log-likelihood within a supervised learning framework. We introduce a goal-conditioned policy, distinct in that it incorporates all the sub-goals planned from prior coarse-grained dynamics models:\n\n$$\n\\operatorname* { m a x } _ { \\phi } \\mathbb { E } _ { ( a _ { t } , \\kappa ^ { ( N ) } ) \\sim \\mathcal { D } } \\left[ \\log \\pi _ { \\phi } ( a _ { t } \\mid \\kappa ^ { ( N ) } ) \\right] .\n$$\n\nAt the evaluation stage, given the current state $s _ { t }$ , the action is determined by both the coarse-grained dynamics model and the goal-conditioned policy with:\n\n$$\n\\widehat { a _ { t } } = \\mathop { \\arg \\operatorname* { m a x } } _ { a _ { t } } \\hat { \\pi } _ { \\phi } \\big ( a _ { t } | \\widehat { \\kappa ^ { ( N ) } } \\big ) \\big )\n$$\n\nwhere $\\widehat { \\kappa ^ { ( N ) } } = \\big ( s _ { t } , \\widehat { s _ { K ^ { ( N - 1 ) } } } , \\cdot \\cdot \\cdot , \\widehat { s _ { K ^ { ( 0 ) } } } , g \\big )$ is obtained from Eq. 6  cdursively.\n\nThe final procedure in Algorithm 2 consists of three steps. Initially, it relabels the dataset $\\mathcal { D }$ , augmenting each transition with the future skip-step sub-goal and final goal ground truths. Subsequently, RSP learns $N$ coarse-grained dynamics models from Eq. 5 and a goal-conditioned policy from Eq. 7 that determines the action based on the sub-goals predicted with all the dynamics models. Finally, RSP plans the sub-goal sequence recursively with Eq. 6 and executes action from sub-goals with Eq. 8.\n\n# Discussions\n\nIn this section, we provide an in-depth explanation of why recursive skip-step planning might outperform from the perspective of model rollout prediction errors. We conduct experiments on different (lowest-level) skip-step horizons $k =$\n\nÔºÅ  \n1.75 1.75 1.75 [32] 1.75 [64][32,64] [64,128]  \n1.50 [816] 1.50 [16] 1.50 [32 64128256) 1.50 64.128,256.512]  \n1.25 [8,16,32] 1.25 [16,32] 1.25 1.25[8,16,32,64] [16,32,64][16,32,64,128]  \n1.00 1.00 1.00 1.000 250 500 750 1000 0 250 500 750 1000 0 250 500 750 1000 250 500 750 1000# Step # Step # Step # Step\n\n8, 16, 32, 64) and recursion depths $( N = 1 , 2 , 3 , 4 )$ ) in the long-horizon Antmaze-Ultra-Diverse task, where trajectories can extend up to 3000 steps. For example, [16,32,64] in Fig. 3 denotes $N = 3$ and $k = 1 6$ . We plot the root mean squared errors (RMSE) between the model rollout predictions and the ground truths of skip-step state trajectories, averaged over 10 seeds. Specifically, we obtain the next skip-step prediction $\\hat { s } _ { t + k }$ through the recursive approach of RSP; we iteratively query the next skip-step prediction $\\hat { s } _ { t + n k }$ from the last prediction $\\hat { s } _ { t + ( n - 1 ) k }$ and finally obtain rollout sub-goal predictions $s _ { t } , \\cdot \\cdot \\cdot , \\hat { s } _ { t + ( n - 1 ) k } , \\hat { s } _ { t + n k } , \\cdot \\cdot \\cdot , \\hat { s } _ { t + 1 0 2 4 }$ ; then calculate RMSE between the predictions and ground truths.\n\nIn Fig. 3, as $k$ increases from 8 to 64, the accuracy of the model rollout prediction improves, aligning more closely with the ground truth over longer-horizon rollouts. This suggests that coarse-grained prediction outperforms fine-grained prediction in mitigating accumulated compounding errors as the skip-step horizon $k$ increases, thereby enhancing longhorizon planning capabilities. Besides, as the recursion depth $N$ increases, we observe that the RMSE of model rollout predictions achieves a lower asymptotic value and remains closer to the ground truth over longer-horizon rollouts, demonstrating the effectiveness of recursive planning.\n\nChoices of recursion depth and skip-step horizon. At first glance, Fig. 3 indicates that increasing recursion depth and skip-step horizon improves the planning performance of RSP. However, these plots assume that an oracle policy model perfectly follow the skip-step state predictions. In practice, deviations from optimal state trajectories can easily occur due to suboptimal policies, especially when the policy model $\\pi ( a _ { t } | s _ { t } , s _ { t + k } , \\cdot \\cdot \\cdot \\stackrel { \\textstyle } { , } g )$ is conditioned on a distant skip-step state prediction $s _ { t + k }$ , which offers minimal guidance for current actions. Therefore, selecting recursion depth and horizon requires to strike the delicate balance between the expressiveness of policy models and the long-horizon capability of dynamics models. Given that we use a 2-layer MLP for policy learning for simplicity in experiments, we opt for $k = 3 2$ and $N = 1$ in recursive skip-step dynamics models across all tasks in order to provide sufficient long-horizon planning ability while easing the burden for policy learning. Practitioners using more advanced policy modeling and learning methods may consider enlarging the horizon and deepening the recursion process accordingly.\n\n# Experiments\n\nIn this section, we conduct comprehensive evaluations to showcase the effectiveness of RSP. Specifically, we compare RSP against other competitive baselines on the standard offline RL benchmark, D4RL (Fu et al. 2020). The results indicate that despite its simplicity, RSP can obtain on-par or superior performance compared to others using expressive models, especially on the long-horizon Antmaze and multistage Kitchen tasks (Section D4RL Benchmark Results). Moreover, RSP exhibits fast training and inference time, with training for only around 180 seconds and an inference latency of under 1ms (Section Training Cost and Inference Latency). At last, we provide extensive ablation studies on some critical components of RSP including the planning horizon and recursion depth (Section Ablation Studies).\n\n# D4RL Benchmark Results\n\nEvaluation setups We evaluate the performance of various methods using a diverse set of challenging tasks from D4RL benchmark (Fu et al. 2020), including Antmaze navigation, Kitchen Manipulation, Adroit hand manipulation, and Mujoco locomotion tasks. The Antmaze navigation tasks require intricate longhorizon planning to guide an ant navigate from the starting point to the goal location, which are pretty challenging for traditional offline RL to solve. To test the planning capabilities on extremely long-horizon tasks $\\left( > 2 0 0 0 \\right.$ steps), we consider a more extensive and challenging Ultra Antmaze environment. Kitchen tasks involve a 9-DoF Franka robot interacting with a kitchen scene to complete a combination of household sub-tasks, which requires multi-skill long-horizon composition ability. In contrast, the Adroit hand manipulation tasks involve controlling a dexterous robot hand to complete tasks like opening a door or rotating a pen, with planning horizons typically ranging from 100 to 200 steps, which are relatively simple. Finally, the Mujoco locomotion tasks are believed as the simplest tasks among all these tasks. They only focus on controlling different robots to move forward as fast as possible, which are relatively simple since the control pattern is repetitive. To assess performance, we normalize the final scores by following the benchmark standards (Fu et al. 2020), where 0 represents random policy performance and 100 means expert-level performance. We use 10 random seeds and 100 episodes per seed for every task in our evaluation, with mean and standard deviation reported. All the results are obtained on a server with 512G RAM, $4 \\times$ A800 PCIe 80GiB (GPU) and $2 \\times$ AMD EPYC 7T83 64-Core Processor (CPU).\n\nBaselines We compare RSP against the following stateof-the-art (SOTA) baselines. $B C$ is the simplest imitation learning approach that imitates the offline dataset; $R \\nu S$ (Emmons et al. 2022) is an SOTA conditioned imitation learning method that casts offline RL as supervised learning problems; CQL (Kumar et al. 2020) is an offline RL method that penalizes Q-values for OOD regions; IQL (Kostrikov, Nair, and Levine 2022) is an SOTA in-sample learning offline RL method; $D T$ (Chen et al. 2021) and TT (Janner, Li, and Levine 2021) are two methods that utilize transformer (Vaswani et al. 2017) architecture, regarding offline RL problems as sequential modeling problems; $T A P$ (Zhang et al. 2022) is also a transformer-based method that plans in a latent space encoded by VQ-VAE (Van Den Oord, Vinyals et al. 2017); Diffuser (Janner et al. 2022) builds on top of diffusion model, directly generating the entire fixed-length trajectory and executing the first step.\n\n![](images/2ddd36f04582f4c66dfbf6c90ec12ef1989894b085b4daf88e34d7c258353699.jpg)  \nFigure 4: Ablations on choices of recursion depth and horizon.\n\nMain results We present evaluation results on D4RL benchmark in Table 11. The results demonstrate that RSP consistently outperforms or obtains on-par performance compared to other baselines. In particular, RSP achieves unparalleled success in complex Antmaze navigation and Kitchen manipulation tasks that necessitate long-horizon and/or multi-stage planning. For instance, all baselines fail miserably on the extremely challenging Antmaze-Ultra tasks, while RSP can obtain similar success rates as it achieves on the Antmaze-Large tasks. This indicates that the increased difficulty resulting from longer planning horizons has a marginal impact for RSP, demonstrating the superior long-horizon planning capabilities of RSP. Apart from the long-horizon tasks, RSP also demonstrates consistently good performance on Adroit hand manipulation and Mujoco locomotion tasks. On these tasks, Table 1 shows that even with much simpler model architecture, RSP can surprisingly obtain on-par performance results compared to RL as sequential modeling methods that utilize expressive models such as TT, TAP, and Diffuser, as well as TD-based offline RL baselines such as CQL and IQL.\n\n# Training Cost and Inference Latency\n\nTo further showcase the advantages of RSP, we monitor the average training time (1M steps) and inference latency (1 step) of different methods on Antmaze tasks and illustrate the connections between training time, inference latency and the performance for different methods in Fig. 1. All measurements were taken with models trained and inferred on a single GPU in isolation, ensuring no interference from other GPU activities on the server. We can clearly observe that by leveraging expressive models for sequential modeling, one can sometimes obtain minor improvements in policy performance againts TD-based offline RL approaches. However, these marginal gains come at the expense of exponentially increased training time and inference latency. This inevitably imposes significant computational burdens, restricting the applicability of expressive models in many real-world scenarios. In contrast, RSP with horizon $k = 8$ and recursion depth $N = 1 , 4$ obtains the best performance and meanwhile enjoys fast training and minimal inference latency. In particular, RSP $N = 1$ ) completes the training in approximately 180 seconds and exhibits an inference latency of under 1ms, highlighting its exceptional computational efficiency. This sheds light on its potential real-world applicability, given its impressive performance, simplicity, and efficiency.\n\n# Ablation Studies\n\nIn this section, we conduct extensive ablation studies on the critical components of RSP including the recursion depth $N$ and the skip-step horizon of the lowest-level sub-goal for policy extraction $k$ (referred to as ‚Äúhorizon‚Äù hereafter for simplicity). The results are presented in Fig. 4.\n\nWe first fix the horizon $k = 8$ and conduct ablation studies on different recursion depths, i.e. $N = 1 , 2 , 3 , 4$ . The results in Fig. 4(a) indicate that a deeper recursion process consistently improves performance. However, performance in some tasks tends to plateau once $N > 2$ . This is because increasing recursion depth can introduce cumulative errors from each prediction level even though each gets more accurate, which may counterbalance the benefits of RSP for low-level policy learning and result in no further performance improvement. Then we can conclude the following implementation insights: (1) choose minimal recursion depth that achieves saturated performance: $N = 2$ is sufficient for most benchmark tasks or those with similar configurations. There is no need to risk RSP performance by unnecessarily increasing depth; (2) advance policy learning as mentioned in Section Discussions: Strong policy models may exhibit robustness to slight inaccuracies that accumulate in sub-goal predictions, enabling safe extension of recursion depth and fully harnessing the potential of RSP. Other design considerations in policy formulation, such as how to appropriately condition actions on predicted sub-goals, could further enhance performance.\n\nTable 1: Comparison of performance in AntMaze navigation, Kitchen manipulation, Mujoco locomotion tasks, and Adroit hand manipulation tasks from the D4RL dataset. AntMaze tasks require long-horizon planning to navigate from the starting point to the goal. Adroit tasks involve high-dimensional action space control of a 24-DoF robot hand. Baselines in teal utilize expressive models including Transformer or diffusion models. Emphasis is placed on scores within $5 \\%$ of the maximum for locomotion tasks following, while maximum scores are highlighted for other tasks. $+ G$ denotes goal-conditioned methods. $+ Q$ denotes using Q-function as a search heuristic following (Janner, Li, and Levine 2021). We use ‚Äù if the original papers do not report scores on the specific tasks. Note that only a few baseline methods report their performance on the challenging Adroit and Kitchen tasks, so we only compare with baselines having reported scores.   \n\n<html><body><table><tr><td>Dataset</td><td>Environment</td><td>BC</td><td>RvS</td><td>CQL</td><td>IQL</td><td>HIQL</td><td>Diffuser</td><td>DT</td><td>TT</td><td>TAP</td><td>RSP (Ours)</td></tr><tr><td>Umaze</td><td>AntMaze</td><td>65.0</td><td>65.4 (+G)</td><td>74.0</td><td>88.2</td><td>1</td><td></td><td>59.2</td><td>100.0(+Q)</td><td>1</td><td>88.2 ¬±5.7 (+G)</td></tr><tr><td>Umaze-Diverse</td><td>AntMaze</td><td>55.0</td><td>60.9 (+G)</td><td>84.0</td><td>66.7</td><td>1</td><td></td><td>53.2</td><td>‰∏Ä</td><td>1</td><td>92.9 ¬±3.5 (+G)</td></tr><tr><td>Medium-Play</td><td>AntMaze</td><td>0.0</td><td>58.1 (+G)</td><td>61.2</td><td>70.4</td><td>84.1</td><td>1</td><td>0.0</td><td>93.3(+Q)</td><td>78.0 (+G)</td><td>91.4 ¬±8.8 (+G)</td></tr><tr><td>Medium-Diverse</td><td>AntMaze</td><td>0.0</td><td>67.3 (+G)</td><td>53.7</td><td>74.6</td><td>86.8</td><td>1</td><td>0.0</td><td>100.0(+Q)</td><td>85.0 (+G)</td><td>92.7 ¬±3.4 (+G)</td></tr><tr><td>Large-Play</td><td>AntMaze</td><td>0.0</td><td>32.4 (+G)</td><td>15.8</td><td>43.5</td><td>86.1</td><td>1</td><td>0.0</td><td>66.7(+Q)</td><td>74.0 (+G)</td><td>83.0 ¬±3.7 (+G)</td></tr><tr><td>Large-Diverse</td><td>AntMaze</td><td>0.0</td><td>36.9 (+G)</td><td>14.9</td><td>45.6</td><td>88.2</td><td>1</td><td>0.0</td><td>60.0(+Q)</td><td>82.0 (+G)</td><td>86.0 ¬±3.8 (+G)</td></tr><tr><td>Ultra-Play</td><td>AntMaze</td><td>0.0</td><td>33.2 (+G)</td><td>1</td><td>8.3</td><td>39.2</td><td>1</td><td>0.0</td><td>20.0(+Q)</td><td>22.0 (+G)</td><td>80.3 ¬±4.2 (+G)</td></tr><tr><td>Ultra-Diverse</td><td>AntMaze</td><td>0.0</td><td>31.6 (+G)</td><td>1</td><td>15.6</td><td>52.9</td><td></td><td>0.0</td><td>33.3(+Q)</td><td>26.0 (+G)</td><td>80.5 ¬±4.1 (+G)</td></tr><tr><td>Average</td><td></td><td>‰∏Ä 15.0</td><td>48.2 (+G)</td><td>50.6</td><td>51.6</td><td>1</td><td>1</td><td>14.0</td><td>67.6(+Q)</td><td>61.2 (+G)</td><td>86.9 ¬±4.7 (+G)</td></tr><tr><td>Partial</td><td>Kitchen</td><td>38.0</td><td>51.4 (+G)</td><td>20.8</td><td>46.3</td><td>65.0</td><td></td><td></td><td></td><td>1</td><td>76.5 ¬±8.1 (+G)</td></tr><tr><td>Mixed</td><td>Kitchen</td><td>51.5</td><td>60.3 (+G)</td><td>24.2</td><td>51.0</td><td>67.7</td><td>1 1</td><td>1 1</td><td>1 1</td><td>‰∫å</td><td>64.0 ¬±20.4 (+G)</td></tr><tr><td colspan=\"2\">Average</td><td>44.8</td><td>55.9 (+G)</td><td>22.5</td><td>48.7</td><td>66.4</td><td>1</td><td>1</td><td></td><td>1</td><td>70.2 ¬±14.3 (+G)</td></tr><tr><td>Medium-Expert</td><td>HalfCheetah</td><td>55.2</td><td>92.2</td><td>91.6</td><td>86.7</td><td>1</td><td>88.9</td><td>86.8</td><td>95.0</td><td>91.8</td><td>92.5 ¬±0.5</td></tr><tr><td>Medium-Expert</td><td>Hopper</td><td>52.5</td><td>101.7</td><td>105.4</td><td>91.5</td><td>1</td><td>103.3</td><td>107.6</td><td>110.0</td><td>105.5</td><td>109.6 ¬±0.7</td></tr><tr><td>Medium-Expert</td><td>Walker2d</td><td>107.5</td><td>106.0</td><td>108.8</td><td>109.6</td><td>1</td><td>106.9</td><td>108.1</td><td>101.9</td><td>107.4</td><td>106.7 ¬±1.0</td></tr><tr><td>Medium</td><td>HalfCheetah</td><td>42.6</td><td>41.6</td><td>44.0</td><td>47.4</td><td></td><td>42.8</td><td>42.6</td><td>46.9</td><td>45.0</td><td>42.9 ¬±0.2</td></tr><tr><td>Medium</td><td>Hopper</td><td>52.9</td><td>60.2</td><td>58.5</td><td>66.3</td><td>1</td><td>74.3</td><td>67.6</td><td>61.1</td><td>63.4</td><td>63.4 ¬±2.5</td></tr><tr><td>Medium</td><td>Walker2d</td><td>75.3</td><td>71.7</td><td>72.5</td><td>78.3</td><td>1</td><td>79.6</td><td>74.0</td><td>79.0</td><td>64.9</td><td>76.6 ¬±1.6</td></tr><tr><td>Medium-Replay</td><td>HalfCheetah</td><td>36.6</td><td>38.0</td><td>45.5</td><td>44.2</td><td>1</td><td>37.7</td><td>36.6</td><td>41.9</td><td>40.8</td><td>40.4 ¬±0.4</td></tr><tr><td>Medium-Replay</td><td>Hopper</td><td>18.1</td><td>73.5</td><td>95.0</td><td>94.7</td><td>1</td><td>93.6</td><td>82.7</td><td>91.5</td><td>87.3</td><td>88.2 ¬±5.6</td></tr><tr><td>Medium-Replay</td><td>Walker2d</td><td>26.0</td><td>60.6</td><td>77.2</td><td>73.9</td><td>1</td><td>70.6</td><td>66.6</td><td>82.6</td><td>66.8</td><td>66.9 ¬±2.9</td></tr><tr><td colspan=\"2\">Average</td><td>51.9</td><td>71.7</td><td>77.6</td><td>77.0</td><td>‰∫å</td><td>77.5</td><td>74.7</td><td>78.9</td><td>74.8</td><td>76.4 ¬±1.7</td></tr><tr><td>Cloned</td><td>Pen</td><td>56.9</td><td>1</td><td>39.2</td><td>37.3</td><td>1</td><td>1</td><td>1</td><td>11.4</td><td>57.4</td><td>67.9 ¬±6.9</td></tr><tr><td>Cloned</td><td>Hammer</td><td>0.8</td><td>1</td><td>2.1</td><td>2.1</td><td>1</td><td>1</td><td>1</td><td>0.5</td><td>1.2</td><td>3.6 ¬±3.1</td></tr><tr><td>Cloned</td><td>Door</td><td>-0.1</td><td>1</td><td>0.4</td><td>1.6</td><td>1</td><td>1</td><td>1</td><td>-0.1</td><td>11.7</td><td>0.8¬±0.6</td></tr><tr><td>Cloned</td><td>Relocate</td><td>-0.1</td><td>1</td><td>-0.1</td><td>-0.2</td><td>1</td><td>1</td><td>1</td><td>-0.1</td><td>-0.2</td><td>0.0 ¬±0.0</td></tr><tr><td>Expert</td><td>Pen</td><td>85.1</td><td>1</td><td>107.0</td><td>133.1</td><td>1</td><td>1</td><td>1</td><td>72.0</td><td>127.4</td><td>120.9 ¬±4.6</td></tr><tr><td>Expert</td><td>Hammer</td><td>125.6</td><td>1</td><td>86.7</td><td>119.5</td><td>1</td><td></td><td>1</td><td>15.5</td><td>127.6</td><td>129.7 ¬±0.9</td></tr><tr><td>Expert</td><td>Door</td><td>34.9</td><td>1</td><td>101.5</td><td>105.7</td><td>1</td><td>__-</td><td>1</td><td>94.1</td><td>104.8</td><td>105.0 ¬±0.3</td></tr><tr><td>Expert</td><td>Relocate</td><td>101.3</td><td>1</td><td>95.0</td><td>106.1</td><td>1</td><td></td><td>‰∫å</td><td>10.3</td><td>105.8</td><td>108.1 ¬±0.4</td></tr><tr><td colspan=\"2\">Average</td><td>50.6</td><td>1</td><td>54.0</td><td>63.1</td><td>1</td><td>1</td><td>1</td><td>25.4</td><td>67.0</td><td>67.0 ¬±2.1</td></tr></table></body></html>\n\nNext, we fix the recursion depth $N = 1$ and ablate on the choices of $k = 1 , 4 , 1 6 , 3 2$ . A larger horizon corresponds to more coarse-grained planning. We observe that the horizon has minimal impact on short-horizon tasks, such as Adroit and MuJoCo, as shown in Fig. 4(b). However, reducing the horizon significantly leads to performance degradation in long-horizon or multi-stage tasks, such as Antmaze and Kitchen, as seen in Fig. 4(c) and (d).\n\n# Conclusion\n\nIn this study, we propose a novel recursive skip-step planning framework for offline RL, which leverages a set of coarsegrained dynamics models to perform recursive sub-goal prediction, and use a goal-conditioned policy to extract planned actions based on these sub-goals. Our method can smartly bypass the long-horizon compounding error issue in existing sequence modeling-based offline RL methods through hierarchical recursive planning, while still maintaining the advantage of learning in a completely supervised manner. Notably, even using lightweight MLP networks, our method can provide comparable or even better performance as compared to sequence modeling methods that use heavy architectures like Transformers or diffusion models, while providing much better training and inference efficiency. Our work highlights the need to rethink the existing design principles for offline RL algorithms: instead of relying on increasingly heavier models, maybe it‚Äôs time to introduce more elegant and lightweight modeling schemes to tackle existing challenges in offline RL.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÔºàOffline RLÔºâ‰∏≠ÊòØÂê¶ÈúÄË¶Å‰ΩøÁî®Â§ßÂûãË°®ËææÊÄßÊ®°ÂûãÔºàexpressive modelsÔºâÊù•Á°Æ‰øùÁ≠ñÁï•ÊÄßËÉΩ„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñËøô‰∫õÊ®°ÂûãÊù•ÊçïÊçâÈïøÂ∫èÂàóÊï∞ÊçÆ‰∏≠ÁöÑÂä®ÊÄÅÔºå‰ΩÜÂ∏¶Êù•‰∫ÜÊòæËëóÁöÑËÆ°ÁÆóÂíåÊé®ÁêÜÂª∂ËøüÈóÆÈ¢ò„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éÔºåÁ¶ªÁ∫øRLÂú®Áé∞ÂÆû‰∏ñÁïå‰ªªÂä°‰∏≠ÂÖ∑ÊúâÂπøÊ≥õÂ∫îÁî®Ôºå‰ΩÜÁé∞ÊúâÊñπÊ≥ïÂú®ÊïàÁéáÂíåÊÄßËÉΩ‰πãÈó¥ÁöÑÂπ≥Ë°°‰∏ç‰Ω≥ÔºåÈôêÂà∂‰∫ÜÂÖ∂ÂÆûÈôÖÂ∫îÁî®„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫‚ÄúÈÄíÂΩíË∑≥Ê≠•ËßÑÂàí‚ÄùÔºàRecursive Skip-Step Planning, RSPÔºâÁöÑÊñπÊ≥ïÔºåÈÄöËøáÈÄíÂΩíËßÑÂàíÁ≤óÁ≤íÂ∫¶ÁöÑÊú™Êù•Â≠êÁõÆÊ†áÔºåÂπ∂‰ΩøÁî®ÁÆÄÂçïÁöÑ2Â±ÇMLPÊ®°ÂûãÂÆûÁé∞È´òÊïàÁöÑÈïøÂ∫èÂàóÂª∫Ê®°„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ1Ôºö** ÊèêÂá∫RSPÊ°ÜÊû∂ÔºåÈÄöËøáÈÄíÂΩíË∑≥Ê≠•ËßÑÂàíÊòæËëóÂáèÂ∞ë‰∫ÜÈïøÂ∫èÂàóÂª∫Ê®°‰∏≠ÁöÑËØØÂ∑ÆÁ¥ØÁßØ„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ2Ôºö** Â±ïÁ§∫‰∫ÜËΩªÈáèÁ∫ßÊ®°ÂûãÔºàÂ¶Ç2Â±ÇMLPÔºâÂú®Á¶ªÁ∫øRL‰ªªÂä°‰∏≠ÂèØ‰ª•ËææÂà∞ÁîöËá≥Ë∂ÖË∂äÂ§ßÂûãË°®ËææÊÄßÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ3Ôºö** Âú®D4RLÂü∫ÂáÜÊµãËØï‰∏≠ÔºåRSPÂú®ÈïøÂ∫èÂàó‰ªªÂä°ÔºàÂ¶ÇAntMazeÂíåKitchenÔºâ‰∏≠ËææÂà∞‰∫ÜÊñ∞ÁöÑSOTAÊÄßËÉΩÔºåÂêåÊó∂ËÆ≠ÁªÉÊó∂Èó¥‰ªÖ‰∏∫180ÁßíÔºåÊé®ÁêÜÂª∂Ëøü‰Ωé‰∫é1ÊØ´Áßí„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ4Ôºö** RSPÂú®AntMaze-Ultra‰ªªÂä°‰∏äÁöÑÊàêÂäüÁéá‰∏∫80.3%ÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãTTÔºà20.0%ÔºâÂíåTAPÔºà22.0%Ôºâ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   RSPÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÈÄöËøáÈÄíÂΩíËßÑÂàíÁ≤óÁ≤íÂ∫¶ÁöÑÂ≠êÁõÆÊ†áÔºåÈÅøÂÖç‰º†ÁªüÁªÜÁ≤íÂ∫¶ÔºàÈÄêÊ≠•ÔºâËßÑÂàí‰∏≠ÁöÑËØØÂ∑ÆÁ¥ØÁßØÈóÆÈ¢ò„ÄÇËøôÁßçÊñπÊ≥ïÂà©Áî®ËΩªÈáèÁ∫ßÊ®°ÂûãÂÆûÁé∞È´òÊïàÁöÑÈïøÂ∫èÂàóÂª∫Ê®°„ÄÇ\\n> *   ÂÖ∂ËÆæËÆ°Âì≤Â≠¶ÊòØ‚ÄúÁ≤óÁ≤íÂ∫¶ËßÑÂàí‰ºò‰∫éÁªÜÁ≤íÂ∫¶ËßÑÂàí‚ÄùÔºåÈÄöËøáË∑≥Ëøá‰∏≠Èó¥Ê≠•È™§Áõ¥Êé•È¢ÑÊµãÊõ¥ËøúÁöÑÂ≠êÁõÆÊ†áÔºåÂáèÂ∞ëËØØÂ∑Æ‰º†Êí≠„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** Áé∞ÊúâÊñπÊ≥ïÔºàÂ¶ÇTransformerÂíåDiffuserÔºâ‰æùËµñÂ§ßÂûãË°®ËææÊÄßÊ®°ÂûãËøõË°åÈïøÂ∫èÂàóÂª∫Ê®°Ôºå‰ΩÜËÆ°ÁÆóÊàêÊú¨È´ò‰∏îÊé®ÁêÜÂª∂ËøüÂ§ß„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** RSPÈÄöËøáÈÄíÂΩíË∑≥Ê≠•ËßÑÂàíÔºå‰ªÖÈúÄËΩªÈáèÁ∫ßÊ®°ÂûãÂç≥ÂèØÂÆûÁé∞È´òÊïàÁöÑÈïøÂ∫èÂàóÂª∫Ê®°ÔºåÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóË¥üÊãÖ„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.  **Êï∞ÊçÆÈáçÊ†áÊ≥®Ôºö** Â∞ÜÁ¶ªÁ∫øÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÊØè‰∏™Áä∂ÊÄÅ-Âä®‰ΩúÂØπÈáçÊ†áÊ≥®‰∏∫Áä∂ÊÄÅ-Âä®‰Ωú-Â≠êÁõÆÊ†áÂÖÉÁªÑ„ÄÇ\\n> 2.  **ÈÄíÂΩíÂ≠êÁõÆÊ†áËßÑÂàíÔºö** ‰ΩøÁî®Â§öÁ∫ßÁ≤óÁ≤íÂ∫¶Âä®ÊÄÅÊ®°ÂûãÈÄíÂΩíÈ¢ÑÊµãÂ≠êÁõÆÊ†áÔºåÊØèÁ∫ßÊ®°ÂûãÈ¢ÑÊµãÊõ¥ËøúÁöÑÂ≠êÁõÆÊ†á„ÄÇ\\n> 3.  **Á≠ñÁï•ÊèêÂèñÔºö** Âü∫‰∫éÈ¢ÑÊµãÁöÑÂ≠êÁõÆÊ†áÔºåÈÄöËøáÊúÄÂ§ßÂåñÂØπÊï∞‰ººÁÑ∂Â≠¶‰π†ÁõÆÊ†áÊù°‰ª∂Á≠ñÁï•„ÄÇ\\n> 4.  **ËØÑ‰º∞Ôºö** Âú®ÊµãËØïÊó∂ÔºåÈÄíÂΩíÁîüÊàêÂ≠êÁõÆÊ†áÂ∫èÂàóÂπ∂ÊâßË°åÁ≠ñÁï•„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   BC„ÄÅRvS„ÄÅCQL„ÄÅIQL„ÄÅDT„ÄÅTT„ÄÅTAP„ÄÅDiffuser„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®ÈïøÂ∫èÂàó‰ªªÂä°ÔºàAntMaze-UltraÔºâ‰∏äÔºö** RSPÁöÑÊàêÂäüÁéá‰∏∫80.3%ÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãTTÔºà20.0%ÔºâÂíåTAPÔºà22.0%Ôºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÔºàHIQLÔºå52.9%ÔºâÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü27.4‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®ËÆ≠ÁªÉÊó∂Èó¥‰∏äÔºö** RSPÁöÑËÆ≠ÁªÉÊó∂Èó¥‰∏∫180ÁßíÔºåËøú‰Ωé‰∫éTTÂíåDiffuserÔºàË∂ÖËøá1000ÁßíÔºâÔºåÂêåÊó∂‰∏éËΩªÈáèÁ∫ßÊ®°ÂûãRvSÔºà60ÁßíÔºâÁõ∏ÂΩì„ÄÇ\\n> *   **Âú®Êé®ÁêÜÂª∂Ëøü‰∏äÔºö** RSPÁöÑÊé®ÁêÜÂª∂Ëøü‰Ωé‰∫é1ÊØ´ÁßíÔºåËøú‰Ωé‰∫é‰ΩøÁî®Â§ßÂûãË°®ËææÊÄßÊ®°ÂûãÁöÑÂü∫Á∫øÔºàÂ¶ÇTTÂíåDiffuserÔºåÂª∂ËøüË∂ÖËøá100ÊØ´ÁßíÔºâ„ÄÇ\\n> *   **Âú®Kitchen‰ªªÂä°‰∏äÔºö** RSPÂú®PartialÂíåMixed‰ªªÂä°‰∏äÁöÑÂπ≥ÂùáÂæóÂàÜ‰∏∫70.2ÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øRvSÔºà55.9ÔºâÂíåIQLÔºà48.7Ôºâ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   Á¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π† (Offline Reinforcement Learning, Offline RL)\\n*   ÈÄíÂΩíË∑≥Ê≠•ËßÑÂàí (Recursive Skip-Step Planning, RSP)\\n*   ÁõÆÊ†áÊù°‰ª∂ÁõëÁù£Â≠¶‰π† (Goal-Conditioned Supervised Learning, GCSL)\\n*   ÈïøÂ∫èÂàóÂª∫Ê®° (Long-Horizon Modeling, N/A)\\n*   ËΩªÈáèÁ∫ßÊ®°Âûã (Lightweight Models, N/A)\\n*   ËÆ°ÁÆóÊïàÁéá (Computational Efficiency, N/A)\\n*   D4RLÂü∫ÂáÜ (D4RL Benchmark, N/A)\\n*   ËØØÂ∑ÆÁ¥ØÁßØ (Error Accumulation, N/A)\"\n}\n```"
}