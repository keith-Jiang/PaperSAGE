{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.15511",
    "link": "https://arxiv.org/abs/2412.15511",
    "pdf_link": "https://arxiv.org/pdf/2412.15511.pdf",
    "title": "RESQUE: Quantifying Estimator to Task and Distribution Shift for Sustainable Model Reusability",
    "authors": [
        "Vishwesh Sangarya",
        "Jung-Eun Kim"
    ],
    "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
    ],
    "publication_date": "2024-12-20",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "North Carolina State University"
    ],
    "paper_content": "# RESQUE: Quantifying Estimator to Task and Distribution Shift for Sustainable Model Reusability\n\nVishwesh Sangarya and Jung-Eun Kim\\*\n\nDepartment of Computer Science, North Carolina State University\n\n# Abstract\n\nAs a strategy for sustainability of deep learning, reusing an existing model by retraining it rather than training a new model from scratch is critical. In this paper, we propose REpresentation Shift QUantifying Estimator (RESQUE), a predictive quantifier to estimate the retraining cost of a model to distributional shifts or change of tasks. It provides a single concise index for an estimate of resources required for retraining the model. Through extensive experiments, we show that RESQUE has a strong correlation with various retraining measures. Our results validate that RESQUE is an effective indicator in terms of epochs, gradient norms, changes of parameter magnitude, energy, and carbon emissions. The measures align well with RESQUE for new tasks, multiple noise types, and varying noise intensities. As a result, RESQUE enables users to make informed decisions for retraining to different tasks/distribution shifts and determine the most costeffective and sustainable option, allowing for the reuse of a model with a much smaller footprint in the environment.\n\n# Introduction\n\nWith the rapid and ever increasing use of deep learning models in everyday applications, a crucial aspect that is often overlooked is the adaptation of a model to changes and new data. Models need to be adaptive and capable of learning from these new inputs. This dynamic adaptation is essential for maintaining model performance and relevance over time. Particularly, the continuous series of changes a model may need to undergo subject to distributional shifts, or need for primary task updates. For example, new data with better relevant features may be introduced, the environment of deployment may undergo a change leading to distribution shift of the input data, or the prediction classes and task may change. Retraining helps the model remain effective with satisfactory performance for the desired deployed application while shortening the adaptation time. While new models can be developed from scratch when the data undergoes a distribution shift or the task changes, leveraging existing learned features is a resource-efficient option. Reusing models not only saves time and resources but also aligns with the principles of sustainable AI development. This is of primary importance with the overwhelmingly increasing deployments of AI models in the current era, which in turn has given rise to the socially important field of Green AI (Schwartz et al. 2019; Wu et al. 2022; Verdecchia, Sallou, and Cruz 2023; Salehi and Schmeink 2024). It is not uncommon for models to have several billion parameters (Menghani 2023), and as these models grow in complexity, their computational demands also increase. With the increased computational requirements, there is an accompanying need for resources required to perform the computation and byproducts of the computation to deal with. These demands place significant strain on available resources and highlight the importance of addressing both the direct and indirect impacts of AI model use. Consequently, there exists a need for sustainable and efficient development of AI models so as to reduce the environmental impact, especially when looking at the carbon footprint (Ali et al. 2023; Strubell, Ganesh, and McCallum 2019; Bannour et al. 2021; Wu et al. 2024) and energy consumption (Dodge et al. 2022; Strubell, Ganesh, and McCallum 2019; Ali et al. 2023; Wu et al. 2024) involved in the research, evolution, and utilization of AI models.\n\nWith the goal of reusing models and reducing computation, energy consumption, and carbon emissions, we propose a novel estimating index called the REpresentation Shift QUantifying Estimator (RESQUE). This estimator provides a single index to predict the cost of retraining a model for new distributions or tasks before any computation is performed. RESQUE enables deep learning practitioners and researchers to quickly estimate the costs associated with adapting a model, offering valuable insights for informed decision-making. This quantifiable estimator helps them achieve sustainability goals when using and developing the models. For the context of distribution shifts, the estimator referred to as $\\mathrm { R E S Q U E _ { d i s t } }$ , measures the shift in the modelâ€™s representation outputs between the original and new distribution. To obtain $\\mathrm { R E S Q U E _ { d i s t } }$ , the input data is propagated through the model just once with no backward propagation or computation. For the case of a new target task, the estimator, specifically termed as $\\mathrm { R E S Q U E _ { t a s k } }$ , quantifies the separation in class decision boundary of the new target task in the representation space, using the original task model. RESQUE obtained after the forward pass serves as an index to estimate the resources required to adapt the model to a new target task or new distribution. Through extensive experiments and by theoretical reasoning, we show that a lower value of RESQUE correlates with a lower cost of retraining to a new target task or new distribution.\n\nAdditionally, we show that RESQUE not only correlates with important sustainability costs such as energy consumption and carbon emissions, but also has strong correlation with computational related measures such as training epochs, gradients, and model parameter change magnitudes. Furthermore, RESQUE is model/architecture-agnostic, proving effective against either convolutional networks or transformers. Our experiments span a wide range of target tasks, various vision datasets, and different types and intensities of distribution shifts, demonstrating the practical value of RESQUE. Using RESQUE aids in meeting resource and sustainability targets while reducing the computational effort required for adapting AI models. The code for this work is available here:\n\nhttps://github.com/JEKimLab/AAAI2025ReSQuE\n\n# Related Work\n\nSustainable computing and Green AI (Schwartz et al. 2019; Verdecchia, Sallou, and Cruz 2023; Wu et al. 2022; Salehi and Schmeink 2024) are gaining more and more attention due to their potential impacts on the modern AI era of large models. As the demand for AI grows (Liu, Liu, and Lee 2024; Wu et al. 2024), the need for efficient and sustainable computing practices has become important, leading to an important research domain. (Verdecchia, Sallou, and Cruz 2023; Wu et al. 2024) perform a detailed study regarding the potential quantification of undesirable effects of AI, they highlight the importance of reporting energy consumption and carbon emissions as key sustainability measures. Other studies (Wu et al. 2022; Schwartz et al. 2019; Salehi and Schmeink 2024) bring forth the need for a sustainable and greener development of AI, while highlighting the environmental consequences of neglecting the impact of inconsiderate use of resources.\n\n(Georgiou et al. 2022; Strubell, Ganesh, and McCallum 2019) conduct detailed studies regarding energy usage of neural networks. In particular (Strubell, Ganesh, and McCallum 2019; Bannour et al. 2021; Georgiou et al. 2022) explore the increasing energy demands of neural networks from the perspective of computing power, operational costs, and training time. Along with energy consumption, (Henderson et al. 2020; Patterson et al. 2021) highlight the absence of carbon emissions reporting associated with deep learning research. Besides these studies, other works such as $\\mathrm { \\Delta X u }$ et al. 2023; GarcÂ´Ä±a-MartÂ´Ä±n et al. 2019; McDonald et al. 2022) discuss the environmental impact of training models from a carbon emissions perspective. To aid in tracking carbon emissions and energy usage associated with deep learning development, (Schmidt et al. 2021; Anthony, Kanding, and Selvan 2020; Lacoste et al. 2019) propose useful quantifying strategies, including frameworks and libraries which can accurately log all energy consumption and resulting carbon emissions from training. (Dodge et al. 2022) demonstrate that training Vision Transformers (Dosovitskiy et al. 2021)\n\nresults in significantly higher carbon emissions than convolutional networks due to their greater energy requirements from extensive training characteristics. On a similar stream, (Strubell, Ganesh, and McCallum 2019) assess the substantial energy consumption of transformer-based models.\n\nChanges in data due to varying input distributions can arise due to various factors, as elaborated in (Hendrycks and Dietterich 2019; Arjovsky et al. 2020; Hendrycks et al. 2021). To adapt to the specific distributional changes, augmentation strategies (Hendrycks et al. 2020; Liu et al. 2022; Kim, Choo, and Song 2020; Lee et al. 2020) were effective to a limited extent. Although these techniques improve performance compared to a standard trained model against certain types of shifts, they are computationally costly and require training models from scratch. Moreover, the performance improvements are for a limited number of shifts in the distributions, while providing no benefits or a drop in performance for other types of distribution shifts. Studies such as (Geirhos et al. 2020; Yin et al. 2020; Ford et al. 2019) detail the non-uniform performance gains, where improvements in performance cannot be controlled and may result in lower than acceptable performance at the end of training. Adaptation during test time, such as (Lim et al. 2023; Niu et al. 2022; Goyal et al. 2022; Wang et al. 2022), addresses the computational cost concerns by having additional tuned components that can be added to the model. However, these approaches heavily rely on batch data and tend to provide uncertain and low-confidence outputs when there is a greater variation of distribution shifts in the input.\n\nGradients play a crucial role in evaluating the complexity and difficulty of learning when training models, as demonstrated in (Agarwal, Dâ€™souza, and Hooker 2022; Lee and AlRegib 2020; Huang, Geng, and Li 2021). These studies tell us that difficult samples produce greater gradients and thus require larger computation costs for convergence. (Sangarya, Bradford, and Kim 2024, 2023) provide measures for evaluating distribution shifts when a shifted input sample is only derived from a corresponding original sample, which is not a realistic scenario. In particular, (Sangarya, Bradford, and $\\mathrm { K i m } 2 0 2 3 \\$ ) only operates on individual shifts without the ability to evaluate multiple types of shifts simultaneously. Moreover, both studies only focused on shifts in data but did not take into account the practical scenario where the task itself undergoes a change. Additionally, studies such as (Stacke et al. 2020) measure changes in layer-wise representations to analyze different shifts in pathology data.\n\nAdjusted Rand Index (Hubert and Arabie 1985), is a useful metric for comparing clustering algorithms. However, it can also be used to assess the performance of supervised classification and feature selection as demonstrated by (Santos and Embrechts 2009). With regard to the case of task change, we refer to research in various subfields of deep learning, such as (Ravi and Larochelle 2016; Oreshkin, RodrÂ´Ä±guez LoÂ´pez, and Lacoste 2018; Caruana 1997), which defines a new task as any change in the class data or the introduction of completely new classes of associated data. (Achille et al. 2019) is designed to measure task similarity and perform an evaluation of pretrained models. However, adapting to a new task from a current task model requires dataset and task-specific analysis to estimate retraining costs. Likewise, although (Li et al. 2023) consider factors such as dataset and model characteristics, they do not specifically address the magnitude of retraining needed to adapt a model to a new task.\n\n# RESQUE\n\nIn this section, we provide details of $\\mathrm { R E S Q U E _ { d i s t } }$ for distributional shifts and $\\mathtt { R E S Q U E } _ { \\mathrm { t a s k } }$ for change of task.\n\n# Distribution Shift\n\nIn this scenario, we consider the case when a model needs to be updated and adapted as it saw a distributional shift in data - for example, because some ground truths might be changed, some data samples might become stale, or some new data samples might need to come into play, etc. We explain how to obtain and use $\\mathrm { R E S Q U E _ { d i s t } }$ , which is a predictive quantifier for model retraining to distributional shifts. It quantifies the distance between representations of the original and the shifted distribution so as to determine how familiar or foreign the distribution is.\n\nNormalized Embedding Vectors for Distributions For a realistic scenario where a distribution-shifted sample has no corresponding clean image, or the number of distributed shifted samples may not be equal to the number of original clean samples, we make use of all the samples to generate representation embedding vectors. For the case of distribution shifts, since there is no change in the original task and therefore no change in the number of classes, we obtain the class-wise representation embedding vectors for each class individually. To obtain the data representations, we use the output representation from the final convolutional layer in convolutional layers and the final dense layer in Vision Transformers. We perform a forward pass of each dataset and obtain a summed embedding representation vector of each class as follows,\n\n$$\nV _ { l , s u m } ^ { D } = \\sum _ { i = 0 } ^ { n _ { l } } V _ { l , i } ^ { D }\n$$\n\nwhere, $V _ { l , i } ^ { D }$ represents the flattened representation of sample $i$ with class label $l$ within dataset $D$ . For each class $l$ , where $n _ { l }$ is the number of samples within the class, the representation vectors are summed to obtain the summed embedding representation vector $V _ { l , s u m } ^ { D }$ . The summed embedding vector for each class label $l$ is then normalized as follow,\n\n$$\nV _ { l , n o r m } ^ { D } = \\frac { V _ { l , s u m } ^ { D } } { \\vert \\vert V _ { l , s u m } ^ { D } \\vert \\vert }\n$$\n\n$\\mathbf { R E S Q U E _ { d i s t } }$ from Normalized Embeddings The final representation angle is obtained by averaging the inverse cosine angle between each classâ€™ normed representation vector for the original dataset and the distribution-shifted dataset. Let $O$ represent the original dataset, $S$ represent the distribution shifted dataset, and $k$ represent the number of classes. RESQUE $\\mathrm { d i s t }$ is then obtained as,\n\n$$\nR E S Q U E _ { d i s t } = \\frac { \\sum _ { i = 0 } ^ { k } \\operatorname { a r c c o s } ( V _ { i , n o r m } ^ { O } , V _ { i , n o r m } ^ { S } ) } { k }\n$$\n\nInput: Number of samples $n _ { s }$ , Number of labels $k$ , Model $M$ producing representation output $( R , Y )$ ,   \nOutput: Centroids matrix CentM Cent $M \\gets I$ Empty Vector of size $k$ for $i \\gets 1$ to $k$ do current centroid â† âˆ’â†’0 label count 0 for $j  1$ to $n _ { s }$ do $( R , Y )  M ( X _ { j } )$ ${ \\vartriangleright } { \\vline } \\triangleright X _ { j }$ is the $j$ th sample if $Y = = i$ then current centroid current centroi $d + R$ $l a b e l _ { - } c o u n t \\gets l a b e l _ { - } c o u n t + 1$ end if end for CentMi current centroid/label count end for\n\n# Change of Task\n\nFor the first, we consider a case in which a model is required to be updated for application toward a new target task. We show how a model can be retrained and reused to meet such a requirement. To characterize the resources and cost of retraining to a new task, we make use of the separation of classes within the latent space of representations. The separation of classes in the representation latent space is an effective gauge of the modelâ€™s ability to provide accurate outputs. We relate this to the distance between decision boundaries for classes within the representation space, where distinct boundaries result in correct and confident predictions, whereas class boundaries that overlap or are close to each other lead to low confidence and incorrect predictions. We use this trait of deep learning models to evaluate how well a modelâ€™s current learned features and representation space translate into learning a new task.\n\nQuantifying Class Separation from Known Task Data to New Task Data We use the representation of the data produced by the model and a clustering algorithm to assign cluster labels for each data sample in the representation latent space. Using the assigned cluster labels, we derive our estimator, $\\mathtt { R E S Q U E } _ { \\mathrm { t a s k } }$ , by applying the Adjusted Rand Index (Hubert and Arabie 1985). $\\mathtt { R E S Q U E } _ { \\mathrm { t a s k } }$ can quantify the effectiveness of class separation by using the cluster labels and the true data labels. To obtain the cluster labels, we first retrain the original model with the new task just for a single epoch. This creates a representation that is primarily of the original task but incorporates features of the new task and updates the output classes of the model to match the new task classes. Following this step, the new task data is used to perform a forward pass and assign a label, which is a representation label, obtained via clustering on all samples.\n\nClustering Mechanism for the New Unknown Task The data representation is obtained from the final convolution layer for a convolutional networkâ€™ case while from the final dense layer in the last transformer encoder block for a vision transformerâ€™s case. To obtain the representation labels by clustering, we use KMeans with 3 different centroid initialization techniques as follows:\n\n1. Using the original data to obtain centroids as detailed in Algorithm 1,   \n2. Initializing by Kmeans $\\vdash +$ (Arthur, Vassilvitskii et al. 2007), and   \n3. Initializing by random cluster assignment, and selecting the cluster with least entropy among 20 random initialization seeds.\n\nThe above three initialization schemes result in similar clustering labels. We use Algorithm 1 due to its computational efficiency as it does not require random re-initialization or iterative assignment of centroids.\n\nIn Algorithm 1, we begin by creating an empty vector that has a size equal to the number of class labels, as depicted in line 1. CentM is a vector of size $k$ when initialized, but as the classesâ€™ centroids are obtained, each entry in $C e n t M$ is a vector itself. In the end, CentM is a matrix of size $k$ x size of the flattened representation.\n\nAdjusted Rand Index to Quantify Class Separation Adjusted Rand Index takes the value 0 for purely random clustering, and 1 for identical clustering. For our estimator, it is required to have a low value for decision boundaries which are well separated and high value for boundaries which overlap and result in incorrect representation cluster labels. Hence, for our estimator, $\\mathtt { R E S Q U E } _ { \\mathrm { t a s k } }$ , we take the complement of Adjusted Rand Index and define it in Eq. (4). In Eq. (4), $t l$ represents the total count of true labels for each label in the contingency table of true labels vs. representation labels via clustering. $r c$ represents the summed values of representation cluster labels in the contingency table. A contingency table in this scenario is a matrix that summarizes the number of samples belonging to the same cluster or having the same label in both clustering scenarios. Here, by â€˜both clustering scenarios,â€™ we refer to the representationbased clustering labels and the true labels. $n _ { c }$ is the number of cluster labels, which is equal to the number of class labels. $n _ { i , j }$ represents the value in each entry of the contingency table, which is common to both cluster labels for a given label $i$ and $j$ . $n _ { s }$ is the total number of samples.\n\nThe overall formulation RESQUE $\\mathrm { t a s k }$ is as follows,\n\n$$\n\\begin{array} { r l } & { \\frac { \\frac { 1 } { 2 } \\Big [ \\sum _ { i = 0 } ^ { n _ { c } } \\binom { r c _ { i } } { 2 } + \\sum _ { j = 0 } ^ { n _ { c } } \\binom { t l _ { j } } { 2 } \\Big ] - \\sum _ { i , j = 0 } ^ { n _ { c } } \\binom { n _ { i , j } } { 2 } } { \\frac { 1 } { 2 } \\Big [ \\sum _ { i = 0 } ^ { n _ { c } } \\binom { r c _ { i } } { 2 } + \\sum _ { j = 0 } ^ { n _ { c } } \\binom { t l _ { j } } { 2 } \\Big ] - \\Big [ \\sum _ { i = 0 } ^ { n _ { c } } \\binom { r c _ { i } } { 2 } \\sum _ { j = 0 } ^ { n _ { c } } \\binom { t l _ { j } } { 2 } \\Big ] / \\binom { n } { 2 } } } \\end{array}\n$$\n\n# Retraining Measures\n\nRetraining measures refer to the resources and costs expended when a model is adapted to a new task or distribution. We demonstrate that RESQUE is strongly correlated with these measures and serves as an effective estimator. Using RESQUE, users can estimate the resource expenditure required for retraining the models. We evaluate several measures including carbon emissions, energy consumption, epochs, total gradient norm, and normalized parameter change.\n\nTable 1: Correlation coefficients (and associated p-value) for multiple models   \n\n<html><body><table><tr><td>Model</td><td>Correlation</td><td>Epochs</td><td>GradNorm</td><td>Param. change</td></tr><tr><td rowspan=\"2\">ResNet18 (CIFAR100)</td><td>Pearson</td><td>0.77 (6e-07)</td><td>0.74 (2e-06)</td><td>0.70 (1e-05)</td></tr><tr><td>Spearman</td><td>0.97 (1e-19)</td><td>0.97 (1e-20)</td><td>0.95 (6e-17)</td></tr><tr><td rowspan=\"2\">ViT (CIFAR10)</td><td>Pearson</td><td>0.87 (2e-10)</td><td>0.88 (1e-10)</td><td>0.84 (3e-09)</td></tr><tr><td>Spearman</td><td>0.88 (9e-11)</td><td>0.88 (8e-11)</td><td>0.88 (1e-10)</td></tr><tr><td rowspan=\"2\">VGG16 (SVHN)</td><td>Pearson</td><td>0.93 (2e-08)</td><td>0.95 (1e-16)</td><td>0.94 (1e-15)</td></tr><tr><td>Spearman</td><td>0.92 (5e-13)</td><td>0.95 (9e-16)</td><td>0.93 (3e-14)</td></tr></table></body></html>\n\nCarbon Emissions As highlighted in earlier sections, carbon emissions reporting (Henderson et al. 2020) is vital for sustained development as it represents the environmental cost linked to model training. We utilize a carbon tracking library (Schmidt et al. 2021), to track the carbon emissions associated with training. The library estimates the carbon footprint by taking into account the method of energy generation in the region where the GPUs are used for model training, and, along with the energy consumed, calculates the estimated carbon compound byproducts.\n\nEnergy Consumption Energy consumption is another vital sustainability cost that needs to be measured and reported. We utilize the same library (Schmidt et al. 2021) used for carbon tracking to measure the energy consumed for the entire training and retraining process. To measure the energy consumed, the energy of processes that operate on the GPU, CPU, and RAM are taken into account to provide the total energy consumption.\n\nEpochs Along with carbon emissions and energy consumption, the training cost can be quantified by the number of epochs required to reach the desired performance when adapting to new distributions or tasks. We provide detailed empirical results showing that RESQUE consistently aligns with the number of epochs, confirming its effectiveness as an estimator. The epochs reported are the number of epochs a model requires to reach a predefined test accuracy. Due to common hyperparameters, as additional termination conditions, training is halted after 25 or 50 epochs if the accuracy falls within $0 . 5 \\%$ or $1 \\%$ of the selected cutoff, respectively.\n\nTotal Gradient Norm Gradient norm is a common measure of the difficulty of learning algorithms. Research studies such as (Agarwal, Dâ€™souza, and Hooker 2022; Huang, Geng, and Li 2021; Lee and AlRegib 2020) use gradient norm as a proxy for sample difficulty. In this paper, we report the gradient norm by aggregating the gradient norm at each time interval during retraining. The final gradient norm value represents the total magnitude of gradients the model experienced from the beginning to the end of the training.\n\nNormalized Parameter Change The change in the model parameters denotes the extent of update a model is supposed to undergo when estimating the retraining measure. A larger normalized parameter change indicates significant model deviation and higher resource use, conversely, a lower change suggests lower costs and faster convergence.\n\nNoiseâ€¦levels 1 2 3 4 5 6 7 8 9 Retrain 1 Scratch Epochs Gradâ€¦Norm Param.â€¦change Energyâ€¦(kWh) Carbonâ€¦(kg) 0.4   \n200 0.50 2000 0.1   \n100 0.25 0.2 0 1 2 3 4 5 6 7 8 910 0 1 2 3 4 5 6 7 8 910 0.00 1 2 3 4 5 6 7 8 910 0.0 1 2 3 4 5 6 7 8 910 0.0 1 2 3 4 5 6 7 8 910\n\nSimilar to (Zhang, Bengio, and Singer 2022), we calculate parameter change but instead of comparing initial and final values, we aggregate changes between consecutive time steps of training and retraining. For parameters of layer $l$ at time $t$ , the normalized change $N _ { l , t }$ , between current parameters and the previous time stepâ€™s parameters is given by\n\n$$\nN _ { l , t } = \\| W _ { l , t } - W _ { l , t - 1 } \\| _ { 2 } / \\sqrt { \\| W _ { l , t } \\| }\n$$\n\nHere, $t \\left( \\geq 1 \\right)$ represents the current time interval, while $t - 1$ represents the previous time interval. The distance between current parameters $W _ { l , t }$ and the parameters from the previous time interval $W _ { l , t - 1 }$ is calculated for each layer in the model. The final value of the normalized parameter change is obtained by summing up the total change per time interval, per layer, and averaged by the number of layers.\n\n# Experiments\n\nAll experiments were carried out on NVIDIA RTX GPUs, specifically utilizing the RTX 2060, 2070, 2080, 3060Ti, 4060Ti, and A100 models. Carbon emissions and energy were measured on RTX 2060. All experiment results are an average of three runs.\n\n# Distribution Shift\n\nIn this section, we assess $\\mathrm { R E S Q U E _ { d i s t } }$ in the context of distributional shifts using three datasets : CIFAR10 (Krizhevsky, Hinton et al. 2009), CIFAR100 (Krizhevsky, Hinton et al. 2009), and SVHN (Netzer et al. 2011). We utilize 3 types of noises - Gaussian noise, Image Blur, and Salt-Pepper noise, which represent various real world noises that can occur due to hardware changes, environmental factors, or artifacts in the data, respectively. For each noise type, we generate 10 levels of noise intensity, with level 1 corresponding to minimal noise and level 10 aligning with severity 4 as described in (Hendrycks and Dietterich 2019). To represent the reusability of pre-existing models, we initially train a randomly initialized model on the original data distribution until it achieves the minimum required accuracy for each dataset for fair comparisons. While tuning hyper-parameters for higher noise levels could expedite convergence, it may hinder comparisons across different noise types and levels. Therefore, we maintain consistent hyper-parameters across all experiments for consistency.\n\nIt is important to note that, for the initial model training on original data without distributional shifts, we utilize $70 \\%$ of the entire dataset. For retraining to data with distributional shifts, we utilize $50 \\%$ of the dataset with added noise. The $20 \\%$ excess is the overlap data that was common in the original distribution and the new distribution shifted data.\n\nTable 2: Correlation coefficients (and p-values) of ResNet and ViT for different new target tasks.   \n\n<html><body><table><tr><td>Model</td><td>Correlation</td><td>Epochs</td><td>Grad Norm</td><td>Param. change</td></tr><tr><td>ResNet18</td><td>Pearson</td><td>0.86 (0.012)</td><td>0.82 (0.022)</td><td>0.65 (0.100)</td></tr><tr><td>(CIFAR10)</td><td>Spearman</td><td>0.96 (4e-04)</td><td>0.92 (0.002)</td><td>0.75 (0.052)</td></tr><tr><td>ResNet18</td><td>Pearson</td><td>0.75 (0.050)</td><td>0.74 (0.053)</td><td>0.71(0.071)</td></tr><tr><td>(CIFAR100)</td><td>Spearman</td><td>0.89 (0.006)</td><td>0.85 (0.014)</td><td>0.78 (0.036)</td></tr><tr><td>ResNet18</td><td>Pearson</td><td>0.93 (0.002)</td><td>0.83 (0.020)</td><td>0.93 (0.002)</td></tr><tr><td>(GTSRB)</td><td>Spearman</td><td>0.75 (0.052)</td><td>0.75 (0.052)</td><td>0.64 (0.119)</td></tr><tr><td>ViT B/16</td><td>Pearson</td><td>0.93 (7e-04)</td><td>0.93 (7e-04)</td><td>0.92 (0.001)</td></tr><tr><td>(CIFAR10)</td><td>Spearman</td><td>0.83 (0.009)</td><td>0.85 (0.006)</td><td>0.71 (0.046)</td></tr></table></body></html>\n\nFig. 1 illustrates the retraining measures for VGG16 trained from scratch versus retrained on SVHN under varying intensities of Gaussian noise. The results clearly show that retraining a model requires significantly fewer resources than training from scratch. All retraining measures for retraining are substantially lower compared to those for training a new model, highlighting the efficiency of retraining to distribution shifts.\n\nFor evaluating ${ \\mathrm { R E S Q U E } } _ { \\mathrm { d i s t } }$ as an estimator for the retraining measures, we evaluate convolutional networks and vision transformers, retrained to various noise types and levels on different datasets. We train and retrain ResNet18 (He et al. 2016) on CIFAR100, VGG16 (Simonyan 2014) on SVHN, and ViT (Dosovitskiy et al. 2021) on CIFAR10. For the Vision Transformer, we utilize a ViT model with a patch size of 4, comprising 8 transformer blocks, a latent vector size of 512, 8 attention heads, and an MLP with a hidden layer size of 1024.\n\nFigure 2 displays the correlation between RESQUEdist and the retraining measures, for all three models on the three datasets. Across various datasets and architectures, $\\mathrm { R E S Q U E _ { d i s t } }$ consistently aligns with the retraining measures for different types of distribution shifts. Furthermore, Table 1 provides the Pearson and Spearman correlation coefficients, along with associated $\\boldsymbol { \\mathrm { \\tt ~ p ~ } }$ -values, between RESQUE and all the retraining measures, demonstrating a strong correlation between RESQUE $\\mathrm { d i s t }$ and the retraining measures.\n\n# Task Change\n\nWe evaluate the effectiveness of RESQUE to estimate resources for learning a new task from an original task. First, we compare training a model to a target task from scratch vs.\n\n![](images/14783e9d56d85ff0d8469cec1f05bc17a9c4d13d546a87f2f9d4f9c9a809e6c8.jpg)  \nFigure 2: RESQUE and retraining measures for different models and datasets   \nFigure 3: As for the new target task, CIFAR10, comparisons of retraining from Food101 vs. training from scratch. Retraining consumes significantly less resources, epochs, energy, and carbon, than training from scratch.\n\n150 1050 0.00125050 Scratch Carbonâ€¦(kg)   \n0.2 0.08   \n1050 0.06   \n0.04   \n0.02   \n0 0.0 0.00\n\nretraining a model from the original task. For training from scratch, we randomly initialize a new model and train it on the target task. We set a common cutoff accuracy for a fair comparison. Fig. 3 displays the cost indicators for ResNet18, which was trained on Food101 for the cases of retraining vs. training just from scratch. The target task is CIFAR10. It is clear that for all retraining measures, retraining a model requires significantly fewer resources than training a model from scratch. When retraining an original task model to a new task, since the features of the new task are not known, the model experiences higher initial gradient norm and parameter change. However, this is not the case when retraining to distribution shifts, since the model has already learned a good amount of features of the data, resulting in lower gradient norm and parameter changes, as shown in Fig. 1.\n\nExperiments Across Different Original Tasks We show how well RESQUEtask and the retraining measures are aligned for different original tasks. The original tasks evaluated in Fig. 4 are trained on CIFAR10 (Krizhevsky, Hinton et al. 2009), CIFAR100 (Krizhevsky, Hinton et al. 2009),\n\nEMNIST (Cohen et al. 2017), Fashion MNIST (Xiao, Rasul, and Vollgraf 2017), Food101 (Bossard, Guillaumin, and Van Gool 2014), GTSRB (Stallkamp et al. 2012), MNIST (LeCun et al. 1998), and SVHN (Netzer et al. 2011). We use ResNet18 and ViT-B/16 (Dosovitskiy et al. 2021), and perform clustering to obtain $\\mathtt { R E S Q U E } _ { \\mathrm { t a s k } }$ using the last convolutional layer of ResNet18, and the final dense layer in the last transformer encoder block of the ViT. During training and retraining, images are resized to $2 2 4 \\mathrm { x } 2 2 4$ for ViT-B/16 and to 32x32 for ResNet18. To achieve better performance and to ensure consistent training conditions across all new target tasks, we retrain all layers of the model, which provides better accuracy performance than retraining only the final few layers as outlined in (Deng et al. 2023).\n\nFig. 4 depicts the relation between $\\mathtt { R E S Q U E } _ { \\mathrm { t a s k } }$ and all the retraining measures for ResNet and ViT on different target tasks. For the target tasks, CIFAR10 and GTSRB, we use multiple cutoff accuracies to evaluate how learning differs from the early stage to the later stage. From the figure, it is evident that $\\mathtt { R E S Q U E } _ { \\mathrm { t a s k } }$ aligns with the retraining measures and has a strong correlation.\n\nA lower value of $\\mathtt { R E S Q U E } _ { \\mathrm { t a s k } }$ indicates that the model will require fewer resources to converge. For the target tasks, CIFAR10 and GTSRB, during the early learning stage, the trends exhibited are linear for all the cases of the original tasks. However, as training progresses, the number of epochs required by models with higher $\\mathtt { R E S Q U E } _ { \\mathrm { t a s k } }$ increases substantially. We relate this to the difficulty of learning and the usefulness of task and feature similarity.\n\nIn an original task model with aligned and well-learned initial features, the increase in epochs from low accuracy to high cutoff accuracy is smaller. In contrast, for a model with poorly learned initial features, there is a substantial increase\n\nCIFAR100â€¦(ResNet18) CIFAR10â€¦(ResNet18) GTSRBâ€¦(ResNet18) CIFAR10â€¦(iVT) 1080 150 150 40 Cutoff12 100 20 . Finalâ€¦Cutfof 60 50 . 50 0 . 40.825 0.850 0.875 0.900 0.925 0.950 0 0.50 0.55 0.60 0.65 0.16 0.18 0.20 0.22 0.24 0.76 0.78 0.80 0.82 0.84 Gradientâ€¦ 600 600 400 400 100 2300 Originalâ€¦taskâ€¦ 50 CImFoAdRel1s0 . 200 100 200 0.825 0.850 0.875 0.900 0.925 0.950 1 0.50 0.55 ä¸­ 0.60 0.65 0 . 0.16 0.18 0.20 0.22 0.24 ã€‚ï¼ 0.76 0.78 0.80 0.82 0.84 CIFAR100 01.680 1.5 0.80.015 EMNIST 1.0 ä¸­ 1.0 . 0.40.010 FMasNhISMTNIST 0.5 4 1 1 + 0.0 + 0.000 Food101 1.025050 2.5 0.50 0.55 0.60 0.65 1.00 0.16 0.18 0.20 0.22 0.24 0.76 0.78 0.80 0.82 0.84 GTSRB 2.0 8 0.75 STL10 1.5 0.50 SVHN 1.0 è™¹ 1 1 0.25 â–  0 . 0.825 0.850 0.875 0.900 0.925 0.950 0.50 0.55 0.60 0.65 0.16 0.18 0.20 0.22 0.24 0.76 0.78 0.80 0.82 0.84 0.6 1.0 0.4 0 1.00 Carbonâ€¦ 0.45 0.68 0.23 0.75 0.3 ï¼š 0.4 . : T 0.1 â–  0.25 0.825 0.850 0.875 0.900 0.925 0.950 0.50 0.55 0.60 0.65 0.16 0.18 0.20 0.22 0.24 0.76 0.78 0.80 0.82 0.84 RESQUE RESQUE RESQUE RESQUE\n\n1.0 Newâ€¦tagretâ€¦tasks CIFAR100 0.9 EMNIST FashMNIST MNIST 0.7 Food101 . STL10 GTSRB 0.6 SVHN 0.0 0.2 0.4 0.6 0.8 RESQUE\n\nin epochs from low accuracy to high cutoff accuracy.\n\nThis is due to the fact that learning becomes progressively more difficult for models with less aligned or poorly learned features. This implies two crucial learning challenges - not only does the model lack relevant prior knowledge, but additionally, it also struggles to extract effective features from its initial training, leading to steeper learning curves and thus resulting in a larger number of epochs and resources expended. For ViT B/16 in Fig. 4, some of the prior task models fail to reach a high cutoff accuracy. CNNs overfit and achieved low accuracy on STL10 due to the limited sample size and the need for resizing to $3 2 \\mathrm { x } 3 2$ , hence, we did not use STL10-trained CNNs for new target task retraining.\n\nof a strong correlation between $\\mathtt { R E S Q U E } _ { \\mathrm { t a s k } }$ and the retraining measures. RESQUE $\\mathrm { t a s k }$ has a strong positive Pearson correlation and Spearman correlation coefficient, with a low p-value for both experiments. This indicates that there is a strong and statistically significant relationship between $\\mathtt { R E S Q U E } _ { \\mathrm { t a s k } }$ and the retraining measures.\n\nTable 2 provides numerical correlation values as evidence\n\nExperiments Across Different Target Tasks RESQUEtask can also accurately estimate the performance an original task model can achieve on different new target tasks. This can help profile and categorize task similarity and provide useful information for retraining a current model in future instances. For retraining to different target tasks, we retrain the original task model for a fixed number of epochs across all new target tasks. Fig. 5 illustrates the relation between $\\mathtt { R E S Q U E } _ { \\mathrm { t a s k } }$ and the highest test accuracy reached on each of the new task datasets (original task is CIFAR10). Using $\\mathtt { R E S Q U E } _ { \\mathrm { t a s k } }$ , we obtain an accurate estimation regarding the peak performance a model can attain on the new task.\n\n# Conclusion\n\nWe introduced a novel metric, RESQUE, to estimate the various resources that would be expended when reusing a model by adapting to distributional shifts or retraining to new target tasks. We validate the effectiveness of RESQUE on CNNs and ViTs. All the results consistently validate that RESQUE is an effective estimator of various retraining measures, including energy consumption and carbon emission, enabling sustainable decisions with regard to model reusability.",
    "summary": "```json\n{\n  \"core_summary\": \"### ğŸ¯ æ ¸å¿ƒæ¦‚è¦\\n\\n> **é—®é¢˜å®šä¹‰ (Problem Definition)**\\n> *   è®ºæ–‡è§£å†³çš„æ ¸å¿ƒé—®é¢˜æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨é¢ä¸´æ•°æ®åˆ†å¸ƒå˜åŒ–æˆ–ä»»åŠ¡å˜æ›´æ—¶ï¼Œé‡æ–°è®­ç»ƒï¼ˆretrainingï¼‰çš„èµ„æºæ¶ˆè€—ä¼°ç®—é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹èµ„æºæ¶ˆè€—çš„é‡åŒ–é¢„æµ‹ï¼Œå¯¼è‡´æ¨¡å‹é‡ç”¨å†³ç­–ç¼ºä¹ä¾æ®ã€‚\\n> *   è¯¥é—®é¢˜çš„é‡è¦æ€§åœ¨äºï¼šéšç€AIæ¨¡å‹è§„æ¨¡çš„å¢å¤§ï¼Œè®­ç»ƒå’Œé‡æ–°è®­ç»ƒçš„èµ„æºæ¶ˆè€—ï¼ˆå¦‚èƒ½æºã€ç¢³æ’æ”¾ï¼‰å·²æˆä¸ºå¯æŒç»­AIå‘å±•çš„å…³é”®ç“¶é¢ˆã€‚é€šè¿‡ç²¾å‡†é¢„æµ‹é‡æ–°è®­ç»ƒæˆæœ¬ï¼Œå¯ä»¥æ˜¾è‘—é™ä½ç¯å¢ƒè´Ÿæ‹…å¹¶æå‡èµ„æºåˆ©ç”¨ç‡ã€‚\\n\\n> **æ–¹æ³•æ¦‚è¿° (Method Overview)**\\n> *   è®ºæ–‡æå‡ºREpresentation Shift QUantifying Estimator (RESQUE)ï¼Œä¸€ç§é€šè¿‡å•æ¬¡å‰å‘ä¼ æ’­å³å¯é¢„æµ‹æ¨¡å‹é€‚åº”æ–°ä»»åŠ¡æˆ–æ•°æ®åˆ†å¸ƒæ‰€éœ€èµ„æºæ¶ˆè€—çš„é‡åŒ–æŒ‡æ ‡ã€‚\\n\\n> **ä¸»è¦è´¡çŒ®ä¸æ•ˆæœ (Contributions & Results)**\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹1ï¼š** æå‡ºRESQUEæŒ‡æ ‡ï¼Œé¦–æ¬¡å®ç°æ— éœ€å®é™…é‡æ–°è®­ç»ƒå³å¯é¢„æµ‹èµ„æºæ¶ˆè€—ï¼ˆå¦‚epochsã€æ¢¯åº¦èŒƒæ•°ã€å‚æ•°å˜åŒ–é‡ã€èƒ½æºå’Œç¢³æ’æ”¾ï¼‰ã€‚å®éªŒæ˜¾ç¤ºå…¶ä¸çœŸå®é‡æ–°è®­ç»ƒæˆæœ¬çš„Pearsonç›¸å…³ç³»æ•°æœ€é«˜è¾¾0.97ï¼ˆp<1e-20ï¼‰ã€‚\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹2ï¼š** è®¾è®¡ä¸¤ç§åœºæ™¯ä¸“ç”¨å­æŒ‡æ ‡ï¼šRESQUE_distï¼ˆåˆ†å¸ƒåç§»ï¼‰é€šè¿‡è¡¨å¾ç©ºé—´è§’åº¦é‡åŒ–æ•°æ®åˆ†å¸ƒå˜åŒ–ï¼ŒRESQUE_taskï¼ˆä»»åŠ¡å˜æ›´ï¼‰é€šè¿‡è°ƒæ•´RandæŒ‡æ•°è¯„ä¼°ç±»åˆ«å†³ç­–è¾¹ç•Œåˆ†ç¦»åº¦ã€‚\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹3ï¼š** éªŒè¯RESQUEçš„æ¨¡å‹æ— å…³æ€§ï¼Œåœ¨CNNï¼ˆå¦‚ResNet18/VGG16ï¼‰å’ŒTransformerï¼ˆViTï¼‰ä¸Šå‡ä¿æŒå¼ºç›¸å…³æ€§ï¼Œè¦†ç›–CIFAR10/100ã€SVHNç­‰æ•°æ®é›†ã€‚\",\n  \"algorithm_details\": \"### âš™ï¸ ç®—æ³•/æ–¹æ¡ˆè¯¦è§£\\n\\n> **æ ¸å¿ƒæ€æƒ³ (Core Idea)**\\n> *   RESQUEçš„æ ¸å¿ƒåŸç†æ˜¯é€šè¿‡æ¨¡å‹è¡¨å¾ç©ºé—´çš„å‡ ä½•ç‰¹æ€§ï¼ˆå¦‚è§’åº¦è·ç¦»ã€ç±»åˆ«åˆ†ç¦»åº¦ï¼‰é—´æ¥åæ˜ é‡æ–°è®­ç»ƒçš„éš¾åº¦ã€‚å…¶æœ‰æ•ˆæ€§æºäºæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼Œè¡¨å¾ç©ºé—´çš„ç›¸ä¼¼æ€§ä¸è®­ç»ƒæ”¶æ•›é€Ÿåº¦çš„å¼ºç›¸å…³æ€§ã€‚\\n\\n> **åˆ›æ–°ç‚¹ (Innovations)**\\n> *   **å…ˆå‰å·¥ä½œå±€é™ï¼š** ç°æœ‰æ–¹æ³•éœ€å®é™…æ‰§è¡Œé‡æ–°è®­ç»ƒæ‰èƒ½è¯„ä¼°æˆæœ¬ï¼ˆå¦‚(Sangarya et al. 2023)ä»…æ”¯æŒå•ä¸€æ ·æœ¬åç§»è¯„ä¼°ï¼‰ï¼Œæˆ–ä»…å…³æ³¨ç‰¹å®šåœºæ™¯ï¼ˆå¦‚ä»…æ•°æ®åˆ†å¸ƒæˆ–ä»…ä»»åŠ¡å˜æ›´ï¼‰ã€‚\\n> *   **æœ¬æ–‡æ”¹è¿›ï¼š** \\n>     1. **ç»Ÿä¸€æ¡†æ¶ï¼š** é€šè¿‡RESQUE_distå’ŒRESQUE_taskè¦†ç›–ä¸¤ç§ä¸»è¦åœºæ™¯ï¼›\\n>     2. **é›¶åå‘ä¼ æ’­ï¼š** ä»…éœ€å•æ¬¡å‰å‘ä¼ æ’­å³å¯è®¡ç®—æŒ‡æ ‡ï¼›\\n>     3. **å¤šç²’åº¦é‡åŒ–ï¼š** å¼•å…¥ç±»çº§åˆ«å½’ä¸€åŒ–è¡¨å¾å‘é‡ï¼ˆRESQUE_distï¼‰å’Œèšç±»æ ‡ç­¾ä¸€è‡´æ€§ï¼ˆRESQUE_taskï¼‰ã€‚\\n\\n> **å…·ä½“å®ç°æ­¥éª¤ (Implementation Steps)**\\n> 1. **RESQUE_distè®¡ç®—æµç¨‹ï¼š**\\n>    - å¯¹åŸå§‹æ•°æ®é›†$O$å’Œåç§»æ•°æ®é›†$S$ï¼Œåˆ†åˆ«è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å½’ä¸€åŒ–è¡¨å¾å‘é‡ï¼š\\n>      $$V_{l,norm}^D = \\\\frac{\\\\sum_{i=0}^{n_l} V_{l,i}^D}{\\\\|\\\\sum_{i=0}^{n_l} V_{l,i}^D\\\\|_2}$$\\n>    - è®¡ç®—ç±»é—´è¡¨å¾è§’åº¦çš„å¹³å‡å€¼ï¼š\\n>      $$RESQUE_{dist} = \\\\frac{1}{k}\\\\sum_{i=0}^k \\\\arccos(V_{i,norm}^O, V_{i,norm}^S)$$\\n> 2. **RESQUE_taskè®¡ç®—æµç¨‹ï¼š**\\n>    - å¯¹æ–°ä»»åŠ¡æ•°æ®æ‰§è¡Œå•epochå¾®è°ƒåï¼Œé€šè¿‡KMeansèšç±»ç”Ÿæˆä¼ªæ ‡ç­¾ï¼›\\n>    - è®¡ç®—è°ƒæ•´RandæŒ‡æ•°ï¼ˆARIï¼‰çš„è¡¥æ•°ï¼š\\n>      $$RESQUE_{task} = 1 - \\\\frac{\\\\sum \\\\binom{n_{i,j}}{2} - [\\\\sum \\\\binom{rc_i}{2}\\\\sum \\\\binom{tl_j}{2}]/\\\\binom{n}{2}}{\\\\frac{1}{2}[\\\\sum \\\\binom{rc_i}{2} + \\\\sum \\\\binom{tl_j}{2}] - [\\\\sum \\\\binom{rc_i}{2}\\\\sum \\\\binom{tl_j}{2}]/\\\\binom{n}{2}}$$\\n\\n> **æ¡ˆä¾‹è§£æ (Case Study)**\\n> *   è®ºæ–‡æœªæ˜ç¡®æä¾›æ­¤éƒ¨åˆ†ä¿¡æ¯\",\n  \"comparative_analysis\": \"### ğŸ“Š å¯¹æ¯”å®éªŒåˆ†æ\\n\\n> **åŸºçº¿æ¨¡å‹ (Baselines)**\\n> *   å®éªŒæœªç›´æ¥å¯¹æ¯”å…¶ä»–é¢„æµ‹æ–¹æ³•ï¼Œè€Œæ˜¯éªŒè¯RESQUEæŒ‡æ ‡ä¸å®é™…é‡æ–°è®­ç»ƒæˆæœ¬çš„ç›¸å…³æ€§ã€‚\\n\\n> **æ€§èƒ½å¯¹æ¯” (Performance Comparison)**\\n> *   **åœ¨epochæ•°é‡é¢„æµ‹ä¸Šï¼š** RESQUEåœ¨ResNet18(CIFAR100)ä¸Šçš„Spearmanç›¸å…³ç³»æ•°è¾¾0.97ï¼ˆp<1e-19ï¼‰ï¼Œåœ¨ViT(CIFAR10)ä¸ŠPearsonç›¸å…³ç³»æ•°ä¸º0.87ï¼ˆp=2e-10ï¼‰ã€‚\\n> *   **åœ¨æ¢¯åº¦èŒƒæ•°é¢„æµ‹ä¸Šï¼š** VGG16(SVHN)çš„æ¢¯åº¦èŒƒæ•°é¢„æµ‹Pearsonç›¸å…³ç³»æ•°ä¸º0.95ï¼ˆp<1e-16ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–å¯å‘å¼æ–¹æ³•ã€‚\\n> *   **åœ¨ç¢³æ’æ”¾é¢„æµ‹ä¸Šï¼š** ä½¿ç”¨RTX 2060 GPUå®æµ‹æ˜¾ç¤ºï¼ŒRESQUE_taskä¸çœŸå®ç¢³æ’æ”¾é‡çš„ç›¸å…³æ€§è¾¾0.88ï¼ˆp=9e-11ï¼‰ï¼Œä¾‹å¦‚å½“RESQUE_task=0.2æ—¶ï¼Œå®é™…ç¢³æ’æ”¾ä¸º0.04kgï¼Œè€ŒRESQUE_task=0.8æ—¶å‡è‡³0.23kgã€‚\\n> *   **åœ¨è·¨æ¶æ„æ³›åŒ–æ€§ä¸Šï¼š** RESQUEåœ¨CNNï¼ˆå¦‚ResNet18ï¼‰å’ŒTransformerï¼ˆViT-B/16ï¼‰ä¸Šçš„ç›¸å…³ç³»æ•°å·®å¼‚å°äº0.1ï¼Œè¯æ˜å…¶æ¨¡å‹æ— å…³æ€§ã€‚\",\n  \"keywords\": \"### ğŸ”‘ å…³é”®è¯\\n\\n*   å¯æŒç»­äººå·¥æ™ºèƒ½ (Sustainable Artificial Intelligence, Green AI)\\n*   è¡¨å¾åç§»é‡åŒ– (Representation Shift Quantification, N/A)\\n*   æ¨¡å‹é‡ç”¨ (Model Reusability, N/A)\\n*   ç¢³æ’æ”¾é¢„æµ‹ (Carbon Emission Estimation, N/A)\\n*   åˆ†å¸ƒåç§» (Distribution Shift, N/A)\\n*   ä»»åŠ¡é€‚åº” (Task Adaptation, N/A)\\n*   èµ„æºæ•ˆç‡ (Resource Efficiency, N/A)\\n*   è°ƒæ•´RandæŒ‡æ•° (Adjusted Rand Index, ARI)\"\n}\n```"
}