{
    "source": "Semantic Scholar",
    "arxiv_id": "2503.18042",
    "link": "https://arxiv.org/abs/2503.18042",
    "pdf_link": "https://arxiv.org/pdf/2503.18042.pdf",
    "title": "DualCP: Rehearsal-Free Domain-Incremental Learning via Dual-Level Concept Prototype",
    "authors": [
        "Qiang Wang",
        "Yuhang He",
        "Songlin Dong",
        "Xiang Song",
        "Jizhou Han",
        "Haoyu Luo",
        "Yihong Gong"
    ],
    "categories": [
        "cs.CV"
    ],
    "publication_date": "2025-03-23",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Xi‚Äôan Jiaotong University",
        "Shenzhen University of Advanced Technology"
    ],
    "paper_content": "# DualCP: Rehearsal-Free Domain-Incremental Learning via Dual-Level Concept Prototype\n\nQiang Wang1, Yuhang $\\mathbf { H e } ^ { 1 * }$ , Songlin Dong1, Xiang Song1, Jizhou $\\mathbf { H a n } ^ { 1 }$ , Haoyu Luo1, Yihong Gong 1,2\\*\n\n1Xi‚Äôan Jiaotong University 2Shenzhen University of Advanced Technology qwang $@$ stu.xjtu.edu.cn, heyuhang@xjtu.edu.cn, {dsl972731417, songxiang, jizhou han, luohaoyu}@stu.xjtu.edu.cn, ygong@mail.xjtu.edu.cn\n\n# Abstract\n\nDomain-Incremental Learning (DIL) enables vision models to adapt to changing conditions in real-world environments while maintaining the knowledge acquired from previous domains. Given privacy concerns and training time, RehearsalFree DIL (RFDIL) is more practical. Inspired by the incremental cognitive process of the human brain, we design Dual-level Concept Prototypes (DualCP) for each class to address the conflict between learning new knowledge and retaining old knowledge in RFDIL. To construct DualCP, we propose a Concept Prototype Generator (CPG) that generates both coarse-grained and fine-grained prototypes for each class. Additionally, we introduce a Coarse-to-Fine calibrator (C2F) to align image features with DualCP. Finally, we propose a Dual Dot-Regression (DDR) loss function to optimize our C2F module. Extensive experiments on the DomainNet, CDDB, and CORe50 datasets demonstrate the effectiveness of our method.\n\n# Introduction\n\nDomain-Incremental Learning (DIL) aims to train a unified model incrementally across continuously encountered domains. It has drawn remarkable attention in recent years (Zhang and Mueller 2022; Verwimp et al. 2023) and has a wide range of applications. For example, in a visual recognition model on an autonomous vehicle, the model is expected to incrementally learn and adapt to new and dynamic environments (Wang et al. 2024) such as forests, deserts, cities, etc. As the number of domains increases, the model may forget previously acquired knowledge. Many studies (Isele and Cosgun 2018; Zhao et al. 2021) address the retention of knowledge by preserving and retraining old samples, known as rehearsal. Rehearsal-based methods, however, require longer training time, and storing old domain images may raise privacy concerns (Wan et al. 2024a). Therefore, Rehearsal-Free DIL (RFDIL) is more practical for real-world scenarios compared to rehearsal-based approaches.\n\nThe primary challenge of RFDIL is the conflict between learning new domains and preventing forgetting of old ones.\n\n![](images/ebd4a413cd146e13366190027863318a2a169347eb676fe8aafc39b639a3974a.jpg)  \nFigure 1: (a) Illustration of most existing methods, where new domain features and old domain features compete within the feature space. (b) Illustration of our method based on dual-level concept prototypes. Different colors (red, green, blue, and purple) represent different classes, while different shapes ( , ‚ñ°, and $0$ ) represent different domains. Best viewed in color.\n\nOn the one hand, learning new domain knowledge may overwrite parameters related to old domains, leading to catastrophic forgetting (McCloskey and Cohen 1989). On the other hand, adding artificial constraints to prevent forgetting old domains will hinder learning new ones. To mitigate this dilemma, the method (Zhu et al. 2021) proposes generating samples for the old domains as a supplement to retain knowledge; the methods (Rebuffi et al. 2017; Hou et al. 2019) suggest employing knowledge distillation strategies to slow down the forgetting of old knowledge; the methods (Douillard et al. 2022; Wang et al. 2022b; Smith et al. 2023; Wang, Huang, and Hong 2022) propose to train a set of learnable prompt tokens for knowledge retention. However, most methods aim to balance new-domain learning and old-domain forgetting, resulting in a zero-sum game. As illustrated in Fig. 1 (a), the increase in the number of domains causes more features to crowd within the same feature space, escalating the competition between new and old domains. This raises a question: are different domains necessarily in opposition?\n\nTo address this question, we draw inspiration from the incremental cognitive process of humans. The cognitive science research (Bar 2003; Fenske et al. 2006) indicate that the early visual areas of the brain extract both low and high spatial frequency signals (LF & HF) from the visual stimuli after encountering a new object. The LF signals are projected to the prefrontal cortex, forming coarse-grained features within approximately $1 4 0 \\mathrm { m s }$ (Barcelo, Suwazono, and Knight 2000). For example, when seeing a photograph or a cartoon image of a cat, the brain initially forms a general impression of an animal, without determining the cat‚Äôs color or whether the cat is real. This process occurs so quickly that we are often unaware of it in everyday life. The HF signals are processed by the ventral stream to generate fine-grained features, such as the texture of the image (identifying the domain it belongs to) or the precise category of the object (e.g., cat or dog within the animal category). From this research, we derive two key insights: (a) The learning of new knowledge is hierarchical. The brain first forms a general concept of a new object before focusing on its details. In contrast, existing deep classification models treat all given categories equally, without considering their inherent semantic relationships. (b) New and old domains are not completely distinct. Although domain shifts cause visual differences among different domains, the coarse-grained representations of the same concept are similar from a human perspective. This answers the above question.\n\nBased on the above insights, we design a framework based on two principles. (a) We propose treating images of the same class across multiple domains as a single concept. This concept remains consistent regardless of the domain. For instance, the concept of a ‚Äúcat‚Äù does not change whether it is represented by photos, cartoons, or simple sketches of cats. During the initial domain training in RFDIL, we employ neural networks to mine the commonalities of the same concept across different domains. Specifically, we use the semantic features of the concept to construct a concept prototype. To maximize the separability between different prototypes, we represent the concept prototype using a semantically guided simplex Equiangular Tight Frame (ETF) through the neural collapse theory. The ETF is a mathematical structure that maximizes the distances between feature pairs, with further details provided in the preliminary section. (b) We introduce Dual-level Concept Prototypes (DualCP) to model the human cognitive process of recognizing new objects. we first group all classes based on their superordinate, as shown in Fig. 1 (b). For example, cats and dogs belong to the animal group, while cars and buses belong to the vehicle group. First-level concept prototypes are designed for each superordinate concept to simulate the coarse-grained features formed by LF signals in the prefrontal cortex. Within each group, we design secondlevel concept prototypes to simulate the process by which HF signals in the ventral stream recognize specific classes.\n\nFinally, we introduce a coarse-to-fine calibrator (C2F) to align image features with their corresponding concept prototypes. A Dual-level Dot Regression (DDR) loss function is employed to optimize the training of the C2F. We conduct extensive experiments on three benchmark datasets, including DomainNet, CORe50, and CDDB, comparing our DualCP with state-of-the-art methods.\n\nOur contributions can be summarized as follows:\n\n‚Ä¢ Inspired by the incremental cognition process of humans, we propose treating cross-domain images of the same class as a single concept. We construct concept prototypes for all domains to prevent a zero-sum game between new and old domains.   \n‚Ä¢ We introduce DualCP, an RFDIL method based on duallevel concept prototypes, which further enhances the separability between classes by performing fine-grained classification on similar categories.   \n‚Ä¢ We propose the C2F module and the DDR loss function to align objects with their corresponding coarse-grained and fine-grained concept prototypes.   \n‚Ä¢ Extensive experiments demonstrate that the proposed DualCP method outperforms other RFDIL methods on three benchmark datasets, with a margin of up to $7 . 6 \\%$ on the CDDB dataset.\n\n# Related Work\n\n# Domain-Incremental Learning\n\nMultiple studies focus on addressing catastrophic forgetting (McCloskey and Cohen 1989) in Domain-Incremental Learning (DIL). For instance, (Kirkpatrick et al. 2017; Akyu¬®rek et al. 2021; Shi et al. 2023) restrict the model‚Äôs plasticity to balance the learning of new domains with the forgetting of old ones, resulting in a zero-sum game between new and old domains. Rehearsal-based methods (Rebuff et al. 2017; Chaudhry et al. 2019) mitigate the forgetting of old domains by replaying a subset of representative samples from the old domains, which raises privacy concerns. Rehearsal-free methods (Van De Ven, Li, and Tolias 2021; Wang et al. 2023; Wan et al. 2024b) use synthetic data replay from old domains to reduce forgetting. (Wang et al. 2022b; Liu, Peng, and Zhou 2024; Gao et al. 2024; Wang et al. 2025) dynamically expand the model to incrementally store knowledge from new domains, aiming to separate the learning of new and old domain knowledge to avoid conflicts.\n\n# Neural Collapse\n\nThe Neural Collapse (NC) theory (Papyan, Han, and Donoho 2020) posits that features of the same class in the final layer collapse to a single point on a hypersphere in a well-trained classification model. Besides, the distance between features of any two different classes is maximized, and these features of all classes collectively can be defined by a simplex Equiangular Tight Frame (ETF). This indicates that a ‚Äútraining endpoint‚Äù can be easily constructed for the model. We can align image features to the pre-designed endpoint, which is independent of the model‚Äôs initial parameters. The papers (Mixon, Parshall, and Pi 2020; Ji et al. 2021;\n\nZhou et al. 2022) have shown that the accuracy of models trained using the simplex ETF is comparable to those trained with conventional cross-entropy loss.\n\n# Applications of Neural Collapse in Other Tasks\n\nRecent studies have utilized the above characteristics of NC to address various tasks, such as imbalanced learning (Yang et al. 2022), few-shot class-incremental learning (Yang et al. 2023), and federated learning (Huang et al. 2023). Compared with existing research, the proposed DualCP is not a simple combination of NC and RFDIL, but is a novel method inspired by the cognitive science research, i.e., the learning of new knowledge is hierarchical and the new and old domains are not completely distinct. We build concept prototypes based on the two insights to solve the RFDIL problem, while the ETF serves as a tool to realize our idea.\n\nFor the classification of a large number of classes, (Jiang et al. 2023) proposed maximizing the minimum one-vs-rest margins to achieve generalized neural collapse. However, this method has overlooked the inherent similarities among classes. Our proposed DualCP instead groups the numerous classes using text features, then performs coarse-grained classification between groups and fine-grained classification within groups. Our proposed dual-level concept prototypes are not only easy to implement but also accommodate up to $d ^ { 2 }$ classes when the feature dimension is $d$ , which is sufficient for most real-world scenarios.\n\n# Preliminary\n\n# Problem Formulation\n\nRehearsal-Free Domain-Incremental Learning (RFDIL) aims to train a unified model progressively on the data from $T$ domains $\\mathcal { D } = \\{ \\mathcal { D } _ { t } \\} _ { t = 1 } ^ { T }$ , where $T$ is the total number of domains. The data in each domain is split into the training set $\\mathcal { X } _ { t }$ and the test set $\\mathcal { Z } _ { t }$ , denoted as $\\mathcal { D } _ { t } = ( \\mathcal { X } _ { t } , \\mathcal { Z } _ { t } )$ . At the $t$ -th stage of training, the model is only allowed to train on $\\textstyle { \\mathcal { X } } _ { t }$ and does not have access to $\\begin{array} { r } { \\chi _ { 1 \\sim t - 1 } = \\bigcup _ { \\tau = 1 } ^ { t - 1 } \\mathcal { X } _ { \\tau } } \\end{array}$ . After the $t$ -th training, the model is tested on $\\begin{array} { r } { \\mathcal { Z } _ { 1 \\sim t } = \\bigcup _ { \\tau = 1 } ^ { t } \\mathcal { Z } _ { \\tau } } \\end{array}$ . The training set $\\textstyle { \\mathcal { X } } _ { t }$ is composed of $N _ { t }$ tuples, denoted as $\\mathcal { X } _ { t } ~ = ~ \\{ ( x _ { t , i } , y _ { t , i } ) \\} _ { i = 1 } ^ { N _ { t } }$ , where $x _ { t , i }$ signifies the $i$ -th image from the $t$ -th domain, and $y _ { t , i }$ denotes the label of $x _ { t , i }$ . The test set $\\mathcal { Z } _ { t }$ follows a similar structure. Moreover, define $\\mathcal { C } _ { t }$ as the set of labels for the $t$ -th domain, i.e., $\\forall i \\in [ 1 , N _ { t } ] , y _ { t , i } \\in$ $\\mathcal { C } _ { t }$ . In other words, $\\left| \\mathcal { C } _ { t } \\right|$ , the number of classes in each domain, remains constant in the same dataset.\n\n# Definition of Simplex ETF\n\nFor a well-trained classification model with $K$ classes, the within-class means correspond to $K$ prototypes, denoted as $\\mathbf { m } _ { i } \\in \\mathbb { R } ^ { d } , i = 1 , 2 , \\cdot \\cdot \\cdot , K$ , where $K \\leq d + 1$ . The collection of these prototypes, i.e., $\\mathbf { M } = [ \\mathbf { m } _ { 1 } , \\cdots , \\mathbf { m } _ { K } ]$ in $\\mathbb { R } ^ { d \\times K }$ , is called a simplex ETF, which means:\n\n$$\n\\mathbf { M } = \\sqrt { \\frac { K } { K - 1 } } \\mathbf { U } ( \\mathbf { I } _ { K } - \\frac { 1 } { K } \\mathbf { 1 } _ { K } \\mathbf { 1 } _ { K } ^ { T } ) ,\n$$\n\nwhere $\\mathbf { U }$ satisfies $\\mathbf { U } ^ { T } \\mathbf { U } = \\mathbf { I } _ { K }$ , and $\\mathbf { 1 } _ { K }$ is a $K$ -dimensional all-ones vector. All prototypes $\\mathbf { m } _ { i }$ have the same $l _ { 2 }$ norm,\n\ni.e., $| \\mathbf { m } _ { i } | = 1 , i = 1 , 2 , \\cdots , K$ , and the same pair-wise angle, i.e.,\n\n$$\n\\mathbf { m } _ { i } ^ { T } \\mathbf { m } _ { j } = \\frac { K } { K - 1 } \\delta _ { i , j } - \\frac { 1 } { K - 1 } , \\forall i , j \\in [ 1 , K ] ,\n$$\n\nwhere $\\delta _ { i , j }$ equals to 1 when $i = j$ and 0 otherwise. The pairwise angle $- \\frac { 1 } { K - 1 }$ is the maximal equiangular separation of $K$ prototypes in the $d$ -dimension feature space.\n\nBased on the definition of simplex ETF, the NC phenomenon can be summarized as: (NC1) Feature collapse. The last-layer features of the same class will collapse to their within-class mean, i.e., $\\textstyle \\sum _ { W } \\to 0$ , where $\\begin{array} { r } { \\sum _ { W } = \\mathrm { A v g } \\{ ( \\mu _ { k , i } - \\mu _ { k } ) ( \\mu _ { k , i } - \\mu _ { k } ) ^ { T } \\} } \\end{array}$ , $\\mu _ { k , i }$ is the feature of the $i$ -th sample of the $k$ -th class and $\\mu _ { k }$ is the within-class mean of the $k$ -th class. (NC2) Convergence to simplex ETF. The within-class means of all $K$ classes is centered by the global mean $\\mu _ { G } ~ = ~ \\mathrm { A v } \\mathbf { g } _ { i , k } ( \\mu _ { k , i } )$ . These means $\\mu _ { k }$ will converge to the $K$ prototypes of a simplex ETF, i.e., $\\hat { \\mu } _ { k } ~ = ~ ( \\mu _ { k } ~ -$ $\\mu _ { G } ) / \\left\\| \\mu _ { k } - \\mu _ { G } \\right\\| , 1 \\leq k \\leq K ,$ , where $\\hat { \\bf M } = [ \\hat { \\mu } _ { 1 } , \\cdot \\cdot \\cdot , \\hat { \\mu } _ { K } ]$ satisfies Eq. (1), and $\\hat { \\mu } _ { k }$ satisfies Eq. (2). (NC3) Self-duality. The within-class feature means will be aligned with their corresponding classifier weights $\\mathbf { w } _ { k }$ , i.e., $\\hat { \\mu } _ { k } = \\mathbf { w } _ { k } / \\left\\| \\mathbf { w } _ { k } \\right\\|$ . (NC4) Based on (NC1)-(NC3), the model prediction using the classifier can be simplified to select the nearest class center, i.e., a $\\operatorname { \\cdot g m a x } _ { k } \\langle \\mu , \\mathbf { w } _ { k } \\rangle = \\operatorname { \\ a r g m i n } _ { k } \\| \\mu - \\mu _ { k } \\|$ , where $\\langle \\cdot \\rangle$ is the inner product operator, and $\\mu$ is the last-layer feature of a sample for prediction.\n\n# Method\n\n# Overall Framework\n\nFig. 2 illustrates the framework of our proposed DualCP. Our method contains three main components: (a) a pretrained image feature extractor $f _ { i } ( \\cdot ; \\theta _ { i } )$ , (b) the Concept Prototype Generator (CPG) with a pre-trained text feature extractor $f _ { t } ( \\cdot ; \\theta _ { t } )$ , and (c) the Coarse-to-Fine calibrator (C2F), which includes a coarse-grained layer $g _ { C } ( \\cdot ; \\varphi _ { C } )$ and multiple fine-grained layers $g _ { F _ { k } } ( \\cdot ; \\varphi _ { F _ { k } } )$ .\n\nFor simplicity, we abbreviate $x _ { t , i }$ as $x$ and $y _ { t , i }$ as $y$ . Given a set of images $x$ and their class names $y$ , we first extract the image features by $\\mathbf { x } = f _ { i } ( x ; \\theta _ { i } )$ . Then, the class names are fed into the CPG to extract text features and construct dual-level concept prototypes, which include coarse-grained prototypes $\\mathbf { E } _ { C }$ and fine-grained prototypes $\\mathbf { E } _ { F _ { i } }$ . Finally, we train the C2F to align the image features $\\mathbf { x }$ with the corresponding concept prototypes by minimizing the proposed Dual Dot-Regression (DDR) loss function. For further details on the training and inference of our DualCP framework, please refer to the appendix.\n\n# Concept Prototype Generator\n\nPreparation for generating prototypes. First, we collect the names of all classes, denoted as $\\mathcal { C } _ { t } = \\{ y _ { 1 } , y _ { 2 } , \\cdot \\cdot \\cdot , y _ { K } \\}$ , e.g., $\\begin{array} { r } {  { \\mathcal { C } } _ { t } ~ = ~ \\left\\{ \\begin{array} { r l r l } \\end{array} \\right. } \\end{array}$ {airplane, bike, cat, ..., zebra}. $K = | \\mathcal { C } _ { t } |$ is the\n\n(b) Concept Prototype Generator (CPG) catflower o Text Features „ÄÇ O LA dog boat Texre O O ? Concatenate bike bus Extractor C Image Features Class name y ft(-;0t) Similarity_S G ‚ÜìG' Coarse-grained ‚ñ° DDR Loss Ec Image Features xc Fine-grained gc(-;œÜc) ‰∫´ Image Features Coarse-grained Ôºö Concept Prototypes 2 fi(-;0i) Image Fine-grained featuresx [x,xc]gFk(;œÜFkÔºâxF EF1 Concept Prototypes Data (a) Feature Stream Extractor (c) Coarse-to-Fine Calibrator (C2F)\n\nFigure 2: The framework of the proposed DualCP. DualCP comprises three main components: (a) a feature extractor to get the image features, (b) the CPG module to construct the dual-level concept prototype based on the text features of the class names, and (c) the C2F module to align the image features with the corresponding prototypes. We selected six common classes from the DomainNet dataset, i.e., cat, flower, dog, boat, bike, and bus, to further illustrate our method. Similar classes were grouped, such as cats and dogs. We constructed coarse-grained prototypes between groups and fine-grained prototypes within groups. This coarse-to-fine classification approach helps the model better distinguish similar categories. Best viewed in color.\n\nnumber of classes. Text features are extracted by:\n\n$$\n\\begin{array} { r } { { \\bf { y } } _ { i } = \\frac { f _ { t } \\left( y _ { i } ; \\theta _ { t } \\right) } { \\left\\| f _ { t } \\left( y _ { i } ; \\theta _ { t } \\right) \\right\\| } , } \\\\ { { \\bf { Y } } = [ { \\bf { y } } _ { 1 } , { \\bf { y } } _ { 2 } , \\cdot \\cdot \\cdot , { \\bf { y } } _ { K } ] , } \\end{array}\n$$\n\nwhere $f _ { t } ( \\cdot ; \\theta _ { t } )$ denotes a pre-trained text feature extractor, such as the CLIP text encoder (Radford et al. 2021). The symbol $[ \\cdot ]$ represents the concatenation operation, the matrix $\\mathbf { Y } \\in \\mathbb { R } ^ { d \\times K }$ is the collection of text features, with $d$ being the feature dimension. Note that text features have been normalized, i.e., $\\forall i \\in [ 1 , K ]$ , $\\| \\mathbf { y } _ { i } \\| = 1$ .\n\nSecond, we introduce two strategies of the CPG, namely, vanilla concept prototype and dual-level concept prototype. We will elucidate our approaches successively.\n\nVanilla Concept Prototype. We constructed a simplex ETF based on text features to maximize the separability between each pair of generated concept prototypes. Specifically, we perform a QR decomposition on $\\mathbf { Y }$ to obtain the orthogonal basis of text features by:\n\n$$\n\\mathbf { Y } = \\mathbf { Q R } ,\n$$\n\nwhere $\\mathbf { Q } = [ \\mathbf { q } _ { 1 } , \\mathbf { q } _ { 2 } , \\cdot \\cdot \\cdot , \\mathbf { q } _ { K } ] \\in \\mathbb { R } ^ { d \\times K }$ is an orthogonal matrix, $\\mathbf { q } _ { i } ^ { T } \\mathbf { q } _ { j } = 1$ , when $i = j$ , and 0 otherwise. $\\mathbf { R } \\in \\mathbb { R } ^ { K \\times K }$ is an upper triangular matrix. Then we can compute the vanilla concept prototypes following Eq. (1) by:\n\n$$\n\\mathbf { E } = \\sqrt { \\frac { K } { K - 1 } } \\mathbf { Q } ( \\mathbf { I } _ { K } - \\frac { 1 } { K } \\mathbf { 1 } _ { K } \\mathbf { 1 } _ { K } ^ { T } ) ,\n$$\n\nwhere $\\mathbf { I } _ { K }$ is a $K \\times K$ identity matrix, and $\\mathbf { 1 } _ { K }$ represents a $K$ -dimensional all-one vector. Besides, $\\mathrm { ~ \\bf ~ E ~ } =$ $[ \\mathbf { e } _ { 1 } , \\mathbf { e } _ { 2 } , \\cdot \\cdot \\cdot , \\mathbf { e } _ { K } ] \\in \\mathbb { R } ^ { d \\times K }$ encompasses the concept prototypes for all classes, where ${ \\bf e } _ { i } \\in \\mathbb { R } ^ { d }$ denotes the prototype for the $i$ -th class. The similarity between pairwise prototypes can be expressed as:\n\n$$\n\\mathbf { e } _ { i } ^ { T } \\mathbf { e } _ { j } = \\frac { K } { K - 1 } \\mathbf { q } _ { i } ^ { T } \\mathbf { q } _ { j } - \\frac { 1 } { K - 1 } , \\forall i , j \\in [ 1 , K ] ,\n$$\n\nDual-level Concept Prototype. The construction of vanilla concept prototypes treats each class equally, without considering the similarity between classes. To enhance the distinction between similar categories, a direct idea is to perform more fine-grained differentiation for similar classes. Specifically, we set the similar classes as a group and construct dual-level concept prototypes, comprising coarsegrained prototypes for all groups and fine-grained prototypes for all classes within each group. We commence by computing the similarity matrix S between all classes as $\\mathbf { S } = \\mathbf { Y } ^ { \\hat { T } } \\mathbf { Y }$ . We use the matrix S to obtain the adjacency matrix A with the hyperparameter $p$ by:\n\n$$\n\\mathbf { A } _ { i j } = \\{ \\mathbf { S } _ { i j } > p \\} , \\forall i , j \\in [ 1 , K ] ,\n$$\n\nwhere $\\{ \\cdot \\}$ denotes an Iverson bracket, a mathematical notation used to represent a logical value based on a condition. It equals 1 if the condition is true and 0 if false.\n\nSubsequently, we construct a connectivity graph ${ \\mathcal { G } } \\ =$ $\\{ \\gamma , \\varepsilon \\}$ , representing the relationships between all classes. $\\nu$ denotes the set of nodes (classes), and $| \\mathcal { V } | = K . \\mathcal { E }$ denotes the set of edges (whether the two classes are similar), and $\\begin{array} { r } { \\left| \\mathcal { E } \\right| = \\sum _ { i = 1 } ^ { K } \\bar { \\sum _ { j = 1 } ^ { K } \\mathbf { A } _ { i j } } } \\end{array}$ . To group similar nodes based on the ad cency matrix A, we conduct a connectivity analysis on the graph $\\mathcal { G }$ and get another graph $\\mathcal { G } ^ { \\prime }$ . Connectivity analysis refers to the process where if a path exists between two classes, they should be placed in the same group. The details can be found in the appendix. Please refer to the illustration in Fig. 2 (b) for a simple example.\n\nBased on the above algorithms, we can group the text features $\\mathbf { Y } = [ \\mathbf { Y } _ { 1 } , \\mathbf { Y } _ { 2 } , \\cdot \\cdot \\cdot \\mathbf { \\sigma } , \\mathbf { Y } _ { N _ { g } } ]$ , where $N _ { g }$ is the number of groups. $\\mathbf { Y } _ { i } \\in \\mathbb { R } ^ { | g _ { k } | \\times d }$ represents the text features for the $i$ -th group, and $\\left| g _ { k } \\right|$ is the number of concepts in the $k$ -th group. Then we compute the average text feature for each group by:\n\n$$\n\\bar { \\mathbf { Y } } = [ \\frac { 1 } { | g _ { 1 } | } \\sum _ { i = 1 } ^ { | g _ { 1 } | } \\mathbf { Y } _ { 1 , i } , \\frac { 1 } { | g _ { 2 } | } \\sum _ { i = 1 } ^ { | g _ { 2 } | } \\mathbf { Y } _ { 2 , i } , \\cdots , \\frac { 1 } { | g _ { N _ { g } } | } \\sum _ { i = 1 } ^ { | g _ { N _ { g } } | } \\mathbf { Y } _ { N _ { g } , i } ] .\n$$\n\nBased on Eqs. (5) to (7) and (9), we can calculate the coarsegrained concept prototypes (CCP) through $\\bar { \\mathbf Y }$ , denoted as:\n\n$$\n\\mathbf { E } _ { C } = [ \\mathbf { e } _ { C , 1 } , \\mathbf { e } _ { C , 2 } , \\cdot \\cdot \\cdot , \\mathbf { e } _ { C , N _ { g } } ] \\in \\mathbb { R } ^ { d \\times N _ { g } } .\n$$\n\nAdditionally, we can calculate the fine-grained concept prototypes (FCP) using $\\mathbf { Y } _ { i }$ , represented as:\n\n$$\n{ \\bf E } _ { F _ { k } } = [ { \\bf e } _ { F _ { k } , 1 } , { \\bf e } _ { F _ { k } , 2 } , \\cdot \\cdot \\cdot , { \\bf e } _ { F _ { k } , | g _ { k } | } ] \\in \\mathbb { R } ^ { d \\times | g _ { k } | } .\n$$\n\nNow we construct the dual-level concept prototypes for all classes. In other words, our CPG generates a coarse-grained prototype $\\mathbf { e } _ { C }$ and a fine-grained prototype $\\mathbf { e } _ { F }$ for each class.\n\n# Coarse-to-Fine Calibrator\n\nThe coarse-to-fine calibrator (C2F) is proposed to align the image features with the corresponding prototypes. Given an input image $x$ and its class name $y$ , we extract features of the image $x$ as $\\mathbf { x } ~ = ~ f _ { i } ( x ; \\theta _ { i } ) ~ \\in ~ \\mathbb { R } ^ { d }$ , where $f _ { i } ( \\cdot ; \\theta _ { i } )$ is a pre-trained model, such as ViT (Dosovitskiy et al. 2020) or CLIP image encoder (Radford et al. 2021). Assuming $y$ is the $j$ -th class from the $i$ -th group, its concept prototypes are referred to as $\\mathbf { e } _ { C , i }$ and $\\mathbf { e } _ { F _ { i } , j }$ . The C2F module consists of the coarse-grained layer $g _ { C } ( \\cdot ; \\varphi _ { C } )$ and the fine-grained layers $g _ { F _ { k } } ( \\cdot ; \\varphi _ { F _ { k } } )$ . The coarse-grained feature $\\mathbf { x } _ { C }$ and fine-grained features $\\mathbf { x } _ { F }$ can be computed by:\n\n$$\n\\begin{array} { r l } & { \\mathbf { x } _ { C } = g _ { C } ( \\mathbf { x } ; \\varphi _ { C } ) \\in \\mathbb { R } ^ { d } , } \\\\ & { \\mathbf { x } _ { F } = g _ { F _ { i } } ( [ \\mathbf { x } , \\mathbf { x } _ { C } ] ; \\varphi _ { F } ) \\in \\mathbb { R } ^ { d } . } \\end{array}\n$$\n\nDual Dot-Regression Loss. To train the C2F module, we propose the Dual Dot-Regression (DDR) loss function, denoted as $\\mathcal { L }$ . We train the parameter sets $\\varphi _ { C }$ and $\\varphi _ { F }$ by minimize the DDR loss with a hyperparameter $\\alpha$ :\n\n$$\n\\begin{array} { r l } & { \\underset { \\varphi _ { C } , \\varphi _ { F } } { \\operatorname* { m i n } } \\mathcal { L } ( \\mathbf { x } _ { C } , \\mathbf { x } _ { F } , \\mathbf { e } _ { C , i } , \\mathbf { e } _ { F _ { i } , j } ) = } \\\\ & { \\quad \\quad \\quad \\alpha ( \\mathbf { x } _ { C } ^ { T } \\mathbf { e } _ { C , i } - 1 ) ^ { 2 } + ( 1 - \\alpha ) ( \\mathbf { x } _ { F } ^ { T } \\mathbf { e } _ { F _ { i } , j } - 1 ) ^ { 2 } , } \\end{array}\n$$\n\n# Theoretical Analysis\n\nTheorem 1. The angle between any pair of CCPs or FCPs is larger than or equal to the angle between any pair of vanilla concept prototypes:\n\n$$\n\\begin{array}{c} \\begin{array} { r } { \\langle \\mathbf { e } _ { C , m } , \\mathbf { e } _ { C , n } \\rangle \\geq \\langle \\mathbf { e } _ { i } , \\mathbf { e } _ { j } \\rangle , } \\\\ { \\langle \\mathbf { e } _ { F _ { k } , m } , \\mathbf { e } _ { F _ { k } , n } \\rangle \\geq \\langle \\mathbf { e } _ { i } , \\mathbf { e } _ { j } \\rangle , \\forall k , \\mathbf { \\Omega } \\end{array} \\Bigg \\} \\forall i \\neq j , m \\neq n . } \\end{array}\n$$\n\nThis theorem demonstrates that our proposed dual-level concept prototype has larger inter-class angles, indicating better classification capability than the vanilla concept prototype. The proof of the theorem is provided in the appendix.\n\nTable 1: Experimental results on the DomainNet dataset. $\\dagger$ denotes that the method is based on the pre-trained ViTB/16 model. \\* denotes that DARE is based on ResNet-18. The best result within rehearsal-free methods is indicated by bold, and the second is marked by underline.   \n\n<html><body><table><tr><td>Method</td><td>Buffer(‚Üì)Ar(‚Üë)Fr(‚Üë)</td><td></td></tr><tr><td>DyTox (Douillard et al. 2022) DARE(Jee.,Ara.,and Zon.2024) DARE++ (Jee.,Ara.,and Zon.2024)</td><td>50/class 32.32* -22.98</td><td>62.94 40.51* =</td></tr><tr><td>EWC (Kirkpatrick et al. 2017) LwF (Li and Hoiem 2017) SimCLR (Chen etal.2020) BYOL (Grill et al. 2020) Barlow Twins (Zbontar et al.2021) SupCon (Khosla et al. 2020) L2P (Wang et al. 2022b) DualPrompt (Wang et al. 2022a)</td><td>47.62 49.19 44.20 49.70 48.90 50.90 O/class 40.15‚Ä† 43.79t</td><td>-5.01 -2.25 -2.03</td></tr><tr><td>S-iP (Wang,Huang,and Hong 2022) CODA-P (Smith et al. 2023) C-Prompt (Liu,Peng,and Zhou 2024)</td><td>50.62‚Ä† 47.42‚Ä†</td><td>-2.85 -3.46</td></tr><tr><td>DualCP (ours)</td><td>58.68t 60.13‚Ä†</td><td> -1.96</td></tr></table></body></html>\n\n# Experiments\n\n# Experimental Settings\n\nDatasets. We conducted experiments on three multidomain datasets, include DomainNet (Peng et al. 2019), CDDB (Li et al. 2023), and CORe50 (Lomonaco and Maltoni 2017). DomainNet emerges as a large-scale dataset for DIL and domain adaptation, whose images are sourced from six domains marked by prominent inter-domain variations, with each domain including 345 categories. The training set of DomainNet consists of 409,832 images, while the test set comprises 176,743 images. CORe50 is an object recognition dataset involving 11 distinct domains (50 classes per domain), with 8 domains for training and 3 for testing. CDDB is specifically crafted for deepfake detection, encompassing 12 distinct deepfake methodologies and 3 different evaluation scenarios. We opt for the most challenging HARD track as suggested by S-Prompts (Wang, Huang, and Hong 2022).\n\nEvaluation Metrics. There are three commonly used evaluation metrics for DIL: (1) the average accuracy $( A _ { T } )$ at the end of training on all $T$ domains; (2) the forgetting degree $( F _ { T } )$ following (Li et al. 2023), and the formulas for calculating $A _ { T }$ and $F _ { T }$ are detailed in the appendix; (3) the ‚ÄúBuffer‚Äù represents additional data stored by the model for incremental learning. This data may include images from old domains used for rehearsal.\n\nImplementation Details. We employ an SGD optimizer with an initial learning rate of 0.1 and a cosine decay schedule. Additionally, we apply the weight decay of $2 e ^ { - 4 }$ for regularization to mitigate overfitting. The training consists of 20 epochs on all datasets except DomainNet, which extends to 30 epochs. The mini-batch size is set to 128. The hyperparameters $p$ and $\\alpha$ are set to 0.85 and 0.5, respectively. The ablations of $p$ and $\\alpha$ are provided in the appendix.\n\nTable 2: Experimental results on the Hard track of CDDB. $\\dagger$ denotes that the method is based on the pretrained ViT-B/16 model. The best result within rehearsalfree methods is indicated by bold, and the second is marked by underline.   \n\n<html><body><table><tr><td>Method</td><td>Buffer(‚Üì)</td><td>Ar(‚Üë)</td><td>Fr(‚Üë)</td></tr><tr><td>LRCIL (Pellegrini et al. 2020) iCaRL (Marra et al. 2019) LUCIR (Hou et al. 2019)</td><td>100/class</td><td>76.39 79.76 82.53</td><td>-4.39 -8.73 -5.34 -8.62</td></tr><tr><td>LRCIL (Pellegrini et al. 2020) iCaRL (Marra etal. 2019) LUCIR (Hou et al. 2019) DyTox (Douillard etal.2022)</td><td>50/class</td><td>74.01 73.98 80.77 86.21</td><td>-14.50 -7.85 -1.55 -42.62</td></tr><tr><td>EWC (Kirkpatrick etal.2017) LwF(Liand Hoiem 2017) DyTox (Douillard etal.2022) L2P (Wang et al. 2022b) DualPrompt (Wang et al.2022a)</td><td>0/class</td><td>50.59 60.94 51.27 61.28‚Ä† 64.80‚Ä†</td><td>-13.53 -45.85 -9.23 -8.74</td></tr><tr><td>S-iP (Wang,Huang,and Hong 2022) CODA-P (Smith et al.2023) DualCP (ours)</td><td></td><td>74.51‚Ä† 70.54‚Ä† 82.16‚Ä†</td><td>-1.30 -5.53 -0.73</td></tr></table></body></html>\n\n# Main Results\n\nBaselines. The current DIL methods can be broadly categorized into rehearsal-based and rehearsal-free methods. Rehearsal-based methods select and retrain a subset of images as exemplars of the domain when training. Representative methods include ER, LRCIL, iCaRL, and LUCIR. Rehearsal-free methods do not require saving images from the old domain. Representative methods include EWC, LwF, DyTox, L2P, S-Prompts, etc. Note that rehearsal-based methods often require storing thousands of images, ranging in size from 100MB to 3GB. In contrast, rehearsal-free methods may require only a small amount of learnable parameters, occupying 1-50MB of space. Therefore, rehearsal-free methods significantly outperform rehearsal-based methods in terms of storage space requirements. The proposed DualCP belongs to the rehearsal-free setting, so we prioritize comparing it with similar methods. Additionally, we also list the state-of-the-art rehearsal-based methods for reference.\n\nComparison with State-of-the-arts. We compare our approach with other state-of-the-art (SOTA) methods on three DIL benchmark datasets. The methods are grouped based on the number of images per class to be retained, with ‚Äú0/class‚Äù indicating a rehearsal-free approach.\n\nTable 1 illustrates the comparison results on the DomainNet dataset. Our DualCP surpasses the SOTA method CPrompt $( 6 0 . 1 3 \\%$ vs. $5 8 . 6 8 \\%$ ), even coming close to the performance of the DyTox method that utilizes rehearsal. Besides, our method achieved the best performance in preventing forgetting among all methods $( - 1 . 9 6 \\% )$ . Table 2 showcases results on CDDB, where our DualCP surpasses the best rehearsal-free method by a large margin $( 8 2 . 4 6 \\%$ vs. $7 4 . 5 1 \\%$ ), and is comparable to LUCIR, which utilizes a substantial buffer (‚Äú100/class‚Äù). It falls just behind the rehearsal-based DyTox method. Additionally, our method outperforms the SOTA method in mitigating forgetting (- $0 . 7 3 \\%$ vs. $- 1 . 3 0 \\%$ ). Table 3 presents the comparisons on CORe50, revealing that our method achieves the best results in both rehearsal-based $8 8 . 1 0 \\%$ vs. $8 1 . 0 7 \\%$ ) and rehearsalfree ( $8 8 . 1 0 \\%$ vs. $8 5 . 6 8 \\%$ ) tracks.\n\nTable 3: Experimental results on the CORe50 dataset. Note that $F _ { T }$ is not applicable to CORe50 because the training and test domains do not overlap. $\\dagger$ denotes that the method is based on the pre-trained ViT-B/16 model. The best result within rehearsal-free methods is indicated by bold, and the second is marked by underline.   \n\n<html><body><table><tr><td>Method</td><td>Buffer(‚Üì)</td><td>Ar(‚Üë)</td></tr><tr><td>ER(Chaudhry et al.2019) GDumb (Prabhu,Torr,and Dokania 2020) BiC (Wu et al. 2019) DER++ (Buzzega et al. 2020) Co¬≤L (Cha,Lee,and Shin 2021) DyTox (Douillard et al. 2022) L2P(Wang et al. 2022b)</td><td>50/class</td><td>80.10 74.92 79.28 79.70 79.75 79.21 81.07 74.82</td></tr><tr><td>EWC(Kirkpatrick etal.2017) LwF (Li and Hoiem 2017) L2P (Wang et al.2022b) DualPrompt (Wang et al. 2022a) S-iP (Wang,Huang,and Hong 2022) CODA-P (Smith et al.2023)</td><td>O/class</td><td>75.45 78.33t 80.25‚Ä† 83.13‚Ä† 85.68‚Ä†</td></tr><tr><td>C-Prompt (Liu,Peng,and Zhou 2024) DualCP (ours)</td><td></td><td>85.31‚Ä† 88.10‚Ä†</td></tr></table></body></html>\n\nTable 4: Comparison of different concept prototype designs in our framework. \\* represents that CDDB only contains two classes: real and fake, thus it cannot be divided into more groups and cannot apply to the DualCP.   \n\n<html><body><table><tr><td>MethodsDatasets</td><td>D45 ainNes)</td><td>CDDBs)</td><td>CORe50</td></tr><tr><td></td><td>Ar(‚Üë)Fr(‚Üë)</td><td>Ar(‚Üë)Fr(‚Üë)</td><td>Ar(‚Üë)</td></tr><tr><td>VanillaCP</td><td>56.22 -2.80</td><td>82.16 -0.73</td><td>86.27</td></tr><tr><td>DualCP (proposed)</td><td>60.13 -1.96</td><td>82.16* -0.73*</td><td>88.10</td></tr></table></body></html>\n\n# Ablation Study\n\nEffectiveness of Dual Concept Prototype. We introduce two solutions to generate the concept prototypes: a singlelevel concept prototype (VanillaCP) and a dual-level concept prototype (DualCP). Table 4 demonstrates a consistent improvement of DualCP over VanillaCP, especially on datasets containing a large number of classes. This indicates that our DualCP contributes to distinguishing similar classes.\n\nComparsion on Different Backbones. To validate the generality of our approach, we conducted experiments on both ViT and CLIP, a multimodal model based on contrastive learning with an image encoder and a text encoder. Table 5 presents the accuracy of our DualCP under different settings, as well as that of S-Prompts and MoP-CLIP.\n\nImage Feature Extractor. We extract image features using two optional setting, i.e., ViT-B/16 or CLIP image en\n\n<html><body><table><tr><td rowspan=\"2\">Methods</td><td rowspan=\"2\">ImageFeatureExtractor ViT-B/16 CLIP-Image</td><td colspan=\"2\">CPGGuidance</td><td colspan=\"2\">DomainNet</td><td colspan=\"2\">CDDB</td><td>CORe50</td></tr><tr><td>ViT-B/16-BD</td><td>CLIP-Text Ar(‚Üë)</td><td>Fr(‚Üë)</td><td></td><td>Ar(‚Üë)</td><td>Fr(‚Üë)</td><td>Ar(‚Üë)</td></tr><tr><td>S-Prompts</td><td>‚àö</td><td></td><td></td><td>50.62</td><td>-2.85</td><td>74.51</td><td>-1.30</td><td>83.13</td></tr><tr><td></td><td>‚àö</td><td>*</td><td></td><td>67.78</td><td>-1.64</td><td>88.65</td><td>-0.69</td><td>89.06</td></tr><tr><td>MoP-CLIP</td><td>‚àö ‚àö</td><td></td><td>‚àö*</td><td>69.70</td><td></td><td>88.54</td><td>-0.79</td><td>92.29</td></tr><tr><td rowspan=\"3\">DualCP (ours)</td><td></td><td></td><td>60.13</td><td>-1.96</td><td>82.16</td><td></td><td>-0.73</td><td>88.10</td></tr><tr><td>‚àö</td><td>‚àö ‚àö</td><td>62.73</td><td>-1.81</td><td></td><td>83.05</td><td>-0.76</td><td>88.55</td></tr><tr><td>‚àö ‚àö</td><td>‚àö</td><td>69.31 72.46</td><td>-1.49 -1.26</td><td></td><td>91.86 92.34</td><td>-0.35 -0.32</td><td>89.98 90.59</td></tr></table></body></html>\n\n62 -10 0.5x 60 1x -8 C 58 2x 56 4x F 54 -4 52 50 -2 1 layer\n\ncoder, as the feature extractor $( f _ { i } ( \\cdot ; \\theta _ { i } ) )$ .\n\nCPG Guidance. We proposed two methods to extract the concept prototype in the CPG module. One is based on the CLIP text encoder, as shown in Eq. (3). When CLIP is unavailable, we introduce an alternative approach using ViT. We use ViT-B/16 to extract image features from the base domain, and then calculate the mean features $\\mathbf { y } _ { i } ^ { \\prime }$ for each class. We use $\\mathbf { Y } ^ { \\prime } = \\{ \\mathbf { y } _ { 1 } ^ { \\prime } , \\mathbf { y } _ { 2 } ^ { \\prime } , \\cdot \\cdot \\cdot , \\mathbf { y } _ { k } ^ { \\prime } \\}$ to guide the construction of the concept prototype, as shown in Eq. (5).\n\nAblations of C2F Design. Our C2F comprises a coarsegrained layer $g _ { C }$ and multiple fine-grained layers $g _ { F _ { i } }$ , which are implemented as a multi-layer perceptron (MLP). As depicted in Fig. 3, we conduct ablation experiments on the number of layers and the hidden dimension of our C2F to assess the influence of MLP on model performance.\n\n# Visualization\n\n![](images/9e5d9445a1aa12eeea2ceee648ecb49f53a136fb12636ef21ffa8a825c532429.jpg)  \nTable 5: Comparison on different backbones. ViT-B/16-BD means that image features extracted from the base domain by ViT-B/16 are used to guide the CPG module. \\* indicates that S-Prompts and MoP-CLIP utilize CLIP text encoder but are unrelated to the CPG module. The proposed CPG module is only used for our DualCP.   \nFigure 3: Ablation study of the C2F module on DomainNet dataset. $A _ { T }$ , $F _ { T }$ denotes the average accuracy and the forgetting degree, respectively. The hidden dimensions are set as multiples of the image feature dimensions of 768 in the ViT-B/16 backbone. $\\mathrm { \\cdot } 0 . 5 \\mathrm { x }$ , 1x, 2x, $4 \\mathrm { x } ^ { \\prime \\prime }$ correspond to hidden dimensions of 384, 768, 1536, and 3072, respectively.   \nFigure 4: t-SNE visualization of feature space for common classes in DomainNet, with flower represented in green, cat and dog in (c), and boat, bicycle, and bus in (d). The pentagrams represent the average image features of a group.\n\nWe utilize t-SNE visualization to demonstrate the effectiveness of our DualCP. We choose six common classes (cat, flower, dog, boat, bicycle, and bus) from DomainNet for presentation. As shown in Fig. 4, (a) represents the image features extracted by the pre-trained ViT model. (b-d) depict the image features extracted by our DualCP. Our model categorizes similar classes into the same group based on semantics, such as cat and dog. (b) represents the coarsegrained features extracted by DualCP. (c) and (d) represent fine-grained features of different groups. As illustrated in Fig. 4 (c)(d), our dual-level concept prototypes effectively distinguish similar classes.\n\n# Conclusion and Future Works\n\nThis paper introduces a novel approach to RFDIL inspired by humans‚Äô incremental cognitive processes. We proposed constructing dual-level concept prototypes (DualCP) for each class across domains to address the zero-sum game between learning new domains and preserving old ones. By aligning features from different domains to the same feature space, we avoid compromising the feature space of old domains while accommodating new domain features. Extensive experiments on three datasets with different backbones consistently show that our DualCP outperforms existing SOTA methods. Furthermore, our method is expected to be extended to other applications, such as domain-incremental object detection. It may also provide a reference for the development of generalized neural collapse.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáËß£ÂÜ≥ÁöÑÊ†∏ÂøÉÈóÆÈ¢òÊòØ**Êó†ÊéíÁªÉÂüüÂ¢ûÈáèÂ≠¶‰π†ÔºàRehearsal-Free Domain-Incremental Learning, RFDILÔºâ**‰∏≠ÁöÑÊñ∞ÊóßÁü•ËØÜÂÜ≤Á™ÅÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Âú®Êñ∞ÊóßÂüüÁâπÂæÅ‰πãÈó¥ËøõË°åÈõ∂ÂíåÂçöÂºàÔºåÂØºËá¥ÁâπÂæÅÁ©∫Èó¥Êã•Êå§ÂíåÊÄßËÉΩ‰∏ãÈôç„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÂú®Ëá™Âä®È©æÈ©∂„ÄÅÂåªÁñóÂΩ±ÂÉèÁ≠âÂä®ÊÄÅÁéØÂ¢É‰∏ãÁöÑËßÜËßâËØÜÂà´‰∏≠ÂÖ∑ÊúâÂÖ≥ÈîÆ‰ª∑ÂÄºÔºåÂõ†‰∏∫ÂÆÉÊó†ÈúÄÂ≠òÂÇ®ÊóßÂüüÊï∞ÊçÆÔºàÈÅøÂÖçÈöêÁßÅÈóÆÈ¢òÔºâ‰∏îËÆ≠ÁªÉÊïàÁéáÊõ¥È´ò„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   Âèó‰∫∫Á±ªÂàÜÂ±ÇËÆ§Áü•ËøáÁ®ãÂêØÂèëÔºåÊèêÂá∫**ÂèåÂ±ÇÊ¨°Ê¶ÇÂøµÂéüÂûãÔºàDual-level Concept Prototypes, DualCPÔºâ**Ê°ÜÊû∂ÔºåÈÄöËøáÊûÑÂª∫Á≤óÁ≤íÂ∫¶ÔºàÁ±ªÁæ§ÔºâÂíåÁªÜÁ≤íÂ∫¶ÔºàÁ±ªÂÜÖÔºâÂéüÂûãÔºåÂàÜÁ¶ªÊñ∞ÊóßÂüüÁâπÂæÅÁ©∫Èó¥„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ1Ôºö** ÊèêÂá∫DualCPÔºåÈ¶ñÊ¨°Â∞ÜÁ•ûÁªèÂ¥©Ê∫ÉÁêÜËÆ∫ÔºàNeural Collapse, NCÔºâ‰∏éËÆ§Áü•ÁßëÂ≠¶ÁªìÂêàÔºåÂú®DomainNetÊï∞ÊçÆÈõÜ‰∏äÂáÜÁ°ÆÁéáÊèêÂçáËá≥60.13%Ôºà+7.6% vs SOTAÔºâ„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ2Ôºö** ËÆæËÆ°Á≤óÂà∞ÁªÜÊ†°ÂáÜÂô®ÔºàCoarse-to-Fine Calibrator, C2FÔºâÂíåÂèåÁÇπÂõûÂΩíÊçüÂ§±ÔºàDual Dot-Regression Loss, DDRÔºâÔºåÂú®CDDBÊï∞ÊçÆÈõÜ‰∏äÈÅóÂøòÂ∫¶‰ªÖ-0.73%Ôºà‰ºò‰∫éÂü∫Á∫ø1.30%Ôºâ„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ3Ôºö** Êó†ÈúÄ‰ªª‰ΩïÊéíÁªÉÊï∞ÊçÆÔºåÂ≠òÂÇ®ÂºÄÈîÄÈôç‰ΩéËá≥1-50MBÔºàÊéíÁªÉÊñπÊ≥ïÈúÄ100MB-3GBÔºâ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   ‰∫∫Á±ªËÆ§Áü•Êñ∞Áâ©‰ΩìÊó∂ÔºåÂÖàÂΩ¢ÊàêÁ≤óÁ≤íÂ∫¶Ê¶ÇÂøµÔºàÂ¶Ç‚ÄúÂä®Áâ©‚ÄùÔºâÔºåÂÜçÁªÜÂåñÂà∞ÂÖ∑‰ΩìÁ±ªÂà´ÔºàÂ¶Ç‚ÄúÁå´‚ÄùÔºâ„ÄÇDualCPÊ®°ÊãüÊ≠§ËøáÁ®ãÔºåÈÄöËøá**ËØ≠‰πâÂàÜÁªÑ**Âíå**ÂéüÂûãÂØπÈΩê**ÈÅøÂÖçÁâπÂæÅÁ©∫Èó¥Á´û‰∫â„ÄÇ\\n> *   **ÊúâÊïàÊÄßÂéüÁêÜÔºö** Á•ûÁªèÂ¥©Ê∫ÉÁêÜËÆ∫‰øùËØÅÂéüÂûãÂú®Ë∂ÖÁêÉÈù¢‰∏äÁöÑÊúÄÂ§ßÂèØÂàÜÊÄßÔºåËÄåÂàÜÂ±ÇËÆæËÆ°Â¢ûÂº∫Áõ∏‰ººÁ±ªÂà´ÁöÑÂå∫ÂàÜÂ∫¶„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **ÂÖàÂâçÂ±ÄÈôêÔºö** ‰º†ÁªüÊñπÊ≥ïÔºàÂ¶ÇEWC„ÄÅLwFÔºâÂπ≥Ë°°Êñ∞ÊóßÂüüÂ≠¶‰π†ÂØºËá¥Èõ∂ÂíåÂçöÂºàÔºõÊéíÁªÉÊñπÊ≥ïÔºàÂ¶ÇiCaRLÔºâÂ≠òÂú®ÈöêÁßÅÈ£éÈô©„ÄÇ\\n> *   **Êú¨ÊñáÊîπËøõÔºö** \\n>     1.  **Ê¶ÇÂøµÂéüÂûãÁîüÊàêÂô®ÔºàConcept Prototype Generator, CPGÔºâ**ÔºöÂà©Áî®CLIPÊñáÊú¨ÁâπÂæÅÊûÑÂª∫ËØ≠‰πâÂºïÂØºÁöÑÂçïÁ∫ØÂΩ¢ETFÔºàEq. 5-7ÔºâÔºåÊúÄÂ§ßÂåñÂéüÂûãÈó¥Ë∑ù„ÄÇ\\n>     2.  **ÂèåÂ±ÇÊ¨°ÂàÜÁªÑ**ÔºöÂü∫‰∫éÁõ∏‰ººÂ∫¶Áü©ÈòµÔºàEq. 8ÔºâÂ∞ÜÁ±ªÂàÜ‰∏∫Áæ§ÁªÑÔºàÂ¶ÇÂä®Áâ©/ËΩ¶ËæÜÔºâÔºåÂàÜÂà´ÁîüÊàêÁ≤ó/ÁªÜÁ≤íÂ∫¶ÂéüÂûãÔºàEq. 10-11Ôºâ„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.  **ÁâπÂæÅÊèêÂèñ**ÔºöÂõæÂÉèÁâπÂæÅÈÄöËøáViT/CLIPÁºñÁ†ÅÂô®ÊèêÂèñÔºåÁ±ªÂêçÊñáÊú¨ÁâπÂæÅÈÄöËøáCLIPÁºñÁ†Å„ÄÇ\\n> 2.  **ÂéüÂûãÁîüÊàê**Ôºö\\n>     - ËÆ°ÁÆóÊñáÊú¨ÁâπÂæÅÁõ∏‰ººÂ∫¶Áü©Èòµ **S**ÔºàEq. 8ÔºâÔºåÂàÜÁªÑÂêéÁîüÊàêÁ≤óÁ≤íÂ∫¶ÂéüÂûã **E_C**ÔºàEq. 10ÔºâÂíåÁªÜÁ≤íÂ∫¶ÂéüÂûã **E_F**ÔºàEq. 11Ôºâ„ÄÇ\\n>     - ÂÖ≥ÈîÆÂÖ¨ÂºèÔºö$\\\\mathbf{E} = \\\\sqrt{\\\\frac{K}{K-1}} \\\\mathbf{Q}(\\\\mathbf{I}_K - \\\\frac{1}{K}\\\\mathbf{1}_K\\\\mathbf{1}_K^T)$ÔºàEq. 5Ôºâ„ÄÇ\\n> 3.  **Ê†°ÂáÜ‰∏éËÆ≠ÁªÉ**ÔºöC2FÊ®°ÂùóÈÄöËøáDDRÊçüÂ§±ÔºàEq. 12ÔºâÂØπÈΩêÂõæÂÉèÁâπÂæÅ‰∏éÂéüÂûãÔºåË∂ÖÂèÇÊï∞Œ±=0.5Âπ≥Ë°°Á≤ó/ÁªÜÁ≤íÂ∫¶‰ºòÂåñ„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   Â¶ÇÂõæ2ÊâÄÁ§∫ÔºåÂØπDomainNet‰∏≠ÁöÑ‚ÄúÁå´/Áãó‚ÄùÔºàÂä®Áâ©ÁªÑÔºâÂíå‚ÄúËá™Ë°åËΩ¶/Â∑¥Â£´‚ÄùÔºàËΩ¶ËæÜÁªÑÔºâÔºåDualCPÂÖàÂú®Á≤óÁ≤íÂ∫¶Âå∫ÂàÜÁªÑÈó¥ÁâπÂæÅÔºåÂÜçÂú®ÁªÜÁ≤íÂ∫¶Âå∫ÂàÜÁªÑÂÜÖÁõ∏‰ººÁ±ª„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   ÊéíÁªÉÊñπÊ≥ïÔºöDyTox„ÄÅiCaRL„ÄÅLUCIRÔºõÊó†ÊéíÁªÉÊñπÊ≥ïÔºöEWC„ÄÅLwF„ÄÅL2P„ÄÅS-Prompts„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®Âπ≥ÂùáÂáÜÁ°ÆÁéáÔºà$A_T$Ôºâ‰∏äÔºö** DualCPÂú®DomainNet‰∏äËææÂà∞60.13%ÔºåÊòæËëó‰ºò‰∫éS-PromptsÔºà50.62%ÔºâÂíåL2PÔºà40.15%ÔºâÔºåÊèêÂçáËææ9.51‰∏™ÁôæÂàÜÁÇπ„ÄÇÂú®CDDB‰∏ä‰ª•82.16%Ë∂ÖË∂äCODA-PÔºà70.54%Ôºâ„ÄÇ\\n> *   **Âú®ÈÅóÂøòÂ∫¶Ôºà$F_T$Ôºâ‰∏äÔºö** DualCPÂú®DomainNet‰∏ä‰ªÖ-1.96%Ôºå‰ºò‰∫éÊâÄÊúâÂü∫Á∫øÔºàÂ¶ÇS-PromptsÁöÑ-2.85%Ôºâ„ÄÇ\\n> *   **Âú®Â≠òÂÇ®ÊïàÁéá‰∏äÔºö** Áõ∏ÊØîDyToxÔºàÈúÄ50Ê†∑Êú¨/Á±ªÂ≠òÂÇ®ÔºâÔºåDualCPÂÆåÂÖ®Êó†ÊéíÁªÉÔºåÂÜÖÂ≠òÂç†Áî®Èôç‰Ωé99%‰ª•‰∏ä„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   ÂüüÂ¢ûÈáèÂ≠¶‰π† (Domain-Incremental Learning, DIL)\\n*   Êó†ÊéíÁªÉÂ≠¶‰π† (Rehearsal-Free Learning, N/A)\\n*   ÂèåÂ±ÇÊ¨°Ê¶ÇÂøµÂéüÂûã (Dual-level Concept Prototypes, DualCP)\\n*   Á•ûÁªèÂ¥©Ê∫É (Neural Collapse, NC)\\n*   ÂçïÁ∫ØÂΩ¢Á≠âËßíÁ¥ßÊ°ÜÊû∂ (Simplex Equiangular Tight Frame, ETF)\\n*   Á≤óÂà∞ÁªÜÊ†°ÂáÜÂô® (Coarse-to-Fine Calibrator, C2F)\\n*   Ëá™Âä®È©æÈ©∂ËßÜËßâ (Autonomous Vehicle Vision, N/A)\\n*   ÁâπÂæÅÁ©∫Èó¥ÂØπÈΩê (Feature Space Alignment, N/A)\"\n}\n```"
}