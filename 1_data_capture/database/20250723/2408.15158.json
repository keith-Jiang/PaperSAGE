{
    "source": "Semantic Scholar",
    "arxiv_id": "2408.15158",
    "link": "https://arxiv.org/abs/2408.15158",
    "pdf_link": "https://arxiv.org/pdf/2408.15158.pdf",
    "title": "Delay as Payoff in MAB",
    "authors": [
        "Ofir Schlisselberg",
        "Ido Cohen",
        "Tal Lancewicki",
        "Yishay Mansour"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2024-08-27",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 3,
    "influential_citation_count": 1,
    "institutions": [
        "Tel Aviv University",
        "Google Research"
    ],
    "paper_content": "# Delay as Payoff in MAB\n\nOfir Schlisselberg\\*1, Ido Cohen\\*1, Tal Lancewicki1, Yishay Mansour1,2\n\n1Tel Aviv University 2Google Research {ofirs4, idoc, lancewicki}@mail.tau.ac.il, mansour.yishay $@$ gmail.com\n\n# Abstract\n\nIn this paper, we investigate a variant of the classical stochastic Multi-armed Bandit (MAB) problem, where the payoff received by an agent (either cost or reward) is both delayed, and directly corresponds to the magnitude of the delay. This setting models faithfully many real world scenarios such as the time it takes for a data packet to traverse a network given a choice of route (where delay serves as the agent‚Äôs cost); or a user‚Äôs time spent on a web page given a choice of content (where delay serves as the agent‚Äôs reward). Our main contributions are tight upper and lower bounds for both the cost and reward settings. For the case that delays serve as costs, which we are the first to consider, we prove optimal regret that scales $\\begin{array} { r } { \\sum _ { i : \\Delta _ { i } > 0 } \\frac { \\log T } { \\Delta _ { i } } + d ^ { * } } \\end{array}$ , where $T$ is the maximal number of steps, $\\Delta _ { i }$ are the sub-optimality gaps and $d ^ { * }$ is the minimal expected delay amongst arms. For the case that delays serves as rewards, we show optimal regret of i:‚àÜi>0 ‚àÜ log T + d¬Ø, where $\\bar { d }$ is the second maximal expec ed delay. These improve over the regret in the general delay-dependent payoff setting, which scales as i:‚àÜi>0 lo‚àÜg T + D, where D is the maximum possible delay. Our regret bounds highlight the difference between the cost and reward scenarios, showing that the improvement in the cost scenario is more significant than for the reward. Finally, we accompany our theoretical results with an empirical evaluation.\n\n# 1 Introduction\n\nClassical stochastic Multi-armed Bandit (MAB) is a well studied theoretical framework for sequential decision making, where at every step an agent chooses an action and immediately receives some payoff, be it reward or cost. A natural generalization of this framework considers the situation where the payoff is only received after a certain delay. This is known as the stochastic MAB problem with randomized delays (Joulani, Gyorgy, and Szepesva¬¥ri 2013), and was extensively researched in previous work under various variants (Vernade, Cappe¬¥, and Perchet 2017; Pike-Burke et al. 2018; Zhou, Xu, and Blanchet 2019; Manegueu et al. 2020; Vernade et al. 2020; Wu and Wager 2022; Howson, PikeBurke, and Filippi 2023; Shi, Wang, and Wu 2023). In all these works the delay was considered reward-independent, namely, the reward and delay are sampled from independent distributions (more on this in the following sections). Later, Lancewicki et al. (2021) introduced reward-dependent delay, where the delay and reward are sampled from a joint distribution. This model is more challenging as it introduces selection bias into the observed payoffs. Tang, Wang, and Zheng (2024) consider a special case of this they call strongly-dependent, where for all intents and purposes the delay is exactly the reward that we are trying to maximize. We second their motivation to study this case, and additionally generalize this to delay as cost that we are trying to minimize. This motivation is twofold. First, it models well many real world scenarios. Second, from a performance perspective, it offers a significant gain in the regret. We show that the delay as payoff scenario is actually ‚Äúsimpler‚Äù than the general payoff-dependent setting by providing a tighter upper bound compared to (Tang, Wang, and Zheng 2024), and a significantly better bound for the cost setting.\n\nTo motivate the cost scenario, consider a communication network where we route packets from node $a$ to node $b$ , and would like to do it in the fastest way possible. We can model this as a stochastic MAB, where every route $a  b$ is an arm (action) and the time it takes the packet to arrive (formally known as Round Trip Time (RTT), see Postel (1981)) is our payoff. For routing we want to minimize the $R T T$ and so we call the payoff ‚Äúcost‚Äù. To motivate the reward scenario, consider a web page with some dynamic content. We wish to capture the viewer‚Äôs attention for as long as possible, by choosing the content wisely. A common metric used in advertising is Average Time on Page (ATP), used, for example, by Google Analytics. In this case we want to maximize the $A T P$ and so we call the payoff ‚Äúreward‚Äù. In both scenarios the payoff (RTT or $A T P$ ) is the time elapsed from choosing an action until its payoff is final, hence it can be modeled as delay.\n\nAs far as performance, an immediate observation is that while the agent is waiting for an action‚Äôs final payoff, it gains partial knowledge about its payoff as time progresses. More specifically, if at time $t _ { 1 }$ an arm is played and by time $t _ { 2 }$ the payoff has not been revealed, we can learn that the delay (hence the payoff) is at least $t _ { 2 } - t _ { 1 }$ . This is a crucial observation that we use. Taking advantage of this knowledge, we can improve the regret bounds. Notice that this knowledge is one-sided in the sense that every time-step that passes provides an improved lower bound on the delay, but the same cannot be said for an upper bound. This is a challenging limitation that is the key difference between cost and reward, and explains how a better regret can be achieved for cost. (This issue is further discussed in Section 1.2.)\n\n# 1.1 Our Contributions\n\nWe study both reward and cost as payoff in the special case of payoff-dependent delay where delay serves as payoff. This setting presents an opportunity to use the partial knowledge accumulated while waiting for the payoff, to achieve better regret bounds. In order to conform with the literature, we normalize the payoff to be in $[ 0 , 1 ]$ , by setting it to the actual delay divided by the maximum delay. This has no implications on the analysis, and is aimed to be inline with the existing regret bounds. Our main contributions are the following:\n\n1. In the case of cost, we offer tight lower and upper bounds that scale as i:‚àÜ >0 lo‚àÜg T $\\begin{array} { r } { \\sum _ { i : \\Delta _ { i } > 0 } \\frac { \\log T } { \\Delta _ { i } } + \\operatorname* { m i n } \\{ d ^ { * } , D \\Delta _ { m a x } ^ { - } \\} } \\end{array}$ $d ^ { * }$ is the minimal expected delay.   \n2. In the case of rewards, we offer tight lower and upper bounds that scale as i:‚àÜ >0 lo‚àÜg T $\\begin{array} { r } { \\sum _ { i : \\Delta _ { i } > 0 } \\frac { \\log T } { \\Delta _ { i } } + \\operatorname* { m i n } \\{ \\bar { d } , D \\Delta _ { m a x } \\} } \\end{array}$ where $\\bar { d }$ is the second maximal expected delay. Note that the cost regret bound can be significantly smaller than the reward regret bound, on the same problem instance.   \n3. We complement our theoretical results in an experimental evaluation.\n\nOur main results, along with a concise comparison to previous work, are presented in Table 1. The two bounds provided, improve both on the general delay-dependent payoff setting which scales as i:‚àÜi>0 ‚àÜ log  +D (Lancewicki et al. 2021)1 and i:‚àÜ > 0 lo‚àÜg i +D Pi:‚àÜi>0 ‚àÜi by (Tang, Wang, and Zheng 2024). Note that $D \\Delta _ { m a x }$ is already an improvement of factor $K$ , but more significantly if $\\bar { d } \\dot { \\ll } D \\Delta _ { m a x }$ our bound is substantially lower. In the cost case this becomes even more clear as $d ^ { * }$ is potentially much smaller than $\\bar { d }$ .\n\n# 1.2 Cost vs Reward ‚Äì Intuition\n\nThe goal of a player in a MAB environment can be either to maximize his total payoff, in which case the payoff is called the ‚Äúreward‚Äù, or to minimize his total payoff, then the payoff is called the ‚Äúcost‚Äù. Normally when considering a MAB setting without delay, the choice of using cost or reward is interchangeable by simply changing the sign of the payoff, and most algorithms would be oblivious to the change. We argue that in the case of delayed payoff, where the payoff is the delay, cost and reward are very different. Specifically, when minimizing cost we can make better use of partial knowledge that we gain while waiting for payoff feedback.\n\nWe provide here some informal intuition for this, and we will make it more formal by providing lower bounds for both cases in Sections 4.3 and 5.3. Consider a scenario where we have $K$ arms with constant delays (and thus, payoff) sorted from low to high $\\{ d _ { i } \\} _ { i = 1 } ^ { K }$ . If we maximize reward the best arm is $i _ { K }$ with delay $d _ { K }$ . No matter how we play, arm $i _ { K - 1 }$ and arm $i _ { K }$ are indistinguishable until after $d _ { K - 1 }$ time steps, simply because no feedback is received from either arm before $d _ { K - 1 }$ . So the number of times we play sub-optimal arms depends on the second highest delay. In comparison, when minimizing cost, the best arm is $i _ { 1 }$ with delay $d _ { 1 }$ . After $d _ { 1 }$ time steps we can already start getting some information about the cost of $i _ { 1 }$ , and so we can hope to stop playing sub-optimal arms as early as $O ( d _ { 1 } )$ .\n\nPaper organization The rest of the paper is organized as follows. In Section 2 we discuss related work and in Section 3 we formally present our settings. In Section 4 we present our main algorithm and analysis for the delay as cost setting. In Section 5 we present our algorithm and results for the delay as reward setting. In Section 6 we present empirical evaluation of our algorithms compared to previous related works. Section 7 is a discussion. Most of the proofs are deferred to the the full version of the paper (Schlisselberg et al. 2024).\n\n# 2 Related Work\n\nThe delayed payoff in MAB has recently gained significant attention. Most previous works have been devoted to payoff-independent delays, often treating them as some unknown distribution. This line of work started with (Dudik et al. 2011) who introduced a constant delay, and offered a regret bound with linear dependence on the delay. Joulani, Gyorgy, and Szepesva¬¥ri (2013) extended this to stochastic, yet bounded, delay. Later variations include Zhou, Xu, and Blanchet (2019), who made a distinction between armdependent and arm-independent delay. And Pike-Burke et al. (2018), who consider an aggregated rewards model.\n\nDelayed payoff was also studied from an adversarial perspective, where both the delay and rewards are adversarial. This includes works such as (Cesa-Bianchi, Gentile, and Mansour 2019; Thune, Cesa-Bianchi, and Seldin 2019; Bistritz et al. 2019; Gyorgy and Joulani 2021). Masoudian, Zimmert, and Seldin (2022, 2023) presented a ‚Äúbest-of-bothworlds‚Äù algorithm for the delayed setting, which is a modification of Zimmert and Seldin (2020). Interestingly, in the adversarial setting, if the delay is adversarial and arm dependant as in (Van Der Hoeven and Cesa-Bianchi 2022), the adversary can correlate the payoff and the delay, thus the payoff also depends on the delay. While this resembles the payoff-dependant setting, the resulting regret bounds are very different. In particular, the delay has a multiplicative effect on the regret, while in the stochastic case we only suffer an additive term.\n\nOnly few works considered regret in the rewarddependent setting. Lancewicki et al. (2021) considers the case where the delay and reward are sampled from a joint distribution. In their work, there is no assumption on the delay distribution, in particular it may be unbounded. However the reward is still bounded in $[ 0 , 1 ]$ . Note that in our special case, where the payoff directly corresponds to the magnitude of the delay, this bounded payoff implies that the delay is also bounded. Their regret bound has an additive term that scales as the $( 1 - \\Delta _ { \\mathrm { m i n } } / 4 )$ -quantile of the delay distribution. In general, this can easily be as large as the maximal delay $D$ . For instance, when there is an arm with Bernoulli payoffs with $\\mu ( i ) > 1 / 4$ . Lancewicki et al. (2021) have also considered the case where the delay and reward are independent, which falls outside the scope of the reward-dependent case we consider here. Later, Tang, Wang, and Zheng (2024), consider the setting where the delay equals the reward. They consider general distributions, and their bound scales with a complex quantity dependent on these distributions. In the case where the distribution is bounded by $D$ , they establish a regret bound with an additive term of $D \\sum _ { i } \\bar { \\Delta _ { i } }$ . For the same setting we show an additive regret of $\\bar { d }$ , which we can also improve to $\\operatorname* { m i n } ( \\bar { d } , D \\operatorname* { m a x } _ { i } \\Delta _ { i } )$ .\n\nTable 1: Pseudo regret comparison for works on delay-dependent payoff. $T$ is the total number of steps, $\\Delta _ { i }$ is the sub-optimality gap of arm $\\mathbf { \\chi } _ { i }$ and $\\Delta _ { m a x } = \\operatorname* { m a x } _ { i } \\Delta _ { i }$ , $D$ is the maximum possible delay, $\\bar { d }$ is the second maximal expected delay (across arms), and $d ^ { * }$ is minimal expected delay. Bounds shown hide logarithmic factors that are independent of $\\mathrm { \\Delta T }$ .   \n\n<html><body><table><tr><td></td><td>Reward</td><td>Cost</td></tr><tr><td>(Lancewicki et al. 2021)</td><td>‚àë‚ñ≥>0 logT+D</td><td>Œ©i‚ñ≥i>0 logT+D</td></tr><tr><td>(Tang, Wang,and Zheng 2024)</td><td>Œ©‚ñ≥>0 +D‚àë‚ñ≥‚ñ≥</td><td>N/A</td></tr><tr><td>This work</td><td>lgT + min{d,D‚ñ≥max} ‚ñ≥ ¬£i‚ñ≥>0</td><td>¬£i‚ñ≥>0 logT+ min{d*,D‚ñ≥max} ‚ñ≥</td></tr></table></body></html>\n\n# 3 Problem Setup\n\nOur delay-as-payoff model is as follows. There is a set $[ K ]$ of $K$ arms. Each arm $i \\in [ K ]$ has a distribution $\\mathcal { D } _ { i }$ with support $[ D ] \\cup \\{ 0 \\}$ , where $D$ is the maximum delay. In each step $t = 1 , 2 , . . . , T$ , the agent chooses arm $i _ { t } \\in [ K ]$ , and incurs a delay $d _ { t } \\sim \\mathcal { D } _ { i _ { t } }$ . The agent observes the payoff of $i _ { t }$ at time $t + d _ { t }$ . The payoff is $d _ { t } / D$ , which we denote by $\\boldsymbol { r } _ { t }$ for rewards, or $c _ { t }$ for cost. Thus, the average payoff is $E _ { X \\sim \\mathcal { D } _ { i } } [ X / D ]$ denoted by $\\mu ( i )$ . Until step $t + d _ { t }$ , we refer to the payoff of arm $i _ { t }$ as missing, since we do not know its actual delay, and thus payoff, yet. At step $t + d _ { t }$ , $d _ { t }$ is revealed, and thus the agent observes the payoff. The interaction protocol is in Algorithm 1.\n\n# Algorithm 1 Protocol1\n\n<html><body><table><tr><td>fort‚àà[T]do</td></tr><tr><td></td></tr><tr><td>Agent picks an action it ‚àà[K]</td></tr><tr><td>Environment samples dt ~ Dit</td></tr><tr><td>Agent observes feedback {ds :t = s+ds}</td></tr></table></body></html>\n\nThe performance of the agent is measured by the expected pseudo regret, which is the difference between the algorithm‚Äôs cumulative expected payoff and the best expected payoff of any fixed arm. In the case of reward this will be:\n\n$$\n\\mathcal { R } _ { T } = \\mathop { \\operatorname* { m a x } } _ { i \\in [ K ] } T \\mu ( i ) - \\mathbb { E } \\left[ \\sum _ { t = 1 } ^ { T } r _ { t } \\right] = \\mathbb { E } \\left[ \\sum _ { t = 1 } ^ { T } \\Delta _ { i _ { t } } \\right]\n$$\n\nAnd in the case of cost:\n\n$$\n\\mathcal { R } _ { T } = \\mathbb { E } \\left[ \\sum _ { t = 1 } ^ { T } c _ { t } \\right] - \\operatorname* { m i n } _ { i \\in [ K ] } T \\mu ( i ) = \\mathbb { E } \\left[ \\sum _ { t = 1 } ^ { T } \\Delta _ { i _ { t } } \\right]\n$$\n\nwhere $\\Delta _ { i }$ is the sub-optimality gap of arm $i$ , i.e., $\\Delta _ { i } \\ =$ $| \\mu ( i ) - \\mu ^ { * } |$ and $\\mu ^ { * } \\ = \\ \\mathrm { m a x } _ { i \\in [ K ] } \\mu ( i )$ , for rewards, and $\\begin{array} { r } { \\mu ^ { * } = \\operatorname* { m i n } _ { i \\in [ K ] } \\mu ( i ) } \\end{array}$ , for cost. Respectively we define $i ^ { * } =$ $i \\in [ K ]$ s.t. $\\dot { \\mu ( i ) } = \\mu ^ { * }$ . Which, without loss of generality, we assume to be single. For readability, we make use of the following additional notations; $d ( i ) \\stackrel { \\cdot } { = } D \\mu ( i )$ is the mean delay of arm $i \\in [ K ]$ and $\\Delta _ { m a x } = \\mathrm { m a x } _ { i \\in [ K ] } \\Delta _ { i }$ .\n\n# 4 Delay As Cost\n\nIn this section, we consider the case where the cost is proportional to the agent‚Äôs delay. We introduce our main algorithm, Bounded Doubling Successive Elimination (BDSE, Algorithm 3), and its associated subroutine, Cost Successive Elimination (CSE, Algorithm 2). CSE builds on the well-known Successive Elimination (SE) algorithm (EvenDar, Mannor, and Mansour 2006), and as discussed in Section 4.1, it introduces an improved lower confidence bound (LCB). This LCB leverages not only the observed payoff but also takes into account the number of missing observations and their current duration. As we discuss later in this section, a similar improvement cannot be obtained for an upper confidence bound. Instead, BDSE employs a doubling scheme that upper bounds $\\mu ^ { * }$ . This combination is a key component that enables us to achieve our optimal bounds. In the following subsections, we expand on these algorithms and their regret guarantees.\n\n# 4.1 CSE Algorithm\n\nMuch like standard SE, CSE maintains a set of active arms, where initially all arms are active. The algorithm works in rounds, where in each round each active arm is selected once. Unlike standard SE, which eliminates an arm only when there is confidence that it is suboptimal, CSE also eliminates an arm when there is confidence that it is worse than a specific threshold parameter $B$ . In our following definitions we distinguish between the three following groups (Note that they are not mutually exclusive):\n\n1. $M _ { t } ( i )$ are the time steps with chosen arm $\\mathbf { \\chi } _ { i }$ that have not returned feedback by time $t$ . Formally, $M _ { t } ( i ) { \\bf \\omega } =$ $\\{ s \\in [ t ] | i _ { s } = i \\wedge s + d _ { s } \\geq t \\} \\rangle$ ). We denote the size of this group $m _ { t } ( i ) = | M _ { t } ( i ) |$ .\n\n2. $\\mathcal { O } _ { t } ( i )$ are time steps with chosen arm $i$ that have returned feedback by time $t$ . Formally, $\\begin{array} { r l } { \\mathcal { O } _ { t } ( i ) } & { { } = } \\end{array}$ $\\{ s \\in [ t ] | i _ { s } = i \\wedge s + \\bar { d } _ { s } < t \\}$ . 3. $F _ { t } ( i )$ are the time steps with chosen arm $i$ that are at least $D$ time steps ago, hence their feedback must have returned. Formally, $\\bar { F } _ { t } ( i ) = \\{ s \\in [ t ] | s \\leq t - D \\}$ 4. Additionally, $n _ { t } ( i ) = | \\{ i _ { s } = i | 1 \\le s \\le t \\} |$ is the number of all plays of arm $i \\in [ K ]$ before step $t \\in [ T ]$ .\n\nOur lower-confidence-bound comprises three terms, $L C B _ { t } ( i ) = \\operatorname* { m a x } \\{ L _ { t } ^ { 1 } ( i ) , L _ { t } ^ { 2 } ( i ) , L _ { t } ^ { 3 } ( i ) \\}$ ; each bounds the expected cost with high probability:\n\n1. $L _ { t } ^ { 1 } ( i )$ incorporates both observed and unobserved samples, optimistically assuming that the payoff of unobserved samples will be received in the next round. Formally,\n\n$$\nL _ { t } ^ { 1 } = \\hat { \\mu } _ { t } ^ { - } ( i ) - \\sqrt { \\frac { 2 \\log T } { n _ { t } ( i ) } } ,\n$$\n\nwhere $\\begin{array} { r } { \\hat { \\mu } _ { t } ^ { - } ( i ) = \\frac { 1 } { n _ { t } ( i ) } ( \\sum _ { s \\in M _ { t } ( i ) } \\frac { t - s } { D } + \\sum _ { s \\in \\mathcal { O } _ { t } ( i ) } c _ { s } ) . } \\end{array}$\n\n2. $L _ { t } ^ { 2 } ( i )$ uses only observed samples that were played up to time $t - D$ :\n\n$$\nL _ { t } ^ { 2 } ( i ) = \\hat { \\mu } _ { t } ^ { F } ( i ) - \\sqrt { \\frac { 2 \\log T } { | F _ { t } ( i ) | \\vee 1 } }\n$$\n\nwhere $\\begin{array} { r } { \\hat { \\mu } _ { t } ^ { F } ( i ) = \\frac { 1 } { | F _ { t } ( i ) | \\vee 1 } ( \\sum _ { s \\in F _ { t } ( i ) } c _ { s } ) } \\end{array}$ is the empirical average of those samples ( $\\vee$ indicates max). We take maximum in the denominator for the case that some arm was not played by $t - D$ , this can occur until $t = D + K$ .\n\n3. $L _ { t } ^ { 3 } ( i )$ directly leverages the fact the cost corresponds to the magnitude of delay. In particular, as we establish in Lemma 4.1, the number of missing samples can‚Äôt be much larger than a factor of the expected delay. We define,\n\n$$\nL _ { t } ^ { 3 } ( i ) = \\frac { | S _ { t } | } { D } \\bigg ( \\frac { m _ { t } ( i ) } { 2 } - 8 \\log T - 1 \\bigg )\n$$\n\nwhere $S _ { t }$ is the set of active arms at time $t$ . With the use of Lemma 4.1, $L _ { t } ^ { 3 } ( i )$ serves as a valid lower-confidence bound for $\\mu ( i )$ .\n\nFor the elimination step we require an upper confidence bound. We use a similar bound as in $L _ { t } ^ { 2 }$ :\n\n$$\nU C B _ { t } ( i ) = \\hat { \\mu } _ { t } ^ { F } ( i ) + \\sqrt { \\frac { 2 \\log T } { | F _ { t } ( i ) | \\vee 1 } }\n$$\n\nThe CSE algorithm is formally described in Algorithm 2 and as a full pseudo code in Algorithm 6 in the the full version of the paper (Schlisselberg et al. 2024).\n\nNote that if the delays were deterministic, then we would have $m _ { t } ( i ) \\leq d ( i ) / R$ , for every arm $i$ . The following lemma handles the case that the delays are stochastic with expectation $d ( i )$ .\n\n# Algorithm 2 Cost Successive Elimination (CSE)\n\nInput: number of rounds $T$ , number of arms $K$ , maxi  \nmum delay $D$ , Elimination Threshold $B$ .   \nInitialization: $t \\gets 1$ , $S \\gets [ K ]$   \nOutput : Status (either Success or Fail) and $t$ number   \nof time steps performed.   \nwhile $t < T$ do Play each arm $i \\in S$ Observe any incoming feedback Set $t \\gets t + | S |$ for $i \\in S$ do $L C B _ { t } ( i ) \\gets \\operatorname* { m a x } \\{ L _ { t } ^ { 1 } ( i ) , L _ { t } ^ { 2 } ( i ) , L _ { t } ^ { 3 } ( i ) \\}$ as defined in Equations (1) to (3) Update $U C B _ { t } ( \\bar { i } )$ as defined in Equation (4) $D$ Elimination step Remove from $S$ any arm $i$ if there exists $j$ such that min $\\{ U C B _ { t } ( j ) , B \\} < L C B _ { t } ( i )$ if $S = \\emptyset$ then Return (Fail,t)   \nReturn (Success,t)\n\nLemma 4.1 For every step $t ,$ , if the last $\\operatorname* { m i n } \\left\\{ D , t \\right\\}$ steps was played with a round robin of a set of size at least $R$ :\n\n$$\nP r \\bigg [ m _ { t } ( i ) \\leq \\frac { 2 d ( i ) } { R } + 1 6 \\log T + 2 \\bigg ] \\geq 1 - \\frac { 1 } { T ^ { 2 } }\n$$\n\nNote that using the missing plays to upper bound the mean delay results in a significantly weaker bound, and thus unhelpful. With that in hand, and standard concentration bounds, we can define an event $G$ that happens with high probability.\n\nDefinition 4.2 Assume that the actions were played in a round robin manner. Denote $R _ { t }$ to be minimum size of the round robin by time $t \\in [ T ]$ . Let $G$ be the event that for every $t \\in [ T ]$ and $i \\in [ K ]$ :\n\n$$\n\\begin{array} { c } { { m _ { t } ( i ) \\leq \\displaystyle \\frac { 2 d ( i ) } { R _ { t } } + 1 6 \\log T + 2 } } \\\\ { { | \\mu ( i ) - \\hat { \\mu } _ { t } ( i ) | \\leq \\displaystyle \\sqrt { \\frac { 2 \\log T } { n _ { t } ( i ) } } } } \\end{array}\n$$\n\nwhere, $\\begin{array} { r } { \\hat { \\mu } _ { t } ( i ) ~ = ~ \\frac { 1 } { n _ { t } ( i ) } \\sum _ { s \\in \\{ 1 \\leq s \\leq t | i _ { s } = i \\} } r _ { s } } \\end{array}$ is the empirical average of payoff of time steps with chosen arm $i$ . Note that due to missing plays, this is likely unknown to the algorithm at time $t$ .\n\nWe show that $G$ holds with high probability.\n\nLemma 4.3 The event $G$ holds with probability $1 - 3 / T ^ { 2 }$ .\n\nAs previously mentioned, CSE adopts a less conservative elimination rule than standard SE, as it also eliminates arms that perform worse than a specified threshold $B$ . Consequently, it might eliminate all arms, in which case it would return a Fail. For this reason we have a main program BDSE that call CSE with the threshold $B$ . When CSE return a Fail back to BDSE, then BDSE doubles the threshold $B$ and calls CSE with the new threshold.\n\nThe following theorem shows that CSE will not eliminate the optimal arm, if $B \\geq \\mu ^ { * }$ .\n\nLemma 4.4 (Safe Elimination) Assuming $G$ holds and $B \\geq \\mu ^ { * }$ , the procedure CSE will not fail and $i ^ { * }$ will not be eliminated.\n\nThe following theorem bounds the regret suffered in one call to the procedure CSE.\n\nTheorem 4.5 The regret of CSE (Algorithm 2) with elimination threshold $B$ is bounded by,\n\n$$\n\\sum _ { i : \\Delta > 0 } { \\frac { 1 2 9 \\log T } { \\Delta _ { i } } } + 8 D \\operatorname* { m i n } \\left\\{ B , \\Delta _ { m a x } \\right\\} \\log K\n$$\n\nThe first term in the regret scales as optimal instancedependent, non-delayed MAB. The second term scales with the magnitude of $B$ . Note that, by Lemma 4.4, $B$ will remain smaller than $2 \\mu ^ { * }$ and thus the second term is at most ${ \\tilde { O } } ( d ^ { * } )$ .\n\nProof sketch: For the sake of simplicity we provide the proof sketch only for the $B$ term in the min. Assume the good event $G$ holds. Let $S _ { t }$ be the set $S$ at time $t$ . Fix any sub-optimal arm $i \\in [ K ]$ , and let $\\tau _ { i }$ be the last elimination step which arm $i$ remained active.\n\nRecall that $L _ { t } ^ { 1 }$ is computed with an optimistic empirical average $\\hat { \\mu } _ { t } ^ { - } ( i )$ . That is, any missing sample is assumed to be observed in the next round. At worse, such missing sample eventually would return after $D$ steps and would have cost of 1. Thus, the difference between $\\bar { \\mu _ { t } ^ { - } } ( i )$ and the actual empirical mean $\\hat { \\mu } _ { t } ( i )$ is at most $m _ { t } ( i ) / n _ { t }$ . Since the good event $G$ holds, and the arm $i$ was not yet eliminated at time $\\tau _ { i }$ ,\n\n$$\n\\begin{array} { l } { \\displaystyle \\mu ( i ) \\leq L C B _ { \\tau _ { i } } ( i ) + 2 \\sqrt { \\frac { 2 \\log T } { n _ { \\tau _ { i } } ( i ) } } + \\frac { m _ { \\tau _ { i } } ( i ) } { n _ { \\tau _ { i } } ( i ) } } \\\\ { \\displaystyle \\quad \\leq B + 2 \\sqrt { \\frac { 2 \\log T } { n _ { \\tau _ { i } } ( i ) } } + \\frac { m _ { \\tau _ { i } } ( i ) } { n _ { \\tau _ { i } } ( i ) } } \\\\ { \\displaystyle \\quad \\approx B + 2 \\sqrt { \\frac { 2 \\log T } { n _ { \\tau _ { i } } ( i ) } } + \\frac { 2 D L _ { \\tau _ { i } } ^ { 3 } ( i ) } { n _ { \\tau _ { i } } ( i ) [ S _ { \\tau _ { i } } ] } } \\\\ { \\displaystyle \\quad \\leq B + 2 \\sqrt { \\frac { 2 \\log T } { n _ { \\tau _ { i } } ( i ) } } + \\frac { 2 D B } { n _ { \\tau _ { i } } ( i ) [ S _ { \\tau _ { i } } ] } } \\end{array}\n$$\n\nWe consider three cases: (i) $\\mu ^ { * } < B$ and $\\mu ( i ) < 2 B$ , (ii) $B \\leq \\mu ^ { * }$ and (iii) $2 B \\le \\mu ( i )$ .\n\ncase (i): By Lemma 4.4, we know that $i ^ { * }$ will not be eliminated (since $\\mu ^ { * } < B )$ . Since $i$ was not eliminated at time $\\tau _ { i }$ $\\phantom { } _ { i } , L _ { \\tau _ { i } } ^ { 2 } ( i ) \\leq { \\dot { U } } C B _ { \\tau _ { i } } ( i )$ . Using standard arguments this implies that $\\Delta _ { i } n _ { t _ { \\tau _ { i } } } ^ { F } ( i ) \\le { \\cal O } ( \\log ( T ) / \\Delta _ { i } )$ . Recall that $n _ { t _ { \\tau _ { i } } } ^ { F } ( i )$ is the number of times we played $i$ until time $\\tau _ { i } - D$ . In the last $D$ plays $i$ was played approximately $D / | S _ { \\tau _ { i } } |$ times due to the round-robin, which causes additional regret of $\\begin{array} { r } { \\Delta _ { i } \\frac { D } { | S _ { \\tau _ { i } } | } \\leq O \\big ( \\frac { D B } { | S _ { \\tau _ { i } } | } \\big ) } \\end{array}$ . This accumulates to a total regret of $\\begin{array} { r } { O \\big ( \\frac { \\log T } { \\Delta _ { i } } + \\frac { D B } { | S _ { \\tau _ { i } } | } \\big ) } \\end{array}$\n\ncase (ii): We use Equation (6) to show that $\\begin{array} { r l } { \\Delta _ { i } } & { { } \\leq } \\end{array}$ $\\begin{array} { r } { 2 \\sqrt { \\frac { 2 \\log T } { n _ { \\tau _ { i } } ( i ) } } + \\frac { 2 D B } { n _ { \\tau _ { i } } ( i ) | S _ { \\tau _ { i } } | } } \\end{array}$ . This implies that either $\\begin{array} { l l } { \\Delta _ { i } } & { \\leq } \\end{array}$ ¬∑\n\n# Algorithm 3 Bounded Doubling Successive Elimination\n\nInput: number of rounds $T$ , number of arms $K$ , maxi  \nmum delay $D$ .   \nInitialization: $B  1 / D$   \nwhile $t < T$ do Run $( \\mathrm { r e t } , \\tau ) \\gets \\mathrm { C S E } ( T - t , K , D , B )$ if ret=Fail then B 2B t t + œÑ\n\n$4 \\sqrt { \\frac { 2 \\log T } { n _ { \\tau _ { i } } ( i ) } }$ or $\\begin{array} { r } { \\begin{array} { l l l } { \\Delta _ { i } } & { \\leq } & { \\frac { 4 D B } { n _ { \\tau _ { i } } ( i ) \\vert S _ { \\tau _ { i } } \\vert } } \\end{array} } \\end{array}$ nœÑ 4(iD)BSœÑ . In the first case we get that the regret from arm $i$ is bounded by $O ( \\log ( T ) / \\Delta _ { i } )$ . Similarly, in the second case the regret is bounded by $O ( ( D B ) / \\vert S _ { \\tau _ { i } } \\vert )$ .\n\ncase (iii): The third case assumes that $B \\ \\leq \\ \\mu ( i ) / 2$ . By rearranging the terms of Equation (6), we have that $\\Delta _ { i } \\leq$ $\\begin{array} { r } { \\mu ( i ) \\mathop { \\leq } O \\biggl ( \\sqrt { \\frac { \\log T } { n _ { \\tau _ { i } } ( i ) } } + \\frac { D B } { n | S _ { \\tau _ { i } } | } \\biggr ) } \\end{array}$ . Similarly to case (ii), the total regret is bouœÑnided by $\\begin{array} { r } { O \\big ( \\frac { \\log T } { \\Delta _ { i } } + \\frac { D B } { | S _ { \\tau _ { i } } | } \\big ) } \\end{array}$\n\nNow we can sum over all sub-optimal arms $i$ and bound the regret by O i:‚àÜ>0 ‚àÜ $\\begin{array} { r } { O \\big ( \\sum _ { i : \\Delta > 0 } \\frac { \\log T } { \\Delta _ { i } } + \\bar { D B } \\log K \\big ) } \\end{array}$ log  + DB log K . Note that the regret when $G$ does not hold is in expectation only $O ( 1 )$ .\n\n# 4.2 Bounded Doubling Successive Elimination\n\nCSE (Algorithm 2) demands a parameters $B$ , which is not available for the agent. In this algorithm we estimate $B$ using the ‚Äùdoubling‚Äù technique.\n\nCorollary 4.6 Algorithm BDSE has a regret of at most,\n\n$$\n\\sum _ { i : \\Delta > 0 } \\frac { 1 2 9 \\log T \\log d ^ { * } } { \\Delta _ { i } } + 8 \\operatorname* { m i n } \\left\\{ d ^ { * } , D \\Delta _ { m a x } \\log d ^ { * } \\right\\} \\log K\n$$\n\nProof: From Lemma 4.4 we know that if $B \\geq \\mu ^ { * }$ then CSE will not fail, which means that the number of calls to CSE is at most $\\log d ^ { * }$ . Notice that on the $j$ ‚Äôs call of $B S E$ , $D B = 2 ^ { j }$ . The total regret will be at most\n\n$$\n\\begin{array} { r l } & { \\displaystyle \\sum _ { j = 0 } ^ { \\log d ^ { * } } \\left( \\sum _ { i : \\Delta > 0 } \\frac { 1 2 9 \\log T } { \\Delta _ { i } } + 8 \\cdot \\operatorname* { m i n } \\left\\{ 2 ^ { j } , D \\Delta _ { m a x } \\right\\} \\log K \\right) } \\\\ & { \\displaystyle = \\sum _ { i : \\Delta > 0 } \\frac { 1 2 9 \\log T \\log d ^ { * } } { \\Delta _ { i } } } \\\\ & { \\qquad + 8 \\operatorname* { m i n } \\left\\{ d ^ { * } , D \\Delta _ { m a x } \\log d ^ { * } \\right\\} \\log K } \\end{array}\n$$\n\n# 4.3 Lower Bound\n\nIn this section, we show two lower bounds for the cost setting. The first is a general lower bound which nearly matches the regret bound of our algorithm. And the second, a lower bound for classical SE algorithms. The main challenge is to understand the impact of $d ^ { * }$ on the regret. We focus on the second term of the upper bound, as the first term, $\\sum { _ { i : \\Delta > 0 } \\frac { \\log T } { \\Delta _ { i } } }$ lo‚àÜg  , is a well-known instance-dependent bound even when there are no delays (Bubeck and Cesa-Bianchi 2012).\n\nTheorem 4.7 In the cost scenario, for every choice of $d ^ { * } \\leq$ $D / 2$ , there is an instance for which any algorithm will have a regret of $\\Omega ( d ^ { * } )$\n\nProof: We consider two arms with deterministic delays, one is $d ^ { * }$ (and cost $\\mu ^ { * } = d ^ { * } / D \\leq 1 / 2 )$ and the other is $D$ (with cost $\\mu = 1 \\mathrm { \\cdot }$ ). We select at random which arm has delay $d ^ { * }$ and which $D$ . Until time $d ^ { * }$ both arms are indistinguishable, and hence the regret is $( 1 - \\mu ^ { * } ) d ^ { * }$ . Since $\\mu ^ { * } \\leq 1 / \\bar { 2 }$ we have a regret of at least $d ^ { * } / 2$ . T\n\nConservative SE algorithms: We show, for a natural class of SE algorithms, which are also conservative (w.h.p. do not eliminate the optimal action), a lower bound of $\\sqrt { D d ^ { * } }$ . Interestingly, this bound is also tight, as we show in the full version of the paper (Schlisselberg et al. 2024), a conservative SE algorithm which attains it.\n\nFor this impossibility result we use the following two problem instances. In the first problem instance we have the delay of arm 1 to be $\\sqrt { D d ^ { * } } / \\bar { 2 }$ w.p. $2 \\sqrt { d ^ { * } / D }$ and otherwise 0. For arm 2 the delay is deterministic $D$ . In the second problem instance we have the delay of arm 1 to be $D$ w.p. $2 \\sqrt { d ^ { * } / D }$ and otherwise 0. For arm 2 the delay is deterministic $\\sqrt { D d ^ { * } }$ . In the first instance the best arm is 1 while in the second it is arm 2. Until time $\\sqrt { D d ^ { * } }$ we cannot distinguish between the two instances, so a conservative SE algorithm will keep playing both arms in a round-robin manner, and have a regret of $\\Omega ( \\sqrt { D d ^ { * } } )$ in the first instance.\n\nIt is worth observing why our BDSE overcomes those two problem instances. Due to the doubling scheme, every time the number of missing plays reaches (roughly) the current threshold, both arms will be eliminated until the threshold surpasses $\\mu ^ { * }$ . In the first instance, this will happen after $d ^ { * }$ steps, at which point only the optimal arm will remain, and thus the regret is at most $d ^ { * } \\Delta = d ^ { * }$ . Similarly, in the second instance, the threshold will surpass $\\mu ^ { * }$ after $\\sqrt { D d ^ { * } }$ steps. The $\\Delta$ here is $\\sqrt { d ^ { * } / D }$ and so the regret is at most $d ^ { * }$ .\n\n# 5 Delay As Reward\n\nIn this section, we consider the case where the delay corresponds to the agent‚Äôs reward. Similarly to cost, we have a main program Bounded Halving Successive Elimination algorithm (BHSE, Algorithm 5), and an associated subroutine Reward Successive Elimination (RSE). Besides the transition from minimization to maximization, the main difference is that the missing feedbacks at time $t$ should be interpreted differently. In the following subsections, we include the details of these algorithms and their regret guarantees.\n\n# 5.1 RSE Algorithm\n\nAs in the cost scenario, we start with a Reward Successive Elimination algorithm. Since we consider rewards, we would like the threshold $B$ to decrease with time (rather than increase, as was done in the cost scenario). Eventually, RSE expects $B \\le \\mu ^ { * }$ to guarantee success. As in the CSE algorithm, we will eliminate arms based on suboptimality, in comparison to other arms, or when there is confidence that they are worse than the parameter $B$ .\n\n# Algorithm 4 Reward Successive Elimination\n\nInput: number of rounds $T$ , number of arms $K$ , maxi  \nmum delay $D$ , Elimination Threshold $B$ .   \nInitialization: $t \\gets 1$ , $S \\gets [ K ]$   \nOutput : Status (either Success or Fail) and $t$ number   \nof time steps performed.   \nwhile $t < T$ do Play each arm $i \\in S$ Observe incoming payoff from $\\{ s : s + d _ { s } = t \\}$ Set $t \\gets t + | S |$ for $i \\in S$ do $U C B _ { t } ( i ) \\gets \\operatorname* { m i n } \\{ U _ { t } ^ { 1 } ( i ) , U _ { t } ^ { 2 } ( i ) \\}$ as defined in Equations (7) and (8) Update $L C B _ { t } ( \\bar { i } )$ as defined in Equation (9) $D$ Elimination step Remove from $S$ any arm $i$ if there exists $j$ such that max $\\{ L C B _ { t } ( j ) , B \\} > U C B _ { t } ( i )$ if $S = \\emptyset$ then Return (Fail, t)   \nReturn (Success, t)\n\nOur upper-confidence-bound comprises only two terms, $U C B _ { t } ( i ) \\stackrel { \\cdot } { = } \\operatorname* { m i n } \\{ U _ { t } ^ { 1 } ( i ) , U _ { t } ^ { 2 } ( i ) \\}$ ; which are analogues to $L _ { 1 }$ and $L _ { 2 }$ in the cost case. Formally,\n\n$$\nU _ { t } ^ { 1 } = \\hat { \\mu } _ { t } ^ { + } ( i ) + \\sqrt { \\frac { 2 \\log T } { n _ { t } ( i ) } } ,\n$$\n\nwhere $\\begin{array} { r } { \\hat { \\mu } _ { t } ^ { + } ( i ) = \\frac { 1 } { n _ { t } ( i ) } ( \\sum _ { s \\in M _ { t } ( i ) } 1 + \\sum _ { s \\in \\mathcal { O } _ { t } ( i ) } r _ { s } ) } \\end{array}$ is an optimistic estimate of $\\mu ( i )$ . (Recall that $r _ { s } = d _ { s } / D$ .) Similarly, $U _ { 2 }$ as well as $L C B _ { t }$ are defined by,\n\n$$\nU _ { t } ^ { 2 } ( i ) = \\hat { \\mu } _ { t } ^ { F } ( i ) + \\sqrt { \\frac { 2 \\log T } { | F _ { t } ( i ) | \\vee 1 } } ,\n$$\n\nFor the LCB we have,\n\n$$\nL C B _ { t } ( i ) = \\hat { \\mu } _ { t } ^ { F } ( i ) - \\sqrt { \\frac { 2 \\log T } { | F _ { t } ( i ) | \\vee 1 } }\n$$\n\nwhere $\\begin{array} { r } { \\hat { \\mu } _ { t } ^ { F } ( i ) = \\frac { 1 } { | F _ { t } ( i ) | \\vee 1 } \\big ( \\sum _ { s \\in F _ { t } ( i ) } r _ { s } \\big ) } \\end{array}$\n\nWe use the same good event $G$ as defined in Definition 4.2 in the previous section which also holds here w.h.p.\n\nTheorem 5.1 (Safe Elimination) Assuming $G$ holds and $B \\leq \\mu ^ { * }$ , the procedure RSE will not return $F a \\dot { \\perp } \\bar { \\varDelta }$ and $i ^ { * }$ will not be eliminated.\n\nThe following theorem bounds RSE‚Äôs regret.\n\nTheorem 5.2 Assume $B \\geq { \\frac { \\mu ^ { * } } { 2 } } $ . The regret of RSE (Algorithm 4) with elimination threshold $B$ is bounded by,\n\n$$\n\\sum _ { i : \\Delta > 0 } \\frac { 2 8 9 \\log T } { \\Delta _ { i } } + 1 2 \\operatorname* { m i n } \\left\\{ \\bar { d } , D \\Delta _ { m a x } \\right\\} \\log K ,\n$$\n\nwhere $\\bar { d }$ is the second highest expected delay.\n\nThe assumption that $B \\geq \\mu ^ { * } / 2$ is satisfied under the main program BHSE (Algorithm 5) due to Theorem 5.1.\n\n![](images/575d641b8f3ef9bf10008c4aa6f12adcec66dde4e98fd0fbfb044b15313ace87.jpg)  \nFigure 1: This graph shows results of experiments on different algorithms (color) and different distributions (line style).\n\n# 5.2 Bounded Halving Successive Elimination\n\nSimilar to the main program in the cost case, BHSE estimates a lower bound for $\\mu ^ { * }$ . It starts with an over-estimation of $B = 1$ and this time halves it by 2 whenever RSE returns Fail.\n\nCorollary 5.3 Algorithm BHSE has regret of at most,   \n\n<html><body><table><tr><td>Algorithm 5 Bounded Halving Successive Elimination (BHSE)</td></tr><tr><td>Input:number of rounds T,number of arms K,maxi- mum delay D.</td></tr><tr><td>Initialization:B‚Üê1 whilet<Tdo</td></tr><tr><td>Run(ret,œÑ)=RSE(T-t,K,D,B)</td></tr><tr><td>ifret=Failthen</td></tr><tr><td>B‚ÜëB/2;t‚Üêt+T</td></tr></table></body></html>\n\n$$\n\\displaystyle \\left( \\sum _ { i : \\Delta > 0 } \\frac { 2 8 9 \\log T } { \\Delta _ { i } } + 1 2 \\operatorname* { m i n } \\left\\{ \\bar { d } , D \\Delta _ { m a x } \\right\\} \\log K \\right) \\log \\frac { 1 } { \\mu ^ { * } }\n$$\n\nProof: From Theorem 5.1 we know that if $\\textit { B } \\leq \\textit { \\mu } ^ { * }$ BSE will not fail, which means that the loop will run a maximum of $\\log ( 1 / \\mu ^ { * } )$ times. This also means that $B \\geq \\mu ^ { * } / 2$ , as needed. Therefore, the total regret will be $\\begin{array} { r l } {  { ( \\sum _ { i : \\Delta > 0 } \\frac { 2 8 9 \\log T } { \\Delta _ { i } } + 1 2 \\operatorname* { m i n } \\{ \\bar { d } , D \\Delta _ { m a x } \\} \\log K ) \\log \\frac { 1 } { \\mu ^ { * } } } } & { { } } \\end{array}$\n\nNote that without loss of generality we can assume that $\\mu ^ { * } \\geq 1 / T$ , since otherwise $\\Delta _ { i } \\ \\leq \\ 1 / T$ for all arms and the regret is trivially bounded by 1. Therefore the term $\\log ( 1 / \\mu ^ { * } )$ would be at most $\\log T$ .\n\nNotice that unlike Corollary 4.6, the regret bound depends on $\\bar { d }$ . On the one hand, this is better than the delay of the best arm (which has the maximal expected delay). On the other hand, this can be much larger than the regret in the cost scenario, which depends on the minimal expected delay.\n\n# 5.3 Lower Bound\n\nIn this section we show a lower bound for the reward setting, which nearly matches our regret bound.\n\nTheorem 5.4 In the reward scenario, for every choice of $\\bar { d } \\le D / 2$ , there is an instance for which any algorithm will have a regret of $\\Omega ( \\bar { d } )$\n\nProof: Consider the case of $K$ arms. We have one arm with constant delay $D$ , one arm with constant delay $\\bar { d } =$ $D / 2$ , and the remaining arms have delay 0 (and hence reward 0). We select at random the identities of the arms. This implies that for any sub-optimal arm $i$ we have $\\Delta _ { i } \\geq 1 / 2$ . Clearly, until time $\\bar { d }$ the best two arms are indistinguishable hence the regret is at least of order of $\\bar { d } \\operatorname* { m i n } _ { i } \\Delta _ { i } \\geq \\bar { d } / 2$ .\n\nConservative SE algorithms: Using similar arguments as in the cost case we can show here a lower bound of $\\sqrt { D \\bar { d } }$ .\n\n# 6 Experiments\n\nWe conducted synthetic experiments for both the cost and reward settings, using the algorithms in Table 1 as baselines. We show results on two representative distributions: Truncated Normal (bounded in $[ 0 , D ] )$ and Bernoulli. Due to space constraints, we defer the cost experiments to the the full version of the paper (Schlisselberg et al. 2024). All experiments use $\\scriptstyle { T = 1 5 0 , 0 0 0 }$ , $K { = } 3 0$ and $D { = } 5 0 0 0$ . For the truncated Normal we sample $K$ means and standard deviations (std), and adjust them to get a truncated version. Since our additive term in the regret is $\\operatorname* { m i n } \\{ \\bar { d } , D \\Delta _ { m a x } \\}$ , our contribution is mainly for instances where $d < D \\Delta _ { m a x }$ . Hence, we show the result on such instance, by using an exponential distribution to sample the means of the arms, creating sparsity in the higher regime. The stds are sampled uniformly in $[ 0 , D ]$ . For the Bernoulli distribution, we sample $K$ probabilities $p _ { i }$ uniformly in $[ 0 , 1 ]$ , so that arm $i$ gets 0 with probability $p _ { i }$ and $D$ with probability $1 - p _ { i }$ . Figure 1 shows the average cumulative regret over 10 runs. The shaded region is the std of these runs. BHSE (our algorithm) outperforms OPSE (from Lancewicki et al. (2021)) and CensoredUCB (from Tang, Wang, and Zheng (2024)) in both distributions. Bernoulli distribution is more challenging, resulting in higher regret and std.\n\n# 7 Discussion\n\nIn this paper, we explored a variant of the classical MAB problem, where the payoff is both delayed and directly corresponds to the magnitude of the delay. For the delay as reward setting we introduced tighter upper and lower regret bounds compared to those established in previous works. We are the first to generalize also to cost, highlighting the inherent difference between cost and reward in this setting.\n\nThere are several interesting future directions. First, as our motivation for the reward setting is the online advertising, it is a natural question to ask if we can expect similar results in the a delay as payoff contextual bandit setting or other variants of the MAB problem. Furthermore, it remains unclear whether our results can be generalized for more general delay distributions which are potentially unbounded (but have a bounded expectation). Finally, adopting this perspective in the adversarial setting, where delays serves as payoffs, is a challenging new problem.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáÁ†îÁ©∂‰∫Ü‰∏ÄÁßçÁªèÂÖ∏ÈöèÊú∫Â§öËáÇËÄÅËôéÊú∫ÔºàMABÔºâÈóÆÈ¢òÁöÑÂèò‰ΩìÔºåÂÖ∂‰∏≠‰ª£ÁêÜÊé•Êî∂ÁöÑÊî∂ÁõäÔºàÊàêÊú¨ÊàñÂ•ñÂä±ÔºâÊó¢ÊòØÂª∂ËøüÁöÑÔºåÂèàÁõ¥Êé•ÂØπÂ∫î‰∫éÂª∂ËøüÁöÑÂπÖÂ∫¶„ÄÇËøôÁßçËÆæÁΩÆÂèØ‰ª•ÂæàÂ•ΩÂú∞Âª∫Ê®°ËÆ∏Â§öÁé∞ÂÆûÂú∫ÊôØÔºå‰æãÂ¶ÇÊï∞ÊçÆÂåÖÂú®ÁΩëÁªú‰∏≠ÈÄâÊã©Ë∑ØÁî±ÁöÑ‰º†ËæìÊó∂Èó¥ÔºàÂª∂Ëøü‰Ωú‰∏∫‰ª£ÁêÜÁöÑÊàêÊú¨ÔºâÔºåÊàñËÄÖÁî®Êà∑Âú®ÁΩëÈ°µ‰∏äÈÄâÊã©ÂÜÖÂÆπÂêéÁöÑÂÅúÁïôÊó∂Èó¥ÔºàÂª∂Ëøü‰Ωú‰∏∫‰ª£ÁêÜÁöÑÂ•ñÂä±Ôºâ„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éÂÆÉÂÖãÊúç‰∫ÜÁé∞ÊúâÊñπÊ≥ï‰∏≠Âª∂Ëøü‰∏éÊî∂ÁõäÁã¨Á´ãÁöÑÂÅáËÆæÔºåÊõ¥Ë¥¥ËøëÂÆûÈôÖÂ∫îÁî®Âú∫ÊôØÔºåÂ¶ÇÁΩëÁªúË∑ØÁî±ÂíåÂú®Á∫øÂπøÂëä‰ºòÂåñ„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏§ÁßçÁÆóÊ≥ïÔºöBounded Doubling Successive Elimination (BDSE) Áî®‰∫éÂª∂Ëøü‰Ωú‰∏∫ÊàêÊú¨ÁöÑÊÉÖÂÜµÔºå‰ª•Âèä Bounded Halving Successive Elimination (BHSE) Áî®‰∫éÂª∂Ëøü‰Ωú‰∏∫Â•ñÂä±ÁöÑÊÉÖÂÜµ„ÄÇËøô‰∫õÁÆóÊ≥ïÈÄöËøáÂà©Áî®Á≠âÂæÖÊî∂ÁõäÊó∂ÁßØÁ¥ØÁöÑÈÉ®ÂàÜÁü•ËØÜÔºåÂÆûÁé∞‰∫ÜÊõ¥‰ºòÁöÑÈÅóÊÜæËæπÁïå„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **Ë¥°ÁåÆ1Ôºö** Âú®Âª∂Ëøü‰Ωú‰∏∫ÊàêÊú¨ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊèê‰æõ‰∫ÜÁ¥ßËá¥ÁöÑ‰∏ä‰∏ãÁïåÈÅóÊÜæËæπÁïåÔºåÂÖ∂ÂΩ¢Âºè‰∏∫ \\\\(\\\\sum_{i:\\\\Delta_i>0} \\\\frac{\\\\log T}{\\\\Delta_i} + d^*\\\\)ÔºåÂÖ∂‰∏≠ \\\\(d^*\\\\) ÊòØËáÇÁöÑÊúÄÂ∞èÈ¢ÑÊúüÂª∂Ëøü„ÄÇ\\n> *   **Ë¥°ÁåÆ2Ôºö** Âú®Âª∂Ëøü‰Ωú‰∏∫Â•ñÂä±ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊèê‰æõ‰∫ÜÁ¥ßËá¥ÁöÑ‰∏ä‰∏ãÁïåÈÅóÊÜæËæπÁïåÔºåÂÖ∂ÂΩ¢Âºè‰∏∫ \\\\(\\\\sum_{i:\\\\Delta_i>0} \\\\frac{\\\\log T}{\\\\Delta_i} + \\\\bar{d}\\\\)ÔºåÂÖ∂‰∏≠ \\\\(\\\\bar{d}\\\\) ÊòØÁ¨¨‰∫åÂ§ßÁöÑÈ¢ÑÊúüÂª∂Ëøü„ÄÇ\\n> *   **Ë¥°ÁåÆ3Ôºö** ÈÄöËøáÂÆûÈ™åÈ™åËØÅ‰∫ÜÁêÜËÆ∫ÁªìÊûúÔºåÂ±ïÁ§∫‰∫ÜÁÆóÊ≥ïÂú®ÂêàÊàêÊï∞ÊçÆ‰∏äÁöÑ‰ºòË∂äÊÄßËÉΩ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   ÁÆóÊ≥ïÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂà©Áî®Âª∂Ëøü‰Ωú‰∏∫Êî∂ÁõäÁöÑÁâπÊÄßÔºåÈÄöËøáËßÇÂØüÊú™ÂèçÈ¶àÁöÑÊ†∑Êú¨Êï∞ÈáèÂèäÂÖ∂ÊåÅÁª≠Êó∂Èó¥ÔºåÊûÑÂª∫Êõ¥Á¥ßËá¥ÁöÑÁΩÆ‰ø°ËæπÁïå„ÄÇÂú®ÊàêÊú¨ÊÉÖÂÜµ‰∏ãÔºåÁÆóÊ≥ïÂèØ‰ª•Êõ¥Êó©Âú∞ÊéíÈô§Ê¨°‰ºòËáÇÔºõÂú®Â•ñÂä±ÊÉÖÂÜµ‰∏ãÔºåÁÆóÊ≥ïÈÄöËøáÈÄêÊ≠•Èôç‰ΩéÈòàÂÄºÊù•‰ºòÂåñÈÄâÊã©„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** ÂÖàÂâçÁöÑÂ∑•‰ΩúÂÅáËÆæÂª∂Ëøü‰∏éÊî∂ÁõäÁã¨Á´ãÔºåÊàñËÄÖ‰ªÖËÄÉËôëÂª∂Ëøü‰Ωú‰∏∫Â•ñÂä±ÁöÑÊÉÖÂÜµ„ÄÇÊú¨ÊñáÈ¶ñÊ¨°Á†îÁ©∂‰∫ÜÂª∂Ëøü‰Ωú‰∏∫ÊàêÊú¨ÁöÑÊÉÖÂÜµÔºåÂπ∂ÊèêÂá∫‰∫ÜÈíàÂØπÊÄßÁöÑÁÆóÊ≥ï„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** Êú¨ÊñáÈÄöËøáÂºïÂÖ•Êñ∞ÁöÑÁΩÆ‰ø°ËæπÁïåÔºàÂ¶Ç \\\\(L_t^3(i)\\\\)ÔºâÂíåÂä®ÊÄÅË∞ÉÊï¥ÈòàÂÄºÔºàÂ¶ÇBDSE‰∏≠ÁöÑÂä†ÂÄçÁ≠ñÁï•ÔºâÔºåÊòæËëóÊèêÂçá‰∫ÜÈÅóÊÜæËæπÁïåÁöÑÁ¥ßËá¥ÊÄß„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> *   **Ê≠•È™§1Ôºö** ÂàùÂßãÂåñÊ¥ªÂä®ËáÇÈõÜÂêàÂíåÈòàÂÄº \\\\(B\\\\)„ÄÇ\\n> *   **Ê≠•È™§2Ôºö** Âú®ÊØè‰∏ÄËΩÆ‰∏≠Ôºå‰ª•ËΩÆËØ¢ÊñπÂºèÈÄâÊã©Ê¥ªÂä®ËáÇÔºåÂπ∂ËßÇÂØüÂèçÈ¶à„ÄÇ\\n> *   **Ê≠•È™§3Ôºö** ËÆ°ÁÆóÊØè‰∏™ËáÇÁöÑÁΩÆ‰ø°ËæπÁïåÔºàÂ¶Ç \\\\(LCB_t(i)\\\\) Âíå \\\\(UCB_t(i)\\\\)Ôºâ„ÄÇ\\n> *   **Ê≠•È™§4Ôºö** Ê†πÊçÆÁΩÆ‰ø°ËæπÁïåÂíåÈòàÂÄº \\\\(B\\\\) ÊéíÈô§Ê¨°‰ºòËáÇ„ÄÇ\\n> *   **Ê≠•È™§5Ôºö** Â¶ÇÊûúÊâÄÊúâËáÇË¢´ÊéíÈô§ÔºåÂàôË∞ÉÊï¥ÈòàÂÄº \\\\(B\\\\)ÔºàÂä†ÂÄçÊàñÂáèÂçäÔºâÂπ∂ÈáçÊñ∞ÂºÄÂßã„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   OPSEÔºàÊù•Ëá™ Lancewicki et al. (2021)Ôºâ\\n> *   CensoredUCBÔºàÊù•Ëá™ Tang, Wang, and Zheng (2024)Ôºâ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®Á¥ØÁßØÈÅóÊÜæ‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÔºàBHSEÔºâÂú®Êà™Êñ≠Ê≠£ÊÄÅÂàÜÂ∏ÉÊï∞ÊçÆÈõÜ‰∏äËææÂà∞‰∫ÜÊòæËëó‰Ωé‰∫éÂü∫Á∫øÊ®°ÂûãÁöÑÁ¥ØÁßØÈÅóÊÜæÔºå‰ºò‰∫éOPSEÂíåCensoredUCB„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫ÜÁ∫¶20%ÁöÑÊÄßËÉΩ„ÄÇ\\n> *   **Âú®Á®≥ÂÆöÊÄß‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®‰ºØÂä™Âà©ÂàÜÂ∏ÉÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫ËæÉ‰ΩéÁöÑÊñπÂ∑ÆÔºåË°®ÊòéÂÖ∂Âú®‰∏çÂêåÂú∫ÊôØ‰∏ãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   Â§öËáÇËÄÅËôéÊú∫ (Multi-armed Bandit, MAB)\\n*   Âª∂ËøüÊî∂Áõä (Delayed Payoff, N/A)\\n*   ÊàêÊú¨‰ºòÂåñ (Cost Optimization, N/A)\\n*   Â•ñÂä±ÊúÄÂ§ßÂåñ (Reward Maximization, N/A)\\n*   ÈÅóÊÜæËæπÁïå (Regret Bound, N/A)\\n*   ÁΩëÁªúË∑ØÁî± (Network Routing, N/A)\\n*   Âú®Á∫øÂπøÂëä (Online Advertising, N/A)\"\n}\n```"
}