{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.19279",
    "link": "https://arxiv.org/abs/2412.19279",
    "pdf_link": "https://arxiv.org/pdf/2412.19279.pdf",
    "title": "Improving Generalization for AI-Synthesized Voice Detection",
    "authors": [
        "Hainan Ren",
        "Li Lin",
        "Chun-Hao Liu",
        "Xin Wang",
        "Shu Hu"
    ],
    "categories": [
        "cs.SD",
        "cs.LG"
    ],
    "publication_date": "2024-12-26",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science",
        "Engineering"
    ],
    "citation_count": 4,
    "influential_citation_count": 0,
    "institutions": [
        "Purdue University",
        "Amazon",
        "University at Albany, SUNY"
    ],
    "paper_content": "# Improving Generalization for AI-Synthesized Voice Detection\n\nHainan Ren\\*, Li Lin1, Chun-Hao Liu2, Xin Wang3, Shu Hu1‚Ä†\n\n1 Purdue University 2 Amazon 3 University at Albany, SUNY hnren666 $@$ gmail.com, lin1785 $@$ purdue.edu, chunhaol@amazon.com, xwang56@albany.edu, hu968@purdue.edu\n\n# Abstract\n\nAI-synthesized voice technology has the potential to create realistic human voices for beneficial applications, but it can also be misused for malicious purposes. While existing AI-synthesized voice detection models excel in intra-domain evaluation, they face challenges in generalizing across different domains, potentially becoming obsolete as new voice generators emerge. Current solutions use diverse data and advanced machine learning techniques (e.g., domain-invariant representation, self-supervised learning), but are limited by predefined vocoders and sensitivity to factors like background noise and speaker identity. In this work, we introduce an innovative disentanglement framework aimed at extracting domain-agnostic artifact features related to vocoders. Utilizing these features, we enhance model learning in a flat loss landscape, enabling escape from suboptimal solutions and improving generalization. Extensive experiments on benchmarks show our approach outperforms state-of-the-art methods, achieving up to $5 . 1 2 \\%$ improvement in the equal error rate metric in intra-domain and $7 . 5 9 \\%$ in cross-domain evaluations.\n\nCode ‚Äî https://github.com/Purdue-M2/AI-SynthesizedVoice-Generalization\n\n# Introduction\n\nAI voice technology uses advanced models to generate natural human speech, mimicking tone and pronunciation to the extent that it is indistinguishable from real recordings. This technology has a wide range of applications, including voice assistants, audiobooks, voiceovers for videos and advertisements, and in fields such as healthcare and accessibility, where it can assist individuals with speech impairments. AI voice synthesis is rapidly evolving, using advanced variational auto-encoder (VAE) (Peng et al. 2020), generative adversarial network (GAN) models (Kumar et al. 2019; Yamamoto, Song, and $\\mathrm { K i m } \\ 2 0 2 0$ ; Lee et al. 2022), and diffusion models (Chen et al. 2020; Kong et al. 2020; Liu et al. 2023a) to generate audio waveforms from mel-spectrograms that are nearly indistinguishable from human recordings.\n\n![](images/7df1506e92358f7f5ca92550955719417c47f88242b4027257f35cdfdfeb7931.jpg)  \nFigure 1: Comparison of audio deepfake detection methods. (a) The conventional method uses entire voice features and excels in distinguishing between human and AI-synthesized voices within familiar domains but struggles with voices generated from unseen vocoders, leading to inaccurate separation. (b) Our method achieves intra-&cross-domain detection by exposing domain-agnostic features for learning on a flattened loss landscape.\n\nHowever, the widespread use of AI-synthesized voice has raised concerns about potential misuse, such as creating fake voice recordings, or ‚ÄòAudio Deepfakes‚Äô, for impersonation or fraud. For instance, in 2019, fraudsters used AI to mimic a CEO‚Äôs voice and stole over $\\$ 243,000$ via a phone call (Forbes 2019). To address these concerns, research into detecting and mitigating AI-synthesized voice has become essential, with much of the progress (Barrington et al. 2023; Sun et al. 2023) driven by ASVspoof challenges and datasets (ASVspoof 2023). While these methods show promising results in intra-domain evaluations (i.e., training and testing data come from the same vocoder), they face significant performance drops in cross-domain testing (i.e., testing data is generated by an unseen vocoder).\n\nNu¬®ller et al. (Mu¬®ller et al. 2022) provide a comprehensive evaluation of audio deepfake detectors‚Äô generalization capabilities. They found that current detection methods, trained or designed based on the ASVspoof benchmark, perform poorly when detecting novel real-world audio deepfakes. To date, limited research has addressed the issue of generalization in detecting audio deepfakes. Previous efforts have focused on domain-invariant representation learning (Xie et al. 2023) and self-supervised learning (Wang and Yamagishi 2023). However, these approaches rely on predefined vocoders and are often influenced by extraneous factors like background noise and speaker identity.\n\nTo tackle these challenges, we propose a novel method that improves the generalization of AI-synthesized voice detection by addressing both feature-level and optimizationlevel factors (see Figure 1). We start by experimentally analyzing the complex interactions of features and how the sharpness of loss landscapes impacts generalization in AIsynthesized voice detection. We then introduce a novel disentanglement framework that combines multi-task and contrastive learning to extract domain-agnostic artifact features common across various vocoders. This process involves separating domain-agnostic from domain-specific artifacts (i.e., artifacts linked to specific vocoders) and applying reconstruction regularization to maintain consistency between the original and reconstructed voices. To make the domainagnostic features universally applicable and enhance generalization, we use content feature distribution as a benchmark and apply mutual information loss. This aligns the domain-agnostic features with the reference distribution. Finally, we optimize the model by flattening the loss landscape to avoid suboptimal solutions and further improve generalization. Our key contributions include:\n\n1. We empirically analyze the factors from the feature and optimization levels affecting the generalization of AIsynthesized voice detection models.   \n2. We propose the first disentanglement framework to improve the generalization of AI-synthesized voice detection, focusing on both feature and optimization aspects. Specifically, we use disentanglement learning to extract domain-agnostic artifact features, which are utilized to enhance learning within a flattened loss landscape.   \n3. Our extensive experiments conducted on various prominent audio deepfake datasets demonstrate the effectiveness of our framework, which surpasses the performance of state-of-the-art methods in improving the generalization for cross-domain detection.\n\n# Related Work\n\nVoice Synthesis. Recent years witnessed remarkable improvements in synthesized speech quality, driven by deep learning (Tan et al. 2021). Two main approaches, Textto-Speech (TTS) and Voice Conversion (VC), both utilize vocoders to synthesize voices. Early neural vocoders, like WaveNet (Oord et al. 2016) and WaveRNN (Kalchbrenner et al. 2018), used features directly for waveform generation. Recent ones, such as SC-WaveRNN (Paul, Pantazis, and Stylianou 2020) and MB WaveRNN (Yu et al. 2020) adopted mel-spectrograms as input, but incurred slower inference times. To address this, alternative generative methods have emerged in waveform generation. These include flowbased methods like WaveGlow (Prenger, Valle, and Catanzaro 2019), FloWaveNet (Kim et al. 2019), and SqueezeWave (Zhai et al. 2020). GAN-based approaches namely MelGAN (Kumar et al. 2019), Parallel WaveGAN (Yamamoto, Song, and Kim 2020), and Fre-GAN (Lee et al. 2022). VAE-based techniques such as Wave-VAE (Peng et al. 2020). Additionally, diffusion-based methods like WaveGrad (Chen et al. 2020), DiffWave (Kong et al. 2020), and AudioLDM (Liu et al. 2023a).\n\nAI-Synthesized Voice Detection. AI-synthesized voice detection is a fundamental task in machine learning, distinguishing authentic human speech from synthesized audio. The end-to-end approaches like RawNet2 (Tak et al. 2021) and RawFormer (Liu et al. 2023b) gained traction due to their competitive performance and efficiency. While these methods excel on intra-domain datasets, they often falter when dealing with cross-domain datasets. Some techniques (Salvi, Bestagini, and Tubaro 2023; Wang et al. 2023) Enhance generalization by identifying the most relevant signal segments or combining multi-view features to detect fake audio. However, they rely on predefined vocoders and consider the entire feature space, making them vulnerable to background noise and speaker identity. These challenges are the focal points of our work.\n\nDisentangled Representation Learning. Disentanglement representation learning (Wang et al. 2022) is used in voice conversion, and voice style transfer during speech synthesis (Zhang et al. 2019; Aloufi, Haddadi, and Boyle 2020; Luong and Tran 2021; Champion, Jouvet, and Larcher 2022). Only Yadav et al. (2023) applies disentanglement representation learning in AI-synthesized voice detection. However, their approach‚Äîwhich extracts features directly from speech spectrograms‚Äîdoes not account for the artifact features characteristic of different vocoders. Additionally, it fails to address the domain-specific features that are intrinsic to different synthesis techniques. This leads to a limited generalization capability since it only eliminates the influence of the content features but may still overfit domain-specific patterns. In contrast, our proposed method effectively disentangles both domain-specific and domain-agnostic artifact features associated with various vocoders.\n\n# Motivating Experiments\n\nComplex Entangled Information. The generalization issue in AI-synthesized voice detection arises from two main factors. Firstly, many detectors overly emphasize irrelevant content, like identity and background noise. Secondly, different forgery techniques produce unique artifacts, as shown in Figure 2 (Left). The red circle shows unique large-scale artifacts in various AI-synthesized voices, easily detected by a vocoder-specific detector. However, detectors may become overly specialized in specific forgery, hindering generalization to unseen forgeries. To support this hypothesis, we extract features from the LibriSeVoc (Sun et al. 2023) dataset using RawNet2 (Tak et al. 2021) and Sun et al. (Sun et al. 2023). The UMAP visualizations (McInnes, Healy, and Melville 2018) show forgery vocoders‚Äô data clustering closely within baseline feature distribution (Figure 2 Middle (a) and (b)), while different vocoders‚Äô data exhibit more distinct separations. This phenomenon is also observed in the domain-specific feature distribution from our method (Figure 2 (Middle (c)). Moreover, generalizable detectors should treat domain features from AI-synthesized voices equally but distinguish them from features of human voices (as illustrated in Figure 2 (Middle (d))). They should also treat content features equally, whether they are from human voices or AI-synthesized ones (as shown in Figure 2 (Middle (e))).\n\n![](images/ed452e6369bc40085327d031a6dcea43df998870c194f0b6b67709305823db65.jpg)  \nFigure 2: Experimental results for Motivation. (Left) The differences of mel-spectrogram between human voices and AIsynthesized ones (e.g., based on WaveGrad (Chen et al. 2020) and WaveRNN (Kalchbrenner et al. 2018) vocoders). The red circles highlight AI vocoder artifacts. More details about these differences can be found in Appendix. (Middle) The UMAP (McInnes, Healy, and Melville 2018) visualization of features from related methods and our framework on LibriSeVoc (Sun et al. 2023). The genuine and forged voices from six vocoders are separated in the latent space. The baselines (RawNet2 (Tak et al. 2021), Sun et al. (Sun et al. 2023)) and our domain-specific module can learn the domain-specific features, whereas the domain-agnostic module of our method captures the shared forgery features across different vocoders, and the content module exclusively captures forgery-irrelevant features. (Right) Visualization of loss landscape for RawNet2 and Sun et al. The sharp local and global minima could lead to models with poor generalization.\n\nSharpness of Loss Landscape. Existing DNN-based AIsynthesized voice detection models, such as RawNet2 (Tak et al. 2021) and Sun et al (Sun et al. 2023), are highly overparameterized and tend to memorize data patterns during training. This results in sharp loss landscapes with multiple minima (Figure2 Right). Such sharpness presents challenges for models to locate the correct global minima for better generalization. Flattening the loss landscape is crucial to smooth the optimization path and enable robust generalization.\n\n# Method\n\nProblem Setup. Given a training dataset $\\begin{array} { r l } { s } & { { } = } \\end{array}$ $\\{ ( X _ { i } , D _ { i } , Y _ { i } ) \\} _ { i = 1 } ^ { n ^ { - } }$ of size $n$ , where $\\mathbf { \\bar { \\Phi } } _ { X _ { i } } \\quad \\in \\quad \\mathbb { R } ^ { d }$ represents the waveform of voice with feature dimension $d$ and $Y _ { i }$ corresponds to the target label (e.g., synthetic voice or real voice). Moreover, $D _ { i }$ serves as the domain label, indicating the voice generation source of $X _ { i }$ . For instance, in the LibriSeVoc dataset (Sun et al. 2023), $D _ { i } \\ \\in$ real, WaveNet (Oord et al. 2016), WaveRNN (Kalchbrenner et al. 2018), MelGAN (Kumar et al. 2019), Parallel WaveGAN (Yamamoto, Song, and Kim 2020), WaveGrad (Chen et al. 2020), DiffWave (Kong et al. 2020) Our goal is to train a synthesized voice detector on $s$ for promising performance and high generalization to unseen synthesized voice data.\n\nFramework. Our method, shown in Figure 3, consists of three main components: an encoder, a decoder, and two classification heads. The encoder contains two parts: a content encoder and an artifact encoder, each responsible for extracting content and artifact features. These two encoders share a common structure but operate with distinct sets of parameters. The decoder facilitates audio reconstruction by utilizing artifact features as a conditioning factor alongside content features. The classification module uses two heads: one for capturing domain-specific features and the other for extracting domain-agnostic features across different vocoders. The training process is guided by an optimization module designed to flatten the loss landscape. More details about the architecture can be found in Appendix.\n\n# Disentanglement Process\n\nWe introduce a disentanglement learning module to extract vocoder-agnostic artifact features from the input voice for detection. To elaborate, consider a pair of voices $( X _ { i } , X _ { j } )$ , where $i , j \\in \\{ 1 , \\cdots , n \\}$ and $i \\neq j$ . Here, $X _ { i }$ represents the synthetic human voice (or real human voice), and $X _ { j }$ corresponds to a real human voice (or synthetic human voice). Our encoder, denoted as $\\mathbf { E } ( \\cdot )$ , comprises both a content encoder and an artifact encoder for the extraction of content features $c$ and artifact features $a$ . Notably, artifact features include domain-specific artifacts $a ^ { s }$ (vocoder-specific features) and domain-agnostic artifacts $a ^ { g }$ (common features across different vocoder models). The operation of the encoder can be represented as follows: $c _ { i } , \\bar { a _ { i } ^ { s } } , a _ { i } ^ { g } = { \\bf E } ( X _ { i } )$ .\n\nClassification Loss. To separate domain-specific artifacts from domain-agnostic ones, we use a simple approach with multi-task learning, applying cross-entropy loss to each. The loss function can be formulated as follows: $L _ { c l s } ~ =$ $C ( \\overline { { h } } ( a _ { i } ^ { s } ) , D _ { i } ) + \\lambda _ { 1 } C ( \\widetilde { h } ( a _ { i } ^ { g } ) , Y _ { i } )$ , where $C ( \\cdot , \\cdot )$ represents the CE loss. $\\overline { { h } }$ and $\\widetilde { h }$ denote the classification heads for $a _ { i } ^ { s }$ and $a _ { i } ^ { g }$ , respectively.e $\\lambda _ { 1 }$ represents a hyperparameter. Training with this classification loss allows the encoder to learn both specific and shared artifact information, improving the model‚Äôs generalization capabilities.\n\n![](images/1c8dc362b091addcdc88cf8dfd227d4b1fde67d219bde46d0b3de5a958d87d61.jpg)  \nFigure 3: The overall architecture of our proposed approach. (a) In the encoder module, two RawNet2 (Tak et al. 2021)- structured backbones are added to the audio data signal to extract content and artifact features. Additionally, two headers further categorize artifact features into domain-agnostic and domain-specific categories. (b) In the decoder module, we use the content features as the base and fuse them with their own forgery features, as well as those from other samples for audio reconstruction. (c) Domain-agnostic features are made universally applicable by maximizing mutual information between domain-agnostic features and content features through joint and marginal distributions. (d) The sharpness-aware minimization (SAM) serves as an optimization technique to guide the model toward a flatter loss landscape to enhance its generalization. (e) For the inference, we take the predicted results of the domain-agnostic classification header.\n\nContrastive Loss. The classification loss mentioned above only considers individual voice information, neglecting the important global correlations between voices that can improve the encoder‚Äôs representation. Inspired by contrastive learning (Oord, Li, and Vinyals 2018; Yan et al. 2023; Lin et al. 2024), we address this by introducing a contrastive loss: $L _ { c o n } = [ b + | | a _ { \\mathrm { a n c h o r } } - a _ { + } | | _ { 2 } - | | a _ { \\mathrm { a n c h o r } } - a _ { - } | | _ { 2 } ] + \\cdot \\nonumber$ where ${ a _ { \\mathrm { a n c h o r } } }$ represents anchor artifact features of a voice, and $a _ { + }$ and $a _ { - }$ correspond to its positive counterpart from the same source and its negative counterpart from a different source, respectively. The hyperparameter $b$ is introduced, and $[ \\cdot ] _ { + } = \\dot { \\operatorname* { m a x } } \\{ 0 , \\cdot \\}$ denotes a hinge function. We apply $L _ { c o n }$ to both domain-specific and domain-agnostic artifact features. For domain-specific features, the source is the neural vocoder type, and the contrastive loss drives the encoder to capture specific vocoder representations. For domainagnostic features, the source is either synthetic or real human voice, encouraging the encoder to learn a generalizable representation independent of any particular vocoder.\n\nReconstruction Loss. To ensure the integrity of the extracted features and maintain consistency between the original and reconstructed voices, we apply a reconstruction loss, defined as follows: $L _ { r e c } = \\| X _ { i } - \\mathbf { D } ( c _ { i } , a _ { i } ^ { s } , a _ { i } ^ { g } ) \\| _ { 1 } +$ $\\| X _ { i } - \\mathbf { D } ( c _ { i } , a _ { j } ^ { s } , a _ { j } ^ { g } ) \\| _ { 1 }$ , where $\\mathbf { D } ( \\cdot , \\cdot , \\cdot )$ represents a decoder responsible for voice reconstruction based on the disentangled feature representations. In the $L _ { r e c }$ loss, the first term is the self-reconstruction loss, which uses the input voice‚Äôs latent features to minimize reconstruction errors. The second term is the cross-reconstruction loss, which penalizes errors using the partner‚Äôs forgery feature. Together, these terms promote feature disentanglement.\n\nMutual Information Loss. To enhance generalization, it‚Äôs crucial to maintain consistent distributions of domainagnostic features across different vocoders. This can be directly achieved by aligning their mutual relationships with the distributions of content features. While linear dependence/independence techniques (He et al. 2017, 2018) could be considered for this purpose, they often fail to capture the mutual relationships between content and domain-agnostic features in high-dimensional, nonlinear spaces. In contrast, mutual information (Kinney and Atwal 2014) is more effective for capturing arbitrary dependencies between variables.\n\nInspired by (Belghazi et al. 2018), we employ the Kullback-Leibler (KL) divergence (Joyce 2011), which is an equivalent form of mutual information, to quantify the dependencies between $c$ and $a ^ { g }$ . This is expressed as follows: $\\mathbf { M } [ c ; a ^ { g } ) = \\mathbb { D } _ { \\mathbf { K L } } \\big ( \\mathbb { P } ( c , a ^ { g } ) | | \\mathbb { P } ( c ) \\otimes \\mathbb { P } ( a ^ { g } ) \\big )$ . In this equation, $\\mathbb { P } ( \\cdot , \\cdot )$ represents the joint probability distribution, $\\mathbb { P } ( \\cdot )$ denotes the marginal probability distribution, $\\otimes$ signifies the product of the marginals, and $\\mathbb { D } _ { \\mathbb { K L } }$ is the KL divergence.\n\nGiven that the probability densities $\\mathbb { P } ( c , a ^ { g } )$ and $\\mathbb { P } ( c ) \\otimes \\mathbb { P } ( a ^ { g } )$ are not directly known, maximizing $\\mathbb { D } _ { \\mathrm { K L } } ( \\mathbb { P } ( c , a ^ { g } ) | | \\mathbb { P } ( c ) \\otimes \\mathbb { P } ( a ^ { g } ) )$ is a challenging task. However, we can maximize its lower bound ${ \\cal L } _ { M I }$ using the DonskerVaradhan representation (Donsker and Varadhan 1983), which can be expressed as: $L _ { M I } : = \\mathbb { E } _ { x \\sim \\mathbb { P } ( c , a ^ { g } ) } [ T ( x ) ] -$ $\\log \\mathbb { E } _ { x \\sim \\mathbb { P } ( c ) \\otimes \\mathbb { P } ( a ^ { g } ) } [ e ^ { T ( x ) } ]$ . where $T : \\mathbb { R } ^ { d _ { c } } \\times \\mathbb { R } ^ { d _ { a } g }  \\mathbb { R }$ represents a mutual information estimator. Inspired by (Hjelm et al. 2018), $T$ can be absorbed into the encoder, combining the content feature $c _ { i }$ and domain-agnostic feature $a _ { i } ^ { g }$ in practice.\n\n<html><body><table><tr><td rowspan=\"3\">Methods</td><td colspan=\"8\">LibriSeVoc</td><td colspan=\"8\">ASVspoof2019</td></tr><tr><td></td><td>Seenvocoder</td><td></td><td></td><td></td><td>Unseen vocoder</td><td></td><td></td><td>Avg</td><td>Seen vocoder</td><td></td><td>Avg</td><td>ASP</td><td>Unseen vocoder</td><td></td><td></td></tr><tr><td>Avg</td><td>LSV</td><td>ASP</td><td>WF</td><td>Avg</td><td>ASP</td><td>FAVC</td><td>WF</td><td></td><td>ASP</td><td>LSV</td><td></td><td></td><td>FAVC</td><td>LSV</td><td>WF</td></tr><tr><td>LCNN (Lavrentyeva et al.2019)</td><td>34.04</td><td>7.80</td><td>46.21</td><td>48.10</td><td>45.08</td><td>41.90</td><td>49.28</td><td>44.06</td><td>33.02</td><td>16.15</td><td>49.88</td><td>41.21</td><td>9.81</td><td>50.98</td><td>51.93</td><td>52.13</td></tr><tr><td>RawNet2 (Tak et al.2021)</td><td>20.21</td><td>1.59</td><td>29.86</td><td>29.18</td><td>30.16</td><td>24.09</td><td>33.92</td><td>32.47</td><td>20.79</td><td>1.89</td><td>39.68</td><td>39.55</td><td>6.46</td><td>51.37</td><td>47.71</td><td>52.65</td></tr><tr><td>WavLM (Chen et al. 2022)</td><td>27.26</td><td>14.12</td><td>32.88</td><td>34.79</td><td>29.54</td><td>27.18</td><td>25.64</td><td>35.80</td><td>50.75</td><td>14.22</td><td>87.28</td><td>59.27</td><td>7.91</td><td>83.84</td><td>87.28</td><td>58.07</td></tr><tr><td>XLS-R (Babu et al. 2021)</td><td>33.47</td><td>11.21</td><td>45.37</td><td>43.83</td><td>45.11</td><td>51.23</td><td>42.91</td><td>41.18</td><td>53.65</td><td>9.04</td><td>98.26</td><td>74.85</td><td>6.40</td><td>94.91</td><td>98.26</td><td>99.82</td></tr><tr><td>Sun et al. (Sun et al. 2023)</td><td>18.67</td><td>3.79</td><td>22.77</td><td>29.45</td><td>27.86</td><td>24.42</td><td>25.52</td><td>33.65</td><td>22.18</td><td>3.92</td><td>40.44</td><td>41.59</td><td>8.38</td><td>54.78</td><td>50.59</td><td>52.62</td></tr><tr><td>Ours</td><td>13.55</td><td>0.30</td><td>15.66</td><td>24.69</td><td>20.27</td><td>16.29</td><td>18.02</td><td>26.50</td><td>20.39</td><td>1.55</td><td>39.23</td><td>38.20</td><td>5.72</td><td>48.43</td><td>46.42</td><td>52.24</td></tr></table></body></html>\n\nTable 1: Comparisons of intra (seen)/cross (unseen)-domain generalization ability with comparable methods under EER $( \\% )$ . ‚ÄòLSV‚Äô, ‚ÄòASP‚Äô, ‚ÄòWF‚Äô, and ‚ÄòFAVC‚Äô stand for LibriSeVoc, ASVspoof2019, WaveFake, and FakeAVCeleb, respectively. ‚ÄòAvg represents the average EER $( \\% )$ across the seen or unseen vocoders within each dataset.\n\n# Optimization with Flattening Loss Landscape\n\nTo sum up, the final loss function can be expressed as\n\n$$\n\\operatorname* { m i n } _ { \\theta } \\mathcal { L } ( \\theta ) : = \\frac { 1 } { n } \\sum _ { i } [ L _ { c l s } + \\lambda _ { 2 } L _ { c o n } + \\lambda _ { 3 } L _ { r e c } ] - \\lambda _ { 4 } L _ { M I } .\n$$\n\nHere, we assume that the model weights of the entire framework are denoted as $\\theta$ . The hyperparameters $\\lambda _ { 2 } , \\lambda _ { 3 }$ , and $\\lambda _ { 4 }$ are introduced to strike a balance between each term in the loss function. In practice, Eq. (1) can be solved by using a gradient descent approach to update $\\theta$ . To help the model avoid suboptimal solutions common in overparameterized DNNs and further improve generalization, we apply the SAM technique (Foret et al. 2020) to flatten the loss landscape. Specifically, this involves finding an optimal $\\epsilon ^ { * }$ to perturb $\\theta$ in a way that maximizes the loss, expressed as:\n\n$$\n\\epsilon ^ { * } = \\arg \\operatorname* { m a x } _ { \\| \\epsilon \\| _ { 2 } \\leq \\gamma } \\mathcal { L } ( \\theta + \\epsilon ) \\approx \\arg \\operatorname* { m a x } _ { \\| \\epsilon \\| _ { 2 } \\leq \\gamma } \\epsilon ^ { \\top } \\nabla _ { \\theta } \\mathcal { L } = \\gamma \\mathrm { s } \\mathrm { i } \\mathrm { g n } ( \\nabla _ { \\theta } \\mathcal { L } ) ,\n$$\n\nHere, $\\gamma$ is a hyperparameter that controls the perturbation magnitude, and $\\nabla _ { \\boldsymbol { \\theta } } \\mathcal { L }$ is the gradient of $\\mathcal { L }$ with respect to $\\theta$ . The approximation term is derived using a first-order Taylor expansion, assuming $\\epsilon$ is small. The final equation results from solving a dual norm problem, using the sign function. Thus, the model weights are updated by solving:\n\n$$\n\\operatorname* { m i n } _ { \\theta } { \\mathcal { L } } ( \\theta + \\epsilon ^ { * } ) .\n$$\n\nThe underlying idea is that perturbing the model in the direction of the gradient norm increases the loss value, thereby improving generalization. We optimize Eq. (3) using stochastic gradient descent, and the related algorithm is provided in the Appendix. Note that this is the first time to adapt SAM for AI-synthesized voice detection. Our ablation study demonstrates the effectiveness of this novel application of it in enhancing generalization.\n\n# Experiment\n\n# Experimental Settings\n\nDatasets. To assess the generalization of our method, we tested it on various mainstream audio benchmarks, including LibriSeVoc (Sun et al. 2023), WaveFake (Frank and Scho¬®nherr 2021), ASVspoof 2019 (Lavrentyeva et al. 2019), and the audio segment of FakeAVCeleb (Khalid et al. 2021). More details of them and evaluations on other datasets (e.g., ASVspoof2021 (Yamagishi et al. 2021)) are in Appendix.\n\nBaseline Methods. To assess our method‚Äôs generalization capacity, we compared it with the following baselines: 1) LCNN (Lavrentyeva et al. 2019) achieved the second-best performance in ASVspoof 2021 Speech Deepfake track. 2) RawNet2 (Tak et al. 2021) achieved the top performance in ASVspoof 2021 Speech Deepfake track. 3) WavLM (Chen et al. 2022), developed by Microsoft, is a multilingual pretrained model for general audio tasks. 4) XLS-R (Babu et al. 2021), a robust cross-lingual speech model, excels in various domains, such as speech translation, speech recognition, and language identification. 5) Sun et al. (Sun et al. 2023) is the latest method in detecting AI-synthesized voices by identifying the artifacts of vocoders in audio signals.\n\nEvaluation Metrics. We evaluate our method using the Equal Error Rate (EER) metric, as commonly employed in previous studies (Frank and Sch¬®onherr 2021; Yamagishi et al. 2021) and baselines. A lower value means better performance. Furthermore, we add additional evaluations (Yamagishi et al. 2021) that include metrics AUC, False Acceptance Rate of Synthesized Voices, Rejection Rate of Real Voices, and Detection Cost Function in the Appendix, which also show the strong generalization of our method.\n\nImplementation Details. Our encoders are based on RawNet2 (Tak et al. 2021) but exclude the last fully connected layer and utilize only features before it as encoder outputs. We employ the Adam (Kingma and Ba 2014) optimizer with a learning rate set to 0.0002 and a batch size of 16. Hyperparameters $\\lambda _ { 1 }$ , $\\lambda _ { 2 }$ , $\\lambda _ { 3 }$ , and $\\lambda _ { 4 }$ are set to 0.1, 0.3, 0.05, and 0.03, respectively. The margin $b$ in $L _ { c o n }$ is set to 3. The $\\gamma$ in Eq. (2) is set to 0.07. We also use the original voice signal as input and apply the same data preprocessing as RawNet2 (Tak et al. 2021), padding all signals to the same size. More details can be found in the Appendix.\n\n# Results\n\nPerformance on Various Datasets. To demonstrate the universality of our method, we expand our training set to include data from LibriSeVoc (Sun et al. 2023) and ASVspoof2019 (Lavrentyeva et al. 2019), respectively. Our model is subsequently evaluated on four distinct datasets: LibriSeVoc (Sun et al. 2023), ASVspoof2019 (Lavrentyeva et al. 2019), WaveFake (Frank and Scho¬®nherr 2021), and FakeAVCeleb (Khalid et al. 2021). We divide the test sets into two categories: seen vocoders from the same domain and unseen vocoders for cross-domain evaluation, based on the vocoder categories present in the training set. More details of dataset-vocoder partitions can be found in Appendix.\n\nTable 2: Detection EER $( \\% )$ of cross-dataset evaluated on WaveFake Benchmark and trained on LibriSeVoc.   \n\n<html><body><table><tr><td colspan=\"2\">Vocoders</td><td colspan=\"4\">LCNNRawNet2 WavLMXLS-R Sun et al. Ours</td></tr><tr><td rowspan=\"2\">Seen</td><td>MelGAN</td><td>26.77 9.60</td><td>37.10</td><td>47.08 17.16</td><td>2.93</td></tr><tr><td>PWGAN</td><td>59.21 43.81</td><td>31.05</td><td>40.08 36.71</td><td>31.12</td></tr><tr><td></td><td>Avg WvGlow</td><td>42.99 26.70 28.21 4.14</td><td>34.08</td><td>43.58</td><td>26.94 17.03 0.50</td></tr><tr><td rowspan=\"4\">Unseen</td><td>MBMGAN</td><td></td><td>36.47</td><td>31.34</td><td>12.87 37.91</td></tr><tr><td></td><td>46.06 37.21</td><td>31.77</td><td>44.13</td><td>22.92</td></tr><tr><td>FBMGAN</td><td>48.69 44.65</td><td>37.08</td><td>45.42 46.85</td><td>39.79</td></tr><tr><td>HiFiGAN</td><td>38.98 38.58</td><td>37.69</td><td>40.69 38.60</td><td>29.76</td></tr><tr><td></td><td>Avg</td><td>40.49 31.14</td><td>35.75</td><td>40.40</td><td>34.06 23.38</td></tr></table></body></html>\n\nAs shown in Table 1, our method excels baselines on the same domain and cross-domain scenarios. Specifically, our method outperforms Sun et al. (Sun et al. 2023) by nearly $5 . 1 2 \\%$ $1 3 . 5 5 \\%$ vs. $1 8 . 6 7 \\%$ trained on LibriSeVoc) and $1 . 7 9 \\%$ $2 0 . 3 9 \\%$ vs. $2 2 . 1 8 \\%$ trained on ASVspoof2019) in the intra-domain setting, and by approximately $7 . 5 9 \\%$ $( 2 0 . 2 7 \\%$ vs. $2 7 . 8 6 \\%$ trained on LibriSevoc) and $1 . 3 4 \\%$ $3 8 . 2 0 \\%$ vs. $3 9 . 5 5 \\%$ trained on ASVspoof2019) in the crossdomain scene. Furthermore, despite the exceptional performance of the baselines in various audio downstream tasks, WavLM (Chen et al. 2022) and XLS-R (Babu et al. 2021) exhibit inferior performance in detecting AI-synthesized voices. While the LCNN method excels in performance for vocoder ‚ÄòWF‚Äô on ASVspoof2019, our approach surpasses others in overall performance.\n\nPerformance on Vocoders. To assess our method on intra-domain and cross-domain vocoders, we train the model on LibriSeVoc (Sun et al. 2023) and evaluate it on WaveFake (Frank and Scho¬®nherr 2021). We categorize WaveFake into seen/unseen subsets and conduct testing on all vocoders within each subset. The results are reported in Table 2. We observe that our method not only achieves the best performance on seen vocoders but also exhibits a significant performance boost on unseen vocoders $( 2 3 . 3 8 \\%$ vs. $3 4 . 0 6 \\%$ ).\n\nPerformance Affected by Specific Vocoder. To further investigate the model‚Äôs generalization capabilities and assess whether the models heavily rely on a specific vocoder, we remove one vocoder from LibriSeVoc (Sun et al. 2023) one by one, resulting in six sub-training sets. Then, we conduct tests on LibriSeVoc (Sun et al. 2023), ASVspoof2019 (Lavrentyeva et al. 2019), WaveFake (Frank and Scho¬®nherr 2021), and FakeAVCeleb (Khalid et al. 2021). Similarly, we categorize these benchmarks into seen and unseen domains and report the Equal Error Rate (EER) metric within these domains. As shown in Table 3, we observe that our method consistently achieves top performance on all six sub-training sets. Notably, Our feature decoupling approach generally performs better in unseen domains, demonstrating stable generalization in cross-domain scenarios.\n\nTable 3: Comparisons of generalization ability with compared methods in EER $( \\% )$ . We use LSV (LibriSeVoc) as a train set, and test on four datasets: LSV, WF, ASP, and FAVC. The abbreviations ‚Äòw/o WaveNet‚Äô represent the voices generated by the WaveNet vocoder and are removed from LSV.   \n\n<html><body><table><tr><td rowspan=\"2\">Train Set</td><td rowspan=\"2\">Methods</td><td colspan=\"2\">Seen vocoder</td><td colspan=\"2\">Unseen vocoder</td></tr><tr><td>Avg</td><td>LSV ASP WF</td><td>Avg</td><td>ASP FAVC WF</td></tr><tr><td>LSV</td><td>LCNN</td><td>33.9</td><td>8.3 45.148.4</td><td>46.2</td><td>40.6 54.1 44.0</td></tr><tr><td rowspan=\"3\">w/o WaveNet</td><td>RawNet2</td><td>21.2</td><td>7.5 36.5 19.6</td><td>27.2</td><td>28.2 23.0 30.4</td></tr><tr><td>Sun et al.</td><td>23.9</td><td>8.6 36.9 26.3</td><td>32.1</td><td>30.8 32.0 33.5</td></tr><tr><td>Ours</td><td>13.9</td><td>1.9 20.5 19.2</td><td>22.7</td><td>19.7 23.3 24.9</td></tr><tr><td rowspan=\"3\">LSV w/o WaveRNN</td><td>LCNN</td><td>36.5</td><td>9.8 49.9 50.047.7</td><td></td><td>50.0 43.5 49.7</td></tr><tr><td>RawNet2</td><td>23.8</td><td>11.2 34.7 25.329.5</td><td></td><td>30.2 30.7 27.5</td></tr><tr><td>Sun et al.</td><td>25.3</td><td>13.1 33.3 29.4</td><td>28.7</td><td>23.1 30.8 32.1</td></tr><tr><td rowspan=\"3\">LSV w/o</td><td>Ours</td><td>18.6</td><td>8.7 24.8 22.323.5</td><td></td><td>22.1 22.7 25.6</td></tr><tr><td>LCNN</td><td>33.1</td><td>7.8 43.9 47.544.642.1</td><td></td><td>45.8 45.9</td></tr><tr><td>RawNet2</td><td>18.4</td><td>2.8 24.9 27.429.0|20.7</td><td></td><td>35.6 30.6</td></tr><tr><td rowspan=\"2\">WaveGrad</td><td>Sun et al.</td><td>24.8</td><td>3.0 34.7 36.638.1</td><td></td><td>27.8 45.0 41.5</td></tr><tr><td>Ours</td><td>15.0</td><td>1.1 20.1 23.923.3</td><td></td><td>18.8 22.3 28.6</td></tr><tr><td rowspan=\"4\">LSV w/o DiffWave</td><td>LCNN</td><td>37.3</td><td>10.0 52.3 49.750.0|49.0</td><td></td><td>56.2 45.0</td></tr><tr><td>RawNet2</td><td>24.5</td><td>2.6 39.5 31.434.6</td><td></td><td>29.7 40.3 33.8</td></tr><tr><td>Sun et al.</td><td>29.3</td><td>6.7 38.7 42.4</td><td>33.4</td><td>29.4 33.4 37.4</td></tr><tr><td>Ours</td><td>16.4</td><td>1.2 17.3 30.627.5</td><td></td><td>19.8 31.5 31.3</td></tr><tr><td rowspan=\"4\">LSV w/o MelGAN</td><td>LCNN</td><td>36.4</td><td>12.5 49.8 46.8</td><td>45.648.2</td><td>47.6 41.1</td></tr><tr><td>RawNet2</td><td>21.2</td><td>1.0 36.1 26.427.0|26.7</td><td></td><td>25.7 28.5</td></tr><tr><td>Sun et al.</td><td>21.4</td><td>0.8 35.6 27.8</td><td>29.4</td><td>25.4 29.6 33.2</td></tr><tr><td>Ours</td><td>13.6</td><td>0.8 17.7 22.3</td><td>24.1</td><td>20.3 25.4 26.6</td></tr><tr><td rowspan=\"4\">LSV w/o PWaveGan</td><td>LCNN</td><td>36.0</td><td>7.9 50.0 50.347.046.5</td><td></td><td>50.0 44.6</td></tr><tr><td>RawNet2</td><td>21.1</td><td>2.1 32.7 28.3</td><td>28.8</td><td>26.4 27.3 32.8</td></tr><tr><td>Sun et al.</td><td>27.9</td><td>3.9 43.8 36.0</td><td>31.2</td><td>32.3 27.7 33.6</td></tr><tr><td>Ours</td><td>16.8</td><td>0.8 26.0 23.5</td><td>26.2</td><td>21.3 25.9 31.3</td></tr></table></body></html>\n\n# Ablation Study\n\nAnalyzing the Framework Components in a Deconstructive Trajectory. To evaluate the impact of each component in our proposed method, we conduct an analysis on several datasets. The results are reported in Table 4. Our modules consistently improve performance on seen or unseen vocoders. More detailed observations are as follows: 1) $V _ { A }$ outperforms the baseline RawNet2 (Tak et al. 2021) on most datasets, especially on seen WF and unseen FAVC, revealing the efficacy of our reconstruction module. 2) $V _ { B }$ and $V _ { C }$ achieve relatively similar results and further enhance the performance compared with $V _ { A }$ (e.g., $6 . 6 9 \\%$ and $9 . 2 7 \\%$ improvement of EER on seen ASP), indicating the necessity of our multi-task header and contrastive learning module. 3) Implementing the mutual information module $( V _ { D } )$ greatly improves the performance in cross-domain evaluation (e.g., $6 . 8 5 \\%$ enhancement of EER on unseen FAVC compared\n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"6\">Modulecon</td><td colspan=\"4\">Lsev vocoder</td><td colspan=\"4\">UAsPn FAVCr</td></tr><tr><td>RawNet2</td><td>Rec</td><td></td><td></td><td>MI</td><td>SAM</td><td>Avg</td><td></td><td></td><td>WF</td><td>Avg</td><td></td><td></td><td>WF</td></tr><tr><td>RawNet2</td><td>‚àö</td><td></td><td></td><td></td><td></td><td></td><td>20.21</td><td>1.59</td><td>29.86</td><td>29.18</td><td>30.16</td><td>24.09</td><td>33.92</td><td>32.47</td></tr><tr><td>VA</td><td>‚àö</td><td>‚àö</td><td></td><td></td><td></td><td></td><td>19.16</td><td>0.53</td><td>31.84</td><td>25.11</td><td>28.57</td><td>27.49</td><td>26.35</td><td>31.87</td></tr><tr><td>VB</td><td>‚àö</td><td>‚àö</td><td>‚àö</td><td></td><td></td><td></td><td>16.58</td><td>0.30</td><td>25.15</td><td>24.30</td><td>26.07</td><td>22.38</td><td>27.13</td><td>28.71</td></tr><tr><td>Vc</td><td><</td><td>‚àö</td><td>‚àö</td><td>‚àö</td><td></td><td></td><td>15.90</td><td>0.42</td><td>22.57</td><td>24.72</td><td>26.83</td><td>24.15</td><td>27.64</td><td>28.71</td></tr><tr><td>VD</td><td>‚àö</td><td>‚àö</td><td>‚àö</td><td>‚àö</td><td>‚àö</td><td></td><td>15.71</td><td>0.38</td><td>25.62</td><td>21.12</td><td>23.23</td><td>23.52</td><td>20.79</td><td>25.37</td></tr><tr><td>Ours</td><td>‚àö</td><td>‚àö</td><td>‚àö</td><td>‚àö</td><td>‚àö</td><td>‚àö</td><td>13.55</td><td>0.30</td><td>15.66</td><td>24.69</td><td>20.27</td><td>16.29</td><td>18.02</td><td>26.50</td></tr></table></body></html>\n\nTable 4: An analysis study of our key modules: RawNet2 (Tak et al. 2021), ‚ÄòRec‚Äô(Reconstruction module), ‚ÄòCls‚Äô(multi-task header), ‚ÄòCon‚Äô(contrastive learning module), ‚ÄòMI‚Äô(mutual information module), and ‚ÄòSAM‚Äô. These modules are constructed and trained under LSV and then tested on seen/unseen parts of LSV, ASP, WF, and FAVC.\n\nsensitivity of Œª4 in MI sensitivityofyin SAM sensitivity of#vocoders Ours aEERs Ours aEERs Ours aEERs w/o MI (Œ¶Œ≠) w/ MI (Œ¶ŒÆ) ËÇ≤ RawNet2 aEERs E RawNet2 aEERs 2 RawNet2 aEERs 15 w/o Loss w/ Loss 0.010.020.030.040.050.060.07 10.010.020.030.040.050.060.07 1 2 3 4 5 6 Flattening Flattening (a) (b) # vocoders\n\nwith $V _ { C }$ ), To further illustrate the impact of mutual information, we present demographic-agnostic features in Figure 4 (Left-Top). It clearly shows that incorporating the mutual information module in the model leads to a more irregular distribution of fake features in the feature space, independent of the forgery techniques employed. 4) Combining all components (Ours) achieves optimal results in both intra-domain and cross-domain evaluation, indicating the efficacy of the SAM approach. Figure 4 (Left-Bottom) illustrates SAM‚Äôs impact on the loss landscape, showing its ability to transform the rugged surface into a smoother one and enhance the model‚Äôs generalization capability.\n\nEffect of Hyperparameters. With an increasing impact of mutual information $( \\lambda _ { 4 } )$ , the model‚Äôs performance improves on both seen and unseen data. However, as $\\lambda _ { 4 }$ continues to increase, the voice detection information within domain-agnostic features gradually diminishes, leading to a decline in the model‚Äôs ability to distinguish between Human and AI-synthesized voices. With increasing $\\lambda _ { 4 }$ , voice detection information in domain-agnostic features diminishes, reducing the model‚Äôs ability to distinguish between human and AI-synthesized voices. Notably, we observe optimal performance when $\\lambda _ { 4 } = 0 . 0 3$ (Figure 4 (Middle (a))). As SAM intensity increases, model performance initially slightly decreases, followed by continuous improvement, and reaching optimal performance at $\\gamma = 0 . 0 7$ (Figure 4 (Middle (b))). This suggests that the model generalization benefits from a smoother loss landscape.\n\nAblation on the number of vocoders in the train set. To illustrate the impact of vocoder diversity in the training dataset on the model generalization. We create subsets of the training data with different combinations of vocoder types, ranging from 1 to 6, sourced from LibriSeVoc. Trained models are evaluated on the similar seen/unseen manner, and the aEERs and aEERu are reported. Figure 4 (Right) indicates a clear trend: increasing vocoder diversity in training data enhances model generalizability.\n\n# Conclusion\n\nExisting AI-synthesized voice detection models are limited by predefined vocoders and sensitivity to factors like background noise and speaker identity. While excelling in intradomain evaluation, they struggle to generalize across different domains with emerging voice generation advancements. To address this challenge, we propose a new disentanglement framework with a module extracting domain-agnostic features related to vocoders. Furthermore, we use these features to aid model learning in a flattened loss landscape, helping the model escape suboptimal solutions and enhance generalization. Extensive experiments on various datasets, compared to state-of-the-art methods, demonstrate the superior detection generalization of our framework. Limitation: One limitation of our method is its dependency on datasets including AI-synthesized voices generated by different types of vocoders. A few datasets contain synthesized voice from a single vocoder, limiting the extraction of domain-agnostic artifact features. Future Work: In future work, we aim to explore methods that can directly detect synthetic audio and improve detection generalization not limited to datasets containing multiple vocoders. We also plan to extend our approach to multi-modal tasks for audio and video detection.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáËß£ÂÜ≥ÁöÑÊ†∏ÂøÉÈóÆÈ¢òÊòØAIÂêàÊàêËØ≠Èü≥Ê£ÄÊµãÊ®°ÂûãÂú®Èù¢ÂØπ‰∏çÂêåÂ£∞Á†ÅÂô®ÔºàvocoderÔºâÁîüÊàêÁöÑËØ≠Èü≥Êó∂Ê≥õÂåñËÉΩÂäõ‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÁé∞ÊúâÊ®°ÂûãÂú®ËÆ≠ÁªÉÂíåÊµãËØïÊï∞ÊçÆÊù•Ëá™Áõ∏ÂêåÂ£∞Á†ÅÂô®Êó∂Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®Èù¢ÂØπÊú™Áü•Â£∞Á†ÅÂô®ÁîüÊàêÁöÑËØ≠Èü≥Êó∂ÊÄßËÉΩÊòæËëó‰∏ãÈôç„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éÔºåÈöèÁùÄÊñ∞ÂûãÂ£∞Á†ÅÂô®ÁöÑ‰∏çÊñ≠Ê∂åÁé∞ÔºåÁº∫‰πèÊ≥õÂåñËÉΩÂäõÁöÑÊ£ÄÊµãÊ®°Âûã‰ºöËøÖÈÄüÂ§±ÊïàÔºåÊó†Ê≥ïÂ∫îÂØπÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÂ§öÊ†∑ÂåñÊîªÂáªÔºåÂ¶ÇÈáëËûçËØàÈ™ó„ÄÅË∫´‰ªΩÂÜíÁî®Á≠âÊÅ∂ÊÑèË°å‰∏∫„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂàõÊñ∞ÁöÑËß£ËÄ¶Ê°ÜÊû∂ÔºåÈÄöËøáÂàÜÁ¶ªÂüüÊó†ÂÖ≥Ôºàvocoder-agnosticÔºâÂíåÂüüÁõ∏ÂÖ≥Ôºàvocoder-specificÔºâÁöÑ‰º™ÂΩ±ÁâπÂæÅÔºåÂπ∂ÁªìÂêàÂπ≥Âù¶ÂåñÊçüÂ§±ÊôØËßÇÔºàflattened loss landscapeÔºâÁöÑ‰ºòÂåñÁ≠ñÁï•ÔºåÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑË∑®ÂüüÊ£ÄÊµãËÉΩÂäõ„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **Ë¥°ÁåÆ1Ôºö** È¶ñÊ¨°ÊèêÂá∫Èù¢ÂêëAIÂêàÊàêËØ≠Èü≥Ê£ÄÊµãÁöÑËß£ËÄ¶Ê°ÜÊû∂ÔºåÈÄöËøáÂ§ö‰ªªÂä°ÂíåÂØπÊØîÂ≠¶‰π†ÂàÜÁ¶ªÂüüÊó†ÂÖ≥/Áõ∏ÂÖ≥ÁâπÂæÅ„ÄÇ\\n> *   **Ë¥°ÁåÆ2Ôºö** ÂºïÂÖ•‰∫í‰ø°ÊÅØÊçüÂ§±ÂíåSAM‰ºòÂåñÂô®ÔºåÂÆûÁé∞ÁâπÂæÅÂàÜÂ∏ÉÂØπÈΩêÂíåÊçüÂ§±ÊôØËßÇÂπ≥Âù¶Âåñ„ÄÇ\\n> *   **Ë¥°ÁåÆ3Ôºö** Âú®Ë∑®ÂüüÊµãËØï‰∏≠ÔºåEERÊåáÊ†áÊØîÊúÄ‰Ω≥Âü∫Á∫øÊèêÂçá7.59%ÔºàLibriSeVocÊï∞ÊçÆÈõÜÔºâÂíå1.34%ÔºàASVspoof2019Êï∞ÊçÆÈõÜÔºâ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   Ê†∏ÂøÉÂéüÁêÜÊòØÔºö‰∏çÂêåÂ£∞Á†ÅÂô®‰∫ßÁîüÁöÑÂêàÊàêËØ≠Èü≥Êó¢ÂåÖÂê´ÂÖ±‰∫´ÁöÑÂüüÊó†ÂÖ≥‰º™ÂΩ±ÔºàÂ¶ÇÁîüÊàêËøáÁ®ãÁöÑÂÖ±ÊúâÂ§±ÁúüÔºâÔºå‰πüÂåÖÂê´Â£∞Á†ÅÂô®ÁâπÊúâÁöÑÂüüÁõ∏ÂÖ≥‰º™ÂΩ±„ÄÇÈÄöËøáËß£ËÄ¶Ëøô‰∏§Á±ªÁâπÂæÅÔºåÊ®°ÂûãÂèØ‰ª•‰∏ìÊ≥®‰∫éÂ≠¶‰π†ÊôÆÈÄÇÁöÑÊ£ÄÊµãÊ®°Âºè„ÄÇ\\n> *   ËÆæËÆ°Âì≤Â≠¶Ê∫ê‰∫éÂØπÁé∞ÊúâÊñπÊ≥ïËøáÊãüÂêàÁâπÂÆöÂ£∞Á†ÅÂô®Ê®°ÂºèÁöÑÂàÜÊûêÔºå‰ª•ÂèäÊçüÂ§±ÊôØËßÇÂ∞ñÈîêÊÄßÂØπÊ≥õÂåñËÉΩÂäõÁöÑÂΩ±Âìç„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **ÂÖàÂâçÂ±ÄÈôêÔºö** ‰º†ÁªüÊñπÊ≥ïÁõ¥Êé•‰ΩøÁî®ÂÆåÊï¥ËØ≠Èü≥ÁâπÂæÅÔºåÊòìÂèóËÉåÊôØÂô™Â£∞ÂíåËØ¥ËØù‰∫∫Ë∫´‰ªΩÂπ≤Êâ∞ÔºõÁé∞ÊúâËß£ËÄ¶ÊñπÊ≥ï‰ªÖËÄÉËôëÂÜÖÂÆπÁâπÂæÅËÄåÂøΩÁï•Â£∞Á†ÅÂô®‰º™ÂΩ±ÁöÑÂ±ÇÊ¨°ÊÄß„ÄÇ\\n> *   **Êú¨ÊñáÊîπËøõÔºö** \\n>     1. ËÆæËÆ°ÂÜÖÂÆπÁºñÁ†ÅÂô®Âíå‰º™ÂΩ±ÁºñÁ†ÅÂô®ÁöÑÂèåÂàÜÊîØÁªìÊûÑ\\n>     2. ÈÄöËøáÂØπÊØîÊçüÂ§±ÂàÜÁ¶ªÂüüÊó†ÂÖ≥/Áõ∏ÂÖ≥‰º™ÂΩ±\\n>     3. Áî®‰∫í‰ø°ÊÅØÊçüÂ§±ÂØπÈΩêÂüüÊó†ÂÖ≥ÁâπÂæÅÂàÜÂ∏É\\n>     4. ÈááÁî®SAM‰ºòÂåñÂô®Âπ≥Âù¶ÂåñÊçüÂ§±ÊôØËßÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1. **ÁâπÂæÅÊèêÂèñÔºö** ‰ΩøÁî®RawNet2Êû∂ÊûÑÁöÑÁºñÁ†ÅÂô®ÔºåÂàÜÂà´ÊèêÂèñÂÜÖÂÆπÁâπÂæÅ$c_i$Âíå‰º™ÂΩ±ÁâπÂæÅ$a_i$ÔºàÂê´ÂüüÁõ∏ÂÖ≥$a_i^s$ÂíåÂüüÊó†ÂÖ≥$a_i^g$Ôºâ\\n> 2. **Â§ö‰ªªÂä°Â≠¶‰π†Ôºö** ÂàÜÁ±ªÊçüÂ§±$L_{cls}=C(\\\\overline{h}(a_i^s),D_i)+\\\\lambda_1 C(\\\\tilde{h}(a_i^g),Y_i)$\\n> 3. **ÂØπÊØîÂ≠¶‰π†Ôºö** ‰∏âÂÖÉÁªÑÊçüÂ§±$L_{con}=[b+||a_{anchor}-a_+||_2-||a_{anchor}-a_-||_2]_+$\\n> 4. **‰∫í‰ø°ÊÅØ‰ºòÂåñÔºö** ÊúÄÂ§ßÂåñ$\\\\mathcal{L}_{MI}=\\\\mathbb{E}_{x\\\\sim\\\\mathbb{P}(c,a^g)}[T(x)]-\\\\log\\\\mathbb{E}_{x\\\\sim\\\\mathbb{P}(c)\\\\otimes\\\\mathbb{P}(a^g)}[e^{T(x)}]$\\n> 5. **SAM‰ºòÂåñÔºö** ÊùÉÈáçÊõ¥Êñ∞ÈááÁî®$\\\\theta+\\\\gamma\\\\cdot sign(\\\\nabla_\\\\theta\\\\mathcal{L})$Êâ∞Âä®\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   LCNN (Lavrentyeva et al.2019)\\n> *   RawNet2 (Tak et al.2021)\\n> *   WavLM (Chen et al. 2022)\\n> *   XLS-R (Babu et al. 2021)\\n> *   Sun et al. (Sun et al. 2023)\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®EERÊåáÊ†áÔºàLibriSeVocÊï∞ÊçÆÈõÜÔºâ‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®Ë∑®ÂüüÊµãËØï‰∏≠ËææÂà∞20.27%ÔºåÊòæËëó‰ºò‰∫éSun et al.(27.86%)ÂíåRawNet2(30.16%)„ÄÇ‰∏éÊúÄ‰Ω≥Âü∫Á∫øÁõ∏ÊØîÊèêÂçá7.59‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®EERÊåáÊ†áÔºàASVspoof2019Êï∞ÊçÆÈõÜÔºâ‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®Ë∑®ÂüüÊµãËØï‰∏≠ËææÂà∞38.20%Ôºå‰ºò‰∫éSun et al.(39.55%)ÂíåWavLM(59.27%)ÔºåÂ∞§ÂÖ∂ÂØπWaveFakeÂ£∞Á†ÅÂô®ÁöÑÊ£ÄÊµãÊèêÂçáËææ12.87‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®ËÆ≠ÁªÉÊï∞ÊçÆÊïèÊÑüÊÄß‰∏äÔºö** ÂΩìÁßªÈô§Êüê‰∏™Â£∞Á†ÅÂô®ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÊó∂ÔºåÊú¨ÊñáÊñπÊ≥ïÂú®Êú™Áü•Â£∞Á†ÅÂô®ÊµãËØï‰∏≠ÁöÑEERÊ≥¢Âä®Â∞è‰∫é2%ÔºåËÄåÂü∫Á∫øÊ®°ÂûãÁöÑÊ≥¢Âä®È´òËææ15%„ÄÇ\\n> *   **Âú®WaveFakeÊï∞ÊçÆÈõÜ‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®Êú™Áü•Â£∞Á†ÅÂô®ÊµãËØï‰∏≠ËææÂà∞23.38%ÁöÑEERÔºåÊòæËëó‰ºò‰∫éSun et al.(34.06%)ÂíåRawNet2(31.14%)„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   AIÂêàÊàêËØ≠Èü≥Ê£ÄÊµã (AI-Synthesized Voice Detection, ASVD)\\n*   Â£∞Á†ÅÂô® (Vocoder, N/A)\\n*   Ëß£ËÄ¶Ë°®Á§∫Â≠¶‰π† (Disentangled Representation Learning, DRL)\\n*   ÂüüÊ≥õÂåñ (Domain Generalization, DG)\\n*   ‰∫í‰ø°ÊÅØÊúÄÂ§ßÂåñ (Mutual Information Maximization, MIM)\\n*   Âπ≥Âù¶ÊçüÂ§±ÊôØËßÇ (Flat Loss Landscape, FLL)\\n*   Èü≥È¢ëÊ∑±Â∫¶‰º™ÈÄ† (Audio Deepfake, N/A)\\n*   ÂØπÊØîÂ≠¶‰π† (Contrastive Learning, CL)\"\n}\n```"
}