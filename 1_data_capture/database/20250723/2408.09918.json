{
    "source": "Semantic Scholar",
    "arxiv_id": "2408.09918",
    "link": "https://arxiv.org/abs/2408.09918",
    "pdf_link": "https://arxiv.org/pdf/2408.09918.pdf",
    "title": "Expressive Power of Temporal Message Passing",
    "authors": [
        "P. Walega",
        "Michael Rawson"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2024-08-19",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 4,
    "influential_citation_count": 1,
    "institutions": [
        "Queen Mary University of London",
        "University of Oxford",
        "University of Southampton"
    ],
    "paper_content": "# Expressive Power of Temporal Message Passing\n\nPrzemysÅ‚aw Andrzej WaÅ‚eË›ga1,2, Michael Rawson3\n\n1Queen Mary University of London, UK 2University of Oxford, UK 3University of Southampton, UK p.walega@qmul.ac.uk, michael $@$ rawsons.uk\n\n# Abstract\n\nGraph neural networks (GNNs) have recently been adapted to temporal settings, often employing temporal versions of the message-passing mechanism known from GNNs. We divide temporal message passing mechanisms from literature into two main types: global and local, and establish WeisfeilerLeman characterisations for both. This allows us to formally analyse expressive power of temporal message-passing models. We show that global and local temporal message-passing mechanisms have incomparable expressive power when applied to arbitrary temporal graphs. However, the local mechanism is strictly more expressive than the global mechanism when applied to colour-persistent temporal graphs, whose node colours are initially the same in all time points. Our theoretical findings are supported by experimental evidence, underlining practical implications of our analysis.\n\n# Introduction\n\nMessage-passing graph neural networks (or MP-GNNs) (Gilmer et al. 2017) are prominent models for graph learning, which have achieved state-of-the art performance in tasks of link prediction as well as in node and graph classification. Importantly, they proved successful in a number of real-world applications including social networks, protein-protein interactions, and knowledge graphs (Zhou et al. 2020).\n\nIn recent years, there has been growing interest in adapting MP-GNNs to process temporal graphs (for an example of a temporal graph see the left part of Figure 1) which are particularly well-suited for dynamic applications such as recommender systems (Wu et al. 2023), traffic forecasting (Yu, Yin, and Zhu 2018), finance networks (Pareja et al. 2020), and modelling the spread of diseases (Kapoor et al. 2020). Research in this direction gave rise to various temporal MP-GNNs (MP-TGNNs) obtained by introducing temporal variants of the message-passing mechanism (Longa et al. 2023; Skarding, Gabrys, and Musial 2021; Gao and Ribeiro 2022). This can be obtained by assigning to graph nodes different embeddings (feature vectors) for different time points and then passing messages between timestamped nodes. Depending on the routes of messages-passing between timestamped nodes and on the encoding of the temporal component in the messages, we arrive at various temporal message-passing mechanisms. In this paper we distinguish two main groups of MP-TGNNs: global, where messages can be passed between nodes stamped with different times and local, where messages are passed only between nodes stamped with the same time, while information about other times is encoded within messages.\n\nAlthough several variants of global (Longa et al. 2023; Xu et al. 2020; Luo and Li 2022) and local (Rossi et al. 2020; Qu et al. 2020) MP-TGNNs have been designed and successfully applied, we still lack a good understanding of modelling capabilities dictated by their temporal messagepassing mechanisms, and do not have answers to the following fundamental questions. What tools can we use to analyse the expressive power of global and local MP-TGNNs? Are there limits to the expressive power of either type? Which type can express more? How does the difference in expressiveness affect practical performance? Answers to these questions are key when choosing an appropriate temporal message-passing mechanism for a particular task and when designing new MP-TGNNs. The importance of answering such questions has been clearly shown by research on expressive power of static MP-GNNs, which equipped us with powerful tools and gave rise to a whole new research direction (Morris et al. 2019; Xu et al. 2019; Cai, FÃ¼rer, and Immerman 1992; Grohe 2023; BarcelÃ³ et al. 2020). In the temporal setting, however, such an analysis is still missing. We aim to fill this urgent gap.\n\nContributions. Our main contributions are as follows:\n\nâ€¢ We formalise the two main types of MP-TGNNs, global and local, depending on the form of the adopted temporal message-passing mechanism. â€¢ We characterise the expressive power of both types. To determine which temporal nodes can be distinguished by MP-TGNNs, we construct a knowledge graph and then apply the 1-dimensional Weisfeiler-Leman test (1-WL). As depicted in Figure 1, our construction of the knowledge graph is different for global and local MP-TGNNs, but in both cases this approach allows us to precisely capture the expressive power of MP-TGNNs. For example, given the temporal graph $T G$ in Figure 1, global and local MP-TGNNs distinguish the same nodes, since the colourings in the rightmost graphs are the same.\n\nNodes distinguishable by global MP-TGNNs:\n\n![](images/fa3c03b32a324974d619fbd6eebb881123bfffc75aa8f45995631f003a36e454.jpg)  \nFigure 1: Our approach to determine which nodes in a temporal graph $T G$ are distinguishable by MP-TGNN: we construct of knowledge graphs $\\kappa _ { \\mathsf { g l o b } } ( T G )$ and $\\textstyle \\mathcal { K } _ { \\mathrm { l o c } } ( T G )$ , and then apply 1-WL\n\nâ€¢ We use the above characterisation to show that, quite surprisingly, both global and local MP-TGNNs can distinguish nodes which are pointwise isomorphic. This leads us to introduce a stronger timewise isomorphism, wellsuited for characterisation of MP-TGNNs.\n\nâ€¢ The Weisfeiler-Leman characterisation also allows us to show that global and local MP-TGNNs have incomparable expressive power: each of the types can distinguish nodes which are indistinguishable by the other type. However, if the input temporal graph is colourpersistent (initial embedding of each node is the same at all time points), local MP-TGNNs are more expressive than global MP-TGNNs. We can extend these results to a complete expressiveness classification as in Figure 2.\n\nglobal MP-TGNNs, global MP-TGNNs, any T Gs colour-persistent T Gs #ï¿¥ > local MP-TGNNs, local MP-TGNNs, any T Gs colour-persistent T Gs â€¢ Finally, we experimentally validate our theoretical results by constructing proof-of-concept global and local models. We show that, indeed, on colour-persistent graphs local models outperform global models, when compared on the temporal link-prediction and TGB 2.0 benchmark suite (Gastinger et al. 2024). This is the case when models use the same number of layers, and the difference in performance increases further if we choose optimal number of layers for each type of model separately.\n\n# Background\n\nTemporal graphs. We focus on temporal graphs in the socalled snapshot representation (Longa et al. 2023; Gao and Ribeiro 2022; Skarding, Gabrys, and Musial 2021) shown in Figure 3. A temporal graph is a finite sequence $T G =$ $( G _ { 1 } , \\bar { t } _ { 1 } ) , \\dots , ( G _ { n } , \\bar { t } _ { n } )$ of undirected node-coloured graphs\n\n$G _ { i } = ( V _ { i } , E _ { i } , c _ { i } )$ , where $t _ { 1 } < \\cdots < t _ { n }$ are real-valued time po nts, constituting the temporal domain time $( T G )$ . Each $V _ { i }$ is a finite set of nodes, each $E _ { i } \\subseteq \\{ \\{ u , v \\} \\subseteq V _ { i } \\mid ^ { \\cdot } u \\neq v \\}$ is a set of undirected edges, and $c _ { i } : V _ { i }  D$ aâŠ†ssignâˆ£ s nâ‰ ode}s colours from some set $D$ , which could be real feature vectors. Following standard notation, we sometimes use $\\mathbf { x } _ { v } ( t _ { i } )$ instead of $c _ { i } ( \\bar { \\upsilon } )$ . We represent $c _ { i }$ using different colours(fo)r nodes in figures. We assume that the domain of nodes does not change over time, so $V _ { 1 } = . . . = V _ { n } = V ( T G )$ . We call a pair of a node $v \\in V ( T G )$ a=nd a t=ime p=oint( $t \\in$ t)ime $( T G )$ a timestamped nodâˆˆe $( v , t )$ )and we let $t â€“ n o d e s ( T G )$ b(e th)e set of all timestampe(d no)des in $T G$ . For the sa(ke of) a clear presentation we assume that edges are not labelled.\n\n![](images/7366f7ad86cb5782648cc7fbe856e8b84112ff7aed714778d1b92ad41e804f97.jpg)  \nFigure 2: Relative expressive power of MP-TGNNs   \nFigure 3: A temporal graph in the snapshot representation\n\nWe say that a temporal graph is colour-persistent if initial colours of nodes do not change in time, so $c _ { i } ( v ) \\ = \\ c _ { j } ( v )$ for each node $v$ and all $i , j \\in \\{ 1 , \\ldots , n \\}$ , see F(ig)ur=e 4 (a ). Colour-persistent graphs c nâˆˆ b{e also re}presented as static edge-labelled multi graphs, called aggregated form (Gao and Ribeiro 2022), as depicted in Figure 4 (b).\n\nTemporal graph neural networks with message-passing. We let a message-passing temporal graph neural network (MP-TGNN) be a model $\\mathcal { A }$ which, given a temporal graph $T G$ , computes embeddingAs for all timestamped nodes by implementing a temporal variant of message-passing. Embeddings are then used to predict links or classify nodes and graphs. Some models (e.g. TGAT, NAT, and TGN) apply temporal message-passing to arbitrary temporal graphs, whereas others (e.g. TDGNN) are applicable to colourpersistent temporal graphs (or equivalently, to the aggregated representation) only. Below we present a general form of an MP-TGNN model $\\mathcal { A }$ with $L$ layers, which subsumes a number of message-passiAng mechanisms. Given a temporal graph $T G = ( G _ { 1 } , t _ { 1 } ) , \\ldots , ( G _ { n } , t _ { n } )$ , a model $\\mathcal { A }$ computes for each no=de( $v$ , time) poin t( $t$ , and )layer $\\ell \\in \\{ 0 , \\ldots , L \\}$ an embedding ${ \\bf h } _ { v } ^ { ( \\ell ) } ( t )$ as follows:\n\n![](images/05d1643f38668c02284824dba7b7d98bb9c1d133f9c518455de24384ae048315.jpg)  \nFigure 4: A colour-persistent temporal graph (a) and its aggregated representation (b)\n\n$$\n\\begin{array} { r l } & { \\mathbf { h } _ { v } ^ { \\left( 0 \\right) } ( t ) = \\mathbf { x } _ { v } ( t ) , } \\\\ & { \\mathbf { h } _ { v } ^ { \\left( \\ell \\right) } ( t ) = \\mathsf { C O M } ^ { \\left( \\ell \\right) } \\Big ( \\mathbf { h } _ { v } ^ { \\left( \\ell - 1 \\right) } ( t ) , \\mathsf { A G G } ^ { \\left( \\ell \\right) } \\Big ( } \\\\ & { \\qquad \\quad \\big \\{ \\{ ( \\star , g ( t - t ^ { \\prime } ) ) \\mid ( u , t ^ { \\prime } ) \\in \\mathcal { N } ( v , t ) \\} \\big \\} \\Big ) \\Big ) , } \\end{array}\n$$\n\nwhere:\n\nâ€¢ $\\mathsf { C O M } ^ { ( \\ell ) }$ and $\\mathsf { A G G } ^ { ( \\ell ) }$ are combination and aggregation functions in layer $\\ell$ ; $\\mathsf { C O M } ^ { ( \\ell ) }$ maps a pair of vectors into a single vector, whereas $\\mathsf { A G G } ^ { ( \\ell ) }$ maps a multiset, represented as $\\{ \\{ \\cdots \\} \\}$ , into a single vector,\n\nâ€¢ $g$ maps a time duration into a vector or scalar quantity, â€¢ $\\mathcal { N } ( v , t )$ is the temporal neighbourhood of $( v , t )$ , defined aNs (follo)ws (Souza et al. 2022):\n\n$$\n\\begin{array} { r } { \\mathcal { N } ( v , t ) = \\Big \\{ ( u , t ^ { \\prime } ) \\ | \\ t ^ { \\prime } = t _ { i } \\ \\mathrm { a n d } \\ \\{ u , v \\} \\in E _ { i } , \\ } \\\\ { \\mathrm { f o r ~ s o m e ~ } ( G _ { i } , t _ { i } ) \\in T G \\mathrm { ~ w i t h ~ } t _ { i } \\leq t \\Big \\} . } \\end{array}\n$$\n\nHence, $\\mathcal { N } ( v , t )$ is the set of timestamped nodes $( u , t ^ { \\prime } )$ such thatNt(here)is an edge between $u$ and $v$ at $t ^ { \\prime } \\leq t$ (.\n\nRecently 1-WL has been applied to knowledge graphs $K G = ( \\dot { V _ { \\star } } E , R , c )$ where $V$ are nodes, $E \\subseteq R \\times V \\times V$ are direct=ed( edges wit)h labels from $R$ , and $c : V \\to D$ Ã—colours nodes (Huang et al. 2023; BarcelÃ³ et al. 2âˆ¶02 .â†’An isomorphism between knowledge graphs $K G _ { 1 } = \\left( V _ { 1 } , E _ { 1 } , R _ { 1 } , c _ { 1 } \\right)$ and $K G _ { 2 } = \\left( V _ { 2 } , E _ { 2 } , R _ { 2 } , c _ { 2 } \\right)$ is any bijec=t o(n $f : V _ { 1 } \\to V _ { 2 }$ such that, =o (all $u , v \\in V _ { 1 }$ an)d $r \\in R _ { 1 } ;$ : (i) $c _ { 1 } ( v ) = c _ { 2 } ( f ( v ) )$ and (ii) $( r , u , v ) \\in E _ { 1 }$ if and onlâˆˆy if $( r , f ( u ) , f ( v ) ) \\in E _ { 2 }$ . A relatio(nal loc)alâˆˆ1-WL algorithm (co(nven(tio)nall(y $\\mathsf { r w l } _ { 1 }$ , but we write rwl) is a natural extension of 1-WL to the case of knowledge graphs (Huang et al. 2023). Given a knowledge graph $K G = \\left( V , E , R , c \\right)$ , the algorithm computes iteratively, for all $v \\in V$ and $\\ell \\in \\mathbb { N }$ , values $\\mathsf { r w l } ^ { ( \\ell ) } ( v )$ as follows:\n\nâ€¢ $\\bigstar$ is either $\\mathbf { h } _ { u } ^ { ( \\ell - 1 ) } ( t ^ { \\prime } )$ or $\\mathbf { h } _ { u } ^ { ( \\ell - 1 ) } ( t )$ . If $\\star = \\mathbf { h } _ { u } ^ { ( \\ell - 1 ) } ( t ^ { \\prime } )$ wâ˜€e say that $\\mathcal { A }$ is a (glo)bal (in tim(e))TGNâ˜€N, =as comp(uta)- tion of ${ \\bf h } _ { v } ^ { ( \\ell ) } ( t )$ requires aggregation of embeddings in all past time  (in)ts $t ^ { \\prime }$ ; global TGNNs are also called Temporal Embedding TGNNs (Longa et al. 2023) and include TGAT and NAT. If $\\star = \\mathbf { h } _ { u } ^ { ( \\ell - 1 ) } ( t )$ we say that $\\mathcal { A }$ is local, as only embeddings from the current time point $t$ are aggregated; local TGNNs include TGN and TDGNN.\n\n$$\n\\begin{array} { r l } & { \\mathsf { r w l } ^ { ( 0 ) } ( v ) = c ( v ) , } \\\\ & { \\mathsf { r w l } ^ { ( \\ell ) } ( v ) = \\tau \\Big ( \\mathsf { r w l } ^ { ( \\ell - 1 ) } ( v ) , } \\\\ & { \\qquad \\quad \\{ \\{ ( \\mathsf { r w l } ^ { ( \\ell - 1 ) } ( u ) , r ) \\mid u \\in \\mathcal { N } _ { r } ( v ) , r \\in R \\} \\} \\Big ) , } \\end{array}\n$$\n\nWeisfeiler-Leman algorithm. An isomorphism between undirected node-coloured graphs $G _ { 1 } ~ = ~ \\left( V _ { 1 } , E _ { 1 } , c _ { 1 } \\right)$ and $G _ { 2 } ~ = ~ ( V _ { 2 } , E _ { 2 } , c _ { 2 } )$ is any bijection $f : V _ { 1 } \\to V _ { 2 }$ , sa)tisfying f=or(any $u$ and) $v$ : (i) $c _ { 1 } ( v ) = c _ { 2 } ( f ( v ) )$ â†’d (ii) $\\{ u , v \\} \\in$ $E _ { 1 }$ if and only if $\\{ f ( u ) , f ( v ) \\} \\in E _ { 2 }$ (. )T)he $\\jmath$ -dime{nsion}aâˆˆl Weisfeiler-Leman a{lgo(rit)hm( 1)-}WâˆˆL) (Weisfeiler and Leman 1968) is a powerful heuristic for graph isomorphism (Babai and Kucera 1979), which has the same expressive power as MP-GNNs with injective aggregation and combination (Morris et al. 2019; Xu et al. 2019).\n\nwhere $\\mathcal { N } _ { r } ( v ) = \\{ u \\ | \\ ( r , u , v ) \\in E \\}$ is the $r$ -neighbourhood of $v$ , and $\\tau$ is an injective function. It is shown that rwl has the same expressive power as R-MPNNs, that is, MP-GNNs processing knowledge graphs (Huang et al. 2023).\n\n# Related Work\n\nThere is recently an increasing interest in temporal and dynamic graph neural networks (Longa et al. 2023; Qin and Yeung 2024; Skarding, Gabrys, and Musial 2021; Kazemi et al. 2020). Pertinent models include TGN (Rossi et al. 2020), TGAT (Xu et al. 2020), TDGNN (Qu et al. 2020), and NAT (Luo and Li 2022), which are all based on temporal message-passing mechanisms.\n\nExpressive power results for temporal models are very limited. Souza et al. (2022) compared expressive power of temporal graph neural networks exploiting temporal walks, with those based on local message passing combined with recurrent memory modules. Gao and Ribeiro (2022) compared time-and-graph with time-then-graph models, which are obtained by different combinations of static graph neural networks and recurrent neural networks. In the context of temporal knowledge graphs, expressive power of similar models was recently considered by Chen and Wang (2023).\n\nMore mature results have been established for models processing edge-labelled graphs. Such graphs are closely related to temporal graphs, since the aggregated representation of a temporal graph (Gao and Ribeiro 2022), Figure 4 (b), is a multigraph with edges labelled by time points. However, since the aggregated representation does not allow us to assign different colours to the same node in different time points, not all temporal graphs can be directly transformed into multigraphs. BarcelÃ³ et al. (2022) introduced 1-WL for models processing undirected multirelational graphs, whereas Beddar-Wiesing et al. (2024) introduced 1-WL for dynamic graphs. Huang et al. (2023) proposed 1-WL for models processing directed multi-relational graphs (i.e. knowledge graphs), namely for relational message passing neural networks (R-MPNNs), which encompass several known models such as RGCN (Schlichtkrull et al. 2018) and CompGCN (Vashishth et al. 2020).\n\nTemporal graphs can be also given in the event-based representation (Longa et al. 2023), as a sequence of timestamped events that add/delete edges or modify feature vectors of nodes. Since temporal graphs in the aggregated and event-based representations can be transformed into the snapshot representation (Gao and Ribeiro 2022; Longa et al. 2023), we focus on the snapshot representation in the paper.\n\n# Temporal Weisfeiler-Leman Characterisation\n\nWe provide a general approach for establishing expressive power of MP-TGNNs using standard 1-WL. To do so, we transform a temporal graph $T G$ into a knowledge graph $K G$ such that MP-TGNNs can distinguish exactly those nodes in $T G$ whose counterparts in $K G$ can be distinguished by the standard 1-WL. This contrasts with approaches studying expressive power by modifying 1-WL for particular types of temporal graph neural networks (Souza et al. 2022; Gao and Ribeiro 2022). Note that our results concern distinguishability of nodes, not graphs. Node distinguishability is likely of more practical interest and can be used to distinguish graphs.\n\nWe transform $T G$ into two knowledge graphs: $\\kappa _ { \\mathsf { g l o b } } ( T G )$ and $\\textstyle \\mathcal { K } _ { \\mathrm { l o c } } ( T G )$ , suitable for analysing, respectivKely, g(loba)l and oKcal(MP-)TGNNs. We first introduce $\\kappa _ { \\mathrm { g l o b } } ( T G )$ , whose edges correspond to temporal message -Kpassi(ng in) global MP-TGNNs (Figure 5). Intuitively, $\\kappa _ { \\mathsf { g l o b } } ( T G )$ contains a separate node $( v , t )$ for each timestamKped(node) in $T G$ and an edge betwee(n $( v , t )$ and $( u , t ^ { \\prime } )$ labelled by $t - t ^ { \\prime }$ if $( u , t ^ { \\prime } )$ is in the temporal(neig)hbour(hood)of $( v , t )$ .\n\n![](images/09108853621be1436cebbffd00d51332ab5fd34af02ef895a0c972d7256f40ba.jpg)  \nFigure 5: $\\kappa _ { \\mathsf { g l o b } } ( T G )$ constructed for $T G$ from Figure 3\n\nDefinition 1. Let $T G = ( G _ { 1 } , t _ { 1 } ) , \\dots , ( G _ { n } , t _ { n } )$ be a temporal graph with $G _ { i } ~ = ~ ( V _ { i } , E _ { i } , c _ { i } )$ . We( define )a knowledge graph $\\mathcal { K } _ { \\mathrm { g l o b } } ( T G ) = \\left( V , E , R , c \\right)$ with components:\n\nâ€¢ $V = t { - } n o d e s ( T G )$ ,   \nâ€¢ $E = \\big \\{ \\big ( t _ { j } - t _ { i } , \\big ( v , t _ { i } \\big ) , \\big ( u , t _ { j } \\big ) \\big ) \\big | i \\leq j a n d \\big \\{ u , v \\big \\} \\in E _ { i } \\big \\} ,$ â€¢ $R = \\{ 0 , \\ldots , n - 1 \\}$ ,   \nâ€¢ $c : V \\to R$ satisfies1 $c ( v , t _ { i } ) = c _ { i } ( v )$ , for all $( v , t _ { i } ) \\in V$ .\n\nWe use $\\kappa _ { \\mathrm { g l o b } } ( T G )$ to bridge the expressive power of global MP-KTGN(Ns a)nd 1-WL. First we show that global MP-TGNNs cannot distinguish more timestamped nodes over $T G$ than 1-WL over $\\bar { \\kappa _ { \\mathrm { g l o b } } } ( T G )$ .\n\nTheorem 2. For any temporal graph $T G$ , any timestamped nodes $( v , t )$ and $( u , t ^ { \\prime } )$ in $T G$ , and any $\\ell \\in \\mathbb { N }$ :\n\n1for brevity we will drop double brackets, e.g. from $c ( ( v , t _ { i } ) )$\n\nProof sketch. By induction on $\\ell$ . The base case holds since ${ \\sf r w l } ^ { ( 0 ) } ( v , t _ { i } ) = c ( v , t _ { i } ) = c _ { i } ( v ) = { \\bf h } _ { v } ^ { ( 0 ) } \\left( t _ { i } \\right)$ , for each $( v , t _ { i } )$ in $T G$ (. Th)e =ind(uctive) =step pr)o=ceed i(n a similar w(ay to) GNNs (Morris et al. 2019), but additionally exploits the following key property of $\\mathcal { K } _ { \\mathrm { g l o b } } ( T G )$ : $( u , t ^ { \\prime } ) \\ \\in \\ \\mathcal { N } _ { r } ( v , t )$ in $\\kappa _ { \\mathrm { g l o b } } ( T G )$ iff $\\bar { \\boldsymbol { r } } = \\dot { \\boldsymbol { t } } - \\boldsymbol { t } ^ { \\prime }$ Kand $( u , t ^ { \\prime } ) \\in \\mathcal { N } ( v , t )$ iNn $T G$ ,)for aKll tim(esta)mped n=odesâˆ’ $( v , t )$ an(d $( u , t ^ { \\prime } )$ iNn $T G$ .) â–¡\n\nMoreover, we can show the opposite direction: there is a global MP-TGNN distinguishing exactly the same nodes over $T G$ as 1-WL over $\\kappa _ { \\mathsf { g l o b } } ( T G )$ . By Theorem 2 each global MP-TGNN is no Kmore(expr)essive than 1-WL over $\\bar { \\kappa } _ { \\mathrm { g l o b } } ( T G )$ , so for the next theorem it suffices to construct an KMP-T(GNN) at least as expressive as 1-WL over $\\kappa _ { \\mathsf { g l o b } } ( T G )$ .\n\nTheorem 3. For any temporal graph $T G$ and any $L \\in \\mathbb { N }$ , there exists a global MP-TGNN $\\mathcal { A }$ with $L$ layers suchâˆˆthat for all timestamped nodes $( v , t ) , ( u , t ^ { \\prime } )$ in $T G$ and all $\\ell \\leq L$ the following are equivalen(t:\n\n$$\n\\begin{array} { r l } & { \\bullet { \\bf \\Upsilon } { \\bf h } _ { v } ^ { ( \\ell ) } ( t ) = { \\bf h } _ { u } ^ { ( \\ell ) } ( t ^ { \\prime } ) i n { \\cal A } , } \\\\ & { \\bullet { \\bf \\Upsilon } { \\bf r } { \\bf w } | ^ { ( \\ell ) } ( v , t ) = { \\bf r } { \\bf w } | ^ { ( \\ell ) } ( u , t ^ { \\prime } ) i n \\mathcal { K } _ { \\mathrm { g l o b } } ( T G ) . } \\end{array}\n$$\n\nProof sketch. The important part of the proof is for the forward implication, as the other implication follows from Theorem 2. We use the result of Huang et al. (2023)[Theorem A.1] showing that for any knowledge graph and in particular $\\kappa _ { \\mathsf { g l o b } } ( T G )$ , there is a relational message-passing neural netwKork(R-M)PNN) model $\\boldsymbol { B }$ such that if two nodes $( v , t )$ and $( u , t ^ { \\prime } )$ of $\\kappa _ { \\mathsf { g l o b } } ( T G )$ havBe the same embeddings (at a )layer $\\ell$ , we get $\\mathsf { r w l } ^ { ( \\ell ) } ( v , t ) = \\mathsf { r w l } ^ { ( \\ell ) } ( u , t ^ { \\prime } )$ . Huang et al.â€™s model $\\boldsymbol { B }$ computes $\\overline { { \\mathbf { h } } } _ { ( v , t ) } ^ { ( \\ell ) }$ as follows: $\\overline { { \\mathbf { h } } } _ { ( v , t ) } ^ { ( 0 ) } = c ( v , t )$ and $\\overline { { \\mathbf { h } } } _ { ( v , t ) } ^ { ( \\ell ) } =$ sign W(â„“) h(â„“v,âˆ’t1) r R u,tâ€² r v,t Î±rh(â„“uâˆ’,t1â€²) To finish the proof, we construct a global MP-TGNN such that ${ \\bf h } _ { v } ^ { ( \\ell ) } ( t )$ computed by $\\mathcal { A }$ on $T G$ coincide with $\\overline { { \\mathbf { h } } } _ { ( v , t ) } ^ { ( \\ell ) }$ computed by $\\boldsymbol { B }$ on $\\kappa _ { \\mathsf { g l o b } } ( T G )$ . We obtain it by setting in Equation (2) functions $\\mathsf { A G G } ^ { ( \\ell ) }$ to the sum and $\\mathsf { C O M } ^ { ( \\ell ) }$ to the sign of a particular linear combination. â–¡\n\nNext, we show that we can also construct a knowledge graph representing message-passing in local MP-TGNNs. In contrast to $\\kappa _ { \\mathsf { g l o b } } ( T G )$ , edges of the new knowledge graph $\\textstyle \\mathcal { K } _ { \\mathrm { l o c } } ( T G )$ aKre bi(direc)tional and hold only between nodes sKtam(ped )with the same time. Such a knowledge graph is presented in Figure 6 and formally defined below.\n\n![](images/bb1cf28ed6894946ce4fc653b6679152faafa55c1c99636b23689ad3495d9dce.jpg)  \nFigure 6: Knowledge graph $\\textstyle \\mathcal { K } _ { \\mathrm { l o c } } ( T G )$ for $T G$ from Figure 3\n\nDefinition 4. Let $T G = ( G _ { 1 } , t _ { 1 } ) , \\dots , ( G _ { n } , t _ { n } )$ be a temporal graph with $G _ { i } ~ = ~ ( V _ { i } , E _ { i } , c _ { i } )$ . We( define )a knowledge knowledge graph ${ \\mathcal { K } } _ { \\sf l o c } ( T G ) = ( V , E , R , c )$ with:\n\nâ€¢ $V = t { - } n o d e s ( T G )$ ,   \nâ€¢ $E = \\big \\{ \\big ( t _ { j } - t _ { i } , \\big ( v , \\dot { t } _ { j } \\big ) , ( u , t _ { j } ) \\big ) \\big | i \\le j a n d \\big \\{ u , v \\big \\} \\in E _ { i } \\big \\} ,$ â€¢ $R = \\{ 0 , \\ldots , n - 1 \\}$ ,   \nâ€¢ $c : V \\to R$ satisâˆ’fies} $c ( v , t _ { i } ) = c _ { i } ( v )$ , for all $( v , t _ { i } ) \\in V$ .\n\nWe can show that local MP-TGNNs can distinguish exactly the same nodes in $T G$ as rwl can distinguish in $\\textstyle \\bigwedge _ { \\mathrm { l o c } } ( T G )$ , as formally stated in the following two theorems.\n\nTheorem 5. For any temporal graph $T G$ , any timestamped nodes $( v , t )$ and $( u , t ^ { \\prime } )$ in $T G$ , and any $\\ell \\in \\mathbb { N }$ :\n\nâ€¢ $I f \\mathsf { r w l } ^ { ( \\ell ) } ( v , t ) = \\mathsf { r w l } ^ { ( \\ell ) } ( u , t ^ { \\prime } )$ in $\\textstyle \\sum _ { \\mathrm { l o c } } ( T G )$ , â€¢ then $\\mathbf { h } _ { v } ^ { ( \\ell ) } ( t ) = \\mathbf { h } _ { u } ^ { ( \\ell ) } ( t ^ { \\prime } )$ in any local MP-TGNN.\n\nTheorem 6. For any temporal graph $T G$ and any $L \\in \\mathbb { N }$ , there exists a local MP-TGNN $\\mathcal { A }$ with $L$ layers such thaâˆˆt for all timestamped nodes $( v , t ) , ( u , t ^ { \\prime } )$ in $T G$ and all $\\ell \\leq L$ , the following are equivalent:\n\nâ€¢ $\\mathbf { h } _ { v } ^ { ( \\ell ) } ( t ) = \\mathbf { h } _ { u } ^ { ( \\ell ) } ( t ^ { \\prime } )$ in $\\mathcal { A } ,$ , â€¢ rw ) v, t wl(â„“) u, tâ€² in loc T G .\n\nThe Weisfeiler-Leman characterisation of global and local MP-TGNNs established in the above theorems provides us with a versatile tool for analysing expressive power, which we will intensively apply in the following parts of the paper.\n\n# Timewise Isomorphism\n\nWhile message-passing GNNs (corresponding to 1-WL) provide us with a heuristic for graph isomorphism, their temporal extensions can be seen as heuristics for isomorphism between temporal graphs. However, in the temporal setting it is not clear what notion of isomorphism we should use to obtain an analogous correspondence. We use the characterisation from the previous section to show, quite surprisingly, that both global and local MP-TGNNs can distinguish nodes which are pointwise isomorphicâ€”called isomorphic by Beddar-Wiesing et al. (2024). This observation leads us to definition of timewise isomorphism as a suitable notion for node indistinguishability in temporal graphs.\n\nPointwise isomorphism requires that pairs of corresponding snapshots in two temporal graphs are isomorphic. For example $( a , t _ { 2 } )$ in $T G$ and $( a ^ { \\prime } , t _ { 2 } )$ in $T G ^ { \\prime }$ from Figure 7 are pointwise( isom)orphic since( $f _ { 1 }$ wi)th $f _ { 1 } ( a ) = b ^ { \\prime }$ , ${ \\tilde { f _ { 1 } } } ( b ) = c ^ { \\prime }$ , and $f _ { 1 } ( c ) = a ^ { \\prime }$ is an isomorphism betw(ee)n $G _ { 1 }$ and $G _ { 1 } ^ { \\prime }$ ,=and $f _ { 2 }$ with( $f _ { 2 } ( a ) = a ^ { \\prime }$ , $f _ { 2 } ( b ) = { \\overline { { b } } } ^ { \\prime }$ , and $f _ { 2 } ( c ) = c ^ { \\prime }$ is an isomorphism betw(ee)n $G _ { 2 }$ and( $G _ { 2 } ^ { \\prime }$ .=A formal d(ef)in=ition is below.\n\n![](images/3058dbd46c0e04761a9eba52a6775ac360d3d547b501c204253d3f4c8d9ae5f5.jpg)  \nFigure 7: Pointwise isomorphic $( a , t _ { 2 } )$ and $( a ^ { \\prime } , t _ { 2 } )$\n\nDefinition 7. Temporal graphs $T G = ( G _ { 1 } , t _ { 1 } ) , \\dots , ( G _ { n } , t _ { n } )$ and $T G ^ { \\prime } = \\left( G _ { 1 } ^ { \\prime } , t _ { 1 } ^ { \\prime } \\right) , \\dots , \\left( G _ { m } ^ { \\prime } , t _ { m } ^ { \\prime } \\right)$ (are poi)ntwise(isomor)- phic if both of the following hold:\n\nâ€¢ $\\mathsf { t i m e } ( T G ) = \\mathsf { t i m e } ( T G ^ { \\prime } )$ (so $n = m$ and $t _ { i } = t _ { i } ^ { \\prime }$ for all $i \\in \\{ 1 , \\ldots , n \\} ,$ )   \nâ€¢ foâˆˆr{every $i \\in \\{ 1 , \\ldots , n \\}$ there exists an isomorphism $f _ { i }$ between $G _ { i }$ âˆˆan{d $G _ { i } ^ { \\prime }$ .\n\nIf this is the case and $f _ { i } ( v ) = u ,$ , we say that $( v , t _ { i } )$ and $( u , t _ { i } )$ are pointwise isom(or)phi=c.\n\nIt turns out that both global and local MP-TGNNs can distinguish pointwise isomorphic nodes. In particular, they can distinguish $( a , t _ { 2 } )$ and $( \\bar { a ^ { \\prime } } , t _ { 2 } )$ from Figure 7, as we show below using(Theo)rem 3(and T)heorem 6.\n\nTheorem 8. There are temporal graphs T G and $T G ^ { \\prime }$ with pointwise isomorphic $( v , t )$ and $( u , t ^ { \\prime } )$ such that ${ \\bf h } _ { v } ^ { ( 1 ) } ( t ) \\neq$ $\\mathbf { h } _ { u } ^ { ( 1 ) } ( t ^ { \\prime } )$ for some glob(al an)d loca(l MP)-TGNNs.\n\nProof sketch. Consider $T G$ and $T G ^ { \\prime }$ from Figure 7, where $( a , t _ { 2 } )$ is pointwise isomorphic to $( a ^ { \\prime } , t _ { 2 } )$ . If we apply rwl to $\\kappa _ { \\mathsf { g l o b } } ( T G )$ and $\\kappa _ { \\mathrm { g l o b } } ( T G ^ { \\prime } )$ , $\\mathsf { r w l } ^ { ( 1 ) } ( a , t _ { 2 } ) \\neq \\mathsf { r w l } ^ { ( 1 ) } ( a ^ { \\prime } , t _ { 2 } )$ , bKecau(se $( a , t _ { 2 } )$ hKas o(ne inc)oming e(dge i)n $\\kappa _ { \\mathsf { g l o b } } ( T G )$ , bu)t $( a ^ { \\prime } , t _ { 2 } )$ h(as no) incoming edges in $\\mathcal { K } _ { \\mathsf { g l o b } } ( T G ^ { \\prime } )$ . (The )same (holds i)f we apply rwl to $K _ { \\mathrm { g l o b } } ( T G ^ { \\prime } )$ an(d $\\kappa _ { \\mathsf { l o c } } ( T G ^ { \\prime } )$ . So, by Theorem 3 and Theo eKm 6,( there) are glKoba(l and)local MP-TGNNs in which ${ \\bf h } _ { a } ^ { ( 1 ) } ( t _ { 2 } ) \\neq { \\bf h } _ { a ^ { \\prime } } ^ { ( 1 ) } ( t _ { 2 } )$ . â–¡\n\nTheorem 8 shows that pointwise isomorphism is unsuitable for detecting node indistinguishability in MP-TGNNs. We obtain an adequate isomorphism notion by, on the one hand, requiring additionally that all $f _ { i }$ mentioned in the definition of pointwise isomorphism coincide but, on the other hand, relaxing the requirement tim $\\mathsf { \\Omega } _ { : } ( T G ) = \\mathsf { t i m e } ( T G ^ { \\prime } )$ .\n\nDefinition 9. Temporal graphs $T G = ( G _ { 1 } , t _ { 1 } ) , \\dots , ( G _ { n } , t _ { n } )$ and $T G ^ { \\prime } ~ = ~ ( \\bar { G _ { 1 } ^ { \\prime } } , \\bar { t _ { 1 } ^ { \\prime } } ) , \\dots , ( \\bar { G _ { m } ^ { \\prime } } , t _ { m } ^ { \\prime } )$ (are tim)ewise(isomor)- phic if both of the following hold:\n\nâ€¢ $n = m$ and $t _ { i + 1 } - t _ { i } = t _ { i + 1 } ^ { \\prime } - t _ { i } ^ { \\prime } ,$ , for every $i \\in \\left\\{ 1 , \\ldots , n - 1 \\right\\}$ , â€¢ the=re exists a+ fâˆ’unct=ion+ $f$ âˆ’which is an isâˆˆo{morphismâˆ’be}- tween $G _ { i }$ and $G _ { i } ^ { \\prime }$ , for every $i \\in \\{ 1 , \\ldots , n \\}$ .\n\nIf $f ( v ) ~ = ~ u$ , we say that $( v , t _ { i } )$ and $( u , t _ { i } ^ { \\prime } )$ are timewise isomorphic, for any $t _ { i } \\in \\mathsf { t i m e } ( T G )$ .\n\nNext we show that the timewise isomorphism is an adequate notion for timestamped nodes indistinguishability since timestamped nodes which are timewise isomorphic cannot be distinguished by any (global or local) MP-TGNN.\n\nTheorem 10. If $( v , t )$ and $( u , t ^ { \\prime } )$ are timewise isomorphic, then $\\mathbf { h } _ { v } ^ { ( \\ell ) } ( t ) = \\mathbf { h } _ { u } ^ { ( \\ell ) } ( t ^ { \\prime } )$ in an(y M)P-TGNN and any $\\ell \\in \\mathbb { N }$ .\n\nProof sketch. Assume that $( v , t )$ from $T G$ and $( u , t ^ { \\prime } )$ from $T G ^ { \\prime }$ are timewise isomorph(ic. H) ence, by Defini(tion )9, $T G$ and $T G ^ { \\prime }$ are of the forms $T G = ( G _ { 1 } , t _ { 1 } ) , \\dots , ( G _ { n } , t _ { n } )$ and $T G ^ { \\prime } = ( G _ { 1 } ^ { \\prime } , t _ { 1 } ^ { \\prime } ) , \\dots , ( G _ { n } ^ { \\prime } , t _ { n } ^ { \\prime } )$ =as( well a)s $t = t _ { i }$ and $t ^ { \\prime } = t _ { i } ^ { \\prime }$ for so=m(e $i \\in \\{ 1 , \\ldots , n \\}$ . M)oreover, $f ( v ) \\ = \\ u$ for so=me $f : V ( T G ) \\to V ( T G ^ { \\prime } )$ s}atisfying requi(re)me=nts in Definitioâˆ¶n 9(. We)deâ†’fine( $f ^ { \\prime } : \\dot { t } \\ â€“ n o d e \\dot { s } ( T \\dot { G } ) \\stackrel { - } {  } t \\ â€“ n o d e s ( T G ^ { \\prime } )$ such that $f ^ { \\prime } ( w , t _ { j } ) = ( f ( w ) , t _ { j } ^ { \\prime } )$ fo(r all $( w , t _ { j } ) \\in t \\ â€“ n o d e s ( T G )$ . We can show that $f ^ { \\prime }$ is an isomorphism between knowledge graphs $\\kappa _ { \\mathrm { g l o b } } ( T G )$ and $\\mathcal { K } _ { \\mathrm { g l o b } } ( T G ^ { \\prime } )$ , as well as between $\\mathcal { K } _ { \\mathrm { l o c } } ( T \\breve { G } )$ (and $\\mathcal { K } _ { \\mathrm { l o c } } ( T G ^ { \\vee } )$ . Henc)e, in both cases $f ^ { \\prime } ( v , t ) = ( u , t ^ { \\prime } )$ implies ${ \\sf r w l } ^ { ( \\ell ) } ( v , t ) = { \\sf r w l } ^ { ( \\ell ) } ( u , t ^ { \\prime } )$ , for all $\\ell \\in \\mathbb { N }$ . Thus, by Theorem 2 and Theorem 5, we obtain that $\\mathbf { h } _ { v } ^ { ( \\ell ) } ( t ) = \\mathbf { h } _ { u } ^ { ( \\ell ) } \\dot { ( t ^ { \\prime } ) }$ for any global and local MP-TGNNs. â–¡\n\n# Relative Expressiveness of Temporal Message Passing Mechanisms\n\nIn this section we will use temporal Weisfeiler-Leman characterisation to prove expressive power results summarised in Figure 2. Our results are on the discriminative (also called separating) power, which aims to determine if a given type of models is able to distinguish two timestamped nodes. Formally, we say that a model distinguishes a timestamped node $( v , t )$ in a temporal graph $T G$ from $( u , t ^ { \\prime } )$ in $T G ^ { \\prime }$ if this (mod)el computes different embedding  (or $( v , t )$ and $( u , t ^ { \\prime } )$ at some layer $\\ell$ , that is, $\\mathbf { h } _ { v } ^ { ( \\ell ) } ( t ) \\neq \\mathbf { h } _ { u } ^ { ( \\ell ) } ( t ^ { \\prime } )$ . (We s)ay th(at a ty)pe of models (e.g. global or (oc)alâ‰ MP T(G)NNs) can distinguish $( v , t )$ from $( u , t ^ { \\prime } )$ , if some model of this type distinguishes $( v , t )$ from $( u , t ^ { \\prime } )$ .\n\nW)e start (by sh)owing that global MP-TGNNs can distinguish timestamped nodes which are indistinguishable by local MP-TGNNs. The reason is that in a global MP-TGNN an embedding of $( v , t )$ can depend on embeddings at $t ^ { \\prime } < t$ , but this cannot ha(ppen)in a local MP-TGNN.\n\nTheorem 11. There are timestamped nodes that can be distinguished by global, but not by local MP-TGNNs.\n\nProof sketch. Consider $( b , t _ { 4 } )$ from $T G$ in Figure 3 and $( b , t _ { 4 } )$ from $T G ^ { \\prime }$ in Figu(re 4 ()a). We can show that, for any $\\ell \\in \\mathbb { N }$ , application of $\\mathsf { r w l } ^ { ( \\ell ) }$ to $\\kappa _ { \\mathrm { l o c } } ( T G )$ and $\\kappa _ { \\mathsf { l o c } } ( T G ^ { \\prime } )$ assiâˆˆgns the same labels to theseKtim(esta )mped nKode(s. He)nce, by Theorem 5, local MP-TGNNs cannot distinguish these nodes. On the other hand, for any $\\ell \\geq 1$ , application of $\\mathsf { r w l } ^ { ( \\ell ) }$ to $\\kappa _ { \\mathrm { g l o b } } ( T G )$ and $\\kappa _ { \\mathrm { g l o b } } ( T G ^ { \\prime } )$ assigns different labels to thesKe no(des. )There oKre, b(y The)orem 3, global MP-TGNNs can distinguish these nodes. â–¡\n\nBased on the observation from Theorem 11, one could expect that global MP-TGNNs are strictly more expressive than local MP-TGNN. Surprisingly, this is not the case. Indeed, as we show next, there are timestamped nodes which can be distinguished by local, but not by global MP-TGNNs.\n\nTheorem 12. There are timestamped nodes that can be distinguished by local, but not by global MP-TGNNs. This holds true even for colour-persistent temporal graphs.\n\nProof sketch. Consider $( a , t _ { 2 } )$ from $T G$ and $( a ^ { \\prime } , t _ { 2 } )$ from $T G ^ { \\prime }$ in Figure 8. Obse v(e tha)t $( a , t _ { 2 } )$ in $\\kappa _ { \\mathsf { g l o b } } ( T G )$ )is isomorphic to $( a ^ { \\prime } , t _ { 2 } )$ in $K _ { \\mathrm { g l o b } } ( T G ^ { \\prime } )$ so,)by TKheor(em 2,) $( a , t _ { 2 } )$ and $( a ^ { \\prime } , t _ { 2 } )$ (canno)t beKdistin(guish)ed by global MP-T(GNNs). How(ever, ) $( a , t _ { 2 } )$ has one outgoing path of length 2 in $\\textstyle { \\mathcal { K } } _ { \\mathrm { l o c } } ( T G )$ ,(but n)ot in $\\kappa _ { \\mathrm { l o c } } ( T G ^ { \\bar { \\prime } } )$ . Hence, two iterations of rKwl d(istin)guish these noKdes.(Thus), by Theorem 6, $( a , t _ { 2 } )$ and $( a ^ { \\prime } , t _ { 2 } )$ can be distinguished by local MP-TGNNs(. Note) that $T G$ an)d $T G ^ { \\prime }$ are colour-persistent. â–¡\n\nTheorem 11 and Theorem 12 show us that neither global or local MP-TGNNs are strictly more expressive, when compared over all temporal graphs. Does the same result hold over colour-persistent graphs? Interestingly, it is not the case: in colour-persistent graphs local MP-TGNNs are strictly more expressive than global MP-TGNNs. Hence Theorem 11 cannot hold for colour-persistent graphs.\n\n![](images/e6ec572653914319331e88999e4809e93d001b36a3970bb68eae5bf5b2abc907.jpg)  \nFigure 8: $( a , t _ { 2 } )$ and $( a ^ { \\prime } , t _ { 2 } )$ which cannot be distinguished by global (but c)an be (disting)uished by local MP-TGNNs\n\nTheorem 13. In colour-persistent graphs local MP-TGNNs are strictly more expressive than global MP-TGNNs.\n\nProof sketch. Due to the result established in Theorem 12, it remains to show that over colour-persistent temporal graphs, if $( v , t )$ and $( u , t ^ { \\prime } )$ can be distinguished by global MP-TGNN(s, t)hen th(ey ca)n be distinguished also by local MP-TGNNs. Hence, by Theorem 2 and Theorem 6, we need to show the $\\mathsf { r w l } _ { \\mathsf { g l o b } } ^ { ( \\ell ) } ( v , t ) \\ \\neq \\ \\mathsf { r w l } _ { \\mathsf { g l o b } } ^ { ( \\ell ) } ( u , t ^ { \\prime } )$ implies $\\mathsf { r w l } _ { \\mathsf { l o c } } ^ { ( \\ell ) } ( v , t ) \\neq \\mathsf { r w l } _ { \\mathsf { l o c } } ^ { ( \\ell ) } ( u , t ^ { \\prime } )$ for all $\\ell \\in \\mathbb { N }$ . We show this implicat i(on in)dâ‰ uctively(on $\\ell$ ,)where theâˆˆinductive step requires proving several non-trivial statements, for example, showing (by another induction) that ${ \\sf r w l } ^ { ( \\ell ) } ( v , t ) \\neq { \\sf r w l } ^ { ( \\ell ) } ( u , t ^ { \\prime } )$ implies ${ \\sf r w l } ^ { ( \\ell ) } ( v , t + k ) \\neq { \\sf r w l } ^ { ( \\ell ) } ( u , t ^ { \\prime } + k )$ , )forâ‰ any $k$ . ( â–¡\n\nTo finish the expressive power landscape announced in Figure 2, it remains to make two more observations. On the one hand, temporal graphs which are not colour-persistent allow global MP-TGNNs to distinguish more elements than colour-persistent graphs. Indeed, this is the case since global MP-TGNNs allow us to pass information about colours between nodes stamped with different time points. On the other hand, this is not allowed in local MP-TGNNs, and so colourpersistence does not impact their expressiveness.\n\n# Experiments\n\nWe implement and train basic variants of global and local models on standard temporal link-prediction benchmarks. We emphasise that the goal of our experiments is not to achieve models with high-level performance, but to examine how our expressive power results impact practical performance of MP-TGNNs.\n\nBenchmarks. We use the Temporal Graph Benchmark (TGB) 2.0 suite (Gastinger et al. 2024) with small-tomedium temporal datasets tgbl-wiki, tgbl-review, and tgbl-coin, whose statistics are in Table 1. They do not have node features and we discard the edge features. We consider a link-prediction task, where the goal is to predict whether there is a link between two given nodes at the next time point, given information about all previous links. We follow normative training and evaluation procedures supplied by TGB.\n\nModels. We implement global and local MP-TGNNs with combination and aggregation functions being concatenation $( \\parallel )$ and summation $\\left( \\sum \\right)$ , respectively, which are among standâˆ£aâˆ£rd choices (Rossiâˆ‘and Ahmed 2015; Xu et al. 2019). Hence, embedding ${ \\bf h } _ { v } ^ { ( \\ell ) } ( t )$ is computed as\n\n$$\nW _ { 2 } ^ { ( \\ell ) } [ \\mathbf { h } _ { v } ^ { ( \\ell - 1 ) } ( t ) \\left| \\right| \\sigma ( W _ { 1 } ^ { ( \\ell ) } ( \\sum _ { ( u , t ^ { \\prime } ) \\in \\mathcal { N } ( v , t ) } \\star \\left| \\right| g ( t - t ^ { \\prime } ) ) ) ] ,\n$$\n\nwhere $W _ { 1 }$ and $W _ { 2 }$ are learnable, $\\sigma$ is the rectified linear unit, $\\bigstar$ is either $\\mathbf { h } _ { u } ^ { ( \\ell - 1 ) } ( t ^ { \\prime } )$ (giving rise to a global model) or $\\mathbf { h } _ { u } ^ { ( \\ell - 1 ) } ( t )$ (local mod(el), and $g \\big ( t - t ^ { \\prime } \\big ) = t - t ^ { \\prime }$ . After $\\ell = 4$ layers, a m(u)lti-layer perceptron w(ithâˆ’10)2=4 hiâˆ’dden units p=redicts a link between $u$ and $v$ , given ${ \\bf h } _ { u } ^ { ( \\ell ) } ( t )$ and ${ \\bf h } _ { v } ^ { ( \\ell ) } ( t )$ . Aggregating the entire temporal neighbo r(h)ood incu (ov)er time a linear computational penalty, precluding larger benchmarks; real-world models approximate this calculation.\n\nImplementation. Our implementation2 is based on PyTorch (Paszke et al. 2019), and in particular its hardwareaccelerated scatter operations for temporal aggregation. The use of scatter operations means that results may differ between hardware-accelerated runs.\n\nAs stated, the global model would require enormous compute and memory in order to use node embeddings from all previous time points. In order to make this tractable, we apply a train-time approximation: during an epoch, embeddings from previous timepoints are â€œfrozenâ€: detached from the computation graph and not updated as model weights change. This interferes with training as the model must use a mixture of stale and fresh embeddings, but at test time the result is exact. A further observation is that only those embeddings whose nodes are connected at some time need be computed and retained due to the definition of $\\mathcal { N }$ .\n\nMinibatching can be achieved in the temporal context by predicting the next $k$ links given all previous links. Unlike traditional minibatching, this can have a detrimental impact on model accuracy, because earlier links in the batch may help to predict links later in the batch. However, it is computationally very demanding to set $k = 1$ for even â€œsmallâ€ datasets like tgbl-review contain =g millions of links, so a compromise must be found. We set $k = 3 2$ for tgbl-wiki and $k = 1 0 2 4$ for all others.\n\nTraining We used Adam (Kingma and Ba 2015) for optimisation with PyTorch defaults $\\gamma ~ = ~ 0 . 0 0 1$ , $\\beta _ { 1 } ~ = ~ 0 . 9$ , $\\beta _ { 2 } = 0 . 9 9 9$ , and no L2 penalty. We w e= able to sign i=cantly stab=ilise and accelerate training by normalising $g ( { \\bar { t } } - t ^ { \\prime } )$ with respect to elapsed time and by applying batch(noâˆ’rm)alisation (Ioffe and Szegedy 2015) immediately after summation of temporal neighbours. With the exception of the above stability measures, we have not tuned further as we are not aiming for state-of-the-art performance. Training continued until validation loss failed to improve for 10 epochs. Experiments involving tgbl-wiki and tgbl-review can be run on desktop hardware (NVIDIA GT730), or even without acceleration, whereas tgbl-coin requires a large GPU.\n\nTable 1: Statistics (nodes and edges) and MRR scores   \n\n<html><body><table><tr><td>tgbl-wiki</td><td></td><td>tgbl-review|tgbl-coin</td><td></td></tr><tr><td>nodes edges</td><td>9,227 157,474</td><td>352.637 4,873,540</td><td>638,486 22,809,486</td></tr><tr><td>global local</td><td>0.223 0.264</td><td>0.321 0.359</td><td>0.628 0.635</td></tr></table></body></html>\n\nResults. Table 1 shows the mean reciprocal rank (MRR) score (higher is better) used in TGB. We observe that the scores are relatively high given the simplicity of models and lack of tuning. We have written in bold higher among MRRs obtained by global and local MP-TGNNs. In all three datasets, local MP-TGNN obtains higher scores, but the difference between the scores of local and global models is relatively small. We observe that higher performance of the local model aligns with our theoretical result from Theorem 13, which states that over colour-persistent temporal graphs (as here), local MP-TGNNs are stricly more expressive than global MP-TGNNs.\n\nLayers. We also investigated the effect of the increasing number of layers $\\ell$ on MRR. We performed experiments on tgbl-wiki with the number of layers increasing from 1 to 8. As presented in Figure 9, the highest MRR for the global model is obtained when $\\ell = 5$ and for the local model when $\\ell = 7$ . Interestingly, for $\\ell = 5$ (which is optimal for the globa=l model), MRR for both =models is almost the same. This, again, aligns with our theoretical results, showing that local MP-TGNNs are more expressive than global.\n\n![](images/b5b066f66bccddeeb6124886eabe8a024cb7953c74a91d76d280f9f1e7f9f5ac.jpg)  \nFigure 9: MRR against number of layers on tgbl-wiki\n\nVariations. The lack of node features in benchmarks led us to try both random node features, which did not significantly alter results, and (transductively) learnable node features, which caused drastic overfitting.\n\n# Conclusions\n\nWe have categorised temporal message-passing graph neural networks into global and local, depending on their temporal message passing mechanism. One might expect that global models have higher expressive power than local, but surprisingly we find that the two are incomparable. Further, if node colours (feature vectors) do not change over time, local models are strictly more powerful than global. Our experimental results align with the theoretical findings, showing that local models obtain higher performance on temporal link-prediction tasks.",
    "summary": "```json\n{\n  \"core_summary\": \"### ðŸŽ¯ æ ¸å¿ƒæ¦‚è¦\\n\\n> **é—®é¢˜å®šä¹‰ (Problem Definition)**\\n> *   è®ºæ–‡ç³»ç»Ÿç ”ç©¶äº†æ—¶åºå›¾ç¥žç»ç½‘ç»œï¼ˆTemporal Graph Neural Networks, MP-TGNNsï¼‰ä¸­å…¨å±€å’Œå±€éƒ¨æ¶ˆæ¯ä¼ é€’æœºåˆ¶çš„**è¡¨è¾¾èƒ½åŠ›ç†è®ºè¾¹ç•Œ**é—®é¢˜ã€‚çŽ°æœ‰å·¥ä½œç¼ºä¹å¯¹ä¸¤ç±»æœºåˆ¶è¡¨è¾¾èƒ½åŠ›çš„ä¸¥æ ¼å½¢å¼åŒ–åˆ†æžï¼Œå¯¼è‡´æ¨¡åž‹é€‰æ‹©ç¼ºä¹ç†è®ºä¾æ®ã€‚\\n> *   è¯¥é—®é¢˜çš„çªç ´æ€§åœ¨äºŽï¼šé¦–æ¬¡å»ºç«‹äº†MP-TGNNsä¸ŽWeisfeiler-Lemanæµ‹è¯•çš„ç†è®ºè”ç³»ï¼Œä¸ºåŠ¨æ€æŽ¨èç³»ç»Ÿã€äº¤é€šé¢„æµ‹ç­‰æ—¶åºå›¾å­¦ä¹ ä»»åŠ¡æä¾›äº†**æ¨¡åž‹é€‰æ‹©çš„ç†è®ºåŸºç¡€**ã€‚\\n\\n> **æ–¹æ³•æ¦‚è¿° (Method Overview)**\\n> *   é€šè¿‡æž„é€ å…¨å±€çŸ¥è¯†å›¾$\\\\kappa_{\\\\mathsf{glob}}(TG)$å’Œå±€éƒ¨çŸ¥è¯†å›¾$\\\\kappa_{\\\\mathsf{loc}}(TG)$ï¼Œå°†MP-TGNNsçš„èŠ‚ç‚¹åŒºåˆ†èƒ½åŠ›ç­‰ä»·è½¬åŒ–ä¸ºæ ‡å‡†1-WLæµ‹è¯•é—®é¢˜ã€‚\\n\\n> **ä¸»è¦è´¡çŒ®ä¸Žæ•ˆæžœ (Contributions & Results)**\\n> *   **ç†è®ºçªç ´ï¼š** è¯æ˜Žäº†å…¨å±€ä¸Žå±€éƒ¨MP-TGNNsåœ¨ä¸€èˆ¬æ—¶åºå›¾ä¸Šè¡¨è¾¾èƒ½åŠ›ä¸å¯æ¯”ï¼ˆTheorem 11-12ï¼‰ï¼Œä½†åœ¨é¢œè‰²æŒä¹…æ€§æ—¶åºå›¾ä¸Šå±€éƒ¨æœºåˆ¶ä¸¥æ ¼æ›´ä¼˜ï¼ˆTheorem 13ï¼‰ã€‚\\n>   *   **æ–¹æ³•è®ºåˆ›æ–°ï¼š** æå‡ºæ—¶é—´åŒæž„ï¼ˆtimewise isomorphismï¼‰æ¦‚å¿µï¼Œè§£å†³äº†ä¼ ç»Ÿç‚¹åŒæž„ï¼ˆpointwise isomorphismï¼‰ä¸é€‚ç”¨äºŽMP-TGNNsåˆ†æžçš„é—®é¢˜ï¼ˆTheorem 8-10ï¼‰ã€‚\\n>   *   **å®žéªŒéªŒè¯ï¼š** åœ¨TGB 2.0åŸºå‡†æµ‹è¯•ä¸­ï¼Œå±€éƒ¨æ¨¡åž‹MRRæœ€é«˜æå‡18.4%ï¼ˆtgbl-wikiæ•°æ®é›†ï¼‰ï¼Œä¸”æœ€ä¼˜å±‚æ•°éœ€æ±‚ï¼ˆ7å±‚ï¼‰é«˜äºŽå…¨å±€æ¨¡åž‹ï¼ˆ5å±‚ï¼‰ï¼Œå°è¯ç†è®ºç»“è®ºã€‚\",\n  \"algorithm_details\": \"### âš™ï¸ ç®—æ³•/æ–¹æ¡ˆè¯¦è§£\\n\\n> **æ ¸å¿ƒæ€æƒ³ (Core Idea)**\\n> *   é€šè¿‡**çŸ¥è¯†å›¾è½¬æ¢**å°†æ—¶åºå›¾å­¦ä¹ é—®é¢˜é™ç»´è‡³é™æ€å›¾åˆ†æžæ¡†æž¶ï¼šå°†å…¨å±€MP-TGNNsçš„è·¨æ—¶é—´æ¶ˆæ¯ä¼ é€’æ˜ å°„ä¸ºçŸ¥è¯†å›¾ä¸­çš„è·¨æ—¶é—´è¾¹ï¼ˆå›¾5ï¼‰ï¼Œå±€éƒ¨æœºåˆ¶åˆ™æ˜ å°„ä¸ºæ—¶é—´å†…éƒ¨è¾¹ï¼ˆå›¾6ï¼‰ã€‚\\n> *   è®¾è®¡å“²å­¦æºäºŽ**è¡¨è¾¾èƒ½åŠ›ç­‰ä»·æ€§**ï¼šMP-TGNNså¯¹$(v,t)$çš„åŒºåˆ†èƒ½åŠ›ç­‰ä»·äºŽ1-WLåœ¨å¯¹åº”çŸ¥è¯†å›¾ä¸Šå¯¹$(v,t)$çš„åŒºåˆ†èƒ½åŠ›ã€‚\\n\\n> **åˆ›æ–°ç‚¹ (Innovations)**\\n> *   **ä¸Žå…ˆå‰å·¥ä½œçš„å¯¹æ¯”ï¼š** å…ˆå‰æ—¶åºGNNè¡¨è¾¾èƒ½åŠ›ç ”ç©¶ï¼ˆå¦‚Souza et al. 2022ï¼‰ä»…é’ˆå¯¹ç‰¹å®šæž¶æž„ï¼Œç¼ºä¹ç»Ÿä¸€åˆ†æžæ¡†æž¶ã€‚\\n> *   **æœ¬æ–‡çš„æ”¹è¿›ï¼š** æå‡ºé€šç”¨çŸ¥è¯†å›¾æž„é€ æ–¹æ³•ï¼ˆDefinition 1 & 4ï¼‰ï¼Œå¹¶ä¸¥æ ¼è¯æ˜Žå…¶ä¸ŽMP-TGNNsçš„ç­‰ä»·æ€§ï¼ˆTheorem 2-6ï¼‰ã€‚\\n\\n> **å…·ä½“å®žçŽ°æ­¥éª¤ (Implementation Steps)**\\n> 1.   **æ—¶åºå›¾ç¼–ç ï¼š** å°†æ—¶åºå›¾$TG$è¡¨ç¤ºä¸ºå¿«ç…§åºåˆ—$(G_i,t_i)$ï¼Œå…¶ä¸­$G_i=(V_i,E_i,c_i)$ï¼ˆè§Backgroundç« èŠ‚ï¼‰ã€‚\\n> 2.   **çŸ¥è¯†å›¾æž„é€ ï¼š** \\n>      - å…¨å±€çŸ¥è¯†å›¾$\\\\kappa_{\\\\mathsf{glob}}(TG)$ï¼šè·¨æ—¶é—´è¾¹$(t_j-t_i,(v,t_i),(u,t_j))$å½“ä¸”ä»…å½“$i\\\\leq j$ä¸”$\\\\{u,v\\\\}\\\\in E_i$ï¼ˆDefinition 1ï¼‰ã€‚\\n>      - å±€éƒ¨çŸ¥è¯†å›¾$\\\\kappa_{\\\\mathsf{loc}}(TG)$ï¼šä»…å«åŒæ—¶é—´è¾¹$(t_j-t_i,(v,t_j),(u,t_j))$ï¼ˆDefinition 4ï¼‰ã€‚\\n> 3.   **1-WLåº”ç”¨ï¼š** åœ¨çŸ¥è¯†å›¾ä¸Šè¿è¡Œå…³ç³»åž‹1-WLç®—æ³•ï¼ˆè§Backgroundä¸­$\\\\mathsf{rwl}$å®šä¹‰ï¼‰ï¼Œå…¶è¿­ä»£å…¬å¼ä¸ºï¼š\\n>      $$\\\\mathsf{rwl}^{(\\\\ell)}(v)=\\\\tau\\\\left(\\\\mathsf{rwl}^{(\\\\ell-1)}(v),\\\\{\\\\{(\\\\mathsf{rwl}^{(\\\\ell-1)}(u),r)|u\\\\in\\\\mathcal{N}_r(v),r\\\\in R\\\\}\\\\}\\\\right)$$\\n> 4.   **ç­‰ä»·æ€§è¯æ˜Žï¼š** é€šè¿‡æž„é€ æ€§è¯æ˜Žï¼ˆTheorem 3 & 6ï¼‰å±•ç¤ºMP-TGNNsåµŒå…¥$\\\\mathbf{h}_v^{(\\\\ell)}(t)$ä¸Ž$\\\\mathsf{rwl}^{(\\\\ell)}(v,t)$çš„å¯¹åº”å…³ç³»ã€‚\\n\\n> **æ¡ˆä¾‹è§£æž (Case Study)**\\n> *   å›¾7å±•ç¤ºç‚¹åŒæž„å¤±æ•ˆæ¡ˆä¾‹ï¼š$(a,t_2)$ä¸Ž$(a',t_2)$ç‚¹åŒæž„ä½†è¢«MP-TGNNsåŒºåˆ†ï¼ˆTheorem 8ï¼‰ã€‚\\n> *   å›¾8æ¼”ç¤ºé¢œè‰²æŒä¹…æ€§å›¾ä¸Šå±€éƒ¨æœºåˆ¶ä¼˜åŠ¿ï¼š$(a,t_2)$åœ¨$\\\\kappa_{\\\\mathsf{loc}}(TG)$ä¸­æœ‰é•¿åº¦ä¸º2çš„è·¯å¾„ç‰¹å¾ï¼Œè€Œ$(a',t_2)$æ— æ­¤ç‰¹å¾ã€‚\",\n  \"comparative_analysis\": \"### ðŸ“Š å¯¹æ¯”å®žéªŒåˆ†æž\\n\\n> **åŸºçº¿æ¨¡åž‹ (Baselines)**\\n> *   è®ºæ–‡å®žçŽ°çš„åŸºç¡€å…¨å±€/å±€éƒ¨MP-TGNNsï¼ˆè§Experimentsç« èŠ‚ï¼‰ï¼Œé‡‡ç”¨ç›¸åŒç»„åˆå‡½æ•°ï¼ˆ$\\\\parallel$ï¼‰å’Œèšåˆå‡½æ•°ï¼ˆ$\\\\sum$ï¼‰ç¡®ä¿å…¬å¹³å¯¹æ¯”ã€‚\\n\\n> **æ€§èƒ½å¯¹æ¯” (Performance Comparison)**\\n> *   **åœ¨MRRæŒ‡æ ‡ä¸Šï¼š** å±€éƒ¨æ¨¡åž‹åœ¨tgbl-wikiä¸ŠMRRä¸º0.264ï¼Œæ˜¾è‘—ä¼˜äºŽå…¨å±€æ¨¡åž‹ï¼ˆ0.223ï¼‰ï¼Œç›¸å¯¹æå‡18.4%ï¼›åœ¨tgbl-coinä¸ŠMRR 0.635 vs 0.628ï¼ŒéªŒè¯é¢œè‰²æŒä¹…æ€§å›¾çš„å±€éƒ¨ä¼˜åŠ¿ã€‚\\n> *   **åœ¨å±‚æ•°æ•æ„Ÿæ€§ä¸Šï¼š** å±€éƒ¨æ¨¡åž‹éœ€7å±‚è¾¾åˆ°æœ€ä¼˜ï¼ˆMRR=0.264ï¼‰ï¼Œè€Œå…¨å±€æ¨¡åž‹5å±‚å³é¥±å’Œï¼ˆMRR=0.223ï¼‰ï¼Œå°è¯å±€éƒ¨æœºåˆ¶éœ€è¦æ›´æ·±ç½‘ç»œæ•èŽ·æ—¶åºæ¨¡å¼ï¼ˆå›¾9ï¼‰ã€‚\\n> *   **åœ¨è®­ç»ƒæ•ˆçŽ‡ä¸Šï¼š** å…¨å±€æ¨¡åž‹å› è·¨æ—¶é—´æ¶ˆæ¯ä¼ é€’éœ€å†»ç»“åŽ†å²åµŒå…¥ï¼ˆè§Implementationï¼‰ï¼Œå®žé™…è®­ç»ƒè€—æ—¶çº¦ä¸ºå±€éƒ¨æ¨¡åž‹çš„1.8å€ã€‚\",\n  \"keywords\": \"### ðŸ”‘ å…³é”®è¯\\n\\n*   æ—¶åºå›¾ç¥žç»ç½‘ç»œ (Temporal Graph Neural Networks, MP-TGNNs)\\n*   æ¶ˆæ¯ä¼ é€’æœºåˆ¶ (Message Passing Mechanism, N/A)\\n*   Weisfeiler-Lemanæµ‹è¯• (Weisfeiler-Leman Test, 1-WL)\\n*   è¡¨è¾¾èƒ½åŠ› (Expressive Power, N/A)\\n*   çŸ¥è¯†å›¾ (Knowledge Graph, KG)\\n*   æ—¶é—´åŒæž„ (Timewise Isomorphism, N/A)\\n*   é¢œè‰²æŒä¹…æ€§å›¾ (Colour-Persistent Temporal Graphs, N/A)\\n*   æ—¶åºé“¾æŽ¥é¢„æµ‹ (Temporal Link Prediction, N/A)\"\n}\n```"
}