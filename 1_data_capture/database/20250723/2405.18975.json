{
    "source": "Semantic Scholar",
    "arxiv_id": "2405.18975",
    "link": "https://arxiv.org/abs/2405.18975",
    "pdf_link": "https://arxiv.org/pdf/2405.18975.pdf",
    "title": "Hierarchical Classification Auxiliary Network for Time Series Forecasting",
    "authors": [
        "Yanru Sun",
        "Zongxia Xie",
        "Dongyue Chen",
        "Emadeldeen Eldele",
        "Qinghua Hu"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2024-05-29",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 4,
    "influential_citation_count": 0,
    "institutions": [
        "Tianjin Key Lab of Machine Learning",
        "College of Intelligence and Computing",
        "Tianjin University",
        "Centre for Frontier AI Research",
        "Agency for Science, Technology and Research",
        "Institute for InfoComm Research"
    ],
    "paper_content": "# Hierarchical Classification Auxiliary Network for Time Series Forecasting\n\nYanru $\\mathbf { S u n } ^ { 1 }$ , Zongxia $\\mathbf { X _ { i } ^ { \\bullet } }$ , Dongyue Chen1, Emadeldeen Eldele2,3, Qinghua $\\mathbf { H } \\mathbf { u } ^ { 1 }$\n\n1 Tianjin Key Lab of Machine Learning, College of Intelligence and Computing, Tianjin University, China 2 Centre for Frontier AI Research, Agency for Science, Technology and Research, Singapore 3 Institute for InfoComm Research, Agency for Science, Technology and Research, Singapore yanrusun $@$ tju.edu.cn, caddiexie $@$ hotmail.com, dyuechen $@$ tju.edu.cn, emad0002 $@$ ntu.edu.sg, huqinghua $@$ tju.edu.cn\n\n# Abstract\n\nDeep learning has significantly advanced time series forecasting through its powerful capacity to capture sequence relationships. However, training these models with the Mean Square Error (MSE) loss often results in over-smooth predictions, making it challenging to handle the complexity and learn highentropy features from time series data with high variability and unpredictability. In this work, we introduce a novel approach by tokenizing time series values to train forecasting models via cross-entropy loss, while considering the continuous nature of time series data. Specifically, we propose a Hierarchical Classification Auxiliary Network, HCAN, a general model-agnostic component that can be integrated with any forecasting model. HCAN is based on a Hierarchy-Aware Attention module that integrates multi-granularity high-entropy features at different hierarchy levels. At each level, we assign a class label for timesteps to train an Uncertainty-Aware Classifier. This classifier mitigates the over-confidence in softmax loss via evidence theory. We also implement a Hierarchical Consistency Loss to maintain prediction consistency across hierarchy levels. Extensive experiments integrating HCAN with state-of-the-art forecasting models demonstrate substantial improvements over baselines on several real-world datasets.\n\n# Code ‚Äî https://github.com/syrGitHub/HCAN\n\n# Introduction\n\nTime series forecasting has received significant attention due to its wide-ranging social impact. Among existing approaches for time series forecasting, deep learning methods have emerged as significant contributors to this field (Zhou et al. 2021, 2022; Zeng et al. 2023; Ni et al. 2024). These methods showed a powerful capacity to capture sequence continuity features (Wen et al. 2023; Wang et al. 2024) and enhance forecasting performance in practical applications such as finance (Hou et al. 2022), weather forecasting (Lam et al. 2022), resource planning (Chen et al. 2021), and other domains (Shao et al. 2024; Wu et al. 2024).\n\nNevertheless, current time series forecasting methods relying on the Mean Square Error (MSE) loss for feature extraction can suffer inaccurate predictions. The main downside of the MSE loss is compressing the feature representation into a narrow space, limiting its ability to learn complexity and high-entropy feature representations, especially for those features that exhibit significant variability and unpredictability (Zhang et al. 2023; Pintea et al. 2023). Therefore, current methods often produce over-smooth predictions, leading to inaccuracies such as inflating wind speed estimates on sunny days when the actual wind speed is low, and underestimating wind speed on windy days when the actual wind speed is high. This weakness diminishes the utility of forecasting results for downstream applications, as shown in Figure 1a.\n\n![](images/6c834e10f75e7e1dcb67124ef739e104fde65cb0f09e86468a1746cc51f35140.jpg)  \nFigure 1: Comparison between Conventional and Discretized Settings for time series forecasting. (a) Conventional setting keeps features close together, producing over-smooth predictions; (b) Discretized setting spreads the features, resulting in a higher entropy feature space, but can misclassify inter-class boundary timesteps.\n\nRecently, several studies have demonstrated the superiority of cross-entropy loss in capturing high-entropy feature representation from a mutual information perspective (Pintea et al. 2023; Zhang et al. 2023). Therefore, it has been successfully applied in various domains, such as depth estimation (Cao, Wu, and Shen 2017; Fu et al. 2018), age estimation (Rothe, Timofte, and Van Gool 2015; Shah et al. 2024), and crowd counting (Xiong and Yao 2022; Guo et al. 2024).\n\nIn this work, we reformulate time series forecasting as a classification problem. Specifically, we tokenize time series values into different categories based on their magnitude and leverage the cross-entropy loss to train a classifier on these tokenized values. For example, in Figure 1b, we employ quantization to convert the real values into four discrete intervals, where each interval is considered a separate class. In this way, we can generate predictions within the corresponding interval based on the output of the classifier.\n\nHowever, the continuous nature of time series data makes it challenging to classify values near the inter-class boundaries accurately. This difficulty may result in sub-optimal relative improvements, as illustrated by the blue circle in Figure 1b, a phenomenon commonly referred to as the boundary effects (Liu, Zhang, and Duan 2020).\n\nTherefore, we propose Hierarchical Classification Auxiliary Networks (HCAN), a novel model-agnostic component that can be integrated with any forecasting model. The architecture of HCAN is illustrated in Figure 2. In specific, we develop a Hierarchy-Aware Attention (HAA) module to incorporate multi-granular high-entropy features into the main features generated by the encoder network. For each hierarchy level, we propose an Uncertainty-Aware Classifier (UAC), combined with the evidence theory to mitigate the overconfident predictions and enhance the reliability of the features. Last, we propose a Hierarchical Consistency Loss (HCL) to ensure consistency of predicted values between hierarchies. In summary, our contributions are as follows:\n\n‚Ä¢ We reformulate forecasting as a hierarchical classification problem to introduce high-entropy feature representations, which helps to reduce over-smooth predictions. ‚Ä¢ We propose HCAN, a hierarchy-aware attention module supported by uncertainty-aware classifiers and a consistency loss to alleviate issues caused by the boundary effects during the classification of timesteps. ‚Ä¢ Extensive experiments conducted on real-world datasets show the effectiveness of integrating HCAN with various state-of-the-art methods.\n\n# Related Work\n\n# Time Series Forecasting\n\nWith the increased data availability and computational power, deep learning-based models have become an efficient solution to time series forecasting task (Qiu et al. 2024). In overall, based on the underlying network architecture, they can be categorized into models based on Recurrent neural networks (RNNs), Convolutional neural networks (CNNs), Transformer, and multi-layer perceptron (MLP). RNNs are traditionally utilized to capture temporal dependencies, yet they suffer from gradient vanishing and exploding problems. In addition, besides the sequential data processing, RNNs have short-term memory and may not be efficient in learning long-term dependencies. To overcome the limitations of RNNs, Transformer-based models have excelled recently (Zhou et al. 2021, 2022; Yu et al. 2023; Liu et al. 2023). Unlike RNNs, Transformers can process entire sequences simultaneously, benefiting from the parallel computations. In addition, Transformers handle long-range dependencies more effectively than RNNs (Nie et al. 2022).\n\nOn the other hand, recent studies have leveraged the robust abilities of CNNs to capture short-term patterns while attempting to enhance their capabilities for recognizing longrange dependencies (Liu et al. 2022; Eldele et al. 2024). Lastly, the recent development of MLP-based models has resulted in good performance with simple architectures (Zeng et al. 2023; Xu, Zeng, and $\\mathrm { X u } 2 0 2 4 ,$ ).\n\nDespite these advancements, these methods still struggle with capturing high-entropy feature representations due to their reliance on the MSE loss, which often leads to oversmooth predictions (Zhang et al. 2023). Differently, our proposed work aims to overcome this limitation and construct a complex and high-entropy feature space, thereby enhancing feature diversity and improving prediction accuracy.\n\n# Classification for Continuous Targets\n\nOur approach draws inspiration from successful applications of classification in other domains, such as computer vision and pose estimation, where discretizing continuous targets has led to significant improvements (Rabanser et al. 2020; Gu, Yang, and Yao 2021). For instance, in-depth estimation tasks, classifying depth ranges has proven more effective than precise value prediction (Cao, Wu, and Shen 2017; Fu et al. 2018).\n\nIn the context of time series analysis, some recent works have explored limited categorization schemes. For example, DEMM (Wilson et al. 2022) and DEMMA (Wang and Gao 2023) propose frameworks that segment time series into three broad categories. Similarly, ${ \\mathrm { N E C } } +$ (Li, Xu, and Anastasiu 2023) employs binary classification to distinguish between extreme and normal events.\n\nOur work significantly extends and refines these initial explorations by introducing a comprehensive, multi-level classification framework specifically designed for time series forecasting. This novel approach achieves a balance between the simplification benefits of discretization and the need for nuanced, continuous predictions. In addition, it addresses key limitations of previous methods, such as the loss of granularity in predictions and the occurrence of boundary effects near class thresholds.\n\n# Methodology\n\n# Preliminaries\n\nGiven the historical time series data $X = \\{ x ^ { i } \\} _ { i = 1 } ^ { N }$ with $N$ samples, where $\\boldsymbol { x } ^ { i } \\in \\mathbb { R } ^ { L \\times D }$ , the goal of time series forecasting is to predict horizon series $\\boldsymbol { Y } ~ = ~ \\{ y ^ { i } \\} _ { i = 1 } ^ { N }$ , where $y ^ { i } \\in \\mathbb { R } ^ { T \\times D }$ . Here, $L$ is the look-back window, $T$ is the number of future timesteps, and $D$ refers to the number of channels in the multivariate time series.\n\nHCAN reformulates the forecasting task as a hierarchical classification task with 3 levels: the original series, coarse, and fine-grained. The number of categories at each level is $K _ { o } = 1$ , $K _ { c } = 2$ , $K _ { f } = 4$ . At each level, a discretizing mapping function converts the continuous target $y ^ { i }$ into a categorical target $k ^ { i }$ based on which interval $\\mathcal { T } _ { k } = ( \\rho _ { k } ^ { \\mathrm { l e f t } } , \\rho _ { k } ^ { \\mathrm { r i g h t } } )$ the value $y ^ { i }$ falls into. This interval $\\mathcal { T } _ { k }$ represents the range within which $y ^ { i }$ is categorized. The detailed mapping process can be found in the Appendix. Subsequently, the relative forecasting target $\\Delta y ^ { i } = \\dot { y } ^ { i } - \\rho _ { k } ^ { \\mathrm { l e f t } }$ is computed as the offset of $y ^ { i }$ from the lower bound $\\rho _ { k } ^ { \\mathrm { l e f t } }$ of the interval $\\mathcal { T } _ { k }$ , where $\\Delta y ^ { i } \\in \\mathbb { R } ^ { T \\times D }$ . Therefore, the new structure of the dataset becomes $D = \\{ x ^ { i } , y ^ { i } , \\Delta y _ { c } ^ { i } , k _ { c } ^ { i } , \\Delta y _ { f } ^ { i } , k _ { f } ^ { i } \\} _ { i = 1 } ^ { N }$ with $N$ samples.\n\n![](images/68327723813e7343dc3a83dc5f11517167fbce745280617e88e5499a7f655bbd.jpg)  \nFigure 2: The structure of our proposed HCAN. From right to left, time series are first divided into fine-grained classes and coarsegrained classes to form category labels for Hierarchical Classification. According to these category labels, the Uncertainty-Aware Classifier (UAC) at each level obtains reliable multi-granularity high-entropy features using evidence theory. The Hierarchical Consistency Loss (HCL) ensures the consistency of values between hierarchies. Finally, the Hierarchy-Aware Attention (HAA) module integrated the multi-granularity features into the forecasting features obtained by the backbones.\n\n# Hierarchical Classification Auxiliary Network\n\nWe propose a hierarchical structure that trains classifiers at the fine-grained and coarse-grained levels, each with a different number of classes, to obtain high-entropy features represented in multiple granularities. Specifically, the fine-grained feature is obtained from the hierarchy, which has a larger number of categories, providing the model with relatively precise quantification. Conversely, the coarse-grained feature, which corresponds to a hierarchy with fewer categories, aims to enhance classification accuracy, as shown in Figure 2.\n\nTo illustrate the workflow of our HCAN, we begin by extracting features $F \\in \\mathbb { R } ^ { D \\times T }$ from the backbone model. Subsequently, we employ three distinct linear layers to generate three types of features: $\\boldsymbol { \\theta } \\in \\mathbb { R } ^ { D \\times M }$ , $\\boldsymbol { \\phi } \\in \\dot { \\mathbb { R } } ^ { D \\times M }$ , and $\\boldsymbol { \\eta } \\in \\mathbb { R } ^ { D \\times M }$ , representing fine-g‚ààrained, coarse‚àà-grained, and the original temporal features, respectively. Meanwhile, as depicted in the right-most part of Figure 2, we categorize the timesteps into fine-grained and coarse-grained classes based on their magnitude. Specifically, we define the boundary of each group by arranging the time series values in an ascending order and then dividing them based on the number of groups $K$ (see the Appendix). This categorization forms a hierarchical structure and establishes the category labels.\n\nThese hierarchical categories are used as labels to train the Uncertainty-Aware Classifiers (UAC) at the coarse-grained and fine-grained levels. Through backpropagation, the UAC refines the features $\\theta$ and $\\phi$ , transforming them into highentropy feature representations. The temporal feature $\\eta$ is tailored to capture the temporal characteristics of time series forecasting. Furthermore, we implement the Hierarchical Consistency Loss (HCL) to maintain consistency between the coarse-grained and fine-grained levels and to mitigate boundary effects. Finally, we combine $\\theta$ , $\\phi$ , and $\\eta$ with the initial forecasting features $F$ through the Hierarchy-Aware Attention (HAA) module. In the subsequent sections, we provide a detailed description of these components.\n\nUncertainty-Aware Classification In our HCAN, we include a classifier at the coarse-grained and fine-grained levels to create the high entropy features. However, a key challenge is the high confidence often erroneously assigned to incorrect predictions by traditional softmax-based classifiers (Moon et al. 2020; Van Amersfoort et al. 2020). This issue becomes more obvious given our objective of classifying timestepslevel values into distinct classes. To address this issue and improve the robustness of classification across various hierarchical levels, we implement an evidence-based uncertainty estimation technique, which is meant to enhance the precision of uncertainty assessments. Moreover, we consider the case of challenging samples that are usually estimated with high uncertainty by the Evidential Deep Learning (EDL) methods (Han et al. 2022). To prioritize these samples, we propose a novel uncertainty-aware loss function. This loss increases the importance of these challenging samples in the learning process. Essentially, if the sample is hard to classify, it helps the model recognize its difficulty and pays more attention to it.\n\nOur approach utilizes an evidence-based uncertainty estimation technique, leveraging the parameters of the Dirichlet distribution, which is the conjugate prior of the categorical distribution. This method allows us to compute belief masses $( b )$ for different categories and the overall uncertainty mass $( u )$ , derived from the evidence (e) collected from the data.\n\nFor the $K$ -class classification problems, the softmax layer of a conventional neural network classifier is replaced with an activation function layer (i.e., Softplus) to ensure nonnegative outputs, which are then treated as evidence vectors $e \\in \\mathbb { R } _ { + } ^ { K }$ . These vectors are obtained by the classifier network based on the fine-grained feature $\\theta$ or coarse-grained feature $\\phi$ . Next, we use these evidence vectors to construct the parameters of the Dirichlet distribution, i.e., $\\alpha = e + 1$ , and calculate the belief mass $b _ { k }$ and uncertainty $u$ as:\n\n$$\nb _ { k } = { \\frac { e _ { k } } { S } } = { \\frac { \\alpha _ { k } - 1 } { S } } \\quad { \\mathrm { a n d } } \\quad u = { \\frac { K } { S } } ,\n$$\n\nwhere $\\begin{array} { r } { S = \\sum _ { i = 1 } ^ { K } ( e _ { i } + 1 ) = \\sum _ { i = 1 } ^ { K } \\alpha _ { i } } \\end{array}$ represents the Dirichlet strength. In addition, the sum of uncertainty mass $u$ and belief mass $b$ equals $\\begin{array} { r } { 1 , u + \\sum _ { k = 1 } ^ { K } b _ { k } = 1 } \\end{array}$ , where $u \\geqslant 0$ and $b \\geqslant 0$ . Finally, the probability distribution $p$ is calculated as $\\begin{array} { r } { p _ { k } = \\frac { \\alpha _ { k } } { S } } \\end{array}$ .\n\nAccording to Eq. 1, the more evidence observed for the $k$ -th class, the greater the probability allocated to the $k$ -th class. Conversely, the less total evidence observed, the greater the overall uncertainty. Therefore, we use the belief mass to calculate the class uncertainty for each instance. Specifically, for the $i$ -th sample, we use $( \\dot { 1 } - b ^ { i } )$ as class-level uncertainty, which is the uncertainty weight for categories during training. We define the uncertainty-aware (UA) coefficient as: $\\omega ^ { i } \\stackrel { { \\mathbf { \\sigma } } } { = }$ $( 1 - b ^ { i } ) \\bigodot o ^ { i }$ , where $\\odot$ means the Hadamard product.\n\nFinally, the UAC loss is defined as:\n\n$$\n\\begin{array} { l } { \\displaystyle { \\mathcal { L } _ { U A C } = \\lambda _ { U A } \\mathcal { L } _ { U A } ^ { i } + \\lambda _ { K L } \\mathcal { L } _ { K L } ^ { i } } } \\\\ { \\displaystyle \\ = \\lambda _ { U A } \\sum _ { k = 1 } ^ { K } \\omega _ { k } ^ { i } ( \\psi ( S ^ { i } ) - \\psi ( \\alpha _ { k } ^ { i } ) ) } \\\\ { \\displaystyle \\ + \\lambda _ { K L } K L [ D i r ( p ^ { i } | \\widetilde \\alpha ^ { i } ) | | D i r ( p ^ { i } | 1 ) ] , } \\end{array}\n$$\n\nwhere $\\psi ( \\cdot )$ is the digamma functione, and $\\lambda _ { U A } , \\lambda _ { K L }$ are balance factors, and $\\bar { \\operatorname { D i r } } ( p ^ { i } | 1 )$ approximates the uniform distribution. Notably, we make adjustments to the Dirichlet parameters $\\alpha ^ { i }$ by $\\widetilde { \\alpha } ^ { i } = o ^ { i } + ( 1 ^ { ` } - o ^ { i } ) \\bigodot \\alpha ^ { i }$ to remove the non-misleading eviedence.\n\nBy formalizing forecasting as a classification task, we introduce high entropy features into the forecasting feature space. At the same time, to encourage the continuity of the extracted features, we propose a relative prediction strategy, making predictions within each classification bin ( $\\mathrm { \\Delta Y u }$ et al. 2021). We optimize using the MSE loss against the ground truth forecasting interval:\n\n$$\n\\mathcal { L } _ { R E G } = \\sum _ { k = 1 } ^ { K } \\mathbb { I } ( c _ { k } = 1 ) ( \\Delta y _ { k } - \\Delta \\hat { y } _ { k } ) ^ { 2 } ,\n$$\n\nwhere $c _ { k }$ and $\\Delta y _ { k }$ denote the classification and relative prediction labels, respectively, and $\\Delta \\hat { y } _ { k }$ is the relative prediction value obtained by the model.\n\nThe hierarchy loss is formulated across two layers with varying granularity as:\n\n$$\n\\mathcal { L } _ { H I E R } = \\mathcal { L } _ { U A C } ^ { f } + \\alpha \\mathcal { L } _ { R E G } ^ { f } + \\mathcal { L } _ { U A C } ^ { c } + \\alpha \\mathcal { L } _ { R E G } ^ { c } ,\n$$\n\n![](images/eb59452f7e52ccde62d34ce47d717048c424c3bb238eb4483ff61f0ba44eda14.jpg)  \nFigure 3: The hierarchical consistency loss between finegrained and coarse-grained hierarchies encourages consistent predictions among them, alleviating the boundary effects. The $\\boldsymbol { e } _ { f }$ from the fine-grained classifier is converted to $\\hat { e } _ { c }$ , which aligns with the coarse-grained classifier $e _ { c }$ . We minimize the KL divergence loss between their softmax outputs.\n\nwhere $\\alpha$ is the balance factor.\n\nHierarchical Consistency Loss Due to the continuous nature of time series data, directly classifying timestep values may result in misclassified values near the inter-class boundaries, known as boundary effects. Therefore, we propose the Hierarchical Consistency Loss (HCL), which aims to keep the values near the boundary of a fine-grained class within the correct coarse-grained category.\n\nTo reinforce this alignment between the hierarchical classifiers, we propose an HCL to penalize discrepancies between them. As illustrated in Figure 3, we minimize a symmetric version of the Kullback-Leibler (KL) divergence between the class distributions of the fine-grained and coarse-grained classifiers.\n\nFor each fine-grained category, represented by evidence $e _ { f } = [ e _ { f } ^ { A _ { 1 } } , . . . , e _ { f } ^ { \\bar { A _ { | A | } } } , e _ { f } ^ { B _ { 1 } } , . . . , \\bar { e } _ { f } ^ { B _ { | \\bar { B } | } } , . . . ]$ efB|B| , ...], we first convert it to a coarse-grained category evidence $e _ { c } = [ e _ { c } ^ { A } , e _ { c } ^ { B } , \\ldots ]$ . To align $\\boldsymbol { \\mathscr { e } } _ { f }$ and $e _ { c }$ , we average the $\\boldsymbol { e } _ { f }$ values that belong to the same coarse-grained class to produce the converted coarsegrained evidence:\n\n$$\n\\begin{array} { l } { \\hat { e } _ { c } = [ \\hat { e } _ { c } ^ { A } , \\hat { e } _ { c } ^ { B } , \\dots ] } \\\\ { = [ \\frac { e _ { f } ^ { A _ { 1 } } + \\dots + e _ { f } ^ { A _ { | A | } } } { | A | } , \\frac { e _ { f } ^ { B _ { 1 } } + \\dots + e _ { f } ^ { B _ { | B | } } } { | B | } , \\dots ] . } \\end{array}\n$$\n\nThe consistency loss for each coarse-grained class is then defined as a symmetric version of the KL divergence (equivalent to the Jensen-Shannon divergence) between $e$ and $\\hat { e }$ :\n\n$$\n\\mathcal { L } _ { H C L } = \\frac { 1 } { 2 } D _ { K L } ( e _ { c } | | \\hat { e } _ { c } ) + \\frac { 1 } { 2 } D _ { K L } ( \\hat { e } _ { c } | | e _ { c } ) .\n$$\n\nThis approach ensures that our model‚Äôs predictions remain consistent across different hierarchical levels, effectively alleviating boundary effects.\n\nHierarchy-Aware Attention To introduce the high-entropy feature into the forecasting features to alleviate the oversmooth predictions, and optimize the trade-off between forecasting features and high-entropy features at different granularities, we have developed the Hierarchy-Aware Attention (HAA) module.\n\nTable 1: Multivariate long sequence time-series forecasting results. We report the MSE of different prediction lengths. The look-up window is set to $L = 3 3 6$ for PatchTST, DLinear, and SCINet, and $L = 9 6$ for other models. The best results are highlighted in bold. Detailed results of all prediction lengths for MSE/MAE are provided in the Appendix.   \n\n<html><body><table><tr><td>Model Metric</td><td>MSE</td><td>Informer +HCAN|Autoformer +HCAN|PatchTST MSE</td><td>MSE</td><td>MSE</td><td>MSE</td><td>MSE</td><td>MSE</td><td>MSE</td><td>MSE</td><td>MSE</td><td>+HCAN|SCINet +HCAN|Dlinear +HCAN|iTransformer MSE</td><td>+HCAN MSE</td><td>FITS MSE</td><td>+HCAN MSE</td></tr><tr><td>ETTh1</td><td>1.077</td><td>0.897</td><td>0.530</td><td>0.462</td><td>0.421</td><td>0.396</td><td>0.591</td><td>0.536</td><td>0.453</td><td>0.428</td><td>0.457</td><td>0.451</td><td>0.439</td><td>0.436</td></tr><tr><td>ETTh2</td><td>4.779</td><td>2.359</td><td>0.483</td><td>0.406</td><td>0.342</td><td>0.343</td><td>1.041</td><td>0.820</td><td>0.473</td><td>0.411</td><td>0.384</td><td>0.375</td><td>0.375</td><td>0.368</td></tr><tr><td>ETTm1</td><td>0.951</td><td>0.717</td><td>0.606</td><td>0.540</td><td>0.353</td><td>0.350</td><td>0.417</td><td>0.390</td><td>0.359</td><td>0.344</td><td>0.408</td><td>0.403</td><td>0.414</td><td>0.405</td></tr><tr><td>ETTm2</td><td>1.729</td><td>0.981</td><td>0.359</td><td>0.303</td><td>0.258</td><td>0.250</td><td>0.753</td><td>0.685</td><td>0.287</td><td>0.296</td><td>0.292</td><td>0.285</td><td>0.286</td><td>0.280</td></tr><tr><td>Weather</td><td>0.733</td><td>0.370</td><td>0.351</td><td>0.303</td><td>0.268</td><td>0.254</td><td>0.242</td><td>0.225</td><td>0.247</td><td>0.237</td><td>0.260</td><td>0.250</td><td>0.249</td><td>0.248</td></tr><tr><td>Exchange</td><td>1.726</td><td>0.845</td><td>0.525</td><td>0.410</td><td>0.516</td><td>0.344</td><td>0.844</td><td>0.549</td><td>0.369</td><td>0.338</td><td>0.364</td><td>0.395</td><td>0.360</td><td>0.426</td></tr><tr><td>ILI</td><td>2.889</td><td>2.738</td><td>5.012</td><td>4.166</td><td>1.516</td><td>1.428</td><td>3.277</td><td>3.265</td><td>2.347</td><td>2.276</td><td>2.767</td><td>2.741</td><td>3.680</td><td>2.095</td></tr><tr><td>Electricity</td><td>0.352</td><td>0.337</td><td>0.250</td><td>0.236</td><td>0.259</td><td>0.233</td><td>0.213</td><td>0.209</td><td>0.210</td><td>0.208</td><td>0.176</td><td>0.167</td><td>0.217</td><td>0.216</td></tr><tr><td>Traffic</td><td>0.853</td><td>0.818</td><td>0.651</td><td>0.552</td><td>0.490</td><td>0.460</td><td>0.612</td><td>0.527</td><td>0.625</td><td>0.597</td><td>0.422</td><td>0.416</td><td>0.642</td><td>0.624</td></tr><tr><td>SolarWind</td><td>1.953</td><td>1.025</td><td>1.362</td><td>1.057</td><td>1.109</td><td>0.948</td><td>1.174</td><td>1.091</td><td>1.071</td><td>1.019</td><td>1.360</td><td>1.028</td><td>1.349</td><td>1.239</td></tr></table></body></html>\n\nBuilding on the feature architecture of Hierarchical Classification Auxiliary Network, we reshape $\\boldsymbol { \\phi } \\in \\mathbb { R } ^ { H \\times D }$ projections, allowing their dot-products to interact and generate the HAA map $A$ of size $\\mathbb { R } ^ { \\mathbf { \\Delta } D \\times D }$ . This is combined with $F$ through a residual connection to introduce high-entropy feature representations. The overall HAA process is defined as follows:\n\n$$\n\\begin{array} { r l } & { \\hat { Y } = W _ { f } ( W \\cdot \\mathrm { A t t e n t i o n } ( \\theta , \\phi , \\eta ) + F ) + b , } \\\\ & { \\mathrm { A t t e n t i o n } ( \\theta , \\phi , \\eta ) = \\eta \\cdot \\mathrm { S o f t m a x } ( \\theta \\cdot \\phi ) , } \\end{array}\n$$\n\nwhere $W$ and $W _ { f }$ are linear layers, $F$ is the backbone feature map, and $\\hat { Y }$ is the prediction output. The MSE loss is optimized according to $\\bar { \\hat { Y } }$ and the ground truth labels $Y$ as:\n\n$$\n\\mathcal { L } _ { M S E } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( Y ^ { i } - \\hat { Y } ^ { i } ) ^ { 2 } ,\n$$\n\nwhere $N$ represents the number of samples.\n\nTo sum up, the overall training loss is defined as:\n\n$$\n\\mathcal { L } = \\mathcal { L } _ { H I E R } + \\beta \\mathcal { L } _ { H C L } + \\gamma \\mathcal { L } _ { M S E } ,\n$$\n\nwhere $\\beta$ and $\\gamma$ are hyper-parameter loss weights chosen through grid search.\n\n# Experiments\n\nIn this section, we conduct extensive experiments to evaluate the performance of HCAN and further perform ablation studies to justify how each component contributes to the results. Further details about the experimental setup can be found in the Appendix.\n\n# Experimental Settings\n\nDatasets. We ran our experiments on ten publicly available real-world multivariate time series datasets, namely: ETT, Exchange-Rate, Weather, ILI, Electricity, Traffic, and Solar Wind. We followed the standard protocol in the data preprocessing, where we split all datasets into training, validation, and testing in chronological order by a ratio of 6:2:2 for the ETT dataset and 7:1:2 for the other datasets (Zeng et al. 2023). See the Appendix for more details.\n\nBackbone models. We experimented our HCAN on top of several state-of-the-art deep learning-based forecasting models. We selected these models with different architectures, where Informer (Zhou et al. 2021), Autoformer (Wu et al. 2021), PatchTST (Nie et al. 2022), and iTransformer (Liu et al. 2023) are Transformer-based models, SCINet (Liu et al. 2022) is a CNN-based model, while DLinear (Zeng et al. 2023) and FITS (Xu, Zeng, and Xu 2024) are MLP-based models. We evaluate their performance before and after including our HCAN in the multivariate and univariate settings. For the baselines, we re-run their codes in the same settings to ensure fairness and consistency.\n\nExperiments details. Following previous works (Nie et al. 2022; Zeng et al. 2023), we used ADAM (Kingma and Ba 2014) as the default optimizer across all the experiments and reported the MSE and mean absolute error (MAE) as the evaluation metrics. A lower MSE/MAE value indicates a better performance. Detailed results for MSE/MAE are provided in the Appendix. We conducted the experiment for the same number of epochs as the baseline and the initial learning rate is chosen from {5e-3, 1e-3, 5e-4, 1e-4, 5e-5, 1e-5} through a grid search for different datasets. $\\beta$ was chosen from {1, 0.1, 0.01} and $\\gamma$ was chosen from $\\{ 1 , 0 . 1 , 0 . 0 1 \\}$ via grid search to obtain the best results. For HCAN parameters, we set $K _ { c } = 2$ and $K _ { f } = 4$ . All the experiments were repeated five times with fixed random seeds, and we reported the average performance. HCAN was implemented by PyTorch (Paszke et al. 2019) and trained on a single NVIDIA RTX 3090 24GB GPU.\n\n# Main Results\n\nMultivariate Forecasting Results. We present the multivariate forecasting results in Table 1. Notably, our proposed HCAN demonstrates a substantial impact on the performance of the baselines, as it boosts their forecasting results by a noticeable margin. This is evident in 66 out of 70 cases. For instance, HCAN achieves average performance gains of $9 . 1 \\%$ , $3 5 . 5 \\%$ , $1 0 . 2 \\%$ , and $2 2 . 3 \\%$ on the ETT dataset series. Similar improvements also observed on other datasets.\n\nWe attribute these performance enhancements to two primary aspects. First, HCAN incorporates a reliable hierarchical classification structure that captures high-entropy features, effectively alleviating the over-smooth predictions and reducing the boundary discontinuity typically associated with classification tasks. Second, the HAA mechanism enhances prediction accuracy by fusing features at different granular levels, thereby providing more reliable information for prediction. This attribute proves particularly advantageous in long-term forecasting scenarios, which inherently pose greater challenges as the forecast horizon extends. For example, as shown in the Appendix, when forecasting a length of 720 timesteps, the integration of HCAN with Autoformer leads to a significant reduction of $3 1 . 9 \\%$ in MSE on the ETTh2 dataset and a reduction of $1 9 . 3 \\%$ on the Exchange dataset. These results underscore the capability of HCAN to deliver stable and reliable predictions even in long-term forecasting scenarios.\n\nTable 2: Univariate long sequence time-series forecasting results on ETT full benchmark and Solar Wind dataset. We report the MSE of different prediction lengths $T \\in \\{ 9 6 , 1 9 2 , 3 3 6 , 7 2 0 \\}$ for comparison. The look-up window is set to $L = 3 3 6$ for PatchTST, DLinear, and SCINet, and $L = 9 6$ for other models. The best results are highlighted in bold. Detailed results of all prediction lengths for MSE/MAE are provided in the Appendix.   \n\n<html><body><table><tr><td>Model Metric</td><td>Informer MSE</td><td>+HCAN MSE</td><td>Autoformer MSE</td><td>MSE</td><td>+HCAN|PatchTST MSE</td><td>MSE</td><td>+HCAN|SCINet MSE</td><td>+HCAN MSE</td><td>Dlinear MSE</td><td>MSE</td><td>+HCAN|iTransformer</td><td>MSE</td><td>+HCAN FITS MSE MSE</td><td>+HCAN MSE</td></tr><tr><td></td><td>0.255</td><td></td><td>0.088</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.054</td></tr><tr><td>96 192</td><td>0.283</td><td>0.121 0.092</td><td>0.108</td><td>0.082 0.086</td><td>0.055 0.071</td><td>0.055 0.072</td><td>0.088 0.105</td><td>0.068 0.084</td><td>0.057 0.077</td><td>0.053 0.075</td><td>0.061 0.073</td><td>0.060 0.072</td><td>0.056 0.075</td><td>0.072</td></tr><tr><td>336</td><td>0.291</td><td>0.088</td><td>0.118</td><td>0.091</td><td>0.082</td><td>0.078</td><td>0.130</td><td>0.094</td><td>0.097</td><td>0.088</td><td>0.089</td><td>0.087</td><td>0.091</td><td>0.089</td></tr><tr><td>720</td><td>0.256</td><td>0.106</td><td>0.138</td><td>0.121</td><td>0.086</td><td>0.081</td><td>0.214</td><td>0.134</td><td>0.168</td><td>0.164</td><td>0.083</td><td>0.105</td><td>0.104</td><td>0.096</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>92</td><td>0.304</td><td>0.186</td><td>0.169</td><td>0.149</td><td>0.129</td><td>8.122</td><td>0.139</td><td>0.129</td><td>0.133</td><td>0.128</td><td>0.135</td><td>0.13</td><td>0.125</td><td>0.1723</td></tr><tr><td>336</td><td>0.324</td><td>0.223</td><td>0.255</td><td>0.226</td><td>0.187</td><td>0.187</td><td></td><td></td><td></td><td></td><td>0.218</td><td>0.215</td><td>0.222</td><td>0.221</td></tr><tr><td>E 720</td><td>0.302</td><td>0.249</td><td>0.334</td><td>0.292</td><td>0.224</td><td>0.201</td><td>0.198 0.486</td><td>0.220 0.221</td><td>0.212 0.298</td><td>0.225 0.259</td><td>0.240</td><td>0.238</td><td>0.258</td><td>0.255</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>0.093</td><td>0.049</td><td>0.059</td><td>8.047</td><td>0.026</td><td>8.04</td><td>0.049</td><td>0.09</td><td>0.034</td><td>0.06</td><td>0.029</td><td>0.028</td><td>0.049</td><td>8.042</td></tr><tr><td>336</td><td>0.271</td><td></td><td>0.088</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.056</td></tr><tr><td>720</td><td>0.464</td><td>0.108 0.118</td><td>0.122</td><td>0.072 0.079</td><td>0.053 0.074</td><td>0.050 0.070</td><td>0.109</td><td>0.089</td><td>0.064</td><td>0.059</td><td>0.061 0.083</td><td>0.060 0.082</td><td>0.057 0.079</td><td>0.075</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.139</td><td>0.117</td><td>0.081</td><td>0.082</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>0.0934</td><td>0.065</td><td>0.127</td><td></td><td></td><td>0.065</td><td></td><td>0.064</td><td></td><td>0.081</td><td>0.169</td><td>0.1069</td><td>0.170</td><td>0.069</td></tr><tr><td></td><td></td><td></td><td></td><td>0.193</td><td>0.064</td><td></td><td>0.179</td><td></td><td>0.064</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>336 720</td><td>0.178 0.221</td><td>0.141 0.156</td><td>0.217 0.198</td><td>0.126</td><td>0.120</td><td>0.117</td><td>0.130</td><td>0.128</td><td>0.129</td><td>0.120</td><td>0.144</td><td>0.143</td><td>0.128</td><td>0.126 0.176</td></tr><tr><td></td><td></td><td></td><td></td><td>0.184</td><td>0.172</td><td>0.169</td><td>0.175</td><td>0.155</td><td>0.176</td><td>0.181</td><td>0.185</td><td>0.187</td><td>0.178</td><td></td></tr><tr><td>96</td><td>1.443</td><td>1.268</td><td>2.316</td><td>1.289</td><td>1.021</td><td>0.851</td><td>1.518</td><td>1.366</td><td>1.316</td><td>1.223</td><td>1.727</td><td>1.266</td><td>1.669</td><td>1.658</td></tr><tr><td>puIM 192</td><td>1.765</td><td>1.581</td><td>2.765</td><td>1.590</td><td>1.130</td><td>1.030</td><td>1.836</td><td>1.723</td><td>1.568</td><td>1.549</td><td>2.273</td><td>1.568</td><td>2.308</td><td>2.280</td></tr><tr><td>36</td><td>1.849</td><td>1.740</td><td>2.783</td><td>1.715</td><td>1.137</td><td>1.098</td><td>1.853</td><td>1.746</td><td>1.686</td><td>1.671</td><td>2.370</td><td>1.714</td><td>2.355</td><td>2.327</td></tr><tr><td>720</td><td>1.826</td><td>1.694</td><td>2.606</td><td>1.701</td><td>1.125</td><td>1.041</td><td>1.672</td><td>1.547</td><td>1.660</td><td>1.654</td><td>2.228</td><td>1.679</td><td>2.220</td><td>2.189</td></tr></table></body></html>\n\nTable 3: Ablation study of the components of HCAN on the Weather and Solar Wind datasets using Informer as a backbone: Uncertainty-Aware Classification (UAC), Hierarchical Structure (Hierarchy), Hierarchical Consistency Loss $( \\mathcal { L } _ { H C L } )$ , and Hierarchy-Aware Attention (HAA). The results are in terms of MSE for different prediction lengths. The best results are highlighted in bold.   \n\n<html><body><table><tr><td colspan=\"4\">Component</td><td colspan=\"4\"></td><td colspan=\"4\">Solar Wind</td></tr><tr><td>UACfine</td><td>LREG</td><td>Hierarchy LHCL</td><td></td><td>HAA</td><td>96</td><td>192</td><td>336</td><td>720</td><td>96</td><td>192</td><td>336</td><td>720</td></tr><tr><td>LUAC</td><td></td><td></td><td></td><td></td><td>0.352</td><td>0.636</td><td>0.680</td><td>1.265</td><td>1.710</td><td>1.991</td><td>1.958</td><td>2.154</td></tr><tr><td>‚àö</td><td></td><td></td><td></td><td></td><td>0.349</td><td>0.509</td><td>0.613</td><td>0.993</td><td>0.991</td><td>1.077</td><td>1.127</td><td>1.149</td></tr><tr><td>‚àö</td><td>‚àö</td><td>=</td><td></td><td></td><td>0.300 0.515 0.579</td><td></td><td></td><td>0.999</td><td>0.964</td><td>1.060</td><td>1.129</td><td>1.125</td></tr><tr><td>‚àö</td><td>‚àö</td><td>‚àö</td><td></td><td></td><td>0.322 0.406 0.580</td><td></td><td></td><td>0.961</td><td>0.948</td><td>1.048</td><td></td><td>1.099 1.109</td></tr><tr><td>‚àö</td><td>‚àö</td><td>‚àö</td><td>‚àö</td><td></td><td>0.295</td><td>0.345</td><td>0.395</td><td>0.614</td><td>0.935</td><td>1.038</td><td>1.097</td><td>1.083</td></tr><tr><td>‚àö</td><td>‚àö</td><td>‚àö</td><td>L</td><td></td><td>0.291</td><td>0.306</td><td>0.369</td><td>0.513</td><td>0.920</td><td>1.027</td><td>1.087</td><td>1.065</td></tr></table></body></html>\n\nUnivariate Forecasting Results. We also report the univariate forecasting outcomes for the ETT and Solar Wind datasets in Table 2. Compared to the original performance of the baseline methods, incorporating our HCAN into these models yields an overall reduction of $2 3 . 0 \\%$ , $3 5 . 8 \\%$ , $7 . 5 \\%$ , $1 2 . 6 \\%$ , $2 . 5 \\%$ , $2 2 . 8 \\%$ , and $1 . 5 \\%$ in the MSE results. These results validate the effectiveness of our proposed hierarchical structure in enhancing forecasting precision.\n\n# Ablation Study\n\nTable 3 presents an ablation study on the Weather and Solar Wind datasets to assess the effectiveness of each module in HCAN. Referring to Figure 2, we evaluate the following settings: (1) including the UAC with only the fine-grained classes ( $\\ L _ { U A C }$ alone) (2) with adding $\\mathcal { L } _ { R E G }$ to the UAC module, i.e., $\\mathcal { L } _ { U A C } + \\mathcal { L } _ { R E G }$ (3) with including the coarsegrained classes and directly concatenating the multi-level features (Hierarchy) (4) with using $\\mathcal { L } _ { H C L }$ to keep consistency among hierarchy levels (5) with using the attention module for feature fusion instead of direct concatenation.\n\n![](images/10eded38044aae27cc82775b5486a3e48a2b9c7983756cac4bba709d01b08295.jpg)  \nFigure 4: t-SNE visualization of different features for SCINet on the ETTh1 dataset. (a) SCINet keeps features close together. (b)(c) Simply introducing classification spreads the features, obtaining a higher entropy feature space, while the ordinal relationship is lost. (d) By combining the classification features with the forecasting features, a high entropy and ordered feature representation is obtained. Features are coloured based on their predicted value.\n\nImpact of UAC. Initially, applying the UAC on the finegrained features alone with $\\mathcal { L } _ { U A C }$ significantly enhances performance by creating a high-entropy feature space that enriches forecasting representations. Adding $\\mathcal { L } _ { R E G }$ further improves performance by imposing relative forecasting constraints, ensuring feature continuity and coherence.\n\nImpact of Hierarchy Structure. Implementing a hierarchical structure with two layers of UAC layers (by including the coarse-grained features) demonstrates the value of incorporating multi-granularity features, as indicated by performance gains in the ablation study.\n\nImpact of HCL. Performance is further enhanced by integrating $\\mathcal { L } _ { H C L }$ , which imposes a consistency constraint between hierarchies and effectively addresses boundary effects.\n\nImpact of HAA. The best performance is observed when replacing direct concatenation with the HAA mechanism. This change indicates that different features contribute variably to forecasting outcomes, and simple concatenation can lead to sub-optimal results.\n\n# Qualitative Evaluation\n\nHigh-entropy Feature Representation. The t-SNE visualization of the features from SCINet on the ETTh1 dataset is displayed in Figure 4. As depicted in Figure 4a, representations learned from the MSE loss exhibit lower diversity. Figures 4b and 4c illustrate that integrating classification indeed spreads features more broadly, yet it disrupts ordinality in feature space. Figure 4d shows how the HAA mechanism combines hierarchical features with the original features from the backbone model, effectively spreading the feature while maintaining ordinality. In conclusion, HCAN facilitates reliable high-entropy feature representations through hierarchical classification, significantly helping to alleviate over-smooth predictions.\n\n![](images/9daee3a53f73572e4115f9a431223c66339792a5a2b4feac6e970feb0ed666da.jpg)  \nFigure 5: The prediction results (Horizon $\\qquad = ~ 9 6$ ) of (a) PatchTST vs. PatchTS $\\Gamma { + } \\mathrm { H C A N }$ , (b) SCINet vs. SCINet+HCAN, (c) DLinear vs. DLinear+HCAN, (d) FITS vs. FI $\\Gamma \\mathrm { S + H C A N }$ , on randomly-selected sequences from the ETTh1 dataset.\n\nVisualizations. To examine the quality of prediction results with and without our HCAN, Figure 5 presents this comparison on PatchTST, SCINe, DLinear, and FITS backbones on the ETTh1 dataset. Clearly, our HCAN yields more realistic predictions. This enhancement is largely regarded to the proposed hierarchical consistency loss (HCL), which notably improves performance at class boundaries. These results further validate the effectiveness of the high-entropy feature representations. Additionally, they demonstrate that HCL is effective in mitigating the boundary effects.\n\n# Conclusion\n\nIn this study, we addressed the issue of over-smooth predictions in time series forecasting by introducing a novel hierarchical classification from an entropy perspective. We proposed HCAN, a model-agnostic component that enhances forecasting by tokenizing output and integrating mutigranularity high-entropy feature representations through a hierarchical-aware attention module. The HCL loss further aids in mitigating boundary effects, promoting overall accuracy. Extensive experiments on benchmarking datasets demonstrate that HCAN substantially improves the performance of baseline forecasting models. Our results suggest that HCAN can serve as a foundation component in time series forecasting, providing deeper insights into the interplay between classification tasks and forecasting.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáËß£ÂÜ≥‰∫ÜÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµã‰∏≠Âõ†‰ΩøÁî®ÂùáÊñπËØØÂ∑ÆÔºàMSEÔºâÊçüÂ§±ÂáΩÊï∞ÂØºËá¥ÁöÑÈ¢ÑÊµãÁªìÊûúËøá‰∫éÂπ≥ÊªëÁöÑÈóÆÈ¢ò„ÄÇËøô‰∏ÄÈóÆÈ¢òÂú®ÂÖ∑ÊúâÈ´òÂèòÂºÇÊÄßÂíå‰∏çÂèØÈ¢ÑÊµãÊÄßÁöÑÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆ‰∏≠Â∞§‰∏∫Á™ÅÂá∫ÔºåÂΩ±Âìç‰∫ÜÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄßÔºå‰æãÂ¶ÇÂú®È£éÈÄüÈ¢ÑÊµã‰∏≠‰ºöÈ´ò‰º∞Êô¥Â§©È£éÈÄüÂíå‰Ωé‰º∞Â§ßÈ£éÂ§©È£éÈÄü„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éÔºåÈ¢ÑÊµãÁªìÊûúÁöÑÂáÜÁ°ÆÊÄßÁõ¥Êé•ÂΩ±ÂìçÂà∞ÈáëËûç„ÄÅÂ§©Ê∞îÈ¢ÑÊµã„ÄÅËµÑÊ∫êËßÑÂàíÁ≠âÂ§ö‰∏™ÂÖ≥ÈîÆÂ∫îÁî®È¢ÜÂüüÁöÑÂÜ≥Á≠ñË¥®Èáè„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄöÁî®ÁöÑÊ®°ÂûãÊó†ÂÖ≥ÁªÑ‰ª∂‚Äî‚ÄîÂ±ÇÊ¨°ÂàÜÁ±ªËæÖÂä©ÁΩëÁªúÔºàHCANÔºâÔºåÈÄöËøáÂ∞ÜÊó∂Èó¥Â∫èÂàóÂÄºÊ†áËÆ∞‰∏∫‰∏çÂêåÁ±ªÂà´ÔºåÂπ∂Âà©Áî®‰∫§ÂèâÁÜµÊçüÂ§±ËÆ≠ÁªÉÂàÜÁ±ªÂô®Ôºå‰ªéËÄåÂºïÂÖ•È´òÁÜµÁâπÂæÅË°®Á§∫ÔºåÂáèÂ∞ëÈ¢ÑÊµãÁöÑÂπ≥ÊªëÊÄß„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **Â±ÇÊ¨°ÂàÜÁ±ªËæÖÂä©ÁΩëÁªúÔºàHCANÔºâÔºö** ÈÄöËøáÂ±ÇÊ¨°ÊÑüÁü•Ê≥®ÊÑèÂäõÊ®°ÂùóÔºàHAAÔºâÂíåÂ§öÁ≤íÂ∫¶È´òÁÜµÁâπÂæÅÈõÜÊàêÔºåÊòæËëóÊèêÂçá‰∫ÜÈ¢ÑÊµãÁöÑÂáÜÁ°ÆÊÄß„ÄÇÂÆûÈ™åË°®ÊòéÔºåHCANÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÂπ≥ÂùáÊèêÂçá‰∫Ü9.1%Âà∞35.5%ÁöÑÊÄßËÉΩ„ÄÇ\\n> *   **‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•ÂàÜÁ±ªÂô®ÔºàUACÔºâÔºö** Âü∫‰∫éËØÅÊçÆÁêÜËÆ∫ÔºåÂáèÂ∞ë‰∫ÜÂàÜÁ±ªÂô®ÂØπÈîôËØØÈ¢ÑÊµãÁöÑËøáÂ∫¶Ëá™‰ø°ÔºåÊèêÂçá‰∫ÜÁâπÂæÅÁöÑÂèØÈù†ÊÄß„ÄÇ\\n> *   **Â±ÇÊ¨°‰∏ÄËá¥ÊÄßÊçüÂ§±ÔºàHCLÔºâÔºö** ÈÄöËøá‰øùÊåÅ‰∏çÂêåÂ±ÇÊ¨°È¢ÑÊµãÁöÑ‰∏ÄËá¥ÊÄßÔºåÊúâÊïàÁºìËß£‰∫ÜËæπÁïåÊïàÂ∫îÈóÆÈ¢ò„ÄÇ\\n> *   **ÂÆûÈ™åÊïàÊûúÔºö** HCANÂú®10‰∏™ÂÖ¨ÂºÄÊï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÈ™åËØÅÔºåÂú®66/70ÁöÑÊ°à‰æã‰∏≠ÊòæËëóÊèêÂçá‰∫ÜÂü∫Á∫øÊ®°ÂûãÁöÑÊÄßËÉΩÔºå‰æãÂ¶ÇÂú®ETTh2Êï∞ÊçÆÈõÜ‰∏äMSEÈôç‰Ωé‰∫Ü35.5%„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   HCANÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂ∞ÜÊó∂Èó¥Â∫èÂàóÈ¢ÑÊµãÈáçÊñ∞ÂÆö‰πâ‰∏∫Â±ÇÊ¨°ÂàÜÁ±ªÈóÆÈ¢òÔºåÈÄöËøáÂºïÂÖ•È´òÁÜµÁâπÂæÅË°®Á§∫Êù•ÂáèÂ∞ëÈ¢ÑÊµãÁöÑÂπ≥ÊªëÊÄß„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®‰∫§ÂèâÁÜµÊçüÂ§±ËÆ≠ÁªÉÂàÜÁ±ªÂô®ÔºåÂêåÊó∂ËÄÉËôëÂà∞Êó∂Èó¥Â∫èÂàóÊï∞ÊçÆÁöÑËøûÁª≠ÊÄßÔºåÈÄöËøáÂ±ÇÊ¨°‰∏ÄËá¥ÊÄßÊçüÂ§±ÂíåÁõ∏ÂØπÈ¢ÑÊµãÁ≠ñÁï•Êù•‰øùÊåÅÈ¢ÑÊµãÁöÑËøûÁª≠ÊÄß„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** ÂÖàÂâçÁöÑÂ∑•‰Ωú‰∏ªË¶Å‰æùËµñMSEÊçüÂ§±ÂáΩÊï∞ÔºåÂØºËá¥ÁâπÂæÅË°®Á§∫Ë¢´ÂéãÁº©Âà∞Áã≠Á™ÑÁöÑÁ©∫Èó¥ÔºåÈöæ‰ª•Â≠¶‰π†È´òÁÜµÁâπÂæÅ„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** HCANÈÄöËøáÂ±ÇÊ¨°ÂàÜÁ±ªÂíå‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•ÂàÜÁ±ªÂô®ÔºåÂºïÂÖ•‰∫ÜÈ´òÁÜµÁâπÂæÅË°®Á§∫ÔºåÂêåÊó∂ÈÄöËøáÂ±ÇÊ¨°‰∏ÄËá¥ÊÄßÊçüÂ§±ÂíåÁõ∏ÂØπÈ¢ÑÊµãÁ≠ñÁï•Ëß£ÂÜ≥‰∫ÜËæπÁïåÊïàÂ∫îÈóÆÈ¢ò„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.  **Êó∂Èó¥Â∫èÂàóÂÄºÊ†áËÆ∞ÂåñÔºö** Â∞ÜÊó∂Èó¥Â∫èÂàóÂÄºÊ†πÊçÆÂÖ∂Â§ßÂ∞èÂàíÂàÜ‰∏∫‰∏çÂêåÁöÑÁ±ªÂà´ÔºåÊØè‰∏™Âå∫Èó¥Ë¢´ËßÜ‰∏∫‰∏Ä‰∏™ÂçïÁã¨ÁöÑÁ±ª„ÄÇ\\n> 2.  **Â±ÇÊ¨°ÂàÜÁ±ªÔºö** Âú®Á≤óÁ≤íÂ∫¶ÂíåÁªÜÁ≤íÂ∫¶Â±ÇÊ¨°‰∏äËÆ≠ÁªÉÂàÜÁ±ªÂô®ÔºåÁîüÊàêÂ§öÁ≤íÂ∫¶ÁöÑÈ´òÁÜµÁâπÂæÅË°®Á§∫„ÄÇ\\n> 3.  **‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•ÂàÜÁ±ªÂô®ÔºàUACÔºâÔºö** ‰ΩøÁî®ËØÅÊçÆÁêÜËÆ∫ËÆ°ÁÆóÁΩÆ‰ø°Â∫¶Âíå‰∏çÁ°ÆÂÆöÊÄßÔºåÂáèÂ∞ëÂàÜÁ±ªÂô®ÁöÑËøáÂ∫¶Ëá™‰ø°„ÄÇ\\n> 4.  **Â±ÇÊ¨°‰∏ÄËá¥ÊÄßÊçüÂ§±ÔºàHCLÔºâÔºö** ÈÄöËøáÊúÄÂ∞èÂåñKLÊï£Â∫¶Ôºå‰øùÊåÅ‰∏çÂêåÂ±ÇÊ¨°È¢ÑÊµãÁöÑ‰∏ÄËá¥ÊÄß„ÄÇ\\n> 5.  **Â±ÇÊ¨°ÊÑüÁü•Ê≥®ÊÑèÂäõÔºàHAAÔºâÔºö** Â∞ÜÂ§öÁ≤íÂ∫¶ÁâπÂæÅ‰∏é‰∏ªÂπ≤Ê®°ÂûãÁöÑÁâπÂæÅËûçÂêàÔºåÁîüÊàêÊúÄÁªàÁöÑÈ¢ÑÊµãÁªìÊûú„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   ËÆ∫Êñá‰∏≠Áî®‰∫éÂØπÊØîÁöÑÂü∫Á∫øÊ®°ÂûãÂåÖÊã¨Informer„ÄÅAutoformer„ÄÅPatchTST„ÄÅSCINet„ÄÅDLinear„ÄÅiTransformerÂíåFITS„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®MSEÊåáÊ†á‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®ETTh1Êï∞ÊçÆÈõÜ‰∏äËææÂà∞‰∫Ü0.436ÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãInformer (1.077) ÂíåAutoformer (0.530)„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü3.5‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®ETTh2Êï∞ÊçÆÈõÜ‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÁöÑMSE‰∏∫0.368ÔºåËøú‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãInformer (4.779) ÂíåAutoformer (0.483)Ôºå‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü7.5‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®WeatherÊï∞ÊçÆÈõÜ‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÁöÑMSE‰∏∫0.248Ôºå‰∏éÂü∫Á∫øÊ®°ÂûãPatchTST (0.351) ÂíåSCINet (0.242) Áõ∏ÊØîÔºåÊèêÂçá‰∫Ü4.2‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®ÈïøÊúüÈ¢ÑÊµãÊÄßËÉΩ‰∏äÔºö** Âú®È¢ÑÊµã720‰∏™Êó∂Èó¥Ê≠•Êó∂ÔºåHCANÂ∞ÜAutoformerÂú®ETTh2Êï∞ÊçÆÈõÜ‰∏äÁöÑMSEÈôç‰Ωé‰∫Ü31.9%ÔºåÂú®ExchangeÊï∞ÊçÆÈõÜ‰∏äÈôç‰Ωé‰∫Ü19.3%„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n> **ÊèêÂèñ‰∏éÊ†ºÂºèÂåñË¶ÅÊ±Ç**\\n> *   Êó∂Èó¥Â∫èÂàóÈ¢ÑÊµã (Time Series Forecasting, N/A)\\n> *   Â±ÇÊ¨°ÂàÜÁ±ª (Hierarchical Classification, N/A)\\n> *   È´òÁÜµÁâπÂæÅ (High-Entropy Features, N/A)\\n> *   ‰∫§ÂèâÁÜµÊçüÂ§± (Cross-Entropy Loss, N/A)\\n> *   ‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•ÂàÜÁ±ªÂô® (Uncertainty-Aware Classifier, UAC)\\n> *   Â±ÇÊ¨°‰∏ÄËá¥ÊÄßÊçüÂ§± (Hierarchical Consistency Loss, HCL)\\n> *   Â±ÇÊ¨°ÊÑüÁü•Ê≥®ÊÑèÂäõ (Hierarchy-Aware Attention, HAA)\\n> *   ËØÅÊçÆÁêÜËÆ∫ (Evidence Theory, N/A)\"\n}\n```"
}