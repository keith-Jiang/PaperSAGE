{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.13333",
    "link": "https://arxiv.org/abs/2412.13333",
    "pdf_link": "https://arxiv.org/pdf/2412.13333.pdf",
    "title": "Beyond Accuracy: On the Effects of Fine-tuning Towards Vision-Language Model's Prediction Rationality",
    "authors": [
        "Qitong Wang",
        "Tang Li",
        "Kien X. Nguyen",
        "Xi Peng"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2024-12-17",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "DeepREAL Lab",
        "Department of Computer & Information Sciences",
        "University of Delaware"
    ],
    "paper_content": "# Beyond Accuracy: On the Effects of Fine-tuning Towards Vision-Language Model’s Prediction Rationality\n\nQitong Wang, Tang Li, Kien X. Nguyen, Xi Peng\n\nDeepREAL Lab, Department of Computer & Information Sciences, University of Delaware {wqtwjt, tangli, kxnguyen, xipeng}@udel.edu\n\n# Abstract\n\nVision-Language Models (VLMs), such as CLIP, have already seen widespread applications. Researchers actively engage in further fine-tuning VLMs in safetycritical domains. In these domains, prediction rationality is crucial: the prediction should be correct and based on valid evidence. Yet, for VLMs, the impact of fine-tuning on prediction rationality is seldomly investigated. To study this problem, we proposed two new metrics called Prediction Trustworthiness and Inference Reliability. We conducted extensive experiments on various settings and observed some interesting phenomena. On the one hand, we found that the welladopted fine-tuning methods led to more correct predictions based on invalid evidence. This potentially undermines the trustworthiness of correct predictions from fine-tuned VLMs. On the other hand, having identified valid evidence of target objects, fine-tuned VLMs were more likely to make correct predictions. Moreover, the findings are also consistent under distributional shifts and across various experimental settings. We hope our research offer fresh insights to VLM fine-tuning.\n\nCode — https://github.com/deep-real/vlm-pred-rationality Extended version — https://arxiv.org/pdf/2412.13333\n\n# Introduction\n\nVision-Language Models (VLMs), such as CLIP (Radford et al. 2021), have recently begun to see widespread adoption in high-stakes applications, such as healthcare (Wang et al. 2022b) and autonomous driving (Chen et al. 2023). A common practice in utilizing VLMs involves undertaking further fine-tuning (Goyal et al. 2023; Wortsman et al. 2022a; Wang et al. 2022b) in these models to their specific tasks rather than training deep models from scratch. While existing studies have evaluated mainstream fine-tuning methods, they have primarily focused on prediction accuracy (Kumar et al. 2022; Wortsman et al. 2022b; Goyal et al. 2023), overlooking an essential aspect: prediction rationality, where model predictions should not only be accurate but also grounded in valid evidence. Besides, the current academic community widely accepts that “clearly explaining a rationale for a classification decision to an end-user can be as important as the decision itself.” (Hendricks et al. 2016) One significant reason is that neglecting the model’s prediction rationality will cause severe consequences in safety-critical domains. For example, doctors employ a fine-tuned VLM which can accurately predict the presence of cancer tumors from X-ray images, to help decision-making. If its predictions are based on erroneous reasons: the input’s background instead of tumor region, doctors will lack trust in fine-tuned VLM, leading them to disregard the usage of the model. Therefore, in this paper, we study a crucial yet seldom investigated question: how do mainstream fine-tuning methods affect the rationality of VLM predictions?\n\nTo systematically study this question, we propose two new metrics to evaluate the rationality of VLM predictions after fine-tuning: (1) Prediction Trustworthiness (PT): the ratio of correct predictions with valid evidence overall correct predictions. (2) Inference Reliability (IR): the percentage of correct predictions given that the model has identified valid evidence of target objects. To assess whether the model focuses on valid evidence for the image classification task, we measure if the generated explanation heatmap from VLMs focuses on the target objects, based on the “Relevant Mass Accuracy (RMA)” score (Brandt, Raatjens, and Gaydadjiev 2023). We study the mainstream methods including “Zero-Shot” (ZS), “Linear-Probing” (LP), “Finetune Like CLIP Pretrain” (FLCP), and standard “Fine-tuning” (FT). We conducted extensive experiments and have obtained novel and consistent findings. Our results reveal that widely used fine-tuning methods exhibit significant limitations, yet they also possess certain advantages. Our key findings are summarized as follows:\n\nWill mainstream fine-tuning methods hurt the rationality of VLM predictions? Surprisingly yes! With our proposed “Prediction Trustworthiness” metric, fine-tuning results in more appearance of samples with correct predictions based on invalid evidence than zero-shot, making the correct predictions untrustworthy. For instance, with the ALBEF-ViT-B/16 model, compared with ZS, the PT scores of LP, FLCP, and FT drop $1 7 . { \\overset { } { 2 } } \\%$ , $1 3 . 8 5 \\%$ and $2 7 . 3 1 \\%$ respectively, on CalTech-101 (Li et al. 2022b) dataset, despite improving prediction accuracies. And with the CLIP\n\nViT-B/16 model, compared with ZS, the PT scores of LP, FLCP, and FT drop $6 . 4 \\%$ , $5 . 6 5 \\%$ , and $4 . 0 7 \\%$ respectively, on ImageNet-1K (Russakovsky et al. 2015) dataset. Notably, existing work (Goyal et al. 2023) highlights the effectiveness of fine-tuning for VLMs, asserting that FLCP consistently improves prediction accuracy and should be considered the “standard” method for fine-tuning CLIP. However, our findings suggest that this conclusion does not hold when evaluating the rationality of VLM predictions. This discrepancy underscores the importance of considering different possibilities when evaluating prediction rationality.\n\nWill valid evidence help enhance predictions made by fine-tuned VLMs? Yes. Using our “Inference Reliability” metric, we find that when VLMs focus on valid evidence of target objects, the prediction accuracy of fine-tuned VLMs improves. For example, in the ImageNet-1K dataset, with the CLIP-ViT-B/16 model, LP, FLCP, and FT outperform ZS in IR scores by $1 2 . 6 \\%$ , $8 . 6 7 \\%$ , and $1 6 . 9 2 \\%$ respectively. Existing works (Kumar et al. 2022; Wortsman et al. 2022b; Goyal et al. 2023), which study the positive impacts of VLM fine-tuning, are limited to the prediction accuracies. Our research provides insights into the impact of fine-tuning VLMs from a novel perspective, highlighting the benefits of fine-tuning in terms of enhancing prediction rationality.\n\nWill out-of-distribution data change our observations? No. There is a critical need to make sure that models work reliably in real-world situations, where the data distribution they encounter might be different from what they were trained on. For instance, the model must maintain stability and effectiveness in autonomous driving applications across various weather conditions. In parallel, previous work (Radford et al. 2021) has demonstrated the remarkable predictive performance of CLIP in both in-distribution and out-of-distribution data. Therefore, we discuss how our observations might change in the context of out-ofdistribution data. We find that all our findings remain consistent across various types and magnitudes of distributional shifts, as demonstrated through experiments in ImageNetC (Hendrycks and Dietterich 2018).\n\nLastly, we conducted ablation studies to verify the consistency of our findings, which remain consistent across various experimental settings, including different training optimizers, learning rates, explanation heatmap methods, and fine-tuning techniques such as prompt tuning (Zhou et al. 2022) and adapter tuning (Zhang et al. 2022).\n\nOur contribution lies in discovering new findings through extensive experiments across various benchmarks including ImageNet (Russakovsky et al. 2015), which are typical and widely used in the community. We provide novel insights about both the strengths and weaknesses of widely adopted fine-tuning strategies for VLMs, from the perspective of the rationality of VLM predictions. Moreover, our findings remain consistent across evaluation scenarios involving both in-distribution data and out-of-distribution data, as well as under various experimental settings. This paper provides new insights for people to rethink the effects of mainstream fine-tuning methods for VLMs.\n\n![](images/2972c2c7743d6767c81e0b71d15169f85ada402d9c4e6c914b438ee0492f64f5.jpg)  \nFigure 1: Both (a) and (b) have low responses to the background while (a) pays more attention to the whole body of the bird and (b) pays more attention to the discriminative feature of the bird (head). Compared with the IoU score between (a) and (b), the difference between them is negligible. Moreover, both achieve correct predictions. Input is from CUB-200-2011 (Wah et al. 2011) dataset. “GT” denotes abbreviation of “Ground Truth” and “Explan” denotes abbreviation of “Explanation”.\n\n# Preliminaries\n\nThere has been a surge of people exploring VLMs for their downstream tasks. A typical way is to use them for image classification (Goyal et al. 2023). In our prediction evaluations, we study the image classification task and measure model performances using the top-1 accuracy metric.\n\nWe evaluate whether the model provides valid evidence for its predictions by examining whether the explanation heatmap generated by VLMs focuses on the target objects. Specifically, a heatmap that strongly highlights key object regions while showing minimal responsiveness to background pixels indicates valid evidence. Therefore, we rely on the “Relevant Mass Accuracy (RMA)” score (Arras, Osman, and Samek 2022; Brandt, Raatjens, and Gaydadjiev 2023), which satisfies this criterion by measuring how much “mass” one method assigns to pixels within the region of target objects (ground truth). RMA score is calculated by determining the ratio of the total heatmap pixel values within the target object regions, to the sum of all pixel values across the entire heatmap. It requires both the generated explanation heatmap $( H )$ from VLMs and the ground truth explanation mask $( M )$ , whose pixels on the target objects are marked as 1 otherwise marked as 0. RMA score is defined as:\n\n$$\n\\mathrm { R M A } ( H , M ) = \\frac { \\sum H \\odot M } { \\sum H } ,\n$$\n\nwhere $\\odot$ represents Hadamard product. Note that the evaluations from many studies (Selvaraju et al. 2020; Arras, Osman, and Samek 2022) require the presence of ground-truth mask for heatmap localization.\n\nWe emphasize that the RMA metric provides a more reasonable evaluation for classification tasks compared to metrics like “Intersection over Union (IoU)” used in other works. For instance, Grad-CAM (Selvaraju et al. 2020) relies on the IoU score to measure the overlap between the explanation heatmap and the ground truth mask. However, the IoU score fails to reasonably evaluate two vastly different yet valid pieces of evidence. In Figure 1, we show two explanation heatmaps, (a) and (b), that are from different models.\n\n![](images/42cf5a8a8ba695819cbdc0f4facb239405b533c1185c054496f9bd732b81de3c.jpg)  \nFigure 2: Overview of the four quadrants (RR, RW, WR, WW) of Accuracy and Rationale that are utilized to evaluate prediction rationality.\n\nEven though the IoU metric treats them differently, both of them achieve correct predictions with valid evidence. They both exhibit a low response to background pixels. (a) pays attention to the whole body of the bird. (b) is also reasonable because it effectively identifies the distinguishing features of the bird, despite not highlighting the more complete bird region as in (a). This indicates that compared to IoU, RMA evaluation can fairly treat two distinct but valid evidence.\n\nExplanation Heatmap Generation. The method we use is directly from “Generic Attention Attribution” (Chefer, Gur, and Wolf 2021). In this case, the heatmaps are generated from attention maps of the transformer-based model, which is one of the most well-adopted methods, used in recent works including (Mao et al. 2023). It has been demonstrated in existing work (Liu et al. 2022) that it achieves the best faithfulness performance among all well-known explanation methods when applied to transformer-based models. The main idea is Hadamard’s product between attention maps and their gradient to the output. It is defined as:\n\n$$\n\\overline { { \\mathbf { A } } } = \\mathbb { E } _ { h } \\big ( \\big ( \\bigtriangledown \\mathbf { A } \\odot \\mathbf { A } \\big ) ^ { + } \\big ) ,\n$$\n\nwhere $\\odot$ is the Hadamard product, $\\begin{array} { r } { \\nabla \\mathbf { A } : = \\frac { \\partial y _ { t } } { \\partial A } } \\end{array}$ ∂yt for yt which is the model’s output for the class $t$ that we wish to visualize. $\\mathbb { E } _ { h }$ is the mean across the heads dimension. The $^ +$ indicates that the negative contributions are removed before averaging. Note that the class we explain are based on the index given by the annotations instead of predictions.\n\n# Our Proposed Evaluations\n\nWe present our evaluation protocols with two criteria in mind. (1) A trustworthy VLM should not produce instances of invalid evidence among samples with correct predictions. (2) When focusing on the correct predicted objects, a reliable VLM should leverage such valid evidence to achieve correct predictions. To determine whether the evidence (or rationale) of the model is correct, we use a threshold of 0.5 on the RMA measure. Specifically, an RMA score of 0.5 or above is considered valid evidence and vice versa. As a result, we achieve four scenarios: RR, RW, WR, and WW (Figure 2) that are used to formalize our two novel metrics:\n\n1. Prediction Trustworthiness (PT). A dependable and trustworthy model should generate valid evidence that corresponds to accurate predictions. Hence, we introduce the “PT” metric, which calculates the proportion of samples where the prediction is “right” and its evidence is also valid or “right” (RR) among all samples with right predictions, defined as:\n\n$$\n\\mathrm { P T } = \\frac { \\mathrm { R R } } { \\mathrm { R R } + \\mathrm { R W } } ,\n$$\n\nwhere “RW” denotes data with the “right” classifications based on invalid or “wrong” evidence. It is evident that an increase in the number of RW samples, i.e. irrational predictions, results in a decrease in PT scores.\n\n2. Inference Reliability $\\mathbf { \\left( I R \\right) }$ . Given that the model could pinpoint the regions of target objects, a reliable model should make correct predictions. Consequently, we introduce the “IR” metric, which calculates the proportion of samples with correct prediction and valid evidence among all samples with valid evidence of target objects, defined as:\n\n$$\n\\mathrm { I R } = \\frac { \\mathrm { R R } } { \\mathrm { R R } + \\mathrm { W R } } ,\n$$\n\nwhere “WR” denotes data with incorrect classifications with valid evidence. An increase in the number of WR samples results in a decrease in IR scores.\n\n# Experiments\n\n# Experimental Setup\n\nFine-tuning Methods. In this paper, we study fundamental methods including: (1) Zero-Shot (ZS), (2) Linear-Probing (LP), (3) Finetune Like CLIP Pretrain (FLCP), and (4) Finetuning (FT). For detailed information on these methods, please refer to our supplementary material in the extended version of our paper.\n\nModels. We study four VLMs: the first two models are CLIP-ViT-B/32 & 16 (Radford et al. 2021) from OpenAI, which manifest powerful zero-shot performances on image classification. The next two models are ALBEF-ViTB/16 (Li et al. 2021), pretrained on 14M image-text pairs, and BLIP-ViT-B/16 (Li et al. 2022c), pretrained on 129M image-text pairs, both developed by Salesforce. Their performances on the image classification task are also investigated in many works (Jonathan Roberts and Albanie 2023; Wang et al. 2022a).\n\nFine-tuning Setups. We maintain a consistent batch size and training epoch across all three fine-tuning methods (LP, FLCP, FT) for the same dataset and model. We employ the Adam (Kingma and Ba 2014) optimizer during the finetuning. For more details about fine-tuning, please consult our supplementary material.\n\nDatasets. In this paper, we conduct experiments on several datasets, including ImageNet (Russakovsky et al. 2015), CalTech-101 (Li et al. 2022b), Stanford-Dogs (Khosla et al. 2011), CUB-200-2011 (Wah et al. 2011), and ImageNetC (Hendrycks and Dietterich 2018). In CUB-200-2011 and CalTech-101 datasets, the 0-1 segmentation mask annotations directly serve as ground truth explanation masks. For images with bounding box annotations surrounding predicted instances (ImageNet, ImageNet-C, Stanford-Dogs), we generate ground truth explanation masks as follows: given initial masks whose pixel values are all zero, we mark the mask areas within boxes as one. For more detailed information about these datasets, please refer to the supplementary material.\n\nTable 1: Comparisons of four methods regarding prediction accuracy $( \\% )$ . The best-averaged score among the four methods is bolded, while the second-place averaged score is underlined. Due to the space limit, we abbreviate the names of datasets. Here, “IN”, “CT”, “SD”, “CUB” denote “ImageNet-1K”, “CalTech- $1 0 1 ^ { \\prime \\prime }$ , “Stanford-Dogs”, “CUB200-2011” respectively.   \n\n<html><body><table><tr><td rowspan=\"2\">Methods</td><td rowspan=\"2\">VLMs</td><td colspan=\"4\">Datasets</td><td rowspan=\"2\">Avg.</td></tr><tr><td>IN</td><td>CT</td><td>SD</td><td>CUB</td></tr><tr><td rowspan=\"4\">ZS</td><td>ALBEF-ViT-B/16</td><td>46.48</td><td>77.02</td><td>29.25</td><td>12.43</td><td rowspan=\"4\">53.74</td></tr><tr><td>BLIP-ViT-B/16</td><td>46.30</td><td>85.89</td><td>32.38</td><td>16.88</td></tr><tr><td>CLIP-ViT-B/16</td><td>63.30</td><td>84.22</td><td>60.61</td><td>54.94</td></tr><tr><td>CLIP-ViT-B/32</td><td>58.41</td><td>84.79</td><td>54.62</td><td>52.33</td></tr><tr><td rowspan=\"4\">LP</td><td>ALBEF-ViT-B/16</td><td>72.03</td><td>90.38</td><td>65.10</td><td>48.46</td><td rowspan=\"4\">72.50</td></tr><tr><td>BLIP-ViT-B/16</td><td>72.46</td><td>90.26</td><td>64.23</td><td>47.77</td></tr><tr><td>CLIP-ViT-B/16</td><td>76.69</td><td>94.64</td><td>74.14</td><td>70.14</td></tr><tr><td>CLIP-ViT-B/32</td><td>72.21</td><td>93.09</td><td>67.27</td><td>61.08</td></tr><tr><td rowspan=\"4\">FLCP</td><td>ALBEF-ViT-B/16</td><td>77.58</td><td>95.85</td><td>77.88</td><td>77.27</td><td rowspan=\"4\">80.99</td></tr><tr><td>BLIP-ViT-B/16</td><td>78.67</td><td>94.99</td><td>77.89</td><td></td></tr><tr><td>CLIP-ViT-B/16</td><td>72.41</td><td>96.20</td><td></td><td>68.85</td></tr><tr><td>CLIP-ViT-B/32</td><td>70.81</td><td>95.74</td><td>80.70 75.70</td><td>80.76 74.49</td></tr><tr><td rowspan=\"4\">FT</td><td>ALBEF-ViT-B/16</td><td>80.82</td><td>95.91</td><td></td><td></td><td rowspan=\"4\">81.63</td></tr><tr><td></td><td></td><td></td><td>81.32</td><td>80.48</td></tr><tr><td>BLIP-ViT-B/16 CLIP-ViT-B/16</td><td>80.75</td><td>92.74</td><td>78.68</td><td>68.98</td></tr><tr><td>CLIP-ViT-B/32</td><td>81.19 76.62</td><td>93.03 94.30</td><td>81.56 72.42</td><td>79.25 68.07</td></tr></table></body></html>\n\n# Weaknesses of Fine-tuning\n\nQuestion: Will mainstream fine-tuning methods hurt the rationality of VLM predictions?\n\nAnswer: Surprisingly yes! The well-adopted fine-tuning methods decrease the trustworthiness of VLM predictions in most settings: causing more samples with correct predictions based on invalid evidence.\n\nAlthough fine-tuning is able to improve the prediction accuracies of VLMs (see Table 1), we find mainstream fine-tuning methods lead to worse prediction trustworthiness, as shown in Table 2. For instance, in the ImageNet1K dataset, with CLIP-ViT-B/16 model, compared with ZS, fine-tuning deteriorates “Prediction Trustworthiness (PT)” performances by $6 . 4 \\%$ , $5 . 6 5 \\%$ and $4 . 0 7 \\%$ respectively. Our experimental results confirm the significant drawbacks of mainstream fine-tuning methods for VLMs: fine-tuning results in more instances where predictions are correct but the evidence which VLMs base on is invalid. This results in a reduced level of trustworthiness to VLM predictions. Lastly, there are rare exceptions with increased PT scores. This is likely due to the low zero-shot prediction accuracy of ALBEF $( 1 2 . 4 3 \\% )$ and BLIP $( 1 6 . 8 8 \\% )$ on CUB-200-2011. Finetuning introduces the missing knowledge to these models,\n\nleading to increased PT.\n\nTo further support our observation, we provide visualizations of the explanation heatmaps in Figure 3. We observe that widely adopted fine-tuning methods often amplify the responses of VLMs to pixels containing information irrelevant to the predicted objects. For instance, from the leftmost first-row comparisons, fine-tuning makes VLMs enhance responses on the human body or background instead of the hat (predicted category). Here we only show results on the CLIP-ViT-B/32 model with ImageNet-1K datasets due to space constraints. Please refer to our supplementary material for more visualizations.\n\nWhy does finetuning decrease trustworthiness? (1) VLMs tend to exploit the easiest path to minimize loss during finetuning, often picking up on spurious correlations or shortcuts present in the data. For instance, if all images of a particular class contain a common watermark or background, VLMs may associate that feature with the class label instead of learning the actual characteristics of the object. (2) Standard fine-tuning objectives usually prioritize improving prediction accuracy, but they do not account for the validity of the evidence used. As a result, there is no built-in mechanism to guide the model to focus on valid evidence.\n\nIn recent years, there have been some discussions regarding the excellence of fine-tuning for VLMs. For example, existing work (Goyal et al. 2023) shows that FLCP leads to uniformly better prediction performances. They claim that FLCP should be adopted as the “standard” method for finetuning CLIP. However, based on our discoveries, we contend that this conclusion doesn’t apply when considering the rationality of VLM predictions. Although FLCP significantly enhances VLMs’ prediction accuracies, we find that FLCP leads VLMs to provide more invalid evidence when making correct predictions, weakening the prediction trustworthiness of VLMs than ZS. This disparity highlights the significance of considering different possibilities when evaluating VLMs’ prediction rationality.\n\n# Strengths of Fine-tuning\n\nQuestion: Will valid evidence help enhance predictions made by fine-tuned VLMs?\n\nAnswer: Yes, they exhibit good inference reliability; i.e., when focusing on the valid evidence of target objects, finetuned VLMs are more likely to make correct predictions.\n\nThis phenomenon indicates better inference reliability of fine-tuning compared with ZS, as shown in Table 2. For example, in the ImageNet-1K dataset, with the CLIP-ViT-B/16 model, LP, FLCP, and FT outperform ZS by $1 2 . 6 \\%$ , $8 . 6 7 \\%$ , and $1 6 . 9 2 \\%$ respectively; with the CLIP-ViT-B/32 model, LP, FLCP, and FT outperform ZS by $1 3 . 8 2 \\%$ , $1 0 . 7 5 \\%$ , and $1 8 . 2 5 \\%$ respectively. This indicates that fine-tuning approaches contribute to less WR than ZS. When VLMs identify valid evidence for target objects, fine-tuning is more likely to produce correct predictions.\n\nExisting works (Kumar et al. 2022; Wortsman et al.\n\nTable 2: Comparisons of four methods with proposed “PT” and “IR” metrics. Here we observe that mainstream fine-tuning methods come with both strengths and weaknesses. We show that fine-tuning mostly leads to a worse capability of prediction trustworthiness but enhances the inference reliability of VLMs than the ZS method. The best-averaged score among the four methods is bolded, while the second-place averaged score is underlined.   \n\n<html><body><table><tr><td rowspan=\"2\">Evaluations</td><td rowspan=\"2\">Methods</td><td rowspan=\"2\">VLMs</td><td colspan=\"4\">CalTech-10Stanford-Dogs</td><td rowspan=\"2\">Avg.</td></tr><tr><td>ImageNet-1K</td><td></td><td></td><td>CUB-200-2011</td></tr><tr><td rowspan=\"10\">Prediction TrusPT,.rthiness</td><td rowspan=\"4\">ZS</td><td>ALBEF-ViT-B/16</td><td>90.61</td><td>76.28</td><td>95.02</td><td>49.31</td><td rowspan=\"4\">71.26</td></tr><tr><td>BLIP-ViT-B/16</td><td>89.01</td><td>61.72</td><td>93.95</td><td>23.93</td></tr><tr><td>CLIP-ViT-B/16</td><td>87.05</td><td>62.99</td><td>92.96</td><td>29.38</td></tr><tr><td>CLIP-ViT-B/32</td><td>89.39</td><td>73.44</td><td>94.58</td><td>30.57</td></tr><tr><td rowspan=\"3\">LP</td><td>ALBEF-ViT-B/16</td><td>82.37</td><td>59.08</td><td>90.30</td><td>19.91</td><td rowspan=\"3\">64.78</td></tr><tr><td>BLIP-ViT-B/16</td><td>80.36</td><td>52.57</td><td>92.63</td><td>12.98</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"3\">FLCP</td><td>CLIP-ViT-B/32</td><td>84.05</td><td>68.22</td><td>92.76</td><td>35.89</td><td rowspan=\"3\">67.95</td></tr><tr><td>ABLIP-VT-B/6</td><td>87.7</td><td></td><td>91</td><td>3617</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"3\">FT</td><td></td><td>85.48</td><td></td><td></td><td></td><td rowspan=\"3\"></td></tr><tr><td>CLIP-ViT-B/32 ALBEF-ViT-B/16</td><td></td><td>71.29</td><td>91.59</td><td>23.84</td></tr><tr><td></td><td>86.28</td><td>48.97</td><td>92.22</td><td>24.98</td></tr><tr><td rowspan=\"3\"></td><td>BLIP-ViT-B/16</td><td>85.54</td><td>39.96</td><td>93.13</td><td>25.85</td><td rowspan=\"3\">67.01</td></tr><tr><td>CLIP-ViT-B/32</td><td>86.29</td><td>80.01</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td>94.17</td><td>55.43</td></tr><tr><td rowspan=\"9\">Inference ReRiabi Ty</td><td rowspan=\"3\">ZS</td><td>ALBEF-ViT-B/16</td><td>48.95</td><td>76.74</td><td>30.56</td><td>16.43</td><td rowspan=\"3\">56.65</td></tr><tr><td></td><td>4.63</td><td>90.08</td><td>3.87</td><td>18.92</td></tr><tr><td>BLIP-ViT-B/16</td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"3\">LP</td><td>CLIP-ViT-B/32</td><td>61.09</td><td>85.23</td><td>56.12</td><td>56.80</td><td rowspan=\"3\">75.67</td></tr><tr><td>ALBEF-ViT-B/16</td><td>74.76</td><td>92.56</td><td>66.21</td><td>55.46</td></tr><tr><td>BLIP-ViT-B/16</td><td>74.78</td><td></td><td></td><td></td></tr><tr><td rowspan=\"3\"></td><td></td><td></td><td>90.89</td><td>65.11</td><td>59.91</td><td rowspan=\"3\"></td></tr><tr><td>CLIP-ViT-B/32</td><td>74.91</td><td>93.76</td><td>68.53</td><td>67.37</td></tr><tr><td>ABLIPVT-B/6</td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"3\">FLCP</td><td></td><td></td><td></td><td></td><td></td><td rowspan=\"3\">81.71</td></tr><tr><td></td><td>8</td><td></td><td></td><td>7</td></tr><tr><td>CLIP-ViT-B/32</td><td>71.84</td><td>95.46</td><td>76.43</td><td>73.87</td></tr><tr><td rowspan=\"3\">FT</td><td>ALBEF-ViT-B/16</td><td>82.95</td><td>94.41</td><td>81.93</td><td>81.87</td><td rowspan=\"3\">83.52</td></tr><tr><td></td><td>82.86</td><td>91.5</td><td>79.18</td><td>85.22</td></tr><tr><td>BLIP-ViT-B/16</td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"3\"></td><td>CLIP-ViT-B/32</td><td>79.34</td><td>93.86</td><td>73.22</td><td>72.72</td><td rowspan=\"3\"></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>\n\n2022b; Goyal et al. 2023) are limited to the impact of mainstream VLM fine-tuning methods regarding predictive accuracies, ignoring their positive impacts on VLM prediction rationality. In this paper, we have analyzed and explored the benefits of fine-tuning VLMs from a new perspective. Our experimental results show that fine-tuning has its merits and is not completely worthless for the prediction rationality of VLMs.\n\nIn summary, we conducted extensive experiments to validate the existing mainstream VLM fine-tuning methods in terms of both their strengths and weaknesses from a prediction rationality perspective. On the one hand, fine-tuning leads to good inference reliability: when provided with valid evidence of target objects, fine-tuned VLMs are more likely to generate accurate predictions. On the other hand, we also confirm that mainstream fine-tuning methods tend to hurt the inherent capabilities of VLMs, specifically in terms of prediction trustworthiness. These are aspects that merit attention from the community of machine learning.\n\n# Analysis on Out-of-Distribution Data\n\n# Question: Will out-of-distribution data change our observations?\n\nAnswer: No, all findings remain consistent.\n\nDistributional shifts has garnered significant attention in the field of machine learning (Qiao, Zhao, and Peng 2020; Qiao and Peng 2023). During the fine-tuning, the distributional discrepancy between the fine-tuning and testing data is worth considering. Real-world data distributions can change due to factors such as time, location, and environment. Testing on out-of-distribution data helps simulate these changes, ensuring the model performs well in diverse scenarios. For example, in autonomous driving, the models need to remain stable in multiple weather conditions.\n\nIn this section, we study the fine-tuning methods when testing on out-of-distribution data. Here we use the ImageNet-C dataset, which includes multiple corruption categories and levels of severity. As shown in Figure 4, our key\n\n![](images/50acf95cd5c09c67b2410e88ce9aac5faec5db7365260c6930763ad5ec085fa5.jpg)  \nFigure 3: Visualization comparisons among different methods. Compared with zero-shot (ZS), current mainstream fine-tuning methods (LP, FLCP, and FT) for VLMs tend to show enhanced responses in background pixels that are irrelevant to predictions. Here we select the samples for which all four methods make correct predictions. Here we display bounding box annotations indicating the positions of the predicted target.\n\nfindings are as follows:\n\n1. Fine-tuning on in-distribution data can enhance the prediction accuracy for out-of-distribution data.   \n2. However, the mainstream fine-tuning methods still compromise the prediction trustworthiness of VLM, which brings more samples with correct prediction based on invalid evidence, compared with zero-shot.   \n3. Fine-tuning tends to enhance the inference reliability of VLMs: when focusing on correct prediction objects, finetuned VLMs are more likely to give correct predictions.\n\nTherefore, we extend our previous findings to scenarios involving out-of-distribution data, demonstrating the consistency of our discoveries.\n\nOur conclusions also remain unaffected when the prediction accuracies decrease caused by corruption strength increases. Therefore, we think our findings may not change with variations in model prediction accuracy.\n\n# Ablations studies\n\nTo ensure the consistency of our findings across different experimental settings, we perform a comprehensive series of ablation studies. We investigate the effects under different setups including: (1) Experiments with another popular optimizer: AdamW (Loshchilov and Hutter 2017). (2) Experiments with another widely-used explanation method: gradient of attention $( \\nabla \\mathbf { A } )$ based (Serrano and Smith 2019) method. The main idea of this method is to utilize the gradient of attention to the output as an explanation heatmap, where $\\begin{array} { r } { \\nabla \\mathbf { A } : = \\frac { \\partial y _ { t } } { \\partial A } } \\end{array}$ for $y _ { t }$ which is the model’s output for the class $t$ . (3) Results with different fine-tuning learning rates (abbreviated as “LR”): $5 e - 4$ for “LP”, $3 e - 6$ for “FLCP”, and $2 e { \\mathrm { - } } 5$ for “FT”, compared with the original setup, where we set learning rates as $1 e - 3$ for “LP”, $5 e - 6$ for “FLCP”, and $1 e \\mathrm { ~ - ~ } 5$ for “FT”. For the original learning rate settings regarding other models and datasets please refer to our supplementary material. Note that the aforementioned three experiments are conducted with the CLIP-ViT-B/32 model on the ImageNet-1K.\n\nAs shown in Table 3, our findings remain unaffected with multiple setups. On the one hand, prevalent fine-tuning approaches tend to increase the instances with correct predictions based on invalid evidence, despite the enhancement in prediction accuracy. On the other hand, fine-tuning typically demonstrates strong inference reliability.\n\nRecently, there have been other fine-tuning techniques proposed by the community including prompt tuning (Zhou et al. 2022), and adapter tuning (Zhang et al. 2022). We find that our findings are also consistent under these fine-tuning methods. Due to the space limits please refer to our supplementary material for the related experimental results and introduction of these methods.\n\n# Related Works\n\n# Multimodal Foundation Models\n\nIn recent years, there has been a surge of interest in research regarding Vision-Language Models (VLMs). These VLMs (Radford et al. 2021; Li et al. 2021, 2022c; Singh et al. 2022; Jia et al. 2021; Li et al. 2022a,e; Yuan et al. 2021; Li et al. 2022d, 2023; Chen and Wang 2022; Zhong et al. 2022; Kim, Son, and Kim 2021; Chen et al. 2020), have attracted substantial attention due to their remarkable capacity to achieve robust performance, both in zero-shot and fine-tuned scenarios, across a diverse spectrum of visionlanguage-related tasks (Antol et al. 2015; Vinyals and Le 2015; Xie et al. 2019; Suhr et al. 2017). Notably, CLIP (Radford et al. 2021), as a prominent exemplar in this domain, has also demonstrated exceptional zero-shot performance in image classification. The contrastive learning approach it employs has also found applications in fields such as mul\n\n![](images/93eb44ffb7f6d88f2ad6baa352f5186c1ce1b3ea105596b7af94c124f77466c7.jpg)  \nFigure 4: Experimental results on out-of-distribution data. Our discoveries remain consistent across various types and magnitudes of distributional shifts. The $\\mathbf { X }$ -axis in all figures represents the strength of corruption, where a strength of 0 indicates the results of different methods on the original ImageNet validation data. Due to space constraints, we only show results with CLIP-ViT-B/32 and four types of corruption in the main paper. For more results, please refer to our supplementary material.\n\nTable 3: Ablation studies with prediction accuracy, and our proposed “Prediction Trustworthiness (PT)” and “Inference Reliability (IR)” metrics. Our findings are unaffected under different experimental setups. The best score is bolded.   \n\n<html><body><table><tr><td rowspan=\"2\">Setup</td><td rowspan=\"2\">Evaluations</td><td colspan=\"4\">Methods</td></tr><tr><td>ZS</td><td>LP</td><td>FLCP</td><td>FT</td></tr><tr><td rowspan=\"2\">AdamW Optimizer</td><td>Pred. Acc.(%) ↑</td><td>58.41</td><td>72.22</td><td>70.88</td><td>76.53</td></tr><tr><td>PT(%) ↑</td><td>89.39</td><td>84.23</td><td>85.53</td><td>86.46</td></tr><tr><td></td><td>IR(%) ↑</td><td>61.09</td><td>74.93</td><td>71.93</td><td>79.26</td></tr><tr><td>A</td><td>Pred. Acc.(%) ↑</td><td>58.41</td><td>72.21</td><td>70.81</td><td>76.62</td></tr><tr><td>Explanation</td><td>PT(%) ↑</td><td>74.79</td><td>63.62</td><td>65.21</td><td>65.76</td></tr><tr><td>Heatmap</td><td>IR(%) ↑</td><td>61.18</td><td>75.18</td><td>71.87</td><td>79.68</td></tr><tr><td>Different LRs</td><td>Pred. Acc.(%) ↑</td><td>58.41</td><td>72.28</td><td>70.05</td><td>75.51</td></tr><tr><td>Compared with</td><td>PT(%) ↑</td><td>89.39</td><td>84.72</td><td>86.19</td><td>86.59</td></tr><tr><td>Original</td><td>IR(%) ↑</td><td>61.09</td><td>74.91</td><td>71.21</td><td>78.62</td></tr></table></body></html>\n\nDas, and Saenko 2018) study from the perspective of faithfulness; i.e., how accurately an explanation method reflects the true decision-making process of a model. In parallel, Mao et al. (Mao et al. 2023) propose the concept of a reliable model, emphasizing the importance of the ”doublyright” criterion: both accurate predictions and fine-grained language explanations of model decision-making. Recently, some works (Li, Ma, and Peng 2024a,b) have increasingly required VLM models to deliver not only accurate predictions but also correct rationales. In this paper, we explore the impact of widely accepted fine-tuning methods on the prediction rationality of VLMs for vision tasks such as image classification, providing novel insights about VLM finetuning within the XML research community. And we highlight that faithfulness is beyond the scope of our study due to two reasons. On the one hand, faithfulness evaluations primarily focus on assessing the correctness of heatmap explanation methods. On the other hand, existing work (Liu et al. 2022) verified the superiority of our employed explanation generation method.\n\ntiview analysis (Tian, Krishnan, and Isola 2020) and egocentric video understanding (Wang et al. 2023). Recently, researchers have engaged in fine-tuning (Goyal et al. 2023) VLMs to better adapt them to specific downstream tasks. However, the impact of such training on the prediction rationality of these models remains an open research problem, one that warrants in-depth exploration and investigation.\n\n# Explainable Machine Learning\n\nExplainable Machine Learning (XML) is crucial for promoting transparency, trust, accountability, and fairness in AI systems. Researchers frequently employ techniques to explain neural network operations and decision-making regarding input data. Activation heatmaps such as GradCAM (Selvaraju et al. 2020), visualize important regions for specific classes. In light of the proliferation of transformerbased models (Dosovitskiy et al. 2020), researchers start exploring the feasibility of utilizing attention maps, taking it as a way to provide explanations (Chefer, Gur, and Wolf 2021). In order to evaluate the quality of these explanation generation methods, existing works including (Petsiuk,\n\n# Conclusion\n\nPrediction rationality is an important aspect to consider when fine-tuning Vision-Language Models (VLMs), especially in high-stakes applications. This paper provides a comprehensive assessment of the commonly used finetuning approaches, presenting some insights on both advantages and disadvantages. On the one hand, they generally demonstrate strong inference reliability. More specifically, when focusing on the valid evidence of target objects, the fine-tuned VLMs are more likely to make correct predictions. On the other hand, fine-tuning often results in undermining the trustworthiness of VLM predictions by bringing more data samples with correct predictions based on invalid evidence. We further observe that our discoveries are consistent across various types and magnitudes of distributional shifts, and remain unaffected with multiple setups. To ensure that VLMs can be reliably used in high-stack applications, it will be crucial to study new fine-tuning methods that can improve VLM prediction rationality. We leave it as future works. We expect our research may provide useful experience and advance the study of VLM fine-tuning.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文研究了主流微调方法对视觉语言模型（VLMs）预测合理性的影响。预测合理性要求模型不仅预测准确，还应基于有效证据。这一问题在安全关键领域（如医疗和自动驾驶）尤为重要，因为错误的预测依据会削弱模型的可信度。\\n> *   现有研究主要关注微调对预测准确性的提升，而忽视了预测合理性，导致微调后的VLMs可能基于无效证据做出正确预测，从而降低其可信度。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了两个新指标——预测可信度（PT）和推理可靠性（IR），用于系统评估微调后VLMs的预测合理性，并通过大量实验验证了主流微调方法的优缺点。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **贡献1：** 提出PT和IR指标，首次系统评估微调对VLMs预测合理性的影响。\\n> *   **贡献2：** 发现主流微调方法（如LP、FLCP、FT）虽然提升准确性，但会降低PT（例如，CLIP-ViT-B/16在ImageNet-1K上PT下降4.07%-6.4%）。\\n> *   **贡献3：** 验证微调能提高IR（例如，CLIP-ViT-B/16在ImageNet-1K上IR提升8.67%-16.92%），表明当VLMs关注有效证据时，微调模型更可能做出正确预测。\\n> *   **贡献4：** 所有发现在分布外数据（如ImageNet-C）上保持一致，表明结论具有鲁棒性。\\n> *   **贡献5：** 通过消融实验验证了不同优化器、学习率和解释热图方法对结论的影响，确保结果的稳定性。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   论文的核心是通过量化VLMs预测的合理性，揭示微调方法的潜在问题。PT衡量正确预测中基于有效证据的比例，IR衡量在有效证据下模型做出正确预测的概率。\\n> *   设计哲学是：在安全关键领域，模型的预测不仅需要正确，还应基于合理的证据，否则可能引发信任危机。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 现有研究仅关注微调对准确性的提升，而忽略了证据有效性（如Goyal et al. 2023主张FLCP是微调CLIP的“标准方法”，但未评估其合理性）。\\n> *   **本文的改进：** 提出PT和IR指标，结合RMA评分（评估热图是否聚焦目标物体），首次从合理性角度分析微调效果。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.   **数据准备：** 使用ImageNet、CalTech-101等数据集，生成目标物体的真实掩码（Ground Truth Mask）。\\n> 2.   **热图生成：** 采用“通用注意力归因”方法（Chefer et al. 2021）生成解释热图，计算其与真实掩码的RMA分数（公式：$\\\\mathrm{RMA}(H, M) = \\\\frac{\\\\sum H \\\\odot M}{\\\\sum H}$）。\\n> 3.   **指标计算：** 定义PT为RR/(RR+RW)，IR为RR/(RR+WR)，其中RR为“正确预测+有效证据”，RW为“正确预测+无效证据”，WR为“错误预测+有效证据”。\\n> 4.   **实验验证：** 对比零样本（ZS）、线性探测（LP）、FLCP和标准微调（FT）在不同数据集和模型上的PT与IR表现。\\n\\n> **案例解析 (Case Study)**\\n> *   论文提供了可视化热图比较（如图3），显示微调方法（如LP、FLCP、FT）相比零样本（ZS）增强了背景像素的响应，而忽略了目标物体区域。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   零样本（ZS）、线性探测（LP）、FLCP（Finetune Like CLIP Pretrain）、标准微调（FT）。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在预测准确性上：** 微调方法（LP、FLCP、FT）显著优于ZS。例如，CLIP-ViT-B/16在ImageNet-1K上，FT的准确率达76.62%，远高于ZS的58.41%。\\n> *   **在预测可信度（PT）上：** 微调方法普遍降低PT。例如，CLIP-ViT-B/16在ImageNet-1K上，FT的PT为86.29%，低于ZS的89.39%。\\n> *   **在推理可靠性（IR）上：** 微调方法提升IR。例如，CLIP-ViT-B/16在ImageNet-1K上，FT的IR为79.34%，高于ZS的61.09%。\\n> *   **在分布外数据上：** 所有结论在ImageNet-C数据集上保持一致，表明微调对分布偏移具有鲁棒性。\\n> *   **在消融实验上：** 使用不同优化器（如AdamW）、学习率和解释热图方法（如梯度注意力）时，PT和IR的变化趋势与主实验一致。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   视觉语言模型 (Vision-Language Models, VLMs)\\n*   微调 (Fine-tuning, FT)\\n*   预测合理性 (Prediction Rationality, N/A)\\n*   预测可信度 (Prediction Trustworthiness, PT)\\n*   推理可靠性 (Inference Reliability, IR)\\n*   相关质量准确率 (Relevant Mass Accuracy, RMA)\\n*   安全关键领域 (Safety-Critical Domains, N/A)\\n*   分布偏移 (Distributional Shifts, N/A)\\n*   注意力归因 (Attention Attribution, N/A)\"\n}\n```"
}