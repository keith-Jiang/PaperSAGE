{
    "source": "Semantic Scholar",
    "arxiv_id": "2410.09437",
    "link": "https://arxiv.org/abs/2410.09437",
    "pdf_link": "https://arxiv.org/pdf/2410.09437.pdf",
    "title": "MTL-LoRA: Low-Rank Adaptation for Multi-Task Learning",
    "authors": [
        "Yaming Yang",
        "Dilixat Muhtar",
        "Yelong Shen",
        "Yuefeng Zhan",
        "Jianfeng Liu",
        "Yujing Wang",
        "Hao Sun",
        "Denvy Deng",
        "Feng Sun",
        "Qi Zhang",
        "Weizhu Chen",
        "Yunhai Tong"
    ],
    "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
    ],
    "publication_date": "2024-10-12",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 4,
    "influential_citation_count": 1,
    "institutions": [
        "Peking University",
        "Nanjing University",
        "Microsoft Corporation"
    ],
    "paper_content": "# MTL-LoRA: Low-Rank Adaptation for Multi-Task Learning\n\nYaming $\\mathbf { Y a n g } ^ { 1 * \\dagger }$ , Dilxat Muhtar2\\*‚Ä°, Yelong Shen3\\*, Yuefeng Zhan3, Jianfeng $\\mathbf { L i u } ^ { 3 }$ , Yujing Wang3 ¬ß, Hao $\\mathbf { S u n } ^ { 3 }$ , Weiwei Deng3, Feng $\\mathbf { S u n } ^ { 3 }$ , Qi Zhang3, Weizhu Chen3, Yunhai Tong1\n\n1Peking University 2Nanjing University 3Microsoft Corporation yamingyang@stu.pku.edu.cn, dmuhtar@smail.nju.edu.cn, {yeshe, yuefzh, jianfengliu, yujwang}@microsoft.com yhtong@pku.edu.cn\n\n# Abstract\n\nParameter-efficient fine-tuning (PEFT) has been widely employed for domain adaptation, with LoRA being one of the most prominent methods due to its simplicity and effectiveness. However, in multi-task learning (MTL) scenarios, LoRA tends to obscure the distinction between tasks by projecting sparse high-dimensional features from different tasks into the same dense low-dimensional intrinsic space. This leads to task interference and suboptimal performance for LoRA and its variants. To tackle this challenge, we propose MTL-LoRA, which retains the advantages of low-rank adaptation while significantly enhancing MTL capabilities. MTL-LoRA augments LoRA by incorporating additional task-adaptive parameters that differentiate task-specific information and capture shared knowledge across various tasks within low-dimensional spaces. This approach enables pretrained models to jointly adapt to different target domains with a limited number of trainable parameters. Comprehensive experimental results, including evaluations on public academic benchmarks for natural language understanding, commonsense reasoning, and image-text understanding, as well as real-world industrial text Ads relevance datasets, demonstrate that MTL-LoRA outperforms LoRA and its various variants with comparable or even fewer learnable parameters in MTL setting.\n\n# Code ‚Äî https://github.com/microsoft/MTL-LoRA Extended version ‚Äî https://arxiv.org/abs/2410.09437\n\n# Introduction\n\nLarge language models (LLMs) (Brown et al. 2020; Touvron et al. 2023; Team 2023; Yang et al. 2024; Abdin et al. 2024) now drive critical advances in artificial intelligence. By scaling model size and leveraging extensive datasets, LLMs demonstrate exceptional generalization and advanced multitask capabilities (Wei et al. 2022a; Hoffmann et al. 2022). The concept of ‚Äúserving one model for different tasks‚Äù has led to numerous applications, ranging from natural language processing (Qin et al. 2023) to various domain-specific implementations (Zhao et al. 2023; Wei et al. 2022b; Min et al. 2023). Despite their high generalizability, LLMs still require fine-tuning for specific domains or to update their knowledge base. However, the vast number of parameters in LLMs poses significant challenges regarding computational efficiency and memory consumption during fine-tuning.\n\n![](images/925422fd2acee91c145b752ca07700c5d01c08ab40f14ba2f362d3e0c08f0716.jpg)  \nFigure 1: t-SNE visualization of task-specific features extracted from the $\\mathbf { o }$ linear layer of the final block in the LLaMA2-7B model, comparing LoRA and MTL-LoRA after fine-tuning on a commonsense reasoning dataset.\n\nParameter-efficient fine-tuning (PEFT) addresses this challenge by keeping the pre-trained model frozen and finetuning lightweight adapters (He et al. 2021; Houlsby et al. 2019). A prominent PEFT approach is low-rank adaptation (LoRA) (Hu et al. 2021), which trains low-rank ‚Äùadapter‚Äù layers in selected model components. LoRA builds on the insight that fine-tuning updates in pre-trained LLMs exhibit low ‚Äùintrinsic rank‚Äù during task specialization (Aghajanyan, Zettlemoyer, and Gupta 2020), enabling effective approximation through targeted adapters. While LoRA and its recent variants (Liu et al. 2024; Shi et al. 2024; Hayou, Ghosh, and $\\mathrm { Y u } 2 0 2 4 )$ have shown promise across diverse LLM adaptation scenarios (Liu et al. 2023a; Huang et al. 2023), the growing task complexity and individual fine-tuning costs have spurred research into simultaneous multi-task LoRA training. In this scenario, LoRA projects features from different tasks from a sparse high-dimensional space into a shared, dense low-dimensional space. This projection causes interference and task confusion, amplifying the loss of taskspecific information (as shown in the first row of Figure 1). While information sharing is essential in multi-task learning (MTL), it is crucial that different task combinations exchange information distinctly. This necessitates additional design to enable LLMs to adaptively learn varied task information sharing strategies during fine-tuning. Recent LoRA variants have attempted improvements in the multi-task setting, such as ensembling multiple LoRA adapters (Wang et al. 2023) or adopting Mixture of Experts (MoE) structures for soft information specification (Liu et al. 2023b). However, these approaches do not effectively segregate taskspecific information or implement distinct information sharing strategies, making them less effective in multi-task scenarios.\n\nIn this work, we introduce MTL-LoRA, a LoRA method designed to enhance LLMs with the capability to tackle a variety of tasks in a parameter-efficient manner. MTL-LoRA innovates by implementing task-specific transformations in low-rank space along with a strategy for adaptively exploring multiple information sharing methods. Specifically, MTL-LoRA begins by projecting inputs into lower intrinsic dimensions similar to LoRA. To mitigate the risk of crosstask information interference (Hofmann, Scho¬®lkopf, and Smola 2008) within such a condensed space, MTL-LoRA introduces a learnable transformation for each task, ensuring the preservation of task-specific information. Furthermore, acknowledging the role of information sharing among different tasks, especially in enhancing the performance of tasks with limited resources (Crawshaw 2020), MTL-LoRA adopts a dynamic approach to learn different strategies for information sharing. With these improvements, MTL-LoRA efficiently assimilates both task-specific and shared information with minimal trainable parameters. Comprehensive experiments on public academic benchmarks as well as realworld applications demonstrate that MTL-LoRA unleashes the multi-tasking capabilities of LLMs by fine-tuning a limited number of parameters, outperforming LoRA and its variants including.\n\nThe key contributions of our work can be summarized as follows:\n\n1 We present MTL-LoRA, which improves the capability\n\nof LoRA in MTL through an innovative approach for extracting task-specific information and enhancing crosstask information sharing.   \n2 Comprehensive experimental evaluations validate the efficacy of MTL-LoRA on both public academic benchmarks and real-world applications.   \n3 Through extensive ablation experiments and analyses, we confirm the effectiveness of each component of MTLLoRA and validate the underlying design motivations.\n\n# Related Work\n\n# Parameter-Efficient Fine-Tuning\n\nParameter-efficient fine-tuning (PEFT) adapts large pretrained models for specific tasks or domains using a small portion of parameters while keeping the main model frozen (He et al. 2021). A popular approach involves inserting trainable, continuous prompts or embeddings into the original text sequence to leverage the base model‚Äôs knowledge for new tasks (Li and Liang 2021; Liu et al. 2022). Another approach adds additional neural modules, like adapter structures, into pre-trained models (Houlsby et al. 2019; Pfeiffer et al. 2020; Lin, Madotto, and Fung 2020). In this trend, LoRA (Hu et al. 2021) utilizes the concept of low intrinsic dimension (Aghajanyan, Zettlemoyer, and Gupta 2020), introducing two trainable rank decomposition matrices into frozen pre-trained models to estimate the accumulated gradient update during fine-tuning. Due to its lower inference latency and superior performance, LoRA has been widely adopted, and many studies are exploring ways to enhance its efficiency and stability. For example, AdaLoRA (Zhang et al. 2023) incorporates an importanceaware rank allocation method to assign ranks according to layer importance. $\\mathrm { L o R A + }$ (Hayou, Ghosh, and Yu 2024) aims to improve LoRA‚Äôs training stability by using different learning rates for different low-rank matrices. DoRA (Liu et al. 2024) further enhances both the learning capacity and training stability of LoRA by decomposing the pretrained weights into two components, magnitude and direction, for fine-tuning. While LoRA and its variants show promise in single-task adaptations, their effectiveness diminishes in multi-task scenarios as they update parameters uniformly across all tasks, overlooking crucial task-specific information and dynamic task information sharing. Our work focuses on improving LoRA to acquire both task-specific and task-agnostic knowledge, enhancing its performance in multi-task settings.\n\n# Multi-Task Learning\n\nMulti-Task Learning (MTL) aims to optimize all tasks jointly and adapt a single trained model to serve for all tasks (Crawshaw 2020). Since language models trained on large-scale datasets can extract universal representations, previous multi-task learning methods, such as MTDNN (Liu et al. 2019), typically use a shared pre-trained model with task-specific heads to jointly adapt the model to different tasks. However, as the size of pre-trained models continues to increase, full fine-tuning (FT) introduces significant computational overhead and an increased risk of catastrophic forgetting (Biderman et al. 2024). While LoRA offers an alternative to FT, it does not perform as well in multi-task settings (Wang et al. 2023). Approaches like MultiLoRA (Wang et al. 2023) and MoELoRA (Liu et al. 2023b) improve LoRA‚Äôs multi-task performance in joint training scenarios by integrating multiple LoRAs or utilizing expert routing. However, they fail to strike a good balance between task-specific information and task-information sharing, resulting in suboptimal performance.\n\n![](images/cc3a24b1fce70bba92d5f9ff351bd958739d22e94bcc764a1993e221bd64cbd9.jpg)  \nFigure 2: The overall architecture of MTL-LoRA. MTLLoRA employs task-specific transformation matrices and multiple up-projection matrices to learn both task-specific and shared information.\n\n# Method\n\nIn this section, we first introduce the low-rank adaptation method, followed by an in-depth explanation of the proposed MTL-LoRA for multi-task learning.\n\n# Low-Rank Adaption\n\nCurrent LLMs generally follow a decoder-only structure, characterized by a series of blocks, each comprising two key components with residual connections: a multi-head self-attention (MHA) layer and a feed-forward network (FFN) (Brown et al. 2020; Touvron et al. 2023; Team 2023). The MHA layer involves using dense learnable matrices $\\mathbf { W } _ { q }$ , $\\mathbf { W } _ { k }$ , $\\mathbf { W } _ { v }$ , and $\\mathbf { W } _ { o }$ to mix the sequence $x$ according to inter-relationships between tokens:\n\n$$\n\\mathbf { M H A } ( \\mathbf { x } ) = \\mathrm { S o f t m a x } \\left( \\frac { ( \\mathbf { W } _ { q } \\mathbf { x } ) ^ { T } \\mathbf { W } _ { k } \\mathbf { x } } { \\sqrt { k } } \\right) ( \\mathbf { W } _ { v } \\mathbf { x } ) ^ { T } \\mathbf { W } _ { o } ,\n$$\n\nwhere we assume a single attention head and $k$ denotes the hidden dimension for the head. The FFN layer is usually an MLP with two dense linear projection layers, $\\mathbf { W } _ { d o w n }$ and $\\mathbf { W } _ { u p }$ , and a non-linear activation function $\\sigma ( \\cdot )$ for channel mixing:\n\n$$\n\\begin{array} { r } { \\mathrm { F F N } ( \\mathrm { x } ) = \\sigma ( \\mathbf { x W } _ { d o w n } ) \\mathbf { W } _ { u p } . } \\end{array}\n$$\n\nAlthough LLMs pre-trained with extensive general domain datasets have demonstrated remarkable generalization abilities, there is a need to adapt these models for specific tasks or domains with limited resources. To achieve this, low-rank adaptation (LoRA), inspired by the concept of low intrinsic dimensionality in LLMs, decomposes the weight gradient $\\Delta \\mathbf { W }$ into low-rank matrices, thereby reducing the number of trainable parameters. Specifically, for a dense weight matrix $\\mathbf { W } \\in \\mathbf { \\overline { { \\mathbb { R } } } } ^ { d \\times k }$ , LoRA employs two low-rank matrices, $\\mathbf { B } \\in \\mathbb { R } ^ { d \\times r }$ and $\\mathbf { A } \\in \\mathbb { R } ^ { r \\times k }$ , to approximate the accumulated gradient updates $\\Delta \\mathbf { W }$ . The rank $r$ is chosen to be much smaller than the minimum of $d$ and $k$ , effectively decreasing the number of trainable parameters. Consequently, the resulting weight matrix is expressed as $\\mathbf { W } + \\mathbf { B } \\mathbf { A }$ , and the output $h$ for an input $x$ through this updated weight matrix is formulated as:\n\n$$\nh = ( \\mathbf { W } + \\Delta \\mathbf { W } ) x = \\mathbf { W } x + \\mathbf { B } \\mathbf { A } x\n$$\n\nIn implementation, the low-rank matrix $\\mathbf { A }$ is initialized with Kaiming Uniform (He et al. 2015) and $\\mathbf { B }$ is initialized with zero to ensure $\\Delta \\mathbf { W } = 0$ at the start, thereby contributing to training stability. A constant scale factor $\\alpha$ is also introduced to adjust the magnitude of the changes of the updated matrix $\\Delta \\mathbf { W }$ made by LoRA modules.\n\n# MTL-LoRA\n\nWhile LoRA effectively fine-tunes LLMs for specific domains using minimal trainable parameters, it does not fully accommodate the dynamics of task-specific and shared knowledge within different tasks or domains, thereby limiting its effectiveness in MTL settings. To address this issue, we introduce MTL-LoRA to improve LoRA with enhanced MTL abilities. The architecture of the proposed MTL-LoRA is detailed in Figure 2. For a given input ${ \\bf x } _ { t }$ corresponding to task $t$ , MTL-LoRA projects ${ \\bf x } _ { t }$ to low dimension space through A as in LoRA. However, to enhance the differentiation of tasks within this low, information-dense feature space and maintain task-specific information, MTL-LoRA incorporates a low-rank learnable matrix $\\Lambda _ { t } \\ \\in \\ \\mathbb { R } ^ { r \\times r }$ for each task. This process involves transforming the projected sample $\\mathbf { A } \\mathrm { x } _ { t }$ via $\\Lambda _ { t }$ to isolate information pertinent to the specific task. Furthermore, we argue that diverse informationsharing strategies are critical for leveraging knowledge from different tasks to improve overall performance. Therefore, rather than relying on a single up-project matrix for information aggregation, we utilize multiple low-rank matrices to explore various information sharing strategies. These combinations are then integrated using a weighted averaging strategy, thereby facilitating adaptive information sharing among different tasks. Specifically, assuming that the upprojection matrix is denoted as $\\mathbf { B } ^ { i } \\in \\mathbf { \\overline { { \\mathbb { R } } } } ^ { d \\times r }$ and the learnable averaging weight for task t is represented by wt ‚àà Rn√ó1, where $n$ is the number of up-projection low-rank matrices, the output of MTL-LoRA for task $t$ is formulated as:\n\n$$\n\\begin{array} { l } { { \\displaystyle h _ { t } = ( \\mathbf { W } + \\Delta \\mathbf { W } _ { t } ) x _ { t } } \\ ~ } \\\\ { { \\displaystyle ~ = \\mathbf { W } x _ { t } + \\sum _ { i = 1 } ^ { n } \\frac { \\exp ( w _ { t } ^ { i } / \\tau ) \\mathbf { B } ^ { i } } { \\sum _ { j = 1 } ^ { n } \\exp ( w _ { t } ^ { j } / \\tau ) } \\Lambda _ { t } \\mathbf { A } x _ { t } } } \\end{array}\n$$\n\nwhere $\\tau$ is a hyperparameter to control the sharpness of the weight distribution, and the superscript represents the indices of the corresponding up-projection matrix and averaging weight. We initialize $\\Lambda _ { i }$ as a diagonal matrix with each diagonal element being 1, thereby ensuring that $\\Delta \\mathbf { W } = 0$ at the start of training. Building upon these advancements, MTL-LoRA maintains the benefits of parameter efficiency while substantially boosting the MTL capabilities of LoRA.\n\nTable 1: Performance of different adaption methods on the GLUE benchmark, with results reported for the validation set. FT full parameters fine-tuning. ST: single task fine-tuning. MT: multi-task fine-tuning.   \n\n<html><body><table><tr><td>Model&Method</td><td># Trainable (%)</td><td>CoLA</td><td>MNLI</td><td>MRPC</td><td>QNLI</td><td>QQP</td><td>RTE</td><td>SST2</td><td>STSB</td><td rowspan=\"2\">Avg.</td></tr><tr><td></td><td></td><td>Mcc.</td><td>Acc</td><td>Acc</td><td>Acc</td><td>Acc</td><td>Acc</td><td>Acc</td><td>Pea.</td></tr><tr><td>MPT-FT</td><td>100√ó8</td><td>0.6712</td><td>0.9105</td><td>0.9044</td><td>0.9551</td><td>0.9213</td><td>0.9036</td><td>0.9701</td><td>0.9148</td><td>0.8939</td></tr><tr><td>MPT-LoRA-ST</td><td>0.12 √ó8</td><td>0.6161</td><td>0.9097</td><td>0.8578</td><td>0.9588</td><td>0.9033</td><td>0.8881</td><td>0.9636</td><td>0.8858</td><td>0.8729</td></tr><tr><td>MPT-LoRA-MT</td><td>0.24</td><td>0.6528</td><td>0.9102</td><td>0.8775</td><td>0.9590</td><td>0.9094</td><td>0.9206</td><td>0.9679</td><td>0.9173</td><td>0.8893</td></tr><tr><td>MPT-MultiLoRA</td><td>0.38</td><td>0.6441</td><td>0.9092</td><td>0.8701</td><td>0.9564</td><td>0.9085</td><td>0.9061</td><td>0.9679</td><td>0.9148</td><td>0.8846</td></tr><tr><td>MPT-MoELoRA</td><td>0.24</td><td>0.6421</td><td>0.9096</td><td>0.8695</td><td>0.9570</td><td>0.9119</td><td>0.9159</td><td>0.9639</td><td>0.8872</td><td>0.8820</td></tr><tr><td>MPT-MTL-LoRA</td><td>0.24</td><td>0.6754</td><td>0.9104</td><td>0.8995</td><td>0.9584</td><td>0.9122</td><td>0.9170</td><td>0.9644</td><td>0.9230</td><td>0.8950</td></tr><tr><td>LLaMA2-FT</td><td>100√ó8</td><td>0.7009</td><td>0.9144</td><td>0.8480</td><td>0.9690</td><td>0.9273</td><td>0.8773</td><td>0.9701</td><td>0.8965</td><td>0.8879</td></tr><tr><td>LLaMA2-LoRA-ST</td><td>0.12 √ó 8</td><td>0.6266</td><td>0.9099</td><td>0.8484</td><td>0.9518</td><td>0.9049</td><td>0.8989</td><td>0.9667</td><td>0.8915</td><td>0.8748</td></tr><tr><td>LLaMA2-LoRA-MT</td><td>0.24</td><td>0.6591</td><td>0.9139</td><td>0.8603</td><td>0.9600</td><td>0.9088</td><td>0.9170</td><td>0.9713</td><td>0.9188</td><td>0.8887</td></tr><tr><td>LLaMA2-MultiLoRA</td><td>0.38</td><td>0.6134</td><td>0.9100</td><td>0.8628</td><td>0.9552</td><td>0.9003</td><td>0.9097</td><td>0.9633</td><td>0.9184</td><td>0.8791</td></tr><tr><td>LLaMA2-MoELoRA</td><td>0.24</td><td>0.6366</td><td>0.9118</td><td>0.8554</td><td>0.9568</td><td>0.9062</td><td>0.9206</td><td>0.9656</td><td>0.9218</td><td>0.8844</td></tr><tr><td>LLaMA2-MTL-LoRA</td><td>0.24</td><td>0.6797</td><td>0.9143</td><td>0.9020</td><td>0.9628</td><td>0.9142</td><td>0.9242</td><td>0.9713</td><td>0.9279</td><td>0.8996</td></tr></table></body></html>\n\n<html><body><table><tr><td>Method</td><td># Trainable(%)</td><td>BoolQ</td><td>PIQA</td><td>SIQA</td><td>Winogrande</td><td>OBQA</td><td>Hellaswag</td><td>ARC-E</td><td>ARC-C</td><td>Avg</td></tr><tr><td>LoRAt</td><td>0.83</td><td>69.8</td><td>79.9</td><td>79.5</td><td>82.6</td><td>81.0</td><td>83.6</td><td>79.8</td><td>64.7</td><td>77.6</td></tr><tr><td>DoRA‚Ä†</td><td>0.43</td><td>72.0</td><td>83.1</td><td>79.9</td><td>83.0</td><td>81.2</td><td>89.1</td><td>84.5</td><td>71.0</td><td>80.5</td></tr><tr><td>MultiLoRA</td><td>0.40</td><td>66.5</td><td>65.8</td><td>62.8</td><td>79.3</td><td>75.4</td><td>79.2</td><td>76.7</td><td>59.6</td><td>70.7</td></tr><tr><td>MoELoRA</td><td>0.25</td><td>68.0</td><td>83.5</td><td>70.4</td><td>82.5</td><td>83.2</td><td>90.6</td><td>86.8</td><td>61.5</td><td>78.3</td></tr><tr><td>MTL-LoRA</td><td>0.25</td><td>71.0</td><td>84.4</td><td>80.8</td><td>84.9</td><td>82.6</td><td>93.1</td><td>87.0</td><td>73.4</td><td>82.1</td></tr></table></body></html>\n\nTable 2: Commonsense reasoning results. We follow the setting from (Liu et al. 2024; Hu et al. 2023) for jointly training all tasks.‚Ä† means the results from original DoRA paper (Liu et al. 2024).\n\n# Experiments\n\nWe conduct a series of experiments to demonstrate the effectiveness of MTL-LoRA on various tasks, including natural language understanding (NLU), commonsense reasoning, and image-text understanding. Additionally, we perform ablation studies to illustrate the effectiveness of each component of MTL-LoRA. Finally, we conduct a sensitivity analysis to examine its stability across different hyperparameter configurations.\n\n# Evaluation on Public Benchmark\n\nNatural Language Understanding We compare MTLLoRA against several baseline methods, including fullparameter tuning, single-task fine-tuning with LoRA, multitask fine-tuning with LoRA, MultiLoRA, and MoELoRA, on both MPT-7B (Team 2023) and LLaMA2-7B (Touvron et al. 2023) models. We use the widely recognized GLUE (Wang et al. 2018) benchmark for evaluation. The GLUE benchmark comprises nine NLU tasks, covering a diverse range of linguistic challenges such as sentiment analysis, textual entailment, and sentence similarity.\n\nTo ensure that the LLM with decoder-only architecture generates stable classification results, we adopt the approach of MT-DNN (Liu et al. 2019) by assigning each task its respective classification head. To harness the generative capabilities of LLMs, we reformat each task using a specific template and initialize the weights of the classification head with the corresponding word embeddings of the target answer from the original language model. For each PEFT method, we train only the adapter parameters and the taskspecific classification head. Details on hyperparameters and templates can be found in Section A.1 and Section C of the supplementary material.\n\nThe results presented in Table 1 demonstrate that MTLLoRA achieves superior performance, surpassing other baseline methods across both LLMs. Notably, MTL-LoRA not only outperforms the strong MultiLoRA baseline with only $64 \\%$ trainable parameters but also exceeds the performance of FT while requiring significantly fewer trainable parameters (merely $0 . 0 3 \\%$ per task compared to FT). Furthermore, as shown in Table 1, MTL with LoRA (i.e., LoRA-MT) outperforms single task fine-tuning with LoRA (i.e., LoRA-ST) across all eight tasks and largely reduces the number of adapters that need to be maintained.\n\nWhile both MoELoRA and MultiLoRA have boosted LoRA‚Äôs multi-tasking performance, optimizing these models for multi-task scenarios remains challenging, leading to suboptimal performance. In contrast, MTL-LoRA effectively exploits both task-specific and task-agnostic informa\n\nTable 3: Results of different methods on the test set of Ads dataset. ‚àó denotes that the significance test is passed at a $90 \\%$ confidence level. ST: single task fine-tuning. MT: multi-task fine-tuning.   \n\n<html><body><table><tr><td rowspan=\"3\">Model& Method</td><td rowspan=\"3\"># Trainable (%)</td><td colspan=\"10\">3</td><td rowspan=\"2\"></td><td rowspan=\"3\"></td><td rowspan=\"3\"></td><td rowspan=\"3\">13</td><td rowspan=\"3\">Avg.</td></tr><tr><td rowspan=\"2\">0</td><td rowspan=\"2\">1</td><td colspan=\"3\">2</td><td colspan=\"3\">Task Index 6</td><td colspan=\"3\">9</td></tr><tr><td></td><td></td><td></td><td>AUC-ROC</td><td>7</td><td>8</td><td></td><td>10</td><td>11</td><td>12</td></tr><tr><td>MPT-LoRA-ST</td><td>0.12 √ó 14</td><td>0.8846</td><td>0.8361</td><td></td><td></td><td></td><td>0.8833</td><td>0.8941</td><td>0.8589</td><td>0.8677</td><td>0.8676</td><td>0.8490</td><td>0.8699</td><td>0.8519</td><td>0.8689</td><td>0.8712</td></tr><tr><td>MPT-LoRA-MT</td><td>0.24</td><td></td><td>0.8286</td><td>0.8979 0.8863</td><td>0.8868 0.8722</td><td>0.8806 0.8765</td><td>0.8827</td><td>0.8858</td><td>0.8576</td><td>0.8584</td><td>0.8590</td><td>0.8399</td><td>0.8680</td><td>0.8504</td><td>0.8639</td><td>0.8650</td></tr><tr><td>MPT-MultiLoRA</td><td>0.38</td><td>0.8812 0.8874</td><td>0.8382</td><td>0.8947</td><td>0.8871</td><td>0.8835</td><td>0.8890</td><td>0.8944</td><td>0.8674</td><td>0.8680</td><td>0.8712</td><td>0.8538</td><td>0.8786</td><td>0.8598</td><td>0.8703</td><td>0.8745</td></tr><tr><td>MPT-MoELoRA</td><td>0.24</td><td>0.8860</td><td>0.8382</td><td>0.8898</td><td>0.8837</td><td>0.8829</td><td>0.8889</td><td>0.8950</td><td>0.8641</td><td>0.8682</td><td>0.8720</td><td>0.8540</td><td>0.8802</td><td>0.8576</td><td>0.8769</td><td>0.8741</td></tr><tr><td>MPT-7B-MTL-LoRA</td><td>0.24</td><td>0.8876</td><td>0.8384</td><td>0.9005*</td><td>0.8918*</td><td>0.8840*</td><td>0.8897*</td><td>0.8956*</td><td>0.8678</td><td>0.8692*</td><td>0.8728*</td><td>0.8561*</td><td>0.8810*</td><td>0.8607*</td><td>0.8714</td><td>0.8762</td></tr><tr><td>LLaMA2-LoRA-ST</td><td>0.12 √ó 14</td><td>0.8838</td><td>0.8349</td><td>0.8992</td><td>0.8886</td><td>0.8792</td><td>0.884</td><td>0.8948</td><td>0.8575</td><td>0.8665</td><td>0.8685</td><td>0.8495</td><td>0.8706</td><td>0.8523</td><td>0.8703</td><td>0.8714</td></tr><tr><td>LLaMA2-LoRA-MT</td><td>0.24</td><td></td><td>0.8329</td><td>0.8932</td><td>0.8867</td><td>0.8806</td><td>0.8859</td><td>0.8935</td><td>0.8646</td><td>0.8652</td><td>0.8671</td><td></td><td></td><td>0.8571</td><td>0.8689</td><td>0.8722</td></tr><tr><td>LLaMA2-MultiLoRA</td><td>0.38</td><td>0.8842 0.8879</td><td>0.8369</td><td>0.8958</td><td>0.8922</td><td>0.8853</td><td>0.8901</td><td>0.8950</td><td>0.8687</td><td>0.8693</td><td>0.8743</td><td>0.8545 0.8540</td><td>0.8763 0.8808</td><td>0.8599</td><td>0.8726</td><td>0.8759</td></tr><tr><td>LLaMA2-MoELoRA</td><td>0.24</td><td>0.8850</td><td>0.8370</td><td>0.8962</td><td>0.8921</td><td>0.8850</td><td>0.8876</td><td>0.8961</td><td>0.8685</td><td>0.8686</td><td>0.8737</td><td>0.8550</td><td>0.8800</td><td>0.8594</td><td>0.8722</td><td>0.8755</td></tr><tr><td>LLaMA2-MTL-LoRA</td><td>0.24</td><td>0.8883</td><td>0.8374</td><td>0.9016*</td><td>0.8929*</td><td>0.8856</td><td>0.8899</td><td>0.8968*</td><td>0.8688</td><td>0.8706*</td><td>0.8754*</td><td>0.8582*</td><td>0.8810</td><td>0.8607*</td><td>0.8723</td><td>0.8771</td></tr></table></body></html>\n\n<html><body><table><tr><td>Method</td><td># Trainable (%)</td><td>VQAV2</td><td>GQA</td><td>NVLR¬≤</td><td>CoCo Cap</td><td>Avg.</td></tr><tr><td>FT‚Ä†</td><td>100</td><td>66.9</td><td>56.7</td><td>73.7</td><td>112.0</td><td>77.3</td></tr><tr><td>LoRAt</td><td>5.93</td><td>65.2</td><td>53.6</td><td>71.9</td><td>115.3</td><td>76.5</td></tr><tr><td>DoRAt</td><td>5.96</td><td>65.8</td><td>54.7</td><td>73.1</td><td>115.9</td><td>77.4</td></tr><tr><td>MTL-LoRA</td><td>5.19</td><td>68.6</td><td>54.9</td><td>72.6</td><td>114.6</td><td>77.7</td></tr></table></body></html>\n\nTable 4: The multi-task evaluation results on VQA, GQA, $\\mathrm { \\Delta N V L R ^ { 2 } }$ , and CoCo Caption using the VL-BART backbone. ‚Ä† indicates results taken from the original DoRA paper (Liu et al. 2024).\n\ntion, thereby outperforming LoRA-MT in nearly all tasks.\n\nCommonsense Reasoning In these experiments, we perform a comparison of MTL-LoRA against LoRA and various LoRA variants on LLaMA2-7B for commonsense reasoning tasks. We train each model on eight sub-tasks jointly and evaluate performance on the individual test dataset for each task. Following the same train-test split protocol and instruction prompts as in (Hu et al. 2023; Liu et al. 2024), we report the test set accuracy for each method. Detailed hyperparameter settings can be found in Section A.2 of the supplementary material. Where possible, we report model results as presented in the original papers.\n\nThe results in Table 2 show that both MoELoRA and DoRA outperform LoRA and MultiLoRA. We hypothesize that this is because commonsense reasoning tasks require fine-grained task routing or gradient decomposition, rather than merely ensembling different LoRAs. Despite this, MTL-LoRA consistently outperforms all baseline methods. Notably, with only one-third of the LoRA parameters, MTLLoRA surpasses LoRA by approximately $4 \\%$ . Moreover, with only half the parameters, MTL-LoRA outperforms the strong baseline DoRA by a large margin of $2 \\%$ . These results further highlight MTL-LoRA‚Äôs efficiency and effectiveness in optimizing model performance while minimizing training overhead in multi-task adaptation.\n\nImage-Text Understanding To evaluate the performance of MTL-LoRA in a multimodal multitask fine-tuning context, we compare it with LoRA, DoRA, and FT using VLBART in four distinct image text tasks. The results for FT, LoRA, and DoRA are taken from the original DoRA paper. For MTL-LoRA, we follow the same settings as DoRA, applying the adapter to the $\\mathbf { Q }$ and $\\mathbf { V }$ linear layers of the language model. We also unfreeze the bias and layer normalization parameters, training for 20 epochs with the AdamW optimizer and a learning rate of $1 \\times 1 0 ^ { - 3 }$ , in line with DoRA‚Äôs configuration. The rank, alpha, number of up-projection matrices, and temperature for MTL-LoRA are set to 64, 16, 2, and 0.8, respectively.\n\nThe results are presented in Table 4. Both MTL-LoRA and DoRA outperform LoRA by $1 \\%$ with the same or even fewer parameters. Furthermore, MTL-LoRA and DoRA surpass FT while unfreezing only $5 \\%$ of the model‚Äôs parameters. In the multimodal multitasking scenario, MTL-LoRA also outperforms DoRA with approximately $1 \\%$ fewer learnable parameters, demonstrating its learning effectiveness in this setting.\n\n# Evaluation on In-house Dataset\n\nTo further validate the performance of MTL-LoRA in largescale, complex multi-task scenarios, we compared different multi-task low-rank adaptation strategies on an in-house text Ads relevance dataset (referred to as the Ads dataset).\n\nThe text Ads relevance task involves determining whether a query is semantically relevant to a given Ad. This dataset encompasses 14 tasks, covering various production scenarios. The query and Ad pairs are collected from a commercial sponsored search engine, with relevance labels provided by experts. For evaluation, we treat the task as a binary classification problem and report AUC-ROC metrics, following production practices. The dataset consists of $1 3 \\ \\mathrm { m i l } .$ - lion examples in the training set and 2 million examples in the test set, with data collected in multiple languages from global markets. This benchmark is particularly challenging because, while all tasks are from the ad domain, they span different product scenarios. Effectively modeling the correlation between tasks is crucial for achieving optimal performance.\n\nWe follow the same experimental design used for finetuning in the GLUE benchmark. For further details on the Ads dataset and experimental settings, please refer to Section B and Section A.1 of the supplementary material.\n\nThe results are presented in Table 3. In the large-scale Ads dataset, LoRA-MT consistently outperforms LoRA-ST, further underscoring the effectiveness of multi-task fine-tuning. In this context, MTL-LoRA either surpasses or matches other methods on all 14 tasks. Notably, while other multitask adaptations exhibit a seesaw effect when compared to the LoRA-ST approach‚Äîimproving performance on some tasks but underperforming on others‚ÄîMTL-LoRA consistently outperforms LoRA-ST across all tasks. This indicates that MTL-LoRA effectively mitigates interference between tasks during adaptation. Furthermore, compared to the second-ranking method in each task, MTL-LoRA achieved statistical significance with confidence $90 \\%$ in 10 of 14 tasks on the MPT model and 7 of 14 tasks on the LLaMA2 model. This significant performance advantage of MTL-LoRA in the Ads dataset validates its enhanced capability for MTL.\n\nTable 5: Results of ablation studies on MTL-LoRA across from the GLUE benchmark. MT: multi-task fine-tuning   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td>CMLA</td><td></td><td></td><td></td><td></td><td></td><td></td><td rowspan=\"2\">STsB</td><td rowspan=\"2\">Avg.</td></tr><tr><td></td><td>MNLI</td><td>MRPC</td><td>ONLI</td><td>QQP</td><td>RTE.</td><td>SST2</td></tr><tr><td>LoRA-MT</td><td>0.6591</td><td>0.9193</td><td>0.8603</td><td>0.9600</td><td>0.9088</td><td>0.9170</td><td>0.9713</td><td>0.9188</td><td>0.8887</td></tr><tr><td>MTL-LoRA</td><td>0.6797</td><td>0.9143</td><td>0.9020</td><td>0.9628</td><td>0.9142</td><td>0.9242</td><td>0.9713</td><td>0.9279</td><td>0.8996</td></tr><tr><td>w/o At</td><td>0.6567</td><td>0.9164</td><td>0.8971</td><td>0.9627</td><td>0.9140</td><td>0.9242</td><td>0.9679</td><td>0.9261</td><td>0.8956 (-0.0039)</td></tr><tr><td>W/o T</td><td>0.6592</td><td>0.9144</td><td>0.8676</td><td>0.9606</td><td>0.9114</td><td>0.9097</td><td>0.9713</td><td>0.9214</td><td>0.8895 (-0.0101)</td></tr><tr><td>n=1</td><td>0.6360</td><td>0.9150</td><td>0.8725</td><td>0.9625</td><td>0.9105</td><td>0.9170</td><td>0.9702</td><td>0.9248</td><td>0.8886 (-0.0110)</td></tr></table></body></html>\n\n<html><body><table><tr><td rowspan=\"3\">Method&Model</td><td colspan=\"10\">Task Index</td><td colspan=\"5\"></td></tr><tr><td>0 1</td><td></td><td>2</td><td></td><td>3</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td><td>Avg.</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>F1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LLaMA2-LoRA-MT</td><td>0.60</td><td>0.17</td><td>0.58</td><td>0.37</td><td>0.38</td><td>0.52</td><td>0.59</td><td>0.18</td><td>0.61</td><td>0.71</td><td>0.44</td><td>0.13</td><td>0.18</td><td>0.12</td><td>0.39</td></tr><tr><td>LLaMA2-MTL-LoRA</td><td>0.86</td><td>0.84</td><td>0.81</td><td>0.99</td><td>0.99</td><td>0.97</td><td>0.94</td><td>0.66</td><td>0.99</td><td>0.93</td><td>0.43</td><td>0.73</td><td>0.34</td><td>0.55</td><td>0.79</td></tr><tr><td>MPT-LoRA-MT</td><td>0.59</td><td>0.27</td><td>0.60</td><td>0.34</td><td>0.36</td><td>0.49</td><td>0.52</td><td>0.30</td><td>0.51</td><td>0.68</td><td>0.26</td><td>0.23</td><td>0.14</td><td>0.14</td><td>0.39</td></tr><tr><td>MPT-MTL-LoRA</td><td>0.83</td><td>0.92</td><td>0.81</td><td>0.98</td><td>0.97</td><td>0.86</td><td>0.65</td><td>0.75</td><td>0.87</td><td>0.83</td><td>0.39</td><td>0.75</td><td>0.72</td><td>0.45</td><td>0.77</td></tr></table></body></html>\n\nTable 6: Task classification results on Ads dataset using SVM classifier with features extracted by various methods. All result are averaged over five run with different train-test split seed. MT: multi-task fine-tuning.\n\n# Ablation Study\n\nWe conduct ablation studies on MTL-LoRA to assess the impact of three key components: (1) the task-specific learnable transformation matrix $\\Lambda _ { t }$ , (2) the temperature coefficient $\\tau$ in Eq. 4, and (3) multiple low-rank up-projection matrices $n$ .\n\nFor each ablation study, we use LLaMA2-7B as the backbone model and train it on GLUE dataset. In each ablation experiment, we systematically remove or disable one of the key components while keeping all other settings unchanged and report the GLUE metrics. The performance of each modified setting is compared against the full MTLLoRA configuration. Additionally, we add LoRA‚Äôs results on multi-task fine-tuning as a reference. Detailed hyperparameter settings are provided in Section A.3 of the supplementary material.\n\nThe results, presented in Table 5, clearly illustrate the importance of each component of MTL-LoRA. Omitting any one of these components results in a decline in performance. Notably, the inclusion of multiple low-rank up-projection matrices has the most significant impact. When $n = 1$ , the performance of MTL-LoRA falls below that of the LoRAbased multi-task fine-tuning on the GLUE benchmark. This suggests that effectively extracting task-specific information using $\\Lambda _ { t }$ relies on methods that aggregate diverse combinations of information across tasks.\n\n# Sensitivity Analysis\n\nIn this section, we analyze the robustness of MTL-LoRA under various parameter settings. Our investigation focus on how different values of $n , \\tau .$ , and $r$ impact the performance of MTL-LoRA. We use LLaMA2-7B as the backbone model and conduct comparison on the GLUE benchmark. The results, presented in Figure 3, demonstrate that multiple upprojection metrics are crucial for MTL-LoRA, with $n = 3$ yielding superior results in most scenarios. The value of temperature coefficient also affects the model performance, supporting that different up-projection matrices capture distinct aggregated information. Additionally, the analysis of different values for the rank $r$ reveals that MTL-LoRA maintains robustness at higher rank compared to vanilla LoRA.\n\n# Task Differentiation\n\nThe primary goal of MTL-LoRA is to enhance the effectiveness of LoRA in multi-task scenarios by preventing crosstask information interference while facilitating task information sharing. Consequently, we assert that the representation outputs of MTL-LoRA should be task-relevant. To validate this, we use the outputs from MTL-LoRA as features for SVM classification, with labels corresponding to their respective tasks. We use the Ads dataset for comparison because it encompasses a larger number of tasks, all within the same Ad domain, thus providing a challenging context for differentiating tasks. Specifically, we randomly sample 1,000 examples for each task from the Ads dataset and use the outputs of different LoRA adapters from the final block of the underlying LLMs as input features. An SVM classifier is trained on $40 \\%$ of the samples for each method, with the remaining $60 \\%$ reserved for evaluation. For detailed information on the experimental settings and hyperparameters of the SVM, please refer to Section A.4 of the supplementary material.\n\n![](images/2b070c9ce0fd1ddc72815e522308e61e0941a6e9947dbd3505e23c55b8b92cd1.jpg)  \nFigure 3: The performance of MTL-LoRA on GLUE benchmark with different hyperparameter configurations.\n\n![](images/0e0f4c8a0f0bbaf398f61be8d640db331b591696764a33987d9e246d9686a426.jpg)  \nFigure 4: Inference latency of different low-rank adapters on LLaMA2-7B with varying batch sizes. The results are averaged over 100 runs on a single A100-80G GPU. DoRA and MultiLoRA exhibit similar performance to LoRA as they can also be merged into pre-trained weights.\n\nThe results, as shown in Table 6, indicate that MTL-LoRA significantly outperforms LoRA in multi-task fine-tuning, achieving a substantial improvement margin with both MPT and LLaMA2. The t-SNE visualization in Figure 1 further supports this observation, revealing that, in the multi-task adaptation scenario, LoRA tends to blend features from different tasks in the low-rank space, whereas MTL-LoRA effectively separates these tasks. When projected back into the full-rank space, MTL-LoRA forms distinct ‚Äòtask groups‚Äô, indicating that information is shared within each group while remaining distinct between groups. These findings confirm MTL-LoRA‚Äôs enhanced ability to distinguish between tasks and effectively reduce task interference. Furthermore, the relatively lower performance of LoRA in this setting highlights its limitations in extracting task-specific information.\n\n# Inference Overhead\n\nAlthough MTL-LoRA demonstrates exceptional performance in multi-task learning, it relies on task-specific information to determine the appropriate transformation matrix for routing ( $\\boldsymbol { \\Lambda } _ { t }$ in Eq.4). This design choice prevents the merging of all parameters into the original weights as LoRA does, which introduces additional inference latency. However, thanks to MTL-LoRA‚Äôs task-level routing design, all operations are executed using matrix multiplication, avoiding the expert-level looping required by MoELoRA. This allows MTL-LoRA to fully utilize computational resources without incurring significant inference latency.\n\nTo validate this, we compared the inference latency of LoRA, MoELoRA, and MTL-LoRA across different batch sizes on the commonsense reasoning task, as depicted in Figure4. The results clearly demonstrate that MTL-LoRA significantly outperforms MoELoRA in terms of inference speed. Compared to merged LoRA, MTL-LoRA introduces only minimal additional latency during inference, with this difference diminishes further under computationally intensive conditions with larger batch sizes. We believe that in latency-sensitive applications, the gap in inference speed between MTL-LoRA and merged LoRA can be further reduced through system-level optimizations, which we plan to explore in future work.\n\n# Conclusion\n\nWe propose MTL-LoRA, a novel, advanced parameterefficient fine-tuning method for multi-task low-rank adaptation. MTL-LoRA enhances the multi-task learning capability of LoRA by incorporating task-specific transformations in low-rank space along with a strategy for adaptively exploring multiple information sharing methods. This approach facilitates the learning of both task-specific and shared information. Comprehensive experiments on multiple public academic benchmarks and a large-scale text Ads relevance dataset demonstrate that MTL-LoRA outperforms LoRA and its variants, including MultiLoRA, MoELoRA, and DoRA, validating its effectiveness in multi-task learning. Furthermore, extensive analysis of intermediate lowrank features and visualizations support our design motivations. For future studies , we will focus on optimizing the design of MTL-LoRA to reduce the additional inference time while maintaining its effectiveness.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáËß£ÂÜ≥‰∫ÜÂú®Â§ö‰ªªÂä°Â≠¶‰π†ÔºàMTLÔºâÂú∫ÊôØ‰∏≠Ôºå‰ΩéÁß©ÈÄÇÂ∫îÔºàLoRAÔºâÊñπÊ≥ïÂõ†Â∞Ü‰∏çÂêå‰ªªÂä°ÁöÑÈ´òÁª¥Á®ÄÁñèÁâπÂæÅÊäïÂΩ±Âà∞Áõ∏ÂêåÁöÑ‰ΩéÁª¥ÂØÜÈõÜÁ©∫Èó¥‰∏≠ËÄåÂØºËá¥ÁöÑ‰ªªÂä°Âπ≤Êâ∞ÂíåÊÄßËÉΩ‰∏ãÈôçÈóÆÈ¢ò„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éÔºåÁé∞ÊúâÁöÑLoRAÂèäÂÖ∂Âèò‰ΩìÂú®Â§ö‰ªªÂä°Â≠¶‰π†‰∏≠Êó†Ê≥ïÊúâÊïàÂå∫ÂàÜ‰ªªÂä°ÁâπÂÆö‰ø°ÊÅØÔºåÂØºËá¥ÊÄßËÉΩ‰∏ç‰Ω≥ÔºåËÄåMTL-LoRAÈÄöËøáÊîπËøõLoRAÂú®Â§ö‰ªªÂä°Â≠¶‰π†‰∏≠ÁöÑË°®Áé∞ÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÂú®Â§ö‰∏™‰ªªÂä°‰∏äÁöÑÈÄÇÂ∫îËÉΩÂäõ„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   MTL-LoRAÈÄöËøáÂºïÂÖ•‰ªªÂä°ÁâπÂÆöÁöÑ‰ΩéÁß©ÂèØÂ≠¶‰π†Áü©ÈòµÂíåÂ§ö‰∏™‰∏äÊäïÂΩ±Áü©ÈòµÔºåÂ¢ûÂº∫‰∫ÜLoRAÂú®Â§ö‰ªªÂä°Â≠¶‰π†‰∏≠ÁöÑËÉΩÂäõÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÂèÇÊï∞ÊïàÁéá„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ1Ôºö** ÊèêÂá∫‰∫ÜMTL-LoRAÔºåÈÄöËøá‰ªªÂä°ÁâπÂÆöÁöÑ‰ΩéÁß©ÂèòÊç¢Áü©ÈòµÂíåÂä®ÊÄÅ‰ø°ÊÅØÂÖ±‰∫´Á≠ñÁï•ÔºåÊòæËëóÊèêÂçá‰∫ÜLoRAÂú®Â§ö‰ªªÂä°Â≠¶‰π†‰∏≠ÁöÑÊÄßËÉΩ„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ2Ôºö** Âú®Â§ö‰∏™ÂÖ¨ÂºÄÂ≠¶ÊúØÂü∫ÂáÜÂíåÂÆûÈôÖÂ∑•‰∏öÊï∞ÊçÆÈõÜ‰∏äÈ™åËØÅ‰∫ÜMTL-LoRAÁöÑ‰ºòË∂äÊÄßÔºå‰æãÂ¶ÇÂú®GLUEÂü∫ÂáÜ‰∏äÔºåMTL-LoRAÁöÑÂπ≥ÂùáÊÄßËÉΩÊèêÂçá‰∫ÜÁ∫¶1.1‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ3Ôºö** ÈÄöËøáÂπøÊ≥õÁöÑÊ∂àËûçÂÆûÈ™åÂíåÂàÜÊûêÔºåÈ™åËØÅ‰∫ÜMTL-LoRAÂêÑÁªÑÊàêÈÉ®ÂàÜÁöÑÊúâÊïàÊÄßÔºåÂπ∂Â±ïÁ§∫‰∫ÜÂÖ∂Âú®‰ªªÂä°Âå∫ÂàÜÂíå‰ø°ÊÅØÂÖ±‰∫´ÊñπÈù¢ÁöÑ‰ºòÂäø„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   MTL-LoRAÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÈÄöËøá‰ªªÂä°ÁâπÂÆöÁöÑ‰ΩéÁß©ÂèòÊç¢Áü©ÈòµÂíåÂ§ö‰∏™‰∏äÊäïÂΩ±Áü©ÈòµÔºåÂú®‰ΩéÁª¥Á©∫Èó¥‰∏≠Âå∫ÂàÜ‰ªªÂä°ÁâπÂÆö‰ø°ÊÅØÂπ∂Âä®ÊÄÅÂÖ±‰∫´Ë∑®‰ªªÂä°Áü•ËØÜÔºå‰ªéËÄåÂáèÂ∞ë‰ªªÂä°Âπ≤Êâ∞Âπ∂ÊèêÂçáÂ§ö‰ªªÂä°Â≠¶‰π†ÊÄßËÉΩ„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** Áé∞ÊúâÁöÑLoRAÂèäÂÖ∂Âèò‰ΩìÂú®Â§ö‰ªªÂä°Â≠¶‰π†‰∏≠Êó†Ê≥ïÊúâÊïàÂå∫ÂàÜ‰ªªÂä°ÁâπÂÆö‰ø°ÊÅØÔºåÂØºËá¥‰ªªÂä°Âπ≤Êâ∞ÂíåÊÄßËÉΩ‰∏ãÈôç„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** MTL-LoRAÂºïÂÖ•‰∫Ü‰ªªÂä°ÁâπÂÆöÁöÑ‰ΩéÁß©ÂèòÊç¢Áü©ÈòµŒõ_tÂíåÂ§ö‰∏™‰∏äÊäïÂΩ±Áü©ÈòµB^iÔºåÈÄöËøáÂä®ÊÄÅÂä†ÊùÉÂπ≥ÂùáÁ≠ñÁï•ÂÆûÁé∞Ëá™ÈÄÇÂ∫î‰ø°ÊÅØÂÖ±‰∫´Ôºå‰ªéËÄåÂú®Â§ö‰ªªÂä°Â≠¶‰π†‰∏≠ÊòæËëóÊèêÂçá‰∫ÜÊÄßËÉΩ„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> *   1. **ËæìÂÖ•ÊäïÂΩ±Ôºö** Â∞ÜËæìÂÖ•x_tÊäïÂΩ±Âà∞‰ΩéÁª¥Á©∫Èó¥ÔºåÁ±ª‰ºº‰∫éÊ†áÂáÜLoRAÁöÑAÁü©Èòµ„ÄÇ\\n> *   2. **‰ªªÂä°ÁâπÂÆöÂèòÊç¢Ôºö** ÈÄöËøá‰ªªÂä°ÁâπÂÆöÁöÑ‰ΩéÁß©Áü©ÈòµŒõ_tÂØπÊäïÂΩ±ÂêéÁöÑÊ†∑Êú¨ËøõË°åÂèòÊç¢Ôºå‰ª•‰øùÁïô‰ªªÂä°ÁâπÂÆö‰ø°ÊÅØ„ÄÇ\\n> *   3. **Âä®ÊÄÅ‰ø°ÊÅØÂÖ±‰∫´Ôºö** ‰ΩøÁî®Â§ö‰∏™‰∏äÊäïÂΩ±Áü©ÈòµB^iÂíåÂèØÂ≠¶‰π†ÁöÑÂä†ÊùÉÂπ≥ÂùáÁ≠ñÁï•ÔºåÂä®ÊÄÅÁªÑÂêà‰∏çÂêå‰ªªÂä°ÁöÑ‰ø°ÊÅØ„ÄÇ\\n> *   4. **ËæìÂá∫ËÆ°ÁÆóÔºö** ÊúÄÁªàËæìÂá∫h_tÈÄöËøáÂä†ÊùÉÂπ≥ÂùáÁöÑ‰∏äÊäïÂΩ±Áü©ÈòµÂíå‰ªªÂä°ÁâπÂÆöÂèòÊç¢ÂêéÁöÑÁâπÂæÅËÆ°ÁÆóÂæóÂà∞„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   ËÆ∫Êñá‰∏≠Áî®‰∫éÂØπÊØîÁöÑÂü∫Á∫øÊ®°ÂûãÂåÖÊã¨ÔºöÂÖ®ÂèÇÊï∞ÂæÆË∞ÉÔºàFTÔºâ„ÄÅÂçï‰ªªÂä°LoRAÔºàLoRA-STÔºâ„ÄÅÂ§ö‰ªªÂä°LoRAÔºàLoRA-MTÔºâ„ÄÅMultiLoRA„ÄÅMoELoRAÂíåDoRA„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®GLUEÂü∫ÂáÜ‰∏äÔºö** MTL-LoRAÂú®LLaMA2-7BÊ®°Âûã‰∏äÁöÑÂπ≥ÂùáÊÄßËÉΩËææÂà∞‰∫Ü89.96%ÔºåÊòæËëó‰ºò‰∫éLoRA-MTÔºà88.87%ÔºâÂíåMultiLoRAÔºà87.91%Ôºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øLoRA-MTÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü1.09‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®Â∏∏ËØÜÊé®ÁêÜ‰ªªÂä°‰∏äÔºö** MTL-LoRAÁöÑÂπ≥ÂùáÂáÜÁ°ÆÁéá‰∏∫82.1%Ôºå‰ºò‰∫éLoRAÔºà77.6%ÔºâÂíåDoRAÔºà80.5%ÔºâÔºå‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øDoRAÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü1.6‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®ÂõæÂÉèÊñáÊú¨ÁêÜËß£‰ªªÂä°‰∏äÔºö** MTL-LoRAÁöÑÂπ≥ÂùáÊÄßËÉΩ‰∏∫77.7%Ôºå‰ºò‰∫éLoRAÔºà76.5%ÔºâÂíåDoRAÔºà77.4%ÔºâÔºå‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øDoRAÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü0.3‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®Â∑•‰∏öÂπøÂëäÊï∞ÊçÆÈõÜ‰∏äÔºö** MTL-LoRAÂú®14‰∏™‰ªªÂä°‰∏≠ÁöÑ10‰∏™‰ªªÂä°‰∏äÊòæËëó‰ºò‰∫éÂÖ∂‰ªñÊñπÊ≥ïÔºåÂπ≥ÂùáAUC-ROCËææÂà∞‰∫Ü87.71%Ôºå‰ºò‰∫éLoRA-MTÔºà86.50%ÔºâÂíåMultiLoRAÔºà87.45%Ôºâ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   Â§ö‰ªªÂä°Â≠¶‰π† (Multi-Task Learning, MTL)\\n*   ‰ΩéÁß©ÈÄÇÂ∫î (Low-Rank Adaptation, LoRA)\\n*   ÂèÇÊï∞È´òÊïàÂæÆË∞É (Parameter-Efficient Fine-Tuning, PEFT)\\n*   ‰ªªÂä°ÁâπÂÆö‰ø°ÊÅØ (Task-Specific Information, N/A)\\n*   Âä®ÊÄÅ‰ø°ÊÅØÂÖ±‰∫´ (Dynamic Information Sharing, N/A)\\n*   Ëá™ÁÑ∂ËØ≠Ë®ÄÁêÜËß£ (Natural Language Understanding, NLU)\\n*   Â∏∏ËØÜÊé®ÁêÜ (Commonsense Reasoning, N/A)\\n*   ÂõæÂÉèÊñáÊú¨ÁêÜËß£ (Image-Text Understanding, N/A)\"\n}\n```"
}