{
    "source": "Semantic Scholar",
    "arxiv_id": "2503.03135",
    "link": "https://arxiv.org/abs/2503.03135",
    "pdf_link": "https://arxiv.org/pdf/2503.03135.pdf",
    "title": "Bridging Molecular Graphs and Large Language Models",
    "authors": [
        "Runze Wang",
        "Mingqi Yang",
        "Yanming Shen"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2025-03-05",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Dalian University of Technology",
        "National University of Singapore"
    ],
    "paper_content": "# Bridging Molecular Graphs and Large Language Models\n\nRunze Wang1, Mingqi Yang2, Yanming Shen1\\*\n\n1School of Computer Science and Technology, Dalian University of Technology, China 2School of Computing, National University of Singapore, Singapore runze wang@mail.dlut.edu.cn, mqyang@nus.edu.sg, shen@dlut.edu.cn\n\n# Abstract\n\nWhile Large Language Models (LLMs) have shown exceptional generalization capabilities, their ability to process graph data, such as molecular structures, remains limited. To bridge this gap, this paper proposes Graph2Token, an efficient solution that aligns graph tokens to LLM tokens. The key idea is to represent a graph token with the LLM token vocabulary, without fine-tuning the LLM backbone. To achieve this goal, we first construct a molecule-text paired dataset from multisources, including CHEBI and HMDB, to train a graph structure encoder, which reduces the distance between graphs and texts representations in the feature space. Then, we propose a novel alignment strategy that associates a graph token with LLM tokens. To further unleash the potential of LLMs, we collect molecular IUPAC name identifiers, which are incorporated into the LLM prompts. By aligning molecular graphs as special tokens, we can activate LLMs‚Äô generalization ability to molecular few-shot learning. Extensive experiments on molecular classification and regression tasks demonstrate the effectiveness of our proposed Graph2Token.\n\nCode ‚Äî https://github.com/GraphMoLab/Graph2Token\n\n# Introduction\n\nRecent studies have shown promising results in applying large language models (LLMs) to graph machine learning, particularly demonstrating the potential in few-shot or zero-shot learning on knowledge graphs and text-attributed graphs (Fan et al. 2024; Tang et al. 2024; Chen et al. 2024a). However, their capability to handle graph-structured data such as molecules is still not well investigated. The intrinsic properties of molecules demand a deeper understanding of their structures beyond node attributes, posing a challenge that requires a distinct approach compared to handling text attribute graphs. Extending the functionality of LLMs to effectively process and analyze molecules will open up opportunities for molecular related tasks.\n\nTo apply LLMs for molecular tasks, existing solutions often involve converting molecular structures into a format that can be processed by LLMs. One common approach is to use the Simplified Molecular Input Line Entry System\n\nAnswer Answer   \nLarge Language Model Large Language Model ‰∏ç ‰∏™ Tokenizer Tokenizer ‰∏™   \n‚Ä¢ Molecule: CC(C=C1)C=O ‚Ä¢ (i,j) means node i connects j.   \n‚Ä¢ What's the homo-lumo gap ‚Ä¢ What's the homo-lumo gap Molecular graph (0,1)‚Ä¶(1,3). of this molecule? of this molecule? (a) SMILES as input (b) Textualized graph as input Answer Answer   \nLarge Language Model Large Language Model   \n000 1000 ‰∏™   \nTokenizer Projector Tokenizer Graph Tokenizer ‰∏™ ‰∏™ Tokenizer   \nWhat's the Encoding Learn a new token   \nhomo-lumo   \ngap of this ‚Ä¶molecule <|graph|>‚Ä¶graph‚Ä¶ molecule? What is the homo-lumo gap‚Ä¶   \n(c) Graph-language tuning (d) Graph2Token (ours)\n\n(SMILES) or SELF-referencIng Embedded Strings (SELFIES), which represent molecules as text strings (Fig. 1.(a)). For instance, Guo et al. (2023b) use SMILES as molecular representations and employ in-context learning to guide ChatGPT in understanding molecular structures. However, a significant limitation is that LLMs often lack a proper understanding of molecular representations in SMILES strings, which in many cases leads to inaccurate or inconsistent results (Guo et al. 2023b). Therefore, parameter-efficient fine-tuning is applied to enhance LLMs‚Äô comprehension of molecular text representations (Fang et al. 2023). However, this method tends to overlook molecular structure and inevitably weakens the generalization ability of LLMs by altering their semantic space during fine-tuning. This underscores the limitations of using text data to represent molecules within the context of LLMs.\n\nAnother line of methods implicitly leverages molecular graph structure information by converting graph structure into textual representations before feeding it to the model (Fig. 1.(b)). These methods typically involve describing the adjacency relationships between nodes of the graph and representing the properties of nodes using text (Wang et al. 2024; Fatemi, Halcrow, and Perozzi 2024; Zhao et al. 2023b; Liu and Wu 2023). Combined with zero-shot or few-shot learning techniques, as well as prompting methods, they guide LLMs in understanding complex structures. However, relying solely on textual representation of structured data is insufficient for conducting graph reasoning using LLMs (Fatemi, Halcrow, and Perozzi 2024).\n\nGiven the limitations of representing structured data in text, inspired by the success of multimodal large language models, researchers are exploring the use of graph-language tuning (Fig. 1.(c)). This involves leveraging the relationship between structured data and textual descriptions to align them in embedding space by fine-tuning a small number of parameters. As illustrated in Fig. 1.(c), the core component is a trainable projector, which maps graph features into the text space. In existing work, various projectors are designed to align graph structures with text space using available molecular-text pairs (Liu et al. 2023a). Cao et al. (2023) utilize linear mapping, whereas Liu et al. (2023b) and Li et al. (2024) implement Q-Former (Li et al. 2023).\n\nThe graph-language tuning approaches usually adopt parameter-efficient fine-tuning, such as LoRA fine-tuning (Hu et al. 2021). Although only a small number of parameters are tuned, it still leads to the forgetting of knowledge in some tasks, affecting the model‚Äôs generalization ability to a certain extent. The reason lies in the inherent differences between graph-language models and vision-language models. For vision-language models, the success is largely due to the access to extensive, high-quality datasets. For instance, InstructBLIP‚Äôs visual encoder capitalizes on $4 0 0 \\mathbf { M }$ image-text pairs (Radford et al. 2021), while the training of its projector utilizes a refined vision-language dataset covering 26 datasets to ensure diversity, each featuring superior quality (Dai et al. 2023). Conversely, the biological domain suffers from a scarcity of such data, unable to match the quantity and quality of data in the vision field. Given these constraints, the challenging questions arise:(1) Can we harness the rich prior knowledge inherent in LLMs to learn a molecular graph representation without fine-tuning the LLM backbone? (2) Will this approach preserve LLM‚Äôs remarkable few-shot generalization, vital in biomolecular domains with limited samples?\n\nIn this paper, we give an affirmative answer by proposing Graph2Token, a simple and effective solution, which generates a molecule graph token and aligns it to LLM tokens. The key idea is to learn a graph token representation using the LLM token vocabulary. In this way, a graph token can be naturally adapted by the LLM, without finetuning the LLM backbone. Intuitively, for LLMs to comprehend an unseen graph token from scratch, it is analogous to a human expert who would associate a given unseen representation with existing prior knowledge, and then retrieve relevant information from its knowledge base rooted in their association. Building upon this insight, we propose a novel alignment strategy that associates the molecular graph with LLM pre-trained token embeddings through cross multi-head attention, then retrieve useful contents from LLM token embeddings based on the computed attentions to represent the graph token. To better generate a graph token representation, we construct a molecular-text paired dataset from multiple data sources (CHEBI and HMDB), aiming to augment the dataset with biomolecular data related to human metabolism. Furthermore, we concurrently construct a dataset of molecular IUPAC name identifiers, incorporating them into the prompts to activate the LLMs knowledge for target molecules. Experiments results in fewshot scenarios, specifically those with large label distribution shifts and unseen tasks, show the competitive performance of Graph2Token. Our main contributions are as follows:\n\n‚Ä¢ We introduce a novel concept of learning a new graph token for LLMs and propose a lightweight token alignment approach that can adapt a molecular graph token to LLMs without fine-tuning the LLM backbone.   \n‚Ä¢ We construct a molecular-text dataset and IUPAC name dataset to reduce the gap between the graph and text modality.   \n‚Ä¢ By extensive experiments in few-shot learning scenarios, our method achieves superior performance, even when encountering the unseen new tasks and greatly varied label distributions.\n\n# Related Work\n\n# Textual Molecules for LLMs\n\nRecently, some studies have explored the application of LLMs in chemistry and materials science (Jablonka et al. 2023a,b; Castro Nascimento and Pimentel 2023), where the SMILES or SELFIES representations of molecules are taken as input to LLMs. Guo et al. (2023b) establish a benchmark containing eight chemistry tasks that feed the SMILES strings to LLMs like GPT-4 (Achiam et al. 2023), Llama (Touvron et al. 2023) and Galactica (Taylor et al. 2022), etc, to evaluate the capabilities of understanding and reasoning for molecules. However, a significant limitation of LLMs is their lack of understanding of molecular representations in SMILES strings, which in many cases leads to inaccurate or inconsistent results. Therefore, Fang et al. (2023) employ Parameter-Efficient Fine-Tuning (PEFT) to train a moleculeoriented domain LLM using molecular related instructions and SELFIES strings.\n\nNote that representing molecules solely through SMILES and SELFIES often neglects inherent structural information. Molecules can be naturally modeled as graphs (Xia et al. 2022). Some works have delved into translating graphs into natural language, thus enabling to directly apply LLMs for analysis and inference (Wang et al. 2024; Fatemi, Halcrow, and Perozzi 2024; Liu and Wu 2023; Guo et al. 2023a; Zhao et al. 2023b). This kind of methods can be regarded as describing the graph as implicit structural information for LLMs to solve graph tasks (Fan et al. 2024), e.g., the\n\nStage1 Molecular Graph Projector GNN Encoder m1m2m3‚Ä¶mN t1 Text Description Text Projector t2 t3 Encoder Ëá™   \ntage2 IUPAC Graph Domain   \n<|graph|> Name Tokenizer Task Instruction Large Language Model Output Task Head\n\nadjacency structure of molecular graphs is described and input into LLMs. However, due to the limitation of input length, LLMs can only obtain local structural information, and long contexts may weaken the reasoning ability (Liu et al. 2024b) and instruction following ability (Chen et al. 2024b) of LLMs.\n\nGraph-Language Tuning Graph-language tuning leverages graph-text pairs to map the graph modality to the text modality through training a portion of parameters. This concept stems from the multi-modal large models, where diverse modalities such as images (Liu et al. 2024a), 3D point clouds (Panagopoulou et al. 2023), and videos (Huang et al. 2024) are represented in the text space through instructiontuned mapping functions, facilitated by large-scale and highquality visual-instruction datasets. Similar attempts have been made with text-attributed graphs (Tang et al. 2024; Zhang et al. 2024), and molecules (Cao et al. 2023; Liu et al. 2023b; Li et al. 2024; Zhang et al. 2023). However, molecular graph-language fine-tuning methods have not demonstrated the same level of generalization capability as visual large language models, primarily due to the scarcity of molecular-text pair data in the biomolecular domain and the inherent complexity of tasks compared to the visual realm.\n\n# Methods\n\nThe main idea of our approach is to encode a graph into a token, and leverage the pre-trained vocabulary of LLMs to learn a new graph token representation. In this way, it enables the model to transfer from unknown to known contexts without fine-tuning the LLM backbone. The model involves two training stages as shown in Fig. 2: the first stage trains a molecular graph encoder for encoding graph structure and transforming textual semantics; the second stage utilizes this encoder to learn a graph tokenizer, converting unknown graph tokens into the LLM tokens.\n\n# Multi-source Molecular-Text Dataset and Pretrained Graph Encoder\n\nTo train a molecular graph encoder that reduces the distance between graphs and texts representations in the feature space, we first construct a multi-source molecular-text dataset, integrating molecular-description pairs from CHEBI (Degtyarenko et al. 2007) and HMDB (Wishart et al. 2022). Existing molecular-description datasets typically originate from CHEBI and Pubchem (Wang et al. 2009), focusing on common chemical small molecules with annotations by domain experts. HMDB extends the scope to human metabolism-related molecules, encompassing rare and newly synthesized compounds, sourced from scientific literature. Therefore, HMDB not only augments the data sources but also enhances the representation of biological molecules within the human metabolome category, potentially boosting the predictive performance for biomolecular properties. Our consolidated molecular-description dataset can be used for training an efficient molecular graph encoder.\n\nThe training of stage 1 follows a similar approach as the CLIP framework (Radford et al. 2021), which fuses two modalities through contrastive learning. Initially, a graph encoder and a text encoder are employed to convert molecular graph structures and textual descriptions into feature representations. Subsequently, the linear layers are appended to project feature dimensions for graph and text features, respectively. Following CLIP‚Äôs objective optimization strategy, we also utilize the InfoNCE loss function (Oord, Li, and Vinyals 2018), encouraging the graph structure and text representation of the same molecule within batches to cluster together while pushing mismatched pairs apart. Specifically, we employ the Graph Isomorphism Network (GIN) (Xu et al. 2018) as our molecular graph encoder, renowned for its model expressivity capable of achieving the 1-WL. For text encoding, we leverage BERT (Devlin et al. 2018), which is predominant in embedding texts.\n\n# Aligning Graph Tokens to LLM Token Space\n\nIn order to enable LLMs to effectively comprehend molecular patterns, current studies fine-tune a projector and LLM backbone to align molecular graphs with texts, potentially altering semantics and reducing the capacity to follow instructions for other tasks. Different from existing approaches, we treat a graph as a special token and design a learnable tokenizer that harnesses the prior knowledge embedded in LLMs to align graphs into representations comprehensible by the LLMs, without fine-tuning the backbone. The whole framework is shown in Fig. 3.\n\nLLM input prompts The prompt for the LLMs, as depicted in the left of Fig. 3, comprises four components: the IUPAC name in blue, the graph token in red, the domain task in gray, and the instruction in purple. IUPAC name is prevalent as identifiers in biochemical literature and is possibly included in training corpus of LLMs. And it inherently contains structural information. Therefore, we collect the IUPAC name information from PubChem database and construct the molecular datasets in IUPAC version. By incorporating IUPAC name into prompts, it can guide the LLMs\n\nIUPAC Name   \nThis molecule with IUPAC name is 0.2756 Task Head   \n(2S)-1,1-dimethyl,4-carbaldehyde.   \nGraph Token   \nAnd given the graph representation Large Language Model   \n<|graph|>.   \nDomain Task This molecule‚Ä¶graph representation <|graph|>‚Ä¶analysis‚Ä¶predictions  <|value|>   \nThe HOMO-LUMO gap is the   \nenergy difference between ‚Ä¶ D   \nInstruction Graph Tokenizer ‚ñ°‚ñ° Tuning Frozen   \nWhat is the HOMO-LUMO gap of   \ntaghcriacspouhmntorletpchruelesIeUn?tPaAtPiCloenansaetmoet aakneadlynistios EnGcNodNer AssCorcoisastion VocLaLbuMlary   \nthis molecule, and then generate and Retrieval Compressor   \nthe predictions <|value|>.   \nWhat is the graph ? Cross Association and Retrieval I will learn it with my knowledge. V K . C Molecular Q G SOSf LLM Token Graph Embeddings\n\nto retrieve more relevant information from their knowledge bases and facilitate the understanding of molecules.\n\nThe graph token $\\cdot \\qquad ,$ within the prompt is not initially in LLM vocabulary, and will be processed by the specifically designed graph tokenizer. Domain task encompasses the molecular tasks, and instruction involves both the questions and our requirements posed to LLMs. Note that molecular property prediction often involves tasks whose output typically relies on numerical data, such as HOMOLUMO gap. However, LLMs focus on syntactic relations and optimize cross-entropy loss to predict tokens, which is in contrast to the continuous value distribution required by numeric-centered regression tasks. A straightforward solution is to make the model initially generate text-based outputs that can be converted into the desired format subsequently. Thus, we designate a placeholder $\\cdot \\qquad ,$ to store predicted value within the instruction section. Given that the special placeholder is not included in the pretrained LLM vocabulary, we conduct global meaning of all pre-trained token embeddings and use it to place the embedding after tokenizing the placeholder $\\cdot \\qquad ,$ . As seen in Fig. 3, the prompt containing $\\cdot < | g r a p h | > ^ { \\prime }$ and $\\cdot \\qquad ,$ is directly input into LLM. Except the graph token, which is processed by a dedicated graph tokenizer, the remaining parts are handled using the LLM tokenizer.\n\nLearn a new graph token based on LLM vocabulary Next, we introduce how to design a graph tokenizer that adaptively aligns molecular graph representations into the LLM token space. Specifically, we propose to generate a graph token representation through associating pretrained token embeddings from LLM vocabulary, which can be viewed as its prior knowledge.\n\nAs illustrated in Fig. 3, graph tokenizer consists of three components. A graph neural network pretrained from stage 1 transforms molecular graphs into semantically meaningful graph features $\\pmb { g } \\in \\mathbb { R } ^ { e }$ , which act as query patterns. For the LLM token embeddings $\\mathbf { M } \\in \\mathbb { R } ^ { | \\mathbf { M } | \\times D }$ , where $| \\mathbf { M } |$ is the number of LLM tokens and $D$ is the dimension, it‚Äôs well known that LLM vocabulary possesses an extensive searchable space, exemplified by Llama2 with 32,000 tokens and Llama3 having an even larger number of 128,000 tokens. This poses significant challenges in computing the associations between the graph token and LLM token embeddings. Therefore, we propose a compression module to condense the token embeddings in the entire vocabulary into a fixed set of semantic vectors. To maximize the preservation of semantic information during compression, we employ a linear mapping function to project dense token embeddings into a condensed set of textual vectors $\\mathbf { M ^ { \\prime } } \\in \\mathbb { R } ^ { | \\mathbf { M ^ { \\prime } } | \\times D }$ , where $\\left| \\mathbf { M } ^ { \\prime } \\right| \\ll \\left| \\mathbf { M } \\right|$ , aiming to preserve as much semantic information as possible. Subsequently, both the graph feature $\\textbf {  { g } }$ and the compressed token embeddings $\\mathbf { M } ^ { \\prime }$ are fed into the designed cross association and retrieval module, which uses cross multi-head attention network. The graph feature $\\textbf {  { g } }$ is treated as query pattern while the pretrained token embeddings are key and value patterns. Before feeding to the crossattention network, we first map graph patterns to a common associative space with dimension $d$ using linear transformations $\\mathbf { Q } \\ { \\overset { \\mathbf { \\sigma } } { = } } \\ g \\mathbf { W } _ { Q }$ , $\\textbf { Q } \\in \\ \\mathbb { R } ^ { 1 \\times d }$ . Similarly, we can obtain token embeddings $\\mathbf { K } = \\mathbf { M } ^ { \\prime } \\mathbf { W } _ { K }$ , $\\mathbf { K } \\in \\mathbb { R } ^ { | \\mathbf { M } ^ { \\prime } | \\times d }$ and $\\mathbf { V } = \\mathbf { M } ^ { \\prime } \\mathbf { W } _ { V }$ , $\\mathbf { V } \\in \\mathbb { R } ^ { \\left| \\mathbf { M } ^ { \\prime } \\right| \\times d }$ . $\\mathbf { W } _ { Q }$ , ${ \\bf W } _ { K }$ and $\\mathbf { W } _ { V }$ are trainable parameters. Then we associate the graph pattern and LLM memory pattern via computing cross-attention matrix, and retrieve from memory according to attention matrix:\n\n$$\ng _ { k } = \\mathrm { A T T } ( \\mathbf { Q } _ { k } , \\mathbf { K } _ { k } , \\mathbf { V } _ { k } ) = \\mathrm { S o f t m a x } \\big ( \\frac { \\mathbf { Q } _ { k } \\mathbf { K } _ { k } ^ { \\top } } { \\sqrt { d } } \\big ) \\mathbf { V } _ { k } ,\n$$\n\nwhere $k$ represents the $k$ -th attention head. Ultimately, a linear neural network is utilized to project the updated graph token embeddings $g$ into the dimension $D$ of the LLM token embeddings.\n\nFollowing the aforementioned association and retrieval process, we obtain molecular graph token representations that are comprehensible by the LLMs, effectively replacing the placeholder vector representation $\\cdot < | g r a p h | > ^ { \\prime }$ within the prompt. These graph tokens, along with other prompt embeddings, are then jointly input into the LLM backbone for further processing.\n\n# Output and Task Head\n\nUpon packaging and forwarding the instructions and graph structure embeddings through the frozen LLM backbone, we discard the prefix portion to obtain the output representation aiming to adapt the graph-level tasks. To derive the final predictions, a task head with linear projection is employed. Task heads cater to graph regression tasks and graph classification tasks. The optimization objective for graph regression tasks is to minimize the mean squared error, while cross-entropy loss is used for graph classification tasks.\n\nWe can see that the trainable parameters in our Graph2Token primarily consist of the graph tokenizer module and the task head, which are negligible compared to the parameters of LLMs. By having the original parameters of the LLMs frozen, Graph2Token better preserves their inherent semantics and functionality.\n\n# Experiments\n\nIn this section, we conduct extensive experiments to evaluate Graph2Token for molecular property predictions. We try to answer these four questions: RQ1: Can the model handle the unseen new molecular tasks across different datasets? RQ2: Can Graph2Token effectively generalize to different datasets when the labels vary greatly? RQ3: How does each key component of Graph2Token contribute to enhancing the model‚Äôs capabilities? RQ4: How is the tuned number of parameters of Graph2Token compared with other approaches?\n\n# Experimental Settings\n\nDatasets For RQ1, BBBP, BACE, HIV, TOX21 datasets from MoleculeNet (Wu et al. 2018) are adopted that contain 15 molecular graph classification tasks. For RQ2, QM9 (Ramakrishnan et al. 2014) and PubchemQC (Nakata and Shimazaki 2017) datasets are used to predict energy-related properties of HOMO, LUMO, HOMO-LUMO gap $( \\Delta \\epsilon )$ for graph regression tasks.\n\nFor domain tasks and instructions in our prompts, we follow GIMLET (Zhao et al. 2023a) and Mol-Instructions (Fang et al. 2023), and refine them with GPT4 (Achiam et al. 2023). All the datasets are divided into training, validation, and test sets with a ratio of 8:1:1. In graph classification, datasets follow the setting of GIMLET according to the scaffold splitting way. We collect the IUPAC name information from Pubchem database (Wang et al. 2009) and construct the IUPAC version for all the above datasets.\n\nTraining Setup The GNN is a 5-layer GIN and the employed LLM is the Llama3-8B. As shown in Tab. 3, trainable parameters are from task head and graph tokenizer including GNN, cross association and retrieval, LLM compressor. When transferring to few-shot scenarios, we freeze the LLM compressor and only tune the remaining three parts.\n\nBaselines We incorporate both graph-based models and LLM-based models as baselines. The graph-based models include graph prompt learning method GPF (Fang et al. 2024) with AttrMasking (Hu et al. 2019) and GCL (You et al. 2020) as pretrained GNN models. For the LLM-based models, we consider the following categories: directly input the textual molecules as well as instruction prompts including Galactica-6.7B (Taylor et al. 2022), Galactica-120B (Taylor et al. 2022) and Vicuna-v1.5-13B-16K (Chiang et al. 2023); LoRA fine-tuning methods on the entire training set based on Llama2-Chat-7B (Touvron et al. 2023), Vicunav1.3-7B (Chiang et al. 2023); LoRA fine-tuning methods in few-shot scenarios including Llama3-Chat-8B (Dubey et al. 2024), Vicuna-v1.5-7B (Chiang et al. 2023) and molecule-oriented approach Mol-Instructions (Fang et al. 2023); graph-language tuning methods including MolCA (Liu et al. 2023b) and its variant.\n\n# Few-Shot Performance on Classification Task\n\nSetups To answer RQ1, we design two experiments on molecular classification datasets across 15 tasks. The first experiment involves training the second stage using a synthesized dataset that includes three tasks across BACE, BBBP and HIV datasets, then evaluating the model‚Äôs generalization ability on the TOX21 with 12 tasks. The second experiment focuses on training the second stage exclusively with the TOX21 dataset, followed by evaluating the model on the BACE, BBBP, and HIV datasets individually.\n\nResults The results of $5 \\%$ and $10 \\%$ few-shot learning are shown in Tab. 1. We can see that Graph2Token exceeds all baselines when confronted with the new molecular tasks. We attribute this to the use of LLMs and coupled with a successful way of integrating graph structural information. Indeed, the graph-language tuning model MolCA has also demonstrated impressive results, further underscoring the potential of fusing graph structures into LLMs for tackling molecular tasks. When using $10 \\%$ of the training samples, Graph2Token achieves an average $3 . 7 \\%$ improvement in ROC-AUC compared to the best results of MolCA. Remarkably, despite the limited amounts of training data, Graph2Token can match the performance of LLMs with LoRA fine-tuned on the entire training set, as seen in LoRA Fine-tuneing part in Tab. 1. Similar trends can be observed in the $5 \\%$ few-shot learning scenarios, where our average improvement over MolCA exceeds $4 \\%$ .\n\nTable 1: Few-shot learning on unseen molecular classification tasks using $5 \\%$ and $10 \\%$ training data, respectively. We report the results in ROC-AUC. Instruction Prompt: Using input prompt as shown in Fig.1(a). Graph2Token (tox21): Training on tox21 and transfering to other three datasets. Graph2Token (bace, bbbp, hiv): Training on the synthesized dataset of BACE, BBBP, and HIV, and transfering to tox21. The best results are in bold.   \n\n<html><body><table><tr><td>Method Type</td><td>Method</td><td>BBBP ‚Üë</td><td>BACE ‚Üë</td><td>HIV ‚Üë</td><td>TOX21 ‚Üë</td><td>Avg ‚Üë</td></tr><tr><td rowspan=\"2\">Instruction Prompt</td><td>Galactica-6.7B</td><td>53.5</td><td>58.4</td><td>72.2</td><td></td><td>61.4</td></tr><tr><td>Galactica-120B</td><td>66.1</td><td>61.7</td><td>74.5</td><td></td><td>67.4</td></tr><tr><td rowspan=\"2\">LoRA Fine-tuning</td><td>Vicuna-v1.5-13B-16k [4-shot]</td><td>49.2</td><td>52.7</td><td>50.5</td><td></td><td>50.8</td></tr><tr><td>Llama-2-7B-chat Vicuna-v1.3-7B</td><td>65.6 60.1</td><td>74.8 68.3</td><td>62.3 58.1</td><td></td><td>67.6 62.6</td></tr><tr><td>5% Few-shot learning</td><td>Graph-Based Models</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"8\"></td><td>GPF-AttrMasking</td><td>53.1</td><td>58.9</td><td>66.9</td><td>64.7</td><td>60.9</td></tr><tr><td>GPF-GCL</td><td>52.6</td><td>61.0</td><td>62.3</td><td>52.0</td><td>57.0</td></tr><tr><td>LLM-Based Models</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Llama3-Chat-8B (bace,bbbp,hiv)</td><td></td><td></td><td></td><td>59.9</td><td>59.9</td></tr><tr><td>Llama3-Chat-8B (tox21)</td><td>60.5</td><td>54.3</td><td>63.3</td><td></td><td>59.4</td></tr><tr><td>MolCA-S</td><td>58.9</td><td>60.2</td><td>66.8</td><td>63.2</td><td></td></tr><tr><td>MolCA-GS</td><td>59.1</td><td>61.4</td><td>69.5</td><td>64.3</td><td>62.3</td></tr><tr><td>Graph2Token (bace,bbbp,hiv)</td><td></td><td></td><td></td><td>68.7</td><td>63.6 68.7</td></tr><tr><td>Graph2Token (tox21)</td><td>61.0</td><td>63.1</td><td></td><td>72.3</td><td></td><td>65.5</td></tr><tr><td rowspan=\"10\">10% Few-shot learning</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Graph-Based Models GPF-AttrMasking</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>58.8</td><td>62.2</td><td>71.3</td><td>66.0</td><td>64.6</td></tr><tr><td>GPF-GCL</td><td>56.5</td><td>52.1</td><td>49.3</td><td>63.6</td><td>55.4</td></tr><tr><td>LLM-Based Models</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Llama3-Chat-8B (bace,bbbp,hiv)</td><td></td><td></td><td></td><td>65.6</td><td>65.6</td></tr><tr><td>Llama3-Chat-8B (tox21)</td><td>60.8 62.5</td><td>58.8 62.8</td><td>67.4 69.0</td><td>66.6</td><td>62.3</td></tr><tr><td>MolCA-S MolCA-GS</td><td>63.6</td><td>63.9</td><td>72.7</td><td>68.5</td><td>65.2</td></tr><tr><td>Graph2Token (bace,bbbp,hiv)</td><td></td><td></td><td></td><td>72.1</td><td>67.2 72.1</td></tr><tr><td>Graph2Token (tox21)</td><td>65.2</td><td>66.0</td><td>74.9</td><td></td><td>68.7</td></tr></table></body></html>\n\n# Few-Shot Performance on Regression Task\n\nSetups To answer RQ2, we consolidate three tasks from the QM9 dataset‚ÄîHOMO, LUMO, and $\\Delta \\epsilon$ ‚Äîinto a multitask dataset to conduct the training of stage 2. We evaluate on few-shot scenarios with a subset of training data $( 5 \\%$ and $10 \\%$ molecular samples) on the PubchemQC-IUPAC dataset. For both Vicuna-7B and Llama3-Chat-8B, we employ SMILES as the molecular representation and finetune their backbones with LoRA on the consolidated QM9 dataset, and then transfer to the PubchemQC-IUPAC. MolInstructions has been trained on QM9, and therefore we directly transfer it to PubchemQC-IUPAC.\n\nResults The results of $5 \\%$ and $10 \\%$ few-shot scenarios are presented in Tab. 2. We can see that Graph2Token outperforms all baselines, which underscores the effectiveness of our strategy in aligning a graph token into the manner that LLM can understand. Graph2Token realizes average $7 . 1 \\%$ and $6 . 4 \\%$ reductions in comparison to graphlanguage tuning method MolCA. Compared to Llama3-Chat with LoRA fine-tuning, our average enhancements are $20 \\%$ and $16 \\%$ . We also observe that, in the case where labels vary greatly, LLMs frequently exhibit hallucinations, generating responses that appear plausible yet deviate from factfulness. Graph2Token can adapt to new datasets with limited labeled molecular samples, demonstrating remarkable few-shot generalization capabilities.\n\n# Ablation Study\n\nThis section addresses RQ3 by investigating how the key components of Graph2Token contribute to the performance, i.e., IUPAC name in prompt (-IUPAC), cross association and retrieval for alignment (-Alignment), Molecular-Text data for pretrained GNN (-MT Data) and LLM (-LLM).\n\nTable 2: Few-shot learning on molecular regression tasks using $5 \\%$ and $10 \\%$ training data, respectively. We report the results in MAE on PubchemQC-IUPAC dataset. The best results are in bold.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"4\">5% few-shot learning</td><td colspan=\"4\">10% few-shot learning</td></tr><tr><td>HOMO(eV)‚Üì LUMO(eV)‚Üì ‚ñ≥e(eV)‚Üì Avg(eV)‚Üì HOMO(eV)‚Üì LUMO(eV)‚Üì ‚ñ≥e(eV)‚Üì Avg(eV)‚Üì</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan=\"9\">Graph-Based Models</td></tr><tr><td>GPF-AttrMasking</td><td>0.550</td><td>0.853</td><td>0.880</td><td>0.761</td><td>0.545</td><td>0.852</td><td>0.866</td><td>0.754</td></tr><tr><td>GPF-GCL</td><td>0.545</td><td>0.849</td><td>0.871</td><td>0.755</td><td>0.532</td><td>0.854</td><td>0.850</td><td>0.729</td></tr><tr><td colspan=\"9\">LLM-Based Models</td></tr><tr><td>Mol-Instructions</td><td>1.017</td><td>1.153</td><td>1.239</td><td>1.136</td><td>0.855</td><td>1.121</td><td>0.900</td><td>0.959</td></tr><tr><td>Vicuna-7B</td><td>0.683</td><td>0.795</td><td>0.915</td><td>0.798</td><td>0.440</td><td>0.706</td><td>0.491</td><td>0.546</td></tr><tr><td>Llama3-Chat-8B</td><td>0.453</td><td>0.532</td><td>0.552</td><td>0.512</td><td>0.392</td><td>0.471</td><td>0.462</td><td>0.442</td></tr><tr><td>MolCA-S</td><td>0.436</td><td>0.461</td><td>0.410</td><td>0.436</td><td>0.325</td><td>0.363</td><td>0.390</td><td>0.359</td></tr><tr><td>MolCA-GS</td><td>0.425</td><td>0.453</td><td>0.399</td><td>0.426</td><td>0.320</td><td>0.343</td><td>0.376</td><td>0.346</td></tr><tr><td>Graph2Token (ours)</td><td>0.407</td><td>0.382</td><td>0.386</td><td>0.392</td><td>0.292</td><td>0.333</td><td>0.346</td><td>0.324</td></tr></table></body></html>\n\nTable 3: Ablation results (ROC-AUC) on molecular classification tasks on BBBP and TOX21 datasets.   \n\n<html><body><table><tr><td>Method</td><td>BBBP‚Üë</td><td>TOX21 ‚Üë</td><td>Avg ‚Üë</td></tr><tr><td>5% few-shot learning</td><td></td><td></td><td></td></tr><tr><td>w/o IUPAC</td><td>59.2</td><td>67.8</td><td>63.5</td></tr><tr><td>w/o Alignment</td><td>58.5</td><td>66.7</td><td>62.6</td></tr><tr><td>w/o MT Data</td><td>60.4</td><td>66.8</td><td>63.6</td></tr><tr><td>w/o LLM</td><td>56.9</td><td>65.5</td><td>61.2</td></tr><tr><td>Graph2Token(ours)</td><td>61.0</td><td>68.7</td><td>64.9</td></tr><tr><td colspan=\"4\">10% few-shot learning</td></tr><tr><td>w/o IUPAC</td><td>63.5</td><td>71.0</td><td>67.3</td></tr><tr><td>w/o Alignment</td><td>62.1</td><td>69.4</td><td>65.8</td></tr><tr><td>w/o MT Data</td><td>63.3</td><td>70.3</td><td>66.8</td></tr><tr><td>w/o LLM</td><td>60.0</td><td>66.7</td><td>63.4</td></tr><tr><td>Graph2Token(ours)</td><td>65.2</td><td>72.1</td><td>68.7</td></tr></table></body></html>\n\nAs shown in Tab. 3, when removing the LLM and only retaining the pretrained GNN with a linear layer for specific tasks, there is a sharp performance decline, indicating that our strategy, adding a graph tokenizer without fine-tuning the LLM backbone, can maintain the remarkable few-shot generalization ability of LLMs. For the alignment, we replace the cross-association and retrieval with a single linear mapping to maintain the feature dimensions transformation. The results show that the LLM vocabulary can serve as prior knowledge, with cross attention calculating the relevance to the unknown graph token. In this way, useful embedding information is retrieved based on the calculated association that can be leveraged to represent graph features in a LLMs comprehensible manner. By replacing the graph encoder pre-trained on MT Data with random initialization, we observe a performance drop, indicating the importance of bridging the feature gap between graph and text modality. Furthermore, by incorporating IUPAC name, we activate the LLMs‚Äô relevant knowledge of target molecules, which leads to improved performance.\n\nTable 4: Number of tuned parameters of Graph2Token compared with LLM-Based Models.   \n\n<html><body><table><tr><td>Method</td><td>Param. (M)</td><td>Train Ratio. (%)</td></tr><tr><td>Graph2Token</td><td>8.60</td><td>0.11</td></tr><tr><td>MolCA</td><td>109.09</td><td>7.65</td></tr><tr><td>Mol-Instructions</td><td>39.98</td><td>0.59</td></tr><tr><td>Llama3-Chat-8B</td><td>88.89</td><td>1.03</td></tr><tr><td>Vicuna-7B</td><td>79.95</td><td>1.17</td></tr></table></body></html>\n\n# Efficiency Study\n\nFor RQ 4, Tab. 4 shows the number of tuned parameters of Graph2Token and other LLM-Based baselines. This comparison reveals that our graph alignment approach serves as a lightweight solution, empowering LLMs to comprehend graph structures effectively. As mentioned in the training setup, under the few-shot scenarios, we only tune the graph encoder, cross association and retrieval along with task head, where the trainable parameters account for merely $0 . 1 1 \\%$ of the entire framework. While LoRA fine-tuning represents a parameter-efficient approach to adapt the LLM backbone to vertical domains, our method still prevails in balancing performance and efficiency.\n\n# Conclusion\n\nIn this paper, we propose Graph2Token, which bridges molecular graphs and LLMs via aligning a graph token to LLM token space. Graph2Token is a lightweight solution without fine-tuning LLM backbone. To achieve this, we first construct a molecular-text dataset from multiple resources, as well as a molecular IUPAC name dataset. Then, we design a cross association and retrieval module to align the graph representations with pre-trained LLM token embeddings. Evaluations on few-shot learning scenarios demonstrate Graph2Token maintains the few-shot generalization ability of LLMs and effectively solves the data scarcity issue in the molecular field.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§ÑÁêÜÂõæÊï∞ÊçÆÔºàÂ¶ÇÂàÜÂ≠êÁªìÊûÑÔºâÊó∂Ë°®Áé∞ÊúâÈôêÔºåÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏Â∞ÜÂàÜÂ≠êÁªìÊûÑËΩ¨Êç¢‰∏∫ÊñáÊú¨Ê†ºÂºèÔºàÂ¶ÇSMILESÔºâÔºå‰ΩÜËøôÁßçÊñπÊ≥ïÂøΩÁï•‰∫ÜÂàÜÂ≠êÁªìÊûÑ‰ø°ÊÅØÔºåÂØºËá¥ÁêÜËß£‰∏çÂáÜÁ°Æ„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÂú®ÂàÜÂ≠êÁõ∏ÂÖ≥‰ªªÂä°ÔºàÂ¶ÇÂàÜÂ≠êÂàÜÁ±ªÂíåÂõûÂΩíÔºâ‰∏≠Â∞§‰∏∫ÈáçË¶ÅÔºåÂõ†‰∏∫ÂàÜÂ≠êÁªìÊûÑÁöÑÂáÜÁ°ÆÁêÜËß£ÂØπ‰∫éËçØÁâ©ÂèëÁé∞ÂíåÊùêÊñôËÆæËÆ°Á≠âÂ∫îÁî®Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   Êú¨ÊñáÊèêÂá∫Graph2TokenÔºå‰∏ÄÁßçËΩªÈáèÁ∫ßËß£ÂÜ≥ÊñπÊ°àÔºåÈÄöËøáÂ∞ÜÂàÜÂ≠êÂõæÂØπÈΩêÂà∞LLMÁöÑtokenÁ©∫Èó¥Ôºå‰ΩøLLMsËÉΩÂ§üÁêÜËß£ÂàÜÂ≠êÂõæÁªìÊûÑÔºåËÄåÊó†ÈúÄÂæÆË∞ÉLLM‰∏ªÂπ≤„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ1Ôºö** ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂõætokenÂØπÈΩêÊñπÊ≥ïÔºåÈÄöËøáË∑®Â§öÂ§¥Ê≥®ÊÑèÂäõÂ∞ÜÂàÜÂ≠êÂõæ‰∏éLLMÁöÑÈ¢ÑËÆ≠ÁªÉtokenÂµåÂÖ•ÂÖ≥ËÅîÔºå‰ªéËÄåÁîüÊàêLLMÂèØÁêÜËß£ÁöÑÂõætokenË°®Á§∫„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ2Ôºö** ÊûÑÂª∫‰∫ÜÂ§öÊ∫êÂàÜÂ≠ê-ÊñáÊú¨ÈÖçÂØπÊï∞ÊçÆÈõÜÔºàCHEBIÂíåHMDBÔºâÂíåÂàÜÂ≠êIUPACÂêçÁß∞Êï∞ÊçÆÈõÜÔºå‰ª•ÂáèÂ∞ëÂõæ‰∏éÊñáÊú¨Ê®°ÊÄÅ‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ3Ôºö** Âú®Â∞ëÊ†∑Êú¨Â≠¶‰π†Âú∫ÊôØ‰∏≠ÔºåGraph2TokenÂú®ÂàÜÂ≠êÂàÜÁ±ªÂíåÂõûÂΩí‰ªªÂä°‰∏äË°®Áé∞‰ºòÂºÇÔºå‰æãÂ¶ÇÂú®TOX21Êï∞ÊçÆÈõÜ‰∏äÔºåROC-AUCÊèêÂçá‰∫Ü3.7%Ôºà10%Â∞ëÊ†∑Êú¨ÔºâÂíå4%Ôºà5%Â∞ëÊ†∑Êú¨ÔºâÔºåÂú®PubchemQC-IUPACÊï∞ÊçÆÈõÜ‰∏äÔºåMAEÈôç‰Ωé‰∫Ü6.4%Ôºà10%Â∞ëÊ†∑Êú¨ÔºâÂíå7.1%Ôºà5%Â∞ëÊ†∑Êú¨Ôºâ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   Graph2TokenÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂ∞ÜÂàÜÂ≠êÂõæÁºñÁ†Å‰∏∫‰∏Ä‰∏™ÁâπÊÆäÁöÑtokenÔºåÂπ∂Âà©Áî®LLMÁöÑÈ¢ÑËÆ≠ÁªÉËØçÊ±áË°®Â≠¶‰π†Êñ∞ÁöÑÂõætokenË°®Á§∫„ÄÇËøôÁßçÊñπÊ≥ïÈÄöËøáË∑®Â§öÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÂ∞ÜÂàÜÂ≠êÂõæÁâπÂæÅ‰∏éLLMÁöÑtokenÂµåÂÖ•ÂÖ≥ËÅîÔºå‰ªéËÄåÁîüÊàêLLMÂèØÁêÜËß£ÁöÑÂõætokenË°®Á§∫„ÄÇ\\n> *   ËØ•ÊñπÊ≥ï‰πãÊâÄ‰ª•ÊúâÊïàÔºåÊòØÂõ†‰∏∫ÂÆÉÂà©Áî®‰∫ÜLLMsÁöÑÈ¢ÑËÆ≠ÁªÉËØçÊ±á‰Ωú‰∏∫ÂÖàÈ™åÁü•ËØÜÔºåÈÄöËøáÊ≥®ÊÑèÂäõÊú∫Âà∂Â∞ÜÂàÜÂ≠êÂõæ‰∏éËøô‰∫õËØçÊ±áÂÖ≥ËÅîËµ∑Êù•Ôºå‰ªéËÄåÂÆûÁé∞ÂØπÂàÜÂ≠êÂõæÁöÑÁêÜËß£„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** Áé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏ÈÄöËøáÂæÆË∞ÉLLM‰∏ªÂπ≤ÊàñÂ∞ÜÂõæÁªìÊûÑËΩ¨Êç¢‰∏∫ÊñáÊú¨Êù•Â§ÑÁêÜÂàÜÂ≠êÂõæÔºåÂâçËÄÖ‰ºöÂâäÂº±LLMÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂêéËÄÖÂàôÂøΩÁï•‰∫ÜÁªìÊûÑ‰ø°ÊÅØ„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** Graph2TokenÈÄöËøáËΩªÈáèÁ∫ßÁöÑtokenÂØπÈΩêÊñπÊ≥ïÔºåÂú®‰∏çÂæÆË∞ÉLLM‰∏ªÂπ≤ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ∞ÜÂàÜÂ≠êÂõæÁªìÊûÑ‰ø°ÊÅØËûçÂÖ•LLMÁöÑtokenÁ©∫Èó¥Ôºå‰øùÁïô‰∫ÜLLMÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> *   **Ê≠•È™§1Ôºö** ÊûÑÂª∫Â§öÊ∫êÂàÜÂ≠ê-ÊñáÊú¨ÈÖçÂØπÊï∞ÊçÆÈõÜÔºàCHEBIÂíåHMDBÔºâÔºåËÆ≠ÁªÉÂàÜÂ≠êÂõæÁºñÁ†ÅÂô®‰ª•ÂáèÂ∞ëÂõæ‰∏éÊñáÊú¨ÁâπÂæÅÁ©∫Èó¥ÁöÑÂ∑ÆË∑ù„ÄÇ\\n> *   **Ê≠•È™§2Ôºö** ËÆæËÆ°ÂõætokenizerÔºåÈÄöËøáË∑®Â§öÂ§¥Ê≥®ÊÑèÂäõÂ∞ÜÂàÜÂ≠êÂõæÁâπÂæÅ‰∏éLLMÁöÑtokenÂµåÂÖ•ÂÖ≥ËÅîÔºåÁîüÊàêÂõætokenË°®Á§∫„ÄÇ\\n> *   **Ê≠•È™§3Ôºö** Â∞ÜÂõætoken‰∏éIUPACÂêçÁß∞„ÄÅ‰ªªÂä°Êåá‰ª§Á≠âÁªìÂêàÔºåËæìÂÖ•LLMËøõË°åÈ¢ÑÊµã„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   GPF-AttrMasking„ÄÅGPF-GCL„ÄÅGalactica-6.7B„ÄÅGalactica-120B„ÄÅVicuna-v1.5-13B-16K„ÄÅLlama-2-7B-chat„ÄÅMolCA-S„ÄÅMolCA-GSÁ≠â„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®ROC-AUC‰∏äÔºö** Âú®TOX21Êï∞ÊçÆÈõÜ‰∏äÔºåGraph2TokenÂú®10%Â∞ëÊ†∑Êú¨Âú∫ÊôØ‰∏≠ËææÂà∞‰∫Ü72.1%ÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãMolCA-GSÔºà68.5%ÔºâÂíåLlama3-Chat-8BÔºà66.6%Ôºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü3.7‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®MAE‰∏äÔºö** Âú®PubchemQC-IUPACÊï∞ÊçÆÈõÜ‰∏äÔºåGraph2TokenÂú®5%Â∞ëÊ†∑Êú¨Âú∫ÊôØ‰∏≠ÁöÑÂπ≥ÂùáMAE‰∏∫0.392 eVÔºå‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãMolCA-GSÔºà0.426 eVÔºâÂíåLlama3-Chat-8BÔºà0.512 eVÔºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü8.5%„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   ÂàÜÂ≠êÂõæ (Molecular Graph, N/A)\\n*   Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã (Large Language Model, LLM)\\n*   ÂõætokenÂØπÈΩê (Graph Token Alignment, N/A)\\n*   Â∞ëÊ†∑Êú¨Â≠¶‰π† (Few-shot Learning, FSL)\\n*   ÂàÜÂ≠êÂàÜÁ±ª (Molecular Classification, N/A)\\n*   ÂàÜÂ≠êÂõûÂΩí (Molecular Regression, N/A)\\n*   Ë∑®Â§öÂ§¥Ê≥®ÊÑèÂäõ (Cross Multi-head Attention, N/A)\\n*   IUPACÂêçÁß∞ (IUPAC Name, N/A)\"\n}\n```"
}