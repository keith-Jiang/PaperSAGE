{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.15388",
    "link": "https://arxiv.org/abs/2412.15388",
    "pdf_link": "https://arxiv.org/pdf/2412.15388.pdf",
    "title": "Investigating Relational State Abstraction in Collaborative MARL",
    "authors": [
        "Sharlin Utke",
        "Jeremie Houssineau",
        "Giovanni Montana"
    ],
    "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
    ],
    "publication_date": "2024-12-19",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "University of Warwick",
        "Nanyang Technological University"
    ],
    "paper_content": "# Investigating Relational State Abstraction in Collaborative MARL\n\nSharlin Utke1, Jeremie Houssineau2, Giovanni Montana1\n\n1University of Warwick, Coventry CV4 7AL, United Kingdom 2Nanyang Technological University, 50 Nanyang Avenue, Singapore 639798 sharlin.utke $@$ warwick.ac.uk, jeremie.houssineau $@$ ntu.edu.sg, g.montana $@$ warwick.ac.uk\n\n# Abstract\n\nThis paper explores the impact of relational state abstraction on sample efficiency and performance in collaborative Multi-Agent Reinforcement Learning. The proposed abstraction is based on spatial relationships in environments where direct communication between agents is not allowed, leveraging the ubiquity of spatial reasoning in real-world multiagent scenarios. We introduce MARC (Multi-Agent Relational Critic), a simple yet effective critic architecture incorporating spatial relational inductive biases by transforming the state into a spatial graph and processing it through a relational graph neural network. The performance of MARC is evaluated across six collaborative tasks, including a novel environment with heterogeneous agents. We conduct a comprehensive empirical analysis, comparing MARC against stateof-the-art MARL baselines, demonstrating improvements in both sample efficiency and asymptotic performance, as well as its potential for generalization. Our findings suggest that a minimal integration of spatial relational inductive biases as abstraction can yield substantial benefits without requiring complex designs or task-specific engineering. This work provides insights into the potential of relational state abstraction to address sample efficiency, a key challenge in MARL, offering a promising direction for developing more efficient algorithms in spatially complex environments.\n\nExtended Version with Appendix ‚Äî https://arxiv.org/pdf/2412.15388 Code ‚Äî https://github.com/sharlinu/MARC\n\n# 1 Introduction\n\nMulti-Agent Reinforcement Learning (MARL) has emerged as an extension of single-agent RL, where multiple agents simultaneously interact with the environment to derive their optimal behavior by trial and error. Despite the complexity and dynamics of the learning environment, MARL holds significant promise for modeling real-world systems involving nuanced interactions between multiple entities, such as autonomous vehicle coordination (Shalev-Shwartz, Shammah, and Shashua 2016), traffic flow optimization (Agogino and Tumer 2012), and team robotics (Matignon, Jeanpierre, and Mouaddib 2012). These applications often involve collaborative or competitive dynamics that singleagent RL struggles to capture adequately. However, the increase in dimensionality over state and action spaces, the additional agent interactions, and the information influx this entails make sample efficiency a key challenge.\n\nA critical aspect to sample efficiency is how agents represent the information of what they observe. The observation received by an agent can hold a lot of information, but not all of it is necessary to make an optimal decision. The ability to find abstract features allows for reasoning on a higher, conceptual level and adds robustness to small, task-irrelevant changes; a common principle for good representations (Bengio, Courville, and Vincent 2012). Abstraction leverages the underlying structure of the problem to focus on relevant information while reducing its complexity. That way, agents can learn optimal policies with fewer interactions, leading to improved sample efficiency and knowledge transfer to new situations (Mohan, Zhang, and Lindauer 2024).\n\nA natural structure to present the environment is the decomposition into objects and their relations. Recent work in both deep single-agent RL and MARL has demonstrated the benefits of leveraging this relational structure as a graphbased representation in the learning architecture (Bapst et al. 2019; Jiang et al. 2021; Nayak et al. 2023; Agarwal et al. 2020), improving sample efficiency and generalization capabilities. This seems especially important in MARL, where the complexity often scales with the number of agents. The ability of Graph Convolutional Networks (GCNs) (Scarselli et al. 2009; Welling and Kipf 2016; Gilmer et al. 2017) to model systems where the relationships between entities are critical has brought them to the forefront in many fields of multi-agent systems, ranging from modeling behavior and trajectories in multi-agent systems (e.g. Kipf et al. 2018; Tacchetti et al. 2019; Li et al. 2020; Kipf, van der Pol, and Welling 2020) to enhancing communication (e.g Niu, Paleja, and Gombolay 2021; Jiang et al. 2020; Zhang et al. 2021b).\n\nIn this study, we investigate the integration of a simple yet effective relational abstraction to MARL using the architectural flexibility of GCNs. We focus on collaborative tasks with high spatial complexity where direct communication between agents is not permitted due to cost or security constraints, e.g., in underwater robotics (Song, Stojanovic, and Chitre 2019). We specifically emphasize spatial relationships, as these are ubiquitous and readily available in real-world multi-agent scenarios. Spatial relations provide fundamental information about the relative positions and orientations of agents and objects, which is crucial for navigation, coordination, and collaboration in physical environments. This focus allows us to explore how relational information can be leveraged through implicit information already present in the environment, without relying on explicit communication channels or complex architectural designs. Our research aims to address two key questions: (1) Can the incorporation of a state abstraction using spatial inductive biases improve sample efficiency and asymptotic performance in MARL? (2) How do different choices in the design impact the learning in such relational architectures?\n\nTo address these questions, we propose MARC (MultiAgent Relational Critic), a simple multi-agent actor-critic architecture that abstracts the observation based on the relative positions of the entities into a graph-based representation. MARC utilizes a shared relational component within the critic architecture to efficiently learn a structured representation, further aiding sample efficiency. We conduct comprehensive empirical evaluation against state-of-the-art (SOTA) MARL baselines and across different tasks including a newly created collaborative multi-agent environment, designed to be a spatially demanding task between heterogeneous agents. Ultimately, we examine the impact of different design choices in our relational component in Section 5.\n\n# 2 Related Work\n\nState Abstraction in RL Abstraction has been widely studied, with early work showing theoretical properties in single-agent RL (Li, Walsh, and Littman 2006; Abel 2022). Inspired by Zucker (2003) and focusing on state abstraction, we define abstraction as a mapping of the ground-truth representation of a state to a simpler, more compact representation by preserving desirable properties and removing less critical information. In other words, abstraction simplifies the information representation by dropping the information that is not essential to the task. Many methods have shown the success of embedding an abstract representation. For example, Kipf, van der Pol, and Welling (2020) factorize state inputs into objects and apply a relational, object-centric state abstraction to model a multi-object system. Zhang et al. (2021a) aim to learn an abstract state representation from high-dimensional observations based on the behavioral similarity between states to encode only information relevant to the task. Abdel-Aziz et al. (2024) reduce the computational complexity between communicating agents by learning a state abstraction based on quadtree decomposition (Samet 1984). Zhang et al. (2021b) use a state abstraction component in the MARL setting to reduce the high-dimensional observations into a more compact latent presentation using dense neural networks. While the abstraction method and assumption we take are different, we leverage these methods‚Äô underlying idea to discard any information irrelevant to the task to create a more compact and efficient representation.\n\nRelational Representation in RL Before the integration of deep learning, traditional RL methods often falter in environments with relational structures or when generalization beyond initial training conditions is necessary. Relational RL addresses these challenges by learning the optimal policies over the objects and relations using a relational representation such as first-order logic. Whilst this approach has shown improved generalization and scalability, both in single-agent RL (DzÀáeroski, De Raedt, and Driessens 2001; Sanner and Boutilier 2009; Driessens and DzÀáeroski 2001) and in MARL (Croonenborghs et al. 2006; Ponsen et al. 2010; Li et al. 2022), the use of first-order logic comes with constraints, such as the need to hand-engineer features (Garnelo, Arulkumaran, and Shanahan 2016). Contemporary methods tackle this issue by learning the relations between objects using deep learning methods (Garnelo, Arulkumaran, and Shanahan 2016; Jiang et al. 2021; Zambaldi et al. 2019). These methods assume that observations comprise entities and the relationships between them while using deep learning methods as an inductive bias to learn over these structures. For example, Zambaldi et al. (2019) learns the importance of non-spatial relations between entities using attention mechanisms (Vaswani et al. 2017). They show superior performance and generalization capabilities compared to purely local relations in single-agent RL. Jiang et al. (2021) connect entities of a grid with a broader set of spatial relations, including remote relations, into a heterogeneous graph and passes them through a Relational Graph Convolutional Network (R-GCN) (Schlichtkrull et al. 2018). Their findings indicate that the imposition of structure by inducing a spatial bias can lead to improved asymptotic performance and generalization capabilities in single-agent tasks. However, they treat every cell in the grid as an entity, whether or not it contains an environment object. We extend this idea to MARL by employing a relational representation between agents and objects in the environment where the importance of the induced relations is implicitly learned using R-GCNs. Additionally, we only consider the environment objects, i.e. agents and other objects, as entities and use fewer relations, proposing a lean and abstract representation that better aligns with the computational complexity of MARL and works on continuous domains as well.\n\nRelational Inductive Bias in MARL Relational inductive bias, loosely defined as the imposition of structural constraints on the learning process based on the relationship between objects (Battaglia et al. 2018), is a principle commonly embedded in MARL architectures.\n\nA natural way to leverage structure in MARL is on the agent level. In fact, many of the commonly known MARL algorithms own an architecture that represents a form of relational inductive bias based on the structure between agents. For example, value decomposition methods such as QMIX (Rashid et al. 2018) assume conditional independence between the agents to decompose their value function. MAAC (Iqbal and Sha 2019) assumes that the influence of one agent‚Äôs information on another can vary. Each critic can dynamically select which agents to focus on by assessing the relevance of the encoded information from other agents via a multi-head attention layer that is shared between critics. The actor-critic methods introduced by Liu et al. (2020) extend the constraint imposed in MAAC by using an additional hard-attention layer to strengthen the assumption that not all other agents‚Äô information is relevant to succeed, further reducing the complexity of the game dynamics. Khan et al. (2019) leverage the underlying graph structure and symmetry between large numbers of homogeneous agents to parameterize the policies using a GCN framework.\n\nWhilst the decomposition of the MARL architecture on the agent level is very intuitive, some methods leverage the structure already found within the observation. For instance, VMARL (Liu et al. 2021) transforms high-dimensional visual inputs to an object-centric intermediate state representation where environment objects are linked by their proximity, before being fed to policy and actor networks. MAGNet (Malysheva, Kudenko, and Shpilman 2019) considers all environment objects as entities when pre-training a static relevance graph with known node and edge types, which is then used to represent the observations. Agarwal et al. (2020) and Nayak et al. (2023) employ distance-based observation graphs with learned attention weights between agents and objects, demonstrating that graph representations of the environment allow for a framework invariant to permutation and the number of entities in the environment. They both assume shared rewards among homogeneous agents that allow for communication within their neighborhood. What most of these methods have in common is that they take advantage of the strong relational inductive bias posed by graph neural network architectures, enforcing learning over entities and their relations. We similarly leverage graph neural architectures to enforce a structured observation. However, we leverage inherent spatial symmetries to reduce the observation complexity and employ a MARL architecture also applicable to heterogeneous agents.\n\n# 3 Methodology\n\n# 3.1 Preliminaries\n\nWe work under the framework of partially observable Markov Games, with $S$ being the state space in which each of the $N$ agents has their own action space $A _ { i }$ , with $i =$ $1 , \\ldots , N$ , forming a joint action space $A \\ = \\ A _ { 1 } \\times A _ { 2 } \\times$ $\\cdots \\times A _ { N }$ . After taking an action, each agent $i$ receives an observation $o _ { i } \\in O _ { i } \\subset S$ . Moreover, we assume individual reward functions, $R _ { i } : S \\times A \\to \\mathbb { R }$ , which gives a reward signal after every step. At each time step, the agents simultaneously choose actions according to their respective policies, $\\pi _ { i } : O _ { i } \\mapsto P ( A _ { i } )$ , which depend on the observation they receive. Consequently, the environment changes in line with the transition dynamics $T : S \\times A \\times S \\to [ 0 , 1 ]$ to a new state. The goal is that every agent finds the optimal policy that maximizes their expected cumulative return $\\begin{array} { r } { J _ { i } ( \\pi _ { i } ) = et { } { ' } \\sum _ { t = 0 } ^ { T } \\gamma ^ { t } r _ { i } ^ { t } } \\end{array}$ , where $\\gamma \\in ( 0 , 1 ]$ is the discount factor incorporating uncertainty about future returns, and where $r _ { i } ^ { t }$ is the individual reward received at time step $t$ .\n\n# 3.2 Abstract Observation Representation\n\nOur objective is to design a sample-efficient multi-agent actor-critic architecture that decomposes the observations based on spatial inductive biases. We achieve this by employing a form of state abstraction: we simplify the observation representation by dropping information that is not essential to the task. This can also be described as domain reduction where we collapse observations into equivalent clusters, causing some observations to be indistinguishable and ultimately reducing observation complexity (Zucker 2003).\n\nThe abstraction assumption we make is that the relative positioning of entities is relevant, not their absolute positions. We are inducing an equivalence between observations, where we group observations with a similar spatial structure. This induces a translation invariance that applies to both remote and local relations in the observation space. For this to hold, we assume that the relative spatial relations can be extracted from the observation. This type of spatial information is inherent in many common environments and realworld scenarios and offers an intuitive example of using existing structures in the observations.\n\nWe hypothesize that this abstraction is particularly fruitful in discrete domains. Discrete states can be clearly separated from each other, which makes it easy to exactly define boundaries for any spatial relations. In contrast, continuous state spaces have a higher state complexity, as they are infinite expressions of the state and small changes can have a significant impact on the optimal action. Hence, clustering continuous state spaces can lead to a stronger loss of information in the representation that could impact performance (Li, Walsh, and Littman 2006). Whilst these challenges may affect the effectiveness of our abstraction, we test the robustness of our approach on continuous domains as well.\n\nEffectively leveraging graph structures to impose such inductive bias in MARL poses the key challenges of (a) determining how entities are represented; (b) finding an informative yet efficient use of relations; (c) aggregating information across the graph to propagate relevant signals; (d) finding a computationally efficient way of incorporating the structural information into the MARL architecture. In the following, we address these challenges through specific design choices to create a structured, more compact observation representation that leverages the permutation invariance to the order of entities and the translation invariance to the absolute position between entities. An overview of the steps and our overall architecture can be seen in Figure 1.\n\nEntity Representation In many of the discussed MARL methods, the focus lies on the interaction between agents, where the information they share is usually an encoding of their individual observation and action (e.g. Iqbal and Sha 2019; Liu et al. 2020; Jiang et al. 2020; Zhang et al. 2021b). We want to emphasize the structure already present within the observation itself. Hence, we aim to find a structured representation of the observation that does not only consider the agents but also all the other objects in the environment.\n\nTypically, observations are given as fixed-sized vectors that contain the positions and attributes of agents and objects. This enforces an artificial ordering between the entities that is not desirable. The structure of such an observation is commonly a design choice and can be varied without great loss of generality. Consequently, we assume that the positions and attributes of the agents and environment objects can be extracted. In detail, we first construct an entity set $\\nu$ from all agents and objects. We then take the non-spatial information from all agents and objects, such as their level, type, if they are carrying objects, an identifier or any other attributes. This results in a corresponding entity feature matrix $Z \\in \\mathbb { R } ^ { d \\times | \\mathcal { V } | }$ with $d$ being the number of entity features.\n\n![](images/ecd0bc619b3176a96b18b2feb5b5a2040f7a0e5ee7ce4a99b377cf8857cb1a26.jpg)  \nFigure 1: Overview of our MARC architecture on the example of level-based foraging. Without adding information, the observation is constructed as a graph, with objects and agents as entities and our chosen set of relations. We then pass this relational graph into a shared R-GCN component, followed by an individual head for each agent to estimate the state-action value.\n\nSpatial Relations Our abstraction assumption is that only the relative spatial information is essential to solve the task. In line with that hypothesis, we do not consider the absolute position $( x , y )$ of an entity as an entity feature but transform this information into relative spatial edges between all entities. These edges are established using spatial predicates $r ( a , b ) $ condition, such as $l e f t ( a , b ) \\ \\bar {  } \\ \\bar { x } _ { a } < \\bar { x } _ { b }$ , which indicates that entity $a$ is to the left of entity b. Our chosen set of relations = left, right, top, bottom, adjacent, aligned are directed edges designed to balance sufficient expressiveness with computational efficiency. An evaluation of this selection along with the potential impact of additional relations, are discussed in Section 5. Since some entities, such as the agents, are dynamic, the spatial relations between entities change at every time step. Unlike in other methods (Nayak et al. 2023; Agarwal et al. 2020), the edges do not feature the distance between entities. This allows us to induce translation invariance, building a compact abstraction that treats observations with the same relative spatial structure as equivalent. We refer to the appendix for more details on the invariance of our abstraction.\n\nObservation Encoding Having established the structure of the relational graph, our next objective is to obtain a higher-level representation of the observation, informed by the spatial relation between the entities. For this, we employ R-GCN (Schlichtkrull et al. 2018) updates, chosen for the ability to handle multiple relationship types. It updates entity representations by evaluating the entities‚Äô individual features and aggregating information from connecting entities depending on their relation type.\n\nFormally, our graph is denoted as $\\mathcal { G } ~ = ~ ( \\nu , \\mathcal { E } , \\mathcal { R } , Z )$ , where $\\nu$ is the set of all entities, $\\mathcal { E }$ represents directed edges signifying relationships, $\\mathcal { R }$ categorizes the types of these relationships and $Z$ denotes the entity-feature matrix. The feature update for each entity $ { \\boldsymbol { v } } \\in  { \\mathcal { V } }$ , initially represented by $z _ { v } \\in \\mathbb { R } ^ { d }$ , is governed by $\\begin{array} { r l } { z _ { v } ^ { \\prime } } & { { } = } \\end{array}$ $\\begin{array} { r } { \\sigma \\left( \\sum _ { r \\in \\mathcal { R } } \\sum _ { u \\in \\mathcal { N } _ { r } ( v ) } \\frac { 1 } { | \\mathcal { N } _ { r } ( v ) | } W _ { r } z _ { u } + W _ { 0 } z _ { v } \\right) } \\end{array}$ , where $\\sigma$ is a non-linear activation function. Each relation type $r$ has an associated weight matrix $W _ { r } \\in \\mathbb { R } ^ { d ^ { \\prime } \\times d }$ , customizing the update to the specific nature of the relationship. An auxiliary weight matrix $W _ { 0 } \\in \\mathbb { R } ^ { d ^ { \\prime } \\times d }$ integrates the entity‚Äôs original features. The term $\\mathcal { N } _ { r } ( v )$ represents the neighboring entities of entity $v$ for a given relation type $r$ and the aggregation is normalized by $| \\bar { \\mathcal { N } } _ { r } ( v ) |$ .\n\nAfter applying a number of R-GCN layers as specified, we obtain an updated feature matrix $Z ^ { \\prime } \\in \\mathbf { \\bar { \\mathbb { R } } } ^ { d ^ { \\prime } \\times | \\nu | }$ . To generate a fixed-size representation, we apply a feature-wise max-pooling operation, resulting in observation encodings $e ( o _ { i } ) \\bar { = } \\operatorname* { m a x } \\mathrm { - p o o l } ( Z ^ { \\prime } )$ , where $Z ^ { \\prime }$ implicitly depends on the original observation $o _ { i }$ .\n\nThis entire process - from initial graph construction through to the final pooling operation - acts as a unified observation encoder that aligns with common MARL environments. It transforms individual agent observations into a compact, relational representation, emphasizing the understanding of entity relationships while excluding nonessential details. Reducing the details in the representation enhances computational efficiency without sacrificing essential information for decision-making.\n\nBy transforming the observation into a graph representation and employing R-GCN updates followed by maxpooling, we obtain observation encodings that are invariant to the order of the input elements. This is a very desirable property, as there is no natural ordering between the objects in an environment. Furthermore, by fully removing the absolute information on positions and distance between entities, we not only reduce the observation complexity but also induce a translation-invariant representation.\n\nLearning Algorithm Having encoded the observations into a relational graph, our next step involves feeding the relational representation into the MARL framework. In principle, the observation encoder is agnostic to the backbone MARL algorithm and we include a supporting experiment along with a discussion of this aspect in the appendix. Known for effectively balancing SOTA performance with scalability, we employ the popular centralized training with a decentralized execution regime. This framework enables agents to share information during training while maintaining individual decision-making during execution. To enhance scalability and efficiency even further, we depart from the common practice of feeding observation information from all agents into the critic architecture (e.g. Iqbal and Sha 2019; Nayak et al. 2023). Instead, the individual critic only receives the observation information from its own agent and exchanges information implicitly by collectively learning the parameters of the observation encoder, significantly reducing the input dimensionality to the critic. This shared module is complemented by individual heads for each agent, facilitating efficient learning while preserving the capacity for learning individualized behavior.\n\nIn detail, each agent, indexed by $i$ , maintains its own critic and policy, allowing for distinct reward structures and action spaces. Formally, the critic for each agent is defined as $Q _ { \\psi _ { i } } ( { \\bar { o } } _ { i } , a ) = f _ { i } ( e ( { \\bar { o } } _ { i } ) , a )$ , where $f _ { i }$ is a dense neural network that processes the encoded observation $e ( o _ { i } )$ together with the collective actions $a = ( a _ { 1 } , \\dotsc , a _ { N } ) $ . Here, $\\psi _ { i }$ includes the parameters from both the shared observation encoder $e$ and the agent-specific dense layers $f _ { i }$ . The critics are jointly optimized to minimize the following regression loss:\n\n$$\n\\mathcal { L } _ { Q } ( \\psi ) = \\sum _ { i = 1 } ^ { N } \\mathbb { E } _ { ( o _ { i } , a , r _ { i } , o _ { i } ^ { \\prime } ) \\sim D } \\left[ ( Q _ { \\psi _ { i } } ( o _ { i } , a ) - y _ { i } ) ^ { 2 } \\right] ,\n$$\n\nwhere the target is defined as $\\begin{array} { r l r l } { y _ { i } } & { { } } & { = } & { { } } & { r _ { i } \\mathrm { ~  ~ } + } \\end{array}$ $\\gamma \\mathbb { E } _ { a ^ { \\prime } \\sim \\pi _ { \\bar { \\theta } } } \\left[ Q _ { \\bar { \\psi } _ { \\frac { i } { 2 } } } ( o _ { i } ^ { \\prime } , a ^ { \\prime } ) - \\underline { { \\alpha } } \\log \\pi _ { \\bar { \\theta } _ { i } } ( a _ { i } ^ { \\prime } | o _ { i } ^ { \\prime } ) \\right]$ . Here, $\\gamma$ is the discount factor and $D$ represents the replay buffer. Following the soft actor-critic updates (Haarnoja et al. 2018), we define $\\bar { \\psi } _ { i }$ and ${ \\bar { \\theta } } _ { i }$ as the target critic and policy parameters for each agent, respectively, and $\\alpha$ as the temperature parameter balancing entropy and reward maximization. The joint target policy vector $\\pi _ { \\bar { \\theta } } = ( \\pi _ { \\bar { \\theta } _ { 1 } } , . . . , \\pi _ { \\bar { \\theta } _ { N } } )$ comprises policies, each a dense neural network. The individual policies are learned via gradient ascent as in the SAC (Haarnoja et al. 2018) framework and as described in (Iqbal and Sha 2019) without major modification. The implementation details and hyperparameters can be found in the appendix.\n\n# 4 Experiments\n\nIn this section, we detail the experimental setup designed to evaluate the performance and capabilities of our proposed algorithm. We first describe the environments chosen, which are tailored to challenge and showcase the algorithm‚Äôs spatial reasoning and collaborative capabilities in relationally complex tasks. We then outline the baseline algorithms against which we compare our approach to understand the added value of the relational inductive bias, followed by a comparative analysis of our results.\n\n# 4.1 Environments\n\nWe hypothesize that the introduced abstraction learns effectively in spatially complex coordination tasks with sparse rewards. On this basis, we designed a new highly collaborative environment that requires coordination between different types of agents and several object types. Furthermore, we select other collaborative grid environments as they naturally challenge the algorithm‚Äôs capabilities in spatial reasoning and cooperation under sparse rewards.\n\nWhilst the employed state abstraction is particularly well suited to the discrete nature of these environments, we also evaluate our method on a continuous domain. Following is an overview of the chosen environment and for further details, we refer to the appendix.\n\nCollaborative Pick and Place (CPP) is a new, collaborative environment with two types of agents that need to pick up and drop off a box at a designated goal, entailing heterogeneous, collaborative agents. Only the picker agents can collect a box whereas the delivery agents can only receive a box and drop it at at a goal location. Once a box is dropped at the goal location, no other box can be placed there. At the beginning of each episode, the boxes, agents and goals are randomly spawned on the grid. Depending on their role, agents receive a reward for successful pick-ups, passes, and drop-offs, as well as for prompt completion of the task. In our experiments, we test the challenging setting of a $1 0 \\times 1 0$ grid, 2 picker agents, 2 delivery agents and 3 objects1.\n\nLevel-based Foraging (LBF) (Christianos, Scha¬®fer, and Albrecht 2020) situates agents in a grid world where they are rewarded for collecting fruits. As opposed to the original LBF environment, we assume that fruits are on trees that remain on the grid after the fruit has been collected, with a value of $- 1$ . This alteration demands a higher relational reasoning capability from agents, as they must now navigate around the trees, recognizing them as noncollectable obstacles. For testing high cooperation, our experiments run on a $1 0 \\times 1 0$ grid with 4 agents and 4 foods, enforcing cooperation (denoted as $1 0 \\mathrm { x } 1 0 { - } 4 \\mathrm { a } { - } 4 \\mathrm { f - c o o p } )$ . To assess scalability, we extend the environment to a $1 5 \\times 1 5$ grid with 8 agents and 1 fruit (denoted as 15x15-8a-1f-coop).\n\nIn Wolfpack (Rahman et al. 2023), 3 agents are placed in a $1 0 \\times 1 0$ grid to capture 2 prey. In a departure from the original setup, we have introduced sparse rewards by removing additional rewards based on the proximity to prey, significantly weakening the learning signal.\n\nThe Target task, based on the multi-agent particle environment (Lowe et al. 2017) and modified by Nayak et al. (2023), is a continuous domain environment where a number of agents try to reach their target landmarks while avoiding collision with obstacles and other agents.\n\n# 4.2 Baselines\n\nIn our study, we choose the baselines based on the following criteria: performance, reproducibility, ability to handle discrete action spaces and similarity to our approach. Following is an overview of the selected baselines; implementation details and hyper-parameter selection can be found in the appendix.\n\nMAAC (Iqbal and Sha 2019) also uses SAC as the base RL algorithm. The use of attention between agents represents a different form of relational inductive bias on agent interaction rather than our object-centric representation.\n\nGA-AC is the AC algorithm that makes use of the G2ANet mechanism (Liu et al. 2020). It builds on MAAC with an additional hard attention layer, which allows for an even more nuanced differentiation of information from other agents and represents an even stronger inductive bias than MAAC.\n\nInforMARL (Nayak et al. 2023) introduces a distancebased graph representation of objects and agents that informs policy and critic networks, yielding a similarly structured observation encoding. Tailored to the multi-agent particle environment, it provides a relevant baseline for our experiments in the continuous domain.\n\nQMIX (Rashid et al. 2018) leverages the structural assumption of conditional independence between agents‚Äô value functions to factorize it, yielding a rigorously implemented and strong baseline for comparison.\n\nMAA2C (Papoudakis et al. 2020) is an on-policy approach that learns a centralized critic from joint observations without other agents‚Äô actions. It serves as a fast and strong baseline due to its absence of relational inductive bias, meaning it does not explicitly consider relationships between agents or entities in the environment.\n\nMAPPO (Yu et al. 2022) is an extension of single-agent PPO (Schulman et al. 2017), noted for its performance and, similar to MAA2C, does not incorporate a relational inductive bias. It enhances sample efficiency through multiple updates on batches of training data.\n\n# 4.3 Asymptotic Performance and Sample Efficiency in Discrete Domains\n\nIn this section, we present a comparative analysis of the asymptotic performance and sample efficiency as illustrated in Figure 2. Asymptotically, MARC is competitive and outperforms all baselines across the implemented tasks. Additionally, MARC demonstrates superior sample efficiency, learning all the tasks the fastest. In the LBF-15x15-8a-1fcoop task, MARC reaches an average performance of $9 9 \\%$ after 5.9e5 environment steps, whereas the second-best algorithm, MAAC, takes 7.3 times the number of steps to reach the same performance.\n\nThe most significant margins in asymptotic performance are achieved in CPP and LBF-10x10-4a-4f-coop, where MARC achieves a performance gain of $6 9 . 9 \\%$ and $3 5 . 2 \\%$ respectively, as displayed in Figure 2a and Figure 2b. They require a high level of coordination and spatial understanding between entities to succeed in the task. In the LBF- $1 0 \\mathrm { x } 1 0 \\cdot$ - 4a-4f-coop setting, MARC reaches $26 \\%$ of the maximum returns, on average, in 1e6 steps, while the second-best algorithm, MAPPO, reaches the same performance in 5.6 times the number of steps. MAAC performs relatively well in tasks that highly depend on coordination between agents, such as LBF-15x15-8a-1f-coop and Wolfpack, as visualized in Figure 2c and Figure 2d, respectively. However, MAAC‚Äôs performance deteriorates in CPP and LBF-10x10-4a-4f-coop, where information about objects is essential to gain a good understanding of the environment. GA-AC and MAAC do not have a significant performance difference, indicating that the additional hard-attention layer on the agent interactions does not dramatically impact performance in spatially demanding tasks involving reasoning over environment objects. MAA2C and MAPPO perform reasonably well in LBF and Wolfpack. As both share the critic and policy network across agents, one hypothesis is that this proves beneficial in highly collaborative and homogeneous tasks.\n\nTable 1: Asymptotic performance and standard deviation for the Target task, averaged across 3 seeds.   \n\n<html><body><table><tr><td>Algorithm</td><td>3 Agents (2 √ó 10 steps)</td><td>7 Agents (4 √ó 10 steps)</td></tr><tr><td>MARC</td><td>212.7 ¬± 5.7</td><td>468.2 ¬± 4.2</td></tr><tr><td>InforMARL</td><td>193.5 ¬± 4.3</td><td>426.1 ¬±81.2</td></tr><tr><td>MAAC</td><td>236.1 ¬± 2.9</td><td>527.9 ¬± 5.4</td></tr><tr><td>GA-AC</td><td>236.6¬±3.5</td><td>530.6¬±3.4</td></tr><tr><td>MAA2C</td><td>233.5 ¬± 2.1</td><td>68.8¬± 393.4</td></tr><tr><td>MAPPO</td><td>109.0¬±16.2</td><td>304.7¬±6.0</td></tr><tr><td>QMIX</td><td>21.5 ¬±14.8</td><td>-90.0¬± 79.6</td></tr></table></body></html>\n\nOverall, the comparative analysis demonstrates the effectiveness of MARC in achieving superior asymptotic performance and sample efficiency in the selected multi-agent environments. The spatial inductive bias introduced in MARC proves to be beneficial in understanding the relationships between agents and objects, leading to faster learning and better asymptotic performance compared to the baselines.\n\n# 4.4 Generalisation Performance\n\nTo assess the ability of our method to generalize to outof-distribution settings, we evaluate our model trained on the most difficult scenario of LBF, 10x10-4a-4f-coop, where MARC achieves $8 1 \\%$ of the maximum performance, on a varying number of agents and fruits. We then compare our algorithm by training the best-performing algorithm on this task, MAPPO, with the same varied number of fruits and agents. When reducing the number of agents available to collect fruits to 3, MARC still achieves $3 8 \\%$ of the performance, whilst MAPPO‚Äôs performance fully deteriorates to $0 \\%$ . Increasing the number of agents by 1 makes the task easier and yields an improved performance of $9 3 \\%$ vs. $8 8 \\%$ for MAPPO. This indicates that MARC learns an invariance to the number of agents. The performance decreases to $59 \\%$ with an increase in fruits (from 4 to 6 fruits), but given that the number of environment steps remains fixed it generally becomes more difficult to fulfill in time and can still be considered robust. In comparison, MAPPO‚Äôs performance decreases by $40 \\%$ down to $1 9 \\%$ . An overview of all generalization results can be found in the appendix.\n\n# 4.5 Extension to Continuous Domain\n\nAs seen in Table 1, MARC performs stronger than the SOTA graph-based algorithm InforMARL, underlining the strength of our graph design also in continuous domains. It is also competitive with the best-performing baselines, MAAC, GA-AC and MAA2C. Deeper analysis shows that the performance margin comes from MARC taking, on average, 1-2 steps longer to reach the target. There is a trade-off in abstraction between a simple and efficient abstraction and removing too much information. For example, as environment objects also have velocity in the target task, the agents ideally have a more fine-grained understanding of the proximity to other objects, rather than just knowing once they are adjacent. This could lead to collisions that cannot be avoided or initially going passed their target due to their accumulated velocity. Further experiments, which can be found in the appendix, have confirmed this trade-off, indicating that a coarser abstraction yields improvements in sample efficiency with a decrease in asymptotic performance. Nevertheless, our algorithm demonstrates competitive and robust performance even for the continuous case.\n\n![](images/f049757e8a4b07019bb4d27ae73c3981bcb6925ea9c4faaf82ee73da479117fd.jpg)  \nFigure 2: Mean average performance and $9 5 \\%$ confidence interval for all discrete tasks. For each model, we run 3 random seeds.\n\n# 5 Ablation Studies\n\nGiven the immense flexibility of graph architectures, we aim to shed light on how different design choices affect performance by systematically varying the following aspects:\n\nChoice of Relations To understand how the choice of relations impacts performance, we evaluate our experiments with 3 different groups: our default relations, local relations representing a convolutional kernel, and all relations as the union of the two, detailed in the appendix. We found that purely local relations are not sufficient to learn the task, achieving only $1 0 . 6 \\%$ of the performance achieved by the chosen architecture. This seems intuitive, as the agent gains a deeper spatial understanding if they can infer information from all entities, even if they are further away. Additionally, adding local relations to our default set does not elevate the performance, indicating that our default relations offer a sufficient and strong enough spatial bias.\n\nNumber of Entities We compared our approach of considering only agents and objects in the graph to using all grid elements as entities. Learning over the full grid compared to our choice of compact representation is, despite being more informative, not as sample efficient, reaching only $4 6 . 9 \\%$ of the performance achieved by the chosen architecture in 8e6 environment steps, along with a higher computational cost.\n\nChoice of Graph Architecture We explore alternative choices of aggregating information from connecting entities in the graph. Whilst the choice is vast, we focus on previous work in the single-agent literature, where relational inductive bias between the entities is introduced via multi-head attention (Zambaldi et al. 2019). For this, we construct a binary graph and pass it through a Graph Attention Network (GAT) VelicÀákovic¬¥ et al. (2018). Furthermore, we combine the approaches of spatial relations and varying importance between entities as in GATs by using an R-GAT layer (Busbridge et al. 2019) on the graph constructed in Section 3. For a detailed display of these alternative implementations, we refer to the appendix. Our R-GAT and R-GCN implementation yield indistinguishable performance, indicating that implicitly specifying different importance between entities does not yield a more expressive representation and its computation is therefore not required. In contrast, the use of a GAT layer yields suboptimal performance, asymptotically reaching only $23 . 4 \\%$ of the chosen architecture‚Äôs performance. The non-spatial, weighted interactions among entities might not serve as a robust inductive bias to effectively reason about the inherent structure of the task.\n\n# 6 Conclusion and Future Work\n\nIn this work, we presented a relational state abstraction approach for MARL and demonstrated its effectiveness in environments requiring spatial reasoning and coordination among agents. By incorporating spatial inductive biases into our abstraction, we achieved significant improvements in sample efficiency and asymptotic performance compared to SOTA MARL algorithms. Our findings provide strong evidence for the potential of leveraging relational inductive biases to address the challenges of sample efficiency and generalization in MARL.\n\nTo further enhance our method, future research could explore the incorporation of inductive biases beyond spatial reasoning, an even stronger incorporation of structured representations, for example into the policy network as well, and the fine-tuning to more complex, high-dimensional environments. Investigating the interpretability and transparency of the structured representation could also facilitate the deployment into real-world scenarios.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂú®Âçè‰ΩúÂ§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†ÔºàMARLÔºâ‰∏≠ÔºåÂÖ≥Á≥ªÁä∂ÊÄÅÊäΩË±°ÂØπÊ†∑Êú¨ÊïàÁéáÂíåÊÄßËÉΩÁöÑÂΩ±Âìç„ÄÇÊ†∏ÂøÉÈóÆÈ¢òÊòØÔºöÂ¶Ç‰ΩïÂú®Á¶ÅÊ≠¢Áõ¥Êé•ÈÄö‰ø°ÁöÑÁéØÂ¢É‰∏≠ÔºåÈÄöËøáÁ©∫Èó¥ÂÖ≥Á≥ªÊäΩË±°ÊèêÂçáMARLÁöÑÊ†∑Êú¨ÊïàÁéáÂíåÊ∏êËøõÊÄßËÉΩ„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éÔºåÁé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑÂ§öÊô∫ËÉΩ‰ΩìÂú∫ÊôØÔºàÂ¶ÇËá™Âä®È©æÈ©∂ÂçèË∞É„ÄÅÂõ¢ÈòüÊú∫Âô®‰∫∫ÔºâÈÄöÂ∏∏Ê∂âÂèäÂ§çÊùÇÁöÑÁ©∫Èó¥‰∫§‰∫íÔºåËÄåÁé∞ÊúâÊñπÊ≥ïÂú®È´òÁª¥Áä∂ÊÄÅÂíåÂä®‰ΩúÁ©∫Èó¥‰∏≠Ê†∑Êú¨ÊïàÁéá‰Ωé‰∏ã„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ÊèêÂá∫MARCÔºàÂ§öÊô∫ËÉΩ‰ΩìÂÖ≥Á≥ªËØÑËÆ∫ÂÆ∂ÔºâÔºå‰∏ÄÁßçÂü∫‰∫éÁ©∫Èó¥ÂõæÁöÑÂÖ≥Á≥ªÊäΩË±°Êû∂ÊûÑÔºåÈÄöËøáÂ∞ÜÁä∂ÊÄÅËΩ¨Êç¢‰∏∫Á©∫Èó¥ÂõæÂπ∂Âà©Áî®ÂÖ≥Á≥ªÂõæÁ•ûÁªèÁΩëÁªúÔºàR-GCNÔºâÂ§ÑÁêÜÔºåÂºïÂÖ•Á©∫Èó¥ÂÖ≥Á≥ªÂΩíÁ∫≥ÂÅèÁΩÆ„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ1Ôºö** ÊèêÂá∫‰∏ÄÁßçËΩªÈáèÁ∫ßÁ©∫Èó¥ÂÖ≥Á≥ªÊäΩË±°ÊñπÊ≥ïÔºåÂú®ÂÖ≠È°πÂçè‰Ωú‰ªªÂä°‰∏≠È™åËØÅÂÖ∂ÊúâÊïàÊÄß„ÄÇ\\n>   *   **ÂÖ≥ÈîÆÊï∞ÊçÆÔºö** Âú®LBF-15x15-8a-1f-coop‰ªªÂä°‰∏≠ÔºåMARC‰ªÖÈúÄ5.9e5Ê≠•ËææÂà∞99%ÊÄßËÉΩÔºåËÄåÊ¨°‰ºòÊñπÊ≥ïMAACÈúÄ7.3ÂÄçÊ≠•Êï∞„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ2Ôºö** ËÆæËÆ°ÂºÇÊûÑÊô∫ËÉΩ‰ΩìÂçè‰ΩúÁöÑÊñ∞ÁéØÂ¢ÉCPPÔºåÈ™åËØÅMARCÂú®Â§çÊùÇÁ©∫Èó¥‰ªªÂä°‰∏≠ÁöÑ‰ºòÂäø„ÄÇ\\n>   *   **ÂÖ≥ÈîÆÊï∞ÊçÆÔºö** Âú®CPP‰ªªÂä°‰∏≠ÔºåMARCÊÄßËÉΩÊèêÂçá69.9%„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ3Ôºö** ËØÅÊòéÁ©∫Èó¥ÂÖ≥Á≥ªÊäΩË±°Âú®ËøûÁª≠Âüü‰∏≠ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ\\n>   *   **ÂÖ≥ÈîÆÊï∞ÊçÆÔºö** Âú®Target‰ªªÂä°‰∏≠ÔºåMARCÊÄßËÉΩÔºà212.7¬±5.7Ôºâ‰ºò‰∫éÂõæÂü∫Á∫øInforMARLÔºà193.5¬±4.3Ôºâ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   Ê†∏ÂøÉÂéüÁêÜÊòØÈÄöËøáÁ©∫Èó¥ÂÖ≥Á≥ªÊäΩË±°ÔºàÂ¶ÇÂ∑¶Âè≥„ÄÅÁõ∏ÈÇªÁ≠âË∞ìËØçÔºâÊûÑÂª∫ÂõæË°®Á§∫ÔºåÂà©Áî®R-GCNËÅöÂêà‰ø°ÊÅØÔºå‰øùÁïô‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÁ©∫Èó¥ÁªìÊûÑËÄåÂøΩÁï•ÁªùÂØπ‰ΩçÁΩÆ‰ø°ÊÅØ„ÄÇ\\n> *   ËÆæËÆ°Âì≤Â≠¶ÊòØÔºöÁ©∫Èó¥ÂÖ≥Á≥ªÂú®Áâ©ÁêÜÁéØÂ¢É‰∏≠ÊôÆÈÅçÂ≠òÂú®Ôºå‰∏îÊäΩË±°Ë°®Á§∫ËÉΩÊèêÂçáÂØπÂô™Â£∞ÁöÑÈ≤ÅÊ£íÊÄßÂíåÊ†∑Êú¨ÊïàÁéá„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **ÂÖàÂâçÂ∑•‰ΩúÂ±ÄÈôêÔºö** Áé∞ÊúâÊñπÊ≥ïÔºàÂ¶ÇMAACÔºâ‰æßÈáçÊô∫ËÉΩ‰ΩìÈó¥‰∫§‰∫íÔºåÂøΩÁï•ÁéØÂ¢ÉÂØπË±°ÂÖ≥Á≥ªÔºõÂÖ∂‰ªñÂõæÊñπÊ≥ïÔºàÂ¶ÇInforMARLÔºâ‰æùËµñË∑ùÁ¶ª‰ø°ÊÅØÔºåÁº∫‰πèÂπ≥Áßª‰∏çÂèòÊÄß„ÄÇ\\n> *   **Êú¨ÊñáÊîπËøõÔºö** \\n>     1.  ‰ªÖÂ∞ÜÊô∫ËÉΩ‰ΩìÂíåÁéØÂ¢ÉÂØπË±°‰Ωú‰∏∫ÂõæËäÇÁÇπÔºåÂáèÂ∞ëËÆ°ÁÆóÂ§çÊùÇÂ∫¶Ôºõ\\n>     2.  ‰ΩøÁî®Á©∫Èó¥Ë∞ìËØçÔºàÈùûË∑ùÁ¶ªÔºâÂª∫Á´ãËæπÔºåÂÆûÁé∞Âπ≥Áßª‰∏çÂèòÊÄßÔºõ\\n>     3.  ÂÖ±‰∫´R-GCNÁºñÁ†ÅÂô®‰∏éÁã¨Á´ãËØÑËÆ∫ÂÆ∂Â§¥ÁªìÂêàÔºåÂπ≥Ë°°ÊïàÁéá‰∏é‰∏™ÊÄßÂåñ„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.  **ÂÆû‰ΩìË°®Á§∫Ôºö** ‰ªéËßÇÂØü‰∏≠ÊèêÂèñÊô∫ËÉΩ‰ΩìÂíåÂØπË±°ÁöÑÈùûÁ©∫Èó¥ÁâπÂæÅÔºåÊûÑÂª∫ÂÆû‰ΩìÁâπÂæÅÁü©ÈòµZ‚àà‚Ñù^(d√ó|ŒΩ|)„ÄÇ\\n> 2.  **Á©∫Èó¥ÂÖ≥Á≥ªÊûÑÂª∫Ôºö** ‰ΩøÁî®6ÁßçÁ©∫Èó¥Ë∞ìËØçÔºàÂ¶Çleft(a,b)ÔºâÂª∫Á´ãÊúâÂêëËæπÔºåÂΩ¢ÊàêÂõæG=(ŒΩ,Œµ,R,Z)„ÄÇ\\n> 3.  **R-GCNÁºñÁ†ÅÔºö** ÈÄöËøáÂÖ¨Âºèz_v^'=œÉ(‚àë_(r‚ààR)‚àë_(u‚ààN_r(v))1/|N_r(v)| W_r z_u + W_0 z_v)Êõ¥Êñ∞ËäÇÁÇπÁâπÂæÅ„ÄÇ\\n> 4.  **ÊúÄÂ§ßÊ±†ÂåñÔºö** ÂØπÁºñÁ†ÅÂêéÁöÑÁâπÂæÅÁü©ÈòµZ'‚àà‚Ñù^(d'√ó|ŒΩ|)ÊâßË°åmax-poolingÔºåÁîüÊàêÂõ∫ÂÆöÂ∞∫ÂØ∏ËßÇÂØüÁºñÁ†Åe(o_i)„ÄÇ\\n> 5.  **Á≠ñÁï•Â≠¶‰π†Ôºö** Âü∫‰∫éSACÊ°ÜÊû∂ÔºåÊØè‰∏™Êô∫ËÉΩ‰ΩìÁöÑËØÑËÆ∫ÂÆ∂Q_œà_i(o_i,a)=f_i(e(o_i),a)ËÅîÂêà‰ºòÂåñÂõûÂΩíÊçüÂ§±L_Q(œà)„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   MAACÔºàÂü∫‰∫éÊ≥®ÊÑèÂäõÊú∫Âà∂ÁöÑMARLÔºâ\\n> *   GA-ACÔºàÂ∏¶Á°¨Ê≥®ÊÑèÂäõÁöÑG2ANetÔºâ\\n> *   InforMARLÔºàË∑ùÁ¶ªÂõæË°®Á§∫ÁöÑMARLÔºâ\\n> *   QMIXÔºàÂÄºÂàÜËß£ÊñπÊ≥ïÔºâ\\n> *   MAA2CÔºàÊó†ÂÖ≥Á≥ªÂÅèÁΩÆÁöÑÈõÜ‰∏≠ÂºèËØÑËÆ∫ÂÆ∂Ôºâ\\n> *   MAPPOÔºàPPOÁöÑÂ§öÊô∫ËÉΩ‰ΩìÊâ©Â±ïÔºâ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®Ê∏êËøõÊÄßËÉΩ‰∏äÔºö** Âú®CPP‰ªªÂä°‰∏≠ÔºåMARCÊÄßËÉΩÊèêÂçá69.9%ÔºåÊòæËëó‰ºò‰∫éMAACÂíåGA-ACÔºàÂêéËÄÖ‰ªÖÊèêÂçá<5%Ôºâ„ÄÇÂú®LBF-10x10-4a-4f-coop‰ªªÂä°‰∏≠ÔºåMARCÊèêÂçá35.2%„ÄÇ\\n> *   **Âú®Ê†∑Êú¨ÊïàÁéá‰∏äÔºö** Âú®LBF-15x15-8a-1f-coop‰ªªÂä°‰∏≠ÔºåMARC‰ªÖÈúÄ5.9e5Ê≠•ËææÂà∞99%ÊÄßËÉΩÔºåËÄåMAACÈúÄ7.3ÂÄçÊ≠•Êï∞„ÄÇÂú®LBF-10x10-4a-4f-coop‰∏≠ÔºåMARCËææÂà∞26%ÊÄßËÉΩÈúÄ1e6Ê≠•ÔºåMAPPOÈúÄ5.6ÂÄçÊ≠•Êï∞„ÄÇ\\n> *   **Âú®Ê≥õÂåñÊÄß‰∏äÔºö** ÂΩìLBF‰ªªÂä°Êô∫ËÉΩ‰Ωì‰ªé4ÂáèËá≥3Êó∂ÔºåMARC‰øùÊåÅ38%ÊÄßËÉΩÔºåËÄåMAPPOÈôçËá≥0%ÔºõÊô∫ËÉΩ‰ΩìÂ¢ûËá≥5Êó∂ÔºåMARCÊèêÂçáËá≥93%ÔºàMAPPO‰∏∫88%Ôºâ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   Â§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π† (Multi-Agent Reinforcement Learning, MARL)\\n*   ÂÖ≥Á≥ªÁä∂ÊÄÅÊäΩË±° (Relational State Abstraction, N/A)\\n*   ÂõæÁ•ûÁªèÁΩëÁªú (Graph Neural Network, GNN)\\n*   Á©∫Èó¥ÂÖ≥Á≥ªÂΩíÁ∫≥ÂÅèÁΩÆ (Spatial Relational Inductive Bias, N/A)\\n*   Ê†∑Êú¨ÊïàÁéá (Sample Efficiency, N/A)\\n*   Âçè‰Ωú‰ªªÂä° (Collaborative Tasks, N/A)\\n*   ÂºÇÊûÑÊô∫ËÉΩ‰Ωì (Heterogeneous Agents, N/A)\\n*   Âπ≥Áßª‰∏çÂèòÊÄß (Translation Invariance, N/A)\"\n}\n```"
}