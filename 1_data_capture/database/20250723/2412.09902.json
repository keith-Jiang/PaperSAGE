{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.09902",
    "link": "https://arxiv.org/abs/2412.09902",
    "pdf_link": "https://arxiv.org/pdf/2412.09902.pdf",
    "title": "One Node One Model: Featuring the Missing-Half for Graph Clustering",
    "authors": [
        "Xuanting Xie",
        "Bingheng Li",
        "Erlin Pan",
        "Zhaochen Guo",
        "Zhao Kang",
        "Wenyu Chen"
    ],
    "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.SI"
    ],
    "publication_date": "2024-12-13",
    "venue": "arXiv.org",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 5,
    "influential_citation_count": 0,
    "institutions": [
        "University of Electronic Science and Technology of China",
        "Michigan State University",
        "Alibaba Group"
    ],
    "paper_content": "# One Node One Model: Featuring the Missing-Half for Graph Clustering\n\nXuanting $\\mathbf { X _ { i } ^ { \\bullet } } ^ { 1 }$ , Bingheng $\\mathbf { L i } ^ { 2 }$ , Erlin $\\mathbf { P a n } ^ { 3 }$ , Zhaochen Guo1, Zhao Kang1\\*, Wenyu Chen1\n\n1University of Electronic Science and Technology of China, Chengdu, Sichuan, China 2Michigan State University, East Lansing, US 3Alibaba Group x624361380@outlook.com, libinghe $@$ msu.edu, panerlin.pel $@$ alibaba-inc.com, zkang, cwy $@$ uestc.edu.cn\n\n# Abstract\n\nMost existing graph clustering methods primarily focus on exploiting topological structure, often neglecting the “missinghalf” node feature information, especially how these features can enhance clustering performance. This issue is further compounded by the challenges associated with high-dimensional features. Feature selection in graph clustering is particularly difficult because it requires simultaneously discovering clusters and identifying the relevant features for these clusters. To address this gap, we introduce a novel paradigm called “one node one model”, which builds an exclusive model for each node and defines the node label as a combination of predictions for node groups. Specifically, the proposed “Feature Personalized Graph Clustering (FPGC)” method identifies cluster-relevant features for each node using a squeeze-andexcitation block, integrating these features into each model to form the final representations. Additionally, the concept of feature cross is developed as a data augmentation technique to learn low-order feature interactions. Extensive experimental results demonstrate that FPGC outperforms state-of-the-art clustering methods. Moreover, the plug-and-play nature of our method provides a versatile solution to enhance GNN-based models from a feature perspective.\n\n# Introduction\n\nAs a fundamental task in graph data mining, attributed graph clustering aims to partition nodes into different clusters without labeled data. It is receiving increasing research attention due to the success of Graph Neural Networks (GNNs) (Welling and Kipf 2017; Li, Pan, and Kang 2024; Qian, Li, and Kang 2024). Typically, GNNs use many graph convolutional layers to learn node representations by aggregating neighbor node features into the node. In recent years, many graph clustering techniques have achieved promising performance (Pan and Kang 2023; Liu et al. 2023; Kang et al. 2024; Shen, Wang, and Kang 2024).\n\nMost of these methods apply a self-supervised learning framework to fully explore the graph structure (Zhu et al. 2024; Yang et al. 2024), often neglecting the “missing-half” of node feature information, i.e., how node features can enhance clustering quality. Consequently, their performance heavily depends on the quality of the input graph. If the original graph is of poor quality, such as having many nodes from different classes connected together and missing important links, the representations learned through multilayer aggregation become nondiscrimination. This issue is known as representation collapse (Chen, Wang, and Li 2024).\n\nFor the attributed graph, the node feature and the topological structure should play an equal role in the unsupervised graph embedding and downstream clustering. We argue that a node’s personalized features should be crucial for identifying its cluster label. For example, the Cora dataset contains 2,708 papers, each described by a 1,433-dimensional binary feature indicating the presence of keywords. We visualize the feature distributions for each cluster in Fig. 1(a). It can be seen that each cluster exhibits its own dominant features, while other features may be irrelevant or less important. The cluster-relevant features are the collection of all personalized features. To distinguish between clusters, we consider features that are 20 times greater than the mean value in each cluster and apply the dynamic time warping (DTW) technique (Mu¨ller 2007) to each cluster. DTW is a similarity measure for sequences of different lengths, and we set the maximum distance to 5,000 for better visualization. From Fig. 1(b), we can see that there are significant differences in the cluster-relevant features. Thus, these personalized features are representative of different clusters.\n\nFurthermore, most GNN-based methods capture only higher-order feature interactions and tend to overlook loworder interactions (Kim, Choi, and Kim 2024). The graph filtering process transforms input features into higher-level hidden representations. Recent studies on GNNs focus mainly on designing node aggregation mechanisms that emphasize various connection properties, such as local similarity (Welling and Kipf 2017), structural similarity (Donnat et al. 2018), and multi-hop connectivity (Li et al. 2022). However, the cross feature information is always ignored, despite its importance for model expressiveness (Feng et al. 2021). For example, by crossing a paper’s keywords in the Cora dataset, such as $\\{ t i t l e = a u t o n o m o u s$ & approach ${ } = Q$ − learning & $e x p e r i m e n t = s i m u l a t i o n \\}$ , the model can achieve a better paper representation and produce more accurate clustering results (e.g., this paper is categorized under Reinforcement Learning). Until recently, most existing clustering models have struggled to capture such low-order feature interactions.\n\n![](images/e3566c678ca49ddbc5e9c660751b190ab3254b9bd595253cdd1f23afa93f849e.jpg)  \nFigure 1: Visualization of results on Cora. (a) is the feature distribution of different clusters, which shows that clusters are characterized by different features. (b) is the DTW distance matrix based on cluster-relevant features, which verify their distinctiveness. We can draw the conclusion that the cluster-relevant features contain valuable information about clusters.\n\n![](images/14f67f10a194567df6aa9775ef8a3622c5354d75d5333234dd826c6f511855c6.jpg)  \n(b) DTW distance matrix with only cluster-relevant features on Cora.\n\nTo address these shortcomings, we introduce “Feature Personalized Graph Clustering (FPGC)”. We propose a novel paradigm “one node one model” to learn a personalized model for each node, with a squeeze-and-excitation block selecting cluster-relevant features. A new data augmentation technique based on feature cross is developed to effectively capture low-order feature interactions. In summary, our contributions are as follows.\n\n• Orthogonal to existing works, we tackle graph clustering from a feature perspective. By employing a squeezeand-excitation block, we effectively select cluster-relevant features and propose the “one node one model” paradigm. • We develop a novel data augmentation technique based on feature cross to capture low-order feature interactions. • Extensive experiments on benchmark datasets demonstrate the superiority of our method. Notably, our approach can function as a plug-and-play tool for existing GNNbased models, rather than serving solely as a stand-alone method.\n\n# Related Work\n\nGraph clustering can be roughly divided into three categories (Xie et al. 2024; Li et al. 2024). (1) Shallow methods. FGC (Kang et al. 2022) incorporates higher-order information into graph structure learning for clustering. MCGC (Pan and Kang 2021) uses simplified graph filtering rather than GNNs to obtain a smooth representation. (2) Contrastive methods. SCGC (Liu et al. 2023) simplifies network architecture and data augmentation. CCGC (Yang et al. 2023a) leverages highconfidence clustering information to improve sample quality. CONVERT (Yang et al. 2023b) uses a perturb-recover network and semantic loss for reliable data augmentation. (3) Autoencoder methods. SDCN (Bo et al. 2020) transfers learned representations from auto-encoders to GNN layers, employing data structure information. AGE (Cui et al. 2020) uses Laplacian smoothing filters and adaptive learning to improve node embeddings. DGCN (Pan and Kang 2023) uses an adaptive filter to capture important frequency information and reconstructs heterophilic and homophilic graphs to handle real-world graphs with different levels of homophily. DMGNC (Yang et al. 2024) uses a masked autoencoder for node feature reconstruction. However, their training is highly dependent on the input graph. Consequently, if the original graph is poor quality, the learned representation through multilayer aggregation becomes indiscriminate.\n\nTo improve the discriminability of representation, some recent methods learn node-specific information by considering the distinctness of each node (Zhu et al. 2024; Dai et al. 2021). NDLS (Zhang et al. 2021) assigns different filtering orders to each node based on its influence score. DyFSS (Zhu et al. 2024) learns multiple self-supervised learning task weights derived from a gating network considering the difference in the node neighbors. In an orthogonal way, in this paper, we improve the representation learning from a feature perspective. We perform an in-depth analysis of how the “one node one model” paradigm benefits clustering.\n\n# Methodology\n\n# Notation\n\nDefine the graph data as $\\mathcal { G } = \\{ \\mathcal { V } , E , X \\}$ , where $\\nu$ represents a set of $N$ nodes and $e _ { i j } \\in E$ denotes the edge between node $i$ and node $j$ . $X = \\{ \\bar { X } _ { 1 } , . . . , X _ { N } \\} ^ { \\top } \\in \\mathbb { R } ^ { N \\times d }$ is the feature matrix with $d$ dimensions, $X _ { i }$ and $X _ { \\cdot j }$ indicate the $i$ -th row and the $j$ -th column and of $X$ , respectively. Adjacency matrix A ∈ RN×N represents the graph structure. D represents the de gree matrix. The normalized adjacency matrix is $A =$ D− 21 (A + I)D− 12 , and the corresponding graph Laplacian is $L = I - A$ .\n\n![](images/c489640ee9f5e10603d1a94944eb2dcd4189103e53ed9037846fa8958d528a48.jpg)  \nFigure 2: The pipeline of our FPGC. We preprocess the node features by stacked graph filters. Besides, we also input original features into the squeeze-and-excitation block to select the top $n$ significant features, based on which we learn a model for each node. Then, the contrastive framework encodes the smoothed node features and augmented features to achieve discriminative node representations.\n\n# The Pipeline of FPGC\n\nThe FPGC framework is shown in Fig. 2. Our design comprises two critical components. The first component concretizes “one node one model” with GNNs, with a squeezeand-excitation block selecting cluster-relevant features. The second is the contrastive learning framework with a new data augmentation technique based on feature cross.\n\n# Squeeze-and-Excitation Block\n\nCluster-relevant features characterize the clusters. We design a squeeze-and-excitation block to select them.\n\nSqueeze: The squeeze operation compresses the node features from $X \\in \\mathbb { R } ^ { \\hat { N } \\times d }$ to $\\overset { \\cdot } { q } \\in \\mathbb { R } ^ { 1 \\times d }$ . Specifically, it computes the summation of the features as follows:\n\n$$\n\\boldsymbol { q } = \\boldsymbol { F _ { s q } } \\left( \\boldsymbol { X } \\right) = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\boldsymbol { X _ { i } } ,\n$$\n\nwhere $q$ serves as a channel-wise statistic that captures global feature information.\n\nExcitation: The excitation operation follows the squeeze step and aims to capture dependencies among all channels. It is a simple gating mechanism that uses the sigmoid function $\\sigma ( \\cdot )$ to activate the squeezed feature map, enabling the learning of nonlinear connections between channels. Firstly, the dimension is reduced using a multi-layer perceptron (MLP) $W _ { 1 }$ . Next, a ReLU function $\\delta ( \\cdot )$ and another MLP $W _ { 2 }$ are used to increase the dimensionality, returning it to the original dimension $d$ . Finally, the sigmoid function is applied. The process is summarized as follows:\n\n$$\n\\tilde { q } = F _ { e x } \\left( q \\right) = \\sigma \\left( W _ { 2 } \\delta \\left( W _ { 1 } q \\right) \\right) .\n$$\n\nSelection: The outcome of the excitation operation is considered to be the significance of each feature. Then, we define\n\nthe function $F _ { t o p }$ to select top $n$ important features according to $\\tilde { q }$ . This operation is defined as follows:\n\n$$\nX ^ { n } = F _ { t o p } \\left( \\tilde { q } X \\right) ,\n$$\n\nwhere $X ^ { n } \\in \\mathbb { R } ^ { N \\times n }$ is the node feature matrix containing the top $n$ significant features.\n\n# One Node One Model\n\nIdeally, an exclusive model should be constructed for each individual node, which is the core of our method:\n\n$$\nY _ { i } = f ^ { ( i ) } ( X _ { i } ) ,\n$$\n\nwhere $Y$ and $f$ are the predicted label and the model. In this paradigm, personalization is fully maintained in the models. Unfortunately, the enormous number of nodes makes this scheme unfeasible. One possible solution is establishing several individual models for each type of cluster (the nodes in a group share personalized features). Each node can be considered as a combination of different clusters. For example, in a social network, a person is a member of both a mathematics group and several sports interest groups. Due to the diversity of these different communities, he may exhibit different characteristics when interacting with members from various communities. Specifically, the information about the mathematics group may be related to his professional research, while the information about sports clubs may be associated with his hobbies. As a result, we can decompose the output for a specific node as a combination of predictions for clusters:\n\n$$\nY _ { i } = \\sum _ { j = 1 } ^ { M } w _ { j } f ^ { ( j ) } ( X _ { i } ) ,\n$$\n\nwhere $j$ denotes the model index and there are $M$ models. For an unsupervised task, learning $w _ { j }$ directly is difficult. Instead, we generate $w _ { j }$ from personalized features: $w _ { j } =$ $\\left[ g ( X ^ { n } ) \\right] _ { j }$ , where $g ( X ^ { n } ) = \\sigma \\left( W _ { 3 } X ^ { n } \\right) .$ . In other words, we use personalized features to identify clusters. A higher value of a personalized feature corresponds to a higher weight. $W _ { 3 }$ represents an MLP.\n\nFor simplicity, we only consider SGC (Wu et al. 2019) as our basic model, i.e. $f ^ { ( j ) } ( X _ { i } ) = A _ { i } ^ { k } X W ^ { j }$ , where $A ^ { k }$ denotes the stacked $k$ -layer graph filter. Other GNNs can also be the base models. For instance, DAGNN (Liu, Gao, and Ji 2020) has $f ^ { ( j ) } ( X _ { i } ) \\ = \\ \\sum _ { t = 0 } ^ { k } s _ { t } A _ { i } ^ { t } X W ^ { j } .$ , where $s _ { t }$ is the learnable weight; APPNP (Gasteiger, Bojchevski, and G¨unnemann 2018) has $\\begin{array} { r l } { f ^ { ( j ) } ( X _ { i } ) } & { { } = } \\end{array}$ softmax $\\left( \\eta ( \\pmb { I } - ( 1 - \\eta ) A ) _ { i } ^ { - 1 } X \\right) W ^ { j }$ , where $\\eta$ is the a hyper-parameter. For convenience, we define ${ \\bar { X } } = A ^ { k } X =$ $\\{ \\bar { X } _ { 1 } , . . . , \\bar { X } _ { N } \\} ^ { \\top }$ . Note that $\\bar { X } _ { j } \\in \\mathbb { R } ^ { d }$ and $W ^ { j } \\in \\mathbb { R } ^ { d \\times d _ { o u t } }$ , where $d _ { o u t }$ is the output dimension. We have:\n\n$$\nY _ { i } = \\sum _ { j = 1 } ^ { M } \\left[ g \\left( X ^ { n } \\right) \\right] _ { j } { \\bar { X } } _ { i } W ^ { j } .\n$$\n\nThe $u$ -th entry of $Y _ { i }$ is given by:\n\n$$\nY _ { i u } = \\sum _ { j = 1 } ^ { M } \\sum _ { v = 1 } ^ { d } \\left[ g \\left( X ^ { n } \\right) \\right] _ { j } W _ { u v } ^ { j } \\bar { X } _ { i v } ,\n$$\n\nwhich introduces a complexity of $M$ times. Thus far, we still need to design $M$ individual models to identify clusters, which is computationally demanding. Fortunately, we have enough free parameters to simplify the process. Here we present a simple yet effective way. We set $M = d _ { o u t }$ , which gives $Y _ { i } = \\sum _ { j = 1 } ^ { d _ { o u t } } \\sum _ { v = 1 } ^ { d _ { i n } } \\left( \\left[ g \\left( X ^ { n } \\right) \\right] _ { j } W _ { u v } ^ { j } \\bar { X } _ { i v } \\right)$ . Then we $W _ { u v } ^ { j } = \\left\\{ \\begin{array} { l l } { W _ { u v } , j = u } \\\\ { 0 , j \\ne u } \\end{array} \\right.$ , which yields:\n\n$$\n\\begin{array} { c } { { \\displaystyle Y _ { i u } = [ g \\left( { \\cal X } ^ { n } \\right) ] _ { u } \\sum _ { v = 1 } ^ { d } W _ { u v } \\bar { \\cal X } _ { i v } } } \\\\ { { = [ g \\left( { \\cal X } ^ { n } \\right) ] _ { u } \\bar { \\cal X } _ { i } W _ { . u } , } } \\end{array}\n$$\n\nor equivalently,\n\n$$\nY = g \\left( X ^ { n } \\right) \\odot { \\bar { X } } W ,\n$$\n\nwhere $\\odot$ denotes the Hadamard product. Consequently, learning a model for each node is achieved through element-wise multiplication, which is computationally efficient.\n\nFlexibility: Note that the above approach is a generic method to customize existing techniques rather than a standalone way. It can be seamlessly incorporated with many SOTA GNN models to enhance performance.\n\n# Theoretical Analysis\n\nMost theoretical analysis in the GNN area focuses on graph structures (Mao et al. 2024). In this section, we establish a theoretical analysis from the feature perspective. Without loss of generality, we consider the case with two clusters, $c _ { 1 }$ and $c _ { 2 }$ . Assume the original node features follow the\n\nGaussian distribution: $X _ { i } \\sim { \\cal N } \\left( \\mu _ { 1 } , \\mathbf { I } \\right)$ for $i \\in c _ { 1 }$ and $X _ { i } \\sim$ ${ \\cal N } \\left( { \\pmb \\mu } _ { 2 } , { \\bf I } \\right)$ for $i \\in c _ { 2 }$ $( \\pmb { \\mu } _ { 1 } \\pmb { \\mu } _ { 2 } \\geq 0 )$ . We define the filtered feature as $\\bar { X } = D ^ { - 1 } \\widetilde { A } X$ . We define $f _ { i }$ and $f _ { j }$ as models for $X _ { i }$ and $X _ { j }$ , respectiv ely, focusing on the personalized features in sets $T _ { i }$ and $T _ { j }$ , while other irrelevant features are ignored. We then present the following theorem:\n\nTheorem 1. Assuming the distribution of filtered features $\\bar { X }$ shares the same variance $\\sigma \\mathbf { I }$ and the cluster has a balance distribution $\\mathbf { P } \\left( Y = c _ { 1 } \\right) = \\mathbf { P } \\left( Y = c _ { 2 } \\right)$ . The upper bound of $\\left| \\mathbf { P } \\left( Y _ { i } = c _ { 1 } \\mid f _ { i } ( { \\bar { X } } _ { i } ) \\right) - \\mathbf { P } \\left( Y _ { j } = c _ { 1 } \\mid f _ { j } ( { \\bar { X } } _ { j } ) \\right) \\right|$ is decreasi\fng with respect to $\\sum _ { u \\in T _ { i } \\cap T _ { j } } \\bar { X } _ { i u } \\bar { X } _ { j u }$ .\n\nNote that the assumptions are not strictly necessary but are used to simplify the proof in the Appendix. Theorem 1 indicates that if two nodes share more cluster-relevant features, they are more likely to be classified into one cluster, and vice versa.\n\n# Contrastive Clustering\n\nFeature Cross Augmentation Although graph filtering can smooth the features between node neighborhoods, it only captures high-order feature interactions and suffers from overlooking low-order feature interaction (Feng et al. 2021). Feature cross can provide valuable information for downstream tasks. For example, features like {title & approach & experiment can provide additional information about the paper’s category. Features that collectively represent title can be viewed as a field, like $t i t l e = \\{ 0 , 1 , 0 \\}$ . To this end, we randomly divide the filtered features into $m$ fields:\n\n$$\n\\bar { X } = \\{ \\bar { X } _ { f i e l d _ { 1 } } , \\bar { X } _ { f i e l d _ { 2 } } , . . . , \\bar { X } _ { f i e l d _ { m } } \\} .\n$$\n\nThen, every node has $m$ fields, i.e., $\\begin{array} { r l } { \\bar { X } _ { i } } & { { } = } \\end{array}$ $\\{ \\bar { X } _ { i f i e l d _ { 1 } } , \\bar { X } _ { i f i e l d _ { 2 } } ^ { \\bullet } , . . . , \\bar { X } _ { i f i e l d _ { m } } \\}$ We use $\\bar { X }$ instead of $X$ because $\\bar { X }$ has more information. The feature cross is defined as:\n\n$$\nR _ { i } = \\left\\{ \\bar { X } _ { i f i e l d _ { z } } \\bar { X } _ { i f i e l d _ { j } } ^ { \\top } ~ | ~ 1 \\leq z < j \\leq m \\right\\} ,\n$$\n\nWe input the original features $X$ as a view for contrastive learning. It’s well known that matrix factorization techniques can capture the low-order interaction (Guo et al. 2017). Thus, we propose to perform data augmentation as follows:\n\n$$\nX ^ { a u g } = X + R W ,\n$$\n\nwhere $W$ is an MLP to adjust the dimension. Unlike previous methods, Eq.(12) has two advantages. First, instead of using typical graph enhancements such as feature masking or edge perturbation, our method generates augmented views from filtered features. This maintains the semantics of the augmented view. Second, other learnable augmentations design complex losses to remain close to the initial features. Eq.(12) is based on the feature cross, which can provide rich semantic information. The complexity of the feature cross is $O ( N m ^ { 2 } )$ , which is linear to $N$ . The results can be stored and accessed after a one-time computation.\n\nSimilarly to Eq.(9), the second view is finally formulated as:\n\n$$\nY ^ { \\prime } = g \\left( X ^ { n } \\right) \\odot X ^ { a u g } W .\n$$\n\nNote that the same node in two different views shares the same model.\n\nTable 1: Statistics information of datasets.   \n\n<html><body><table><tr><td>Graph datasets</td><td>Nodes</td><td>Dims.</td><td>Edges</td><td>Clusters</td><td>ACD</td></tr><tr><td>Cora</td><td>2708</td><td>1433</td><td>5429</td><td>7</td><td>0.52</td></tr><tr><td>Citeseer</td><td>3327</td><td>3703</td><td>4732</td><td>6</td><td>0.36</td></tr><tr><td>Pubmed</td><td>19717</td><td>500</td><td>44327</td><td>3</td><td>0.36</td></tr><tr><td>UAT</td><td>1190</td><td>239</td><td>13599</td><td>4</td><td>0.35</td></tr><tr><td>AMAP</td><td>7650</td><td>745</td><td>119081</td><td>8</td><td>0.28</td></tr><tr><td>EAT</td><td>399</td><td>203</td><td>5994</td><td>4</td><td>0.25</td></tr><tr><td>BAT</td><td>1190</td><td>239</td><td>13599</td><td>4</td><td>0.46</td></tr><tr><td>Flickr</td><td>89250</td><td>500</td><td>899756</td><td>7</td><td>0.02</td></tr><tr><td>Twitch-Gamers</td><td>168114</td><td>7</td><td>67997557</td><td>2</td><td>0.09</td></tr></table></body></html>\n\nLoss Function For two views $Y$ and $Y ^ { \\prime }$ , we treat the same node in different views as positive samples and all other nodes as negative samples. The pairwise loss is defined as follows:\n\n$$\n\\begin{array} { l } { { \\ell \\left( Y _ { i } , Y _ { i } ^ { \\prime } \\right) = - \\log \\frac { e ^ { \\sin \\left( Y _ { i } , Y _ { i } ^ { \\prime } \\right) } } { \\sum _ { j = 1 } ^ { N } e ^ { \\sin \\left( Y _ { i } , Y _ { j } ^ { \\prime } \\right) } + \\sum _ { j = 1 } ^ { N } e ^ { \\sin \\left( Y _ { i } , Y _ { j } \\right) } } , } } \\\\ { { \\mathcal { L } _ { c o n } = \\displaystyle \\frac { 1 } { 2 N } \\sum _ { i = 1 } ^ { N } \\left[ \\ell \\left( Y _ { i } , Y _ { i } ^ { \\prime } \\right) + \\ell \\left( Y _ { i } ^ { \\prime } , Y _ { i } \\right) \\right] , } } \\end{array}\n$$\n\nwhere sim is the cosine similarity. Besides, the reconstruction loss can be calculated as follows:\n\n$$\n\\mathcal { L } _ { r e } = \\frac { 1 } { N ^ { 2 } } \\left. Y ^ { \\prime } Y ^ { \\top } - A \\right. _ { F } .\n$$\n\nFinally, the total loss is formulated as:\n\n$$\n\\begin{array} { r } { \\mathcal { L } = \\mathcal { L } _ { r e } + \\lambda \\mathcal { L } _ { c o n } , } \\end{array}\n$$\n\nwhere $\\lambda > 0$ is a trade-off parameter. The clustering results are obtained by performing K-means on ${ \\textstyle \\frac { 1 } { 2 } } ( Y + { \\check { Y } } ^ { \\prime } )$ . The detailed learning process of FPGC is illustrated in the Appendix.\n\n# Experiments\n\n# Datasets\n\nWe select seven graph clustering benchmark datasets, which are: Cora (Yang, Cohen, and Salakhudinov 2016), CiteSeer (Yang, Cohen, and Salakhudinov 2016), Pubmed (Yang, Cohen, and Salakhudinov 2016), Amazon Photo (AMAP) (Liu et al. 2022), USA Air-Traffic (UAT) (Mrabah et al. 2022), Europe Air-Traffic (EAT) (Mrabah et al. 2022), and Brazil Air Traffic (BAT) (Mrabah et al. 2022). We also add two largescale graph datasets, the image relationship network Flickr (Zeng et al. 2020) and the social network Twitch-Gamers (Rozemberczki and Sarkar 2021). To see the relevance between graph structure and downstream task, we also report the Aggregation Class Distance (ACD) (Shen, He, and Kang 2024).The statistics information is summarized in Table 1. We can see that these datasets are inherently low-quality.\n\n# Comparison Methods\n\nTo demonstrate the superiority of FPGC, we compare it to several recent baselines. These methods can be roughly divided into three kinds: 1) traditional GNN-based methods: SSGC (Zhu and Koniusz 2021). 2) shallow methods: MCGC (Pan and Kang 2021), FGC (Kang et al. 2022), and CGC (Xie et al. 2023). 3) contrastive learning-based methods: MVGRL (Hassani and Khasahmadi 2020), SDCN (Bo et al. 2020), DFCN (Tu et al. 2021), SCGC (Liu et al. 2023), CCGC (Yang et al. 2023a), and CONVERT (Yang et al. 2023b). 4) Advanced autoencoder-based methods: AGE (Cui et al. 2020), DGCN (Pan and Kang 2023), DMGNC (Yang et al. 2024), and DyFSS (Zhu et al. 2024).\n\n# Experimental Setting\n\nTo ensure fairness, all experimental settings follow the DGCN (Pan and Kang 2023), which performs a grid search to find the best results. Our network is trained with the Adam optimizer for 400 epochs until convergence. Three MLPs consist of a single embedding layer, with 100 dimensions on Flickr and Twitch-Gamers, and 500 dimensions on other datasets. The learning rate is set to 1e-2 on BAT/EAT/UAT/Twitch-Gamers, 1e-3 on Cora/Citeseer/Pumbed/Flickr, and 1e-4 on AMAP. The graph aggregating layers $k$ is searched in $\\{ 2 , 3 , 4 , 5 \\}$ . The number of fields $m$ is set according to the density of features, i.e., denser features should have smaller values to cross fewer times, the number of important features $n$ is set according to the number of features, i.e., more features should have larger values, the trade-off parameter $\\lambda$ is tuned in $\\{ 0 . 0 0 1 \\$ , $0 . 1 \\bar { , } 1 , 1 0 0 \\}$ . Thus, $\\{ k , m , \\bar { n } , \\lambda \\}$ are set to $\\{ 3 , 6 0 , 1 0 0 , 1 \\}$ on Cora, $\\{ 4 , 5 0 , 5 0 , 0 \\dot { . } 1 \\}$ on Citeseer, $\\{ 5 , 1 0 , 1 0 , 0 . 0 0 1 \\}$ on Pubmed, $\\{ 5 , 2 0 , 1 0 , 1 0 0 \\}$ on UAT, $\\{ 2 , 1 0 , 1 0 , 0 . 0 0 1 \\}$ on AMAP, $\\{ 4 , 1 0 , 1 0 , 0 . 0 0 1 \\}$ on EAT, $\\{ 5 , 2 0 , 2 0 , 1 \\}$ on BAT, $\\{ 4 , 1 0 , 1 0 0 , 1 \\}$ on Flickr and $\\{ 2 , 2 , 4 , 1 \\}$ on Twitch-Gamers. We evaluate clustering performance with two widely used metrics: ACC and NMI. All experiments are conducted on the same machine with the Intel(R) Core(TM) i9-12900k CPU, two GeForce GTX 3090 GPUs, and 128GB RAM 1.\n\n# Results Analysis\n\nThe results are illustrated in Table 2. We find that FPGC achieves dominant performance in all cases. For example, on the Cora dataset, FPGC surpasses the runner-up by $4 . 0 4 \\%$ and $2 . 6 5 \\%$ in terms of ACC and NMI. Traditional GNN-based methods have poor performance compared to other methods, which dig for more structure information or use contrastive learning to implicitly capture the supervision information. Note that SCGC, CCGC, and CONVERT are the most recent contrastive methods that design new augmentations. Our method constantly exceeds advanced GNN-based methods with adaptive filters, which indicates the significance of fully exploring feature information.\n\nDMGNC and DyFSS are the latest methods that explicitly consider feature information. DMGNC’s performance does not show clear advantages compared to other baselines on Cora and Citeseer. Though applying a node-wise feature fusion strategy, DyFSS also performs poorly. For example, our method’s ACC and NMI are $7 . 0 0 \\%$ and $4 . 0 6 \\%$ higher on Cora, and $1 5 . 5 4 \\%$ and $1 3 . 0 1 \\%$ higher on EAT, respectively. This is because they perform general embedding without exploiting the relationship between features and clusters.\n\n<html><body><table><tr><td rowspan=\"2\">Methods</td><td colspan=\"2\">Cora</td><td colspan=\"2\">Citeseer</td><td colspan=\"2\">Pubmed</td><td colspan=\"2\">UAT</td><td colspan=\"2\">AMAP</td><td colspan=\"2\">EAT</td><td colspan=\"2\">BAT</td></tr><tr><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td></tr><tr><td>DFCN</td><td>36.33</td><td>19.36</td><td>69.50</td><td>43.90</td><td></td><td></td><td>33.61</td><td>26.49</td><td>76.88</td><td>69.21</td><td>32.56</td><td>8.27</td><td>35.56</td><td>8.25</td></tr><tr><td>SSGC</td><td>69.60</td><td>54.71</td><td>69.11</td><td>42.87</td><td></td><td></td><td>36.74</td><td>8.04</td><td>60.23</td><td>60.37</td><td>32.41</td><td>4.65</td><td>36.74</td><td>8.04</td></tr><tr><td>MVGRL</td><td>70.47</td><td>55.57</td><td>68.66</td><td>43.66</td><td>_-_</td><td></td><td>44.16</td><td>21.53</td><td>45.19</td><td>36.89</td><td>32.88</td><td>11.72</td><td>37.56</td><td>29.33</td></tr><tr><td>SDCN</td><td>60.24</td><td>50.04</td><td>65.96</td><td>38.71</td><td>65.78</td><td>29.47</td><td>52.25</td><td>21.61</td><td>53.44</td><td>44.85</td><td>39.07</td><td>8.83</td><td>53.05</td><td>25.74</td></tr><tr><td>AGE</td><td>73.50</td><td>57.58</td><td>70.39</td><td>44.92</td><td></td><td>，</td><td>52.37</td><td>23.64</td><td>75.98</td><td></td><td>47.26</td><td>23.74</td><td>56.68</td><td>36.04</td></tr><tr><td>MCGC</td><td>42.85</td><td>24.11</td><td>64.76</td><td>39.11</td><td>66.95</td><td>32.45</td><td>41.93</td><td>16.64</td><td>71.64</td><td>61.54</td><td>32.58</td><td>7.04</td><td>38.93</td><td>23.11</td></tr><tr><td>FGC</td><td>72.90</td><td>56.12</td><td>69.01</td><td>44.02</td><td>70.01</td><td>31.56</td><td>53.03</td><td>27.06</td><td>71.04</td><td>-</td><td>36.84</td><td>10.07</td><td>47.33</td><td>18.90</td></tr><tr><td>CONVERT</td><td>74.07</td><td>55.57</td><td>68.43</td><td>41.62</td><td>68.78</td><td>29.72</td><td>57.36</td><td>28.75</td><td>77.19</td><td>62.70</td><td>58.35</td><td>33.36</td><td>78.02</td><td>53.54</td></tr><tr><td>SCGC</td><td>73.88</td><td>56.10</td><td>71.02</td><td>45.25</td><td>67.73</td><td>28.65</td><td>56.58</td><td>28.07</td><td>77.48</td><td>67.67</td><td>57.94</td><td>33.91</td><td>77.97</td><td>52.91</td></tr><tr><td>CGC</td><td>75.15</td><td>56.90</td><td>69.31</td><td>43.61</td><td>67.43</td><td>33.07</td><td>49.58</td><td>17.49</td><td>73.02</td><td>63.26</td><td>44.32</td><td>30.25</td><td>53.44</td><td>26.97</td></tr><tr><td>CCGC</td><td>73.88</td><td>56.45</td><td>69.84</td><td>44.33</td><td>68.06</td><td>30.92</td><td>56.34</td><td>28.15</td><td>77.25</td><td>67.44</td><td>57.19</td><td>33.85</td><td>75.04</td><td>50.23</td></tr><tr><td>DGCN</td><td>72.19</td><td>56.04</td><td>71.27</td><td>44.13</td><td></td><td></td><td>52.27</td><td>23.54</td><td>76.07</td><td>66.13</td><td>51.27</td><td>31.98</td><td>70.15</td><td>49.52</td></tr><tr><td>DMGNC</td><td>73.12</td><td>54.80</td><td>71.27</td><td>44.40</td><td>70.46</td><td>34.21</td><td></td><td></td><td></td><td>-</td><td></td><td>-</td><td></td><td></td></tr><tr><td>DyFSS</td><td>72.19</td><td>55.49</td><td>70.18</td><td>44.80</td><td>68.05</td><td>26.87</td><td>51.43</td><td>25.52</td><td>76.86</td><td>67.78</td><td>43.36</td><td>21.23</td><td>77.25</td><td>51.33</td></tr><tr><td>FPGC</td><td>79.19</td><td>59.55</td><td>72.59</td><td>46.36</td><td>71.03</td><td>34.57</td><td>58.07</td><td>30.64</td><td>78.44</td><td>69.40</td><td>58.90</td><td>34.24</td><td>79.38</td><td>55.57</td></tr></table></body></html>\n\nTable 2: Clustering Results. The best performance is marked in red. “-” indicates the original paper does not have this result and the provided code can’t produce the result.\n\n![](images/62b4d2b77673673014f883ac76acb6056c2ea6c0ca5ed0fbb0caaf26afd823fd.jpg)  \nFigure 3: Parameter analysis of $m$ and $n$ on Cora and EAT.\n\n# Parameter Analysis\n\nTo assess the impact of parameters, we evaluate the clustering accuracy of FPGC across them on Cora and EAT. First, we test the performance with different $m$ and $n$ . As shown in Fig. 3, Cora is less sensitive to these two parameters than EAT. In addition, Cora prefers a large $m$ while EAT opts for a small $m$ , this is consistent with the density (proportion of non-zero values) of $\\bar { X }$ : $3 8 . 6 9 \\%$ on Cora and $5 0 . 2 5 \\%$ on EAT. Too large $m$ will degrade the performance since excessive augmentation could deteriorate the original information. Cora prefers a large $n$ while EAT opts for a small one, which is reasonable since the attribute dimension of Cora is much larger.\n\nSecondly, the impact of $k$ and $\\lambda$ is shown in Fig. 4. FPGC can perform in effect for a wide range of $\\lambda$ . A small $k$ is enough to achieve a decent result.\n\n# Results on Large-scale Data\n\nTo evaluate the scalability of FPGC, we conduct the experiments on large graph datasets Flickr and Twitch-Gamers, which have 89250 and 168114 nodes, respectively. Note that many methods fail to run on these datasets, such as DGCN. We set the batch size to 1000 for all methods. We select three recent baselines to see their performance and the total training time. Fig. 5 shows the results. FPGC continues to outperform all comparison approaches. The required training time of FPGC is notably lower than that of the CGC and\n\n![](images/0b8ee54d370444b40a7e41a85f0ae1ce36364524b51ef7eb9c07fc15deb20781.jpg)  \nFigure 4: Parameter analysis of $k$ and $\\lambda$ on Cora and EAT.   \nFigure 5: Clustering results and running time on Flickr and Twitch-Gamers.\n\nClustering Results on Large-scale Datasets TrainingTimeonLarge-scaleDatasets 60 1000 Flickr Flickr Twitch-Gamers 800 55   \nGR 600   \nC50   \nA 400 45 200 40 0 CGC CCGCDyFSS FPGC Methods Methods\n\nDyFSS and exhibits a marginal advantage over CCGC. This is because the feature-personalized step involves only matrix multiplication, while graph filtering and feature crossing can be pre-computed and stored. The result not only highlights the effectiveness of FPGC but also underscores its scalability and efficiency in handling large-scale datasets.\n\n# Robustness Analysis\n\nTo demonstrate the significance of the feature, we examine the situation in which the graph structure is of poor quality. Specifically, we add edges to the graphs and remove the same number of original edges from them at random ( $\\mathrm { F u }$ , Zhao, and Bian 2022). We define = #random edges as the random edge rate. With $r = \\{ 0 . 2 , 0 . 4 , 0 . 6 , 0 . 8 \\}$ , we report the ACC of CGC, CCGC, DyFSS, and FPGC in Cora and Citeseer. From Fig. 6, it can be seen that the PFGC achieves the best performance in all cases. Especially when the perturbation rate is extremely high, our method shows a more stable tendency than other clustering methods. The performance of other methods changes dramatically, indicating that they rely highly on the graph structure. Thus, our method is robust to the graph structure noise.\n\nTable 3: Results of ablation study. The best performance is marked in red and the runner-up is marked in bold.   \n\n<html><body><table><tr><td rowspan=\"2\">Methods</td><td colspan=\"2\">FPGC w/o aug</td><td colspan=\"2\">FPGC w aug</td><td colspan=\"2\">FPGC w/o g()</td><td colspan=\"2\">FPGCωDAGNN</td><td colspan=\"2\">FPGC</td></tr><tr><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td></tr><tr><td>Cora</td><td>75.95</td><td>58.98</td><td>76.42</td><td>57.52</td><td>78.52</td><td>58.72</td><td>78.27</td><td>59.02</td><td>79.19</td><td>59.55</td></tr><tr><td>Citeseer</td><td>71.04</td><td>44.90</td><td>71.33</td><td>44.65</td><td>70.23</td><td>44.34</td><td>72.10</td><td>46.02</td><td>72.59</td><td>46.36</td></tr><tr><td>Pubmed</td><td>66.98</td><td>28.47</td><td>67.76</td><td>29.42</td><td>71.20</td><td>34.96</td><td>72.05</td><td>35.26</td><td>71.03</td><td>34.57</td></tr><tr><td>UAT</td><td>57.25</td><td>29.06</td><td>57.61</td><td>29.69</td><td>57.13</td><td>29.86</td><td>58.24</td><td>30.72</td><td>58.07</td><td>30.64</td></tr><tr><td>AMAP</td><td>77.45</td><td>68.32</td><td>78.01</td><td>68.96</td><td>76.88</td><td>66.74</td><td>79.52</td><td>70.22</td><td>78.44</td><td>69.40</td></tr><tr><td>EAT</td><td>57.82</td><td>33.32</td><td>58.05</td><td>33.96</td><td>57.25</td><td>33.19</td><td>59.28</td><td>34.71</td><td>58.90</td><td>34.24</td></tr><tr><td>BAT</td><td>78.12</td><td>53.81</td><td>78.23</td><td>53.96</td><td>77.23</td><td>52.19</td><td>79.03</td><td>55.02</td><td>79.38</td><td>55.57</td></tr></table></body></html>\n\n![](images/119f62c2a2f7fbe5a30e3a155336e6330745fd4f8256404a380197058d7ae7d2.jpg)  \nFigure 6: Results of robustness test on Cora (left) and Citeseer (right).\n\n![](images/8d08c110bc6e6c988841f0df8823a1e105109df032e55207ceeb1a69827196d0.jpg)  \nFigure 7: The results on different important features.\n\n# Ablation Study\n\nTo assess the impact of feature cross, we test the clustering performance after removing it, marking it as “FPGC $w / o$ aug”. The results are shown in Table 3. It is clear that feature cross does improve the clustering performance. In addition, we replace feature cross with classical graph augmentation strategies, including drop edges (Liu et al. 2024), add edges (Xia et al. 2022), graph diffusion (Hassani and Khasahmadi 2020), and mask feature (Yu et al. 2022). We set the change rate to $20 \\%$ according to the suggestions of the original paper. The best performance among these four methods is marked as “FPGC w aug”. Our method can still beat them, thus feature cross can provide more rich information.\n\nTo see the influence of feature selection, we test the performance without $g ( X ^ { n } )$ and mark it as “FPGC $w / o g ( \\bar { ) } ^ { \\dag }$ . We can see the performance degradation in most cases in Table 3. Thus learning a model for each node can successfully keep each node’s personality. FPGC does not achieve better performance on Pubmed. This may be because Pubmed has a large node size but only has 3 clusters, which means that it is difficult to select the cluster-relevant features in this case. We also test the performance with DAGNN (Liu, Gao, and Ji 2020) as the base model and mark this method as “FPGC $w$ DAGNN”. It can be seen that it achieves the best performance in 4 out of 7 cases in Table 3. Thus, our proposed “one node one model” paradigm is promising with other SOTA GNNs.\n\nTo verify that the proposed squeeze-and-excitation block can select essential features, we categorize them into three intervals: features with the top $3 3 . 3 \\%$ (highest) $\\tilde { q }$ , features from $3 3 . 3 \\%$ to $6 6 . 7 \\%$ , and the remaining $6 6 . 7 \\%$ to $100 \\%$ . We use these indexes to select features for $X ^ { n }$ and test their performance, respectively. From Fig. 7, we can see that when selecting the most important features (top $3 3 . 3 \\%$ ), we achieve the best results in all cases. This indicates that features with high value in $\\tilde { q }$ are vital. Therefore, our proposed squeezeand-excitation block can successfully assign high weights to essential features, which is beneficial for downstream task.\n\n# Conclusion\n\nIn this paper, we introduce the “one node one model” paradigm, which addresses the limitations of existing graph clustering methods by emphasizing missing-half feature information. By incorporating a squeeze-and-excitation block to select cluster-relevant features, our method effectively enhances the discriminative power of the learned representations. The proposed feature cross data augmentation further enriches the information available for contrastive learning. Our theoretical analysis and experimental results demonstrate that our method significantly improves clustering performance.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   现有图聚类方法主要关注拓扑结构，忽略了节点特征信息（称为“missing-half”问题），尤其是高维特征如何提升聚类性能的问题。\\n> *   该问题的重要性在于，特征选择在图聚类中尤为困难，因为它需要同时发现聚类并识别这些聚类的相关特征。此外，现有方法在输入图质量较差时（如不同类别的节点连接紧密或重要链接缺失），通过多层聚合学习的表示会变得不具区分性（称为表示崩溃）。\\n\\n> **方法概述 (Method Overview)**\\n> *   提出“Feature Personalized Graph Clustering (FPGC)”方法，通过挤压-激励块（squeeze-and-excitation block）选择聚类相关特征，实现“one node one model”范式。\\n> *   开发了一种基于特征交叉（feature cross）的数据增强技术，以捕获低阶特征交互。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **创新贡献点1：** 提出“one node one model”范式，通过挤压-激励块选择聚类相关特征，显著提升了聚类性能。\\n>     *   **关键数据：** 在Cora数据集上，FPGC的ACC达到79.19%，NMI达到59.55%，分别比最优基线提升4.04%和2.65%。\\n> *   **创新贡献点2：** 开发基于特征交叉的数据增强技术，有效捕获低阶特征交互。\\n>     *   **关键数据：** 在EAT数据集上，FPGC的ACC和NMI分别比最优基线提升15.54%和13.01%。\\n> *   **创新贡献点3：** FPGC具有即插即用（plug-and-play）特性，可作为现有GNN模型的增强工具。\\n>     *   **关键数据：** 在Flickr和Twitch-Gamers等大规模数据集上，FPGC的训练时间显著低于其他方法。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   FPGC的核心思想是通过为每个节点构建专属模型，利用挤压-激励块选择聚类相关特征，从而增强表示的区分性。特征交叉作为数据增强技术，进一步丰富了低阶特征交互信息。\\n> *   设计哲学在于：节点特征和拓扑结构在无监督图嵌入和下游聚类中应发挥同等作用，个性化特征对节点聚类标签的识别至关重要。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 现有方法主要依赖图结构或高阶特征交互，忽视了低阶特征交互和个性化特征选择。\\n> *   **本文的改进：** FPGC通过挤压-激励块动态选择聚类相关特征，并引入特征交叉增强低阶交互，从而解决了现有方法的局限性。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **挤压-激励块：** \\n>     *   **挤压（Squeeze）：** 计算节点特征的全局统计量：\\n>         $$\\n>         \\\\boldsymbol { q } = \\\\boldsymbol { F _ { s q } } \\\\left( \\\\boldsymbol { X } \\\\right) = \\\\frac { 1 } { N } \\\\sum _ { i = 1 } ^ { N } \\\\boldsymbol { X _ { i } }.\\n>         $$\\n>     *   **激励（Excitation）：** 通过MLP和激活函数捕获通道间依赖关系：\\n>         $$\\n>         \\\\tilde { q } = F _ { e x } \\\\left( q \\\\right) = \\\\sigma \\\\left( W _ { 2 } \\\\delta \\\\left( W _ { 1 } q \\\\right) \\\\right).\\n>         $$\\n>     *   **选择（Selection）：** 根据重要性选择top-n特征：\\n>         $$\\n>         X ^ { n } = F _ { t o p } \\\\left( \\\\tilde { q } X \\\\right).\\n>         $$\\n> 2.  **“one node one model”范式：** \\n>     *   为每个节点构建专属模型，输出为聚类预测的加权组合：\\n>         $$\\n>         Y _ { i } = \\\\sum _ { j = 1 } ^ { M } w _ { j } f ^ { ( j ) } ( X _ { i } ).\\n>         $$\\n>     *   通过Hadamard积实现高效计算：\\n>         $$\\n>         Y = g \\\\left( X ^ { n } \\\\right) \\\\odot { \\\\bar { X } } W.\\n>         $$\\n> 3.  **特征交叉增强：** \\n>     *   将过滤后的特征随机分为m个字段，计算特征交叉：\\n>         $$\\n>         R _ { i } = \\\\left\\\\{ \\\\bar { X } _ { i f i e l d _ { z } } \\\\bar { X } _ { i f i e l d _ { j } } ^ { \\\\top } ~ | ~ 1 \\\\leq z < j \\\\leq m \\\\right\\\\}.\\n>         $$\\n>     *   生成增强视图：\\n>         $$\\n>         X ^ { a u g } = X + R W.\\n>         $$\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   传统GNN方法：SSGC。\\n> *   浅层方法：MCGC、FGC、CGC。\\n> *   对比学习方法：MVGRL、SDCN、DFCN、SCGC、CCGC、CONVERT。\\n> *   自编码器方法：AGE、DGCN、DMGNC、DyFSS。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在ACC（准确率）上：** FPGC在Cora数据集上达到79.19%，显著优于基线模型CONVERT（74.07%）和SCGC（73.88%）。与表现最佳的基线相比，提升了4.04个百分点。\\n> *   **在NMI（归一化互信息）上：** FPGC在Citeseer数据集上达到46.36%，优于基线模型DyFSS（44.80%）和DMGNC（44.40%）。与表现最佳的基线相比，提升了1.56个百分点。\\n> *   **在训练时间上：** FPGC在Flickr数据集上的训练时间显著低于CGC和DyFSS，与CCGC相当，但在ACC上远超后者。\\n> *   **在鲁棒性上：** FPGC在随机边扰动率高达80%时，仍保持稳定的性能，而其他方法（如CGC、CCGC）性能急剧下降。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   图聚类 (Graph Clustering, GC)\\n*   挤压-激励块 (Squeeze-and-Excitation Block, SE Block)\\n*   特征交叉 (Feature Cross, N/A)\\n*   低阶特征交互 (Low-order Feature Interaction, N/A)\\n*   即插即用 (Plug-and-Play, N/A)\\n*   数据增强 (Data Augmentation, DA)\\n*   图神经网络 (Graph Neural Network, GNN)\\n*   无监督学习 (Unsupervised Learning, UL)\"\n}\n```"
}