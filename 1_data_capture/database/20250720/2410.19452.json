{
    "source": "Semantic Scholar",
    "arxiv_id": "2410.19452",
    "link": "https://arxiv.org/abs/2410.19452",
    "pdf_link": "https://arxiv.org/pdf/2410.19452.pdf",
    "title": "NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video Reconstruction",
    "authors": [
        "Zixuan Gong",
        "Guangyin Bao",
        "Qi Zhang",
        "Zhongwei Wan",
        "Duoqian Miao",
        "Shoujin Wang",
        "Lei Zhu",
        "Changwei Wang",
        "Rongtao Xu",
        "Liang Hu",
        "Ke Liu",
        "Yu Zhang"
    ],
    "categories": [
        "cs.AI",
        "cs.CV"
    ],
    "publication_date": "2024-10-25",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Engineering",
        "Computer Science"
    ],
    "citation_count": 10,
    "influential_citation_count": 3,
    "institutions": [
        "Tongji University",
        "Ohio State University",
        "University of Technology Sydney",
        "Chinese Academy of Sciences",
        "Beijing Anding Hospital"
    ],
    "paper_content": "# NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video Reconstruction\n\nZixuan $\\mathbf { G o n g ^ { 1 } }$ , Guangyin $\\mathbf { B a o } ^ { 1 }$ , Qi $\\mathbf { Z h a n g ^ { 1 , \\dagger } }$ , Zhongwei $\\mathbf { W a n } ^ { 2 }$ , Duoqian $\\mathbf { M i a o } ^ { 1 , \\dagger }$ , Shoujin Wang3, Lei $\\mathbf { Z } \\mathbf { h } \\mathbf { u } ^ { 1 }$ , Changwei Wang4, Rongtao $\\mathbf { X } \\bar {  { \\mathbf { u } } } ^ { 4 }$ , Liang $\\mathbf { H } \\mathbf { u } ^ { 1 }$ , Ke $\\mathbf { L i u } ^ { 5 }$ , Yu Zhang\n\n1Tongji University 2Ohio State University 3University of Technology Sydney 4Chinese Academy of Sciences 5Beijing Anding Hospital {gongzx,baogy,zhangqi_cs,dqmiao,izy}@tongji.edu.cn\n\n# Abstract\n\nReconstruction of static visual stimuli from non-invasion brain activity fMRI achieves great success, owning to advanced deep learning models such as CLIP and Stable Diffusion. However, the research on fMRI-to-video reconstruction remains limited since decoding the spatiotemporal perception of continuous visual experiences is formidably challenging. We contend that the key to addressing these challenges lies in accurately decoding both high-level semantics and low-level perception flows, as perceived by the brain in response to video stimuli. To the end, we propose NeuroClips, an innovative framework to decode high-fidelity and smooth video from fMRI. NeuroClips utilizes a semantics reconstructor to reconstruct video keyframes, guiding semantic accuracy and consistency, and employs a perception reconstructor to capture low-level perceptual details, ensuring video smoothness. During inference, it adopts a pre-trained T2V diffusion model injected with both keyframes and low-level perception flows for video reconstruction. Evaluated on a publicly available fMRI-video dataset, NeuroClips achieves smooth high-fidelity video reconstruction of up to 6s at 8FPS, gaining significant improvements over state-of-the-art models in various metrics, e.g., a $12 8 \\%$ improvement in SSIM and an $81 \\%$ improvement in spatiotemporal metrics. Our project is available at https://github.com/gongzix/NeuroClips.\n\n# 1 Introduction\n\nDecoding visual stimuli from neural activity is crucial and prospective to unraveling the intricate mechanisms of the human brain. In the context of non-invasive approaches, visual reconstruction from functional magnetic resonance imaging (fMRI), such as fMRI-to-image reconstruction, shows high fidelity [1, 2, 3, 4], largely benefiting from advanced deep learning models such as CLIP [5, 6] and Stable Diffusion [7]. This convergence of brain science and deep learning presents a promising data-driven learning paradigm to explore a comprehensive understanding of the advanced perceptual and semantic functions of the cerebral cortex. Unfortunately, fMRI-to-video reconstruction still presents significant hurdles that discourage researchers, since decoding the spatiotemporal perception of a continuous flow of scenes, motions, and objects is formidably challenging.\n\nAt first glance, fMRI measures blood oxygenation level-dependent (BOLD) signals by snapshotting a few seconds of brain activity, leading to differential temporal resolutions between fMRI (low) and videos (high). The previously advisable solution to address such differential granularity is to perform self-interpolation on fMRI and downsample video frames to pre-align fMRI and videos. Going further, decoding accurate high-level semantics and low-level perception flows has a more profound impact on the ability to reconstruct high-fidelity videos from brain activity. Early studies before 2022 struggled with achieving satisfactory reconstruction performance, as they failed to acquire precise semantics from powerful (pre-trained) diffusion models. The latest research MinD-Video [8] guides the diffusion model conditioned on visual fMRI features, making an initial attempt to address the semantic issue. However, it lacks a design of low-level visual detailing, so it significantly diverges from the brain‚Äôs visual system, exhibiting limitations in perceiving continuous low-level visual details.\n\nThe brain‚Äôs reflection of video stimuli is a crucial factor that influences and enlightens the visual decoding of fMRI-to-video reconstruction. Notably, the human brain perceives videos discreetly [9, 10] due to the persistence of vision [11, 12, 13, 14, 15] and delayed memory [16]. It is impractical to perceive every video frame, and instead, only keyframes elicit significant responses in the brain‚Äôs visual system. Reconstructing keyframes from fMRI avoids the issue of differential temporal resolutions between fMRI and videos. Also, the precise semantics and perceptual details in keyframes ensure the high fidelity and smoothness of reconstructed videos, both within and across successive fMRI inputs. Accordingly, we argue that utilizing keyframe images as anchors for transitional video reconstruction aligns with the brain‚Äôs cognitive mechanisms and holds greater promise.\n\nHowever, relying solely on manipulating fMRI-to-image models, e.g., incorporated with spatiotemporal conditions for successive image reconstruction, to generate keyframes, easily yields suboptimal outcomes. Research dating back to as early as 1960 [17] has shown that the fleeting sequence of images perceived by the retina is hardly discernible during the process of perception. Instead, what emerges is a phenomenal scene or its intriguing features, which can be regarded as non-detailed, low-level images. Initially, the retina captures these low-level perceptions, and subsequently, the higher central nervous system in the brain focuses on and pursues the details, generating high-level images in the cerebral cortex [18]. This process is reflected in the fMRI signal [19, 20, 21, 22, 23, 24]. The video-to-fMRI process naturally incorporates a combination of both low-level and high-level images. Therefore, the reverse fMRI-to-video reconstruction intuitively benefits from taking into account both the high-level semantic features and the low-level perceptual features. Specifically, it is necessary to decode the low-level perception flows, such as motions and dynamic scenes, from brain activity to complement keyframes, which enhances the reconstruction of high-fidelity frames and produces smooth videos.\n\nIn light of the above discussion, we propose a novel fMRI-to-video reconstruction framework NeuroClips that introduces two trainable components of Perception Reconstructor and Semantics Reconstructor for reconstructing low-level perception flows and keyframes, respectively. 1) Perception Reconstructor introduces Inception Extension and Temporal Upsampling modules to adaptively align fMRI with video frames, decoding low-level perception flows, i.e., a blurry video. This blurry video ensures the smoothness and consistency of subsequent video reconstruction. 2) Semantics Reconstructor adopts a diffusion prior and multiple training strategies to concentrate quantities of high-level semantics from various modalities into fMRI embeddings. These fMRI embeddings are mapped to the CLIP image space and decoded to reconstruct high-quality keyframes. 3) During inference, NeuroClips adopts a pre-trained T2V diffusion model injected with keyframes and low-level perception flows for video reconstruction with high fidelity, smoothness, and consistency.\n\nExtensive experiments have validated the superior performance of NeuroClips, which is substantially ahead of SOTA baselines in pixel-level metrics and video consistency. NeuroClips achieves a 0.219 improvement $( 1 2 8 \\% )$ in SSIM and a 0.330 improvement $( 8 1 \\% )$ in spatiotemporal metrics and also performs better overall on most video semantic-level metrics. Meanwhile, using multi-fMRI fusion, NeuroClips pioneers the exploration of longer video reconstruction up to 6s at 8FPS.\n\n# 2 Related Work\n\n# 2.1 Visual Reconstruction\n\nAfter its initial exploration [25], static image reconstruction from fMRI has witnessed remarkable success in recent years. Due to the lack of information in fMRI adapted to deep learning models, a path has been gradually explored that aligns fMRI to specific modal representations such as the common image and text modality [26, 27, 28, 29], and CLIP‚Äôs [5] rich representation of space is pretty favored. Then, the aligned representations can then be fed into the diffusion model to complete the image generation. Along this path, a large body of literature demonstrates that reconstructing images at both pixel level and semantic level achieves great results [1, 2, 4, 30]. However, the field of fMRI-to-video reconstruction remains largely unexplored. Early studies [31, 32, 33] attempted to reconstruct low-level visual content from fMRI, using the embedded fMRI as conditions to guide GANs or AEs in generating multiple static images. The reconstructed videos contained little to no clear semantics recognizable by humans. Due to the excellent performance of diffusion models, MinD-Video [8] reconstructed 3FPS videos from fMRI. Despite this success, the smoothness and semantic accuracy of these videos remain unsatisfactory, leaving substantial space for improvement.\n\n# 2.2 Video Diffusion Model\n\nDiffusion models for image generation have gained significant attention in research communities recently [34, 35, 36]. DALLE¬∑2 [37] improved text-image generation by leveraging the CLIP [5] joint representation space. Stable Diffusion [7] enhanced generation efficiency by moving the diffusion process to the latent space of VQVAE [38]. To achieve customized generation with trained diffusion models, many works focused on adding extra condition control networks, such as ControlNet [39] and T2I-Adapter [40]. In the realm of video generation, it is typical to extend existing diffusion models with temporal modeling [41, 42, 43]. Animatediff [44] trained a plug-and-play motion module that can be seamlessly integrated into any customized image diffusion model to form a video generator. Stable Video Diffusion [45] fine-tunes pre-trained diffusion models using high-quality video datasets from multiple views to achieve powerful generation.\n\n# 3 Method\n\nThe overall framework of NeuroClips is illustrated in Figure 1. NeuroClips consists of three essential components: 1) Perception Reconstructor (PR) generates the blurry but continuous rough video from the perceptual level while ensuring consistency between its consecutive frames. 2) Semantics Reconstructor (SR) reconstructs the high-quality keyframe image from the semantic level. 3) Inference Process is the fMRI-to-video reconstruction process, which employs a T2V diffusion model and combines the reconstructions from PR and SR to reconstruct the final exquisite video with high fidelity, smoothness, and consistency. Furthermore, NeuroClips also pioneers the exploration of Multi-fMRI Fusion for longer video reconstruction.\n\n![](images/a31bf1a9ce77ea834d9f56d72dd76411946b8e074b6a8c264bf04a0aa5e41b2e.jpg)  \nFigure 1: The overall framework of NeuroClips. The red lines represent the infernence process.\n\n# 3.1 Perception Reconstructor\n\nPerception reconstruction is essential not only for video reconstruction but also for semantics reconstruction. Additionally, smoothness and consistency are crucial metrics of video quality, and Perception Reconstructor (PR) plays a key role in ensuring these attributes.\n\nWe split a video into several clips at two-second intervals (i.e., fMRI time resolution). For each clip $c$ , we downsample it and retain part frames at fixed intervals, resulting in a set of frames $\\mathbf { X } = [ \\mathcal { X } _ { 1 } , \\mathcal { X } _ { 2 } , \\cdot \\cdot \\cdot , \\mathcal { X } _ { N _ { f } } ]$ . $\\mathcal { X } _ { i }$ is the $i$ -th retained frame image, with a total of $N _ { f }$ retained frames. ${ { y } _ { c } }$ is the corresponding fMRI signal of clip $c$ . Here we introduce Inception Extension module to extend one fMRI to $N _ { f }$ fMRIs, denoted as ${ \\mathcal { V } } _ { c } \\to \\mathbf { Y }$ , $\\mathbf { Y } = [ \\mathcal { V } _ { 1 } , \\mathcal { V } _ { 2 } , \\cdot \\cdot \\cdot , \\bar { \\mathcal { V } } _ { N _ { f } } ]$ .\n\nSequentially applying a simple MLP and Temporal Upsampling module to obtain $\\mathbf { Y }$ ‚Äôs embedding $\\mathbf { E } _ { \\mathcal { Y } } = [ e _ { \\mathcal { Y } _ { 1 } } , e _ { \\mathcal { Y } _ { 2 } } , \\cdot \\cdot \\cdot , e _ { \\mathcal { Y } _ { N _ { f } } } ]$ , which can be fed into the Stable Diffusion [7] VAE decoder to produce a series of blurry images. We regard this sequence of blurry images as blurry video. We expect the blurry video to lack semantic content, but to exhibit state-of-the-art perceptual metrics, such as position, shape, scene, etc. Thus, we consider using frame set $\\mathbf { X }$ to align $\\mathbf { Y }$ .\n\nTraining Loss. Mapping $\\mathbf { X }$ to the latent space of Stable Diffusion‚Äôs VAE to obtain the perception embedding set $\\mathbf { E } _ { \\mathcal { X } } = [ e _ { \\mathcal { X } _ { 1 } } , e _ { \\mathcal { X } _ { 2 } } , \\cdot \\cdot \\cdot , e _ { \\mathcal { X } _ { N _ { f } } } ]$ . We adopt mean absolute error (MAE) loss and contrastive loss to train the PR, the overall loss $\\mathcal { L } _ { P R }$ of PR can be described as:\n\n$$\n\\begin{array} { r l r } {  { \\mathcal { L } _ { P R } = \\frac { 1 } { N _ { f } } \\sum _ { i = 1 } ^ { N _ { f } } | e { x _ { i } - e y _ { i } } | - \\frac { 1 } { 2 N _ { f } } \\sum _ { j = 1 } ^ { N _ { f } } \\log \\frac { \\exp ( s i m ( e { x _ { j } } , e { y _ { j } } ) / \\tau ) } { \\sum _ { k = 1 } ^ { N _ { f } } \\exp ( s i m ( e { x _ { j } } , e { y _ { k } } ) / \\tau ) } } } \\\\ & { } & { \\displaystyle - \\frac { 1 } { 2 N _ { f } } \\sum _ { j = 1 } ^ { N _ { f } } \\log \\frac { \\exp ( s i m ( e { y _ { j } } , e { x _ { j } } ) / \\tau ) } { \\sum _ { k = 1 } ^ { N _ { f } } \\exp ( s i m ( e { y _ { j } } , e { x _ { k } } ) / \\tau ) } , } \\end{array}\n$$\n\nwhere $\\tau$ is a temperature hyper-parameter. The function $s i m ( , )$ is used to compute the similarity.\n\nTemporal Upsampling. Due to the low signal-to-noise ratio of fMRI, the direct alignment of fMRI to VAE‚Äôs pixel space is highly susceptible to overfitting noise, and the learning task is too complex to guarantee the generation of decent blurry images. A common method is aligning to a coarser-grained pixel space and then upsampling to a fine-grained pixel space. The temporal relationship of the frames also needs to be considered in the video task to maintain consistency. Therefore, to achieve consistency between the retained frames, we integrated temporal attention into the upsampling operation. The fMRI embedding $\\mathbf { E } _ { \\mathcal { Y } }$ has five dimensions, i.e. $\\mathbf { E } _ { \\mathcal { Y } } \\in \\mathbb { R } ^ { b \\times N _ { f } \\times c \\times h \\times \\dot { w } }$ , where $b$ denotes the batch size, $c \\times h \\times w$ is the dimension of embedding space. The upsampling operation merely models spatial relationship, receiving reshaped embedding $\\bar { \\mathbf { E } _ { \\mathcal { Y } } ^ { \\mathrm { s p a t } } } \\in \\mathbb { R } ^ { ( b \\times \\overline { { N _ { f } } } ) \\times \\overline { { c } } \\times h \\times w }$ as input. To model temporal relationship of $N _ { f }$ fMRI, we first reshape $\\mathbf { E } _ { \\mathcal { Y } }$ as ${ \\bf E } _ { y } ^ { \\mathrm { t e m p } } \\in \\mathbb { R } ^ { ( b \\times h \\times w ) \\times N _ { f } \\times c }$ Then, we use learnable mapping to compute the query value ${ \\cal Q } = { \\cal W } ^ { \\cal Q } { \\bf E } _ { y } ^ { \\mathrm { t e m p } }$ and the key value $K = W ^ { K } { \\bf E } _ { \\mathcal { Y } } ^ { \\mathrm { t e m p } }$ . Finally, the output of temporal attention layer is $\\begin{array} { r } { { \\bf E } _ { \\mathcal { V } } ^ { \\prime } = \\mathrm { { S o f t m a x } } ( \\frac { Q ^ { \\top } K } { \\sqrt { c } } ) \\cdot { \\bf E } _ { \\mathcal { V } } ^ { \\mathrm { t e m p } } } \\end{array}$ . We utilize a learnable mixing coefficient $\\eta$ to conduct residual connection:\n\n$$\n\\mathbf { E } _ { \\mathcal { V } } = \\boldsymbol { \\eta } \\cdot \\mathbf { E } _ { \\mathcal { V } } ^ { t e m p } + \\left( 1 - \\boldsymbol { \\eta } \\right) \\cdot \\mathbf { E } _ { \\mathcal { V } } ^ { \\prime } .\n$$\n\nBased on the above design, PR generates the blurry rough video with initial smoothness and great consistency, laying the foundation for subsequent video reconstruction.\n\n# 3.2 Semantics Reconstructor\n\nRecent cognitive neuroscience studies [46, 47] argue that ‚Äôkey-frames‚Äô play a crucial role in how the human brain recalls and connects relevant memories with unfolding events, and other research [48, 49] also demonstrates that video key-frames can be used as representative features of the entire video clip. Building on these conclusions, the core objective of Semantics Reconstructor (SR) is to reconstruct a high-quality keyframe image that can be used to address the issue of frame rate mismatch between visual stimuli and fMRI signals, thereby enhancing the fidelity of the final exquisite video. The existing fMRI-to-image reconstruction studies [1, 2, 4] facilitate our objective, detailed below:\n\nfMRI Low-dimensional Processing. For each clip $c$ , its corresponding fMRI signal is ${ { \\mathcal { V } } _ { c } }$ . We use ridge regression to map ${ \\mathcal { V } } _ { c }$ to a lower-dimensional ${ \\mathcal { V } } _ { c } ^ { \\prime }$ for easier follow-up:\n\n$$\n\\mathcal { V } _ { c } ^ { \\prime } = X ( X ^ { T } X + \\lambda I ) ^ { - 1 } X ^ { T } \\mathcal { V } _ { c } ,\n$$\n\nwhere $X$ is design matrix, $\\lambda$ is regularization parameter, and $I$ is identity matrix. Although the human brain processes information in a highly complex and non-linear way, empirical evidence [1, 26, 2] underscores the effectiveness and sufficiency of linear mapping for achieving desirable reconstruction, due to nonlinear models will easily overfit to fMRI noise and then lead to poor performance [50].\n\nAlignment of Keyframe Image with fMRI. Randomly choose one frame in the clip $c$ as its keyframe $\\mathcal { X } _ { c }$ , and use OpenCLIP ViT-bigG/14 [51] to obtain $e _ { \\mathcal { X } _ { c } }$ , the embedding of keyframe image $\\mathcal { X } _ { c }$ in CLIP image space. $e _ { \\mathcal { Y } _ { c } }$ is the fMRI embedding of ${ \\mathcal { V } } _ { c } ^ { \\prime }$ via another MLP. Consequently, we perform contrastive learning between $e _ { \\mathcal { X } _ { c } }$ and $e _ { \\mathcal { { y } _ { c } } }$ to align the keyframe image $\\mathcal { X } _ { c }$ and the fMRI ${ \\mathcal { V } } _ { c }$ , resulting in enhancing the semantics of $e _ { \\mathcal { Y } _ { c } }$ . It is worth noting that the MLP gets a bidirectional contrastive loss. Previous research [1] has demonstrated that introducing MixCo [52] data augmentation, an extension of mixup utilizing the InfoNCE loss, can effectively help model convergence, especially for scarce fMRI samples. Therefore, the bidirectional loss called BiMixCo $\\mathcal { L } _ { \\mathrm { B i M i x C o } }$ , which combines MixCo and contrastive loss, needs to be used for training.\n\nGeneration of Reconstruction-Embedding. Since the embeddings in the CLIP ViT image space are more approximate to real images compared to fMRI embeddings, transforming fMRI embedding $e y _ { c }$ into CLIP ViT‚Äôs image embedding will significantly benefit the reconstruction quality of the keyframe. Therefore, we have to generate the reconstruction-embedding $e _ { \\mathcal { X } _ { c } } ^ { r e }$ for the keyframe image $\\mathcal { X } _ { c }$ , essentially, which is the image embedding that will be fed to the subsequent generative model for reconstruction. Inspired by DALLE¬∑2 [37], diffusion prior is an effective approach to transforming embedding. So, we map the fMRI embedding $e y _ { c }$ to the OpenCLIP ViT-bigG/14 image space to generate $e _ { \\mathcal { X } _ { c } } ^ { r e }$ . Here, we use the same prior loss ${ \\mathcal { L } } _ { \\mathrm { P r i o r } }$ in DALLE¬∑2 [37] for training.\n\nReconstruction Enhancement from Text Modality. Original fMRI-to-image reconstruction only relies on visual modality embedding. For instance, reconstructing images conditional on the image embeddings generated by diffusion prior. However, text is another critical modality. Incorporating text with higher semantic density can help improve the semantic content of reconstruction embedding, resulting in making semantics reconstruction more straightforward and effective. We adopt BLIP2 [53] to introduce the text modality, i.e., the caption $\\mathcal { T } _ { c }$ of the keyframe images $\\mathcal { X } _ { c }$ . Then, we embed $\\mathcal { T } _ { c }$ to obtain the text embedding $e _ { T _ { c } }$ . Inspired by contrastive learning, we perform contrastive learning between $e _ { \\mathcal { X } _ { c } } ^ { r e }$ and $e _ { T _ { c } }$ to enhance reconstruction-embedding $e _ { \\mathcal { X } _ { c } } ^ { r e }$ via additional text modality. The contrastive loss serves as the training loss ${ \\mathcal { L } } _ { \\mathrm { R e f t m } }$ of this process, similar to Equation 1, omitted here.\n\nTraining Loss. As discussed above, the overall training loss $\\mathcal { L } _ { S R }$ in SR is composite. Therefore, We set mixing coefficients $\\delta$ and $\\mu$ to balance multiple losses:\n\n$$\n\\begin{array} { r } { \\mathcal { L } _ { S R } = \\mathcal { L } _ { \\mathrm { B i M i x C o } } + \\delta \\mathcal { L } _ { \\mathrm { P r i o r } } + \\mu \\mathcal { L } _ { \\mathrm { R e f t m } } . } \\end{array}\n$$\n\n# 3.3 Inference Process\n\nThe inference of NeuroClips is the process of fMRI-to-video reconstruction. We jointly utilize the blurry rough video, the high-quality keyframe image, and the additional text modality, which are $\\alpha$ , $\\beta$ , and $\\gamma$ guidance, to reconstruct the final exquisite video with high fidelity, smoothness, and consistency. And we employ a text-to-video diffusion model to help reconstruct video.\n\nText-to-video Diffusion Model. Pre-training text-to-video (T2V) diffusion models possess a significant amount of prior knowledge from the graphics, image, and video domains. However, like other diffusion models, they face huge challenges in achieving controllable generation. Therefore, directly using the text corresponding to fMRI to reconstruct videos will result in unsatisfactory outcomes, as the semantics of embeddings only originate from the text modality. We also need to enhance the embeddings with semantics from the video and image modalities to produce \"composite semantics\" embeddings, which aid in achieving controllable generation for the T2V diffusion model.\n\n$\\alpha$ Guidance. We consider the blurry rough-video $\\mathbf { V } _ { b l u r r y }$ output from PR as $\\alpha$ Guidance. Treating $\\mathbf { V } _ { b l u r r y }$ as an intermediate noisy video between target video $\\mathbf { V } _ { 0 }$ and noise video $\\mathbf { V } _ { T }$ , the originally required $T$ steps for the complete forward process can now be reduced to $\\vartheta T$ steps. By applying the latent space translation and reparameterization trick, noise $z _ { T }$ can be formalized as:\n\n$$\nz _ { T } = \\sqrt { \\bar { \\alpha } _ { T } / \\bar { \\alpha } _ { \\vartheta T } } z _ { b l u r r y } + \\sqrt { 1 - \\bar { \\alpha } _ { T } / \\bar { \\alpha } _ { \\vartheta T } } \\epsilon , \\quad \\bar { \\alpha } _ { T } = \\prod _ { t = 1 } ^ { T } \\alpha _ { t } , \\quad \\bar { \\alpha } _ { \\vartheta T } = \\prod _ { t = 1 } ^ { \\vartheta T } \\alpha _ { t } ,\n$$\n\nwhere $\\alpha _ { t }$ represents the noise schedule parameter at time step $t$ and $\\epsilon \\sim \\mathcal { N } ( 0 , 1 )$ is Gaussian noise.   \nThe reverse process involves iteratively denoising the noise video from $T$ steps back to 0 steps.\n\nAdopting a pretrained T2V diffusion model $p _ { \\theta }$ to predict the mean and variance of the denoising distribution at each step:\n\n$$\nz _ { t - 1 } \\sim p _ { \\theta } ( z _ { t - 1 } | z _ { t } ) = \\mathcal { N } ( z _ { t - 1 } ; \\mu _ { \\theta } ( z _ { t } , t ) , \\Sigma _ { \\theta } ( z _ { t } , t ) ) ,\n$$\n\nwhere $t = T , T - 1 , . . . , 1$ . After translating $z _ { 0 }$ to pixel space, the reconstructed video ${ \\bf V } _ { 0 }$ is obtained.\n\n$\\beta$ Guidance. $\\alpha$ Guidance only directs the video generation of the T2V diffusion model at the perception level, leading to significant randomness in the semantics of the reconstructed videos. To resolve this issue, we need to incorporate keyframe images with more supplementary semantics to control the generation process, thereby enhancing the fidelity of the reconstructed videos. Compared to directly reconstructing keyframe images from fMRI embeddings, combining perception embeddings will be more beneficial for maintaining the consistency of structural and semantic information. Therefore, we select the first frame $\\nu _ { 1 }$ of blurry $\\mathbf { V } _ { b l u r r y }$ . Input $\\nu _ { 1 }$ ‚Äôs embedding and fMRI embedding to SDXL unCLIP [2] (See Appendix $\\mathrm { ~ D ~ }$ for more discussion about SDXL unCLIP) in SR to reconstruct the keyframe image $\\mathcal X _ { k e y }$ as $\\beta$ Guidance. We employ ControlNet [54] to add $\\beta$ Guidance to the T2V diffusion model, in which the keyframes are used as the first-frame to guide video generation.\n\n$\\gamma$ Guidance. The text is the necessary input for the T2V diffusion model. In order to maintain the consistency of visual semantics, we adopt BLIP-2 [53] to generate the caption for the keyframe image $\\mathcal { X } _ { k e y }$ , which is used as $\\gamma$ Guidance (prompt) for video reconstruction.\n\nThe inference process inputs $\\alpha$ , $\\beta , \\gamma$ Guidance into the T2V diffusion model, and the fMRI-to-video reconstruction can be completed, resulting in the exquisite video with high fidelity and smoothness.\n\n# 3.4 Multi-fMRI Fusion\n\nWhile it is important to emphasize that single-frame fMRI generates higher frame rate video, the more realistic question is how to recover longer video (longer than fMRI temporal resolution). Previous methods treat single-frame fMRI as a sample, and temporal attention is computed at the single-frame fMRI level, thus failing to generate coherent videos longer than 2s. With the help of NeuroClips‚Äô SR, we explored the generation of longer videos for the first time. Current video generative models are built on diffusion-based image generation models and attention-based transformer architectures, both of which incur significant computational overhead. As the number of frames increases, the content scales linearly, highlighting the limitations in generating long and complex videos efficiently. Therefore, we chose a more straightforward fusion strategy that does not require additional GPU training. In the inference process, we consider the semantic similarity of two reconstructed keyframes from two neighboring fMRI samples (here we directly determine whether they belong to the same class of objects, e.g., both are jellyfish). Specifically, we obtain the CLIP representations of reconstructed neighboring keyframes and train a shallow MLP based on the representations to distinguish whether two frames share the same class. If semantically similar, we replace the keyframe of the latter fMRI with the tail-frame of the former fMRI‚Äôs reconstructed video, which will be taken as the first-frame of the latter fMRI to generate the video. As shown in Figure 2, with this strategy, we achieved continuous video reconstruction of up to 6s for the first time.\n\n![](images/5956e1f23dbe16f2427e7d388c1b3a8f32f51c5816e11ad2aad5c3cc8ccc279d.jpg)  \nFigure 2: Visualization of Multi-fMRI fusion. With the semantic relevance measure, we can generate video clips up to 6s long without any additional training.\n\n# 4 Experimental Setup\n\n# 4.1 Dataset and Pre-processing\n\nDataset. In this study, we performed fMRI-to-video reconstruction experiments using the opensource fMRI-video dataset (cc2017 dataset1) [31]. For each subject, the training and testing video clips were presented 2 and 10 times, respectively, and the testing set was averaged across trials. The dataset consists of a training set containing 18 8-minute video clips and a test set containing 5 8-minute video clips. The MRI (T1 and T2-weighted) and fMRI data (with 2s temporal resolution) were collected using a 3-T MRI system. Thus there are 8640 training samples and 1200 testing samples of fMRI-video pairs.\n\nPre-processing. The fMRI data on the $\\mathrm { c c } 2 0 1 7$ were preprocessed using the minimal preprocessing pipeline [55]. The fMRI volumes underwent artifact removal, motion correction (6 DOF), registration to standard space (MNI space), and were further transformed onto the cortical surfaces, which were coregistered onto a cortical surface template [56]. We calculated the voxel-wise correlation between the fMRI voxel signals of each training movie repetition for each subject. The correlation coefficient for each voxel underwent Fisher $z$ -transformation, and the average z scores across 18 training movie segments were tested using the one-sample t-test. The significant voxels (Bonferroni correction, $\\mathbf { P } <$ 0.05) were considered to be stimulus-activated voxels and used for subsequent analysis A total of 13447, 14828, and 9114 activated voxels were observed in the visual cortex for the three subjects. Following the previous works [3, 57, 58], we used the BOLD signals with a delay of 4 seconds as the movie stimulus responses to account for the hemodynamic delay.\n\n# 4.2 Implementation Details\n\nIn this paper, videos from the cc2017 dataset were downsampled from 30FPS to 3FPS to make a fair comparison with the previous methods, and the blurred video was interpolated to 8FPS to generate the final 8FPS video during inference. We split semantic reconstruction into two parts: image contrastive learning and the fine-tuning of the diffusion prior to adapting to the new image distribution space. In PR, all downsampled frames were utilized, and the inception extension was implemented by a shallow MLP. Theoretically, our approach can be used in any text-to-video diffusion model, and we choose the open-source available AnimateDiff [44] as our inference model. $\\vartheta$ is set to 0.3 in $\\alpha$ Guidance, and the inference is performed with 25 DDIM [35] steps (See Appendix A for more implementation details). All experiments were conducted using a single A100 GPU.\n\n# 4.3 Evaluation Metrics\n\nWe conduct the quantitative assessments through frame-based and video-based metrics. Frame-based metrics evaluate each frame individually, providing a snapshot evaluation, whereas video-based metrics evaluate the quality of the video, emphasizing the consistency and smoothness of the video frame sequence. Both are used for a comprehensive analysis from a semantic or pixel perspective.\n\nFrame-based Metrics We evaluate frames at the pixel level and semantic level. We use the structural similarity index measure (SSIM) and Peak Signal-to-Noise Ratio (PSNR) as the pixel-level metric and the N-way top-K accuracy classification test (total 1,000 image classes from ImageNet [59]) as the semantics-level metric. To conduct the classification test, we essentially compare the ground truth (GT) classification results with the predicted frame (PF) results using an ImageNet classifier [30, 8]. We consider the trial successful if the GT class is among the top-K probabilities in the PF classification results (We used top-1), selected from N randomly chosen classes that include the GT class. The reported success rate is based on the results of 100 repeated tests.\n\nVideo-based Metrics We evaluate videos at the semantic level and spatiotemporal(ST)-level. For semantic-level metrics, a similar classification test (total 400 video classes from the Kinetics-400 dataset [60]) is used above, with a video classifier based on VideoMAE [61]. For spatiotemporal-level metrics that measure video consistency, we compute CLIP image embeddings on each frame of the predicted videos and report the average cosine similarity between all pairs of adjacent video frames, which is the common metric CLIP-pcc in video editing.\n\n![](images/bee9f6c7412fbffd98e1b955533ab46934541cd04d0b74a46a794edea3c5aa08.jpg)  \nFigure 3: Video reconstruction on the cc2017 dataset. On the left are the results of the comparison with previous studies, and on the right are additional comparisons with previous SOTA methods. Best viewed with zoom-in. As shown in the leftmost figure group, Mind-Video‚Äôs reconstruction fails to go for detail consistency on the character‚Äôs face, but our NeuroClips achieves an extremely high consistency.\n\nTable 1: Quantitative comparison of NeuroClips reconstruction performance against other methods. Bold font signifies the best performance, while underlined text indicates the second-best performance. MinD-Video and NeuroClips are both results averaged across all three subjects, and the other methods are results from subject 1. Results of baselines are quoted from [62].   \n\n<html><body><table><tr><td rowspan=\"3\">Method</td><td colspan=\"3\">Video-based</td><td colspan=\"4\">Frame-based</td></tr><tr><td colspan=\"2\">Semantic-level</td><td>ST-level</td><td colspan=\"2\">Semantic-level</td><td colspan=\"2\">Pixel-level</td></tr><tr><td>2-way‚Üë</td><td>50-way‚Üë</td><td>CLIP-pcc‚Üë</td><td>2-way‚Üë</td><td>50-way‚Üë</td><td>SSIM‚Üë</td><td>PSNR‚Üë</td></tr><tr><td>Nishimoto [3]</td><td>-</td><td>=</td><td>1</td><td>0.727¬±0.04</td><td></td><td>0.116¬±0.09</td><td>8.012¬±2.31</td></tr><tr><td>Wen[31]</td><td></td><td>0.166¬±0.02</td><td></td><td>0.758¬±0.03</td><td>0.070¬±0.01</td><td>0.114¬±0.15</td><td>7.646¬±3.48</td></tr><tr><td>Wang [32]</td><td>0.773¬±0.03</td><td></td><td>0.402¬±0.41</td><td>0.713¬±0.04</td><td></td><td>0.118¬±0.08</td><td>11.432¬±2.42</td></tr><tr><td>Kupershmidt [33]</td><td>0.771¬±0.03</td><td></td><td>0.386¬±0.47</td><td>0.764¬±0.03</td><td>0.179¬±0.02</td><td>0.135¬±0.08</td><td>8.761¬±2.22</td></tr><tr><td>MinD-Video [8]</td><td>0.839¬±0.03</td><td>0.197¬±0.02</td><td>0.408¬±0.46</td><td>0.796¬±0.03</td><td>0.174¬±0.03</td><td>0.171¬±0.08</td><td>8.662¬±1.52</td></tr><tr><td> NeuroClips</td><td>0.834¬±0.03</td><td>0.220¬±0.01</td><td>0.738¬±0.17</td><td>0.806¬±0.03</td><td>0.203¬±0.01</td><td>0.390¬±0.08</td><td>9.211¬±1.46</td></tr><tr><td>subject 1</td><td>0.830¬±0.03</td><td>0.208¬±0.01</td><td>0.736¬±0.12</td><td>0.799¬±0.03</td><td>0.187¬±0.01</td><td>0.392¬±0.08</td><td>9.226¬±1.42</td></tr><tr><td> subject 2</td><td>0.837¬±0.03</td><td>0.230¬±0.01</td><td>0.742¬±0.19</td><td>0.811¬±0.03</td><td>0.210¬±0.01</td><td>0.392¬±0.08</td><td>9.336¬±1.52</td></tr><tr><td> subject 3</td><td>0.835¬±0.03</td><td>0.221¬±0.01</td><td>0.735¬±0.20</td><td>0.807¬±0.03</td><td>0.213¬±0.01</td><td>0.387¬±0.09</td><td>9.072¬±1.44</td></tr></table></body></html>\n\n# 5 Results\n\nIn this section, we compare NeuroClips with previous video reconstruction methods on the cc2017 dataset. We provide a visual comparison in Figure 3 and report quantitative metrics in Table 1. We purposely focus on the comparison with the previous SOTA method on the right side of Figure 3 due to the lack of obvious semantic information in the premature method. All methods report results for all test sets except Wen [31], whose results are available for only one segment. Our method generates videos at 8fps and even higher frame rates. For a fair comparison with previous studies, we downsampled the 8FPS videos to 3FPS. Unless otherwise noted, the experimental results and visualizations shown below are all at 3FPS.\n\nAs can be seen in Figure 3, earlier methods were unable to produce videos with complete semantics but guaranteed some of the low-level visual features. Compared to MinD-Video, our NeuroClips generates single-frame images with higher quality, more precise semantics (e.g., people, turtles, and airplanes), and smoother movements. At the same time, due to the limited data in the training set, some objects in the test set videos did not appear in the training set, such as the moon, and perfect reproduction of the semantics is difficult. However, thanks to our perception reconstructor (aka $\\alpha$ Guidance), NeuroClips basically reproduces the shape and motion trajectory of the moon, although semantically it is more similar to the aperture, demonstrating the pixel-level reconstruction ability of the video. In terms of quantitative metrics, NeuroClips significantly outperformed 5 of the 7 metrics, with a $12 8 \\%$ improvement in SSIM performance, indicating that the PR complements the lack of pixel-level control. At the semantic level, our metrics overall outperform previous methods, demonstrating the better semantic alignment paradigm of NeuroClips. For the ST-level metric, which evaluates video smoothness, NeuroClips substantially outperforms MinD-Video because we introduce blurry rough-video (aka $\\alpha$ Guidance) for the frozen video generation model, incorporating initial smoothness for video reconstruction. In contrast, MinD-Video lacks perception control, resulting in a substantial decrease in smoothness, as can also be seen in Figure 3, where the human deformations and scene switches are too large within an fMRI frame. In addition, benefiting from our keyframes for first-frame guidance (aka $\\beta$ Guidance) and blurry videos, we can connect semantically similar videos to generate longer videos, which may be the reason for the slightly lower video 2-way metrics, as neighboring reconstructed videos are more similar after multi-fMRI fusion.\n\n# 6 Ablations\n\nIn this section, we discuss the impact of three important intermediate processes on video reconstruction, including keyframes, blurry videos, and keyframe captioning. The quantitative results are in Table 2 and the visualization results are shown in Figure 4, where all the results of the ablation experiments are from subject 1. Since the keyframe captioning must be obtained through the keyframe, by default we eliminate keyframe and keyframe captioning at the same time. From the quantitative results, it can be seen that NeuroClips exhibits better pixel-level metrics without keyframes while showing better semantic-level metrics without blurry clips. This indicates a trade-off between semantic and perception reconstruction, which is similar to the results of a large body of literature on fMRI-to-image reconstruction [1]. The perception reconstruction improves the pixel-level performance and the semantic reconstruction improves the semantic metrics. In addition to this, the best ST-level results for the full model demonstrate the contribution of each module to video consistency.\n\n![](images/45fdd06aed88292480a8decc7a66b1b6237d7f21f927acd534668b88acd6bab5.jpg)  \nFigure 4: Visualization of ablation study.\n\nTable 2: Ablations on the modules of NeuroClips, and all results are from subject 1.   \n\n<html><body><table><tr><td rowspan=\"3\">Method</td><td colspan=\"3\">Video-based</td><td colspan=\"4\">Frame-based</td></tr><tr><td colspan=\"2\">Semantic-level</td><td>ST-level</td><td colspan=\"2\">Semantic-level</td><td colspan=\"2\">Pixel-level</td></tr><tr><td>2-way‚Üë</td><td>50-way‚Üë</td><td>CLIP-pcc‚Üë</td><td>2-way‚Üë</td><td>50-way‚Üë</td><td>SSIM‚Üë</td><td>PSNR‚Üë</td></tr><tr><td>w/o keyframe</td><td>0.751¬±0.04</td><td>0.164¬±0.02</td><td>0.695¬±0.15</td><td>0.702¬±0.04</td><td>0.128¬±0.01</td><td>0.413¬±0.09</td><td>9.334¬±1.30</td></tr><tr><td>w/o blurry clip</td><td>0.838¬±0.03</td><td>0.213¬±0.01</td><td>0.718¬±0.11</td><td>0.805¬±0.03</td><td>0.193¬±0.01</td><td>0.256¬±0.11</td><td>8.989¬±1.37</td></tr><tr><td>GIT captioning</td><td>0.828¬±0.03</td><td>0.195¬±0.01</td><td>0.728¬±0.12</td><td>0.785¬±0.03</td><td>0.174¬±0.01</td><td>0.399¬±0.08</td><td>9.297¬±1.43</td></tr><tr><td> NeuroClips</td><td>0.830¬±0.03</td><td>0.208¬±0.01</td><td>0.736¬±0.12</td><td>0.799¬±0.03</td><td>0.187¬±0.01</td><td>0.392¬±0.08</td><td>9.226¬±1.42</td></tr></table></body></html>\n\nFor the image captioning method, previous studies [2] have used the GIT [63] model to generate keyframe captions directly from fMRI embeddings in image space, and we generated it from BLIP2 [53]. Here we compare the text generation results of these two approaches as shown in the Figure 4. We found that GIT‚Äôs text generation is slightly homogeneous, filled with a large number of similar descriptions, such as ‚Äôa large body of water‚Äô, ‚Äôa man is standing‚Äô. For the diffusion model, the influence of text is significant, so GIT‚Äôs captions degrade the quality of the semantic reconstruction of the video, e.g., generating flowers on water from jellyfish keyframes. This shows that keyframe-based captioning is more flexible compared to representation-based captioning. Finally, we remove the keyframes and keyframe captions and use only blurred videos to guide the reconstruction, with the text input replaced with the generic description ‚Äôa smooth video‚Äô. With this approach, we find that the model generates the video completely blindly, with poor semantic control, demonstrating the strong semantic support that keyframes bring to the NeuroClips generation capability. More ablation analysis can be found in Appendix C.4.\n\n# 7 Interpretation Results\n\nTo explore the neural interpretability of the model, we visualized voxel-level weights on a brain flat map, where we can see comprehensive structural attention throughout the whole region, as shown in Figure 5. It can be seen that the visual cortex occupies an important position regardless of the task. In addition, for semantic-level reconstruction, the weight distribution of voxels is more spread out over the higher visual cortex, indicating the semantic level of the video. For perceptual reconstruction, the weight distribution of voxels is more concentrated on the lower visual cortex, corresponding to the low-level perception of the human brain. See Appendix C.5 for more subjects‚Äô interpretation results.\n\n![](images/9455f37c6abe85fba8f617e44ef2fce38892c2560535f2eb768f4b7c3faf2e36.jpg)  \nFigure 5: Visualization of voxel weights for the first ridge regression layer for subject 1, with each voxel‚Äôs weight averaged and normalized to between 0 and 1 and we set the colorbar to 0.25-0.75 for a clear comparison.\n\n# 8 Conclusion\n\nIn this paper, we present NeuroClips, a novel framework for fMRI-to-video reconstruction. we implement pixel-level and semantic-level visual learning of fMRI through two paths: perception reconstruction and semantic reconstruction. With the learned components, we can configure them into the latest video diffusion models to generate higher quality, higher frame rate, and longer videos without additional training. NeuroClips recovers videos with more accurate semantic-level precision and degree of pixel-level matching, establishing a new state-of-the-art in this domain. In addition to this, we visualized the neuroscience interpretability of NeuroClips with reliable biological principles.\n\n# 9 Limitations\n\nAlthough NeuroClips has achieved high-fidelity, smooth, and consistent multi-fMRI to video reconstruction, there are still slight flaws. Specifically, our framework is slightly bulky and it relies on extending keyframes to the reconstructed video. A model that can reconstruct videos from the CLIP latent space will avoid this intermediate process. Unfortunately, there is no such available model now. In addition, our method does not reconstruct cross-scene fMRI well, i.e., fMRI recorded during video clip switching. Even if such fMRI scans are in a tiny minority, this will be a direction for future research. Moreover, additional subjects and fMRI recordings should be considered in order to reflect real-world visual experiences sufficiently. However, The alleviation of these limitations will require joint advances in multiple areas and significant further effort will be required. This is because improvements in these areas need to be supported by common developments including machine learning, computer vision, brain science, and biomedicine.\n\n# Ethics Statements\n\nThe dataset paper [31] states that informed written consent was obtained from every study participant according to the research protocol approved by the Institutional Review Board at Purdue University.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáËß£ÂÜ≥‰∫Ü‰ªéÂäüËÉΩÊÄßÁ£ÅÂÖ±ÊåØÊàêÂÉèÔºàfMRIÔºâÈáçÂª∫È´ò‰øùÁúüÂ∫¶ÂíåÂπ≥ÊªëËßÜÈ¢ëÁöÑÊåëÊàò„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Ëß£Á†ÅËøûÁª≠ËßÜËßâ‰ΩìÈ™åÁöÑÊó∂Á©∫ÊÑüÁü•ÊñπÈù¢Â≠òÂú®ÊòæËëóÂõ∞ÈöæÔºåÂ∞§ÂÖ∂ÊòØÂú®È´òÂ∏ßÁéáËßÜÈ¢ëÈáçÂª∫‰∏≠ËØ≠‰πâÂáÜÁ°ÆÊÄßÂíå‰ΩéÁ∫ßÂà´ÊÑüÁü•ÁªÜËäÇÁöÑÂπ≥Ë°°ÈóÆÈ¢ò„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éÔºåÂÆÉËÉΩÂ§üÂ∏ÆÂä©ÁêÜËß£Â§ßËÑëÂ¶Ç‰ΩïÂ§ÑÁêÜËøûÁª≠ËßÜËßâ‰ø°ÊÅØÔºåÂπ∂Âú®ÂåªÁñóËØäÊñ≠„ÄÅËÑëÊú∫Êé•Âè£Á≠âÈ¢ÜÂüüÂÖ∑ÊúâÊΩúÂú®Â∫îÁî®‰ª∑ÂÄº„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ËÆ∫ÊñáÊèêÂá∫NeuroClipsÊ°ÜÊû∂ÔºåÈÄöËøáËØ≠‰πâÈáçÂª∫Âô®ÔºàSemantics ReconstructorÔºâÂíåÊÑüÁü•ÈáçÂª∫Âô®ÔºàPerception ReconstructorÔºâÂàÜÂà´Ëß£Á†ÅÈ´òÁ∫ßËØ≠‰πâÂíå‰ΩéÁ∫ßÂà´ÊÑüÁü•ÊµÅÔºåÁªìÂêàÈ¢ÑËÆ≠ÁªÉÁöÑT2VÊâ©Êï£Ê®°ÂûãÂÆûÁé∞È´ò‰øùÁúüÂ∫¶ÂíåÂπ≥ÊªëÁöÑËßÜÈ¢ëÈáçÂª∫„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ1Ôºö** ÊèêÂá∫ÂèåË∑ØÂæÑÈáçÂª∫Ê°ÜÊû∂ÔºåÂàÜÂà´Â§ÑÁêÜËØ≠‰πâÂíåÊÑüÁü•‰ø°ÊÅØÔºåÊòæËëóÊèêÂçáËßÜÈ¢ëÈáçÂª∫Ë¥®Èáè„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ2Ôºö** ÂºïÂÖ•Â§öfMRIËûçÂêàÁ≠ñÁï•ÔºåÈ¶ñÊ¨°ÂÆûÁé∞ÈïøËææ6ÁßíÁöÑËßÜÈ¢ëÈáçÂª∫Ôºà8FPSÔºâ„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ3Ôºö** ÈÄöËøáŒ±„ÄÅŒ≤„ÄÅŒ≥‰∏âÁßçÂºïÂØºÊú∫Âà∂ÔºåÁªìÂêàÊ®°Á≥äËßÜÈ¢ë„ÄÅÂÖ≥ÈîÆÂ∏ßÂíåÊñáÊú¨ÊèèËø∞ÔºåÊèêÂçáËßÜÈ¢ëÁöÑÂπ≥ÊªëÊÄßÂíåËØ≠‰πâÂáÜÁ°ÆÊÄß„ÄÇ\\n> *   **ÂÖ≥ÈîÆÊï∞ÊçÆÔºö** Âú®SSIMÊåáÊ†á‰∏äÊèêÂçá128%Ôºà‰ªé0.171Âà∞0.390ÔºâÔºåÊó∂Á©∫ÊåáÊ†áÔºàCLIP-pccÔºâ‰∏äÊèêÂçá81%Ôºà‰ªé0.408Âà∞0.738Ôºâ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   NeuroClipsÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÈÄöËøáÂàÜÁ¶ªËØ≠‰πâÂíåÊÑüÁü•‰ø°ÊÅØÁöÑËß£Á†ÅÔºåÊ®°ÊãüÂ§ßËÑëÂ§ÑÁêÜËßÜÈ¢ëÂà∫ÊøÄÁöÑÊñπÂºè„ÄÇËØ≠‰πâÈáçÂª∫Âô®‰∏ìÊ≥®‰∫éÂÖ≥ÈîÆÂ∏ßÁöÑÈ´òË¥®ÈáèÈáçÂª∫ÔºåËÄåÊÑüÁü•ÈáçÂª∫Âô®ÂàôÁîüÊàêÊ®°Á≥ä‰ΩÜËøûÁª≠ÁöÑÁ≤óÁ≥ôËßÜÈ¢ëÔºåÁ°Æ‰øùÂπ≥ÊªëÊÄßÂíå‰∏ÄËá¥ÊÄß„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** ÂÖàÂâçÊñπÊ≥ïÔºàÂ¶ÇMinD-VideoÔºâÁº∫‰πè‰ΩéÁ∫ßÂà´ËßÜËßâÁªÜËäÇÁöÑËÆæËÆ°ÔºåÂØºËá¥ËßÜÈ¢ëÂπ≥ÊªëÊÄß‰∏çË∂≥„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** NeuroClipsÈÄöËøáÊÑüÁü•ÈáçÂª∫Âô®ÊçïËé∑‰ΩéÁ∫ßÂà´ÊÑüÁü•ÊµÅÔºåÂπ∂ÁªìÂêàËØ≠‰πâÈáçÂª∫Âô®ÁöÑÂÖ≥ÈîÆÂ∏ßÔºåÊòæËëóÊèêÂçáËßÜÈ¢ëÁöÑÂπ≥ÊªëÊÄßÂíåËØ≠‰πâÂáÜÁ°ÆÊÄß„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.  **ÊÑüÁü•ÈáçÂª∫Âô®ÔºàPRÔºâÔºö** ÈÄöËøáInception ExtensionÂíåTemporal UpsamplingÊ®°ÂùóÂØπÈΩêfMRI‰∏éËßÜÈ¢ëÂ∏ßÔºåÁîüÊàêÊ®°Á≥äËßÜÈ¢ë„ÄÇ\\n> 2.  **ËØ≠‰πâÈáçÂª∫Âô®ÔºàSRÔºâÔºö** ‰ΩøÁî®Êâ©Êï£ÂÖàÈ™åÂíåÂØπÊØîÂ≠¶‰π†Â∞ÜfMRIÂµåÂÖ•Êò†Â∞ÑÂà∞CLIPÂõæÂÉèÁ©∫Èó¥ÔºåÈáçÂª∫È´òË¥®ÈáèÂÖ≥ÈîÆÂ∏ß„ÄÇ\\n> 3.  **Êé®ÁêÜËøáÁ®ãÔºö** ÁªìÂêàÊ®°Á≥äËßÜÈ¢ëÔºàŒ±ÂºïÂØºÔºâ„ÄÅÂÖ≥ÈîÆÂ∏ßÔºàŒ≤ÂºïÂØºÔºâÂíåÊñáÊú¨ÊèèËø∞ÔºàŒ≥ÂºïÂØºÔºâÔºåÈÄöËøáT2VÊâ©Êï£Ê®°ÂûãÁîüÊàêÊúÄÁªàËßÜÈ¢ë„ÄÇ\\n> 4.  **Â§öfMRIËûçÂêàÔºö** ÈÄöËøáËØ≠‰πâÁõ∏‰ººÊÄßÂà§Êñ≠ÔºåÂ∞ÜÁõ∏ÈÇªfMRIÁöÑÂÖ≥ÈîÆÂ∏ßÊõøÊç¢‰∏∫Ââç‰∏ÄfMRIÁöÑÂ∞æÂ∏ßÔºåÂÆûÁé∞ÈïøËææ6ÁßíÁöÑËßÜÈ¢ëÈáçÂª∫„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   Nishimoto [3], Wen [31], Wang [32], Kupershmidt [33], MinD-Video [8]\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®SSIMÊåáÊ†á‰∏äÔºö** NeuroClipsÂú®cc2017Êï∞ÊçÆÈõÜ‰∏äËææÂà∞‰∫Ü0.390ÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãMinD-Video (0.171)ÂíåWang [32] (0.118)„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü128%„ÄÇ\\n> *   **Âú®Êó∂Á©∫ÊåáÊ†áÔºàCLIP-pccÔºâ‰∏äÔºö** NeuroClipsÁöÑÂæóÂàÜ‰∏∫0.738ÔºåËøúÈ´ò‰∫éMinD-Video (0.408)ÂíåKupershmidt [33] (0.386)ÔºåÊèêÂçá‰∫Ü81%„ÄÇ\\n> *   **Âú®ËØ≠‰πâÁ∫ßÊåáÊ†áÔºà50-wayÔºâ‰∏äÔºö** NeuroClipsÁöÑÂæóÂàÜ‰∏∫0.220Ôºå‰ºò‰∫éMinD-Video (0.197)ÂíåKupershmidt [33] (0.179)„ÄÇ\\n> *   **Âú®PSNRÊåáÊ†á‰∏äÔºö** NeuroClipsÁöÑÂæóÂàÜ‰∏∫9.211Ôºå‰ºò‰∫éMinD-Video (8.662)ÂíåWang [32] (11.432)„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   ÂäüËÉΩÊÄßÁ£ÅÂÖ±ÊåØÊàêÂÉè (Functional Magnetic Resonance Imaging, fMRI)\\n*   ËßÜÈ¢ëÈáçÂª∫ (Video Reconstruction, N/A)\\n*   ËØ≠‰πâÈáçÂª∫Âô® (Semantics Reconstructor, SR)\\n*   ÊÑüÁü•ÈáçÂª∫Âô® (Perception Reconstructor, PR)\\n*   Êâ©Êï£Ê®°Âûã (Diffusion Model, DM)\\n*   Â§öfMRIËûçÂêà (Multi-fMRI Fusion, N/A)\\n*   Êó∂Á©∫ÊåáÊ†á (Spatiotemporal Metrics, ST-metrics)\\n*   ËÑëÊú∫Êé•Âè£ (Brain-Computer Interface, BCI)\"\n}\n```"
}