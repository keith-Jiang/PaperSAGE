{
    "source": "Semantic Scholar",
    "arxiv_id": "2312.03701",
    "link": "https://arxiv.org/abs/2312.03701",
    "pdf_link": "https://arxiv.org/pdf/2312.03701.pdf",
    "title": "Return of Unconditional Generation: A Self-supervised Representation Generation Method",
    "authors": [
        "Tianhong Li",
        "Dina Katabi",
        "Kaiming He"
    ],
    "categories": [
        "cs.CV"
    ],
    "publication_date": "2023-12-06",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 31,
    "influential_citation_count": 2,
    "institutions": [
        "MIT CSAIL"
    ],
    "paper_content": "# Return of Unconditional Generation: A Self-supervised Representation Generation Method\n\nTianhong Li\n\nDina Katabi\n\nKaiming He\n\nMIT CSAIL\n\n# Abstract\n\nUnconditional generation‚Äîthe problem of modeling data distribution without relying on human-annotated labels‚Äîis a long-standing and fundamental challenge in generative models, creating a potential of learning from large-scale unlabeled data. In the literature, the generation quality of an unconditional method has been much worse than that of its conditional counterpart. This gap can be attributed to the lack of semantic information provided by labels. In this work, we show that one can close this gap by generating semantic representations in the representation space produced by a self-supervised encoder. These representations can be used to condition the image generator. This framework, called Representation-Conditioned Generation (RCG), provides an effective solution to the unconditional generation problem without using labels. Through comprehensive experiments, we observe that RCG significantly improves unconditional generation quality: e.g., it achieves a new state-of-the-art FID of 2.15 on ImageNet $2 5 6 \\times 2 5 6$ , largely reducing the previous best of 5.91 by a relative $64 \\%$ . Our unconditional results are situated in the same tier as the leading class-conditional ones. We hope these encouraging observations will attract the community‚Äôs attention to the fundamental problem of unconditional generation. Code is available at https://github.com/LTH14/rcg.\n\n# 1 Introduction\n\nGenerative models have been long developed as unsupervised learning methods in the history, e.g., in the seminal works including GAN [27], VAE [39], and diffusion models [57]. These fundamental methods focus on learning the probabilistic distributions of data, without relying on the availability of human annotations. This problem, often categorized as unconditional generation in today‚Äôs community, is in pursuit of utilizing the vast abundance of unannotated data to learn complex distributions.\n\nHowever, unconditional image generation has been largely stagnant in comparison with its conditional counterpart. Recent research [18, 54, 10, 11, 24, 50] has demonstrated compelling image generation quality when conditioned on class labels or text descriptions provided by humans, but its quality degrades significantly without these conditions. Closing the gap between unconditional and conditional generation is a challenging and scientifically valuable problem: it is a necessary step towards unleashing the power of large-scale unannotated data, which is a common goal in today‚Äôs deep learning community.\n\nWe hypothesize that such a performance gap is because human-annotated conditioning introduces rich semantic information to simplify the generative process. In this work, we largely close this gap by taking inspiration from a closely related area‚Äîunsupervised/self-supervised learning.1 We observe that the representations produced by a strong self-supervised encoder (e.g., [30, 12, 8, 14]) can also capture a lot of semantic attributes without human supervision, as reflected by their transfer learning performance in the literature. These self-supervised representations can serve as a form of conditioning without violating the unsupervised nature of unconditional generation, creating an opportunity to get rid of the heavy reliance on human-annotated labels.\n\n![](images/d21d040dc0983718006c7518c1ef8dbfa858ce567ef886e2945b425794cd1ccd.jpg)  \nFigure 1: The Representation-Conditioned Generation (RCG) framework for unconditional generation. RCG consists of three parts: (a) it uses a pre-trained self-supervised encoder to map the image distribution to a representation distribution; (b) it learns a representation generator that samples from a noise distribution and generates a representation subject to the representation distribution; (c) it learns an image generator (e.g., which can be ADM [18], DiT [50], or MAGE [41]) that maps a noise distribution to the image distribution conditioned on the representation distribution.\n\nBased on this observation, we propose to first unconditionally generate a self-supervised representation and then condition on this representation to generate the images. As a preprocessing step (Figure 1a), we use a pre-trained self-supervised encoder (e.g., MoCo [14]) to map the image distribution into the corresponding representation distribution. In this representation space, we train a representation generator without any conditioning (Figure 1b). As this space is low-dimensional and compact [65], learning the representation distribution is favorably feasible with unconditional generation. In practice, we implement it as a very lightweight diffusion model. Given this representation space, we train a second generator that is conditioned on these representations and produces images (Figure 1c). This image generator can conceptually be any image generation model. The overall framework, called Representation-Conditioned Generation (RCG), provides a new paradigm for unconditional generation.2\n\nRCG is conceptually simple, flexible, yet highly effective for unconditional generation. RCG greatly improves unconditional generation quality regardless of the specific choice of the image generator (Figure 2), reducing FID by $71 \\%$ , $76 \\%$ , $82 \\%$ , and $51 \\%$ for LDM-8, ADM, DiT-XL/2, and MAGE-L, respectively. This indicates that RCG largely reduces the reliance of current generative models on manual labels. On the challenging ImageNet $2 5 6 \\times 2 5 6$ benchmark, RCG achieves an unprecedented 2.15 FID for unconditional generation. This performance not only largely outperforms previous unconditional methods, but more surprisingly, can catch up with the strong leading methods that are conditional on class labels. We hope our method and encouraging results will rekindle the community‚Äôs interest in the fundamental problem of unconditional generation.\n\n![](images/2d3ce5b11c3252c127efc337c15a8cda8e587687664a6b6eacdc134e7e70c976.jpg)  \nFigure 2: Unconditional Image Generation can be largely improved by our RCG framework. Regardless of the specific form of the image generator (LDM [54], ADM [18], DiT [50], or MAGE [41]), RCG massively improves the unconditional generation quality. Generation quality is measured by FID on ImageNet with a $2 5 6 \\times 2 5 6$ resolution. All comparisons between models without and with RCG are conducted under controlled conditions to ensure fairness. The technical details and more metrics are in Section 4.1.\n\n# 2 Related Work\n\nGenerative Models. Generative models aim at accurately modeling data distribution to generate new data point that resembles the original data. One stream of generative models is built on top of generative adversarial networks (GANs) [27, 69, 37, 70, 7]. Another stream is based on a twostage scheme [63, 53, 10, 67, 40, 41, 11]: first tokenize the image into a latent space and then apply maximum likelihood estimation and sampling in the token space. Diffusion models [33, 59, 18, 54, 52] have also achieved superior results on image synthesis.\n\nThe design of a generative model is mostly orthogonal to how it is conditioned. However, literature has shown that unconditional generation often significantly lags behind conditional generation under the same design[18, 41, 10], especially on complex data distributions.\n\nUnconditional Generation. Unconditional generation is the fundamental problem in the realm of generative models. It aims to model the data distribution without relying on human annotations, highlighted by seminal works of GAN [27], VAE [39], and diffusion models [57]. It has demonstrated impressive performance in modeling simple image distributions such as scenes or human faces [23, 10, 18, 54], and has also been successful in applications beyond images where human annotation is challenging or impossible, such as novel molecular design [66, 28, 26], medical image synthesis [71, 16, 47], and audio generation [48, 42, 25]. However, recent research in this domain has been limited, largely due to the notable gap between conditional and unconditional generation capabilities of recent generative models on complex data distributions [46, 18, 19, 41, 3, 61].\n\nPrior efforts to narrow this gap mainly group images into clusters in the representation space and use the cluster indices as underlying class labels to provide conditioning [46, 43, 3, 35]. However, these methods assume that the dataset is clusterable, and the optimal number of clusters is close to the number of classes. Additionally, these methods fall short of generating diverse representations‚Äî they are unable to produce different representations within the same cluster or underlying class.\n\nRepresentations for Image Generation. Prior works have explored the effectiveness of exploiting representations for image generation. DALL-E 2 [52], a text-conditional image generation model, first converts text prompts into image embeddings, and then uses these embeddings as the conditions to generate images. In contrast, RCG for the first time demonstrates the possibility of directly generating image representations from scratch, a necessary step to enable conditioning on self-supervised representations in unconditional image generation. Another work, DiffAE [51], trains an image encoder in an end-to-end manner with a diffusion model as decoder, aiming to learn a meaningful and decodable image representation. However, its semantic representation ability is still limited (e.g., compared to self-supervised models like MoCo [14], DINO [8]), which largely hinders its performance in unconditional generation. Another relevant line of work is retrieval-augmented generative models [5, 4, 9], where images are generated based on representations extracted from existing images. Such semi-parametric methods heavily rely on ground-truth images to provide representations during generation, a requirement that is impractical in many generative applications.\n\n![](images/37a0bc0474d94833b15c783ec9ff16f1e8e1384f310ea3092ac781ac9d1c650b.jpg)  \nFigure 3: RCG‚Äôs training framework. The pre-trained self-supervised image encoder extracts representations from images and is fixed during training. To train the representation generator, we add standard Gaussian noise to the representations and ask the network to denoise them. To train the MAGE image generator, we add random masking to the tokenized image and ask the network to reconstruct the missing tokens conditioned on the representation extracted from the same image.\n\n# 3 Method\n\nDirectly modeling a complex high-dimensional image distribution is a challenging task. RCG decomposes it into two much simpler sub-tasks: first modeling the distribution of a compact lowdimensional representation, and then modeling the image distribution conditioned on this representation distribution. Figure 1 illustrates the idea. Next, we describe RCG and its extensions in detail.\n\n# 3.1 The RCG Framework\n\nRCG comprises three key components: a pre-trained self-supervised image encoder, a representation generator, and an image generator. Each component‚Äôs design is elaborated below:\n\nDistribution Mapping. RCG employs an off-the-shelf image encoder to convert the image distribution to a representation distribution. This image encoder has been pre-trained using self-supervised contrastive learning methods, such as MoCo v3 [14], on ImageNet. This approach regularizes the representations on a hyper-sphere while achieving state-of-the-art performance in representation learning. The resulting distribution is characterized by two essential properties: it is simple enough to be modeled effectively by an unconditional representation generator, and it is rich in high-level semantic content, which is crucial for guiding image generation. These attributes are vital for the effectiveness of the following two components.\n\n![](images/b2a0c3851048d40d8bbb4fee7653f4c70291c1fcb33aff53ecaaad5428a09ab1.jpg)  \nFigure 4: Representation generator‚Äôs backbone architecture. Each ‚ÄúLayer‚Äù consists of a LayerNorm layer [1], a SiLU layer [22], and a linear layer. The backbone consists of an input layer that projects the representation to hidden dimension $C$ , followed by $N$ fullyconnected (fc) blocks, and an output layer that projects the hidden latent back to the original representation dimension. The diffusion timestep is embedded and added to every fc block.\n\nRepresentation Generation. In this stage, we want to generate abstract, unstructured representations without conditions. To address this issue, we develop a diffusion model for unconditional representation generation, which we call a representation diffusion model (RDM). RDM employs a fully-connected network with multiple fully-connected residual blocks as its backbone (Figure 4). Each block consists of an input layer, a timestep embedding projection layer, and an output layer, where each layer consists of a LayerNorm [1], a SiLU [22], and a linear layer. Such an architecture is simply controlled by two parameters: the number of blocks, $N$ , and the hidden dimension, $C$ .\n\nRDM follows DDIM [58] for training and inference. As shown in Figure 3a, during training, image representation $z _ { 0 }$ is mixed with standard Gaussian noise variable $\\epsilon$ : $\\bar { z } _ { t } = \\sqrt { \\alpha _ { t } } z _ { 0 } \\dot { + } \\sqrt { 1 - \\alpha _ { t } } \\epsilon$ . The RDM backbone is then trained to denoise $z _ { t }$ back to $z _ { 0 }$ . During inference, RDM generates representations from Gaussian noise following the DDIM sampling process [58]. Since RDM operates on highly compacted representations, it brings marginal computation overheads for both training and generation (Appendix B.1), while providing substantial semantic information for the image generator, introduced next.\n\nImage Generation. The image generator in RCG crafts images conditioned on self-supervised representations. Conceptually, such an image generator can be any modern conditional image generative model by substituting its original conditioning (e.g., class label or text) with representations. In Figure 3b, we take MAGE [41], a parallel decoding generative model as an example. The image generator is trained to reconstruct the original image from a masked version of the image, conditioned on the representation of the same image. During inference, the image generator generates images from a fully masked image, conditioned on the representation generated by the representation generator.\n\nWe experiment with four representative generative models: ADM [18], LDM [54], and DiT [50] are diffusion-based frameworks, and MAGE [41] is a parallel decoding framework. Our experiments show that all four generative models achieve much better performance when conditioned on highlevel self-supervised representations (Table 1).\n\n# 3.2 Extensions\n\nOur RCG framework can easily be extended to support guidance even in the absence of labels, and to support conditional generation when desired. We introduce these extensions as follows.\n\nEnabling Guidance in Unconditional Generation. In class-conditional generation, the presence of labels allows not only for class conditioning but can also provides additional ‚Äúguidance‚Äù in the generative process. This mechanism is often implemented through classifier-free guidance in classconditional generation methods [32, 54, 11, 50]. In RCG, the representation-conditioning behavior enables us to also benefit from such guidance, even in the absence of labels.\n\nSpecifically, RCG follows [32, 11] to incorporate guidance into its MAGE generator. During training, the MAGE generator is trained with a $10 \\%$ probability of not being conditioned on image representations, analogous to [32] which has a $10 \\%$ probability of not being conditioned on labels. For each inference step, the MAGE generator produces a representation-conditioned logit, $l _ { c }$ , and an unconditional logit, $l _ { u }$ , for each masked token. The final logits, $l _ { g }$ , are calculated by adjusting $l _ { c }$ away from $l _ { u }$ by the guidance scale, $\\begin{array} { r } { \\tau \\colon l _ { g } = l _ { c } + \\tau ( l _ { c } - l _ { u } ) } \\end{array}$ . The MAGE generator then uses $l _ { g }$ to sample the remaining masked tokens. Additional implementation details of RCG‚Äôs guidance are provided in Appendix A.\n\nTable 1: RCG significantly improves the unconditional generation performance of current generative models, evaluated on ImageNet $2 5 6 \\times 2 5 6$ . All numbers are reported under the unconditional generation setting.   \n\n<html><body><table><tr><td colspan=\"2\">Unconditional generation FID‚Üì</td><td>IS‚Üë</td></tr><tr><td rowspan=\"2\">LDM-8 [54]</td><td>baseline 39.13</td><td>22.8 w/RCG 11.30(-27.83)101.9(+79.1)</td></tr><tr><td>baseline 26.21</td><td>39.7</td></tr><tr><td rowspan=\"2\">ADM [18]</td><td>w/RCG</td><td>6.24(-19.97) 136.9(+97.2)</td></tr><tr><td>baseline 27.32</td><td>35.9</td></tr><tr><td>DiT-XL/2 [50]</td><td>w/RCG baseline 8.67</td><td>4.89(-22.43)143.2(+107.3) 94.8</td></tr><tr><td rowspan=\"2\">MAGE-B[41]</td><td>w/RCG</td><td>3.98 (-4.69) 177.8(¬±83.0)</td></tr><tr><td></td><td></td></tr><tr><td rowspan=\"2\">MAGE-L [41]</td><td>baseline</td><td>7.04 123.5</td></tr><tr><td>w/RCG</td><td>3.44(-3.60) 186.9 (+63.4)</td></tr></table></body></html>\n\nSimple Extension to Class-conditional Generation. RCG seamlessly enables conditional image generation by training a task-specific conditional RDM. Specifically, a class embedding is integrated into each fully-connected block of the RDM, in addition to the timestep embedding. This enables the generation of class-specific representations. The image generator then crafts the image conditioned on the generated representation. As shown in Table 3 and Appendix C, this simple modification allows users to specify the class of the generated image while keeping RCG‚Äôs superior generative performance, all without the need to retrain the image generator.\n\n# 4 Experiments\n\nWe evaluate RCG on the ImageNet $2 5 6 \\times 2 5 6$ dataset [17], which is a common benchmark for image generation and is especially challenging for unconditional generation. Unless otherwise specified, we do not use ImageNet labels in any of the experiments. We generate 50K images and report the Frechet Inception Distance (FID) [31] and Inception Score (IS) [55] as the standard metrics for assessing the fidelity and diversity of the generated images. Evaluations of precision and recall are included in Appendix B.1. Unless otherwise specified, we follow the evaluation suite provided by ADM [18]. All ablations and results on other datasets are included in Appendix B.1.\n\n# 4.1 Observations\n\nWe extensively evaluate the performance of RCG with various image generators and compare it to the results of state-of-the-art unconditional and conditional image generation methods. Several intriguing properties are observed.\n\nRCG significantly improves the unconditional generation performance of current generative models. In Table 1, we evaluate the proposed RCG framework using different image generators. The results demonstrate that conditioning on generated representations substantially improves the performance of all image generators in unconditional generation. Specifically, it reduces the FID for unconditional LDM-8, ADM, DiT-XL/2, MAGE-B, and MAGE-L by $71 \\%$ , $76 \\%$ , $82 \\%$ , $54 \\%$ , and $51 \\%$ , respectively. We further show that such improvement is also universal across different datasets, as demonstrated by the results on CIFAR-10 and iNaturalist in Appendix B.1. These findings confirm that RCG markedly boosts the performance of current generative models in unconditional generation, significantly reducing their reliance on human-annotated labels.\n\nMoreover, such outstanding performance can be achieved with lower training cost compared to current generative models. In Figure 5, we compare the training cost and unconditional generation FIDs\n\n2530354045Unconditional Generation FID LDM-8 ‚òÖ MAGE-B w/ RCG ‚òÖ MAGE-L w/ RCG DiT-L/2DiT-XL/2   \n105 MAGE-B MAGE-L   \n‚òÖ1   \n0   \n0 2 4 6 8 10 12 14 Training Cost (days)\n\n<html><body><table><tr><td>Unconditional generation</td><td>#params FID‚Üì IS‚Üë ~70M 38.61 24.7</td></tr><tr><td>BigGAN[19] ADM[18]</td><td>554M 26.21 39.7</td></tr><tr><td>MaskGIT[10] RCDM+ [5]</td><td>227M 20.72 42.1 19.0 51.9</td></tr><tr><td>IC-GAN‚Ä† [9] ADDP [61] MAGE-B [41]</td><td>~75M 15.6 59.0 176M 8.9 95.3 176M 8.67 94.8 439M 7.04 123.5</td></tr><tr><td>MAGE-L [41] RDM-IN+ [4]</td><td>400M 5.91 158.8</td></tr><tr><td>RCG (MAGE-B)</td><td>3.98 177.8</td></tr><tr><td>RCG (MAGE-L) RCG-G (MAGE-B)</td><td>239M 502M 3.44 186.9 239M 3.19 212.6 2.15 253.4</td></tr></table></body></html>\n\nof RCG and current generative models. RCG achieves a significantly lower FID with less training cost than current generative models. Specifically, MAGE-B with RCG achieves an unconditional generation FID of 4.87 in less than a day when trained on 64 V100 GPUs. This demonstrates that decomposing the complex tasks of unconditional generation into much simpler sub-tasks can significantly simplify the data modeling process.\n\nRCG largely improves the state-of-the-art in unconditional image generation. In Table 2, we compare MAGE with RCG and previous state-of-the-art methods in unconditional image generation. As shown in Figure 8 and Table 2, RCG can generate images with both high fidelity and diversity, achieving an FID of 3.44 and an Inception Score of 186.9. These results are further enhanced with the guided version of RCG (RCG-G), which reaches an FID of 2.15 and an Inception Score of 253.4, significantly surpassing previous methods of unconditional image generation.\n\nRCG‚Äôs unconditional generation performance rivals leading methods in class-conditional image generation. In Table 3, we perform a system-level comparison between the unconditional RCG and state-of-the-art class-conditional image generation methods. MAGE-L with RCG is comparable to leading class-conditional methods, with and without guidance. These results demonstrate that RCG, for the first time, improves the performance of unconditional image generation on complex data distributions to the same level as that of state-of-the-art class-conditional generation methods, effectively bridging the historical gap between class-conditional and unconditional generation.\n\nIn Table 4, we further conduct an apple-to-apple comparison between the class-conditional versions of LDM-8, ADM, and DiT-XL/2 and their unconditional counterparts using RCG. Surprisingly, with RCG, these generative models consistently outperform their class-conditional versions by a noticeable margin. This demonstrates that the rich semantic information from the unconditionally generated representations can guide the generative process even more effectively than class labels.\n\nTable 3: System-level comparison: RCG‚Äôs unconditional generation performance rivals leading methods in class-conditional image generation on ImageNet $2 5 6 \\times 2 5 6$ . Following common practice, we report the number of parameters used during generation. Class-conditional results are marked in gray.   \n\n<html><body><table><tr><td>Methods</td><td>#params</td><td colspan=\"2\">w/o Guidance FID‚Üì IS‚Üë</td><td colspan=\"2\">w/Guidance FID‚Üì IS‚Üë</td></tr><tr><td>Class-conditional</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ADM[18]</td><td>554M</td><td>10.94</td><td>101.0</td><td>4.59</td><td>186.7</td></tr><tr><td>LDM-4 [54]</td><td>400M</td><td>10.56</td><td>103.5</td><td>3.60</td><td>247.7</td></tr><tr><td>U-ViT-H/2-G [2]</td><td>501M</td><td></td><td></td><td>2.29</td><td>263.9</td></tr><tr><td>DiT-XL/2 [50]</td><td>675M</td><td>9.62</td><td>121.5</td><td>2.27</td><td>278.2</td></tr><tr><td>DiffT[29]</td><td></td><td></td><td></td><td>1.73</td><td>276.5</td></tr><tr><td>BigGAN-deep [6]</td><td>160M</td><td>6.95</td><td>198.2</td><td></td><td></td></tr><tr><td>MaskGIT[10]</td><td>227M</td><td>6.18</td><td>182.1</td><td></td><td></td></tr><tr><td>MDTv2-XL/2 [24]</td><td>676M</td><td>5.06</td><td>155.6</td><td>1 1.58</td><td>- 314.7</td></tr><tr><td>CDM [34]</td><td>‚Üí</td><td>4.88</td><td>158.7</td><td></td><td></td></tr><tr><td>MAGVIT-v2 [68]</td><td>307M</td><td>3.65</td><td>200.5</td><td>1.78</td><td>319.4</td></tr><tr><td>RIN[36]</td><td>410M</td><td>3.42</td><td></td><td></td><td></td></tr><tr><td>VDM++ [38]</td><td></td><td></td><td>182.0</td><td></td><td></td></tr><tr><td>RCG, conditional (MAGE-L)</td><td>2B 512M</td><td>2.40 2.99</td><td>225.3 215.5</td><td>2.12 2.25</td><td>267.7</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>300.7</td></tr><tr><td>Unconditional RCG (MAGE-L)</td><td>502M</td><td>3.44</td><td>186.9</td><td>2.15</td><td>253.4</td></tr></table></body></html>\n\nTable 4: Apple-to-apple comparison: RCG‚Äôs unconditional generation outperforms the classconditional counterparts of current generative models, evaluated on ImageNet $2 5 6 \\times 2 5 6$ . MAGE does not report its class-conditional generation performance. Class-conditional results are marked in gray   \n\n<html><body><table><tr><td>Methods</td><td></td><td>FID‚Üì</td><td>IS‚Üë</td></tr><tr><td rowspan=\"2\">LDM-8 [54]</td><td>w/classlabels</td><td>17.41</td><td>72.9</td></tr><tr><td>w/ RCG</td><td>11.30</td><td>101.9</td></tr><tr><td rowspan=\"2\">ADM[18]</td><td>w/ class labels</td><td>10.94</td><td>101.0</td></tr><tr><td>w/ RCG</td><td>6.24</td><td>136.9</td></tr><tr><td rowspan=\"2\">DiT-XL/2 [50]</td><td>w/ class labels</td><td>9.62</td><td>121.5</td></tr><tr><td>w/ RCG</td><td>4.89</td><td>143.2</td></tr></table></body></html>\n\nAs shown in Table 3 and Appendix C, RCG also supports class-conditional generation with a simple extension. Our representation diffusion model can easily adapt to class-conditional representation generation, thereby enabling RCG to also adeptly perform class-conditional image generation. This result demonstrates the effectiveness of RCG in leveraging its superior unconditional generation performance to benefit downstream conditional generation tasks.\n\nImportantly, such an adaptation does not require retraining the representation-conditioned image generator. For any new conditioning, only the lightweight representation generator needs to be retrained. This potentially enables pre-training of the self-supervised encoder and image generator on large-scale unlabeled datasets, and task-specific training of conditional representation generator on a small-scale labeled dataset. We believe that this property, similar to self-supervised learning, allows RCG to both benefit from large unlabeled datasets and adapt to various downstream generative tasks with minimal overheads. We leave the exploration on this direction to future work.\n\n# 4.2 Qualitative Insights\n\nIn this section, we showcase the visualization results of RCG, providing insights into its superior generative capabilities. Figure 8 illustrates RCG‚Äôs unconditional image generation results on ImageNet $2 5 6 \\times 2 5 6$ . The model is capable of generating both diverse and high-quality images without relying on human annotations. The high-level semantic diversity in RCG‚Äôs generation stems from its representation generator, which models the distribution of representations and samples them with varied semantics. By conditioning on these representations, the complex data distribution is broken down into simpler, representation-conditioned sub-distributions. This decomposition significantly simplifies the task for the image generator, leading to the production of high-quality images.\n\n![](images/4c2acf3a297ca1837006ffefbf1ed6f15708e815c37b375dd62267fe7023ed29.jpg)  \nFigure 6: RCG can generate images with diverse appearances but similar semantics from the same representation. We extract representations from reference images and, for each representation, generate a variety of images from different random seeds.\n\n![](images/952120d4ed9b85db488ab106456685f6fd7d6c0e256d19c84af4c472e95306c0.jpg)  \nFigure 7: RCG‚Äôs results conditioned on interpolated representations from two images. The semantics of the generated images gradually transfer between the two images.\n\nBesides high-quality generation, the image generator can also introduce significant low-level diversity in the generative process. Figure 6 illustrates RCG‚Äôs ability to generate diverse images that semantically align with each other, given the same representation from the reference image. The images generated by RCG can capture the semantic essence of the reference images while differing in specific details. This result highlights RCG‚Äôs capability to leverage semantic information in representations to guide the generative process, without compromising the diversity that is important in unconditional image generation.\n\nFigure 7 further showcases RCG‚Äôs semantic interpolation ability, demonstrating that the representation space is semantically smooth. By leveraging RCG‚Äôs dependency on representations, we can semantically transition between two images by linearly interpolating their respective representations. The interpolated images remain realistic across varying interpolation rates, and their semantic contents smoothly transition from one image to another. For example, interpolating between an image of ‚ÄúTibetan mastiff‚Äù and an image of ‚Äúwool‚Äù could generate a novel image featuring a dog wearing a woolen sweater. This also highlights RCG‚Äôs potential in manipulating image semantics within a low-dimensional representation space, offering new possibilities to control image generation.\n\n# 5 Discussion\n\nComputer vision has entered a new era where learning from extensive, unlabeled datasets is becoming increasingly common. Despite this trend, the training of image generation models still mostly relies on labeled datasets, which could be attributed to the large performance gap between conditional and unconditional image generation. Our paper addresses this issue by exploring RepresentationConditioned Generation, which we propose as a nexus between conditional and unconditional image generation. We demonstrate that the long-standing performance gap can be effectively bridged by generating images conditioned on self-supervised representations and leveraging a representation generator to model and sample from this representation space. We believe this approach has the potential to liberate image generation from the constraints of human annotations, enabling it to fully harness the vast amounts of unlabeled data and even generalize to modalities that are beyond the scope of human annotation capabilities.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   Êó†Êù°‰ª∂ÁîüÊàêÔºàUnconditional GenerationÔºâÊòØÊåá‰∏ç‰æùËµñ‰∫∫Â∑•Ê†áÊ≥®Ê†áÁ≠æÊù•Âª∫Ê®°Êï∞ÊçÆÂàÜÂ∏ÉÁöÑÈóÆÈ¢òÔºåËøôÊòØÁîüÊàêÊ®°Âûã‰∏≠ÁöÑ‰∏Ä‰∏™ÈïøÊúüÂ≠òÂú®ÁöÑÊ†πÊú¨ÊÄßÊåëÊàò„ÄÇÂ∞ΩÁÆ°Êó†Êù°‰ª∂ÁîüÊàêÂÖ∑ÊúâÂà©Áî®Â§ßËßÑÊ®°Êú™Ê†áÊ≥®Êï∞ÊçÆÁöÑÊΩúÂäõÔºå‰ΩÜÂÖ∂ÁîüÊàêË¥®Èáè‰∏ÄÁõ¥Ëøú‰Ωé‰∫éÊúâÊù°‰ª∂ÁîüÊàêÊñπÊ≥ïÔºå‰∏ªË¶ÅÂéüÂõ†ÊòØÁº∫‰πèÊ†áÁ≠æÊèê‰æõÁöÑËØ≠‰πâ‰ø°ÊÅØ„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éÔºåËß£ÂÜ≥Êó†Êù°‰ª∂ÁîüÊàê‰∏éÊúâÊù°‰ª∂ÁîüÊàê‰πãÈó¥ÁöÑÊÄßËÉΩÂ∑ÆË∑ùÔºåÂèØ‰ª•ÈáäÊîæÂ§ßËßÑÊ®°Êú™Ê†áÊ≥®Êï∞ÊçÆÁöÑÊΩúÂäõÔºåÈÄÇÁî®‰∫éËÆ∏Â§öÊó†Ê≥ïÊàñÈöæ‰ª•Ëé∑Âèñ‰∫∫Â∑•Ê†áÊ≥®ÁöÑÂú∫ÊôØÔºåÂ¶ÇÂåªÂ≠¶ÂõæÂÉèÂêàÊàê„ÄÅÂàÜÂ≠êËÆæËÆ°Á≠â„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁß∞‰∏∫‚ÄúË°®Á§∫Êù°‰ª∂ÁîüÊàê‚ÄùÔºàRepresentation-Conditioned Generation, RCGÔºâÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáÁîüÊàêËá™ÁõëÁù£ÁºñÁ†ÅÂô®‰∫ßÁîüÁöÑËØ≠‰πâË°®Á§∫Êù•Êù°‰ª∂ÂåñÂõæÂÉèÁîüÊàêÂô®Ôºå‰ªéËÄåÂú®‰∏ç‰ΩøÁî®Ê†áÁ≠æÁöÑÊÉÖÂÜµ‰∏ãÊòæËëóÊèêÂçáÊó†Êù°‰ª∂ÁîüÊàêÁöÑÊÄßËÉΩ„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ1Ôºö** ÊèêÂá∫RCGÊ°ÜÊû∂ÔºåÈ¶ñÊ¨°ËØÅÊòéÂèØ‰ª•ÈÄöËøáÊó†Êù°‰ª∂ÁîüÊàêËá™ÁõëÁù£Ë°®Á§∫Êù•Êù°‰ª∂ÂåñÂõæÂÉèÁîüÊàêÂô®ÔºåÊòæËëóÁº©Â∞èÊó†Êù°‰ª∂‰∏éÊúâÊù°‰ª∂ÁîüÊàê‰πãÈó¥ÁöÑÊÄßËÉΩÂ∑ÆË∑ù„ÄÇ\\n>     *   ÂÖ≥ÈîÆÊï∞ÊçÆÔºöÂú®ImageNet $256\\\\times256$‰∏äÔºåRCGÂ∞ÜÊó†Êù°‰ª∂ÁîüÊàêÁöÑFID‰ªé5.91ÈôçËá≥2.15ÔºåÁõ∏ÂØπÊèêÂçá64%„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ2Ôºö** Â±ïÁ§∫‰∫ÜRCGÁöÑÈÄöÁî®ÊÄßÔºåÈÄÇÁî®‰∫éÂ§öÁßçÁîüÊàêÊ®°ÂûãÔºàÂ¶ÇADM„ÄÅDiT„ÄÅMAGEÁ≠âÔºâÔºåÂπ∂ËÉΩÊòæËëóÊèêÂçáÂÖ∂Êó†Êù°‰ª∂ÁîüÊàêÊÄßËÉΩ„ÄÇ\\n>     *   ÂÖ≥ÈîÆÊï∞ÊçÆÔºöRCGÂ∞ÜLDM-8„ÄÅADM„ÄÅDiT-XL/2ÂíåMAGE-LÁöÑFIDÂàÜÂà´Èôç‰Ωé‰∫Ü71%„ÄÅ76%„ÄÅ82%Âíå51%„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ3Ôºö** RCGÁöÑÊó†Êù°‰ª∂ÁîüÊàêÊÄßËÉΩÈ¶ñÊ¨°‰∏éÈ¢ÜÂÖàÁöÑÊúâÊù°‰ª∂ÁîüÊàêÊñπÊ≥ïÁõ∏ÂΩìÔºåÁîöËá≥Âú®Êüê‰∫õÊÉÖÂÜµ‰∏ãË∂ÖË∂äÂêéËÄÖ„ÄÇ\\n>     *   ÂÖ≥ÈîÆÊï∞ÊçÆÔºöRCGÔºàMAGE-LÔºâÁöÑFID‰∏∫3.44Ôºå‰ºò‰∫éËÆ∏Â§öÊúâÊù°‰ª∂ÁîüÊàêÊñπÊ≥ïÔºàÂ¶ÇADMÁöÑ4.59Ôºâ„ÄÇ\\n> *   **ÂàõÊñ∞Ë¥°ÁåÆÁÇπ4Ôºö** RCGÂú®ËÆ≠ÁªÉÊàêÊú¨‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºå‰æãÂ¶ÇMAGE-B with RCGÂú®‰∏çÂà∞1Â§©ÁöÑËÆ≠ÁªÉÊó∂Èó¥ÂÜÖËææÂà∞‰∫Ü4.87 FID„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   RCGÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂ∞ÜÂ§çÊùÇÁöÑÊó†Êù°‰ª∂ÂõæÂÉèÁîüÊàê‰ªªÂä°ÂàÜËß£‰∏∫‰∏§‰∏™Êõ¥ÁÆÄÂçïÁöÑÂ≠ê‰ªªÂä°ÔºöÈ¶ñÂÖàÁîüÊàêËá™ÁõëÁù£ÁºñÁ†ÅÂô®‰∫ßÁîüÁöÑËØ≠‰πâË°®Á§∫ÔºåÁÑ∂ÂêéÂü∫‰∫éËøô‰∫õË°®Á§∫ÁîüÊàêÂõæÂÉè„ÄÇËøôÁßçÊñπÊ≥ïÂà©Áî®‰∫ÜËá™ÁõëÁù£Ë°®Á§∫‰∏≠ÁöÑ‰∏∞ÂØåËØ≠‰πâ‰ø°ÊÅØÔºå‰ªéËÄåÂú®‰∏ç‰æùËµñ‰∫∫Â∑•Ê†áÁ≠æÁöÑÊÉÖÂÜµ‰∏ãÊèêÂçáÁîüÊàêË¥®Èáè„ÄÇ\\n> *   ËÆæËÆ°Âì≤Â≠¶Âú®‰∫éÂà©Áî®Ëá™ÁõëÁù£Â≠¶‰π†ÁöÑË°®Á§∫Á©∫Èó¥‰Ωú‰∏∫‰∏≠Èó¥Ê°•Ê¢ÅÔºåÂ∞ÜÊó†Êù°‰ª∂ÁîüÊàêÈóÆÈ¢òËΩ¨Âåñ‰∏∫Ë°®Á§∫Á©∫Èó¥ÁöÑÁîüÊàêÈóÆÈ¢òÔºå‰ªéËÄåÁÆÄÂåñ‰ªªÂä°ÈöæÂ∫¶„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** ÂÖàÂâçÂ∑•‰Ωú‰∏ªË¶ÅÈÄöËøáËÅöÁ±ªË°®Á§∫Á©∫Èó¥Âπ∂Âà©Áî®ËÅöÁ±ªÁ¥¢Âºï‰Ωú‰∏∫Á±ªÊ†áÁ≠æÊù•Êèê‰æõÊù°‰ª∂Ôºå‰ΩÜËøôÁßçÊñπÊ≥ïÂÅáËÆæÊï∞ÊçÆÈõÜÂèØËÅöÁ±ª‰∏îÊó†Ê≥ïÁîüÊàêÂêå‰∏ÄËÅöÁ±ªÂÜÖÁöÑÂ§öÊ†∑ÂåñË°®Á§∫„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** RCGÁõ¥Êé•ÁîüÊàêË°®Á§∫Á©∫Èó¥‰∏≠ÁöÑËØ≠‰πâË°®Á§∫ÔºåÈÅøÂÖç‰∫ÜËÅöÁ±ªÂÅáËÆæÁöÑÈôêÂà∂ÔºåÂπ∂ËÉΩÁîüÊàêÂ§öÊ†∑ÂåñÁöÑË°®Á§∫„ÄÇÊ≠§Â§ñÔºåRCGÊîØÊåÅÊó†Ê†áÁ≠æÊù°‰ª∂‰∏ãÁöÑÂºïÂØºÁîüÊàêÂíåÁ±ªÊù°‰ª∂ÁîüÊàêÊâ©Â±ï„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> *   1. **ÂàÜÂ∏ÉÊò†Â∞ÑÔºàDistribution MappingÔºâÔºö** ‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁöÑËá™ÁõëÁù£ÁºñÁ†ÅÂô®ÔºàÂ¶ÇMoCo v3ÔºâÂ∞ÜÂõæÂÉèÂàÜÂ∏ÉÊò†Â∞ÑÂà∞Ë°®Á§∫ÂàÜÂ∏É„ÄÇ\\n> *   2. **Ë°®Á§∫ÁîüÊàêÔºàRepresentation GenerationÔºâÔºö** ËÆ≠ÁªÉ‰∏Ä‰∏™ËΩªÈáèÁ∫ßÊâ©Êï£Ê®°ÂûãÔºàRDMÔºâÊó†Êù°‰ª∂ÁîüÊàêË°®Á§∫„ÄÇ\\n>   *   ÂÖ≥ÈîÆÂÖ¨ÂºèÔºö$\\\\bar{z}_t = \\\\sqrt{\\\\alpha_t} z_0 + \\\\sqrt{1-\\\\alpha_t} \\\\epsilon$ÔºåRDMÈÄöËøáÂéªÂô™ËÆ≠ÁªÉÁîüÊàêË°®Á§∫„ÄÇ\\n> *   3. **ÂõæÂÉèÁîüÊàêÔºàImage GenerationÔºâÔºö** ËÆ≠ÁªÉ‰∏Ä‰∏™ÂõæÂÉèÁîüÊàêÂô®ÔºàÂ¶ÇMAGE„ÄÅADMÔºâÂü∫‰∫éÁîüÊàêÁöÑË°®Á§∫ÁîüÊàêÂõæÂÉè„ÄÇ\\n>   *   ÂõæÂÉèÁîüÊàêÂô®ÈÄöËøáÊé©Á†ÅÈáçÂª∫‰ªªÂä°ËÆ≠ÁªÉÔºåÊù°‰ª∂‰∏∫Âêå‰∏ÄÂõæÂÉèÁöÑË°®Á§∫„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   Êó†Êù°‰ª∂ÁîüÊàêÂü∫Á∫øÔºöBigGAN„ÄÅADM„ÄÅMaskGIT„ÄÅRCDM+„ÄÅIC-GAN„ÄÅADDP„ÄÅMAGE-B„ÄÅMAGE-L„ÄÅRDM-IN+„ÄÇ\\n> *   ÊúâÊù°‰ª∂ÁîüÊàêÂü∫Á∫øÔºöADM„ÄÅLDM-4„ÄÅU-ViT-H/2-G„ÄÅDiT-XL/2„ÄÅDiffT„ÄÅBigGAN-deep„ÄÅMaskGIT„ÄÅMDTv2-XL/2„ÄÅCDM„ÄÅMAGVIT-v2„ÄÅRIN„ÄÅVDM++„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®FIDÔºàFrechet Inception DistanceÔºâ‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®ImageNet $256\\\\times256$‰∏äËææÂà∞‰∫Ü**2.15**ÔºåÊòæËëó‰ºò‰∫éÊó†Êù°‰ª∂Âü∫Á∫øÊ®°ÂûãMAGE-LÔºà5.91ÔºâÂíåRDM-IN+Ôºà5.91Ôºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü3.76ÔºàÁõ∏ÂØπ64%Ôºâ„ÄÇ\\n> *   **Âú®ISÔºàInception ScoreÔºâ‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÁöÑÂæóÂàÜ‰∏∫**253.4**ÔºåËøúÈ´ò‰∫éÊó†Êù°‰ª∂Âü∫Á∫øÊ®°ÂûãMAGE-LÔºà123.5ÔºâÂíåRDM-IN+Ôºà158.8ÔºâÔºåÁîöËá≥‰ºò‰∫éËÆ∏Â§öÊúâÊù°‰ª∂ÁîüÊàêÊñπÊ≥ïÔºàÂ¶ÇADMÁöÑ186.7Ôºâ„ÄÇ\\n> *   **Âú®ËÆ≠ÁªÉÊàêÊú¨‰∏äÔºö** RCGÂú®Êõ¥‰ΩéÁöÑËÆ≠ÁªÉÊàêÊú¨‰∏ãÂÆûÁé∞‰∫ÜÊõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇ‰æãÂ¶ÇÔºåMAGE-B with RCGÂú®‰∏çÂà∞1Â§©ÁöÑËÆ≠ÁªÉÊó∂Èó¥ÂÜÖËææÂà∞‰∫Ü4.87 FIDÔºåËøú‰Ωé‰∫éÂÖ∂‰ªñÊ®°ÂûãÁöÑËÆ≠ÁªÉÊàêÊú¨„ÄÇ\\n> *   **Âú®ÈÄöÁî®ÊÄß‰∏äÔºö** RCGÊòæËëóÊèêÂçá‰∫ÜÂ§öÁßçÁîüÊàêÊ®°ÂûãÁöÑÊó†Êù°‰ª∂ÁîüÊàêÊÄßËÉΩÔºåÂ¶ÇLDM-8ÔºàFID‰ªé39.13ÈôçËá≥11.30Ôºâ„ÄÅADMÔºàFID‰ªé26.21ÈôçËá≥6.24Ôºâ„ÄÅDiT-XL/2ÔºàFID‰ªé27.32ÈôçËá≥4.89ÔºâÂíåMAGE-LÔºàFID‰ªé7.04ÈôçËá≥3.44Ôºâ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   Êó†Êù°‰ª∂ÁîüÊàê (Unconditional Generation, N/A)\\n*   Ë°®Á§∫Êù°‰ª∂ÁîüÊàê (Representation-Conditioned Generation, RCG)\\n*   Ëá™ÁõëÁù£Â≠¶‰π† (Self-supervised Learning, SSL)\\n*   Êâ©Êï£Ê®°Âûã (Diffusion Model, DM)\\n*   ÂõæÂÉèÁîüÊàê (Image Generation, N/A)\\n*   ËØ≠‰πâË°®Á§∫ (Semantic Representation, N/A)\\n*   ÁîüÊàêÂØπÊäóÁΩëÁªú (Generative Adversarial Network, GAN)\\n*   ÂèòÂàÜËá™ÁºñÁ†ÅÂô® (Variational Autoencoder, VAE)\"\n}\n```"
}