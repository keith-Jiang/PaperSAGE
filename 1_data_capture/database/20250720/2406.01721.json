{
    "source": "Semantic Scholar",
    "arxiv_id": "2406.01721",
    "link": "https://arxiv.org/abs/2406.01721",
    "pdf_link": "https://arxiv.org/pdf/2406.01721.pdf",
    "title": "DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs",
    "authors": [
        "Haokun Lin",
        "Haobo Xu",
        "Yichen Wu",
        "Jingzhi Cui",
        "Yingtao Zhang",
        "Linzhan Mou",
        "Linqi Song",
        "Zhenan Sun",
        "Ying Wei"
    ],
    "categories": [
        "cs.CL"
    ],
    "publication_date": "2024-06-03",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 48,
    "influential_citation_count": 7,
    "institutions": [
        "University of Chinese Academy of Sciences",
        "Tsinghua University",
        "Institute of Automation, CAS",
        "City University of Hong Kong",
        "Zhejiang University"
    ],
    "paper_content": "# DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs\n\nHaokun Lin∗1,3,4, Haobo $\\mathbf { X } \\mathbf { u } ^ { * 2 }$ , Yichen $\\mathbf { W _ { u } } ^ { * 4 }$ , Jingzhi $\\mathbf { C u i ^ { 2 } }$ , Yingtao Zhang2, Linzhan $\\mathbf { M o u } ^ { 5 }$ , Linqi $\\mathbf { S o n g ^ { 4 } }$ , Zhenan Sun† 1,3, Ying Wei† 4,5\n\n1 School of Artificial Intelligence, University of Chinese Academy of Sciences 2 Tsinghua University 3 NLPR & MAIS, Institute of Automation, CAS 4 City University of Hong Kong 5 Zhejiang University\n\nhaokun.lin@cripac.ia.ac.cn xuhb20@mails.tsinghua.edu.cn wuyichen.am97@gmail.com znsun@nlpr.ia.ac.cn ying.wei@zju.edu.cn\n\n# Abstract\n\nQuantization of large language models (LLMs) faces significant challenges, particularly due to the presence of outlier activations that impede efficient low-bit representation. Traditional approaches predominantly address Normal Outliers, which are activations across all tokens with relatively large magnitudes. However, these methods struggle with smoothing Massive Outliers that display significantly larger values, which leads to significant performance degradation in low-bit quantization. In this paper, we introduce DuQuant, a novel approach that utilizes rotation and permutation transformations to more effectively mitigate both massive and normal outliers. First, DuQuant starts by constructing the rotation matrix, using specific outlier dimensions as prior knowledge, to redistribute outliers to adjacent channels by block-wise rotation. Second, We further employ a zigzag permutation to balance the distribution of outliers across blocks, thereby reducing block-wise variance. A subsequent rotation further smooths the activation landscape, enhancing model performance. DuQuant simplifies the quantization process and excels in managing outliers, outperforming the state-of-the-art baselines across various sizes and types of LLMs on multiple tasks, even with 4-bit weight-activation quantization. Our code is available at https://github.com/Hsu1023/DuQuant.\n\n# 1 Introduction\n\nLarge language models (LLMs) [52, 7, 51] have demonstrated exceptional performance across a wide range of natural language processing tasks. However, their billions of parameters present considerable deployment challenges on resource-constrained edge devices, particularly in terms of memory usage and inference speed [23, 15, 57]. In response to these challenges, network quantization methods [20, 24] have been extensively explored to minimize memory usage by converting floatingpoint parameters into low-bit formats [18, 33, 8], and to expedite inference by quantizing both activations and weights for accelerating the matrix multiplication process [65, 35, 76].\n\nAmong LLM quantization methods, a primary issue is the presence of activation outliers, which enlarge the quantization step sizes and subsequently cause significant accuracy loss [60]. To mitigate this problem, current research has developed various methods to address Normal Outliers in activations, which are persistent in several channels across all tokens [13, 65]. However, besides Normal Outliers, there exists another type of activation outlier [49, 36], termed Massive Outliers.\n\nAll tokens, Limited tokens, Extremely large values Max (before smooth) Massive 10 Relevantly large values Max (before smooth) Max (after smooth) Max (after smooth) T 200 10 1.50 0 150 Hard to eliminate New outliers 200 100 dP P Channe D 中 4000 (a) LLaMA2_7B_Layer1_Attn_K_Proj (b) LLaMA2_7B_Layer1_FFN_Down_Proj (c) Activation Change with SmoothQuant (d) Weight Change with SmoothQuant Normal Outliers Massive Outliers SmoothQuant Fails to Eliminate Massive Outliers\n\nThese outliers are characterized by their exceedingly high values and limited occurrence in a subset of tokens, as depicted in Figure 1(b). Unfortunately, existing LLM quantization methods struggle to effectively address these Massive Outliers. For instance, SmoothQuant [65], despite using a smooth factor to shift some of the activation outliers to the weight part, still cannot effectively handle Massive Outliers with extremely large values, as shown in Figure 1(c)(d). OmniQuant [48] and AffineQuant [40], on the other hand, exhibit training instability issues [35] due to the presence of Massive Outliers. Consequently, there is a pressing need for an LLM quantization approach that effectively addresses both Normal and Massive Outliers.\n\nTo tackle this challenge, we propose the Dual transformations Quantization (DuQuant) method. Our motivation is to redistribute the activation outlier values across different channels, facilitating easier quantization. Specifically, we construct the orthogonal rotation matrix and the orthogonal permutation matrix. By multiplying these matrices with the activations, we can effectively perform column transformations on the activations, which in turn allows for the redistribution of outliers. For the rotation transformation aspect, we first identify specific dimensions of outliers as the prior knowledge and employ a greedy algorithm to construct the rotation matrix. To enhance the multiplication efficiency, we utilize diagonal block-wise rotation matrices, with each matrix responsible for a small portion of the activations. However, this approach may result in uneven outlier magnitudes across different blocks. Therefore, we propose the zigzag permutation for reordering the activation channels, which promotes a more uniform distribution across different blocks. Concretely, we distribute the channels with the highest activations across the blocks in a back-and-forth pattern. After establishing blocks with uniformly distributed outlier magnitudes, we employ another rotation transformation to further redistribute the outliers within each block. Note that we multiply the weight matrix with the transpose of the rotation and permutation matrices at the same time, preserving the linear layer equivalence and smoothing weights. Theoretical analysis confirms that the rotation and permutation transformations greatly mitigate quantization challenges induced by outliers.\n\nAs a result, DuQuant offers several clear advantages over QuaRot [2]: (1) DuQuant’s optimal rotation matrix, derived through a greedy search guided by prior knowledge, surpasses QuaRot’s Hadamard rotation in managing outliers; (2) our unique zigzag permutation significantly reduces activation variance across blocks, providing a distinct advantage for handling massive outliers; and (3) by jointly smoothing weights and activations, DuQuant avoids time-consuming GPTQ algorithm in QuaRot. Extensive evaluations demonstrate that our DuQuant approach significantly outperforms existing 4-bit weight-activation quantization baselines across various benchmarks. Notably, DuQuant achieves a $5 \\%$ improvement in Commonsense QA tasks across all LLaMA model sizes and a $10 \\%$ increase in zero-shot MMLU benchmarks for the Vicuna-v1.5-13B. Moreover, in practical applications with the LLaMA2-7B model, DuQuant not only accelerates pre-filling phase by up to $2 . 0 8 \\times$ but also reduces memory usage during decoding phase by $3 . 5 0 \\times$ , with minimal impact on performance: only a 0.61 increase in perplexity and a $2 . 7 1 \\%$ drop in accuracy compared to the FP16 model. These results highlight the effectiveness of DuQuant in enhancing the efficiency and capacity of quantized LLMs.\n\n# 2 Motivation\n\nNormal Outliers and Massive Outliers. Previous works [13, 74, 33] have highlighted the challenge posed by activation outliers in LLMs for model compression. These outlier features consistently manifest large values across specific feature dimensions and are present in all token sequences [65], which we refer to as Normal Outliers. Recently, a distinct type of outlier [49, 36], termed Massive Outliers, has been observed in LLMs. The primary distinctions between normal and massive outliers are: 1) Normal outliers persist across all token sequences, whereas massive outliers are confined to a limited number of tokens. 2) Massive outliers exhibit significantly larger magnitudes, often surpassing 100 and being approximately 1000 times greater than the median of other activations [49]. In our study, we delve deeper into the impact of these two distinct types of outliers on quantization.\n\nMassive Outliers Exist at the Second Linear Layer of FFN Module. In contrast to previous studies [49, 36] that observe massive outliers at the output of Transformer blocks, we first discover that these extremely large activations exist at the input of the down-projection layer within the FFN module. As depicted in Figure 1, the input of the down-projection layer in the LLaMA2-7B model Layer 1 contains a single activation of significant magnitude (approximately 1400). This activation is isolated to one token and therefore classified as one of massive activations. This phenomenon is consistently observed across different layers and sizes of models, as illustrated in Appendix I.\n\nMassive Outliers Enlarge Quantization Difficulty. Although previous studies [65, 48, 40, 1] have proposed various approaches to eliminate outlier features, they still face challenges in effectively managing massive outliers. SmoothQuant [65], for instance, attempts to shift the quantization difficulty from activations to weights by dividing the activation by a per-channel smoothing factor and multiplying it to the weight matrix. Nevertheless, we observe that this transfer at the input of the down-projection layer can cause the weights of the down-projection to display noticeable outliers, as demonstrated in Figure 1 . This issue arises because massive outliers cause the smoothing factor to become significantly large. Moreover, extremely large outliers can lead optimization-based methods to encounter problems with loss explosion. Both OmniQuant [48] and AffineQuant [40] have had to exclude their learnable parameters for the down projection layer due to unstable gradients. Given the poor accuracy observed with 4-bit quantization, QUIK [1] opts to use INT8 quantization for the down projection layer and Atom [76] applies INT8 quantization for 128 outlier channels. Consequently, massive outliers introduce new challenges to the quantization process that existing methods cannot fully address. This observation has motivated us to develop rotation and permutation transformations, which effectively handles both massive and normal outliers and achieves state-of-the-art performance.\n\n# 3 Method\n\nIn this section, we delve into the distribution of outliers and introduce our proposed DuQuant method. The DuQuant method is built on two key components: 1) the block-diagonal rotation matrix, tasked with the local redistribution of feature outliers, and 2) the zigzag permutation, responsible for the global reordering of outliers across different blocks.\n\n# 3.1 Preliminaries\n\nAs the common modules within each transformer block of LLMs, both Multi-head Self-Attention (MSA) and Feed-Forward Network (FFN) fundamentally consist of basic linear layers, which can be represented as, $\\mathbf { Y } = \\mathbf { X } \\cdot \\mathbf { W } \\in \\mathbb { R } ^ { T \\times C _ { o u t } }$ . Here, $\\mathbf { X } \\in \\mathbf { \\bar { \\mathbb { R } } } ^ { T \\times C _ { i n } }$ is the activation input and $\\mathbf { W } \\in$ $\\mathbb { R } ^ { C _ { i n } \\times C _ { o u t } }$ denotes the weight matrix. In this paper, we focus on integer uniform quantization [26] of both activation and weight, aiming to achieve better hardware support. Specifically, the $b$ -bit quantization process maps the FP16 tensor $\\mathbf { X }$ to low-bit integer $\\mathbf { X } _ { q }$ :\n\n$$\n\\mathbf { X } _ { q } = \\mathrm { c l a m p } \\left( \\left\\lfloor \\frac { \\mathbf { X } } { \\Delta } \\right\\rfloor + z , 0 , 2 ^ { b } - 1 \\right) , \\mathrm { w h e r e } \\Delta = \\frac { \\operatorname* { m a x } ( \\mathbf { X } ) - \\operatorname* { m i n } ( \\mathbf { X } ) } { 2 ^ { b } - 1 } , z = - \\left\\lfloor \\frac { \\operatorname* { m i n } ( \\mathbf { X } ) } { \\Delta } \\right\\rceil .\n$$\n\nThe notation $\\lfloor \\cdot \\rceil$ means the nearest rounding operation, $\\Delta$ is the quantization step size and $z$ represents the zero point. Following [65, 48, 35, 40], we employ per-token quantization for activation and per-channel quantization for weight, which entails assigning different step sizes to individual tokens of activations $\\mathbf { \\Delta } \\Delta \\mathbf { x } \\in \\mathbb { R } ^ { T \\times 1 } ,$ ) and different output channels of weights $( \\Delta _ { \\mathbf { W } } \\in \\mathbb { R } ^ { 1 \\times C _ { o u t } } .$ ).\n\n# 3.2 The proposed DuQuant Method\n\nTo address the Normal Outliers issue stated in Section 2, current quantization methods, such as SmoothQuant [65] and OmniQuant [65], usually adopt the smooth technique. Concretely, it involves the utilization of a per-channel smoothing diagonal matrix, denoted as $\\pmb { \\Lambda }$ , to scale the input activation and weight matrix. The adjustment allows us to rewrite the original linear layer as $\\mathbf { Y } = \\bar { \\mathbf { X } } { \\cdot } \\mathbf { W } = ( \\mathbf { X } { \\cdot }$\n\n[1.75 .5 .5 0.30   \n1.50 0.4 0.25   \n1.2 0.20   \n0.75 0.15   \n0.25 Rota①tion Permu②tation Rota③tion 1000 Channel 200 3000 4000 100 → 1000 Channe 2000 3000 4000 y 4000 > 1000 Channe 2000 3000 4000 50 150 (a) Normal outlier 1.3 2.9 0.2 1.2 1.2 3.5 9.6 1.4 4.3 1.3 3.2 0.3 2.4 4.2 2.5 ① Abs Max 0.42 21.13 1.5 0.8 02.82 20.73 8.7 Rotation 0.39 3.4 14.82 1.9 1.20 4.29 43.72 03.74 𝑉𝑎1𝑟.𝑖6𝑎5𝑛𝑐𝑒 1.5 0.6 1.3 0.5 1.8 4.2 8.9 2.2 4.9 0.7 2.9 0.3 3.9 3.5 0.9 Outlier 1.5 2.9 9.9 6.9 2.2 4.2 7.9 9.7 Outlier 2.2 5.5 4.2 5.8 1.2 4.8 4.7 3.4 2000 Activation Matrix Sorted Index 7 2 5 8 3 4 6 1 ② Permutation Easy to Quantize √ Outlier 3.0 0.9 1.5 1.6 2.9 1.2 1.8 1.4 Zigzag Index 1 4 5 8 2 3 6 7 32.08 0.93 01.32 0.26 2.95 0.59 10.87 01.91 Rota③tion 31.29 4.27 1.38 01.32 4.3 24.42 0.7 10.43 𝑉𝑎𝑟𝑖𝑎𝑛𝑐𝑒 1.6 0.4 1.5 1.6 2.1 0.2 0.1 0.2 3.2 4.2 1.0 3.4 4.9 3.4 0.9 0.175 100 2.3 0.9 0.8 0.1 1.9 1.1 0.5 1.4 2.9 3.5 0.7 0.3 4.9 3.9 0.9 2.2 (b) Massive outlier ： (c) Example of Rotation and Permutation Transformation\n\n${ \\pmb { \\Lambda } } ) ( { \\pmb { \\Lambda } } ^ { - 1 } { \\boldsymbol { \\cdot } } { \\mathbf { W } } )$ . The diagonal element $\\Lambda _ { j }$ within $\\pmb { \\Lambda }$ is computed as $\\Lambda _ { j } = \\operatorname* { m a x } ( | \\mathbf { X } _ { j } | ) ^ { \\alpha } / \\operatorname* { m a x } ( | \\mathbf { W } _ { j } | ) ^ { 1 - \\alpha }$ where $\\alpha$ is a hyper-parameter representing the migration strength. However, despite the ability of this smoothing technique to shift the quantization challenge from activations to weights, it still faces difficulties in effectively managing Massive Outliers, as depicted in Figure 1. This challenge stems from the extremely large massive outliers inducing large scaling factors $\\Lambda _ { j }$ , which in turn introduce new outliers in the weight matrix and result in significant performance declines in 4-bit quantization.\n\nAccording to these findings, we propose the DuQuant method, which includes the Rotation and Permutation transformations based on the smooth technique. By combining rotation transformation and channel permutation, our DuQuant method aims to redistribute these features within the activation space, thereby mitigating the effects of both Normal and Massive Outliers.\n\nThe Rotation Transformation. In contrast to the smooth technique, our aim is to apply a rotation matrix for row or column transformations, mitigating the impact of both Normal and Massive outliers. The ideal rotation matrix, denoted as $\\mathbf { R }$ , should possess the following properties: 1) $\\mathbf { R }$ is an orthogonal matrix satisfying $\\mathbf { R R } ^ { \\top } = \\mathbf { I }$ and $| \\mathbf { R } | = \\pm 1$ . This allows us to reformulate the linear layer within the transformer as $\\mathbf { Y } = \\mathbf { X } \\cdot \\mathbf { W } = ( \\mathbf { X } \\mathbf { R } ) ( \\mathbf { R } ^ { \\top } \\mathbf { W } ) ; 2 ) \\mathbf { R }$ should be capable of effectively target the positions of outliers and effectively mitigating them through matrix multiplication. However, due to the Massive Outliers are usually randomly distributed within the activation space, it is challenging to directly identify the optimal rotation matrix $\\mathbf { R }$ capable of mitigating outliers through a single rotation transformation. To address this problem, we employ a greedy search with prior knowledge to compute a rotation matrix $\\hat { \\mathbf { R } }$ , thereby approximating the ideal rotation matrix $\\mathbf { R }$ . Specifically, the calculation of $\\hat { \\mathbf { R } }$ involves the following steps,\n\n$0$ Identify the feature dimension $d ^ { ( 1 ) }$ where the outlier are primarily concentrated, i.e., $d ^ { ( 1 ) } =$ arg $\\operatorname* { m a x } _ { j } ( \\operatorname* { m a x } _ { i } | \\mathbf { X } _ { i j } | )$ . Here, $\\mathbf { X } _ { i j }$ represents the element in the $i$ -th row and $j$ -th column of $\\mathbf { X }$ . $0$ Based on the searched dimensions $d ^ { ( 1 ) }$ , we construct the rotation matrix as follows,\n\n$$\n\\mathbf { R } ^ { 1 } = \\mathbf { E } _ { d ^ { ( 1 ) } } \\tilde { \\mathbf { R } } \\mathbf { Q } \\mathbf { E } _ { d ^ { ( 1 ) } } , \\qquad \\mathbf { Q } = \\left[ \\begin{array} { l l } { 1 } & { \\mathbf { O } } \\\\ { \\mathbf { O } } & { \\mathbf { Q } ^ { \\prime } } \\end{array} \\right] .\n$$\n\nHere, $\\mathbf { E } _ { d ^ { ( 1 ) } }$ is the switching matrix used to swap the first and the $d ^ { ( 1 ) }$ -th columns of the activation, and $\\tilde { \\mathbf { R } }$ represents an orthogonal initialized rotation matrix, in which the first row is specifically uniformly distributed. The motivation behind this is to mitigate outliers in the first column after the transformation by $\\mathbf { E } _ { d ^ { ( 1 ) } }$ . To further increase the randomness of the rotation operation, we retain the first column, where outliers have been mitigated, and randomly rotate the other columns by multiplying them with a random orthogonal matrix $\\mathbf { Q } ^ { \\prime }$ .\n\nLet $N$ denote the greedy search steps, then the approximated rotation matrix $\\hat { \\mathbf { R } } = \\mathbf { R } ^ { 1 } \\mathbf { R } ^ { 2 } \\cdot \\cdot \\cdot \\mathbf { R } ^ { n }$ , where $n = \\arg \\operatorname* { m i n } _ { k \\in [ 1 : N ] }$ $\\left( \\operatorname* { m a x } _ { i , j } | ( \\mathbf { X } \\mathbf { R } ^ { 1 } \\cdot \\cdot \\cdot \\mathbf { \\tilde { R } } ^ { k } ) _ { i j } | \\right)$ . Each $\\mathbf { R } ^ { i }$ is constructed according to Eqn. (2) and the identified feature dimension $\\boldsymbol { d } ^ { ( i ) }$ . Appendix $\\mathbf { G }$ provides detailed pseudo code.\n\nThrough this construction manner, we can ensure that the approximated optimal rotation matrix $\\hat { \\textbf { R } }$ can effectively mitigate outliers with large magnitudes, as opposed to merely using a randomly selected orthogonal rotation matrix. Nevertheless, directly constructing the entire rotation matrix is timeconsuming and results in substantial memory overhead. For fast matrix multiplication, following [64], we approximate the rotation matrix $\\hat { \\mathbf { R } } \\in \\mathbb { R } ^ { \\dot { C } _ { i n } \\times C _ { i n } }$ in a block-wise manner,\n\n$$\n\\begin{array} { r } { \\hat { \\bf R } = \\mathrm { { \\ B l o c k } } { \\mathrm { D i a g } } ( \\hat { \\bf R } _ { b _ { 1 } } , . . . , \\hat { \\bf R } _ { b _ { K } } ) , } \\end{array}\n$$\n\nwhere $\\hat { \\mathbf { R } } _ { b _ { i } } \\in \\mathbb { R } ^ { 2 ^ { n } \\times 2 ^ { n } }$ denotes a square matrix of the $i$ -th block, which is constructed following the three steps mentioned above. And the block numbers $K$ is calculated by $K = C _ { i n } / 2 ^ { n }$ .\n\nThe Permutation Transformation. Despite adopting the block-diagonal rotation matrix $\\hat { \\mathbf { R } }$ for its time and storage efficiency, its focus on local information introduces a potential limitation in further reducing the outliers. This is because the rotation transformation, conducted within each small block, cannot integrate the information across different blocks to further minimize outliers. Consequently, one block may have relatively larger outliers while another block has smaller outliers, resulting in high variance among different blocks, as shown in Figure 2. This limitation explains that merely utilizing the block-diagonal rotation matrix is insufficient to effectively reduce the overall outliers.\n\nTo effectively mitigate the overall outliers, it is essential to balance the outliers’ magnitudes among various blocks. Specifically, within each small block, we denote the largest outlier in dimension $d _ { j }$ as $O _ { j }$ . Meanwhile, $M _ { b _ { i } }$ represents the mean value of all $O _ { j }$ in the $i$ -th block, where $i = 1 , 2 , . . . , \\check { K }$ . Then the variance in activation magnitudes across various blocks can be expressed as,\n\n$$\n\\mathrm { V a r } ( [ M _ { b _ { 1 } } , M _ { b _ { 2 } } , . . . , M _ { b _ { K } } ] ) .\n$$\n\nTo minimize this variance and further reduce the overall outliers, we introduce the zigzag permutation. Concretely, we generate a zigzag sequence that starts by assigning channels with the highest activations to the first block. The process continues by assigning channels with the next highest activations to the subsequent blocks in descending order until the end of block $K$ . Upon reaching the final block, the order reverses, starting from the channel with the next highest activation and proceeding in ascending order. This back-and-forth patterning continues throughout all the blocks, ensuring that no single block consistently receives either the highest or lowest activation channels. It is worth noting that the constructed permutation is an orthogonal matrix, which we denote as $\\mathbf { P }$ , satisfying the conditions $\\mathbf { P } \\mathbf { P } ^ { \\top } = \\mathbf { I }$ and $| \\mathbf { P } | = \\pm 1$ . By employing the zigzag permutation, we achieve a balanced distribution of outliers across different blocks. This allows us to use an additional rotation transformation to further smooth the outliers. Figure 2 provides an illustration of outlier mitigation.\n\nThe Overall DuQuant Method. To effectively mitigate both Normal and Massive Outliers, we first employ the smooth technique to shift the quantization challenge from activations to weights. Next, we introduce the block-diagonal rotation matrix $\\hat { \\mathbf { R } }$ to locally redistribute feature outliers within the activation space. We then propose the zigzag permutation matrix for globally balancing the outliers across different blocks, followed by another application of the block-diagonal rotation transformation. To sum up, the linear layers within the transformer can be rewrite as,\n\n$$\n\\mathbf { Y } = \\mathbf { X } \\cdot \\mathbf { W } = [ ( \\mathbf { X } \\cdot \\underbrace { \\mathbf { \\overset { A } { \\mathbf { X } } } ( \\mathbf { \\overset { \\cdot } { \\mathbf { R } } } _ { ( 1 ) } \\cdot \\mathbf { P } \\cdot \\hat { \\mathbf { R } } _ { ( 2 ) } } _ { \\mathbf { G } } ] \\cdot \\underbrace { [ \\hat { \\mathbf { R } } _ { ( 2 ) } ^ { \\top } \\cdot \\mathbf { P } ^ { \\top } \\cdot \\hat { \\mathbf { R } } _ { ( 1 ) } ^ { \\top } ( \\mathbf { \\overset { \\cdot } { \\mathbf { X } } } ^ { - 1 } \\cdot \\mathbf { W } ) ] } _ { \\mathbf { G } ^ { - 1 } } ] ,\n$$\n\nwhere the notation $\\mathbf { P }$ denotes the orthogonal permutation matrix learned via the zigzag manner, the $\\hat { \\mathbf { R } } _ { ( 1 ) }$ and $\\hat { \\mathbf { R } } _ { ( 2 ) }$ represent the first and second block-diagonal rotation matrix, respectively.\n\nRemark 1. It is worth noting that the proposed DuQuant method can simultaneously smooth the weight matrix. While the commonly adopted smooth technique is effective, it can cause the weight matrix of the down-projection layer to exhibit pronounced outliers, leading to performance degradation. However, in the proposed DuQuant method, the rotation transformation we designed is applied to not only the activation input but also the weight matrix. As a result, the outliers induced by the smooth technique can be mitigated through our approximated rotation matrix $\\hat { \\mathbf { R } }$ , yielding a smoother, more quantization-friendly weight matrix. Moreover, this approach eliminates the reliance on complex weight quantization techniques, such as GPTQ [18] used in Atom [76] and QuaRot [2].\n\nRemark 2. To further decrease the computation and memory costs, we initially construct the $k$ -th block rotation matrix $\\hat { \\mathbf { R } } _ { b _ { k } }$ , with the $k$ -th block containing the largest outlier. We then assign $\\hat { \\mathbf { R } } _ { b _ { i } } =$ $\\hat { \\mathbf { R } } _ { b _ { k } }$ for all $1 \\leq i \\leq K$ . This strategy not only effectively mitigates the impact of outliers, but also reduces the number of block rotation matrices from $K$ to 1, significantly reducing computation and memory requirements. Importantly, incorporating the invertible matrix $\\mathbf { G }$ from Eqn. (5) significantly eases the quantization challenges for $\\mathbf { X }$ and $\\mathbf { W }$ . Consequently, the quantization process acts as $\\mathbf { Y } = ( \\mathbf { X } \\mathbf { G } ) ( \\mathbf { G } ^ { - 1 } \\mathbf { W } ) = \\hat { \\mathbf { X } } \\cdot \\hat { \\dot { \\mathbf { W } } } \\approx \\Delta _ { \\hat { \\mathbf { X } } } \\Delta _ { \\hat { \\mathbf { W } } } ( \\hat { \\mathbf { X } } _ { q } - z _ { \\hat { \\mathbf { X } } } ) ( \\hat { \\mathbf { W } _ { q } } - z _ { \\hat { \\mathbf { W } } } )$ .\n\n# 3.3 Theoretical Analysis\n\nTo further demonstrate the effectiveness of the proposed DuQuant method, we conduct a theoretical analysis of the rotation and permutation transformations. Theorem 1 shows that within each block, the constructed rotation matrix effectively mitigates the maximum outlier, thereby reducing the outlier magnitude through a greedy search. Theorem 2 reveals that the employed zigzag permutation ensures a balanced upper bound shared among different blocks. This suggests that the zigzag permutation effectively reduces the variance shown in Eqn. (4) and thus assists the rotation matrix in further decreasing the outliers. Please refer to Appendix B for detailed proofs.\n\nTheorem 1 (Rotation). For the activation input $\\mathbf { X } \\in \\mathbb { R } ^ { T \\times C _ { i n } }$ , $\\hat { \\mathbf { R } } \\in \\mathbb { R } ^ { 2 ^ { n } \\times 2 ^ { n } }$ is a diagonal block matrix constructed as per Eqn. (3). For a specific block $b _ { i }$ , let $O _ { j } ( \\cdot )$ represent the maximum outlier of the $j$ -th dimension $d _ { j }$ within the input. Then, we can deduce that,\n\n$$\n\\operatorname* { m a x } _ { 1 \\leq j \\leq 2 ^ { n } } \\ O _ { j } ( { \\bf X } _ { b _ { i } } { \\hat { \\bf R } } _ { b _ { i } } ) \\leq \\operatorname* { m a x } _ { 1 \\leq j \\leq 2 ^ { n } } \\ O _ { j } ( { \\bf X } _ { b _ { i } } ) .\n$$\n\nTheorem 2 (Zigzag Permutation). For the activation input $\\mathbf { X } \\in \\mathbb { R } ^ { T \\times C _ { i n } }$ , it can be divided into $K$ blocks, where $K = C _ { i n } / 2 ^ { n }$ . Let $O _ { j }$ denote the max outlier of the dimension $d _ { j }$ in $\\mathbf { X }$ , the reordered outliers from large to small is expressed as ${ \\cal O } ^ { ( 1 ) } , { \\cal O } ^ { ( 2 ) } , . . . , { \\cal O } ^ { ( C _ { i n } ) }$ . Moreover, the $M _ { b _ { i } }$ represents the mean value of all $O _ { j }$ in the $i$ -th block, $i = 1 , 2 , . . . , K$ . Let $\\delta : = \\operatorname* { m a x } \\{ | O ^ { ( i + 1 ) } - O ^ { ( i ) } | \\} , i =$ $1 , 2 , . . . , C _ { i n } - 1$ . Then, following the zigzag permutation described in Section 3.2, the mean value $M _ { b _ { i } }$ within each $i$ -th block consistently satisfies,\n\n$$\nM _ { b _ { i } } \\leq { \\cal O } ^ { ( 1 ) } + \\frac { ( 2 ^ { n } K - 1 ) ( 2 ^ { n - 1 } - 1 ) } { 2 ^ { n } } \\delta , \\qquad i = 1 , 2 , 3 , . . . , K .\n$$\n\n# 4 Experiment\n\nModels and Evaluations. We apply our DuQuant on pre-trained LLMs: LLaMA (7B-65B) [52], LLaMA2 (7B-70B) [53], LLaMA3 (8B, 70B), Mistral, Phi2 and instruction-tuned LLMs: Vicunav1.5 (7B-13B) [10]. We evaluate quantized pre-trained LLMs on language generation tasks and commonsense QA tasks. Specifically, we assess the perplexity on WikiText2 [41] and C4 [45] datasets, as well as the zero-shot accuracy on PIQA [6], ARC [12], BoolQ [11], HellaSwag [71], and WinoGrande [46] datasets. Moreover, we evaluate quantized Vicuna models on MMLU [22] and MT-Bench [78] benchmarks, as well as their long-form generative capabilities on LongBench [4].\n\nImplementation Details. In line with prior studies [35, 48, 40], we apply per-token activation quantization and per-channel weight quantization. Given that W8A8 quantization has been established as lossless in precision by SmoothQuant [65], our primary evaluation in this paper focuses on 4-bit and 6-bit quantization for weights and activations. As for details, we quantize all intermediate activations, excluding the SoftMax output. Moreover, we have developed two types of quantized models, denoted as DuQuant and DuQuant+LWC . For DuQuant, we employ round-to-nearest quantization, using a clipping ratio of 0.9 for activations and 0.8 for weights. To improve weight matrix quantization, DuQuant+LWC integrates the learnable weight clipping (LWC) technique from OmniQuant. Concretely, LWC adjusts weights by training parameters $\\gamma , \\beta \\in [ 0 , 1 ]$ to compute step size $\\begin{array} { r } { \\Delta = \\frac { \\gamma \\operatorname* { m a x } ( \\mathbf { X } ) - \\beta \\operatorname* { m i n } ( \\mathbf { X } ) } { 2 ^ { b } - 1 } } \\end{array}$ in Eqn. (1). Notably, the smoothing diagonal matrix and the learned weight clipping factor can be integrated into the quantized weights, introducing no additional computational or memory costs. More details and hyperparameters are left in Appendix C.\n\nBaselines. We compare with state-of-the-art (SOTA) weight-activation PTQ methods, including SmoothQuant [65], Outlier Supression $^ +$ [60], OmniQuant [48], QLLM [35], AffineQuant [40], and Atom [76]. For Atom, we reproduce the results with no group-wise asymmetric quantization.\n\nTable 1: Perplexity (↓) results under 4-bit weight-activation quantization. The results for W6A6 can be found in Table D8. Atom and OmniQuant unprocessed group-query attention for LLaMA2-70B.   \n\n<html><body><table><tr><td>Dataset</td><td>#Bit</td><td>Method</td><td>1-7B</td><td>1-13B</td><td>1-30B</td><td>1-65B</td><td>2-7B</td><td>2-13B</td><td>2-70B</td></tr><tr><td rowspan=\"6\">WikiText2</td><td>FP16</td><td></td><td>5.68</td><td>5.09</td><td>4.10</td><td>3.53</td><td>5.47</td><td>4.88</td><td>3.31</td></tr><tr><td></td><td>SmoothQuant OmniQuant</td><td>25.25 11.26</td><td>40.05</td><td>192.40</td><td>275.53</td><td>83.12</td><td>35.88</td><td>26.01</td></tr><tr><td rowspan=\"5\">W4A4</td><td></td><td></td><td>10.87</td><td>10.33</td><td>9.17</td><td>14.26</td><td>12.30</td><td>NaN</td></tr><tr><td>AffineQuant</td><td>10.28</td><td>10.32</td><td>9.35</td><td></td><td>12.69</td><td>11.45</td><td></td></tr><tr><td>QLLM</td><td>9.65</td><td>8.41</td><td>8.37</td><td>6.87</td><td>11.75</td><td>9.09</td><td>7.00</td></tr><tr><td>Atom</td><td>8.15</td><td>7.43</td><td>6.52</td><td>5.14</td><td>8.40</td><td>6.96</td><td>NaN</td></tr><tr><td>DuQuant</td><td>6.40</td><td>5.65</td><td>4.72</td><td>4.13</td><td>6.28</td><td>5.42</td><td>3.79</td></tr><tr><td></td><td>FP16</td><td>DuQuant+Lwc</td><td>6.18 7.08</td><td>5.47 6.61</td><td>4.55</td><td>3.93</td><td>6.08</td><td>5.33</td><td>3.76</td></tr><tr><td rowspan=\"6\">C4</td><td></td><td></td><td></td><td></td><td>5.98</td><td>5.62</td><td>6.97</td><td>6.46</td><td>5.52</td></tr><tr><td rowspan=\"6\">W4A4</td><td>SmoothQuant</td><td>32.32</td><td>47.18</td><td>122.38</td><td>244.35</td><td>77.27</td><td>43.19</td><td>34.61</td></tr><tr><td>OmniQuant</td><td>14.51</td><td>13.78</td><td>12.49</td><td>11.28</td><td>18.02</td><td>14.55</td><td>NaN</td></tr><tr><td>AffineQuant</td><td>13.64</td><td>13.44</td><td>11.58</td><td></td><td>15.76</td><td>13.97</td><td></td></tr><tr><td>QLLM</td><td>12.29</td><td>10.58</td><td>11.51</td><td>8.98</td><td>13.26</td><td>11.13</td><td>8.89</td></tr><tr><td>Atom</td><td>10.34</td><td>9.57</td><td>8.56</td><td>8.17</td><td>10.96</td><td>9.12</td><td>NaN</td></tr><tr><td>DuQuant DuQuant+Lwc</td><td>7.84 7.73</td><td>7.16 7.07</td><td>6.45 6.37</td><td>6.03 5.93</td><td>7.90 7.79</td><td>7.05 7.02</td><td>5.87 5.85</td></tr></table></body></html>\n\nTable 2: Zero-shot QA ( ) results of LLaMA1 models under 4-bit weight-activation quantization. The results for LLaMA2 models and W6A6 quantization can be found in Table D1 D9, and D10.   \n\n<html><body><table><tr><td>Model</td><td>Method</td><td>PIQA</td><td>ARC-E</td><td>ARC-C</td><td>BoolQ</td><td>HellaSwag</td><td>WinoGrande</td><td>Avg.</td></tr><tr><td rowspan=\"8\">LLaMA1-7B W4A4</td><td>FP16</td><td>77.47</td><td>52.48</td><td>41.46</td><td>73.08</td><td>73.00</td><td>67.07</td><td>64.09</td></tr><tr><td>SmoothQuant</td><td>49.80</td><td>30.40</td><td>25.80</td><td>49.10</td><td>27.40</td><td>48.00</td><td>38.41</td></tr><tr><td>OS+</td><td>62.73</td><td>39.98</td><td>30.29</td><td>60.21</td><td>44.39</td><td>52.96</td><td>48.43</td></tr><tr><td>OmniQuant</td><td>66.15</td><td>45.20</td><td>31.14</td><td>63.51</td><td>56.44</td><td>53.43</td><td>52.65</td></tr><tr><td>AffineQuant</td><td>69.37</td><td>42.55</td><td>31.91</td><td>63.73</td><td>57.65</td><td>55.33</td><td>53.42</td></tr><tr><td>QLLM</td><td>68.77</td><td>45.20</td><td>31.14</td><td></td><td>57.43</td><td>56.67</td><td>51.84</td></tr><tr><td>Atom</td><td>71.44</td><td>47.74</td><td>35.49</td><td>67.71</td><td>63.89</td><td>55.01</td><td>56.88</td></tr><tr><td>DuQuant</td><td>76.44</td><td>50.04</td><td>38.99</td><td>70.98</td><td>69.39</td><td>64.72</td><td>61.76</td></tr><tr><td rowspan=\"10\">LLaMA1-13B W4A4</td><td>DuQuant+Lwc</td><td>76.22</td><td>50.04</td><td>38.31</td><td>70.09</td><td>69.82</td><td>62.59</td><td>61.18</td></tr><tr><td>FP16</td><td>79.10</td><td>59.89</td><td>44.45</td><td>68.01</td><td>76.21</td><td>70.31</td><td>66.33</td></tr><tr><td>SmoothQuant</td><td>61.04</td><td>39.18</td><td>30.80</td><td>61.80</td><td>52.29</td><td>51.06</td><td>49.36</td></tr><tr><td>OS+</td><td>63.00</td><td>40.32</td><td>30.38</td><td>60.34</td><td>53.61</td><td>51.54</td><td>49.86</td></tr><tr><td>OmniQuant</td><td>69.69</td><td>47.39</td><td>33.10</td><td>62.84</td><td>58.96</td><td>55.80</td><td>54.37</td></tr><tr><td>AffineQuant</td><td>66.32</td><td>43.90</td><td>29.61</td><td>64.10</td><td>56.88</td><td>54.70</td><td>52.58</td></tr><tr><td>QLLM</td><td>71.38</td><td>47.60</td><td>34.30</td><td></td><td>63.70</td><td>59.43</td><td>55.28</td></tr><tr><td>Atom</td><td>71.38</td><td>49.07</td><td>36.69</td><td>64.53</td><td>68.00</td><td>58.56</td><td>58.04</td></tr><tr><td>DuQuant</td><td>77.26</td><td>58.04</td><td>41.55</td><td>67.55</td><td>73.62</td><td>66.69</td><td>64.12</td></tr><tr><td>DuQuant+Lwc</td><td>77.64</td><td>57.32</td><td>41.21</td><td>66.79</td><td>74.12</td><td>65.98</td><td>63.84</td></tr><tr><td rowspan=\"10\">LLaMA1-30B W4A4</td><td>FP16</td><td>80.08</td><td>58.92</td><td>45.47</td><td>68.44</td><td>79.21</td><td>72.53</td><td>67.44</td></tr><tr><td>SmoothQuant</td><td>58.65</td><td>35.53</td><td>27.73</td><td>60.42</td><td>35.56</td><td></td><td></td></tr><tr><td>OS+</td><td>67.63</td><td>46.17</td><td>34.40</td><td>60.70</td><td>54.32</td><td>48.06</td><td>44.83</td></tr><tr><td>OmniQuant</td><td>71.21</td><td>49.45</td><td>34.47</td><td>65.33</td><td></td><td>52.64</td><td>52.62</td></tr><tr><td>AffineQuant</td><td></td><td></td><td></td><td></td><td>64.65</td><td>59.19</td><td>56.63</td></tr><tr><td></td><td>70.84</td><td>49.41</td><td>37.12</td><td>70.12</td><td>65.53</td><td>58.64</td><td>58.61</td></tr><tr><td>QLLM</td><td>73.83</td><td>50.67</td><td>38.40</td><td></td><td>67.91</td><td>58.56</td><td>57.87</td></tr><tr><td>Atom</td><td>71.98</td><td>49.07</td><td>40.02</td><td>66.85</td><td>70.45</td><td>58.64</td><td>59.50</td></tr><tr><td>DuQuant DuQuant+Lwc</td><td>78.56 78.73</td><td>56.99 56.52</td><td>42.32 43.17</td><td>66.73 68.84</td><td>76.70</td><td>69.61</td><td>65.15</td></tr><tr><td>FP16</td><td></td><td></td><td></td><td></td><td>77.53</td><td>70.96</td><td>65.96</td></tr><tr><td rowspan=\"7\">LLaMA1-65B W4A4</td><td></td><td>80.79</td><td>58.71</td><td>46.24</td><td>82.29</td><td>80.72</td><td>77.50</td><td>71.04</td></tr><tr><td>SmoothQuant</td><td>64.47</td><td>40.44</td><td>29.82</td><td>59.38</td><td>39.90</td><td>52.24</td><td>47.71</td></tr><tr><td>OS+</td><td>68.06</td><td>43.98</td><td>35.32</td><td>62.75</td><td>50.73</td><td>54.30</td><td>52.52</td></tr><tr><td>OmniQuant</td><td>71.81</td><td>48.02</td><td>35.92</td><td>73.27</td><td>66.81</td><td>59.51</td><td>59.22</td></tr><tr><td>QLLM</td><td>73.56</td><td>52.06</td><td>39.68</td><td></td><td>70.94</td><td>62.90</td><td>59.83</td></tr><tr><td>Atom</td><td>74.48</td><td>51.60</td><td>40.61</td><td>73.76</td><td>73.78</td><td>62.12</td><td>62.73</td></tr><tr><td>DuQuant DuQuant+Lwc</td><td>79.71 79.98</td><td>57.95 58.29</td><td>45.05 44.80</td><td>79.82 77.89</td><td>78.66 79.22</td><td>72.29 72.21</td><td>68.91 68.73</td></tr></table></body></html>\n\n# 4.1 Main Results\n\nQuantization of LLaMA1 and LLaMA2 Models. We conduct a comprehensive comparison of our DuQuant with several SOTA baselines on LLaMA1 and LLaMA2 models. Results for W4A4 quantization are presented in this Section, while results for W6A6 quantization are provided in\n\nTable 3: Zero-shot and five-shot results on the MMLU benchmark for Vicuna-v1.5-13B under 4-bit weight-activation quantization. The results for Vicuna-v1.5-7b can be found in Table D2.   \n\n<html><body><table><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Method</td><td colspan=\"5\">MMLU (0 shot) ↑</td><td colspan=\"5\">MMLU (5 shot) ↑</td></tr><tr><td>STEM</td><td>Hums</td><td>Social</td><td>Others</td><td>Avg.</td><td>STEM</td><td>Hums</td><td>Social</td><td>Others</td><td>Avg.</td></tr><tr><td rowspan=\"6\">Vicuna-v1.5-13B</td><td>FP16</td><td>43.70</td><td>50.48</td><td>62.72</td><td>62.74</td><td>54.54</td><td>44.96</td><td>51.97</td><td>65.26</td><td>62.40</td><td>55.78</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>324</td><td></td><td>30.3</td><td>23.16</td><td>22.82</td><td>23</td><td>27</td><td>3.2</td><td>27.08</td><td>25.8</td></tr><tr><td>DuQuant</td><td>40.82</td><td>46.61</td><td>58.73</td><td>57.59</td><td>50.94</td><td>40.92</td><td>48.78</td><td>60.42</td><td>57.71</td><td>51.96</td></tr><tr><td>DuQuant+Lwc</td><td>40.13</td><td>47.48</td><td>58.86</td><td> 57.83</td><td>51.08</td><td> 41..42</td><td>48.52</td><td>58.73</td><td>57.74</td><td>51.61</td></tr></table></body></html>\n\nTable 4: Long-context generation results for 4-bit Vicuna models on the LongBench benchmark.   \n\n<html><body><table><tr><td>Vicuna</td><td>Setting</td><td>Qasper</td><td>QMSum</td><td>MultiNews</td><td>TREC</td><td>TriviaQA</td><td>SAMSum</td><td>DuReader</td><td>RepoBench-P</td><td>Avg</td></tr><tr><td rowspan=\"5\">Vicuwa-v1.5-7B</td><td>FP16</td><td>23.27</td><td>21.07</td><td>26.91</td><td>66.00</td><td>82.59</td><td>41.06</td><td>25.53</td><td>48.23</td><td>41.83</td></tr><tr><td></td><td></td><td></td><td>2.</td><td>100</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>1</td><td>204</td><td></td><td></td><td></td><td></td><td></td><td>24</td><td>4</td></tr><tr><td>DuQuant</td><td>19.98</td><td>21.15</td><td>25.85</td><td>64.00</td><td>78.91</td><td>42.24</td><td>23.15</td><td>47.66</td><td>40.37</td></tr><tr><td rowspan=\"5\">Vicuna-v1.5-13B</td><td>FP16</td><td>24.41</td><td>21.24</td><td>26.53</td><td>68.00</td><td>86.81</td><td>41.97</td><td>27.57</td><td>43.08</td><td>42.45</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td>2</td><td>23</td><td>0</td><td></td><td>82</td><td>2</td><td>17</td><td></td></tr><tr><td> DuQuant</td><td>18.93</td><td>20.72</td><td>26.59</td><td>66.50</td><td>83.04</td><td>42.67</td><td>26.02</td><td>38.09</td><td>40.32</td></tr></table></body></html>\n\nTable 5: Perplexity and QA results of LLaMA3-8B under 4-bit/6-bit weight-activation quantization.   \n\n<html><body><table><tr><td>#Bits</td><td>Method</td><td>WikiText2↓</td><td>C4↓</td><td>PTB↓</td><td>PIQA</td><td>ARC-E</td><td>ARC-C</td><td>BoolQ</td><td>HellaSwag</td><td>WinoGrande</td><td>Avg. ↑</td></tr><tr><td>FP16</td><td></td><td>6.14</td><td>8.88</td><td>9.91</td><td>80.85</td><td>77.78</td><td>53.41</td><td>81.28</td><td>79.16</td><td>72.84</td><td>74.22</td></tr><tr><td rowspan=\"5\">LLaMA3-8B W6A6</td><td>SmoothQuant</td><td>7.07</td><td>9.57</td><td>11.69</td><td>78.94</td><td>75.88</td><td>49.49</td><td>77.58</td><td>77.39</td><td>70.8</td><td>71.68</td></tr><tr><td></td><td></td><td></td><td>11.90</td><td>78.90</td><td>73.95</td><td>47.35</td><td>74.95</td><td>76.77</td><td>70.56</td><td>70.41</td></tr><tr><td>AmniQuant</td><td>7.234</td><td>9.82</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DuQuant</td><td>6.27</td><td>8.38</td><td>10.77</td><td>80.20</td><td>77.27</td><td>52.05</td><td>80.12</td><td>79.14</td><td>72.77</td><td>73.59</td></tr><tr><td>DuQuant+Lwc</td><td>6.27</td><td>8.38</td><td>10.78</td><td>79.71</td><td>77.57</td><td>53.07</td><td>80.00</td><td>78.70</td><td>73.09</td><td>73.69</td></tr><tr><td rowspan=\"6\">LLaMA3-8B W4A4</td><td>SmoothQuant</td><td>210.19</td><td>187.93</td><td></td><td></td><td></td><td></td><td></td><td>31.26</td><td></td><td></td></tr><tr><td>OmniQuant</td><td>3.64e3</td><td>2.80e3</td><td>278.02 3.09e3</td><td>54.57 50.22</td><td>31.9 26.94</td><td>24.23 24.57</td><td>52.72 37.98</td><td>26.55</td><td>51.14 50.20</td><td>40.97 36.08</td></tr><tr><td>AffineQuant</td><td>21.21e3</td><td>34.60e3</td><td>16.72e3</td><td>50.71</td><td>25.93</td><td>26.02</td><td>40.55</td><td>26.07</td><td>48.46</td><td>36.29</td></tr><tr><td>Atom</td><td>22.14</td><td>31.83</td><td>40.04</td><td>62.95</td><td>49.45</td><td>30.12</td><td>60.31</td><td>53.75</td><td>56.04</td><td>52.10</td></tr><tr><td>DuQuant</td><td>8.56</td><td>11.98</td><td>13.66</td><td>75.68</td><td>68.48</td><td>41.81</td><td>71.99</td><td>73.07</td><td>66.22</td><td>66.21</td></tr><tr><td>DuQuant+Lwc</td><td>8.06</td><td>11.29</td><td>13.19</td><td>76.22</td><td>70.41</td><td>43.69</td><td>74.34</td><td>73.87</td><td>67.80</td><td>67.72</td></tr></table></body></html>\n\nAppendix D. Table 1 indicates that our DuQuant quantized models notably outperform other baselines on both the WikiText2 and C4 datasets. Notably, LWC technique further enhances model capacity, with our DuQuant+LWC achieving comparable performance with FP16 models. Table 2 and Table D1 showcase the zero-shot accuracy of W4A4 quantization on Commonsense QA tasks, where DuQuant significantly improves the average accuracy. Our method surpasses QLLM by $+ 9 \\%$ , and Atom by $+ 5 \\%$ for all model sizes. These results demonstrate the superiority of our rotation and permutation transformation, which establishes new SOTA performance by effectively eliminating outlier features.\n\nQuantization of Instruction-tuned Models. We quantize Vicuna-v1.5 [10] models to assess the generalizability of our DuQuant. Table 3 illustrates that our quantized models surpass the baselines across all task categories on MMLU benchmark. For Vicuna-13B, our DuQuant+LWC surpasses Atom by $1 0 . 0 1 \\%$ under zero-shot settings and $6 . 9 5 \\%$ under five-shot settings. Moreover, we compare our DuQuant with Atom and OmniQuant using MT-Bench and utilize GPT-4 to evaluate the answers from quantized models. As shown in Figure 3, DuQuant quantized models significantly outperform both Atom and OmniQuant in win rates. Specifically, for Vicuna-7B, DuQuant only lost 16 and 1 times to Atom and OmniQuant, respectively, while achieving 68 and 155 wins against them.\n\nEvaluation of Long-context Generation. To further evaluate the long-text generative capabilities, we follow [38, 34, 56] and conduct a comprehensive comparison of DuQuant against state-of-the-art baselines on the LongBench [4], which includes a variety of generative tasks to provide a broader evaluation. We set the maximum sequence length to 3500 for Vicuna models, with results presented in Table 4. DuQuant achieves performance comparable to FP16 models, demonstrating the effectiveness of our dual transformations. More detailed results on different subtasks are listed in Table D3, D4.\n\nQuantization of LLaMA3 Models. LLaMA3, known for its superior performance in various tasks, faces significant degradation in low-bit quantization [25]. To address this, we apply our DuQuant to quantize LLaMA3-8B. Table 5 displays the perplexity and zero-shot accuracy results. Notably, under W6A6 setting, our DuQuant achieves performance comparable to FP16 model. Furthermore, unlike other methods that show weaker results under W4A4 setting, our DuQuant maintains competitive performance, indicating its robustness with LLaMA3. We attribute this success to the advanced handling of outliers achieved through dual transformations, which is not restricted to specific models.\n\n# 4.2 Ablation Study\n\nModule-wise Impact. We ablate four distinct operations within DuQuant: 1) only the smoothing technology like SmoothQuant; 2) one rotation following the smoothing operation; 3) a sequence of rotation, permutation, and another rotation without smoothing; and k4) full DuQuant approach. Table 6 shows that the smoothing operation plays a basic role in our DuQuant by shifting activation outliers to weight. The initial rotation significantly enhances model performance, yielding competitive PPL results. Finally, permutation combined with a second rotation further enhances the quantized model.\n\nTable 6: Influence of different components in DuQuant under 4-bit weight-activation quantization.   \n\n<html><body><table><tr><td colspan=\"4\">Modules</td><td colspan=\"2\">LLaMA2-7B</td><td colspan=\"2\">LLaMA2-13B</td></tr><tr><td>Smooth</td><td>Rotation 1</td><td>Permutation</td><td>Rotation 2</td><td>WikiText2↓</td><td>C4↓</td><td>WikiText2↓</td><td>C4↓</td></tr><tr><td>√</td><td></td><td></td><td></td><td>NaN</td><td>1379.46</td><td>160.30</td><td>203.87</td></tr><tr><td></td><td>√</td><td></td><td></td><td>8.48</td><td>10.63</td><td>14.32</td><td>21.73</td></tr><tr><td>√</td><td>√</td><td></td><td></td><td>7.92</td><td>10.64</td><td>5.96</td><td>7.94</td></tr><tr><td></td><td>√</td><td>√</td><td>√</td><td>6.79</td><td>8.51</td><td>6.06</td><td>8.03</td></tr><tr><td>√</td><td>√</td><td>√</td><td>√</td><td>6.28</td><td>7.90</td><td>5.42</td><td>7.05</td></tr></table></body></html>\n\nInfluence of Normal/Massive Outliers. In this section, we comprehensively explore the influence of massive and normal outliers on quantization. Notably, we observe that massive outliers primarily occur at the down-projection of the FFN module. To isolate their effect, we remove the rotation and permutation transformations, applying only the smoothing technique to all down-projection inputs. The resulting perplexity for LLaMA2-7B and LLaMA-13B showed significant degradation, presented in Table 7. Conversely, when we eliminate the rotation and permutation transformations for normal outliers, the performance decrease was noticeable but less severe compared to massive outliers. These findings indicate that: 1) massive outliers exert a more substantial impact on quantization, corroborating our claims in Section 2; 2) the smoothing technique alone struggles to fully mitigate the influence of outliers, particularly massive ones; and 3) our rotation and permutation methods prove highly effective against both types of outliers, leading to superior performance.\n\nTable 7: Outliers impact on quantization. We only apply the smooth technique on Normal and Massive outliers for W4A4 quantization.   \n\n<html><body><table><tr><td colspan=\"2\">Outlier Type</td><td colspan=\"2\">LLaMA2-7B</td><td colspan=\"2\">LLaMA2-13B</td></tr><tr><td>Normal</td><td>Massive</td><td>WikiText2↓</td><td>C4↓</td><td>WikiText2↓</td><td>C4↓</td></tr><tr><td></td><td>√</td><td>18.16</td><td>26.42</td><td>10.51</td><td>16.01</td></tr><tr><td>√</td><td></td><td>10.88</td><td>13.89</td><td>7.87</td><td>10.52</td></tr><tr><td>√</td><td>√</td><td>6.28</td><td>7.90</td><td>5.42</td><td>7.05</td></tr></table></body></html>\n\n![](images/d5aa40a7a7f7b0d0d0bcdc28b0907b7f132b57317ba61745bd663e1d5e45c717.jpg)  \nFigure 3: GPT-4 evaluation on the MT-Bench.\n\nComparison with QuaRot [2] In light of the recent introduction of Hadamard rotations by QuaRot[2] to eliminate outlier features, we have undertaken a detailed analysis to highlight the key differences between our DuQuant and QuaRot. To ensure a balanced evaluation, we have reimplemented QuaRot in accordance with our quantization settings. The results demonstrate that 1) the rotation matrix constructed by DuQuant outperforms QuaRot’s approach of simply selecting a randomly initialized Hadamard matrix. As depicted in Figure 10, our DuQuant more effectively smooths activations than QuaRot. This is attributed to the prior knowledge utilized by DuQuant to accurately target the outliers; 2) As demonstrated by the perplexity in Table 8, QuaRot employs GPTQ for their weight quantization method, whereas our DuQuant, with its sophisticated outlier management, attains competitive results using RTN quantization. The superiority proves the effectiveness of our zigzag permutation to enhance capacity. For a more comprehensive comparison, please refer to Appendix F.\n\nTable 8: PPL (↓) comparison under W4A4 setting. Figure 4: LLaMA2-7B Attention key_proj.   \n\n<html><body><table><tr><td>Method</td><td>1-7B</td><td>1-13B</td><td>1-30B</td><td>2-7B</td><td>2-13B</td></tr><tr><td>FP16</td><td>5.68</td><td>5.09</td><td>4.10</td><td>5.47</td><td>4.88</td></tr><tr><td>QuaRot-RTN</td><td>7.08</td><td>6.57</td><td>5.44</td><td>9.66</td><td>6.73</td></tr><tr><td>QuaRot-GPTQ</td><td>6.44</td><td>5.63</td><td>4.73</td><td>6.39</td><td>5.75</td></tr><tr><td>DuQuant</td><td>6.40</td><td>5.65</td><td>4.72</td><td>6.28</td><td>5.42</td></tr><tr><td>DuQuant+Lwc</td><td>6.18</td><td>5.47</td><td>4.55</td><td>6.08</td><td>5.33</td></tr></table></body></html>\n\n![](images/d06beca2495cb755715111d95b95236667cb8f33d15dc55d8c203910065b693d.jpg)\n\nPermutation Frequency. We conduct ablations on the rotation and permutation frequencies in DuQuant. As shown in Figure 5, “Perm 1” (two rotations with one permutation) achieves stronger performance compared with $\\mathrm { \\bf ~ \\tilde { \\Sigma } P e r m \\bf ~ 0 ^ { 3 } ~ }$ (no permutation), while incurring an additional $8 . 9 \\%$ computational cost on LLaMA2-7B and $9 . 3 \\%$ on LLaMA2-13B compared to the W4A4 setup. Considering the approximately $2 \\times$ speedup and the impressive performance, these additional costs are deemed acceptable. Further permutations, like “Perm 2,” do not improve performance and reduce inference efficiency. Consequently, “Perm 1” strikes the best balance between perplexity and inference speed, making it the optimal configuration for DuQuant.\n\nInference Speedup. To assess the inference speedup delivered by our DuQuant, we adopt the measurement strategy and W4A4 kernel from [2]. We evaluate the layer-wise speedup of LLaMA2 models on one NVIDIA 3090 GPU, with results detailed in Table 9 and 10. We set the pre-filling sequence length at 2048 and decode for 128 steps. In the pre-filling stage, DuQuant achieves a $2 . 0 8 \\times$ speedup over FP16 for LLaMA2-7B and a $2 . 3 4 \\times$ speedup for LLaMA2-13B, with slight variations across different batch sizes. In the decoding stage, batching the token generation phase yields high throughput without any downside [44]. Consequently, we enlarge the batch size to 64 and the results for LLaMA2-7B in Table 10 prove DuQuant achieves speedup comparable to QuaRot. More detailed analyses and end-to-end speedup are available in Appendix E.1.\n\nTable 9: Layer-wise speedup during pre-filling stage for 4-bit weight-activation quantization.   \n\n<html><body><table><tr><td>Model</td><td>Batch Size</td><td>Speedup</td></tr><tr><td rowspan=\"3\">LLaMA2-7B</td><td>1</td><td>1.95×</td></tr><tr><td>4</td><td>2.03×</td></tr><tr><td>16</td><td>2.08×</td></tr><tr><td rowspan=\"3\">LLaMA2-13B</td><td>1</td><td>2.15×</td></tr><tr><td>4</td><td>2.30×</td></tr><tr><td>16</td><td>2.34×</td></tr></table></body></html>\n\n11 3.0 2-7B Speedup ( ) 2-7B PPL ( ) 2-13B Speedup ( ) 2-13B PPL ( ) 10 2.5 2.366x 2.324x 12.50 2.105x 2.146x 9 1.946x 1.792x1.955x 78 1.0 6 0.5 5 0.0 4 W4A4 Perm. 0 Perm. 1 Perm. 2\n\nMemory Consumption. We measure the peak memory usage of DuQuant with the W4A4 kernel on LLaMA2-7B using a single NVIDIA 3090 GPU. We process 2048 tokens for pre-filing and run 128 decoding steps, with the results listed in Table 11. In the pre-filling stage, DuQuant, SmoothQuant, and QuaRot achieve up to $3 . 2 \\times$ memory reduction, while QLLM performs worse. In the decoding stage, DuQuant maintains strong memory efficiency, with superior performance.\n\nTable 10: Decoding stage.   \n\n<html><body><table><tr><td>INT4,BS=64</td><td>Speedup</td></tr><tr><td>FP16</td><td></td></tr><tr><td>SmoothQuant</td><td>1.508×</td></tr><tr><td>QLLM</td><td>OOM</td></tr><tr><td>QuaRot</td><td>1.442×</td></tr><tr><td>DuQuant</td><td>1.321×</td></tr></table></body></html>\n\nTable 11: Peak memory usage with a batch size of 1.   \n\n<html><body><table><tr><td>LLaMA2-7B</td><td>Pre-filling (GB)</td><td>Saving</td><td>Decoding (GB)</td><td>Saving</td></tr><tr><td>FP16</td><td>15.282</td><td></td><td>13.638</td><td></td></tr><tr><td>SmoothQuant</td><td>4.782</td><td>3.196×</td><td>3.890</td><td>3.506×</td></tr><tr><td>QLLM</td><td>5.349</td><td>2.857×</td><td>3.894</td><td>3.502×</td></tr><tr><td>QuaRot</td><td>4.784</td><td>3.194×</td><td>3.891</td><td>3.505×</td></tr><tr><td>DuQuant</td><td>4.786</td><td>3.193×</td><td>3.893</td><td>3.503×</td></tr></table></body></html>\n\nRuntime. Our DuQuant stands out for its efficiency, surpassing other baselines [48, 35, 40, 76]. The blockwise rotation ensures fast multiplication between the rotation and activation matrices. Zigzag permutation,\n\nTable 12: Quantization runtime on one NVIDIA A100.   \n\n<html><body><table><tr><td>Model</td><td>Omni.</td><td>Affine.</td><td>QLLM</td><td>Atom</td><td>DuQuant</td></tr><tr><td>LLaMA2-7B</td><td>2.0h</td><td>9.1h</td><td>1.1h</td><td>20min</td><td>50s</td></tr><tr><td>LLaMA2-13B</td><td>3.2h</td><td>16.0h</td><td>1.7h</td><td>36min</td><td>71s</td></tr><tr><td>LLaMA2-70B</td><td>14.6h</td><td>18.6h</td><td>9.3h</td><td>3.5h</td><td>270s</td></tr></table></body></html>\n\ninvolving simple channel swaps, is much faster than complex algorithms like Simulated Annealing, as discussed in Appendix E.3. Moreover, the advanced management of outliers makes DuQuant not rely on GPTQ or gradient-based training. Hence, DuQuant enables a rapid quantization process shown in Table F24, e.g., we successfully quantize LLaMA2-13B in just 71s with superior performance.\n\n# 5 Conclusion\n\nIn conclusion, this paper presents DuQuant, an innovative quantization strategy for large language models (LLMs) that effectively addresses the challenge of outlier activations. By integrating rotation and permutation transformations, DuQuant effectively mitigates the impacts of both massive and normal outliers. This strategic redistribution of outliers not only simplifies the quantization process but also leads to substantial improvements in model performance. Consequently, DuQuant establishes new state-of-the-art results in 4-bit weight-activation quantization scenarios. This advancement enhances the deployment of efficient LLMs in resource-constrained environments.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了大型语言模型（LLMs）量化过程中遇到的异常激活值问题，特别是大规模异常值（Massive Outliers）和普通异常值（Normal Outliers）。这些异常值阻碍了低比特量化的效率，导致显著的性能下降。\\n> *   该问题的重要性在于，现有的量化方法（如SmoothQuant）无法有效处理大规模异常值，导致在4-bit量化中性能显著下降。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种名为DuQuant的新方法，通过旋转和置换变换（rotation and permutation transformations）来更有效地平滑异常值，从而提升量化性能。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **创新贡献点1：** 提出了一种基于贪婪搜索的旋转矩阵构造方法，能够更有效地平滑异常值。\\n>     *   关键数据：在Commonsense QA任务中，DuQuant在所有LLaMA模型大小上提升了5%的准确率。\\n> *   **创新贡献点2：** 引入了zigzag置换（zigzag permutation）来平衡不同块之间的异常值分布，减少块间方差。\\n>     *   关键数据：在Vicuna-v1.5-13B模型的零-shot MMLU基准测试中，DuQuant提升了10%的准确率。\\n> *   **创新贡献点3：** 通过联合平滑权重和激活值，避免了QuaRot中耗时的GPTQ算法。\\n>     *   关键数据：在LLaMA2-7B模型中，DuQuant在预填充阶段加速了2.08倍，解码阶段内存使用减少了3.50倍。\\n> *   **创新贡献点4：** 在LLaMA3-8B模型上，DuQuant在W6A6设置下性能接近FP16模型，在W4A4设置下仍保持竞争力。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   DuQuant的核心思想是通过旋转和置换变换将异常值重新分配到不同的通道中，从而降低量化难度。旋转变换通过贪婪搜索构造旋转矩阵，而置换变换通过zigzag方式平衡块间异常值分布。\\n> *   这种方法之所以有效，是因为它能够同时处理大规模和普通异常值，且通过正交变换保持了线性层的等价性。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 先前的工作（如SmoothQuant）主要通过平滑技术将异常值从激活值转移到权重，但无法有效处理大规模异常值。\\n> *   **本文的改进：** DuQuant通过旋转和置换变换直接重新分配异常值，避免了平滑技术的局限性。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  构造旋转矩阵：通过贪婪搜索识别异常值集中的维度，构造块对角旋转矩阵。\\n> 2.  应用zigzag置换：将通道按激活值大小排序并以zigzag方式分配到不同块中，平衡块间异常值分布。\\n> 3.  再次应用旋转变换：在每个块内进一步平滑异常值。\\n> 4.  联合平滑权重和激活值：通过旋转和置换变换的逆操作保持线性层的等价性。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   SmoothQuant、Outlier Suppression+、OmniQuant、QLLM、AffineQuant、Atom。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在困惑度（Perplexity）上：** 在WikiText2数据集上，DuQuant在LLaMA2-7B模型上的困惑度为6.28，显著优于SmoothQuant（83.12）和OmniQuant（12.30）。与表现最佳的基线Atom（8.40）相比，提升了2.12个百分点。\\n> *   **在零-shot QA任务准确率上：** 在LLaMA1-7B模型上，DuQuant的平均准确率为61.76%，显著优于SmoothQuant（38.41%）和Atom（56.88%）。与表现最佳的基线Atom相比，提升了4.88个百分点。\\n> *   **在推理速度上：** 在LLaMA2-7B模型的预填充阶段，DuQuant的速度提升了2.08倍，内存使用减少了3.50倍。\\n> *   **在LLaMA3-8B模型上：** 在W6A6设置下，DuQuant的性能接近FP16模型；在W4A4设置下，仍保持竞争力，显著优于其他方法。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n> **提取与格式化要求**\\n> *   大型语言模型量化 (Large Language Model Quantization, LLM Quantization)\\n> *   异常值平滑 (Outlier Smoothing, N/A)\\n> *   旋转变换 (Rotation Transformation, N/A)\\n> *   置换变换 (Permutation Transformation, N/A)\\n> *   低比特量化 (Low-bit Quantization, N/A)\\n> *   贪婪搜索 (Greedy Search, N/A)\\n> *   块对角矩阵 (Block Diagonal Matrix, N/A)\\n> *   硬件加速 (Hardware Acceleration, N/A)\"\n}\n```"
}