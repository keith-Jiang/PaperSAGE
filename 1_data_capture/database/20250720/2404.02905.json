{
    "source": "Semantic Scholar",
    "arxiv_id": "2404.02905",
    "link": "https://arxiv.org/abs/2404.02905",
    "pdf_link": "https://arxiv.org/pdf/2404.02905.pdf",
    "title": "Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction",
    "authors": [
        "Keyu Tian",
        "Yi Jiang",
        "Zehuan Yuan",
        "Bingyue Peng",
        "Liwei Wang"
    ],
    "categories": [
        "cs.CV",
        "cs.AI"
    ],
    "publication_date": "2024-04-03",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 349,
    "influential_citation_count": 73,
    "institutions": [
        "Peking University",
        "Bytedance Inc"
    ],
    "paper_content": "# Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction\n\nKeyu Tian1,2, Yi Jiang2,†, Zehuan Yuan2,∗, Bingyue Peng2, Liwei Wang1,∗\n\n1Peking University 2Bytedance Inc keyutian@stu.pku.edu.cn, jiangyi.enjoy@bytedance.com, yuanzehuan@bytedance.com, bingyue.peng@bytedance.com, wanglw@pku.edu.cn\n\nTry and explore our online demo at: https://var.vision Codes and models: https://github.com/FoundationVision/VAR\n\n![](images/a280dc9311fdf3afefef85880932d120513b16673b6e24a905753a8c4f4d01ad.jpg)  \nFigure 1: Generated samples from Visual AutoRegressive (VAR) transformers trained on ImageNet. We show $5 1 2 \\times 5 1 2$ samples (top), $2 5 6 \\times 2 5 6$ samples (middle), and zero-shot image editing results (bottom).\n\n# Abstract\n\nWe present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine “next-scale prediction” or “next-resolution prediction”, diverging from the standard raster-scan “next-token prediction”. This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and can generalize well: VAR, for the first time, makes GPT-style AR models surpass diffusion transformers in image generation. On ImageNet $2 5 6 \\times 2 5 6$ benchmark, VAR significantly improve AR baseline by improving Fréchet inception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to 350.2, with $2 0 \\times$ faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near $- 0 . 9 9 8$ as solid evidence. VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.\n\n![](images/34abb34cae571f9d6b861b6271e361feafb560962a09229f3b5b501d98c7bfa9.jpg)  \nFigure 2: Standard autoregressive modeling (AR) vs. our proposed visual autoregressive modeling (VAR). (a) AR applied to language: sequential text token generation from left to right, word by word; (b) AR applied to images: sequential visual token generation in a raster-scan order, from left to right, top to bottom; (c) VAR for images: multi-scale token maps are autoregressively generated from coarse to fine scales (lower to higher resolutions), with parallel token generation within each scale. VAR requires a multi-scale VQVAE to work.\n\n# 1 Introduction\n\nThe advent of GPT series [66, 67, 15, 63, 1] and more autoregressive (AR) large language models (LLMs) [22, 4, 39, 83, 84, 91, 79, 5, 80] has heralded a new epoch in the field of artificial intelligence. These models exhibit promising intelligence in generality and versatility that, despite issues like hallucinations [40], are still considered to take a solid step toward the general artificial intelligence (AGI). At the core of these models is a self-supervised learning strategy – predicting the next token in a sequence, a simple yet profound approach. Studies into the success of these large AR models have highlighted their scalability and generalizabilty: the former, as exemplified by scaling laws [44, 36], allows us to predict large model’s performance from smaller ones and thus guides better resource allocation, while the latter, as evidenced by zero-shot and few-shot learning [67, 15], underscores the unsupervised-trained models’ adaptability to diverse, unseen tasks. These properties reveal AR models’ potential in learning from vast unlabeled data, encapsulating the essence of “AGI”.\n\nIn parallel, the field of computer vision has been striving to develop large autoregressive or world models [59, 58, 6], aiming to emulate their impressive scalability and generalizability. Trailblazing efforts like VQGAN and DALL-E [30, 68] along with their successors [69, 92, 51, 99] have showcased the potential of AR models in image generation. These models utilize a visual tokenizer to discretize continuous images into grids of 2D tokens, which are then flattened to a 1D sequence for AR learning (Fig. 2 b), mirroring the process of sequential language modeling (Fig. 2 a). However, the scaling laws of these models remain underexplored, and more frustratingly, their performance significantly lags behind diffusion models [64, 3, 52], as shown in Fig. 3. In contrast to the remarkable achievements of LLMs, the power of autoregressive models in computer vision appears to be somewhat locked.\n\nAutoregressive modeling requires defining the order of data. Our work reconsiders how to “order” an image: Humans typically perceive or create images in a hierachical manner, first capturing the global structure and then local details. This multi-scale, coarse-to-fine nature suggests an “order” for images. Also inspired by the widespread multi-scale designs [55, 53, 82, 45], we define autoregressive learning for images as “next-scale prediction” in Fig. 2 (c), diverging from the conventional “next-token prediction” in Fig. 2 (b). Our approach begins by encoding an image into multi-scale token maps. The autoregressive process is then started from the $1 \\times 1$ token map, and progressively expands in resolution: at each step, the transformer predicts the next higher-resolution token map conditioned on all previous ones. We refer to this methodology as Visual AutoRegressive (VAR) modeling.\n\n![](images/c3a1606169185719fc26006a863655e21d2c184819e3d45807e3876d4b553517.jpg)  \nFigure 3: Scaling behavior of different model families on ImageNet $2 5 6 \\times 2 5 6$ generation benchmark. The FID of the validation set serves as a reference lower bound (1.78). VAR with 2B parameters reaches an FID of 1.73, surpassing L-DiT with 3B or 7B parameters.\n\nVAR directly leverages GPT-2-like transformer architecture [67] for visual autoregressive learning. On the ImageNet $2 5 6 \\times 2 5 6$ benchmark, VAR significantly improves its AR baseline, achieving a Fréchet inception distance (FID) of 1.73 and an inception score (IS) of 350.2, with inference speed $2 0 \\times$ faster (see Sec. 6 for details). Notably, VAR surpasses the Diffusion Transformer (DiT) – the foundation of leading diffusion systems like Stable Diffusion 3.0 and SORA [29, 14] – in FID/IS, data efficiency, inference speed, and scalability. VAR models also exhibit scaling laws akin to those witnessed in LLMs. Lastly, we showcase VAR’s zero-shot generalization capabilities in tasks like image in-painting, out-painting, and editing. In summary, our contributions to the community include:\n\n1. A new visual generative framework using a multi-scale autoregressive paradigm with next-scale prediction, offering new insights in autoregressive algorithm design for computer vision. 2. An empirical validation of VAR models’ Scaling Laws and zero-shot generalization potential, which initially emulates the appealing properties of large language models (LLMs). 3. A breakthrough in visual autoregressive model performance, making GPT-style autoregressive methods surpass strong diffusion models in image synthesis for the first time2. 4. A comprehensive open-source code suite, including both VQ tokenizer and autoregressive model training pipelines, to help propel the advancement of visual autoregressive learning.\n\n# 2 Related Work\n\n# 2.1 Properties of large autoregressive language models\n\nScaling laws are found and studied in autoregressive language models [44, 36], which describe a power-law relationship between the scale of model (or dataset, computation, etc.) and the crossentropy loss value on the test set. Scaling laws allow us to directly predict the performance of a larger model from smaller ones [1], thus guiding better resource allocation. More pleasingly, they show that the performance of LLMs can scale well with the growth of model, data, and computation and never saturate, which is considered a key factor in the success of [15, 83, 84, 98, 91, 39]. The success brought by scaling laws has inspired the vision community to explore more similar methods for multimodality understanding and generation [54, 2, 89, 27, 96, 78, 21, 23, 42, 32, 33, 81, 88].\n\nZero-shot generalization. Zero-shot generalization [73] refers to the ability of a model, particularly a Large Language Model, to perform tasks that it has not been explicitly trained on. Within the realm of the computer vision, there is a burgeoning interest in the zero-shot and in-context learning abilities of foundation models, CLIP [65], SAM [49], Dinov2 [62]. Innovations like Painter [90] and LVM [6] extend visual prompters [41, 11] to achieve in-context learning in vision.\n\n# 2.2 Visual generation\n\nRaster-scan autoregressive models for visual generation necessitate the encoding of 2D images into 1D token sequences. Early endeavors [20, 85] have shown the ability to generate RGB (or grouped) pixels in the standard row-by-row, raster-scan manner. [70] extends [85] by using multiple independent trainable networks to do super-resolution repeatedly. VQGAN [30] advances [20, 85] by doing autoregressive learning in the latent space of VQVAE [86]. It employs GPT-2 decoder-only transformer to generate tokens in the raster-scan order, like how ViT [28] serializes 2D images into 1D patches. VQVAE-2 [69] and RQ-Transformer [51] also follow this raster-scan manner but use extra scales or stacked codes. Parti [93], based on the architecture of ViT-VQGAN [92], scales the transformer to 20B parameters and works well in text-to-image synthesis.\n\nMasked-prediction model. MaskGIT [17] employs a VQ autoencoder and a masked prediction transformer similar to BERT [25, 10, 35] to generate VQ tokens through a greedy algorithm. MagViT [94] adapts this approach to videos, and MagViT-2 [95] enhances [17, 94] by introducing an improved VQVAE for both images and videos. MUSE [16] further scales MaskGIT to 3B parameters.\n\nDiffusion models ’ progress has centered around improved learning or sampling [77, 76, 56, 57, 7], guidance [38, 61], latent learning [71], and architectures [37, 64, 72, 31]. DiT and U-ViT [64, 8] replaces or integrates the U-Net with transformer, and inspires recent image [19, 18] or video synthesis systems [12, 34] including Stable Diffusion 3.0 [29], SORA [14], and Vidu [9].\n\n# 3 Method\n\n# 3.1 Preliminary: autoregressive modeling via next-token prediction\n\nFormulation. Consider a sequence of discrete tokens $x = ( x _ { 1 } , x _ { 2 } , \\ldots , x _ { T } )$ , where $x _ { t } \\in [ V ]$ is an integer from a vocabulary of size $V$ . The next-token autoregressive posits the probability of observing the current token $\\mathbf { \\Psi } _ { x _ { t } }$ depends only on its prefix $( x _ { 1 } , x _ { 2 } , \\dots , x _ { t - 1 } )$ . This unidirectional token dependency assumption allows for the factorization of the sequence $x$ ’s likelihood:\n\n$$\np ( x _ { 1 } , x _ { 2 } , \\ldots , x _ { T } ) = \\prod _ { t = 1 } ^ { T } p ( x _ { t } \\mid x _ { 1 } , x _ { 2 } , \\ldots , x _ { t - 1 } ) .\n$$\n\nTraining an autoregressive model $p _ { \\theta }$ involves optimizing $p _ { \\theta } ( x _ { t } \\mid x _ { 1 } , x _ { 2 } , . . . , x _ { t - 1 } )$ over a dataset.   \nThis is known as the “next-token prediction”, and the trained $p _ { \\theta }$ can generate new sequences.\n\nTokenization. Images are inherently 2D continuous signals. To apply autoregressive modeling to images via next-token prediction, we must: 1) tokenize an image into several discrete tokens, and 2) define a 1D order of tokens for unidirectional modeling. For 1), a quantized autoencoder such as [30] is often used to convert the image feature map $f \\in \\mathbb { R } ^ { h \\times w \\times C }$ to discrete tokens $q \\in [ V ] ^ { h \\times w }$\n\n$$\nf = \\mathcal { E } ( i m ) , q = \\mathcal { Q } ( f ) ,\n$$\n\nwhere im denotes the raw image, $\\mathcal { E } ( \\cdot )$ a encoder, and $\\mathscr { Q } ( \\cdot )$ a quantizer. The quantizer typically includes a learnable codebook $Z \\in \\mathbb { R } ^ { V \\times C }$ containing $V$ vectors. The quantization process $\\overset { \\cdot } { \\underset { \\cdot } { q } = \\cdot \\underset { \\cdot } { Q } \\overset { \\cdot } { f } ) }$ will map each feature vector $f ^ { ( i , j ) }$ to the code index $\\boldsymbol q ^ { ( i , j ) }$ of its nearest code in the Euclidean sense:\n\n$$\nq ^ { ( i , j ) } = \\left( \\underset { v \\in [ V ] } { \\arg \\operatorname* { m i n } } \\| \\mathrm { l o o k u p } ( Z , v ) - f ^ { ( i , j ) } \\| _ { 2 } \\right) \\in [ V ] ,\n$$\n\nwhere lookup $( Z , v )$ means taking the $\\boldsymbol { v }$ -th vector in codebook $Z$ . To train the quantized autoencoder, $Z$ is looked up by every $q ^ { ( i , j ) }$ to get $\\hat { f }$ , the approximation of original $f$ . Then a new image $\\hat { i m }$ is reconstructed using the decoder $\\mathcal { D } ( \\cdot )$ given $\\hat { f }$ , and a compound loss $\\mathcal { L }$ is minimized:\n\n$$\n\\begin{array} { r l } & { \\hat { \\boldsymbol f } = \\mathrm { l o o k u p } ( \\boldsymbol Z , \\boldsymbol q ) , \\qquad i \\hat { m } = { \\mathcal D } ( \\hat { \\boldsymbol f } ) , } \\\\ & { { \\mathcal L } = \\| i m - i \\hat { m } \\| _ { 2 } + \\| { \\boldsymbol f } - \\hat { \\boldsymbol f } \\| _ { 2 } + \\lambda _ { \\mathrm { P } } { \\mathcal L } _ { \\mathrm { P } } ( i \\hat { m } ) + \\lambda _ { \\mathrm { G } } { \\mathcal L } _ { \\mathrm { G } } ( i \\hat { m } ) , } \\end{array}\n$$\n\nwhere $\\mathcal { L } _ { \\mathrm { P } } ( \\cdot )$ is a perceptual loss such as LPIPS [97], $\\mathcal { L } _ { \\mathrm { G } } ( \\cdot )$ a discriminative loss like StyleGAN’s discriminator loss [47], and $\\lambda _ { \\mathrm { P } } , \\lambda _ { \\mathrm { G } }$ are loss weights. Once the autoencoder $\\{ \\mathcal { E } , \\mathcal { Q } , \\mathcal { D } \\}$ is fully trained, it will be used to tokenize images for subsequent training of a unidirectional autoregressive model.\n\nThe image tokens in $q \\in [ V ] ^ { h \\times w }$ are arranged in a 2D grid. Unlike natural language sentences with an inherent left-to-right ordering, the order of image tokens must be explicitly defined for unidirectional autoregressive learning. Previous AR methods [30, 92, 51] flatten the 2D grid of $q$ into a 1D sequence $\\boldsymbol { x } = ( \\bar { x } _ { 1 } , \\dots , x _ { h \\times w } )$ using some strategy such as row-major raster scan, spiral, or $\\mathbf { z }$ -curve order. Once flattened, they can extract a set of sequences $x$ from the dataset, and then train an autoregressive model to maximize the likelihood in (1) via next-token prediction.\n\nDiscussion on the weakness of vanilla autoregressive models. The above approach of tokenizing and flattening enable next-token autoregressive learning on images, but introduces several issues:\n\n1) Mathematical premise violation. In quantized autoencoders (VQVAEs), the encoder typically produces an image feature map $f$ with inter-dependent feature vectors $f ^ { ( i , j ) }$ for all $i , j$ . So after quantization and flattening, the token sequence $( x _ { 1 } , x _ { 2 } , \\ldots , x _ { h \\times w } )$ retains bidirectional correlations. This contradicts the unidirectional dependency assumption of autoregressive models, which dictates that each token $\\mathbf { \\Psi } _ { x _ { t } }$ should only depend on its prefix $( x _ { 1 } , x _ { 2 } , \\dots , x _ { t - 1 } )$ .\n\n2) Inability to perform some zero-shot generalization. Similar to issue 1), The unidirectional nature of image autoregressive modeling restricts their generalizability in tasks requiring bidirectional reasoning. E.g., it cannot predict the top part of an image given the bottom part.\n\n3) Structural degradation. The flattening disrupts the spatial locality inherent in image feature maps. For example, the token $\\boldsymbol q ^ { ( i , j ) }$ and its 4 immediate neighbors $\\bar { q ^ { ( i \\pm 1 , j ) } } , q ^ { ( i , j \\pm 1 ) }$ are closely correlated due to their proximity. This spatial relationship is compromised in the linear sequence $x$ , where unidirectional constraints diminish these correlations.\n\n4) Inefficiency. Generating an image token sequence $\\boldsymbol { x } = ( x _ { 1 } , x _ { 2 } , \\dots , x _ { n \\times n } )$ with a conventional self-attention transformer incurs $\\ O ( \\mathcal { O } ( n ^ { 2 } )$ autoregressive steps and $\\mathcal { O } ( n ^ { 6 } )$ computational cost.\n\nIssues 2) and 3) are evident (see examples above). Regarding issue 1), we present empirical evidence in Appendix C. The proof of issue 3) is detailed in Appendix D. These theoretical and practical limitations call for a rethinking of autoregressive models in the context of image generation.\n\n![](images/33ccba90d7a1fdc143285bbd1875b9a4f13359599b644ba187d18465379d88c5.jpg)  \nFigure 4: VAR involves two separated training stages. Stage 1: a multi-scale VQ autoencoder encodes an image into $K$ token maps $R = ( r _ { 1 } , r _ { 2 } , \\ldots , r _ { K } )$ and is trained by a compound loss (5). For details on “Multi-scale quantization” and “Embedding”, check Algorithm 1 and 2. Stage 2: a VAR transformer is trained via next-scale prediction (6): it takes $\\left( \\left[ \\mathbf { s } \\right] , r _ { 1 } , r _ { 2 } , \\ldots , r _ { K - 1 } \\right)$ as input to predict $\\left( r _ { 1 } , r _ { 2 } , r _ { 3 } , . . . , r _ { K } \\right)$ . The attention mask is used in training to ensure each $r _ { k }$ can only attend to $r _ { \\le k }$ . Standard cross-entropy loss is used.\n\n# 3.2 Visual autoregressive modeling via next-scale prediction\n\nReformulation. We reconceptualize the autoregressive modeling on images by shifting from “nexttoken prediction” to “next-scale prediction” strategy. Here, the autoregressive unit is an entire token map, rather than a single token. We start by quantizing a feature map $\\bar { f } \\bar { \\in \\mathbb { R } } ^ { h \\times w \\times C }$ into $K$ multi-scale token maps $( r _ { 1 } , r _ { 2 } , \\dots , r _ { K } )$ , each at a increasingly higher resolution $h _ { k } \\times w _ { k }$ , culminating in $r _ { K }$ matches the original feature map’s resolution $h \\times w$ . The autoregressive likelihood is formulated as:\n\n$$\np ( r _ { 1 } , r _ { 2 } , \\ldots , r _ { K } ) = \\prod _ { k = 1 } ^ { K } p ( r _ { k } \\mid r _ { 1 } , r _ { 2 } , \\ldots , r _ { k - 1 } ) ,\n$$\n\nwhere each autoregressive unit $r _ { k } \\in [ V ] ^ { h _ { k } \\times w _ { k } }$ is the token map at scale $k$ containing $h _ { k } \\times w _ { k }$ tokens, and the sequence $\\left( r _ { 1 } , r _ { 2 } , \\dots , r _ { k - 1 } \\right)$ serves as the the “prefix” for $r _ { k }$ . During the $k$ -th autoregressive step, all distributions over the $h _ { k } \\times w _ { k }$ tokens in $r _ { k }$ will be generated in parallel, conditioned on $r _ { k }$ ’s prefix and associated $k$ -th position embedding map. This “next-scale prediction” methodology is what we define as visual autoregressive modeling (VAR), depicted on the right side of Fig. 4. Note that in the training of VAR, a block-wise causal attention mask is used to ensure that each $r _ { k }$ can only attend to its prefix $r _ { \\le k }$ . During inference, kv-caching can be used and no mask is needed.\n\nDiscussion. VAR addresses the previously mentioned three issues as follows:\n\n1) The mathematical premise is satisfied if we constrain each $r _ { k }$ to depend only on its prefix, that is, the process of getting $r _ { k }$ is solely related to $r _ { \\le k }$ . This constraint is acceptable as it aligns with the natural, coarse-to-fine progression characteristics like human visual perception and artistic drawing (as we discussed in Sec. 1). Further details are provided in the Tokenization below.   \n2) The spatial locality is preserved as (i) there is no flattening operation in VAR, and (ii) tokens in each $r _ { k }$ are fully correlated. The multi-scale design additionally reinforces the spatial structure.   \n3) The complexity for generating an image with $n \\times n$ latent is significantly reduced to $O ( n ^ { 4 } )$ , see Appendix for proof. This efficiency gain arises from the parallel token generation in each $r _ { k }$ .\n\nTokenization. We develope a new multi-scale quantization autoencoder to encode an image to $K$ multi-scale discrete token maps $R = ( r _ { 1 } , r _ { 2 } , \\dots , r _ { K } )$ necessary for VAR learning (6). We employ the same architecture as VQGAN [30] but with a modified multi-scale quantization layer. The encoding and decoding procedures with residual design on $f$ or $\\hat { f }$ are detailed in algorithms 1 and 2. We empirically find this residual-style design, akin to [51], can perform better than independent interpolation. Algorithm 1 shows that each $r _ { k }$ would depend only on its prefix $\\left( r _ { 1 } , r _ { 2 } , \\dots , r _ { k - 1 } \\right)$ .\n\nNote that a shared codebook $Z$ is utilized across all scales, ensuring that each $r _ { k }$ ’s tokens belong to the same vocabulary $[ V ]$ . To address the information loss in upscaling $z _ { k }$ to $h _ { K } \\times w _ { K }$ , we use $K$ extra convolution layers $\\{ \\phi _ { k } \\} _ { k = 1 } ^ { K }$ . No convolution is used after downsampling $f$ to $h _ { k } \\times w _ { k }$ .\n\n<html><body><table><tr><td>Algorithm1:Multi-scale VQVAE Encoding</td></tr><tr><td>Inputs:raw image im;</td></tr><tr><td>2 Hyperparameters:stepsK,resolutions</td></tr><tr><td>(hk,Wk)k=1; 3 f=ε(im),R=[];</td></tr><tr><td>4 fork=1,.,K do</td></tr><tr><td>5 rk=Q(interpolate(f,hk,wk));</td></tr><tr><td>6 R= queue_push(R,rk);</td></tr><tr><td>7 zk =lookup(Z,rk)；</td></tr><tr><td>8 zk =interpolate(zk,hK,wK);</td></tr><tr><td>9 f=f-Φk(zk)；</td></tr><tr><td></td></tr><tr><td>10 Return: multi-scale tokens R;</td></tr></table></body></html>\n\n<html><body><table><tr><td>Algorithm2:Multi-scale VQVAE Reconstruction</td></tr><tr><td>1Inputs: multi-scale token maps R;</td></tr><tr><td>2 Hyperparameters:stepsK,resolutions</td></tr><tr><td>(hk,wk)k=1;</td></tr><tr><td>3f=0;</td></tr><tr><td>4 fork=1,.,K do</td></tr><tr><td>5 rk = queue_pop(R);</td></tr><tr><td>6 zk =lookup(Z,rk);</td></tr><tr><td>7 zk = interpolate(zk,hk,wK);</td></tr><tr><td>8 f=f+k(zk）；</td></tr><tr><td>9im=D(f)；</td></tr><tr><td>10 Return: reconstructed image im;</td></tr></table></body></html>\n\n# 4 Implementation details\n\nVAR tokenizer. As aforementioned, we use the vanilla VQVAE architecture [30] and a multiscale quantization scheme with $K$ extra convolutions (0.03M extra parameters). We use a shared codebook for all scales with $V = 4 0 9 6$ . Following the baseline [30], our tokenizer is also trained on OpenImages [50] with the compound loss (5) and a spatial downsample ratio of $1 6 \\times$ .\n\nVAR transformer. Our main focus is on VAR algorithm so we keep a simple model architecture design. We adopt the architecture of standard decoder-only transformers akin to GPT-2 and VQGAN [67, 30] with adaptive normalization (AdaLN), which has widespread adoption and proven effectiveness in many visual generative models [47, 48, 46, 75, 74, 43, 64, 19]. For class-conditional synthesis, we use the class embedding as the start token [s] and also the condition of AdaLN. We found normalizing queries and keys to unit vectors before attention can stablize the training. We do not use advanced techniques in large language models, such as rotary position embedding (RoPE), SwiGLU MLP, or RMS Norm [83, 84]. Our model shape follows a simple rule like [44] that the width $w$ , head counts $h$ , and drop rate $d r$ are linearly scaled with the depth $d$ as follows:\n\n$$\nw = 6 4 d , \\qquad h = d , \\qquad d r = 0 . 1 \\cdot d / 2 4 .\n$$\n\nConsequently, the main parameter count $N$ of a VAR transformer with depth $d$ is given by3:\n\n$$\n\\begin{array} { r } { N ( d ) = \\underbrace { d \\cdot 4 w ^ { 2 } } _ { \\mathrm { s e l f - a t t e n t i o n } } + \\underbrace { d \\cdot 8 w ^ { 2 } } _ { \\mathrm { f e e d - f o r w a r d } } + \\underbrace { d \\cdot 6 w ^ { 2 } } _ { \\mathrm { a d a p t i v e \\ : l a y e r n o r m } } = 1 8 d w ^ { 2 } = 7 3 7 2 8 d ^ { 3 } . } \\end{array}\n$$\n\nAll models are trained with the similar settings: a base learning rate of $1 0 ^ { - 4 }$ per 256 batch size, an AdamW optimizer with $\\beta _ { 1 } = 0 . 9$ , $\\beta _ { 2 } = 0 . 9 5$ , decay $= 0 . 0 5$ , a batch size from 768 to 1024 and training epochs from 200 to 350 (depends on model size). The evaluations in Sec. 5 suggest that such a simple model design are capable of scaling and generalizing well.\n\n# 5 Empirical Results\n\nThis section first compares VAR with other image generative model families in Sec. 5.1. Evaluations on the scalability and generalizability of VAR models are presented in Sec. 5.2 and Appendix B. For implementation details and ablation study, please see Appendix 4 and Appendix 6.\n\n# 5.1 State-of-the-art image generation\n\nSetup. We test VAR models with depths 16, 20, 24, and 30 on ImageNet $2 5 6 \\times 2 5 6$ and $5 1 2 \\times 5 1 2$ conditional generation benchmarks and compare them with the state-of-the-art image generation model families. Among all VQVAE-based AR or VAR models, VQGAN [30] and ours use the same architecture (CNN) and training data (OpenImages [50]) for VQVAE, while ViT-VQGAN [92] uses a ViT autoencoder, and both it and RQTransformer [51] trains the VQVAE directly on ImageNet. The results are summaried in Tab. 1 and Tab. 2.\n\nTable 1: Generative model family comparison on class-conditional ImageNet $2 5 6 \\times 2 5 6$ . $\\downarrow ^ { \\prime \\prime }$ or “↑” indicate lower or higher values are better. Metrics include Fréchet inception distance (FID), inception score (IS), precision (Pre) and recall (rec). “#Step”: the number of model runs needed to generate an image. Wall-clock inference time relative to VAR is reported. Models with the suffix “-re” used rejection sampling. $\\dagger$ : taken from MaskGIT [17].   \n\n<html><body><table><tr><td>Type</td><td>Model</td><td>FID↓</td><td>IS↑</td><td>Pre↑</td><td>Rec↑</td><td>#Para</td><td>#Step</td><td>Time</td></tr><tr><td>GAN</td><td>BigGAN[13]</td><td>6.95</td><td>224.5</td><td>0.89</td><td>0.38</td><td>112M</td><td>1</td><td>一</td></tr><tr><td>GAN</td><td>GigaGAN[43]</td><td>3.45</td><td>225.5</td><td>0.84</td><td>0.61</td><td>569M</td><td>1</td><td>一</td></tr><tr><td>GAN</td><td>StyleGan-XL [75]</td><td>2.30</td><td>265.1</td><td>0.78</td><td>0.53</td><td>166M</td><td>1</td><td>0.3[75]</td></tr><tr><td>Diff.</td><td>ADM [26]</td><td>10.94</td><td>101.0</td><td>0.69</td><td>0.63</td><td>554M</td><td>250</td><td>168[75]</td></tr><tr><td>Diff.</td><td>CDM [37]</td><td>4.88</td><td>158.7</td><td>一</td><td></td><td></td><td>8100</td><td></td></tr><tr><td>Diff.</td><td>LDM-4-G[71]</td><td>3.60</td><td>247.7</td><td>一</td><td>一</td><td>400M</td><td>250</td><td>1</td></tr><tr><td>Diff.</td><td>DiT-L/2 [64]</td><td>5.02</td><td>167.2</td><td>0.75</td><td>0.57</td><td>458M</td><td>250</td><td>31</td></tr><tr><td>Diff.</td><td>DiT-XL/2 [64]</td><td>2.27</td><td>278.2</td><td>0.83</td><td>0.57</td><td>675M</td><td>250</td><td>45</td></tr><tr><td>Diff.</td><td>L-DiT-3B [3]</td><td>2.10</td><td>304.4</td><td>0.82</td><td>0.60</td><td>3.0B</td><td>250</td><td>>45</td></tr><tr><td>Diff.</td><td>L-DiT-7B [3]</td><td>2.28</td><td>316.2</td><td>0.83</td><td>0.58</td><td>7.0B</td><td>250</td><td>>45</td></tr><tr><td>Mask.</td><td>MaskGIT[17]</td><td>6.18</td><td>182.1</td><td>0.80</td><td>0.51</td><td>227M</td><td>8</td><td>0.5 [17]</td></tr><tr><td>Mask.</td><td>RCG (cond.) [52]</td><td>3.49</td><td>215.5</td><td>1</td><td>1</td><td>502M</td><td>20</td><td>1.9 [52]</td></tr><tr><td>AR</td><td>VQVAE-2† [69]</td><td>31.11</td><td>~45</td><td>0.36</td><td>0.57</td><td>13.5B</td><td>5120</td><td></td></tr><tr><td>AR</td><td>VQGAN+ [30]</td><td>18.65</td><td>80.4</td><td>0.78</td><td>0.26</td><td>227M</td><td>256</td><td>19 [17]</td></tr><tr><td>AR</td><td>VQGAN[30]</td><td>15.78</td><td>74.3</td><td></td><td></td><td>1.4B</td><td>256</td><td>24</td></tr><tr><td>AR</td><td>VQGAN-re [30]</td><td>5.20</td><td>280.3</td><td></td><td>一</td><td>1.4B</td><td>256</td><td>24</td></tr><tr><td>AR</td><td>ViTVQ [92]</td><td>4.17</td><td>175.1</td><td></td><td></td><td>1.7B</td><td>1024</td><td>>24</td></tr><tr><td>AR</td><td>ViTVQ-re [92]</td><td>3.04</td><td>227.4</td><td></td><td></td><td>1.7B</td><td>1024</td><td>>24</td></tr><tr><td>AR</td><td>RQTran. [51]</td><td>7.55</td><td>134.0</td><td>1</td><td>一</td><td>3.8B</td><td>68</td><td>21</td></tr><tr><td>VAR VAR</td><td>VAR-d16 VAR-d20</td><td>3.30</td><td>274.4</td><td>0.84</td><td>0.51</td><td>310M</td><td>10</td><td>0.4</td></tr><tr><td>VAR VAR VAR</td><td>VAR-d24 VAR-d30</td><td>2.57 2.09</td><td>302.6 312.9</td><td>0.83 0.82</td><td>0.56 0.59</td><td>600M 1.0B</td><td>10 10</td><td>0.5 0.6</td></tr></table></body></html>\n\nOverall comparison. In comparison with existing generative approaches including generative adversarial networks (GAN), diffusion models (Diff.), BERT-style masked-prediction models (Mask.), and GPT-style autoregressive models (AR), our visual autoregressive (VAR) establishes a new model class. As shown in Tab. 1, VAR not only achieves the best FID/IS but also demonstrates remarkable speed in image generation. VAR also maintains decent precision and recall, confirming its semantic consistency. These advantages hold true on the $5 1 2 \\times 5 1 2$ synthesis benchmark, as detailed in Tab. 2. Notably, VAR significantly advances traditional AR capabilities. To our knowledge, this is the first time of autoregressive models outperforming Diffusion transformers, a milestone made possible by VAR’s resolution of AR limitations discussed in Section 3.\n\nEfficiency comparison. Conventional autoregressive (AR) models [30, 69, 92, 51] suffer a lot from the high computational cost, as the number of image tokens is quadratic to the image resolution. A full autoregressive generation of $n ^ { 2 }$ tokens requires $\\mathcal { O } ( n ^ { 2 } )$ decoding iterations and $\\mathcal { O } ( n ^ { 6 } )$ total computations. In contrast, VAR only requires ${ \\mathcal { O } } ( \\log ( n ) )$ iterations and $\\mathcal { O } ( n ^ { 4 } )$ total computations. The wall-clock time reported in Tab. 1 also provides empirical evidence that VAR is around 20 times faster than VQGAN and ViT-VQGAN even with more model parameters, reaching the speed of efficient GAN models which only require 1 step to generate an image.\n\nTable 2: ImageNet ${ \\pmb 5 } { \\bf 1 } 2 \\times { \\bf 5 } { \\bf 1 } 2$ conditional generation. †: quoted from MaskGIT [17]. “- ${ \\bf \\cdot s } ^ { \\prime \\prime }$ : a single shared AdaLN layer is used due to resource limitation.   \n\n<html><body><table><tr><td>Type</td><td>Model</td><td>FID↓</td><td>IS↑</td><td>Time</td></tr><tr><td>GAN</td><td>BigGAN[13]</td><td>8.43</td><td>177.9</td><td>1</td></tr><tr><td>Diff.</td><td>ADM [26]</td><td>23.24</td><td>101.0</td><td>1</td></tr><tr><td>Diff.</td><td>DiT-XL/2 [64]</td><td>3.04</td><td>240.8</td><td>81</td></tr><tr><td>Mask.</td><td>MaskGIT[17]</td><td>7.32</td><td>156.0</td><td>0.5†</td></tr><tr><td>AR</td><td>VQGAN [30]</td><td>26.52</td><td>66.8</td><td>25†</td></tr><tr><td>VAR</td><td>VAR-d36-s</td><td>2.63</td><td>303.2</td><td>1</td></tr></table></body></html>\n\nCompared with popular diffusion transformer. The VAR model surpasses the recently popular diffusion models Diffusion Transformer (DiT), which serves as the precursor to the latest StableDiffusion 3 [29] and SORA [14], in multiple dimensions: 1) In image generation diversity and quality (FID and IS), VAR with 2B parameters consistently performs better than DiT-XL/2 [64], L-DiT-3B, and L-DiT-7B [3]. VAR also maintains comparable precision and recall. 2) For inference speed, the DiT-XL/2 requires $4 5 \\times$ the wall-clock time compared to VAR, while 3B and 7B models [3] would cost much more. 3) VAR is considered more data-efficient, as it requires only 350 training epochs compared to DiT-XL/2’s 1400. 4) For scalability, Fig. 3 and Tab. 1 show that DiT only obtains marginal or even negative gains beyond 675M parameters. In contrast, the FID and IS of VAR are consistently improved, aligning with the scaling law study in Sec. 5.2. These results establish VAR as potentially a more efficient and scalable model for image generation than models like DiT.\n\n(a) (b) (c) (d)   \n1.66 LC o=rr(e2l.a.0 =N) 0..293993 1.81 LC o=rr(e2l.a.5 =N) 0.9995 94.9 ECror r=el(a5. =102 0N.p9a9ra7)8 0.02 96.5 ECror r=el(a6. =102 0N.p9a9ra8)9 0.01   \n1.27 1.14 93.1 95.4   \n0.74 0.72 89.6 93.2 10 1 100 10 1 100 10 1 100 10 100 Model Parameters (Billion) Model Parameters (Billion) Model Parameters (Billion) Model Parameters (Billion)\n\n# 5.2 Power-law scaling laws\n\nBackground. Prior research [44, 36, 39, 1] have established that scaling up autoregressive (AR) large language models (LLMs) leads to a predictable decrease in test loss $L$ . This trend correlates with parameter counts $N$ , training tokens $T$ , and optimal training compute $C _ { \\mathrm { m i n } }$ , following a power-law:\n\n$$\n{ \\cal L } = ( \\beta \\cdot X ) ^ { \\alpha } ,\n$$\n\nwhere $X$ can be any of $N , T .$ , or $C _ { \\mathrm { m i n } }$ . The exponent $\\alpha$ reflects the smoothness of power-law, and $L$ denotes the reducible loss normalized by irreducible loss $L _ { \\infty }$ [36] A logarithmic transformation to $L$ and $X$ will reveal a linear relation between $\\log ( L )$ and $\\log ( X )$ :\n\n$$\n\\log ( L ) = \\alpha \\log ( X ) + \\alpha \\log \\beta .\n$$\n\nAn appealing phenomenon is that both [44] and [36] never observed deviation from these linear relationships at the higher end of $X$ , although flattening is inevitable as the loss approaches zero.\n\nThese observed scaling laws [44, 36, 39, 1] not only validate the scalability of LLMs but also serve as a predictive tool for AR modeling, which facilitates the estimation of performance for larger AR models based on their smaller counterparts, thereby saving resource usage by large model performance forecasting. Given these appealing properties of scaling laws brought by LLMs, their replication in computer vision is therefore of significant interest.\n\nSetup of scaling VAR models. Following the protocols from [44, 36, 39, 1], we examine whether our VAR model complies with similar scaling laws. We trained models across 12 different sizes, from 18M to 2B parameters, on the ImageNet training set [24] containing 1.28M images (or 870B image tokens under our VQVAE) per epoch. For models of different sizes, training spanned 200 to 350 epochs, with a maximum number of tokens reaching 305 billion. Below we focus on the scaling laws with model parameters $N$ and optimal training compute $C _ { \\mathrm { m i n } }$ given sufficient token count $T$ .\n\nScaling laws with model parameters $N$ . We first investigate the test loss trend as the VAR model size increases. The number of parameters $N ( d ) = 7 3 7 2 8 \\bar { d } ^ { 3 }$ for a VAR transformer with depth $d$ is specified in (8). We varied $d$ from 6 to 30, yielding 12 models with 18.5M to 2.0B parameters. We assessed the final test cross-entropy loss $L$ and token prediction error rates ${ \\mathbf { } } E r r$ on the ImageNet validation set of 50,000 images [24]. We computed $L$ and ${ \\mathbf { } } E r r$ for both the last scale (at the last next-scale autoregressive step), as well as the global average. Results are plotted in Fig. 5, where we observed a clear power-law scaling trend for $L$ as a function of $N$ , as consistent with [44, 36, 39, 1]. The power-law scaling laws can be expressed as:\n\n![](images/73249b6056bd23ed16302459f2494d9476e5339b330eaa12e3d95f5f85eda93d.jpg)  \nFigure 6: Scaling laws with optimal training compute $C _ { \\bf m i n }$ . Line color denotes different model sizes. Red dashed lines are power-law fits with equations in legend. Axes are on a logarithmic scale. Pearson coefficients near $- 0 . 9 9$ indicate strong linear relationships between $\\mathrm { l o g } ( C _ { \\mathrm { m i n } } )$ vs. $\\log ( L )$ or $\\log ( C _ { \\mathrm { m i n } } ) ~ \\nu s$ . $\\log ( E r r )$ .\n\n$$\nL _ { \\mathrm { l a s t } } = ( 2 . 0 \\cdot N ) ^ { - 0 . 2 3 } \\quad \\mathrm { a n d } \\quad L _ { \\mathrm { a v g } } = ( 2 . 5 \\cdot N ) ^ { - 0 . 2 0 } .\n$$\n\nAlthough the scaling laws are mainly studied on the test loss, we also empirically observed similar power-law trends for the token error rate ${ \\mathbf { } } E r r$ :\n\n$$\nE r r _ { \\mathrm { l a s t } } = ( 4 . 9 \\cdot 1 0 ^ { 2 } N ) ^ { - 0 . 0 1 6 } \\quad \\mathrm { a n d } \\quad E r r _ { \\mathrm { a v g } } = ( 6 . 5 \\cdot 1 0 ^ { 2 } N ) ^ { - 0 . 0 1 0 } .\n$$\n\nThese results verify the strong scalability of VAR, by which scaling up VAR transformers can continuously improve the model’s test performance.\n\nScaling laws with optimal training compute $C _ { \\bf m i n }$ . We then examine the scaling behavior of VAR transformers when increasing training compute $C$ . For each of the 12 models, we traced the test loss $L$ and token error rate ${ \\mathbf { } } E r r$ as a function of $C$ during training quoted in PFlops ( $1 0 ^ { 1 5 }$ floating-point operations per second). The results are plotted in Fig. 6. Here, we draw the Pareto frontier of $L$ and ${ \\mathbf { } } E r r$ to highlight the optimal training compute $C _ { \\mathrm { m i n } }$ required to reach a certain value of loss or error.\n\nThe fitted power-law scaling laws for $L$ and ${ \\mathbf { } } E r r$ as a function of $C _ { \\mathrm { m i n } }$ are:\n\n$$\n\\begin{array} { r l } & { L _ { \\mathrm { l a s t } } = ( 2 . 2 \\cdot 1 0 ^ { - 5 } C _ { \\mathrm { m i n } } ) ^ { - 0 . 1 3 } } \\\\ & { L _ { \\mathrm { a v g } } = ( 1 . 5 \\cdot 1 0 ^ { - 5 } C _ { \\mathrm { m i n } } ) ^ { - 0 . 1 6 } , } \\\\ & { E r r _ { \\mathrm { l a s t } } = ( 8 . 1 \\cdot 1 0 ^ { - 2 } C _ { \\mathrm { m i n } } ) ^ { - 0 . 0 0 6 7 } } \\\\ & { E r r _ { \\mathrm { a v g } } = ( 4 . 4 \\cdot 1 0 ^ { - 2 } C _ { \\mathrm { m i n } } ) ^ { - 0 . 0 1 1 } . } \\end{array}\n$$\n\nThese relations (14, 16) hold across 6 orders of magnitude in $C _ { \\mathrm { m i n } }$ , and our findings are consistent with those in [44, 36]: when trained with sufficient data, larger VAR transformers are more computeefficient because they can reach the same level of performance with less computation.\n\n# 6 Ablation Study\n\nIn this study, we aim to verify the effectiveness and efficiency of our proposed VAR framework.   \nResults are reported in Tab. 3.\n\nEffectiveness and efficiency of VAR. Starting from the vanilla AR transformer baseline implemented by [17], we replace its methodology with our VAR and keep other settings unchanged to get row 2.\n\nTable 3: Ablation study of VAR. The first two rows compare GPT-2-style transformers trained under AR or VAR algorithm without any bells and whistles. Subsequent lines show the influence of VAR enhancements. “AdaLN”: adaptive layernorm. “CFG”: classifier-free guidance. “Attn. Norm.”: normalizing $q$ and $k$ to unit vectors before attention. “Cost”: inference cost relative to the baseline. “ $\\mathbf { \\nabla } \\cdot \\Delta ^ { \\prime \\prime }$ : FID reduction to the baseline.   \n\n<html><body><table><tr><td></td><td>Description</td><td>Para.</td><td>Model</td><td>AdaLN</td><td>Top-k</td><td>CFG</td><td>Cost</td><td>FID↓</td><td>△</td></tr><tr><td>1</td><td>AR [30]</td><td>227M</td><td>AR</td><td>X</td><td>X</td><td>×</td><td>1</td><td>18.65</td><td>0.00</td></tr><tr><td>2</td><td>AR to VAR</td><td>207M</td><td>VAR-d16</td><td></td><td>×</td><td>X</td><td>0.013</td><td>5.22</td><td>-13.43</td></tr><tr><td>3</td><td>+AdaLN</td><td>310M</td><td>VAR-d16</td><td>√</td><td>X</td><td>X</td><td>0.016</td><td>4.95</td><td>-13.70</td></tr><tr><td>4</td><td>+Top-k</td><td>310M</td><td>VAR-d16</td><td>√</td><td>900</td><td>X</td><td>0.016</td><td>4.64</td><td>-14.01</td></tr><tr><td>5</td><td>+CFG</td><td>310M</td><td>VAR-d16</td><td>√</td><td>900</td><td>1.5</td><td>0.022</td><td>3.60</td><td>-15.05</td></tr><tr><td>5</td><td>+Attn. Norm.</td><td>310M</td><td>VAR-d16</td><td>√</td><td>900</td><td>1.5</td><td>0.022</td><td>3.30</td><td>-15.35</td></tr><tr><td>6</td><td>+Scale up</td><td>2.0B</td><td>VAR-d30</td><td>√</td><td>900</td><td>1.5</td><td>0.052</td><td>1.73</td><td>-16.85</td></tr></table></body></html>\n\nVAR achieves a way more better FID (18.65 vs. 5.22) with only $0 . 0 1 3 \\times$ inference wall-clock cost than the AR model, which demonstrates a leap in visual AR model’s performance and efficiency.\n\nComponent-wise ablation. We further test some key components in VAR. By replacing the standard Layer Normalization (LN) with Adaptive Layer Normalization (AdaLN), VAR starts yielding better FID than baseline. By using the top- $k$ sampling similar to the baseline, VAR’s FID is further improved. By using the classifier-free guidance (CFG) with ratio 1.5 and normalizing $q$ and $k$ to unit vectors before attention, we reach the FID of 3.30, which is 15.35 lower to the baseline, and its inference speed is still 45 times faster. We finally scale up VAR size to 2.0B and achieve an FID of 1.73. This is 16.85 better than the baseline FID.\n\n# 7 Limitations and Future Work\n\nIn this work, we mainly focus on the design of learning paradigm and keep the VQVAE architecture and training unchanged from the baseline [30] to better justify VAR framework’s effectiveness. We expect advancing VQVAE tokenizer [99, 60, 95] as another promising way to enhance autoregressive generative models, which is orthogonal to our work. We believe iterating VAR by advanced tokenizer or sampling techniques in these latest work can further improve VAR’s performance or speed.\n\nText-prompt generation is an ongoing direction of our research. Given that our model is fundamentally similar to modern LLMs, it can easily be integrated with them to perform text-to-image generation through either an encoder-decoder or in-context manner.\n\nVideo generation is not implemented in this work, but it can be naturally extended. By considering multi-scale video features as 3D pyramids, we can formulate a similar “3D next-scale prediction” to generate videos via VAR. Compared to diffusion-based generators like SORA [14], our method has inherent advantages in temporal consistency or integration with LLMs, thus can potentially handle longer temporal dependencies. This makes VAR competitive in the video generation field, because traditional AR models can be too inefficient for video generation due to their extremely high computational complexity and slow inference speed: it is becoming prohibitively expensive to generate high-resolution videos with traditional AR models, while VAR is capable to solve this. We therefore foresee a promising future for exploiting VAR models in the realm of video generation.\n\n# 8 Conclusion\n\nWe introduced a new visual generative framework named Visual AutoRegressive modeling (VAR) that 1) theoretically addresses some issues inherent in standard image autoregressive (AR) models, and 2) makes language-model-based AR models first surpass strong diffusion models in terms of image quality, diversity, data efficiency, and inference speed. Upon scaling VAR to 2 billion parameters, we observed a clear power-law relationship between test performance and model parameters or training compute, with Pearson coefficients nearing $- 0 . 9 9 8$ , indicating a robust framework for performance prediction. These scaling laws and the possibility for zero-shot task generalization, as hallmarks of LLMs, have now been initially verified in our VAR transformer models. We hope our findings and open sources can facilitate a more seamless integration of the substantial successes from the natural language processing domain into computer vision, ultimately contributing to the advancement of powerful multi-modal intelligence.\n\n# A Visualization of scaling effect\n\nTo better understand how VAR models are learning when scaled up, we compare some generated $2 5 6 \\times 2 5 6$ samples from VAR models of 4 different sizes (depth 6, 16, 26, 30) and 3 different training stages ( $20 \\%$ , $60 \\%$ , $100 \\%$ of total training tokens) in Fig. 7. To keep the content consistent, a same random seed and teacher-forced initial tokens are used. The observed improvements in visual fidelity and soundness are consistent with the scaling laws, as larger transformers are thought able to learn more complex and fine-grained image distributions.\n\n![](images/0550ae726985a414e2dacf833ed114482d96a21ca538db9f25dfeb5e5b2c17f6.jpg)  \nFigure 7: Scaling model size $N$ and training compute $C$ improves visual fidelity and soundness. Zoom in for a better view. Samples are drawn from VAR models of 4 different sizes and 3 different training stages. 9 class labels (from left to right, top to bottom) are: flamingo 130, arctic wolf 270, macaw 88, Siamese cat 284, oscilloscope 688, husky 250, mollymawk 146, volcano 980, and catamaran 484.\n\n![](images/a033a93d6e5c3d112f2596bc70b0b2d7aed7ee7ccb3716eeaf1c4f70646d2db4.jpg)  \nFigure 8: Zero-shot evaluation in downstream tasks containing in-painting, out-painting, and classconditional editing. The results show that VAR can generalize to novel downstream tasks without special design and finetuning. Zoom in for a better view.\n\n# B Zero-shot task generalization\n\nImage in-painting and out-painting. VAR- $d 3 0$ is tested. For in- and out-painting, we teacher-force ground truth tokens outside the mask and let the model only generate tokens within the mask. No class label information is injected into the model. The results are visualized in Fig. 8. Without modifications to the network architecture or tuning parameters, VAR has achieved decent results on these downstream tasks, substantiating the generalization ability of VAR.\n\nClass-conditional image editing. Following MaskGIT [17] we also tested VAR on the classconditional image editing task. Similar to the case of in-painting, the model is forced to generate tokens only in the bounding box conditional on some class label. Fig. 8 shows the model can produce plausible content that fuses well into the surrounding contexts, again verifying the generality of VAR.\n\n![](images/49519f3630437ae2a2ff36808da48f087eb5d28f945bbdf35b32866b2824d0d6.jpg)  \nFigure 9: Token dependency plotted. The normalized heat map of attention scores in the last self-attention layer of VQGAN encoder is visualized. 4 random $2 5 6 \\times 2 5 6$ images from ImageNet validation set are used.\n\n# C Token dependency in VQVAE\n\nTo examine the token dependency in VQVAE [30], we check the attention scores in the self-attention layer before the vector quantization module. We randomly sample $4 ~ 2 5 6 \\times 2 5 6$ images from the ImageNet validation set for this analysis. Note the self-attention layer in [30] only has 1 head so for each image we just plot one attention map. The heat map in Fig. 9 shows the attention scores of each token to all other tokens, which indicate a strong, bidirectional dependency among all tokens. This is not surprising since the VQVAE model, trained to reconstruct images, leverages self-attention layers without any attention mask. Some work [87] has used causal attention in self-attention layers of a video VAE, but we did not find any image VAE work uses causal self-attention.\n\n# D Time complexity of AR and VAR generation\n\nWe prove the time complexity of AR and VAR generation.\n\nLemma D.1. For a standard self-attention transformer, the time complexity of AR generation is $\\mathcal { O } ( n ^ { 6 } )$ , where $h = w = n$ and $h , w$ are the height and width of the VQ code map, respectively.\n\nProof. The total number of tokens is $h \\times w = n ^ { 2 }$ . For the $i$ -th $( 1 \\leq i \\leq n ^ { 2 } ,$ ) autoregressive iteration, the attention scores between each token and all other tokens need to be computed, which requires $\\mathcal { O } ( i ^ { 2 } )$ time. So the total time complexity would be:\n\n$$\n\\sum _ { i = 1 } ^ { n ^ { 2 } } i ^ { 2 } = \\frac { 1 } { 6 } n ^ { 2 } ( n ^ { 2 } + 1 ) ( 2 n ^ { 2 } + 1 ) ,\n$$\n\nWhich is equivalent to $\\mathcal { O } ( n ^ { 6 } )$ basic computation.\n\nFor VAR, it needs us to define the resolution sequense $( h _ { 1 } , w _ { 1 } , h _ { 2 } , w _ { 2 } , \\dots , h _ { K } , w _ { K } )$ for autoregressive generation, where $h _ { i } , w _ { i }$ are the height and width of the VQ code map at the $i$ -th autoregressive step, and $h _ { K } = h , w _ { K } = w$ reaches the final resolution. Suppose $n _ { k } = h _ { k } = w _ { k }$ for all $1 \\leq k \\leq K$ and $n = h = w$ , for simplicity. We set the resolutions as $n _ { k } = a ^ { ( k - 1 ) }$ where $a > 1$ is a constant such that $a ^ { ( K - 1 ) } = n$ .\n\nLemma D.2. For a standard self-attention transformer and given hyperparameter $a > 1$ , the time complexity of VAR generation is $\\mathcal { O } ( n ^ { 4 } )$ , where $h = w = n$ and $h , w$ are the height and width of the last (largest) $_ { V Q }$ code map, respectively.\n\nProof. Consider the $k$ -th $( 1 \\leq k \\leq K )$ autoregressive generation. The total number of tokens of current all token maps $( r _ { 1 } , r _ { 2 } , \\ldots , r _ { k } )$ is:\n\n$$\n\\sum _ { i = 1 } ^ { k } { n _ { i } ^ { 2 } = \\sum _ { i = 1 } ^ { k } a ^ { 2 \\cdot ( k - 1 ) } = \\frac { a ^ { 2 k } - 1 } { a ^ { 2 } - 1 } } .\n$$\n\nSo the time complexity of the $k$ -th autoregressive generation would be:\n\n$$\n\\left( { \\frac { a ^ { 2 k } - 1 } { a ^ { 2 } - 1 } } \\right) ^ { 2 } .\n$$\n\nBy summing up all autoregressive generations, we have:\n\n$$\n\\begin{array} { l } { { \\displaystyle \\sum _ { k = 1 } ^ { \\log _ { a } ( n ) + 1 } \\left( \\frac { a ^ { 2 k } - 1 } { a ^ { 2 } - 1 } \\right) ^ { 2 } } } \\\\ { { = \\displaystyle \\frac { ( a ^ { 4 } - 1 ) \\log n + \\left( a ^ { 8 } n ^ { 4 } - 2 a ^ { 6 } n ^ { 2 } - 2 a ^ { 4 } ( n ^ { 2 } - 1 ) + 2 a ^ { 2 } - 1 \\right) \\log a } { ( a ^ { 2 } - 1 ) ^ { 3 } ( a ^ { 2 } + 1 ) \\log a } } } \\\\ { { \\sim \\mathcal { O } ( n ^ { 4 } ) . } } \\end{array}\n$$\n\nThis completes the proof.\n\n![](images/3247b8e64b09265cd177c4513ff442909d28b38ae4999f2b0c8af911ee0d4a06.jpg)  \nFigure 10: Model comparison on ImageNet $\\pmb { 2 5 6 } \\times \\pmb { 2 5 6 }$ benchmark. More generated $5 1 2 \\times 5 1 2$ samples by VAR can be found in the submitted Supplementary Material zip file.\n\n![](images/56589341352af8c9af29d7dd41f2e2f1adb942837c7339b670b7df0e76d1dd26.jpg)  \nFigure 11: Some generated $\\pmb { 2 5 6 } \\times \\pmb { 2 5 6 }$ samples by VAR trained on ImageNet. More generated $5 1 2 \\times 5 1 2$ samples by VAR can be found in the submitted Supplementary Material zip file.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   传统自回归（AR）模型在图像生成中存在计算效率低（O(n^6)复杂度）、空间局部性破坏和数学前提违背（双向依赖与自回归假设冲突）等问题，导致其性能显著落后于扩散模型。\\n> *   该问题的重要性在于，解决这些问题可以释放自回归模型在视觉生成领域的潜力，实现与语言模型类似的扩展性和泛化能力。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出视觉自回归建模（VAR），通过多尺度“下一尺度预测”替代传统“下一令牌预测”，重新定义图像生成顺序。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **创新贡献点1：** 提出“下一尺度预测”范式，在ImageNet 256×256上实现FID 1.73（提升16.92点）、IS 350.2（提升269.8点），推理速度比VQGAN快20倍（0.6秒/图）。\\n> *   **创新贡献点2：** 首次使AR模型超越DiT（FID 1.73 vs. DiT-XL/2的2.27），并在数据效率（350 vs. 1400训练epochs）、扩展性（线性相关系数-0.998）上占优。\\n> *   **创新贡献点3：** 验证VAR具有LLM特性：展示清晰的幂律扩展定律（公式14）和零样本图像编辑能力（图8）。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   人类视觉的层次化感知启发：将图像生成建模为从1×1令牌开始的粗到细多尺度序列，每个尺度内部并行生成，保持空间局部性。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 传统AR（图2b）需256步生成16×16令牌，VAR（图2c）仅需log(n)步（如10步），复杂度从O(n^6)降至O(n^4)（附录D证明）。\\n> *   **本文的改进：** 1) 多尺度VQVAE（算法1）确保尺度间因果依赖；2) 块状因果注意力（图4）实现前缀条件生成；3) 共享代码本统一多尺度词汇表。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> *   1. **多尺度编码：** 使用残差式量化（公式5）生成K尺度令牌映射R=(r1,...,rK)，分辨率从1×1递增至h×w。\\n> *   2. **自回归训练：** 按公式6最大化似然，用AdaLN（公式8）注入类别信息，q/k单位向量归一化稳定训练。\\n> *   3. **推理生成：** 从[s]开始逐步预测r1→rK，每步并行生成当前尺度所有令牌（图4）。\\n\\n> **案例解析 (Case Study)**\\n> *   图7展示模型规模（18M→2B参数）和训练计算量（20%→100% tokens）对生成质量的影响，验证扩展定律。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   AR模型：VQGAN（18.65 FID）、ViT-VQGAN（4.17 FID）；扩散模型：DiT-XL/2（2.27 FID）、L-DiT-7B（2.28 FID）；掩码预测模型：MaskGIT（6.18 FID）。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在FID上：** VAR-d30在ImageNet 256×256达到1.73，优于VQGAN（18.65）16.92点，比DiT-XL/2（2.27）低0.54点，且参数量更少（2B vs. 3B/7B）。\\n> *   **在IS上：** VAR-d30获得350.2，远超VQGAN（80.4）269.8点，比DiT-XL/2（278.2）高72点。\\n> *   **在效率上：** VAR-d30仅需0.6秒/图，比DiT-XL/2（45秒）快75倍，数据效率高4倍（350 vs. 1400 epochs）。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   视觉自回归建模 (Visual AutoRegressive Modeling, VAR)\\n*   图像生成 (Image Generation, N/A)\\n*   多尺度预测 (Next-scale Prediction, N/A)\\n*   扩散变换器 (Diffusion Transformer, DiT)\\n*   扩展定律 (Scaling Laws, N/A)\\n*   零样本泛化 (Zero-shot Generalization, N/A)\\n*   量化自编码器 (Vector-Quantized Variational Autoencoder, VQVAE)\\n*   功率律 (Power-law, N/A)\"\n}\n```"
}