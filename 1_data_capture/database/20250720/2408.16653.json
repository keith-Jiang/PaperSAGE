{
    "source": "Semantic Scholar",
    "arxiv_id": "2408.16653",
    "link": "https://arxiv.org/abs/2408.16653",
    "pdf_link": "https://arxiv.org/pdf/2408.16653.pdf",
    "title": "Optimal Parallelization of Boosting",
    "authors": [
        "Arthur da Cunha",
        "M. Hogsgaard",
        "Kasper Green Larsen"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2024-08-29",
    "venue": "Neural Information Processing Systems",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Aarhus University",
        "Department of Computer Science"
    ],
    "paper_content": "# Optimal Parallelization of Boosting\n\nArthur da Cunha Department of Computer Science Aarhus University dac@cs.au.dk\n\nMikael Møller Høgsgaard Department of Computer Science Aarhus University hogsgaard@cs.au.dk\n\nKasper Green Larsen Department of Computer Science Aarhus University larsen@cs.au.dk\n\n# Abstract\n\nRecent works on the parallel complexity of Boosting have established strong lower bounds on the tradeoff between the number of training rounds $p$ and the total parallel work per round $t$ . These works have also presented highly non-trivial parallel algorithms that shed light on different regions of this tradeoff. Despite these advancements, a significant gap persists between the theoretical lower bounds and the performance of these algorithms across much of the tradeoff space. In this work, we essentially close this gap by providing both improved lower bounds on the parallel complexity of weak-to-strong learners, and a parallel Boosting algorithm whose performance matches these bounds across the entire $p$ vs. $t$ compromise spectrum, up to logarithmic factors. Ultimately, this work settles the parallel complexity of Boosting algorithms that are nearly sample-optimal.\n\n# 1 Introduction\n\nBoosting is an extremely powerful and elegant idea that allows one to combine multiple inaccurate classifiers into a highly accurate voting classifier. Algorithms such as AdaBoost [Freund and Schapire, 1997] work by iteratively running a base learning algorithm on reweighted versions of the training data to produce a sequence of classifiers $h _ { 1 } , \\ldots , h _ { p }$ . After obtaining $h _ { i }$ , the weighting of the training data is updated to put larger weights on samples misclassified by $h _ { i }$ , and smaller weights on samples classified correctly. This effectively forces the next training iteration to focus on points with which the previous classifiers struggle. After sufficiently many rounds, the classifiers $h _ { 1 } , \\ldots , h _ { p }$ are finally combined by taking a (weighted) majority vote among their predictions. Many Boosting algorithms have been developed over the years, for example Grove and Schuurmans [1998], Rätsch et al. [2005], Servedio [2003], Friedman [2001], with modern Gradient Boosting [Friedman, 2001] algorithms like XGBoost [Chen and Guestrin, 2016] and LightGBM [Ke et al., 2017] often achieving state-of-the-art performance on learning tasks while requiring little to no data cleaning. See e.g. the excellent survey by Natekin and Knoll [2013] for more background on Boosting.\n\nWhile Boosting enjoys many advantages, it does have one severe drawback, also highlighted in Natekin and Knoll [2013]: Boosting is completely sequential as each of the consecutive training steps requires the output of previous steps to determine the reweighted learning problem. This property is shared by all Boosting algorithms and prohibits the use of computationally heavy training by the base learning algorithm in each iteration. For instance, Gradient Boosting algorithms often require hundreds to thousands of iterations to achieve the best accuracy. The crucial point is that even if you have access to thousands of machines for training, there is no way to parallelize the steps of Boosting and distribute the work among the machines (at least beyond the parallelization possible for the base learner). In effect, the training time of the base learning algorithm is directly multiplied by the number of steps of Boosting.\n\nMultiple recent works [Long and Servedio, 2013, Karbasi and Larsen, 2024, Lyu et al., 2024] have studied parallelization of Boosting from a theoretical point of view, aiming for an understanding of the inherent tradeoffs between the number of training rounds $p$ and the total parallel work per round $t$ . These works include both strong lower bounds on the cost of parallelization and highly non-trivial parallel Boosting algorithms with provable guarantees on accuracy. Previous studies however leave a significant gap between the performance of the parallel algorithms and the proven lower bounds.\n\nThe main contribution of this work is to close this gap by both developing a parallel algorithm with a better tradeoff between $p$ and $t$ , as well as proving a stronger lower bound on this tradeoff. To formally state our improved results and compare them to previous works, we first introduce the theoretical framework under which parallel Boosting is studied.\n\nWeak-to-Strong Learning. Following the previous works Karbasi and Larsen [2024], Lyu et al. [2024], we study parallel Boosting in the theoretical setup of weak-to-strong learning. Weak-tostrong learning was introduced by Kearns [1988], Kearns and Valiant [1994] and has inspired the development of the first Boosting algorithms [Schapire, 1990]. In this framework, we consider binary classification over an input domain $\\chi$ with an unknown target concept $c \\colon \\mathcal { X }  \\{ - 1 , 1 \\}$ assigning labels to samples. A $\\gamma$ -weak learner for $c$ is then a learning algorithm $\\boldsymbol { \\mathcal { W } }$ that for any distribution $\\mathcal { D }$ over $\\chi$ , when given at least some constant $m _ { 0 }$ i.i.d. samples from $\\mathcal { D }$ , produces with constant probability a hypothesis $h$ with $\\mathcal { L } _ { D } ( h ) \\leq 1 / 2 - \\gamma$ . Here $\\mathcal { L } _ { \\mathcal { D } } ( h ) = \\mathrm { P r } _ { { \\mathbf x } \\sim \\mathcal { D } } [ h ( { \\mathbf x } ) \\neq c ( { \\mathbf x } ) ]$ . The goal in weak-to-strong learning is then to boost the accuracy of $\\mathcal { W }$ by invoking it multiple times. Concretely, the aim is to produce a strong learner: A learning algorithm that for any distribution $\\mathcal { D }$ over $\\chi$ and any $0 < \\delta , \\varepsilon < 1$ , when given $m ( \\varepsilon , \\delta )$ i.i.d. samples from $\\mathcal { D }$ , produces with probability at least $1 - \\delta$ a hypothesis $h \\colon \\mathcal { X }  \\{ - 1 , 1 \\}$ such that $\\bar { \\mathcal { L } _ { D } } ( h ) \\bar { \\leq } \\varepsilon$ . We refer to $m ( \\varepsilon , \\delta )$ as the sample complexity of the weak-to-strong learner.\n\nWeak-to-strong learning has been extensively studied over the years, with many proposed algorithms, among which AdaBoost [Freund and Schapire, 1997] is perhaps the most famous. If $\\mathcal { H }$ denotes a hypothesis set such that $\\boldsymbol { \\mathcal { W } }$ always produces hypotheses from $\\mathcal { H }$ , and if $d$ denotes the VC-dimension of $\\mathcal { H }$ , then in terms of sample complexity, AdaBoost is known to produce a strong learner with sample complexity $m _ { \\mathrm { A d a } } ( \\varepsilon , \\delta )$ satisfying\n\n$$\nm _ { \\mathrm { A d a } } ( \\varepsilon , \\delta ) = \\mathrm { O } \\left( \\frac { d \\ln ( \\frac { d } { \\varepsilon \\gamma } ) \\ln ( \\frac { 1 } { \\varepsilon \\gamma } ) } { \\gamma ^ { 2 } \\varepsilon } + \\frac { \\ln ( 1 / \\delta ) } { \\varepsilon } \\right) .\n$$\n\nThis can be proved by observing that after $t = \\operatorname { O } ( \\gamma ^ { - 2 } \\ln m )$ iterations, AdaBoost produces a voting classifier $\\begin{array} { r } { f ( x ) = \\mathrm { s i g n } ( \\sum _ { i = 1 } ^ { t } \\alpha _ { i } h _ { i } ( x ) ) } \\end{array}$ with all margins on the training data being $\\Omega ( \\gamma )$ . The sample complexity bound then follows by invoking the best known generalization bounds for large margin voting classifiers [Breiman, 1999, Gao and Zhou, 2013]. Here the margin of the voting classifier $f$ on a training sample $( x , c ( x ) )$ is defined as $c ( \\boldsymbol { x } ) \\sum _ { i = 1 } ^ { t } { \\alpha _ { i } h _ { i } ( \\boldsymbol { x } ) } / \\sum _ { i = 1 } ^ { t } { | \\alpha _ { i } | }$ . This sample complexity comes within logarithmic factors of the optimal sample complexity $m _ { \\mathrm { O P T } } ( \\varepsilon , \\delta ) = \\Theta ( d / ( \\gamma ^ { 2 } \\varepsilon ) +$ $\\ln ( 1 / \\delta ) / \\varepsilon )$ obtained e.g. in Larsen and Ritzert [2022].\n\nParallel Weak-to-Strong Learning. The recent work by Karbasi and Larsen [2024] formalized parallel Boosting in the above weak-to-strong learning setup. Observing that all training happens in the weak learner, they proposed the following definition of parallel Boosting: A weak-to-strong learning algorithm has parallel complexity $( p , t )$ if for $p$ consecutive rounds it queries the weak learner with $t$ distributions. In each round $i$ , if $D _ { 1 } ^ { i } , \\ldots , D _ { t } ^ { i }$ denotes the distributions queried, the weak learner returns $t$ hypotheses $h _ { 1 } ^ { i } , \\ldots , h _ { t } ^ { i } \\in \\mathcal { H }$ such that $\\mathcal { L } _ { D _ { j } ^ { i } } ( h _ { j } ^ { i } ) \\leq 1 / 2 - \\gamma$ for all $j$ . At the end of the $p$ rounds, the weak-to-strong learner outputs a hypothesis $f \\colon \\mathcal { X }  \\{ - 1 , 1 \\}$ . The queries made in each round and the final hypothesis $f$ must be computable from the training data as well as all hypotheses $h _ { j } ^ { i }$ seen in previous rounds. The motivation for the above definition is that we could let one machine/thread handle each of the $t$ parallel query distributions in a round.\n\nSince parallel weak-to-strong learning is trivial if we make no requirements on $\\mathcal { L } _ { D } ( f )$ for the output $f \\colon \\mathcal { X }  \\{ - 1 , 1 \\}$ (simply output $f ( x ) = 1$ for all $x \\in \\mathcal { X }$ ), we from hereon focus on parallel weakto-strong learners that are near-optimal in terms of the sample complexity and accuracy tradeoff.\n\nMore formally, from the upper bound side, our goal is to obtain a sample complexity matching at least that of AdaBoost, stated in Eq. (1). That is, rewriting the loss $\\varepsilon$ as a function of the number of samples $m$ , we aim for output classifiers $f$ satisfying\n\n$$\n\\mathcal { L } _ { \\mathcal { D } } ( f ) = \\mathrm { O } \\Bigg ( \\frac { d \\ln ( m ) \\ln ( m / d ) + \\ln ( 1 / \\delta ) } { \\gamma ^ { 2 } m } \\Bigg ) .\n$$\n\nWhen stating lower bounds in the following, we have simplified the expressions by requiring that the expected loss satisfies $\\mathcal { L } _ { D } ( f ) = \\mathrm { O } ( m ^ { - 0 . 0 1 } )$ . Note that this is far larger than the upper bounds, except for values of $m$ very close to $\\gamma ^ { - 2 } d$ . This only makes the lower bounds stronger. We remark that all the lower bounds are more general than this, but focusing on $m ^ { - 0 . 0 1 }$ in this introduction yields the cleanest bounds.\n\nWith these definitions, classic AdaBoost and other weak-to-strong learners producing voting classifiers with margins $\\Omega ( \\gamma )$ all have a parallel complexity of $( \\check { \\Theta } ( \\gamma ^ { - 2 } \\ln \\dot { m } ) , 1 )$ : They all need $\\gamma ^ { - 2 } \\ln m$ rounds to obtain $\\Omega ( \\gamma )$ margins. Karbasi and Larsen [2024] presented the first alternative tradeoff by giving an algorithm with parallel complexity $\\bar { ( 1 , \\exp ( \\mathrm { O } ( d \\ln ( m ) / \\gamma ^ { 2 } ) ) } ,$ ). Subsequent work by Lyu et al. [2024] gave a general tradeoff between $p$ and $t$ . When requiring near-optimal accuracy, their tradeoff gives, for any $1 \\leq R \\leq 1 / ( 2 \\gamma )$ , a parallel complexity of $( O ( \\gamma ^ { - 2 } \\ln ( m ) / R )$ , $\\dot { \\exp ( \\mathrm { O } ( d R ^ { 2 } ) ) } \\ln ( \\bar { 1 / \\gamma } ) )$ . The accuracy of both of these algorithms was proved by arguing that they produce a voting classifier with all margins $\\Omega ( \\gamma )$ .\n\nOn the lower bound side, Karbasi and Larsen [2024] showed that one of three things must hold: Either $p ~ \\geq ~ \\operatorname* { m i n } \\{ \\Omega ( \\gamma ^ { - 1 } \\ln m ) , \\exp ( \\Omega ( d ) ) \\}$ , or $t \\ \\geq \\ \\operatorname* { m i n } \\{ \\exp ( \\Omega ( d \\gamma ^ { - 2 } ) ) , \\exp ( \\exp ( \\Omega ( d ) ) ) \\}$ or $p \\ln ( t p ) = \\Omega ( d \\ln ( m ) \\gamma ^ { - 2 } )$ .\n\nLyu et al. [2024] also presented a lower bound that for some parameters is stronger than that of Karbasi and Larsen [2024], and for some is weaker. Concretely, they show that one of the following two must hold: Either $p \\ge \\operatorname* { m i n } \\{ \\Omega ( \\gamma ^ { - 2 } d ) , \\Omega ( \\gamma ^ { - 2 } \\ln m ) , \\exp ( \\Omega ( d ) ) \\}$ , or $t \\geq \\exp ( \\Omega ( d ) )$ . Observe that the constraint on $t$ is only single-exponential in $d$ , whereas the previous lower bound is doubleexponential. On the other hand, the lower bound on $p$ is essentially stronger by a $\\gamma ^ { - 1 }$ factor. Finally, they also give an alternative lower bound for $p = \\bar { \\mathrm { O } ( \\gamma ^ { - 2 } ) }$ , essentially yielding $p \\ln t = \\Omega ( \\gamma ^ { - 2 } d )$ .\n\nEven in light of the previous works, it is still unclear what the true complexity of parallel boosting is. In fact, the upper and lower bounds only match in the single case where $\\begin{array} { r } { \\dot { p } = \\dot { \\Omega } ( \\gamma ^ { - 2 } \\ln m ) } \\end{array}$ and $t = 1$ , i.e. when standard AdaBoost is optimal.\n\nOur Contributions. In this work, we essentially close the gap between the upper and lower bounds for parallel boosting. From the upper bound side, we show the following general result.\n\nTheorem 1.1. Let $c \\colon \\mathcal { X }  \\{ - 1 , 1 \\}$ be an unknown concept, $\\boldsymbol { \\mathcal { W } }$ be a $\\gamma$ -weak learner for c using a hypothesis set of VC-dimension $d$ , $\\mathcal { D }$ be an arbitrary distribution, and $\\mathbf { S } \\sim \\mathcal { D } ^ { m }$ be a training set of size m. For all $R \\in \\mathbb { N } ,$ , Algorithm $\\jmath$ yields a weak-to-strong learner $\\mathcal { A } _ { R }$ with parallel complexity $( p , t )$ for\n\n$$\np = \\mathrm { O } \\left( { \\frac { \\ln m } { \\gamma ^ { 2 } R } } \\right) \\qquad { \\mathit { a n d } } \\qquad t = e ^ { \\mathrm { O } \\left( d R \\right) } \\cdot \\ln { \\frac { \\ln m } { \\delta \\gamma ^ { 2 } } } ,\n$$\n\nsuch that, with probability at least $1 - \\delta$ over S and the randomness of $\\mathcal { A } _ { R }$ , it holds that\n\n$$\n\\mathcal { L } _ { \\mathcal { D } } ( \\mathcal { A } _ { R } ( { \\bf S } ) ) = \\mathrm { O } \\bigg ( \\frac { d \\ln ( m ) \\ln ( m / d ) + \\ln ( 1 / \\delta ) } { \\gamma ^ { 2 } m } \\bigg ) .\n$$\n\nObserve that this is a factor $R$ better than the bound by Lyu et al. [2024] in the exponent of $t$ . Furthermore, if we ignore the $\\ln ( \\ln ( m ) / ( \\delta \\gamma ^ { 2 } ) )$ factor, it gives the clean tradeoff\n\n$$\np \\ln t = \\mathrm { O } \\biggl ( \\frac { d \\ln m } { \\gamma ^ { 2 } } \\biggr ) ,\n$$\n\nfor any $p$ from 1 to $\\operatorname { O } ( \\gamma ^ { - 2 } \\ln m )$ .\n\nWe complement our new upper bound by an essentially matching lower bound. Here we show that\n\nTheorem 1.2. There is a universal constant $C \\geq 1$ for which the following holds. For any $0 <$ $\\gamma < 1 / C$ , any $d \\geq C$ , any sample size $m \\geq C$ , and any weak-to-strong learner $\\mathcal { A }$ with parallel complexity $( p , t )$ , there exists an input domain $\\chi$ , a distribution $\\mathcal { D }$ , a concept $c \\colon \\mathcal { X }  \\{ - 1 , 1 \\}$ , and $\\boldsymbol { a } \\gamma$ -weak learner $\\mathcal { W }$ for c using a hypothesis set $\\mathcal { H }$ of VC-dimension $d$ such that if the expected loss of $\\mathcal { A }$ over the sample is no more than $m ^ { - 0 . 0 1 }$ , then either $p \\ge \\operatorname* { m i n } \\{ \\exp ( \\Omega ( d ) ) , \\Omega ( \\gamma ^ { - 2 } \\ln m ) \\}$ , or $\\dot { t } \\geq \\exp ( \\exp ( \\Omega ( \\dot { d } ) ) )$ , or $p \\ln t = \\Omega ( \\gamma ^ { - 2 } d \\ln m )$ .\n\nComparing Theorem 1.2 to known upper bounds, we first observe that $p = \\Omega ( \\gamma ^ { - 2 } \\ln m )$ corresponds to standard AdaBoost and is thus tight. The term $p = \\exp ( \\Omega ( d ) )$ is also near-tight. In particular, given $m$ samples, by Sauer-Shelah, there are only $\\mathrm { O } ( ( m / d ) ^ { d } ) = \\exp ( \\mathrm { O } ( d \\ln ( m / d ) ) )$ distinct labellings by $\\mathcal { H }$ on the training set. If we run AdaBoost, and in every iteration, we check whether a previously obtained hypothesis has advantage $\\gamma$ under the current weighing, then we make no more than $\\exp ( \\dot { \\mathrm { O } } ( d \\ln ( m / d ) ) )$ queries to the weak learner (since every returned hypothesis must be distinct). The $p \\ln t = \\Omega ( \\gamma ^ { - 2 } d \\ln m )$ matches our new upper bound in Theorem 1.1. Thus, only the $t \\geq \\exp ( \\exp ( \\Omega ( d ) ) )$ term does not match any known upper bound.\n\nOther Related Work. Finally, we mention the work by Long and Servedio [2013], which initiated the study of the parallel complexity of Boosting. In their work, they proved that the parallel complexity $( p , t )$ must satisfy $p = \\dot { \\Omega } ( \\gamma ^ { - 2 } \\ln m )$ , regardless of $t$ (they state it as $p = \\Omega ( \\gamma ^ { - 2 } )$ , but it is not hard to improve by a $\\ln m$ factor for loss $m ^ { - 0 . 0 1 }$ ). This seems to contradict the upper bounds above. The reason is that their lower bound has restrictions on which query distributions the weak-to-strong learner makes to the weak learner. The upper bounds above thus all circumvent these restrictions. As a second restriction, their lower bound instance has a VC-dimension that grows with $m$ .\n\n# 2 Upper Bound\n\nIn this section, we discuss our proposed method, Algorithm 1. Here, $C _ { \\mathrm { n } }$ refers a universal constant shared among results.\n\nWe provide a theoretical analysis of the algorithm, showing that it realizes the claims in Theorem 1.1. Our proof goes via the following intermediate theorem:\n\nTheorem 2.1. There exists universal constant $C _ { \\mathrm { n } } ~ \\ge ~ 1$ such that for all $0 < \\gamma < 1 / 2 , R \\in \\mathbb { N } ,$ , concept $c \\colon \\mathcal { X }  \\{ - 1 , 1 \\}$ , and hypothesis set $\\mathcal { H } \\subseteq \\{ - 1 , 1 \\} ^ { \\mathcal { X } }$ of $V C$ -dimension $d$ , Algorithm $\\jmath$ given an input training set $S \\in \\mathcal { X } ^ { m }$ , $a \\gamma$ -weak learner $\\mathcal { W }$ ,\n\n$$\np \\ge \\frac { 4 \\ln m } { \\gamma ^ { 2 } R } , \\qquad a n d \\qquad t \\ge e ^ { 1 6 C _ { \\mathrm { n } } d R } \\cdot R \\ln \\frac { p R } { \\delta } ,\n$$\n\nproduces a linear classifier g at Line 21 such that with probability at least $1 - \\delta$ over the randomness of Algorithm 1, $\\mathbf { g } ( x ) c ( x ) \\geq \\gamma / 8$ for all $x \\in S$ .\n\nIn Theorem 2.1 and throughout the paper, we define a linear classifier $g$ as linear combination of hypotheses $\\begin{array} { r } { g ( x ) = \\sum _ { i = 1 } ^ { k } \\alpha _ { i } h _ { i } ( x ) } \\end{array}$ with $\\textstyle \\sum _ { i } | \\alpha _ { i } | = 1$ . A linear classifier thus corresponds to a voting classifier with coefficients normalized and no sign operation. Observe that the voting classifier $f ( x ) = \\mathrm { s i g n } ( g ( x ) )$ is correct if and only if $c ( x ) \\bar { g } ( x ) > 0$ , where $c ( x )$ is the correct label of $x$ . Furthermore, $\\overset { \\cdot } { c } ( \\overset { \\cdot } { x } ) \\overset { \\cdot } { g } ( \\overset { \\cdot } { x } )$ is the margin of the voting classifier $f$ on input $x$ .\n\nTheorem 1.1 follows from Theorem 2.1 via generalization bounds for linear classifiers with large margins. Namely, we apply Breiman’s min-margin bound:\n\nTheorem 2.2 (Breiman [1999]). Let $c \\colon \\mathcal { X }  \\{ - 1 , 1 \\}$ be an unknown concept, $\\mathcal { H } \\subseteq \\{ - 1 , 1 \\} ^ { \\mathcal { X } }$ a hypothesis set of VC-dimension $d$ and $\\mathcal { D }$ an arbitrary distribution over $\\chi$ . There is a universal constant $C > 0$ such that with probability at least $1 - \\delta$ over a set of m samples $\\mathbf { S } \\sim \\mathcal { D } ^ { m }$ , it holds for every linear classifier $g$ satisfying $c ( { \\dot { x } } ) g ( x ) \\geq \\gamma$ for all $( x , c ( x ) ) \\in \\mathbf { S }$ that\n\n$$\n\\mathcal { L } _ { \\mathcal { D } } ( \\mathrm { s i g n } ( g ) ) \\leq C \\cdot \\frac { d \\ln ( m ) \\ln ( m / d ) + \\ln ( 1 / \\delta ) } { \\gamma ^ { 2 } m } .\n$$\n\nThus far, our general strategy mirrors that of previous works: We seek to show that given suitable parameters Algorithm 1 produces a linear classifier with margins of order $\\gamma$ with good probability.\n\nAlgorithm 1: Proposed parallel boosting algorithm   \n\n<html><body><table><tr><td colspan=\"2\"></td><td colspan=\"2\">Input : Training set S = {(x1,c(x1)),...,(xm,c(xm))}, γ-weak learner W,number of calls to weak learner per round t, number of rounds p</td></tr><tr><td></td><td colspan=\"2\"></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td>3D1←(</td><td></td></tr><tr><td></td><td colspan=\"2\">4 fork←0top-1do</td></tr><tr><td>5</td><td>parallelforr←1toRdo</td><td></td></tr><tr><td>6</td><td></td><td>parallel for j ←1 to t/Rdo</td></tr><tr><td>7</td><td></td><td>Sample TkR+r.j ~DR+1</td></tr><tr><td>8</td><td></td><td>hkR+r,j ←W(TkR+r,j,Uniform(TkR+r,j))</td></tr><tr><td>9</td><td></td><td>HkR+r ← {hkR+r,1,..,hkR+r,t/R} U{-hkR+r,1,...,-hkR+r,t/R}</td></tr><tr><td>10</td><td>forr←1toRdo</td><td></td></tr><tr><td>11</td><td></td><td>if there exists h* ∈ HkR+r s.t. LDkR+r(h*) ≤1/2- γ/2 then</td></tr><tr><td>12</td><td></td><td>hkR+r ←h*</td></tr><tr><td>13</td><td></td><td>αkR+r←α</td></tr><tr><td>14</td><td>else</td><td></td></tr><tr><td>15</td><td></td><td>hk R+r ← arbitrary hypothesis from Hk R+r</td></tr><tr><td>16</td><td></td><td>αkR+r←0</td></tr><tr><td>17</td><td>fori←1 to m do</td><td></td></tr><tr><td>18</td><td></td><td>DkR+r+1(i) ← DkR+r(i) exp(-αkR+rC(xi)hkR+r(xi))</td></tr><tr><td>19</td><td></td><td>ZkR+r ←∑m=1DkR+r(𝑖)exp(-akR+rc(xi)hkR+r(xi))</td></tr><tr><td>20</td><td>1</td><td>DkR+r+1 ←DkR+r+1/ZkR+r</td></tr><tr><td>21g↑x</td><td>∑=1αjhj(x） pR 22 return f: x -> sign(g(x)) =1αj</td><td></td></tr></table></body></html>\n\nTherefore, this section focuses on the lemmas that describe how, with suitable parameters, Algorithm 1 produces a classifier with large margins. With these results in hand, the proof of Theorem 2.1 becomes quite straightforward, so we defer it to Appendix B.3.\n\nAlgorithm 1 is a variant of Lyu et al. [2024, Algorithm 2]. The core idea is to use bagging to produce (in parallel) a set of hypotheses and use it to simulate a weak learner. To be more precise, we reason in terms of the following definition.\n\nDefinition 1 ( $\\dot { \\varepsilon }$ -approximation). Given a concept $c \\colon \\mathcal { X }  \\{ - 1 , 1 \\}$ , a hypothesis set $\\mathcal { H } \\subseteq \\{ - 1 , 1 \\} ^ { \\mathcal { X } }$ , and a distribution $\\mathcal { D }$ over $\\chi$ , a multiset $T$ is an $\\varepsilon$ -approximation for $\\mathcal { D } , c$ , and $\\mathcal { H }$ if for all $h \\in { \\mathcal { H } }$ , it holds that\n\n$$\n| \\mathcal { L } _ { \\mathcal { D } } ( h ) - \\mathcal { L } _ { T } ( h ) | \\leq \\varepsilon ,\n$$\n\nwhere $\\mathcal L _ { T } ( h ) : = \\mathcal L _ { \\mathrm { U n i f o r m } ( T ) } ( h )$ is the empirical loss of $h$ on $T$ . Moreover, we omit the reference to $c$ and $\\mathcal { H }$ when no confusion seems possible.\n\nConsider a reference distribution $D _ { 0 }$ over a training dataset $S$ . The bagging part of the method leverages the fact that if a subsample $\\mathbf { T } \\sim D _ { 0 } ^ { n }$ is a $\\gamma / 2$ -approximation for $D _ { 0 }$ , then inputting $\\mathbf { T }$ (with the uniform distribution over it) to a $\\gamma$ -weak learner produces a hypothesis $h$ that, besides having advantage $\\gamma$ on $\\mathbf { T }$ , also has advantage $\\gamma / 2$ on the entire dataset $S$ (relative to $D _ { 0 }$ ). Indeed, in this setting, we have that $\\mathcal { L } _ { D _ { 0 } } ( h ) \\leq \\mathcal { L } _ { \\mathbf { T } } ( h ) \\dot { } + \\gamma / 2 \\leq 1 / 2 - \\gamma + \\gamma / 2 = 1 / 2 - \\gamma / 2$ . We can then take $h$ as if produced by a $\\gamma / 2$ -weak learner queried with $( S , D _ { 0 } )$ , and compute a new distribution $D _ { 1 }$ via a standard Boosting step1. That is, we can simulate a $\\gamma / 2$ -weak learner as long as we can provide a $\\gamma / 2$ -approximation for the target distribution. The strategy is to have a parallel bagging step in which we sample $\\mathbf { T } _ { 1 } , \\mathbf { T } _ { 2 } , \\ldots , \\mathbf { T } _ { t } \\overset { \\mathrm { i i d } } { \\sim } D _ { 0 } ^ { n }$ and query the $\\gamma$ -weak learner on each $\\mathbf { T } _ { j }$ to obtain hypotheses $\\mathbf { h } _ { 1 } , \\ldots , \\mathbf { h } _ { t }$ . Then, we search within these hypotheses to sequentially perform $R$ Boosting steps, obtaining distributions $D _ { 1 } , D _ { 2 } , \\ldots , D _ { R }$ . As argued, this approach will succeed whenever we can find at least one $\\gamma / 2$ -approximation for each $D _ { r }$ among $\\mathbf { h } _ { 1 } , \\mathbf { h } _ { 2 } , \\ldots , \\mathbf { h } _ { t }$ . A single parallel round of querying the weak learner is thus sufficient for performing $R$ steps of Boosting, effectively reducing $p$ by a factor $R$ . Crucially, testing the performance of the returned hypotheses $\\mathbf { h } _ { 1 } , \\ldots , \\mathbf { h } _ { t }$ uses only inference/predictions and no calls to the weak learner.\n\nThe challenge is that the distributions $D _ { r }$ diverge (exponentially fast) from $D _ { 0 }$ as we progress in the Boosting steps. For the first Boosting step, the following classic result ensures a good probability of obtaining an approximation for $D _ { 0 }$ when sampling from $D _ { 0 }$ itself.\n\nTheorem 2.3 (Li et al. [2001], Talagrand [1994], Vapnik and Chervonenkis [1971]). There is $a$ universal constant $C > 0$ such that for any $0 < \\varepsilon , \\delta < 1$ , $\\mathcal { H } \\subseteq \\{ - 1 , 1 \\} ^ { \\mathcal { X } }$ of VC-dimension $d ,$ , and distribution $\\mathcal { D }$ over $\\chi$ , it holds with probability at least $1 - \\delta$ over a set $\\mathbf { T } \\sim { \\cal { D } } ^ { n }$ that $\\mathbf { T }$ is an $\\varepsilon$ -approximation for ${ \\mathcal { D } } , c ,$ , and $\\mathcal { H }$ provided that $n \\geq C ( ( d + \\ln ( 1 / \\delta ) ) / \\varepsilon ^ { 2 } )$ .\n\nHowever, we are interested in approximations for $D _ { r }$ when we only have access to samples from $D _ { 0 }$ . Lyu et al. [2024] approaches this problem by tracking the “distance” between the distributions in terms of their max-divergence\n\n$$\n\\mathrm { D } _ { \\infty } ( D _ { r } , D _ { 0 } ) : = \\ln \\bigl ( \\operatorname* { s u p } _ { x \\in \\mathcal { X } } D _ { r } ( x ) / D _ { 0 } ( x ) \\bigr ) .\n$$\n\nBy bounding both $\\mathrm { D } _ { \\infty } ( D _ { r } , D _ { 0 } )$ and $\\mathrm { D } _ { \\infty } ( D _ { 0 } , D _ { r } )$ , the authors can leverage the advanced composition theorem [Dwork et al., $2 0 1 0 ] ^ { 2 }$ from the differential privacy literature to bound the probability of obtaining an approximation for $D _ { r }$ when sampling from $D _ { 0 }$ . In turn, this allows them to relate the number of samples $t$ and the (sufficiently small) number of Boosting steps $R$ in a way that ensures a good probability of success at each step.\n\nBesides setting up the application of advanced composition, the use of the max-divergence also simplifies the analysis since its “locality” allows one to bound the divergence between the two distributions via a worst-case study of a single entry. However, this approach sacrifices global information, limiting how much we can leverage our understanding of the distributions generated by Boosting algorithms. With that in mind, we instead track the distance between $D _ { r }$ and $D _ { 0 }$ in terms of the Kullback-Leibler divergence (KL divergence) [Kullback and Leibler, 1951] between them:\n\n$$\n\\mathrm { K L } ( D _ { r } \\parallel D _ { 0 } ) : = \\sum _ { x \\in \\mathcal { X } } D _ { r } ( x ) \\ln \\frac { D _ { r } ( x ) } { D _ { 0 } ( x ) } .\n$$\n\nComparing this expression to Eq. (2) reveals that the max-divergence is indeed a worst-case estimation of the KL divergence.\n\nThe KL divergence —also known as relative entropy— between two distributions $P$ and $Q$ is always non-negative and equal to zero if and only if $P = Q$ . Moreover, in our setting, it is always finite due to the following remark.3\n\nRemark 1. In the execution Algorithm 1, every distribution $D _ { \\ell }$ , for $\\ell \\in [ p R ]$ , has the same support.   \nThis must be the case since Line 20 always preserves the support of $D _ { 1 }$ .\n\nOn the other hand, the KL divergence is not a proper metric as it is not symmetric and it does not satisfy the triangle inequality, unlike the max-divergence. This introduces a number of difficulties in bounding the divergence between $\\mathcal { D } _ { 0 }$ and $\\textstyle { \\mathcal { D } } _ { r }$ . Overcoming these challenges requires a deeper and highly novel analysis. Our results reveal that the KL divergence captures particularly well the behavior of our Boosting algorithm. We remark that we are not the first to relate KL divergence and Boosting, see e.g. Schapire and Freund [2012, Chapter 8 and the references therein], yet we make several new contributions to this connection.\n\nTo study the probability of obtaining a $\\gamma / 2$ -approximation for $D _ { r }$ when sampling from $D _ { 0 }$ , rather than using advanced composition, we employ the duality formula for variational inference [Donsker and Varadhan, 1975] —also known as Gibbs variational principle, or Donsker-Varadhan formula— to estimate such a probability in terms of $\\mathrm { K L } ( D _ { r } \\parallel D _ { 0 } )$ .\n\nLemma 2.4 (Duality formula4). Given finite probability spaces $( \\Omega , { \\mathcal { F } } , P )$ and $( \\Omega , { \\mathcal { F } } , Q )$ , if $P$ and $Q$ have the same support, then for any real-valued random variable $\\mathbf { X }$ on $( \\Omega , { \\mathcal { F } } , P )$ we have that\n\n$$\n\\ln \\mathbb { E } _ { P } \\big [ e ^ { \\mathbf { X } } \\big ] \\geq \\mathbb { E } _ { Q } [ \\mathbf { X } ] - \\mathrm { K L } ( Q \\parallel P ) .\n$$\n\nLemma 2.4 allows us to prove that if $\\mathrm { K L } ( D _ { r } \\parallel D _ { 0 } )$ is sufficiently small, then the probability of obtaining a $\\gamma / 2$ -approximation for $D _ { r }$ when sampling from $D _ { 0 }$ is sufficiently large. Namely, we prove the following.\n\nLemma 2.5. There exists universal constant $C _ { \\mathrm { n } } \\geq 1$ for which the following holds. Given $0 < \\gamma <$ $1 / 2 , R , m \\in \\mathbb { N }$ , concept $c \\colon \\mathcal { X }  \\{ - 1 , 1 \\}$ , and hypothesis set $\\mathcal { H } \\subseteq \\{ - 1 , 1 \\} ^ { \\mathcal { X } }$ of $V C$ -dimension $d$ , let $\\tilde { D }$ and $D$ be distributions over $[ m ]$ and $\\mathcal { G } \\in [ m ] ^ { * }$ be the family of $\\gamma / 2$ -approximations for $D , c ,$ and $\\mathcal { H }$ . If $\\tilde { D }$ and $D$ have the same support and\n\n$$\n\\mathrm { K L } ( D \\parallel \\tilde { D } ) \\leq 4 \\gamma ^ { 2 } R ,\n$$\n\nthen for all $n \\geq C _ { \\mathrm { n } } \\cdot d / \\gamma ^ { 2 }$ it holds that\n\n$$\n\\begin{array} { r } { \\operatorname* { P r } _ { \\mathbf { \\tilde { \\mu } } ^ { \\tilde { \\mathbf { \\mu } } } } [ \\mathbf { T } \\in \\mathcal { G } ] \\geq \\exp ( - 1 6 C _ { \\mathrm { n } } d R ) . } \\\\ { \\mathbf { T } { \\sim } \\tilde { D } ^ { n } } \\end{array}\n$$\n\nProof sketch. Our argument resembles a proof of the Chernoff bound: After taking exponentials on both sides of Eq. (3), we exploit the generality of Lemma 2.4 by defining the random variable $\\mathbf { X } \\colon T \\mapsto \\lambda \\mathbf { 1 } _ { \\{ T \\in { \\mathcal { G } } \\} }$ and later carefully choosing $\\lambda$ . We then note that Theorem 2.3 ensures that $\\mathbf { X }$ has high expectation for $\\mathbf { T } \\sim D ^ { n }$ . Setting $\\lambda$ to leverage this fact, we obtain a lower bound on the expectation of $\\mathbf { X }$ relative to $\\mathbf { T } \\sim \\tilde { D } ^ { n }$ , yielding the thesis. □\n\nWe defer the detailed proof to Appendix B.1.\n\nWith Lemma 2.5 in hand, recall that our general goal is to show that, with high probability, the linear classifier $g$ produced by Algorithm 1 satisfies that $c ( x ) g ( x ) = \\Omega ( \\gamma )$ for all $x \\in S$ . Standard techniques allow us to further reduce this goal to that of showing that the product of the normalization factors, $\\textstyle \\prod _ { \\ell = 1 } ^ { p R } Z _ { \\ell }$ , is sufficiently small. Accordingly, in our next lemma, we bound the number of samples needed in the bagging step to obtain a small product of the normalization factors produced by the Boosting steps.\n\nHere, the analysis in terms of the KL divergence delivers a clear insight into the problem, revealing an interesting trichotomy: if $\\mathrm { K L } ( D _ { r } \\| D _ { 0 } )$ is small, Lemma 2.5 yields the result; on the other hand, if $D _ { r }$ has diverged too far from $D _ { 0 }$ , then either the algorithm has already made enough progress for us to skip a step, or the negation of some hypothesis used in a previous step has sufficient advantage relative to the distribution at hand. Formally, we prove the following.\n\nLemma 2.6. There exists universal constant $C _ { \\mathrm { n } } \\geq 1$ such that for all $R \\in \\mathbb { N }$ , $0 < \\delta < 1$ , $0 <$ $\\gamma < 1 / 2$ , and $\\gamma$ -weak learner $\\mathcal { W }$ using a hypothesis set $\\mathcal { H } \\subseteq \\{ - 1 , 1 \\} ^ { \\mathcal { X } }$ with VC-dimension d, if $t \\geq$ $R \\cdot \\exp ( 1 6 C _ { \\mathrm n } d R ) \\cdot \\ln ( R / \\delta )$ , then with probability at least $1 - \\delta$ the hypotheses $\\mathbf { h } _ { k R + 1 } , \\dots , \\mathbf { h } _ { k R + R }$ obtained by Algorithm $^ { l }$ induce normalization factors $\\mathbf { Z } _ { k R + 1 } , \\ldots , \\mathbf { Z } _ { k R + R }$ such that\n\n$$\n\\prod _ { r = 1 } ^ { R } { \\bf Z } _ { k R + r } < \\exp ( - \\gamma ^ { 2 } R / 2 ) .\n$$\n\nProof sketch. We assume for simplicity that $k = 0$ and argue by induction on $R ^ { \\prime } \\in [ R ]$ . After handling the somewhat intricate stochastic relationships of the problem, we leverage the simple\n\nremark that $\\mathrm { K L } ( D _ { R ^ { \\prime } } \\| D _ { R ^ { \\prime } } ) = 0$ to reveal the following telescopic decomposition:\n\n$$\n\\begin{array} { r l } { \\mathrm { K L } ( D _ { R ^ { \\prime } } \\parallel D _ { 1 } ) = \\mathrm { K L } ( D _ { R ^ { \\prime } } \\parallel D _ { 1 } ) - \\mathrm { K L } ( D _ { R ^ { \\prime } } \\parallel D _ { R ^ { \\prime } } ) } & { } \\\\ { = \\mathrm { K L } ( D _ { R ^ { \\prime } } \\parallel D _ { 1 } ) - \\mathrm { K L } ( D _ { R ^ { \\prime } } \\parallel D _ { 2 } ) } & { } \\\\ { + \\mathrm { K L } ( D _ { R ^ { \\prime } } \\parallel D _ { 2 } ) - \\mathrm { K L } ( D _ { R ^ { \\prime } } \\parallel D _ { 3 } ) } & { } \\\\ { + \\ldots } & { } \\\\ { + \\mathrm { K L } ( D _ { R ^ { \\prime } } \\parallel D _ { R ^ { \\prime } - 1 } ) - \\mathrm { K L } ( D _ { R ^ { \\prime } } \\parallel D _ { R ^ { \\prime } } ) } & { } \\\\ { = \\displaystyle \\sum _ { r = 1 } ^ { R ^ { \\prime } - 1 } \\mathrm { K L } ( D _ { R ^ { \\prime } } \\parallel D _ { r } ) - \\mathrm { K L } ( D _ { R ^ { \\prime } } \\parallel D _ { r + 1 } ) . } & { } \\end{array}\n$$\n\nMoreover, given $r \\in \\{ 1 , \\ldots , R ^ { \\prime } - 1 \\}$ ,\n\n$$\n\\begin{array} { l } { \\displaystyle \\mathrm { K L } \\big ( \\boldsymbol { D } _ { R ^ { \\prime } } \\parallel \\boldsymbol { D } _ { r } \\big ) - \\mathrm { K L } \\big ( \\boldsymbol { D } _ { R ^ { \\prime } } \\parallel \\boldsymbol { D } _ { r + 1 } \\big ) = \\sum _ { i = 1 } ^ { m } \\boldsymbol { D } _ { R ^ { \\prime } } ( i ) \\ln \\frac { \\boldsymbol { D } _ { R ^ { \\prime } } ( i ) } { \\boldsymbol { D } _ { r } ( i ) } - \\sum _ { i = 1 } ^ { m } \\boldsymbol { D } _ { R ^ { \\prime } } ( i ) \\ln \\frac { \\boldsymbol { D } _ { R ^ { \\prime } } ( i ) } { \\boldsymbol { D } _ { r + 1 } ( i ) } } \\\\ { = \\sum _ { i = 1 } ^ { m } \\boldsymbol { D } _ { R ^ { \\prime } } ( i ) \\ln \\frac { \\boldsymbol { D } _ { r + 1 } ( i ) } { \\boldsymbol { D } _ { r } ( i ) } } \\\\ { = - \\ln \\mathbf { Z } _ { r } - \\sum _ { i = 1 } ^ { m } \\boldsymbol { D } _ { R ^ { \\prime } } ( i ) \\alpha _ { r } c ( x _ { i } ) \\mathbf { h } _ { r } ( x _ { i } ) . } \\end{array}\n$$\n\nAltogether, we obtain that\n\n$$\n\\operatorname { K L } ( \\pmb { D } _ { R ^ { \\prime } } \\parallel \\pmb { D } _ { 1 } ) = - \\ln \\prod _ { r = 1 } ^ { R ^ { \\prime } - 1 } \\mathbf { Z } _ { r } + \\sum _ { r = 1 } ^ { R ^ { \\prime } - 1 } \\alpha _ { r } \\sum _ { i = 1 } ^ { m } \\pmb { D } _ { R ^ { \\prime } } ( i ) c ( x _ { i } ) ( - \\mathbf h _ { r } ( x _ { i } ) ) .\n$$\n\nNow, if $\\mathrm { K L } ( D _ { R ^ { \\prime } } \\| D _ { 1 } )$ is small (at most $4 \\gamma ^ { 2 } R _ { , }$ ), Lemma 2.5 ensures that with sufficient probability there exists a $\\gamma / 2$ -approximation for $\\pmb { D } _ { R ^ { \\prime } }$ within $\\mathbf { T } _ { R ^ { \\prime } , 1 } , \\dotsc , \\mathbf { T } _ { R ^ { \\prime } , t / R }$ , yielding the induction step (by Claim 1). Otherwise, if $\\mathrm { K L } ( D _ { R ^ { \\prime } } \\parallel D _ { 1 } )$ is large, then either $( i )$ the term $\\begin{array} { r } { - \\ln \\prod _ { r = 1 } ^ { R ^ { \\prime } - 1 } \\mathbf { Z } _ { r } } \\end{array}$ is large enough for us to conclude that QrR=′−1 Zr is already less than exp(−γ2R′/2) and we can skip the step; or (ii) the term PrR=′−1 αr Pim=1 DR′ (i)c(xi)(−hr(xi)) is sufficiently large to imply the existence of ${ \\mathbf { h } } ^ { * } \\in \\{ - { \\mathbf { h } } _ { 1 } , \\ldots , - { \\mathbf { h } } _ { R ^ { \\prime } - 1 } \\}$ satisfying that\n\n$$\n\\sum _ { i = 1 } ^ { m } D _ { R ^ { \\prime } } ( i ) c ( x _ { i } ) \\mathbf { h } ^ { * } ( x _ { i } ) > \\gamma ,\n$$\n\nwhich implies that such $\\mathbf { h } ^ { * }$ has margin at least $\\gamma$ with respect to $\\pmb { D } _ { R ^ { \\prime } }$ and we can conclude the induction step as before. □\n\nWe defer the detailed proof to Appendix B.2.\n\n# 3 Overview of the Lower Bound\n\nIn this section, we overview of the main ideas behind our improved lower bound. The details are available in Appendix C. Our lower bound proof is inspired by, and builds upon, that of Lyu et al. [2024]. Let us first give the high level idea in their proof. Similarly to Karbasi and Larsen [2024], they consider an input domain ${ \\mathcal { X } } = [ 2 m ]$ , where $m$ denotes the number if training samples available for a weak-to-strong learner $\\mathcal { A }$ with parallel complexity $( p , t )$ . In their construction, they consider a uniform random concept $\\mathbf { c } \\colon { \\mathcal { X } } \\to { \\dot { \\{ - 1 , 1 \\} } }$ and give a randomized construction of a weak learner. Proving a lower bound on the expected error of $\\mathcal { A }$ under this random choice of concept and weak learner implies, by averaging, the existence of a deterministic choice of concept and weak learner for which $\\mathcal { A }$ has at least the same error.\n\nThe weak learner is constructed by drawing a random hypothesis set $\\varkappa$ , using inspiration from the so-called coin problem. In the coin problem, we observe $p$ independent outcomes of a biased coin and the goal is to determine the direction of the bias. If a coin has a bias of $\\beta$ , then upon seeing $n$ outcomes of the coin, any algorithm for guessing the bias of the coin is wrong with probability at least $\\exp ( - \\mathrm { O } ( \\beta ^ { 2 } n ) )$ . Now to connect this to parallel Boosting, Lyu et al. construct $\\varkappa$ by adding c as well as $p$ random hypotheses $\\mathbf { h } _ { 1 } , \\ldots , \\mathbf { h } _ { p }$ to $\\varkappa$ . Each hypothesis $\\mathbf { h } _ { i }$ has each ${ \\bf h } _ { i } ( x )$ chosen independently with $\\mathbf { h } _ { i } ( x ) = \\mathbf { c } ( x )$ with probability $1 / 2 + 2 \\gamma$ . The weak learner $\\boldsymbol { \\mathcal { W } }$ now processes a query distribution $D$ by returning the first hypothesis $\\mathbf { h } _ { i }$ with advantage $\\gamma$ under $D$ . If no such hypothesis exists, it instead returns $\\mathbf { c }$ . The key observation is that if $\\mathcal { W }$ is never forced to return c, then the only information $\\mathcal { A }$ has about $\\mathbf { c } ( x )$ for each $x$ not in the training data (which is at least half of all $x$ , since $| { \\mathcal { X } } | = 2 m )$ ), is the outcomes of up to $p$ coin tosses that are $2 \\gamma$ biased towards $\\mathbf { c } ( x )$ . Thus, the expected error becomes $\\exp ( - \\mathrm { O } ( \\gamma ^ { 2 } p ) )$ . For this to be smaller than $m ^ { - 0 . 0 1 }$ then requires $p = \\Omega ( \\gamma ^ { - 2 } \\dot { \\ln } m )$ as claimed in their lower bound.\n\nThe last step of their proof, is then to argue that $\\boldsymbol { \\mathcal { W } }$ rarely has to return c upon a query. The idea here is to show that in the $i$ th parallel round, $\\boldsymbol { \\mathcal { W } }$ can use $\\mathbf { h } _ { i }$ to answer all queries, provided that $t$ is small enough. This is done by observing that for any query distribution $D$ that is independent of $\\mathbf { h } _ { i }$ , the expected loss satisfies $\\bar { \\mathbb { E } } _ { { \\mathbf { h } } _ { i } } [ \\mathcal { L } _ { D } ( { \\mathbf { h } } _ { i } \\bar { ) } ] = 1 / 2 - \\bar { 2 } \\bar { \\gamma }$ due to the bias. Using inspiration from [Karbasi and Larsen, 2024], they then show that for sufficiently \"well-spread\" queries $D$ , the loss of $\\mathbf { h } _ { i }$ under $D$ is extremely well concentrated around its expectation (over the random choice of $\\mathbf { h } _ { i }$ ) and thus $\\mathbf { h } _ { i }$ may simultaneously answer all (up to) $t$ well-spread queries in round $i$ . To handle \"concentrated\" queries, i.e. query distribution with most of the weight on a few $x$ , they also use ideas from [Karbasi and Larsen, 2024] to argue that if we add $2 ^ { \\operatorname { O } ( d ) }$ uniform random hypotheses to $\\varkappa$ , then these may be used to answer all concentrated queries.\n\nNote that the proof crucially uses that $\\mathbf { h } _ { i }$ is independent of the queries in the ith round. Here the key idea is that if $\\boldsymbol { \\mathcal { W } }$ can answer all the queries in round $\\mathbf { \\chi } _ { i }$ using $\\mathbf { h } _ { i }$ , then $\\mathbf { h } _ { i + 1 } , \\ldots , \\mathbf { h } _ { p }$ are independent of any queries the weak-to-strong learner makes in round $i + 1$ .\n\nIn our improved lower bound, we observe that the expected error of $\\exp ( - \\mathrm { O } ( \\gamma ^ { 2 } p ) )$ is much larger than $m ^ { - 0 . 0 1 }$ for small $p$ . That is, the previous proof is in some sense showing something much too strong when trying to understand the tradeoff between $p$ and $t$ . What this gives us, is that we can afford to make the coins/hypotheses $\\mathbf { h } _ { i }$ much more biased towards $\\mathbf { c }$ when $p$ is small. Concretely, we can let the bias be as large as $\\beta = \\Theta ( \\sqrt { \\ln ( m ) / p } )$ , which may be much larger than $2 \\gamma$ . This in turns gives us that it is significantly more likely that $\\mathbf { h } _ { i }$ may answer an independently chosen query distribution $D$ . In this way, the same $\\mathbf { h } _ { i }$ may answer a much larger number of queries $t$ , resulting in a tight tradeoff between the parameters. As a second contribution, we also find a better way of analyzing this lower bound instance, improving one term in the lower bound on $t$ from $\\exp ( \\Omega ( d ) )$ to $\\mathrm { e x p } ( \\mathrm { e x p } ( d ) )$ . We refer the reader to the full proof for details.\n\n# 4 Conclusion\n\nIn this paper, we have addressed the parallelization of Boosting algorithms. By establishing both improved lower bounds and an essentially optimal algorithm, we have effectively closed the gap between theoretical lower bounds and performance guarantees across the entire tradeoff spectrum between the number of training rounds and the parallel work per round.\n\nGiven that, we believe future work may focus on better understanding the applicability of the theoretical tools developed here to other settings since some lemmas obtained seem quite general. They may aid, for example, in investigating to which extent the post-processing of hypotheses obtained in the bagging step can improve the complexity of parallel Boosting algorithms, which remains as an interesting research direction.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文研究的是Boosting算法的并行化问题，探讨了在训练轮数$p$和每轮并行工作量$t$之间的权衡。Boosting是一种强大的集成学习方法，但其固有的顺序性限制了其在并行计算环境中的应用。\\n> *   该问题的重要性在于，Boosting算法（如XGBoost、LightGBM）在许多学习任务中表现出色，但其顺序性限制了在大规模数据集上的训练效率，尤其是在需要大量迭代的情况下。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种新的并行Boosting算法，通过改进的上下界分析，几乎完全填补了理论下界与现有算法性能之间的差距。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **改进的上界：** 提出了一种新的并行Boosting算法，其性能在训练轮数（p）与每轮并行工作量（t）的权衡上优于现有算法，具体表现为在t的指数上改进了因子R。\\n> *   **更强的下界：** 证明了几乎匹配的上界和下界，表明所提出的算法在理论上是接近最优的。\\n> *   **理论闭合：** 通过上下界的匹配，几乎完全解决了并行Boosting的复杂度问题。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   通过并行化弱学习器的查询过程，同时利用KL散度来跟踪分布变化，从而在保持Boosting算法理论保证的同时提高并行效率。\\n> *   该方法有效的原因在于，它通过KL散度更精确地捕捉了分布变化，从而能够更高效地利用并行资源。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 先前的工作使用最大散度（max-divergence）来分析分布变化，这牺牲了全局信息，限制了算法的并行效率。\\n> *   **本文的改进：** 本文改用KL散度来分析分布变化，这提供了更全局的视角，从而能够更高效地利用并行资源。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> *   1. **初始化：** 从训练数据中初始化一个分布D1。\\n> *   2. **并行查询：** 在每一轮中，并行地从当前分布中采样t/R个子集，并使用弱学习器在这些子集上训练。\\n> *   3. **假设选择：** 从生成的假设中选择那些在当前分布上表现良好的假设。\\n> *   4. **分布更新：** 根据选择的假设更新分布，类似于AdaBoost的权重更新步骤。\\n> *   5. **重复：** 重复上述步骤p轮，最终输出一个线性分类器。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   AdaBoost\\n> *   Karbasi和Larsen [2024]提出的并行Boosting算法\\n> *   Lyu等人 [2024]提出的并行Boosting算法\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在训练轮数（p）与每轮并行工作量（t）的权衡上：** 本文方法在理论分析中展示了更优的权衡，具体表现为在t的指数上改进了因子R，优于Karbasi和Larsen [2024]和Lyu等人 [2024]的方法。\\n> *   **在样本复杂度上：** 本文方法的样本复杂度与AdaBoost相当，但在并行效率上显著优于AdaBoost。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n> **提取与格式化要求**\\n> *   Boosting算法 (Boosting Algorithm, N/A)\\n> *   并行计算 (Parallel Computing, N/A)\\n> *   弱到强学习 (Weak-to-Strong Learning, N/A)\\n> *   KL散度 (Kullback-Leibler Divergence, KL)\\n> *   样本复杂度 (Sample Complexity, N/A)\\n> *   理论下界 (Theoretical Lower Bound, N/A)\\n> *   机器学习 (Machine Learning, ML)\"\n}\n```"
}