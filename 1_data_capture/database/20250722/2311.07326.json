{
    "source": "Semantic Scholar",
    "arxiv_id": "2311.07326",
    "link": "https://arxiv.org/abs/2311.07326",
    "pdf_link": "https://arxiv.org/pdf/2311.07326.pdf",
    "title": "MetaSymNet: A Tree-like Symbol Network with Adaptive Architecture and Activation Functions",
    "authors": [
        "Yanjie Li",
        "Weijun Li",
        "Lina Yu",
        "Min Wu",
        "Jinyi Liu",
        "Wenqiang Li",
        "Meilan Hao",
        "Shu Wei",
        "Yusong Deng"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "2023-11-13",
    "venue": "Êú™ÊâæÂà∞ÂèëË°®‰ºöËÆÆ",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 2,
    "influential_citation_count": 0,
    "institutions": [
        "AnnLab, Institute of Semiconductor, Chinese Academy of Sciences",
        "School of Electronic, Electrical, and Communication Engineering, University of Chinese Academy of Sciences",
        "Zhongguancun Academy",
        "School of Integrated Circuits, University of Chinese Academy of Sciences"
    ],
    "paper_content": "# MetaSymNet: A Tree-like Symbol Network with Adaptive Architecture and Activation Functions\n\nYanjie $\\mathbf { L i } ^ { 1 , 2 , 3 }$ , Weijun $\\mathbf { L i } ^ { 1 , 2 , 3 , 4 * }$ , Lina $\\mathbf { Y } \\mathbf { u } ^ { 1 * }$ , Min ${ \\mathbf { W } } { \\mathbf { u } } ^ { 1 * }$ , Jingyi $\\mathbf { L i u } ^ { 1 }$ , Shu Wei1,2, Yusong Deng1,2 Meilan Hao1\n\n1AnnLab, Institute of Semiconductor, Chinese Academy of Sciences, Beijing, China 2 School of Electronic, Electrical, and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China 3 Zhongguancun Academy, Beijing, China 4School of Integrated Circuits, University of Chinese Academy of Sciences, Beijing China wjli $@$ semi.ac.cn, yulina $@$ semi.ac.cn, wumin $@$ semi.ac.cn\n\n# Abstract\n\nMathematical formulas are the language of communication between humans and nature. Discovering latent formulas from observed data is an important challenge in artificial intelligence, commonly known as symbolic regression(SR). The current mainstream SR algorithms regard SR as a combinatorial optimization problem and use Genetic Programming (GP) or Reinforcement Learning (RL) to solve the SR problem. These methods perform well on simple problems, but poorly on slightly more complex tasks. In addition, this class of algorithms ignores an important aspect: in SR tasks, symbols have explicit numerical meaning. So can we take full advantage of this important property and try to solve the SR problem with more efficient numerical optimization methods? The Equation Learner (EQL) replaces activation functions in neural networks with basic symbols and sparsifies connections to derive a simplified expression from a large network. However, EQL‚Äôs fixed network structure can not adapt to the complexity of different tasks, often resulting in redundancy or insufficient, limiting its effectiveness. Based on the above analysis, we propose MetaSymNet, a tree-like network that employs the PANGU meta-function as its activation function. PANGU meta-function can evolve into various candidate functions during training. The network structure can also be adaptively adjusted according to different tasks. Then the symbol network evolves into a concise, interpretable mathematical expression. To evaluate the performance of MetaSymNet and five baseline algorithms, we conducted experiments across more than ten datasets, including SRBench. The experimental results show that MetaSymNet has achieved relatively excellent results on various evaluation metrics.\n\n# Code ‚Äî https://github.com/1716757342/MetSymNet\n\n# Introduction\n\nDetermining a comprehensible and succinct mathematical expression from datasets remains a crucial challenge in artificial intelligence research. Symbolic regression endeavors to identify an interpretable equation $\\bar { Y } \\ = \\ F ( X )$ that precisely represents the relationship between the independent variable $X$ and the dependent variable $Y$ . Contemporary predominant approaches to symbolic regression often treat the problem as a combinatorial optimization challenge, wherein each mathematical operator is considered merely an ‚Äôaction‚Äô with no inherent mathematical significance. These methodologies generally employ Genetic Programming (GP) or reinforcement learning techniques. While effective for simpler tasks, their performance deteriorates when addressing complex symbolic regression challenges due to the exponential growth of the search space. On the other hand, in contrast to traditional combinatorial optimization tasks, symbols used in symbolic regression possess inherent mathematical meanings. Consider the classic Traveling Salesman Problem (TSP), where the elements‚Äîfour cities labeled [A, B, C, D]‚Äîlack additional implications beyond their identification. Conversely, in a symbolic regression scenario involving four symbols $[ + , \\times , \\sin , \\mathrm { x } ]$ , these are not merely labels. Each symbol carries a distinct mathematical significance. For instance, these symbols can function as activation functions within neural networks, facilitating processes such as forward or backpropagation.\n\nThe Equation Learner (EQL) modifies the architecture of the Multi-Layer Perceptron (MLP) by consistently incorporating a predetermined symbolic function as the activation function in each layer. Following this, a sparsification technique is applied to eliminate redundant connections within the network. This methodological framework allows EQL to derive a concise mathematical expression from complex data relationships. However, EQL has the following problems: 1) Sparsification Challenges: Achieving effective sparsification within the fully connected network proves to be more problematic than anticipated. It is often difficult to reduce the network to only one or two connections while maintaining high fitting accuracy solely through $L _ { 1 }$ regularization. As a result, many times the resulting expression does not fit the data well. 2) Fixed Network Structure: The network structure is fixed and cannot be adjusted according to the complexity of the tasks. It is easy to cause a mismatch between network structure and task. For instance, an excessively large initialized network may produce overly complex expressions, while a network that is too small may compro\n\nmise fitting performance.\n\nIn this paper, we introduce MetaSymNet, an innovative tree-based symbolic regression network. Its architecture is structured as an expression binary tree, featuring compact connections between nodes, which effectively eliminates the need for the sparsification of connection weights. Furthermore, we propose PANGU metafunctions and Variable metafunctions to serve as activation functions for internal nodes and leaf nodes, respectively. The PANGU function possesses the flexibility to evolve into any operational symbol during the training process, while Variable metafunctions enable the selection of the type of input variable. Notably, MetaSymNet is capable of adaptively modifying its structure, allowing it to grow or shrink in response to the complexity of the task at hand. Our contributions are summarized as follows:\n\n‚Ä¢ We introduce MetaSymNet, an innovative symbolic regression algorithm based on numerical optimization that features dynamic adaptation of node activation functions in response to specific task requirements. Additionally, MetaSymNet‚Äôs network architecture is capable of realtime adaptive adjustments driven by gradient information, optimizing its structure to align closely with the complexities of the tasks at hand.   \n‚Ä¢ We propose a novel activation function, PANGU metafunction, which can adaptively evolve into various candidate functions during the numerical optimization process.   \n‚Ä¢ We propose a structural adjustment mechanism that utilizes the difference in the number of inputs required by unary and binary functions. This approach allows MetaSymNet to modify its tree network structure in real time, guided by gradient, progressively refining its topology toward an optimal configuration.   \n‚Ä¢ We incorporate an entropy loss metric for each set of selected parameters into the loss function. This integration aims to augment MetaSymNet‚Äôs training efficiency and precision.\n\n# Related Work\n\nBased on Genetic algorithm The genetic algorithm (GA) (Mirjalili and Mirjalili 2019; Katoch, Chauhan, and Kumar 2021) is a classical algorithm that imitates biological evolution, and the algorithm that applies the GA algorithm to solve the problem of symbolic regression is the Genetic Programming (GP) (Espejo, Ventura, and Herrera 2009; Fortin et al. 2012; Augusto and Barbosa 2000) algorithm. GP represents each expression in the form of a binary tree, initializes an expression population, and then evolves a better population employing crossover, mutation, etc. Repeat the process until the termination condition is reached.\n\nBased on reinforcement learning Deep Symbolic Regression (DSR) (Petersen et al. 2019) is a very good algorithm for symbolic regression using reinforcement learning. DSR uses a recurrent neural network as the strategy network of the algorithm, which takes as input the parent and sibling nodes to be generated, and outputs as the probability of selecting each symbol. DSR uses risk policy gradients to refine policy network parameters. DSO (Mundhenk et al. 2021) introduces the GP algorithm based on DSR. SPL (Sun et al. 2022) successfully applies MCTS to solve the problem of symbolic regression. In this algorithm, the author uses MCTS to explore the symbolic space and puts forward a modular concept to improve search efficiency. DySymNet (Li et al. 2023) uses reinforcement learning to guide the generation of symbol networks. RSRM (Xu, Liu, and Sun 2023) deeply combines MCTS, Double Q-learning block, and GP, and achieves good performance on many datasets.\n\nBased on neural networks AI Feynman series algorithms are mainly divided into two versions, the main idea of this series of algorithms is to reduce complexity to simplicity. AI Feynman 1.0 (Udrescu and Tegmark 2020) first uses a neural network to fit the data and then uses the curve fitted by the neural network to analyze a series of properties in the data, such as symmetry and separability. Then the formula to be found is divided into simple units by these properties, and finally, the symbol of each unit is selected by random selection. The idea of AI Feynman 2.0 (Udrescu et al. 2020) and 1.0 is very similar, the biggest difference between the two is that version 2.0 introduces more properties so that the search expression can be divided into simpler units, improving the search efficiency. DGP (Zeng et al. 2023) works by normalizing the weight connections and then selecting the corresponding symbols. EQL (Martius and Lampert 2016; Kim et al. 2020) algorithm is an SR algorithm based on a neural network, which replaces the activation function in the fully connected neural network with basic operation symbols such as $[ + , - , . . . , s i n . . . ] ,$ , then removes the excess connections through pruning, and extracts an expression from the network.\n\nBased on Transformer The NeSymReS (Biggio et al. 2021) algorithm treats the symbolic regression problem as a translation problem in which the input $[ x , y ]$ , and the output is a preorder traversal of the expressions. NeSymReS first generates several expressions and then uses the sampled data $[ x , y ]$ from these expressions as inputs and the backbone of the expressions as outputs to train a transformer (Vaswani et al. 2017), pre-training model. When predicting the data, $[ x , y ]$ is entered into the transformer, and then combined with the beam search, a pre-order traversal of the formula is generated in turn. The biggest difference between the end-to-end (Kamienny et al. 2022) algorithm and NeSymReS is that the end-to-end approach can directly predict a constant to a specific value. Instead of predicting a constant placeholder $\\mathbf { \\cdot } _ { \\mathbf { C } } ,$ . (Shojaee et al. 2023) and (Kamienny et al. 2023), use the pre-trained model as a policy network to guide the search process of MCTS to improve the search efficiency. The SNIP (Meidani et al. 2023) uses contrastive learning to train the data feature extractor. Then, it trains a transformer to generate the expression skeleton in a self-supervised manner.\n\n# Methodology\n\nMetaSymNet, a tree-like neural network, treats each node as a neuron with internal nodes using the PANGU metafunction as their activation function, and leaf nodes employing Variable metafunctions. During training, in addition to optimizing amplitude parameters $\\boldsymbol { \\mathcal { W } }$ and bias term $b$ like ordi\n\n![](images/e8f5a8eb0f48987c8e103c5366a38c37fb2bde4601fc009b9cdc05ff5ed7ee94.jpg)  \nFigure 1: Flowchart of the MetaSymNet. (i) First randomly initialize a network, where the internal node $S$ is the PANGU meta-function and the leaf node $S _ { x }$ is the Variable meta-function. (ii) A numerical optimization algorithm is used to optimize the parameters. In this process, the amplitude parameter $\\boldsymbol { \\mathcal { W } }$ and the bias parameter $\\boldsymbol { B }$ are optimized first, and then the selection parameters $\\mathbb { Z }$ and $\\mathbb { D }$ of the PANGU meta-functions and Variable meta-functions are optimized. Iterate several times. (iii) After parameter optimization, we determine the basic candidate symbols to which each PANGU metafunction and Variable metafunction should evolve based on the selection parameters $\\mathbb { Z }$ and $\\mathbb { D }$ . (iv) When the network evolves into an expression, we further refine the constants of the expression and calculate the loss and $R ^ { 2 }$ . The iteration stops when $R ^ { 2 }$ reaches the specified threshold. Otherwise, We replace the internal nodes (operation symbols) of the obtained expression binary tree with PANGU metafunctions and leaf nodes (variable symbols) with Variable metafunctions. The network structure is then supplemented with Variable meta-functions such that each PANGU meta-function has two children. The iteration continues.\n\nnary fully connected neural networks, we also optimize the internal Selecting parameters $\\mathbb { Z } , \\mathbb { D }$ of the PANGU metafunctions and Variable metafunctions. The end-to-end training of MetaSymNet not only results in a high degree of data fit but also facilitates the evolution of the metafunctions into a variety of fundamental symbols, transforming the network into an analyzable and interpretable mathematical formula. MetaSymNet‚Äôs algorithm schematic is shown in Figure 1. See Appendix 1 for the pseudocode   \nWe begin by stochastically initializing a tree-like network whose architecture and neuronal count are arbitrarily defined. The activation functions of the network are the PANGU meta-functions and Variable metafunctions. In the beginning, each neuron has two inputs because our symbol library contains not only unary activation functions like $[ s i n , c o s , e x p , s q r t , l o g ]$ but also binary activation functions like $[ + , - , \\times , \\div ]$ , etc.\n\n# PANGU Meta-Function\n\nIn a standard neural network, the activation function is static, typically restricted to a single type such as ReLU or sigmoid. This makes neural networks ultimately a combination and nesting of multiple sets of activation functions and parameters, often a complex and hard-to-interpret ‚Äòblack box‚Äô. Mathematical formulas in natural science are composed of basic operation symbols such as sin() and cos(). Can we design a meta-activation function that can automatically evolve into various basic operators during training? To realize this, we design a PANGU meta-function, shown in Fig. 2. The\n\nformula is as follows Eq.1:\n\n$$\nO \\mathcal { U } \\mathcal { T } = \\boldsymbol { w } * O \\mathcal { E } ^ { T } + \\boldsymbol { b }\n$$\n\nHere, OUT is the corresponding output of the PANGU meta-function, $\\begin{array} { r l r } { O } & { { } = } & { \\left[ o _ { 1 } , o _ { 2 } , \\dots , o _ { n } \\right] \\quad = } \\end{array}$ $[ x _ { l } + x _ { r } , \\ldots , e ^ { x _ { l } } , x _ { 1 } , \\ldots , x _ { n } ]$ , and $o _ { i }$ is the output of the $i ^ { t h }$ candidate function in the library. The vector $\\begin{array} { r c l } { \\mathcal { Z } } & { = } & { \\left[ z _ { 1 } , z _ { 2 } , . . . , z _ { n } \\right] } \\end{array}$ is a set of selection parameters that can be optimized to control the probability of each activation function being selected. And all internal neuron function selection parameters $\\mathcal { Z }$ form the set $\\begin{array} { l l l } { \\mathbb { Z } } & { = } & { [ \\mathcal { Z } _ { 1 } , \\mathcal { Z } _ { 2 } , . . . , \\mathcal { Z } _ { n } ] } \\end{array}$ . Here $n$ denotes the number of internal neurons. $\\mathcal { E } = \\mathfrak { s o f t m a x } ( \\mathcal { Z } ) = [ e _ { 1 } , e _ { 2 } , \\ldots , e _ { n } ]$ (or $\\mathcal { E } = s o f t m a x ( \\mathcal { Z } - m a x ( \\mathcal { Z } ) )$ (Stevens et al. 2021; Dong, Zhu, and Ma 2019)), $\\begin{array} { r } { e _ { i } ~ = ~ \\frac { e ^ { z _ { i } } } { \\sum _ { j = 1 } ^ { n } e ^ { c * z _ { j } } } } \\end{array}$ is the $i ^ { t h }$ value in vector $\\mathcal { E }$ . And $e _ { i }$ can be the probability of selecting $i ^ { t h }$ candidate activation function.\n\n# Variables Meta-Function\n\nAt the leaf neurons of MetaSymNet, we will initialize a Variable meta-function, shown in Fig 2(bottom), but the function can only evolve into different variables $\\left[ x _ { 1 } , x _ { 2 } , . . . , x _ { n } \\right]$ . Each time, the top two most likely variables are selected. The specific evolution process is shown in the algorithm 2. Its expression is as follows 2:\n\n$$\nO \\mathcal { U } T = w * X \\mathcal { E } ^ { T } + b\n$$\n\n![](images/375e05f9e0d1f0b71f649e482ce33945d83d25d0936a6bf4ebbf9985e68fac7a.jpg)  \nFigure 2: The structure of PANGU metafunctions and Variable metafunctions. The figure depicts the detailed internal structure diagram of the PANGU metafunction (top) and Variable metafunction (bottom). Note: The variable metafunctions are chosen from two variables at a time.\n\nHere, X = [x1, x2, ..., xk], E = [ k e 1ec‚àódi , k e 2ec‚àódi ... k ednec‚àódi ], D = [d1, d2, ..., dk] is a set of variable selection parameters, $\\mathbf { k }$ is the number of variables in the task, and $q$ denotes the total number of leaf nodes. And $w$ is the amplitude control parameter and $\\boldsymbol { \\mathbf { b } }$ is the bias parameter. In MetaSymNet, the variable selection parameter $\\mathbb { D } =$ $\\left[ \\mathcal { D } _ { 1 } , \\mathcal { D } _ { 2 } , . . . , \\mathcal { D } _ { q } \\right]$ and the activation function selection parameter $\\mathbb { Z }$ are optimized together.\n\n# Parameters Optimization\n\nEach neuron is assigned three types of parameters, an amplitude constant $w$ , a bias constant $b$ , and a set of selection parameters $\\mathcal { Z } ~ = ~ [ z _ { 1 } , z _ { 2 } , . . . , z _ { n } ] ^ { \\phantom { \\dagger } }$ (The leaf neurons are $\\mathcal { D } = \\mathbf { \\bar { \\rho } } [ d _ { 1 } , d _ { 2 } , . . . , d _ { n } ] )$ . Here, the amplitude constants $w$ of all neurons form the parameter set $\\mathcal { W }$ . Similarly, all bias constants $b$ form the set $\\boldsymbol { B }$ , and selection parameters $\\mathcal { Z }$ for all internal neurons form the set $\\mathbb { Z }$ . All variable selection parameters form the set $\\mathbb { D }$ . We alternately optimize the above types of parameters with a numerical optimization algorithm (For example, SGD (Bottou 2012, 2010), BFGS (Dai 2002), L-BFGS (Liu and Nocedal 1989) etc ). First, the parameters in the set $W$ and $B$ are optimized, and then the parameters in the set $\\mathbb { Z }$ and $\\mathbb { D }$ are optimized, next the activation functions of internal neurons and the variables of the leaf neurons are selected. Finally, after optimizing the parameters alternately for several rounds, we extract a formula from the network and further optimize $W$ and $B$ to achieve higher accuracy.\n\n# Activation Function Selection\n\nThere are many types of candidate activation functions. This article contains four binary activation functions $[ + , - , \\times , \\div ]$ , five unary activation functions $\\left[ s i n , c o s , e x p , l o g , s q r t \\right]$ , and variables $[ x _ { 1 } , x _ { 2 } , . . . , x _ { k } ]$ .\n\nPANGU Meta-Function Selection Process, We initialize a PANGU meta-function with an optimizable selection parameter vector $\\mathcal { Z } ~ = ~ [ z _ { 1 } , z _ { 2 } , . . . , z _ { n } ]$ , where $n$ represents the class of candidate functions. We optimize $\\mathcal { Z }$ by numerical optimization algorithm, Then the optimized $\\mathcal { Z } _ { n e w }$ each element minus the $M a x ( Z _ { n e w } )$ , and then sent softmax to get $\\mathcal { E } = s o f t m a x ( \\mathcal { Z } _ { n e w } - M a x ( \\mathcal { Z } _ { n e w } ) ) = [ e _ { 1 } , e _ { 2 } , . . . , e _ { n } ]$ . Each PANGU meta-function $s$ has two inputs $x _ { l }$ and $\\scriptstyle x _ { r }$ (The output of the two child nodes), and then the candidate functions do the corresponding numerical operations respectively. We can obtain the vector $\\mathcal { O } = [ x _ { l } + \\mathbf { \\bar { \\chi } } _ { r } , x _ { l } - x _ { r } , \\mathbf { \\bar { \\chi } } _ { r } *$ $x _ { r } , x _ { l } / x _ { r } , s i n ( x _ { l } ) , c o s ( x _ { l } ) , e x p ( x _ { l } ) , \\bar { l } o g ( x _ { l } ) , s q r t ( x _ { l } ) , x _ { 1 } ,$ $x _ { 2 } , . . . , x _ { n } ]$ . Finally, we dot multiply the vectors $\\mathcal { E }$ and $\\mathcal { O }$ , then multiply by a constant $w$ , and add a bias term $b$ to get the final output. After optimizing the vector $\\mathcal { Z }$ multiple times in this way, we map $\\mathcal { Z }$ to one-hot (Rodr¬¥ƒ±guez et al. 2018) form and then dot multiply it with $O$ . The final selected activation function is the symbol corresponding to the index equal to 1 in one-hot (Figure 1 top left, the evolution of PANGU metafunction $S$ ). Complete the activation function selection\n\nVariable Meta-Function Selection Process, First, suppose that the output of the leaf neuron $S _ { x }$ is $\\boldsymbol { v } _ { i }$ after multiple optimizations. Here, $v _ { i } = s o f t m a x ( D _ { i } ) * X$ , $X =$ $\\left[ x _ { 1 } , x _ { 2 } , . . . , x _ { n } \\right]$ . Then the two variables with the highest probability are selected as $x _ { l } , x _ { r }$ according to $s o f t m a x ( D _ { i } )$ (Fig. 1 Top left, the first step in the evolution of the Variable metafunction $S _ { x }$ , selecting two variables (red and pink boxes) ). Finally, $x _ { l }$ and $\\scriptstyle x _ { r }$ are used as the input of each candidate activation function, and the $O _ { i } = [ x _ { l } + x _ { r } , x _ { l } -$ $x _ { r } , . . . , s i n ( x _ { l } ) , . . . , x 1 , . . . , x _ { n } ]$ is obtained, then we choose the symbol corresponding to the value closest to $\\boldsymbol { v } _ { i }$ in vector $O _ { i }$ as the new activation function of the leaf neuron (Figure 1 top left, the second step in the evolution of Variable metafunction $S _ { x }$ ). If the target expression is still not found, or $R ^ { 2 }$ is less than 0.9999. This process is repeated until a stopping condition is reached.\n\n# Network Structure Optimization\n\nThe network structure of traditional neural networks is fixed, which is very easy to cause network structure redundancy. Although there are many pruning methods to simplify the structure of neural networks by pruning unnecessary connections, it is not easy to extract a concise expression from a network (Hermundstad et al. 2011; Maksimenko et al. 2018). We propose an algorithm to optimize the MetaSymNet structure in real time under the guidance of gradient. From the above, we know that the activation function of MetaSymNet can be learned dynamically and each neuron has two inputs. We designed a method to realize the dynamic adjustment of the network structure when the activation function changes. The rules for adjusting the structure according to the activation function changes are as follows: (1), Structural growth\n\nTable 1: Comparison of the coefficient of determination $( R ^ { 2 } )$ between MetaSymNet and five baseline. Bold values indicate state-of-the-art (SOTA) performance. The confidence level is 0.95.   \n\n<html><body><table><tr><td rowspan=\"2\">Group</td><td rowspan=\"2\">Dataset</td><td colspan=\"6\">BASELINES</td></tr><tr><td>MetaSymNet</td><td>DSO</td><td>TPSR</td><td>SPL</td><td>NeSymReS</td><td>EQL</td></tr><tr><td rowspan=\"10\"></td><td>Nguyen</td><td>0.9999¬±0.000</td><td>0.9999 ¬±0.001</td><td>0.9948¬±0.003</td><td>0.9842¬±0.001</td><td>0.8468¬±0.002</td><td>0.9924¬±0.005</td></tr><tr><td>Keijzer</td><td>0.9992¬±0.001</td><td>0.9924¬±0.001</td><td>0.9828¬±0.002</td><td>0.8919¬±0.002</td><td>0.7992¬±0.002</td><td>0.9666¬±0.003</td></tr><tr><td>Korns</td><td>0.9999¬±0.001</td><td>0.9872¬±0.000</td><td>0.9325¬±0.004</td><td>0.8788¬±0.001</td><td>0.8011¬±0.001</td><td>0.9285¬±0.004</td></tr><tr><td></td><td>0.9994000</td><td></td><td></td><td></td><td></td><td>0.9466¬±0.003</td></tr><tr><td>Civeraote</td><td></td><td>0.97484003</td><td>0.93190.00</td><td>0.8942¬±0.002</td><td>0.94440.00</td><td>0.9037¬±0.005</td></tr><tr><td>Vladislavleva</td><td>0.9826¬±0.003</td><td>0.9963¬±0.004</td><td>0.9128¬±0.005</td><td>0.8433¬±0.004</td><td>0.6892¬±0.004</td><td>0.8926¬±0.004</td></tr><tr><td>R</td><td>0.9921¬±0.002</td><td>0.9744¬±0.003</td><td>0.9422¬±0.001</td><td>0.9122¬±0.003</td><td>0.8003¬±0.004</td><td>0.8637¬±0.005</td></tr><tr><td>Jin</td><td>0.9896¬±0.002</td><td>0.9916¬±0.002</td><td>0.9826¬±0.004</td><td>0.9211¬±0.002</td><td>0.8627¬±0.002</td><td>0.9677¬±0.004</td></tr><tr><td>Neat</td><td>0.9953¬±0.004</td><td>0.9827¬±0.003</td><td>0.9319¬±0.004</td><td>0.8828¬±0.003</td><td>0.7996¬±0.005</td><td>0.9631¬±0.004</td></tr><tr><td>Others</td><td>0.9984¬±0.001</td><td>0.9861¬±0.002</td><td>0.9667¬±0.003</td><td>0.9435¬±0.003</td><td>0.8226¬±0.002</td><td>0.9438¬±0.004</td></tr><tr><td rowspan=\"3\"></td><td>Feynman</td><td>0.9960¬±0.002</td><td>0.9610¬±0.003</td><td>0.8928¬±0.003</td><td>0.9284¬±0.003</td><td>0.7025¬±0.003</td><td>0.8725¬±0.005</td></tr><tr><td> Strogatz</td><td>0.9424¬±0.004</td><td>0.9313¬±0.002</td><td>0.8249¬±0.003</td><td>0.8411¬±0.002</td><td>0.6222¬±0.002</td><td>0.8844¬±0.005</td></tr><tr><td>Black-box</td><td>0.9302¬±0.003</td><td>0.9033¬±0.004</td><td>0.8753¬±0.003</td><td>0.9024¬±0.002</td><td>0.6825¬±0.003</td><td>0.7852¬±0.005</td></tr><tr><td></td><td>Average</td><td>0.9859</td><td>0.9749</td><td>0.9024</td><td>0.8997</td><td>0.7528</td><td>0.7852</td></tr></table></body></html>\n\n‚Ä¢ When $S _ { x }$ evolves into a unary activation function. At this point, $S _ { x }$ evolves into the chosen unary operator symbol, and, a variable (leaf node) is selected as an input, (For example, the green symbol $e x p$ in Fig. 1). Finally, in Figure 1 ‚ÄòCompletion structure‚Äô stage, the unary operation symbol is changed into PANGU meta-function $S$ , and two variable meta-functions $S _ { x }$ are added as child nodes. Realize the growth of network structure.\n\n‚Ä¢ When $S _ { x }$ evolves into a binary activation function(e.g. $^ +$ , \\*). At this point, $S _ { x }$ evolves into the chosen binary operator symbol, and two variable leaf nodes are selected as input. Finally, in Figure 1 ‚ÄòCompletion structure‚Äô stage, the binary operation symbol is changed into PANGU metafunction $S$ , and two variable meta-functions $S _ { x }$ are added as child nodes. Realize the growth of network structure.\n\n# (2), Structural reduction\n\n‚Ä¢ When $S$ evolves into a variable symbol (e.g. $\\mathbf { \\Psi } _ { x _ { 1 } , x _ { n } }$ ). At this point, $S$ evolves into the chosen variable symbol, and all child nodes following this node are clipped off. Finally, in Figure 1 ‚ÄòCompletion structure‚Äô stage, the variable symbol is changed into a Variable meta-function $S _ { x }$ . Realize the reduction of network structure.\n\n# Loss Function\n\nIn the process of MetaSymNet training, the loss function has a crucial position, because it directly determines the direction of neural network optimization. In this study, we introduce a new loss function that aims to further optimize the performance of MetaSymNet. For each PANGU meta-function, it has a selection parameter $\\mathcal { Z }$ , and $\\mathcal { Z }$ gets $\\mathcal { E }$ after passing through the softmax function, where $\\varepsilon =$ $[ e _ { 1 } , e _ { 2 } , . . . , e _ { n } ]$ . We introduce the entropy (Wehrl 1978; Re¬¥nyi 1961) of $\\mathcal { E }$ for all PANGU metafunctions as part of the loss. Our goal is for the largest element of each vector $\\mathcal { E }$ to be significantly higher than the others while maintaining a high fitting accuracy. We only have to choose one symbol for the PANGU meta function. To facilitate the evolution of the PANGU meta-function. Specifically, the expression for the loss function is as follows 3:\n\n$$\n\\begin{array} { l } { \\displaystyle \\mathcal { L } = L _ { M S E } + L _ { E n t r } } \\\\ { \\displaystyle \\ = \\frac { 1 } { \\mathcal { N } } \\sum _ { n = 1 } ^ { \\mathcal { N } } ( y _ { i } - \\hat { y } _ { i } ) ^ { 2 } - \\lambda \\frac { 1 } { \\mathcal { M } } \\sum _ { j = 1 } ^ { \\mathcal { M } } l o g ( m a x ( \\mathcal { E } _ { j } ) ) } \\end{array}\n$$\n\nWhere $\\mathcal { N }$ is the number of sample points, y is the true y value, and $\\hat { y }$ is the predicted y value. $\\mathcal { M }$ is the number of PANGU meta-functions under the current network, and $\\mathcal { E } _ { j }$ is the value of the selection parameter of the $j ^ { t h }$ PANGU meta function after passing softmax. $\\lambda$ is the entropy loss regulation coefficient.\n\n# Results\n\nIn order to test the performance of MetaSymNet, we compare the performance of our algorithm with five state-of-theart symbolic regression algorithms (DSO (Mundhenk et al. 2021), NeSymReS (Biggio et al. 2021), SPL (Sun et al. 2022), EQL (Martius and Lampert 2016), and TPSR (Shojaee et al. 2023)) on ten public datasets. Specific formula details are given in Table tables 10 to 14, in the appendix. Fig. 5 shows the Pareto plots(time and accuracy) of MetaSymNet and baselines(in SRBench) on black-box data and Feynman.\n\n![](images/d8db0cce17072c07a67d2f801dbb6d02ea952331d25bd039e04828d566df5775.jpg)  \nFigure 3: Analysis on performance. a demonstrates the trends of $R ^ { 2 }$ values between MetaSymNet and five baseline methods across varying noise levels. b Provides a $R ^ { 2 }$ -time Pareto plot of the various algorithms on individual datasets, showing optimization performance.\n\n# Fit Ability Test\n\nWhen doing comparative experiments, we strictly control the experimental variables and ensure that all the other conditions are the same except for the algorithm type to ensure the fairness and credibility of the experiment. Specifically, (1) At test time, on the same test expression, we sample $x _ { i }$ on the same interval for different algorithms. To ensure the consistency of the test data. See Appendix tables 10 to 14, for the specific sampling range of each expression. (2) For some of the ‚ÄòConstraints‚Äô used in our algorithm, we also use the same ‚ÄòConstraints‚Äô in other algorithms. To ensure that the performance of the individual algorithms is not affected by the difference in the ‚ÄòConstraints‚Äô. In the experiment, we use the coefficient of determination $( R ^ { 2 } )$ (Nagelkerke et al. 1991; Ozer 1985) to judge how well each algorithm fits the expression. The results are shown in Table 1, from which we see that MetSymNet achieves the performance of SOTA on multiple datasets. The expression for $R ^ { 2 }$ is given in the following: 2 = 1 PiN=1(y‚àíyÀÜ)2 ; N (y y)2 Where yÀÜ is the predicted y value and $\\overline { y }$ is the mean of the true y.\n\n# Noise Robustness Test\n\nIn the real world, data often contains noise and uncertainty. This noise can come from measurement errors, interference during data acquisition, or other unavoidable sources (Berglund, Hassmen, and Job 1996; Tam et al. 2008; Beall and Lowe 2007). The Noise robustness test can simulate the noise situation in the real environment, and help to evaluate the performance of the symbolic regression algorithm in the case of imperfect data, to help us understand its reliability and robustness in practical applications. These trials can guide the tuning and selection of the model, ensuring that it will work robustly in real scenarios. (Ziyadinov and Tereshonok 2022; Gao et al. 2020). We generate noisy data $y _ { n o i s e }$ in the following way. To simulate different levels of noise in the real world, we divide the noise into ten levels. First, the noisy data $D a t a _ { n o i s e }$ is obtained by randomly sampling on the interval $[ - \\mathcal { L } * S p a n , + \\mathcal { L } * S p a n ]$ . Here, $\\mathcal { L } \\ = \\ [ 0 . 0 0 . 0 . 0 1 . 0 . 0 2 , . . . , 0 . 1 ]$ ] is the level of noise. $S p a n = a b s | m a x ( y ) - m i n ( y ) |$ is the Span of $y . { \\ y } _ { n o i s e } =$ $y + D a t a _ { n o i s e }$ . We employed the datasets to assess the algorithm‚Äôs noise robustness. For each mathematical expression, we conducted 20 times at varying noise levels. Subsequently, we computed the $R ^ { 2 }$ between the curve derived from each trial and the original curve, serving as a metric to quantify noise resilience. The outcome was determined by averaging the results from the 20 trials. The average $R ^ { 2 }$ of both MetaSymNet and five symbolic regression baselines were compared across different noise levels, and the results are depicted in Figure 6a. The comprehensive analysis demonstrates that MetaSymNet exhibits superior noise immunity performance in contrast to the other five symbolic regression baselines.\n\n# Inference Time Test\n\nIn order to evaluate a symbolic regression algorithm, in addition to fitting ability and anti-noise ability, the inference speed of the algorithm is also an extremely important index. Therefore, in order to test the inference efficiency of MetaSymNet and the various baselines, we picked the part of the expression data in all test datasets where the various algorithms could achieve $R ^ { 2 } > 0 . 9 9$ . These selected expressions make up dataset A. We then test each expression in dataset A 10 times with each algorithm. We plot the $R ^ { 2 } - t i m e$ coordinate plot as shown in Fig 6, For this plot, each color corresponds to an algorithm, and each scatter corresponds to the average $R ^ { 2 }$ and average inference time for all expressions in one test dataset. The red dot indicates the center of gravity of each algorithm, and the closer it is to the bottom-right corner of the figure, the better the overall performance of the algorithm. The red star part represents our algorithm. From the figure, we can see that MetaSymNet achieves a good balance between efficiency and accuracy. In addition, Fig5 shows the graph of MetaSymNet compared with 20 other baselines under the SRBench standard. We believe that the reason for MetaSymNet‚Äôs efficiency is that the symbolic regression is changed from a combinatorial optimization problem to a numerical optimization problem while retaining the binary tree representation of the expression. As we know, in the current field of machine learning, the efficiency of numerical optimization algorithms is higher than that of reinforcement learning and evolutionary computation algorithms. Therefore, compared with the traditional symbolic regression algorithm, MetaSymNet is more efficient.\n\nTable 2: Comparison of the average number of symbols (complexity) of the resulting expressions between MetaSymNet and the other five baselines.   \n\n<html><body><table><tr><td>Dataset</td><td>MetaSymNet</td><td>DSO</td><td>SPL</td><td>TPSR</td><td>NeSymReS</td><td>EQL</td></tr><tr><td>Nguyen</td><td>16.5</td><td>20.3</td><td>26.0</td><td>16.0</td><td>18.2</td><td>22.4</td></tr><tr><td>Keijzer</td><td>18.0</td><td>18.4</td><td>28.4</td><td>20.6</td><td>21.3</td><td>20.8</td></tr><tr><td>Constant</td><td>24.4</td><td>26.6</td><td>33.5</td><td>22.9</td><td>24.1</td><td>32.9</td></tr><tr><td>Livermore</td><td>34.8</td><td>38.2</td><td>47.3</td><td>35.3</td><td>32.9</td><td>41.5</td></tr><tr><td>Vladislavleva</td><td>42.4</td><td>46.3</td><td>59.4</td><td>38.2</td><td>36.2</td><td>49.2</td></tr><tr><td>R</td><td>28.5</td><td>31.3</td><td>38.2</td><td>24.6</td><td>27.3</td><td>36.9</td></tr><tr><td>Jin</td><td>20.6</td><td>22.0</td><td>32.0</td><td>16.2</td><td>19.9</td><td>28.4</td></tr><tr><td>Others</td><td>28.3</td><td>33.2</td><td>39.5</td><td>29.5</td><td>32.2</td><td>37.4</td></tr><tr><td>Neat</td><td>19.5</td><td>22.7</td><td>28.7</td><td>16.4</td><td>20.6</td><td>27.2</td></tr><tr><td>Korns</td><td>25.8</td><td>26.8</td><td>32.4</td><td>22.5</td><td>23.5</td><td>32.4</td></tr><tr><td>Feynman</td><td>23.1</td><td>24.1</td><td>34.2</td><td>21.3</td><td>22.4</td><td>26.6</td></tr><tr><td>Strogatz</td><td>21.4</td><td>27.2</td><td>32.3</td><td>24.4</td><td>28.1</td><td>31.9</td></tr><tr><td>Black-box</td><td>25.5</td><td>32.9</td><td>39.2</td><td>29.3</td><td>33.9</td><td>35.3</td></tr><tr><td>Average</td><td>25.3</td><td>28.5</td><td>33.2</td><td>24.4</td><td>26.2</td><td>32.5</td></tr></table></body></html>\n\n# Result Complexity Test\n\nIn symbolic regression, our ultimate goal is to get a relatively concise expression to fit the observed data. If the resulting expression is too complex, then its interpretability is greatly reduced. Therefore, we set up the following experiment to compare the complexity (the number of nodes in the binary tree) of the resulting expressions obtained by each algorithm. we selected the expression in which all methods in the database can achieve $\\Dot { R } ^ { 2 } \\ > \\ 0 . 9 9 9$ as the test set to compare the complexity (number of nodes) of the expression obtained by different algorithms when $R ^ { 2 } ~ > ~ 0 . { \\dot { 9 } } 9 9 $ . Each expression was run 20 times and then averaged. The maximum length is set to 80 for all algorithms. The specific statistical results are shown in Table 2. As we can see from the table, MetaSymNet has the second-lowest average number of nodes after TPSR.\n\n# Ablation Experiments with Entropy Loss\n\nFor each PANGU metafunction, we have a set of selection $\\mathcal { Z }$ passes through softmax() to obtain $\\mathcal { E } = [ e _ { 1 } , e _ { 2 } , . . . , e _ { n } ]$ , here $\\textstyle \\sum _ { i = 1 } ^ { i = n } e _ { i } ~ = ~ 1$ . To improve the efficiency and performance of MetaSymNet, we introduce the entropy of $\\mathcal { E }$ of all PANGU meta-functions in the network as part of the loss function. Specifically $\\begin{array} { r } { L _ { E n t r } \\ = \\ \\frac { 1 } { \\mathcal { M } } \\sum _ { j = 1 } ^ { \\mathcal { M } } \\bar { l } o g ( m a x ( \\mathcal { E } _ { j } ) ) } \\end{array}$ here, $\\mathcal { M }$ denotes the number of PANGU metafunctions in the network. To demonstrate the effectiveness of entropy loss, we perform ablation experiments on it. The specific experimental results are shown in Table 3. We think that the introduction of entropy loss can promote the values in $\\mathcal { E }$ to appear as ‚Äòbig is bigger, small is smaller‚Äô. Makes One of its values significantly larger than the others, which is closer to the one-hot form. It promotes a more efficient and accurate evolution of the PANGU meta-function to different activation functions.\n\nTable 3: Ablation experiments on whether to introduce entropy loss in the loss function for MetaSymNet.   \n\n<html><body><table><tr><td>MetaSymNet</td><td colspan=\"3\">With entropy loss</td><td colspan=\"3\">Without entropy loss</td></tr><tr><td></td><td>R¬≤‚Üë</td><td></td><td>Nodes‚ÜìTime(s)‚Üì</td><td>R¬≤‚Üë</td><td></td><td>Nodes‚ÜìTime(s)‚Üì</td></tr><tr><td>Nguyen</td><td>0.9999</td><td>16.5</td><td>96</td><td>0.9998</td><td>18.7</td><td>118</td></tr><tr><td>Keijzer</td><td>0.9992</td><td>18.0</td><td>124</td><td>0.9977</td><td>23.8</td><td>135</td></tr><tr><td>Korns</td><td>0.9999</td><td>24.4</td><td>103</td><td>0.9924</td><td>27.3</td><td>132</td></tr><tr><td>Constant</td><td>0.9996</td><td>34.8</td><td>109</td><td>0.9872</td><td>38.3</td><td>124</td></tr><tr><td>Livermore</td><td>0.9924</td><td>42.4</td><td>104</td><td>0.9834</td><td>49.2</td><td>122</td></tr><tr><td>Vladislavleva</td><td>0.9826</td><td>28.5</td><td>122</td><td>0.9807</td><td>32.2</td><td>132</td></tr><tr><td>R</td><td>0.9921</td><td>20.6</td><td>100</td><td>0.9829</td><td>26.4</td><td>111</td></tr><tr><td>Jin</td><td>0.9896</td><td>28.3</td><td>112</td><td>0.9744</td><td>32.9</td><td>125</td></tr><tr><td>Neat</td><td>0.9953</td><td>19.5</td><td>104</td><td>0.9881</td><td>24.7</td><td>131</td></tr><tr><td>Others</td><td>0.9984</td><td>25.8</td><td>121</td><td>0.9904</td><td>31.2</td><td>136</td></tr><tr><td>Feynman</td><td>0.9960</td><td>23.1</td><td>128</td><td>0.9763</td><td>27.4</td><td>142</td></tr><tr><td>Strogatz</td><td>0.9424</td><td>21.4</td><td>132</td><td>0.9114</td><td>28.2</td><td>151</td></tr><tr><td>Black-box</td><td>0.9302</td><td>25.5</td><td>142</td><td>0.9006</td><td>26.3</td><td>164</td></tr><tr><td>Average</td><td>0.9859</td><td>25.3</td><td>115</td><td>0.9743</td><td>29.7</td><td>133</td></tr></table></body></html>\n\n# Discussion and Conclusion\n\nIn this paper, we propose MetaSymNet, which treats the SR as a numerical optimization problem rather than a combinatorial optimization problem. MetaSymNet‚Äôs structure is a tree-like network that is dynamically adjusted during training and can be expanded or reduced. Compared with the baselines, MetaSymNet has a better fitting ability, noise robustness, and complexity. We propose a PANGU metafunction as the activation function of MetaSymNet. The function can autonomously evolve into various candidate functions under the control of selection parameters. In addition, we present variable metafunctions that can be used to select variables. Furthermore, the final result of MetaSymNet is a concise, interpretable expression. This characteristic enhances the credibility of MetaSymNet and presents significant potential for application in fields that involve high-risk decision-making, such as finance, medicine, and law. In such domains, where decisions can profoundly impact people‚Äôs lives, people must understand and trust the algorithm‚Äôs decision-making process. Despite MetaSymNet yielding satisfactory results, it has its limitations. For instance, tuning certain hyperparameters, such as the $\\lambda$ in the loss function, proves to be challenging. Additionally, the method can occasionally become trapped in local optima, resulting in approximate rather than exact expressions. Next, we plan to alter the evolution process of PANGU metafunctions. Specifically, instead of relying on the greedy strategy for function selection, we intend to explore a variety of search methods, including beam search, Monte Carlo Tree Search, and others, to enhance the algorithm‚Äôs performance.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáËß£ÂÜ≥ÁöÑÊ†∏ÂøÉÈóÆÈ¢òÊòØÁ¨¶Âè∑ÂõûÂΩíÔºàSymbolic Regression, SRÔºâÔºåÂç≥‰ªéËßÇÊµãÊï∞ÊçÆ‰∏≠ÂèëÁé∞ÊΩúÂú®ÁöÑÊï∞Â≠¶ÂÖ¨Âºè„ÄÇÁé∞Êúâ‰∏ªÊµÅÊñπÊ≥ïÔºàÂ¶ÇÈÅó‰º†ÁºñÁ®ãÂíåÂº∫ÂåñÂ≠¶‰π†ÔºâÂ∞ÜSRËßÜ‰∏∫ÁªÑÂêà‰ºòÂåñÈóÆÈ¢òÔºåÂøΩÁï•‰∫ÜÁ¨¶Âè∑ÂÖ∑ÊúâÊòéÁ°ÆÊï∞Â≠¶ÊÑè‰πâÁöÑÈáçË¶ÅÁâπÊÄßÔºåÂØºËá¥Âú®Â§çÊùÇ‰ªªÂä°‰∏äË°®Áé∞‰∏ç‰Ω≥„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÂú®ÁßëÂ≠¶ÂèëÁé∞„ÄÅÂ∑•Á®ãÂª∫Ê®°Á≠âÈ¢ÜÂüüÂÖ∑ÊúâÂÖ≥ÈîÆ‰ª∑ÂÄºÔºåËÉΩÂ§üÊèê‰æõÂèØËß£ÈáäÁöÑÊï∞Â≠¶Ê®°ÂûãÔºåÂÖãÊúçÈªëÁÆ±Ê®°ÂûãÁöÑÂ±ÄÈôêÊÄß„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ËÆ∫ÊñáÊèêÂá∫MetaSymNetÔºå‰∏ÄÁßçÂü∫‰∫éÊï∞ÂÄº‰ºòÂåñÁöÑÊ†ëÁä∂Á¨¶Âè∑ÁΩëÁªúÔºåÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥ÁΩëÁªúÁªìÊûÑÂíåËá™ÈÄÇÂ∫îÊøÄÊ¥ªÂáΩÊï∞ÔºàPANGUÂÖÉÂáΩÊï∞ÔºâÊù•Ëß£ÂÜ≥SRÈóÆÈ¢ò„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **Âä®ÊÄÅÁΩëÁªúÁªìÊûÑÔºö** MetaSymNetËÉΩÂ§üÊ†πÊçÆ‰ªªÂä°Â§çÊùÇÂ∫¶ÂÆûÊó∂Ë∞ÉÊï¥ÁΩëÁªúÁªìÊûÑÔºåÈÅøÂÖç‰∫ÜÂõ∫ÂÆöÁªìÊûÑÁöÑÂÜó‰ΩôÊàñ‰∏çË∂≥„ÄÇÂÆûÈ™åÊòæÁ§∫ÂÖ∂Âú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑÂπ≥ÂùáR¬≤ËææÂà∞0.9859Ôºå‰ºò‰∫éÂü∫Á∫øÊ®°Âûã„ÄÇ\\n> *   **PANGUÂÖÉÂáΩÊï∞Ôºö** ÊèêÂá∫‰∏ÄÁßçÂèØÊºîÂåñ‰∏∫Â§öÁßçÂÄôÈÄâÂáΩÊï∞ÁöÑÊøÄÊ¥ªÂáΩÊï∞ÔºåÊèêÂçá‰∫ÜÁ¨¶Âè∑ÈÄâÊã©ÁöÑÁÅµÊ¥ªÊÄßÂíåÊïàÁéá„ÄÇÂú®Âô™Â£∞È≤ÅÊ£íÊÄßÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇ„ÄÇ\\n> *   **È´òÊïàÊï∞ÂÄº‰ºòÂåñÔºö** Â∞ÜSRÈóÆÈ¢òËΩ¨Âåñ‰∏∫Êï∞ÂÄº‰ºòÂåñÈóÆÈ¢òÔºåÊòæËëóÊèêÂçá‰∫ÜËÆ°ÁÆóÊïàÁéáÔºåÂπ≥ÂùáÊé®ÁêÜÊó∂Èó¥‰∏∫115ÁßíÔºå‰∏éËΩªÈáèÁ∫ßÊ®°ÂûãTPSRÁõ∏ÂΩì„ÄÇ\\n> *   **ÁÜµÊçüÂ§±ÂáΩÊï∞Ôºö** ÂºïÂÖ•ÁÜµÊçüÂ§±ÂáΩÊï∞ÊèêÂçáËÆ≠ÁªÉÊïàÁéáÔºåÂÆûÈ™åË°®ÊòéÂÖ∂ÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊî∂ÊïõÈÄüÂ∫¶ÂíåÊúÄÁªàÊÄßËÉΩÔºàËßÅË°®3Ôºâ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   MetaSymNetÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂ∞ÜÁ¨¶Âè∑ÂõûÂΩíÈóÆÈ¢òËΩ¨Âåñ‰∏∫Êï∞ÂÄº‰ºòÂåñÈóÆÈ¢òÔºåÈÄöËøáÊ†ëÁä∂ÁΩëÁªúÁªìÊûÑÂíåÂä®ÊÄÅÊøÄÊ¥ªÂáΩÊï∞ÔºàPANGUÂÖÉÂáΩÊï∞ÔºâÂÆûÁé∞È´òÊïàÁöÑÁ¨¶Âè∑ÂèëÁé∞„ÄÇÂÖ∂ËÆæËÆ°Âì≤Â≠¶ÊòØÂà©Áî®Á¨¶Âè∑ÁöÑÊï∞Â≠¶ÊÑè‰πâÔºåÈÄöËøáÊ¢ØÂ∫¶‰ø°ÊÅØÊåáÂØºÁΩëÁªúÁªìÊûÑË∞ÉÊï¥„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** ‰º†ÁªüÊñπÊ≥ïÔºàÂ¶ÇEQLÔºâ‰ΩøÁî®Âõ∫ÂÆöÁΩëÁªúÁªìÊûÑÂíåÁ®ÄÁñèÂåñÊäÄÊúØÔºåÈöæ‰ª•Âπ≥Ë°°Ë°®ËææËÉΩÂäõÂíåÁÆÄÊ¥ÅÊÄß„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** MetaSymNetÈÄöËøáÂä®ÊÄÅÁªìÊûÑË∞ÉÊï¥ÂíåPANGUÂÖÉÂáΩÊï∞ÔºåÂÆûÁé∞‰∫ÜÁΩëÁªúÁªìÊûÑÁöÑËá™ÈÄÇÂ∫î‰ºòÂåñÔºåÈÅøÂÖç‰∫ÜÂÜó‰ΩôËøûÊé•ÔºàËßÅË°®2Ôºâ„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.  ÈöèÊú∫ÂàùÂßãÂåñÊ†ëÁä∂ÁΩëÁªúÔºåÂÜÖÈÉ®ËäÇÁÇπ‰ΩøÁî®PANGUÂÖÉÂáΩÊï∞ÔºåÂè∂ËäÇÁÇπ‰ΩøÁî®VariableÂÖÉÂáΩÊï∞„ÄÇ\\n> 2.  ‰∫§Êõø‰ºòÂåñÂπÖÂ∫¶ÂèÇÊï∞ÔºàWÔºâ„ÄÅÂÅèÁΩÆÈ°πÔºàBÔºâÂíåÈÄâÊã©ÂèÇÊï∞ÔºàZ, DÔºâ„ÄÇ\\n> 3.  Ê†πÊçÆÈÄâÊã©ÂèÇÊï∞Á°ÆÂÆöPANGUÂÖÉÂáΩÊï∞ÂíåVariableÂÖÉÂáΩÊï∞ÁöÑÂÄôÈÄâÁ¨¶Âè∑„ÄÇ\\n> 4.  ÁΩëÁªúÁªìÊûÑÂä®ÊÄÅË∞ÉÊï¥ÔºöÊ†πÊçÆÊøÄÊ¥ªÂáΩÊï∞Á±ªÂûãÔºà‰∏ÄÂÖÉÊàñ‰∫åÂÖÉÔºâÂ¢ûÈïøÊàñÁº©ÂáèÁªìÊûÑÔºàËßÅÂõæ1Ôºâ„ÄÇ\\n> 5.  ÂºïÂÖ•ÁÜµÊçüÂ§±ÔºàEntropy LossÔºâÊèêÂçáËÆ≠ÁªÉÊïàÁéáÔºàËßÅË°®3Ôºâ„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   DSO„ÄÅNeSymReS„ÄÅSPL„ÄÅEQL„ÄÅTPSR„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®R¬≤ÊåáÊ†á‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®NguyenÊï∞ÊçÆÈõÜ‰∏äËææÂà∞‰∫Ü0.9999¬±0.000ÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãDSOÔºà0.9999¬±0.001ÔºâÂíåEQLÔºà0.9924¬±0.005Ôºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü0.0005‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®Âô™Â£∞È≤ÅÊ£íÊÄß‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®10Á∫ßÂô™Â£∞ÊµãËØï‰∏≠Âπ≥ÂùáR¬≤‰∏∫0.9859ÔºåËøúÈ´ò‰∫éÂü∫Á∫øÊ®°ÂûãEQLÔºà0.7852ÔºâÔºåÂ∞§ÂÖ∂Âú®Â§çÊùÇÊï∞ÊçÆÈõÜÔºàÂ¶ÇVladislavlevaÔºâ‰∏äË°®Áé∞Á™ÅÂá∫„ÄÇ\\n> *   **Âú®Êé®ÁêÜÈÄüÂ∫¶‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÁöÑÂπ≥ÂùáÊé®ÁêÜÊó∂Èó¥‰∏∫115ÁßíÔºå‰∏éËΩªÈáèÁ∫ßÊ®°ÂûãTPSRÔºà124ÁßíÔºâÁõ∏ÂΩìÔºå‰ΩÜÂú®R¬≤ÊåáÊ†á‰∏äËøúË∂ÖÂêéËÄÖÔºà0.9859 vs. 0.9024Ôºâ„ÄÇ\\n> *   **Âú®ÁªìÊûúÂ§çÊùÇÂ∫¶‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÁîüÊàêÁöÑË°®ËææÂºèÂπ≥ÂùáËäÇÁÇπÊï∞‰∏∫25.3ÔºåÊòæËëó‰Ωé‰∫éÂü∫Á∫øÊ®°ÂûãEQLÔºà32.5ÔºâÂíåSPLÔºà33.2ÔºâÔºå‰ªÖÁï•È´ò‰∫éTPSRÔºà24.4Ôºâ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   Á¨¶Âè∑ÂõûÂΩí (Symbolic Regression, SR)\\n*   Ê†ëÁä∂Á¨¶Âè∑ÁΩëÁªú (Tree-like Symbol Network, N/A)\\n*   PANGUÂÖÉÂáΩÊï∞ (PANGU Meta-Function, N/A)\\n*   Êï∞ÂÄº‰ºòÂåñ (Numerical Optimization, N/A)\\n*   Ëá™ÈÄÇÂ∫îÁΩëÁªúÁªìÊûÑ (Adaptive Network Architecture, N/A)\\n*   ÂèØËß£ÈáäÊ®°Âûã (Interpretable Model, N/A)\\n*   Âô™Â£∞È≤ÅÊ£íÊÄß (Noise Robustness, N/A)\\n*   Âä®ÊÄÅÁªìÊûÑË∞ÉÊï¥ (Dynamic Structure Adjustment, N/A)\"\n}\n```"
}