{
    "source": "Semantic Scholar",
    "arxiv_id": "2311.07326",
    "link": "https://arxiv.org/abs/2311.07326",
    "pdf_link": "https://arxiv.org/pdf/2311.07326.pdf",
    "title": "MetaSymNet: A Tree-like Symbol Network with Adaptive Architecture and Activation Functions",
    "authors": [
        "Yanjie Li",
        "Weijun Li",
        "Lina Yu",
        "Min Wu",
        "Jinyi Liu",
        "Wenqiang Li",
        "Meilan Hao",
        "Shu Wei",
        "Yusong Deng"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "2023-11-13",
    "venue": "未找到发表会议",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 2,
    "influential_citation_count": 0,
    "institutions": [
        "AnnLab, Institute of Semiconductor, Chinese Academy of Sciences",
        "School of Electronic, Electrical, and Communication Engineering, University of Chinese Academy of Sciences",
        "Zhongguancun Academy",
        "School of Integrated Circuits, University of Chinese Academy of Sciences"
    ],
    "paper_content": "# MetaSymNet: A Tree-like Symbol Network with Adaptive Architecture and Activation Functions\n\nYanjie $\\mathbf { L i } ^ { 1 , 2 , 3 }$ , Weijun $\\mathbf { L i } ^ { 1 , 2 , 3 , 4 * }$ , Lina $\\mathbf { Y } \\mathbf { u } ^ { 1 * }$ , Min ${ \\mathbf { W } } { \\mathbf { u } } ^ { 1 * }$ , Jingyi $\\mathbf { L i u } ^ { 1 }$ , Shu Wei1,2, Yusong Deng1,2 Meilan Hao1\n\n1AnnLab, Institute of Semiconductor, Chinese Academy of Sciences, Beijing, China 2 School of Electronic, Electrical, and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China 3 Zhongguancun Academy, Beijing, China 4School of Integrated Circuits, University of Chinese Academy of Sciences, Beijing China wjli $@$ semi.ac.cn, yulina $@$ semi.ac.cn, wumin $@$ semi.ac.cn\n\n# Abstract\n\nMathematical formulas are the language of communication between humans and nature. Discovering latent formulas from observed data is an important challenge in artificial intelligence, commonly known as symbolic regression(SR). The current mainstream SR algorithms regard SR as a combinatorial optimization problem and use Genetic Programming (GP) or Reinforcement Learning (RL) to solve the SR problem. These methods perform well on simple problems, but poorly on slightly more complex tasks. In addition, this class of algorithms ignores an important aspect: in SR tasks, symbols have explicit numerical meaning. So can we take full advantage of this important property and try to solve the SR problem with more efficient numerical optimization methods? The Equation Learner (EQL) replaces activation functions in neural networks with basic symbols and sparsifies connections to derive a simplified expression from a large network. However, EQL’s fixed network structure can not adapt to the complexity of different tasks, often resulting in redundancy or insufficient, limiting its effectiveness. Based on the above analysis, we propose MetaSymNet, a tree-like network that employs the PANGU meta-function as its activation function. PANGU meta-function can evolve into various candidate functions during training. The network structure can also be adaptively adjusted according to different tasks. Then the symbol network evolves into a concise, interpretable mathematical expression. To evaluate the performance of MetaSymNet and five baseline algorithms, we conducted experiments across more than ten datasets, including SRBench. The experimental results show that MetaSymNet has achieved relatively excellent results on various evaluation metrics.\n\n# Code — https://github.com/1716757342/MetSymNet\n\n# Introduction\n\nDetermining a comprehensible and succinct mathematical expression from datasets remains a crucial challenge in artificial intelligence research. Symbolic regression endeavors to identify an interpretable equation $\\bar { Y } \\ = \\ F ( X )$ that precisely represents the relationship between the independent variable $X$ and the dependent variable $Y$ . Contemporary predominant approaches to symbolic regression often treat the problem as a combinatorial optimization challenge, wherein each mathematical operator is considered merely an ’action’ with no inherent mathematical significance. These methodologies generally employ Genetic Programming (GP) or reinforcement learning techniques. While effective for simpler tasks, their performance deteriorates when addressing complex symbolic regression challenges due to the exponential growth of the search space. On the other hand, in contrast to traditional combinatorial optimization tasks, symbols used in symbolic regression possess inherent mathematical meanings. Consider the classic Traveling Salesman Problem (TSP), where the elements—four cities labeled [A, B, C, D]—lack additional implications beyond their identification. Conversely, in a symbolic regression scenario involving four symbols $[ + , \\times , \\sin , \\mathrm { x } ]$ , these are not merely labels. Each symbol carries a distinct mathematical significance. For instance, these symbols can function as activation functions within neural networks, facilitating processes such as forward or backpropagation.\n\nThe Equation Learner (EQL) modifies the architecture of the Multi-Layer Perceptron (MLP) by consistently incorporating a predetermined symbolic function as the activation function in each layer. Following this, a sparsification technique is applied to eliminate redundant connections within the network. This methodological framework allows EQL to derive a concise mathematical expression from complex data relationships. However, EQL has the following problems: 1) Sparsification Challenges: Achieving effective sparsification within the fully connected network proves to be more problematic than anticipated. It is often difficult to reduce the network to only one or two connections while maintaining high fitting accuracy solely through $L _ { 1 }$ regularization. As a result, many times the resulting expression does not fit the data well. 2) Fixed Network Structure: The network structure is fixed and cannot be adjusted according to the complexity of the tasks. It is easy to cause a mismatch between network structure and task. For instance, an excessively large initialized network may produce overly complex expressions, while a network that is too small may compro\n\nmise fitting performance.\n\nIn this paper, we introduce MetaSymNet, an innovative tree-based symbolic regression network. Its architecture is structured as an expression binary tree, featuring compact connections between nodes, which effectively eliminates the need for the sparsification of connection weights. Furthermore, we propose PANGU metafunctions and Variable metafunctions to serve as activation functions for internal nodes and leaf nodes, respectively. The PANGU function possesses the flexibility to evolve into any operational symbol during the training process, while Variable metafunctions enable the selection of the type of input variable. Notably, MetaSymNet is capable of adaptively modifying its structure, allowing it to grow or shrink in response to the complexity of the task at hand. Our contributions are summarized as follows:\n\n• We introduce MetaSymNet, an innovative symbolic regression algorithm based on numerical optimization that features dynamic adaptation of node activation functions in response to specific task requirements. Additionally, MetaSymNet’s network architecture is capable of realtime adaptive adjustments driven by gradient information, optimizing its structure to align closely with the complexities of the tasks at hand.   \n• We propose a novel activation function, PANGU metafunction, which can adaptively evolve into various candidate functions during the numerical optimization process.   \n• We propose a structural adjustment mechanism that utilizes the difference in the number of inputs required by unary and binary functions. This approach allows MetaSymNet to modify its tree network structure in real time, guided by gradient, progressively refining its topology toward an optimal configuration.   \n• We incorporate an entropy loss metric for each set of selected parameters into the loss function. This integration aims to augment MetaSymNet’s training efficiency and precision.\n\n# Related Work\n\nBased on Genetic algorithm The genetic algorithm (GA) (Mirjalili and Mirjalili 2019; Katoch, Chauhan, and Kumar 2021) is a classical algorithm that imitates biological evolution, and the algorithm that applies the GA algorithm to solve the problem of symbolic regression is the Genetic Programming (GP) (Espejo, Ventura, and Herrera 2009; Fortin et al. 2012; Augusto and Barbosa 2000) algorithm. GP represents each expression in the form of a binary tree, initializes an expression population, and then evolves a better population employing crossover, mutation, etc. Repeat the process until the termination condition is reached.\n\nBased on reinforcement learning Deep Symbolic Regression (DSR) (Petersen et al. 2019) is a very good algorithm for symbolic regression using reinforcement learning. DSR uses a recurrent neural network as the strategy network of the algorithm, which takes as input the parent and sibling nodes to be generated, and outputs as the probability of selecting each symbol. DSR uses risk policy gradients to refine policy network parameters. DSO (Mundhenk et al. 2021) introduces the GP algorithm based on DSR. SPL (Sun et al. 2022) successfully applies MCTS to solve the problem of symbolic regression. In this algorithm, the author uses MCTS to explore the symbolic space and puts forward a modular concept to improve search efficiency. DySymNet (Li et al. 2023) uses reinforcement learning to guide the generation of symbol networks. RSRM (Xu, Liu, and Sun 2023) deeply combines MCTS, Double Q-learning block, and GP, and achieves good performance on many datasets.\n\nBased on neural networks AI Feynman series algorithms are mainly divided into two versions, the main idea of this series of algorithms is to reduce complexity to simplicity. AI Feynman 1.0 (Udrescu and Tegmark 2020) first uses a neural network to fit the data and then uses the curve fitted by the neural network to analyze a series of properties in the data, such as symmetry and separability. Then the formula to be found is divided into simple units by these properties, and finally, the symbol of each unit is selected by random selection. The idea of AI Feynman 2.0 (Udrescu et al. 2020) and 1.0 is very similar, the biggest difference between the two is that version 2.0 introduces more properties so that the search expression can be divided into simpler units, improving the search efficiency. DGP (Zeng et al. 2023) works by normalizing the weight connections and then selecting the corresponding symbols. EQL (Martius and Lampert 2016; Kim et al. 2020) algorithm is an SR algorithm based on a neural network, which replaces the activation function in the fully connected neural network with basic operation symbols such as $[ + , - , . . . , s i n . . . ] ,$ , then removes the excess connections through pruning, and extracts an expression from the network.\n\nBased on Transformer The NeSymReS (Biggio et al. 2021) algorithm treats the symbolic regression problem as a translation problem in which the input $[ x , y ]$ , and the output is a preorder traversal of the expressions. NeSymReS first generates several expressions and then uses the sampled data $[ x , y ]$ from these expressions as inputs and the backbone of the expressions as outputs to train a transformer (Vaswani et al. 2017), pre-training model. When predicting the data, $[ x , y ]$ is entered into the transformer, and then combined with the beam search, a pre-order traversal of the formula is generated in turn. The biggest difference between the end-to-end (Kamienny et al. 2022) algorithm and NeSymReS is that the end-to-end approach can directly predict a constant to a specific value. Instead of predicting a constant placeholder $\\mathbf { \\cdot } _ { \\mathbf { C } } ,$ . (Shojaee et al. 2023) and (Kamienny et al. 2023), use the pre-trained model as a policy network to guide the search process of MCTS to improve the search efficiency. The SNIP (Meidani et al. 2023) uses contrastive learning to train the data feature extractor. Then, it trains a transformer to generate the expression skeleton in a self-supervised manner.\n\n# Methodology\n\nMetaSymNet, a tree-like neural network, treats each node as a neuron with internal nodes using the PANGU metafunction as their activation function, and leaf nodes employing Variable metafunctions. During training, in addition to optimizing amplitude parameters $\\boldsymbol { \\mathcal { W } }$ and bias term $b$ like ordi\n\n![](images/e8f5a8eb0f48987c8e103c5366a38c37fb2bde4601fc009b9cdc05ff5ed7ee94.jpg)  \nFigure 1: Flowchart of the MetaSymNet. (i) First randomly initialize a network, where the internal node $S$ is the PANGU meta-function and the leaf node $S _ { x }$ is the Variable meta-function. (ii) A numerical optimization algorithm is used to optimize the parameters. In this process, the amplitude parameter $\\boldsymbol { \\mathcal { W } }$ and the bias parameter $\\boldsymbol { B }$ are optimized first, and then the selection parameters $\\mathbb { Z }$ and $\\mathbb { D }$ of the PANGU meta-functions and Variable meta-functions are optimized. Iterate several times. (iii) After parameter optimization, we determine the basic candidate symbols to which each PANGU metafunction and Variable metafunction should evolve based on the selection parameters $\\mathbb { Z }$ and $\\mathbb { D }$ . (iv) When the network evolves into an expression, we further refine the constants of the expression and calculate the loss and $R ^ { 2 }$ . The iteration stops when $R ^ { 2 }$ reaches the specified threshold. Otherwise, We replace the internal nodes (operation symbols) of the obtained expression binary tree with PANGU metafunctions and leaf nodes (variable symbols) with Variable metafunctions. The network structure is then supplemented with Variable meta-functions such that each PANGU meta-function has two children. The iteration continues.\n\nnary fully connected neural networks, we also optimize the internal Selecting parameters $\\mathbb { Z } , \\mathbb { D }$ of the PANGU metafunctions and Variable metafunctions. The end-to-end training of MetaSymNet not only results in a high degree of data fit but also facilitates the evolution of the metafunctions into a variety of fundamental symbols, transforming the network into an analyzable and interpretable mathematical formula. MetaSymNet’s algorithm schematic is shown in Figure 1. See Appendix 1 for the pseudocode   \nWe begin by stochastically initializing a tree-like network whose architecture and neuronal count are arbitrarily defined. The activation functions of the network are the PANGU meta-functions and Variable metafunctions. In the beginning, each neuron has two inputs because our symbol library contains not only unary activation functions like $[ s i n , c o s , e x p , s q r t , l o g ]$ but also binary activation functions like $[ + , - , \\times , \\div ]$ , etc.\n\n# PANGU Meta-Function\n\nIn a standard neural network, the activation function is static, typically restricted to a single type such as ReLU or sigmoid. This makes neural networks ultimately a combination and nesting of multiple sets of activation functions and parameters, often a complex and hard-to-interpret ‘black box’. Mathematical formulas in natural science are composed of basic operation symbols such as sin() and cos(). Can we design a meta-activation function that can automatically evolve into various basic operators during training? To realize this, we design a PANGU meta-function, shown in Fig. 2. The\n\nformula is as follows Eq.1:\n\n$$\nO \\mathcal { U } \\mathcal { T } = \\boldsymbol { w } * O \\mathcal { E } ^ { T } + \\boldsymbol { b }\n$$\n\nHere, OUT is the corresponding output of the PANGU meta-function, $\\begin{array} { r l r } { O } & { { } = } & { \\left[ o _ { 1 } , o _ { 2 } , \\dots , o _ { n } \\right] \\quad = } \\end{array}$ $[ x _ { l } + x _ { r } , \\ldots , e ^ { x _ { l } } , x _ { 1 } , \\ldots , x _ { n } ]$ , and $o _ { i }$ is the output of the $i ^ { t h }$ candidate function in the library. The vector $\\begin{array} { r c l } { \\mathcal { Z } } & { = } & { \\left[ z _ { 1 } , z _ { 2 } , . . . , z _ { n } \\right] } \\end{array}$ is a set of selection parameters that can be optimized to control the probability of each activation function being selected. And all internal neuron function selection parameters $\\mathcal { Z }$ form the set $\\begin{array} { l l l } { \\mathbb { Z } } & { = } & { [ \\mathcal { Z } _ { 1 } , \\mathcal { Z } _ { 2 } , . . . , \\mathcal { Z } _ { n } ] } \\end{array}$ . Here $n$ denotes the number of internal neurons. $\\mathcal { E } = \\mathfrak { s o f t m a x } ( \\mathcal { Z } ) = [ e _ { 1 } , e _ { 2 } , \\ldots , e _ { n } ]$ (or $\\mathcal { E } = s o f t m a x ( \\mathcal { Z } - m a x ( \\mathcal { Z } ) )$ (Stevens et al. 2021; Dong, Zhu, and Ma 2019)), $\\begin{array} { r } { e _ { i } ~ = ~ \\frac { e ^ { z _ { i } } } { \\sum _ { j = 1 } ^ { n } e ^ { c * z _ { j } } } } \\end{array}$ is the $i ^ { t h }$ value in vector $\\mathcal { E }$ . And $e _ { i }$ can be the probability of selecting $i ^ { t h }$ candidate activation function.\n\n# Variables Meta-Function\n\nAt the leaf neurons of MetaSymNet, we will initialize a Variable meta-function, shown in Fig 2(bottom), but the function can only evolve into different variables $\\left[ x _ { 1 } , x _ { 2 } , . . . , x _ { n } \\right]$ . Each time, the top two most likely variables are selected. The specific evolution process is shown in the algorithm 2. Its expression is as follows 2:\n\n$$\nO \\mathcal { U } T = w * X \\mathcal { E } ^ { T } + b\n$$\n\n![](images/375e05f9e0d1f0b71f649e482ce33945d83d25d0936a6bf4ebbf9985e68fac7a.jpg)  \nFigure 2: The structure of PANGU metafunctions and Variable metafunctions. The figure depicts the detailed internal structure diagram of the PANGU metafunction (top) and Variable metafunction (bottom). Note: The variable metafunctions are chosen from two variables at a time.\n\nHere, X = [x1, x2, ..., xk], E = [ k e 1ec∗di , k e 2ec∗di ... k ednec∗di ], D = [d1, d2, ..., dk] is a set of variable selection parameters, $\\mathbf { k }$ is the number of variables in the task, and $q$ denotes the total number of leaf nodes. And $w$ is the amplitude control parameter and $\\boldsymbol { \\mathbf { b } }$ is the bias parameter. In MetaSymNet, the variable selection parameter $\\mathbb { D } =$ $\\left[ \\mathcal { D } _ { 1 } , \\mathcal { D } _ { 2 } , . . . , \\mathcal { D } _ { q } \\right]$ and the activation function selection parameter $\\mathbb { Z }$ are optimized together.\n\n# Parameters Optimization\n\nEach neuron is assigned three types of parameters, an amplitude constant $w$ , a bias constant $b$ , and a set of selection parameters $\\mathcal { Z } ~ = ~ [ z _ { 1 } , z _ { 2 } , . . . , z _ { n } ] ^ { \\phantom { \\dagger } }$ (The leaf neurons are $\\mathcal { D } = \\mathbf { \\bar { \\rho } } [ d _ { 1 } , d _ { 2 } , . . . , d _ { n } ] )$ . Here, the amplitude constants $w$ of all neurons form the parameter set $\\mathcal { W }$ . Similarly, all bias constants $b$ form the set $\\boldsymbol { B }$ , and selection parameters $\\mathcal { Z }$ for all internal neurons form the set $\\mathbb { Z }$ . All variable selection parameters form the set $\\mathbb { D }$ . We alternately optimize the above types of parameters with a numerical optimization algorithm (For example, SGD (Bottou 2012, 2010), BFGS (Dai 2002), L-BFGS (Liu and Nocedal 1989) etc ). First, the parameters in the set $W$ and $B$ are optimized, and then the parameters in the set $\\mathbb { Z }$ and $\\mathbb { D }$ are optimized, next the activation functions of internal neurons and the variables of the leaf neurons are selected. Finally, after optimizing the parameters alternately for several rounds, we extract a formula from the network and further optimize $W$ and $B$ to achieve higher accuracy.\n\n# Activation Function Selection\n\nThere are many types of candidate activation functions. This article contains four binary activation functions $[ + , - , \\times , \\div ]$ , five unary activation functions $\\left[ s i n , c o s , e x p , l o g , s q r t \\right]$ , and variables $[ x _ { 1 } , x _ { 2 } , . . . , x _ { k } ]$ .\n\nPANGU Meta-Function Selection Process, We initialize a PANGU meta-function with an optimizable selection parameter vector $\\mathcal { Z } ~ = ~ [ z _ { 1 } , z _ { 2 } , . . . , z _ { n } ]$ , where $n$ represents the class of candidate functions. We optimize $\\mathcal { Z }$ by numerical optimization algorithm, Then the optimized $\\mathcal { Z } _ { n e w }$ each element minus the $M a x ( Z _ { n e w } )$ , and then sent softmax to get $\\mathcal { E } = s o f t m a x ( \\mathcal { Z } _ { n e w } - M a x ( \\mathcal { Z } _ { n e w } ) ) = [ e _ { 1 } , e _ { 2 } , . . . , e _ { n } ]$ . Each PANGU meta-function $s$ has two inputs $x _ { l }$ and $\\scriptstyle x _ { r }$ (The output of the two child nodes), and then the candidate functions do the corresponding numerical operations respectively. We can obtain the vector $\\mathcal { O } = [ x _ { l } + \\mathbf { \\bar { \\chi } } _ { r } , x _ { l } - x _ { r } , \\mathbf { \\bar { \\chi } } _ { r } *$ $x _ { r } , x _ { l } / x _ { r } , s i n ( x _ { l } ) , c o s ( x _ { l } ) , e x p ( x _ { l } ) , \\bar { l } o g ( x _ { l } ) , s q r t ( x _ { l } ) , x _ { 1 } ,$ $x _ { 2 } , . . . , x _ { n } ]$ . Finally, we dot multiply the vectors $\\mathcal { E }$ and $\\mathcal { O }$ , then multiply by a constant $w$ , and add a bias term $b$ to get the final output. After optimizing the vector $\\mathcal { Z }$ multiple times in this way, we map $\\mathcal { Z }$ to one-hot (Rodr´ıguez et al. 2018) form and then dot multiply it with $O$ . The final selected activation function is the symbol corresponding to the index equal to 1 in one-hot (Figure 1 top left, the evolution of PANGU metafunction $S$ ). Complete the activation function selection\n\nVariable Meta-Function Selection Process, First, suppose that the output of the leaf neuron $S _ { x }$ is $\\boldsymbol { v } _ { i }$ after multiple optimizations. Here, $v _ { i } = s o f t m a x ( D _ { i } ) * X$ , $X =$ $\\left[ x _ { 1 } , x _ { 2 } , . . . , x _ { n } \\right]$ . Then the two variables with the highest probability are selected as $x _ { l } , x _ { r }$ according to $s o f t m a x ( D _ { i } )$ (Fig. 1 Top left, the first step in the evolution of the Variable metafunction $S _ { x }$ , selecting two variables (red and pink boxes) ). Finally, $x _ { l }$ and $\\scriptstyle x _ { r }$ are used as the input of each candidate activation function, and the $O _ { i } = [ x _ { l } + x _ { r } , x _ { l } -$ $x _ { r } , . . . , s i n ( x _ { l } ) , . . . , x 1 , . . . , x _ { n } ]$ is obtained, then we choose the symbol corresponding to the value closest to $\\boldsymbol { v } _ { i }$ in vector $O _ { i }$ as the new activation function of the leaf neuron (Figure 1 top left, the second step in the evolution of Variable metafunction $S _ { x }$ ). If the target expression is still not found, or $R ^ { 2 }$ is less than 0.9999. This process is repeated until a stopping condition is reached.\n\n# Network Structure Optimization\n\nThe network structure of traditional neural networks is fixed, which is very easy to cause network structure redundancy. Although there are many pruning methods to simplify the structure of neural networks by pruning unnecessary connections, it is not easy to extract a concise expression from a network (Hermundstad et al. 2011; Maksimenko et al. 2018). We propose an algorithm to optimize the MetaSymNet structure in real time under the guidance of gradient. From the above, we know that the activation function of MetaSymNet can be learned dynamically and each neuron has two inputs. We designed a method to realize the dynamic adjustment of the network structure when the activation function changes. The rules for adjusting the structure according to the activation function changes are as follows: (1), Structural growth\n\nTable 1: Comparison of the coefficient of determination $( R ^ { 2 } )$ between MetaSymNet and five baseline. Bold values indicate state-of-the-art (SOTA) performance. The confidence level is 0.95.   \n\n<html><body><table><tr><td rowspan=\"2\">Group</td><td rowspan=\"2\">Dataset</td><td colspan=\"6\">BASELINES</td></tr><tr><td>MetaSymNet</td><td>DSO</td><td>TPSR</td><td>SPL</td><td>NeSymReS</td><td>EQL</td></tr><tr><td rowspan=\"10\"></td><td>Nguyen</td><td>0.9999±0.000</td><td>0.9999 ±0.001</td><td>0.9948±0.003</td><td>0.9842±0.001</td><td>0.8468±0.002</td><td>0.9924±0.005</td></tr><tr><td>Keijzer</td><td>0.9992±0.001</td><td>0.9924±0.001</td><td>0.9828±0.002</td><td>0.8919±0.002</td><td>0.7992±0.002</td><td>0.9666±0.003</td></tr><tr><td>Korns</td><td>0.9999±0.001</td><td>0.9872±0.000</td><td>0.9325±0.004</td><td>0.8788±0.001</td><td>0.8011±0.001</td><td>0.9285±0.004</td></tr><tr><td></td><td>0.9994000</td><td></td><td></td><td></td><td></td><td>0.9466±0.003</td></tr><tr><td>Civeraote</td><td></td><td>0.97484003</td><td>0.93190.00</td><td>0.8942±0.002</td><td>0.94440.00</td><td>0.9037±0.005</td></tr><tr><td>Vladislavleva</td><td>0.9826±0.003</td><td>0.9963±0.004</td><td>0.9128±0.005</td><td>0.8433±0.004</td><td>0.6892±0.004</td><td>0.8926±0.004</td></tr><tr><td>R</td><td>0.9921±0.002</td><td>0.9744±0.003</td><td>0.9422±0.001</td><td>0.9122±0.003</td><td>0.8003±0.004</td><td>0.8637±0.005</td></tr><tr><td>Jin</td><td>0.9896±0.002</td><td>0.9916±0.002</td><td>0.9826±0.004</td><td>0.9211±0.002</td><td>0.8627±0.002</td><td>0.9677±0.004</td></tr><tr><td>Neat</td><td>0.9953±0.004</td><td>0.9827±0.003</td><td>0.9319±0.004</td><td>0.8828±0.003</td><td>0.7996±0.005</td><td>0.9631±0.004</td></tr><tr><td>Others</td><td>0.9984±0.001</td><td>0.9861±0.002</td><td>0.9667±0.003</td><td>0.9435±0.003</td><td>0.8226±0.002</td><td>0.9438±0.004</td></tr><tr><td rowspan=\"3\"></td><td>Feynman</td><td>0.9960±0.002</td><td>0.9610±0.003</td><td>0.8928±0.003</td><td>0.9284±0.003</td><td>0.7025±0.003</td><td>0.8725±0.005</td></tr><tr><td> Strogatz</td><td>0.9424±0.004</td><td>0.9313±0.002</td><td>0.8249±0.003</td><td>0.8411±0.002</td><td>0.6222±0.002</td><td>0.8844±0.005</td></tr><tr><td>Black-box</td><td>0.9302±0.003</td><td>0.9033±0.004</td><td>0.8753±0.003</td><td>0.9024±0.002</td><td>0.6825±0.003</td><td>0.7852±0.005</td></tr><tr><td></td><td>Average</td><td>0.9859</td><td>0.9749</td><td>0.9024</td><td>0.8997</td><td>0.7528</td><td>0.7852</td></tr></table></body></html>\n\n• When $S _ { x }$ evolves into a unary activation function. At this point, $S _ { x }$ evolves into the chosen unary operator symbol, and, a variable (leaf node) is selected as an input, (For example, the green symbol $e x p$ in Fig. 1). Finally, in Figure 1 ‘Completion structure’ stage, the unary operation symbol is changed into PANGU meta-function $S$ , and two variable meta-functions $S _ { x }$ are added as child nodes. Realize the growth of network structure.\n\n• When $S _ { x }$ evolves into a binary activation function(e.g. $^ +$ , \\*). At this point, $S _ { x }$ evolves into the chosen binary operator symbol, and two variable leaf nodes are selected as input. Finally, in Figure 1 ‘Completion structure’ stage, the binary operation symbol is changed into PANGU metafunction $S$ , and two variable meta-functions $S _ { x }$ are added as child nodes. Realize the growth of network structure.\n\n# (2), Structural reduction\n\n• When $S$ evolves into a variable symbol (e.g. $\\mathbf { \\Psi } _ { x _ { 1 } , x _ { n } }$ ). At this point, $S$ evolves into the chosen variable symbol, and all child nodes following this node are clipped off. Finally, in Figure 1 ‘Completion structure’ stage, the variable symbol is changed into a Variable meta-function $S _ { x }$ . Realize the reduction of network structure.\n\n# Loss Function\n\nIn the process of MetaSymNet training, the loss function has a crucial position, because it directly determines the direction of neural network optimization. In this study, we introduce a new loss function that aims to further optimize the performance of MetaSymNet. For each PANGU meta-function, it has a selection parameter $\\mathcal { Z }$ , and $\\mathcal { Z }$ gets $\\mathcal { E }$ after passing through the softmax function, where $\\varepsilon =$ $[ e _ { 1 } , e _ { 2 } , . . . , e _ { n } ]$ . We introduce the entropy (Wehrl 1978; Re´nyi 1961) of $\\mathcal { E }$ for all PANGU metafunctions as part of the loss. Our goal is for the largest element of each vector $\\mathcal { E }$ to be significantly higher than the others while maintaining a high fitting accuracy. We only have to choose one symbol for the PANGU meta function. To facilitate the evolution of the PANGU meta-function. Specifically, the expression for the loss function is as follows 3:\n\n$$\n\\begin{array} { l } { \\displaystyle \\mathcal { L } = L _ { M S E } + L _ { E n t r } } \\\\ { \\displaystyle \\ = \\frac { 1 } { \\mathcal { N } } \\sum _ { n = 1 } ^ { \\mathcal { N } } ( y _ { i } - \\hat { y } _ { i } ) ^ { 2 } - \\lambda \\frac { 1 } { \\mathcal { M } } \\sum _ { j = 1 } ^ { \\mathcal { M } } l o g ( m a x ( \\mathcal { E } _ { j } ) ) } \\end{array}\n$$\n\nWhere $\\mathcal { N }$ is the number of sample points, y is the true y value, and $\\hat { y }$ is the predicted y value. $\\mathcal { M }$ is the number of PANGU meta-functions under the current network, and $\\mathcal { E } _ { j }$ is the value of the selection parameter of the $j ^ { t h }$ PANGU meta function after passing softmax. $\\lambda$ is the entropy loss regulation coefficient.\n\n# Results\n\nIn order to test the performance of MetaSymNet, we compare the performance of our algorithm with five state-of-theart symbolic regression algorithms (DSO (Mundhenk et al. 2021), NeSymReS (Biggio et al. 2021), SPL (Sun et al. 2022), EQL (Martius and Lampert 2016), and TPSR (Shojaee et al. 2023)) on ten public datasets. Specific formula details are given in Table tables 10 to 14, in the appendix. Fig. 5 shows the Pareto plots(time and accuracy) of MetaSymNet and baselines(in SRBench) on black-box data and Feynman.\n\n![](images/d8db0cce17072c07a67d2f801dbb6d02ea952331d25bd039e04828d566df5775.jpg)  \nFigure 3: Analysis on performance. a demonstrates the trends of $R ^ { 2 }$ values between MetaSymNet and five baseline methods across varying noise levels. b Provides a $R ^ { 2 }$ -time Pareto plot of the various algorithms on individual datasets, showing optimization performance.\n\n# Fit Ability Test\n\nWhen doing comparative experiments, we strictly control the experimental variables and ensure that all the other conditions are the same except for the algorithm type to ensure the fairness and credibility of the experiment. Specifically, (1) At test time, on the same test expression, we sample $x _ { i }$ on the same interval for different algorithms. To ensure the consistency of the test data. See Appendix tables 10 to 14, for the specific sampling range of each expression. (2) For some of the ‘Constraints’ used in our algorithm, we also use the same ‘Constraints’ in other algorithms. To ensure that the performance of the individual algorithms is not affected by the difference in the ‘Constraints’. In the experiment, we use the coefficient of determination $( R ^ { 2 } )$ (Nagelkerke et al. 1991; Ozer 1985) to judge how well each algorithm fits the expression. The results are shown in Table 1, from which we see that MetSymNet achieves the performance of SOTA on multiple datasets. The expression for $R ^ { 2 }$ is given in the following: 2 = 1 PiN=1(y−yˆ)2 ; N (y y)2 Where yˆ is the predicted y value and $\\overline { y }$ is the mean of the true y.\n\n# Noise Robustness Test\n\nIn the real world, data often contains noise and uncertainty. This noise can come from measurement errors, interference during data acquisition, or other unavoidable sources (Berglund, Hassmen, and Job 1996; Tam et al. 2008; Beall and Lowe 2007). The Noise robustness test can simulate the noise situation in the real environment, and help to evaluate the performance of the symbolic regression algorithm in the case of imperfect data, to help us understand its reliability and robustness in practical applications. These trials can guide the tuning and selection of the model, ensuring that it will work robustly in real scenarios. (Ziyadinov and Tereshonok 2022; Gao et al. 2020). We generate noisy data $y _ { n o i s e }$ in the following way. To simulate different levels of noise in the real world, we divide the noise into ten levels. First, the noisy data $D a t a _ { n o i s e }$ is obtained by randomly sampling on the interval $[ - \\mathcal { L } * S p a n , + \\mathcal { L } * S p a n ]$ . Here, $\\mathcal { L } \\ = \\ [ 0 . 0 0 . 0 . 0 1 . 0 . 0 2 , . . . , 0 . 1 ]$ ] is the level of noise. $S p a n = a b s | m a x ( y ) - m i n ( y ) |$ is the Span of $y . { \\ y } _ { n o i s e } =$ $y + D a t a _ { n o i s e }$ . We employed the datasets to assess the algorithm’s noise robustness. For each mathematical expression, we conducted 20 times at varying noise levels. Subsequently, we computed the $R ^ { 2 }$ between the curve derived from each trial and the original curve, serving as a metric to quantify noise resilience. The outcome was determined by averaging the results from the 20 trials. The average $R ^ { 2 }$ of both MetaSymNet and five symbolic regression baselines were compared across different noise levels, and the results are depicted in Figure 6a. The comprehensive analysis demonstrates that MetaSymNet exhibits superior noise immunity performance in contrast to the other five symbolic regression baselines.\n\n# Inference Time Test\n\nIn order to evaluate a symbolic regression algorithm, in addition to fitting ability and anti-noise ability, the inference speed of the algorithm is also an extremely important index. Therefore, in order to test the inference efficiency of MetaSymNet and the various baselines, we picked the part of the expression data in all test datasets where the various algorithms could achieve $R ^ { 2 } > 0 . 9 9$ . These selected expressions make up dataset A. We then test each expression in dataset A 10 times with each algorithm. We plot the $R ^ { 2 } - t i m e$ coordinate plot as shown in Fig 6, For this plot, each color corresponds to an algorithm, and each scatter corresponds to the average $R ^ { 2 }$ and average inference time for all expressions in one test dataset. The red dot indicates the center of gravity of each algorithm, and the closer it is to the bottom-right corner of the figure, the better the overall performance of the algorithm. The red star part represents our algorithm. From the figure, we can see that MetaSymNet achieves a good balance between efficiency and accuracy. In addition, Fig5 shows the graph of MetaSymNet compared with 20 other baselines under the SRBench standard. We believe that the reason for MetaSymNet’s efficiency is that the symbolic regression is changed from a combinatorial optimization problem to a numerical optimization problem while retaining the binary tree representation of the expression. As we know, in the current field of machine learning, the efficiency of numerical optimization algorithms is higher than that of reinforcement learning and evolutionary computation algorithms. Therefore, compared with the traditional symbolic regression algorithm, MetaSymNet is more efficient.\n\nTable 2: Comparison of the average number of symbols (complexity) of the resulting expressions between MetaSymNet and the other five baselines.   \n\n<html><body><table><tr><td>Dataset</td><td>MetaSymNet</td><td>DSO</td><td>SPL</td><td>TPSR</td><td>NeSymReS</td><td>EQL</td></tr><tr><td>Nguyen</td><td>16.5</td><td>20.3</td><td>26.0</td><td>16.0</td><td>18.2</td><td>22.4</td></tr><tr><td>Keijzer</td><td>18.0</td><td>18.4</td><td>28.4</td><td>20.6</td><td>21.3</td><td>20.8</td></tr><tr><td>Constant</td><td>24.4</td><td>26.6</td><td>33.5</td><td>22.9</td><td>24.1</td><td>32.9</td></tr><tr><td>Livermore</td><td>34.8</td><td>38.2</td><td>47.3</td><td>35.3</td><td>32.9</td><td>41.5</td></tr><tr><td>Vladislavleva</td><td>42.4</td><td>46.3</td><td>59.4</td><td>38.2</td><td>36.2</td><td>49.2</td></tr><tr><td>R</td><td>28.5</td><td>31.3</td><td>38.2</td><td>24.6</td><td>27.3</td><td>36.9</td></tr><tr><td>Jin</td><td>20.6</td><td>22.0</td><td>32.0</td><td>16.2</td><td>19.9</td><td>28.4</td></tr><tr><td>Others</td><td>28.3</td><td>33.2</td><td>39.5</td><td>29.5</td><td>32.2</td><td>37.4</td></tr><tr><td>Neat</td><td>19.5</td><td>22.7</td><td>28.7</td><td>16.4</td><td>20.6</td><td>27.2</td></tr><tr><td>Korns</td><td>25.8</td><td>26.8</td><td>32.4</td><td>22.5</td><td>23.5</td><td>32.4</td></tr><tr><td>Feynman</td><td>23.1</td><td>24.1</td><td>34.2</td><td>21.3</td><td>22.4</td><td>26.6</td></tr><tr><td>Strogatz</td><td>21.4</td><td>27.2</td><td>32.3</td><td>24.4</td><td>28.1</td><td>31.9</td></tr><tr><td>Black-box</td><td>25.5</td><td>32.9</td><td>39.2</td><td>29.3</td><td>33.9</td><td>35.3</td></tr><tr><td>Average</td><td>25.3</td><td>28.5</td><td>33.2</td><td>24.4</td><td>26.2</td><td>32.5</td></tr></table></body></html>\n\n# Result Complexity Test\n\nIn symbolic regression, our ultimate goal is to get a relatively concise expression to fit the observed data. If the resulting expression is too complex, then its interpretability is greatly reduced. Therefore, we set up the following experiment to compare the complexity (the number of nodes in the binary tree) of the resulting expressions obtained by each algorithm. we selected the expression in which all methods in the database can achieve $\\Dot { R } ^ { 2 } \\ > \\ 0 . 9 9 9$ as the test set to compare the complexity (number of nodes) of the expression obtained by different algorithms when $R ^ { 2 } ~ > ~ 0 . { \\dot { 9 } } 9 9 $ . Each expression was run 20 times and then averaged. The maximum length is set to 80 for all algorithms. The specific statistical results are shown in Table 2. As we can see from the table, MetaSymNet has the second-lowest average number of nodes after TPSR.\n\n# Ablation Experiments with Entropy Loss\n\nFor each PANGU metafunction, we have a set of selection $\\mathcal { Z }$ passes through softmax() to obtain $\\mathcal { E } = [ e _ { 1 } , e _ { 2 } , . . . , e _ { n } ]$ , here $\\textstyle \\sum _ { i = 1 } ^ { i = n } e _ { i } ~ = ~ 1$ . To improve the efficiency and performance of MetaSymNet, we introduce the entropy of $\\mathcal { E }$ of all PANGU meta-functions in the network as part of the loss function. Specifically $\\begin{array} { r } { L _ { E n t r } \\ = \\ \\frac { 1 } { \\mathcal { M } } \\sum _ { j = 1 } ^ { \\mathcal { M } } \\bar { l } o g ( m a x ( \\mathcal { E } _ { j } ) ) } \\end{array}$ here, $\\mathcal { M }$ denotes the number of PANGU metafunctions in the network. To demonstrate the effectiveness of entropy loss, we perform ablation experiments on it. The specific experimental results are shown in Table 3. We think that the introduction of entropy loss can promote the values in $\\mathcal { E }$ to appear as ‘big is bigger, small is smaller’. Makes One of its values significantly larger than the others, which is closer to the one-hot form. It promotes a more efficient and accurate evolution of the PANGU meta-function to different activation functions.\n\nTable 3: Ablation experiments on whether to introduce entropy loss in the loss function for MetaSymNet.   \n\n<html><body><table><tr><td>MetaSymNet</td><td colspan=\"3\">With entropy loss</td><td colspan=\"3\">Without entropy loss</td></tr><tr><td></td><td>R²↑</td><td></td><td>Nodes↓Time(s)↓</td><td>R²↑</td><td></td><td>Nodes↓Time(s)↓</td></tr><tr><td>Nguyen</td><td>0.9999</td><td>16.5</td><td>96</td><td>0.9998</td><td>18.7</td><td>118</td></tr><tr><td>Keijzer</td><td>0.9992</td><td>18.0</td><td>124</td><td>0.9977</td><td>23.8</td><td>135</td></tr><tr><td>Korns</td><td>0.9999</td><td>24.4</td><td>103</td><td>0.9924</td><td>27.3</td><td>132</td></tr><tr><td>Constant</td><td>0.9996</td><td>34.8</td><td>109</td><td>0.9872</td><td>38.3</td><td>124</td></tr><tr><td>Livermore</td><td>0.9924</td><td>42.4</td><td>104</td><td>0.9834</td><td>49.2</td><td>122</td></tr><tr><td>Vladislavleva</td><td>0.9826</td><td>28.5</td><td>122</td><td>0.9807</td><td>32.2</td><td>132</td></tr><tr><td>R</td><td>0.9921</td><td>20.6</td><td>100</td><td>0.9829</td><td>26.4</td><td>111</td></tr><tr><td>Jin</td><td>0.9896</td><td>28.3</td><td>112</td><td>0.9744</td><td>32.9</td><td>125</td></tr><tr><td>Neat</td><td>0.9953</td><td>19.5</td><td>104</td><td>0.9881</td><td>24.7</td><td>131</td></tr><tr><td>Others</td><td>0.9984</td><td>25.8</td><td>121</td><td>0.9904</td><td>31.2</td><td>136</td></tr><tr><td>Feynman</td><td>0.9960</td><td>23.1</td><td>128</td><td>0.9763</td><td>27.4</td><td>142</td></tr><tr><td>Strogatz</td><td>0.9424</td><td>21.4</td><td>132</td><td>0.9114</td><td>28.2</td><td>151</td></tr><tr><td>Black-box</td><td>0.9302</td><td>25.5</td><td>142</td><td>0.9006</td><td>26.3</td><td>164</td></tr><tr><td>Average</td><td>0.9859</td><td>25.3</td><td>115</td><td>0.9743</td><td>29.7</td><td>133</td></tr></table></body></html>\n\n# Discussion and Conclusion\n\nIn this paper, we propose MetaSymNet, which treats the SR as a numerical optimization problem rather than a combinatorial optimization problem. MetaSymNet’s structure is a tree-like network that is dynamically adjusted during training and can be expanded or reduced. Compared with the baselines, MetaSymNet has a better fitting ability, noise robustness, and complexity. We propose a PANGU metafunction as the activation function of MetaSymNet. The function can autonomously evolve into various candidate functions under the control of selection parameters. In addition, we present variable metafunctions that can be used to select variables. Furthermore, the final result of MetaSymNet is a concise, interpretable expression. This characteristic enhances the credibility of MetaSymNet and presents significant potential for application in fields that involve high-risk decision-making, such as finance, medicine, and law. In such domains, where decisions can profoundly impact people’s lives, people must understand and trust the algorithm’s decision-making process. Despite MetaSymNet yielding satisfactory results, it has its limitations. For instance, tuning certain hyperparameters, such as the $\\lambda$ in the loss function, proves to be challenging. Additionally, the method can occasionally become trapped in local optima, resulting in approximate rather than exact expressions. Next, we plan to alter the evolution process of PANGU metafunctions. Specifically, instead of relying on the greedy strategy for function selection, we intend to explore a variety of search methods, including beam search, Monte Carlo Tree Search, and others, to enhance the algorithm’s performance.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决的核心问题是符号回归（Symbolic Regression, SR），即从观测数据中发现潜在的数学公式。现有主流方法（如遗传编程和强化学习）将SR视为组合优化问题，忽略了符号具有明确数学意义的重要特性，导致在复杂任务上表现不佳。\\n> *   该问题在科学发现、工程建模等领域具有关键价值，能够提供可解释的数学模型，克服黑箱模型的局限性。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出MetaSymNet，一种基于数值优化的树状符号网络，通过动态调整网络结构和自适应激活函数（PANGU元函数）来解决SR问题。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **动态网络结构：** MetaSymNet能够根据任务复杂度实时调整网络结构，避免了固定结构的冗余或不足。实验显示其在多个数据集上的平均R²达到0.9859，优于基线模型。\\n> *   **PANGU元函数：** 提出一种可演化为多种候选函数的激活函数，提升了符号选择的灵活性和效率。在噪声鲁棒性测试中表现优异。\\n> *   **高效数值优化：** 将SR问题转化为数值优化问题，显著提升了计算效率，平均推理时间为115秒，与轻量级模型TPSR相当。\\n> *   **熵损失函数：** 引入熵损失函数提升训练效率，实验表明其显著提升了模型的收敛速度和最终性能（见表3）。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   MetaSymNet的核心思想是将符号回归问题转化为数值优化问题，通过树状网络结构和动态激活函数（PANGU元函数）实现高效的符号发现。其设计哲学是利用符号的数学意义，通过梯度信息指导网络结构调整。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 传统方法（如EQL）使用固定网络结构和稀疏化技术，难以平衡表达能力和简洁性。\\n> *   **本文的改进：** MetaSymNet通过动态结构调整和PANGU元函数，实现了网络结构的自适应优化，避免了冗余连接（见表2）。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  随机初始化树状网络，内部节点使用PANGU元函数，叶节点使用Variable元函数。\\n> 2.  交替优化幅度参数（W）、偏置项（B）和选择参数（Z, D）。\\n> 3.  根据选择参数确定PANGU元函数和Variable元函数的候选符号。\\n> 4.  网络结构动态调整：根据激活函数类型（一元或二元）增长或缩减结构（见图1）。\\n> 5.  引入熵损失（Entropy Loss）提升训练效率（见表3）。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   DSO、NeSymReS、SPL、EQL、TPSR。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在R²指标上：** 本文方法在Nguyen数据集上达到了0.9999±0.000，显著优于基线模型DSO（0.9999±0.001）和EQL（0.9924±0.005）。与表现最佳的基线相比，提升了0.0005个百分点。\\n> *   **在噪声鲁棒性上：** 本文方法在10级噪声测试中平均R²为0.9859，远高于基线模型EQL（0.7852），尤其在复杂数据集（如Vladislavleva）上表现突出。\\n> *   **在推理速度上：** 本文方法的平均推理时间为115秒，与轻量级模型TPSR（124秒）相当，但在R²指标上远超后者（0.9859 vs. 0.9024）。\\n> *   **在结果复杂度上：** 本文方法生成的表达式平均节点数为25.3，显著低于基线模型EQL（32.5）和SPL（33.2），仅略高于TPSR（24.4）。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   符号回归 (Symbolic Regression, SR)\\n*   树状符号网络 (Tree-like Symbol Network, N/A)\\n*   PANGU元函数 (PANGU Meta-Function, N/A)\\n*   数值优化 (Numerical Optimization, N/A)\\n*   自适应网络结构 (Adaptive Network Architecture, N/A)\\n*   可解释模型 (Interpretable Model, N/A)\\n*   噪声鲁棒性 (Noise Robustness, N/A)\\n*   动态结构调整 (Dynamic Structure Adjustment, N/A)\"\n}\n```"
}