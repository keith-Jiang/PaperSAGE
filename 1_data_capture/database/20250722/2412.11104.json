{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.11104",
    "link": "https://arxiv.org/abs/2412.11104",
    "pdf_link": "https://arxiv.org/pdf/2412.11104.pdf",
    "title": "ABC3: Active Bayesian Causal Inference with Cohn Criteria in Randomized Experiments",
    "authors": [
        "Taehun Cha",
        "Donghun Lee"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "2024-12-15",
    "venue": "arXiv.org",
    "fields_of_study": [
        "Computer Science",
        "Mathematics"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Korea University"
    ],
    "paper_content": "# ABC3: Active Bayesian Causal Inference with Cohn Criteria in Randomized Experiments\n\nTaehun Cha and Donghun Lee\\*\n\nKorea University cth127, holy @korea.ac.kr\n\n# Abstract\n\nIn causal inference, randomized experiment is a de facto method to overcome various theoretical issues in observational study. However, the experimental design requires expensive costs, so an efficient experimental design is necessary. We propose ABC3, a Bayesian active learning policy for causal inference. We show a policy minimizing an estimation error on conditional average treatment effect is equivalent to minimizing an integrated posterior variance, similar to Cohn criteria (Cohn, Ghahramani, and Jordan 1994). We theoretically prove ABC3 also minimizes an imbalance between the treatment and control groups and the type 1 error probability. Imbalance-minimizing characteristic is especially notable as several works have emphasized the importance of achieving balance. Through extensive experiments on real-world data sets, ABC3 achieves the highest efficiency, while empirically showing the theoretical results hold.\n\nCode ‚Äî https://github.com/AIML-K/ActiveBayesianCausal/ Technical appendix ‚Äî https://arxiv.org/abs/2412.11104\n\n# 1 Introduction\n\nThe major goal of causal inference is to estimate the treatment effect which is a relative effect on the treatment group compared to the control group. In randomized experiments, practitioners allocate treatment to subjects to estimate the treatment effect. Randomized experiments free practitioners from various theoretical problems prevailing in an observational study, e.g. unmeasured confounders. However, a randomized experiment is usually more expensive than an observational study, as a result, an efficient experiment design is desirable.\n\nTo achieve efficiency in randomized experiments, Efron (1971) first introduced a biased-coin design, and several works tried to minimize the estimation bias by achieving a balance between treatment and control groups (Atkinson 2014). Antognini and Zagoraiou (2011) extended this to covariate-adaptive design to achieve a balance, not only between treatment-control groups but also within sampling strata. Several recent works target the same goal using an adaptive Neyman allocation (Dai, Gradu, and Harshaw 2023) or Pigeonhole design (Zhao and Zhou 2024). However, these lines of work assume the experimental subjects are given, not actively choosable.\n\nActive learning is a framework where a practitioner can choose unlabeled data points and ask an oracle to label them (Settles 2009). We can further rationalize the experimental design by adopting an active learning framework. For example, internet companies can choose a member whom they implement the A/B test, by utilizing the personal information they have gathered. Pharmaceuticals can choose a subject based on their personal information after the applicants are gathered to save a budget.\n\nTo develop a sound active learning method for randomized experiments, (1) it should not violate the standard assumptions in causal inference. Also, (2) the method should achieve a balance between observation and control groups to make a sound conclusion. We introduce ABC3, a novel active learning policy for randomized experiments to remedy these issues. Using the Gaussian process, our policy targets minimizing the error of individual treatment effect estimation. Our contributions are three folds:\n\n‚Ä¢ We theoretically show minimizing the estimation error on individual treatment effect is equivalent to minimizing the integrated predictive variance in a Bayesian sense.   \n‚Ä¢ ABC3, the policy minimizing the variance, theoretically minimizes the imbalance between treatment and control groups and the type 1 error rate.   \n‚Ä¢ With extensive experiments, we empirically verify ABC3 outperforms other methods while showing theoretical properties hold.\n\nAfter examining related works in Section 2, we formalize our problem in Section 3. Then we introduce ABC3 and its theoretical properties in Section 4. We empirically verify the performance and properties of ABC3 in Section 5. Then we conclude our paper with several discussions and limitations in Section 6 and 7.\n\n# 2 Related Works\n\nThere are several works exploring an active learning policy for observational data (Sundin et al. (2019), Jesson et al. (2021), and Toth et al. (2022)). Especially, Sundin et al.\n\n(2019) proposed an active learning policy for decision making, when treatment-control groups are imbalanced. They theoretically showed the imbalance can result in a type S error, where a practitioner estimates the treatment effect with a different sign. Likewise, Shalit, Johansson, and Sontag (2017) showed that the generalization error is bounded by the imbalance when estimating the treatment effect. However, their work focused on obtaining a balanced representation from observational data, not a randomized experiment setting.\n\nFor randomized experiments, Deng, Pineau, and Murphy (2011) suggests an active learning policy sampling a point with the highest predictive variance which is similar to Mackay‚Äôs criteria (MacKay 1992). Zhu et al. (2024) also proposed Mackay‚Äôs criteria-like method under network interference structure. Song, Mak, and Wu (2023) suggested ACE which targets maximizing the covariance between observed and test data sets. We will compare the effectiveness of our policy with these policies.\n\nSample-constrained causal inference setting shares a similar goal with active learning: achieving lower estimation error with fewer samples. Addanki et al. (2022) and Ghadiri et al. (2023) proposed an efficient sampling and estimation method in a randomized experiment setting. Harshaw et al. (2023) suggested a sampling method balancing covariates. However, their work assumes a linear relationship between covariates and potential outcomes, which is vulnerable in real-world scenarios. We will also compare the effectiveness of these policies.\n\n# 3 Problem Formulation\n\nLet $X \\in \\mathcal { X } , Y \\in \\mathcal { Y }$ and $A \\in \\{ 0 , 1 \\}$ be random variables. $X$ is a covariate representing each subject, $Y$ is an outcome, and $A$ represents a binary treatment. Following the NeymanRubin causal model (Rubin 1974), additionally define $Y ^ { 0 }$ and $Y ^ { 1 }$ , potential outcomes for either control or treatment. Unlike the usual supervised learning settings, a practitioner can observe only one of $Y ^ { 0 }$ and $\\bar { Y } ^ { 1 }$ in a causal inference setting. $x , y , a , \\overset { \\cdot } { y } ^ { 0 }$ and $y ^ { 1 }$ denotes the realizations of each random variable.\n\nLet $D _ { \\Omega } = \\{ ( x _ { i } , y _ { i } ^ { 0 } , y _ { i } ^ { 1 } ) \\} _ { i = 1 } ^ { N }$ be a subject pool with covariate information $x _ { i }$ and potential outcomes $y _ { i } ^ { 0 } , y _ { i } ^ { 1 } \\in \\mathbb { R }$ . Let $D _ { t } ^ { 0 } \\ = \\ \\{ ( x _ { i } , y _ { i } ^ { 0 } ) \\} _ { i \\in I _ { t } ^ { 0 } }$ be an observed control group data set at time $t$ with an index set $I _ { t } ^ { 0 }$ . Likewise, define $D _ { t } ^ { 1 } = \\{ ( x _ { i } , y _ { i } ^ { 1 } ) \\} _ { i \\in I _ { t } ^ { 1 } }$ for a treatment group. Let $X _ { \\Omega } , X _ { t } ^ { 0 }$ and $X _ { t } ^ { 1 }$ be sets of $x \\mathbf { s }$ in each data set, $D _ { \\Omega } , D _ { t } ^ { 0 }$ and $D _ { t } ^ { 1 }$ .\n\nOur quantity of interest is the conditional average treatment effect $( C A T E )$ , $C A T E ( x ) \\ = \\ \\mathbb { E } \\left[ Y ^ { 1 } - Y ^ { 0 } | \\bar { | } X = x \\right]$ for each subject $x$ . Then we can train an estimator $C \\hat { A } \\hat { T } E _ { t } ( x ) ~ = ~ \\hat { y } _ { t } ^ { 1 } ( x ) - \\hat { y } _ { t } ^ { 0 } ( x )$ , where $\\hat { y } _ { t } ^ { a } \\mathbf { s }$ are regressors trained on each data set $D _ { t } ^ { a }$ , $\\dot { a } \\in \\{ 0 , 1 \\}$ .\n\nTo evaluate the trained estimator, we use a standard metric, expected precision in estimation of heterogeneous effect (PEHE, Hill (2011)),\n\n$$\n\\epsilon _ { P E H E } ( C \\hat { A } T E _ { t } ) = \\int _ { \\mathcal X } ( C \\hat { A } T E _ { t } ( x ) - C A T E ( x ) ) ^ { 2 } d \\mathbb { P } ( x ) ,\n$$\n\nwhere $\\mathbb { P }$ is a probability distribution over whole covariates. In the usual case, we can treat $\\mathbb { P }$ as a discrete distribution corresponding to the covariates in $D _ { \\Omega }$ . Then we can write $\\begin{array} { r } { \\mathbb { P } = \\frac { 1 } { N } \\Sigma _ { i = 1 } ^ { N } \\breve { \\delta _ { x _ { i } } } } \\end{array}$ , where $\\delta$ is Dirac-delta function.\n\nMeanwhile, in Bayesian statistics, we do not hypothesize the existence of population parameters, e.g. $C A T E ( x )$ . Instead, we update our belief with observed data points. To derive a Bayesian policy, we need to define the optimization target in the Bayesian sense.\n\n# Definition 3.1.\n\n$$\n\\epsilon _ { P E H E } ^ { \\Omega } ( C \\hat { A } \\hat { T } E _ { t } ) = \\int _ { \\mathcal X } ( C \\hat { A } \\hat { T } E _ { t } ( x ) - C \\hat { A } \\hat { T } E _ { \\Omega } ( x ) ) ^ { 2 } d \\mathbb { P } ( x )\n$$\n\nwhere $C \\hat { A } T E _ { \\Omega } ( x ) = \\hat { y } _ { \\Omega } ^ { 1 } ( x ) - \\hat { y } _ { \\Omega } ^ { 0 } ( x ) , \\hat { y } _ { \\Omega } ^ { a } \\mathbf { s }$ are regressors trained on whole data set $D _ { \\Omega }$ .\n\nIntuitively, $\\boldsymbol { C } \\boldsymbol { A } \\mathbf { \\hat { \\boldsymbol { T } } } \\boldsymbol { E } _ { \\Omega }$ represents the best oracle estimation observing all factual and counterfactual data points. At any $t$ , our active learning problem is to choose an optimal (yet unobserved) subject $x ^ { * }$ and its treatment $a ^ { * }$ which can achieve the lowest expected $\\epsilon _ { P E H E } ^ { \\Omega } ( C \\hat { A T } E _ { t + 1 } )$ , i.e.\n\n$$\n\\begin{array} { r l } & { x ^ { * } , a ^ { * } = \\arg \\operatorname* { m i n } _ { x _ { t + 1 } \\in X _ { \\Omega } \\setminus \\left( X _ { t } ^ { 1 } \\cup X _ { t } ^ { 0 } \\right) , a _ { t + 1 } \\in \\{ 0 , 1 \\} } } \\\\ & { \\quad \\quad \\quad \\mathbb { E } _ { t + 1 } \\left[ \\epsilon _ { P E H E } ^ { \\Omega } \\big ( C \\hat { A } T E _ { t + 1 } \\big ) \\right] , } \\end{array}\n$$\n\nwhere $\\mathbb { E } _ { t } \\left[ \\cdot \\right] = \\mathbb { E } \\left[ \\cdot | \\mathcal { F } _ { t } \\right]$ , $\\mathcal { F } _ { t }$ is a filtration at $t$ . If $x ^ { * } = x _ { j }$ and $\\boldsymbol { a } ^ { * } = \\boldsymbol { a }$ , we observe $y _ { j } ^ { a }$ and append it to the observed data set $D _ { t + 1 } ^ { a } = D _ { t } ^ { a } \\cup \\{ ( x _ { j } , y _ { j } ^ { a } ) \\}$ to be used at $t + 1$ .\n\nWhile optimizing the expected error, an active learning policy should not violate the standard assumptions for causal inference. A standard randomized experiment in causal inference requires several assumptions to identify the causal effects (Rubin 1974).\n\nAssumption 3.2. (Consistency) $Y = A Y ^ { 1 } + ( 1 - A ) Y ^ { 0 }$ Assumption 3.3. (Positivity) $\\mathbb { P } ( A = a | X = x ) > 0 , \\forall x$ Assumption 3.4. (Randomization) $( Y ^ { 0 } , Y ^ { 1 } ) \\bot \\bot A | X$\n\nAssumption 3.2. assumes that the observed outcome $Y$ under treatment $A$ is equivalent to its potential outcome $Y ^ { a }$ . Assumption 3.3. is required to well-define the conditional expectation. Assumption 3.4. implies that the treatment $A$ should be assigned independently from the potential outcome values. The assumptions guarantee an unbiased estimation of CATE, i.e. $C A \\dot { T } E ( x ) \\dot { = } \\mathbb { E } \\left[ Y ^ { 1 } - Y ^ { 0 } | X = x \\right] =$ $\\mathbb { E } \\left[ \\mathbb { E } \\left[ Y | A = 1 , X = x \\right] - \\mathbb { E } \\left[ Y | A = 0 , X = x \\right] \\right]$ .\n\nAssumption 3.4. is crucial in an active learning setting since several active learning algorithms consider $y$ values when querying $x$ and $a$ . For instance, Song, Mak, and $\\mathrm { w } _ { \\mathrm { u } }$ (2023) introduced ACE-UCB, a bandit-like policy that targets a subject with the highest individual treatment effect. However, ACE-UCB exploits previously observed outcomes when choosing a subject and treatment. As a result, ACEUCB can generate confounding between treatment and potential outcomes and may violate Assumption 3.4. So querying $x$ and $a$ without considering ys is crucial when adopting an active learning framework for a causal inference.\n\n# 4 Proposed Method\n\n# ABC3: Active Bayesian Causal Inference with Cohn Criteria\n\nGaussian process ( $\\mathcal G \\mathcal P$ , Rasmussen and Williams (2006)) is a non-parametric machine learning model based on Bayesian statistics. It is a powerful tool as it allows flexible function estimation depending on a pre-defined kernel function. Set priors on $f \\sim \\mathcal { G P } ( \\bar { m } ( x ) , \\bar { k ( x , x ^ { \\prime } ) } )$ , where $m ( x )$ is a mean prior and $k$ is a kernel function. Assume a data set $D =$ $\\mathbf { \\bar { \\{ ( } }  x _ { i } , y _ { i } ) \\} _ { i = 1 } ^ { N }$ with noisy observation $y _ { i } = Y ( x _ { i } ) + \\epsilon _ { i } , \\epsilon _ { i }$ ‚àº $\\mathcal { N } ( 0 , \\sigma _ { \\epsilon } ^ { 2 } )$ . We can obtain a posterior distribution of $f ( x ^ { * } )$ given data set $D$ by computing $f ( x ^ { * } ) | D$ as\n\n$$\n\\begin{array} { r l } & { f ( \\boldsymbol { x } ^ { * } ) | D \\sim \\mathcal { N } ( m ^ { \\prime } ( \\boldsymbol { x } ^ { * } ) , \\sigma ^ { 2 } ( \\boldsymbol { x } ^ { * } ) ) \\mathrm { ~ w h e r e } } \\\\ & { m ^ { \\prime } ( \\boldsymbol { x } ^ { * } ) = m ( \\boldsymbol { x } ^ { * } ) + \\mathbf { k } _ { * } ( \\boldsymbol { x } ^ { * } ) ^ { T } \\left[ \\mathbb { K } + \\sigma _ { \\epsilon } ^ { 2 } \\mathbf { I } \\right] ^ { - 1 } \\mathbf { y } , } \\\\ & { \\mathrm { c o v } ( \\boldsymbol { x } , \\boldsymbol { x } ^ { * } ) = k ( \\boldsymbol { x } , \\boldsymbol { x } ^ { * } ) - \\mathbf { k } _ { * } ( \\boldsymbol { x } ) ^ { T } \\left[ \\mathbb { K } + \\sigma _ { \\epsilon } ^ { 2 } \\mathbf { I } \\right] ^ { - 1 } \\mathbf { k } _ { * } ( \\boldsymbol { x } ^ { * } ) , } \\\\ & { \\sigma ^ { 2 } ( \\boldsymbol { x } ^ { * } ) = \\mathrm { c o v } ( \\boldsymbol { x } ^ { * } , \\boldsymbol { x } ^ { * } ) , } \\\\ & { \\mathbf { k } _ { * } ( \\boldsymbol { x } ^ { * } ) = \\left[ k ( \\boldsymbol { x } _ { i } , \\boldsymbol { x } ^ { * } ) \\right] _ { i = 1 } ^ { N } , \\mathbb { K } = \\left[ k ( \\boldsymbol { x } _ { i } , \\boldsymbol { x } _ { j } ) \\right] _ { i , j = 1 } ^ { N } , } \\\\ & { \\mathbf { y } = \\left[ \\boldsymbol { y } _ { i } \\right] _ { i = 1 } ^ { N } . } \\end{array}\n$$\n\nWe can observe that the posterior variance $\\sigma ^ { 2 } ( x ^ { * } )$ does not depend on y. Adopting the notations from above, we define $\\mathbf { k } _ { t , * } ^ { a } , \\mathbb { K } _ { t } ^ { a } , \\mathbf { y } _ { t } ^ { a }$ and $\\mathrm { c o v } _ { t } ^ { a }$ with observed data set $D _ { t } ^ { a }$ at time $t$ . We also assume zero-mean prior, i.e. $m ( x ) = 0 , \\forall x$ .\n\nThe following theorem identifies our active learning policy with only posterior variance terms.\n\nTheorem 4.1. Assume $| k ( x , x ^ { \\prime } ) | < \\infty$ and $| y _ { i } ^ { a } | < \\infty$ for all $x , x ^ { \\prime } \\in \\mathcal { X }$ and $a , i$ , as a result, $\\epsilon _ { P E H E } ^ { \\Omega } ( C \\hat { A { T } } E _ { t } ) < \\infty , \\forall t .$ . Let our estimator $C \\hat { A } \\hat { T } E _ { t } ( x ) ~ = ~ \\hat { y } _ { t } ^ { 1 } ( x ) - \\hat { y } _ { t } ^ { 0 } ( x )$ , where $\\hat { y } _ { t } ^ { a } ( x ) \\ = \\ \\mathbb { E } _ { t } \\left[ Y ^ { a } ( x ) \\right]$ is a mean posterior distribution of gaussian process $Y ^ { a }$ trained on data set $D _ { t } ^ { a }$ . Assume two Gaussian processes $Y ^ { 1 }$ and $Y ^ { 0 }$ are independent. Then\n\n$$\n\\begin{array} { r l } & { a r g \\ m i n _ { x _ { t + 1 } , a _ { t + 1 } } \\mathbb { E } _ { t + 1 } \\left[ \\displaystyle \\epsilon _ { P E H E } ^ { \\Omega } ( C \\hat { A } T E _ { t + 1 } ) \\right] = } \\\\ & { a r g \\ m i n _ { x _ { t + 1 } , a _ { t + 1 } } \\int _ { \\mathcal X } \\mathbb { V } _ { t + 1 } \\left[ Y ^ { 1 } ( x ) \\right] + \\mathbb { V } _ { t + 1 } \\left[ Y ^ { 0 } ( x ) \\right] d \\mathbb { P } ( x ) } \\end{array}\n$$\n\nProof of Theorem 4.1. is in Appendix A.1. Theorem 4.1. states that minimizing the error on CATE estimation is equivalent to minimizing the integrated posterior variance of the estimator. In active learning literature, the active learning policy minimizing the integrated predictive variance is called Active Learning Cohn (Cohn, Ghahramani, and Jordan (1994) and Gramacy (2020)). Seo et al. (2000) proposed Active Learning Cohn policy utilizing the Gaussian process in a usual supervised learning setting. We extend this line of work to causal inference and name our policy after his name,\n\n# ABC3: Active Bayesian Causal Inference with Cohn Criteria.\n\nComputing the inverse of all hypothetical covariance matrix $\\mathbb { K } _ { t + 1 } ^ { a }$ for all $x$ is computationally infeasible. However, we can efficiently find the minimizer as ${ \\mathbb K } _ { t } ^ { a }$ is a principal submatrix of Kta+1.\n\n# Proposition 4.2.\n\n$$\n\\begin{array} { r l } & { a r g m i n _ { x _ { t + 1 } , a _ { t + 1 } } \\displaystyle \\int _ { \\mathcal X } \\mathbb { V } _ { t + 1 } \\left[ Y ^ { 1 } ( \\boldsymbol { x } ) \\right] + \\mathbb { V } _ { t + 1 } \\left[ Y ^ { 0 } ( \\boldsymbol { x } ) \\right] d \\mathbb { P } ( \\boldsymbol { x } ) } \\\\ & { \\ = a r g m a x _ { x _ { t + 1 } , a } } \\end{array}\n$$\n\n$$\n\\begin{array} { r l r } & { } & { \\frac { \\int _ { \\mathcal { X } } \\Big [ ( \\tilde { \\bf k } _ { t + 1 } ^ { a } ) ^ { T } \\left[ \\mathbb { K } _ { t } ^ { a } + \\sigma _ { \\epsilon } ^ { 2 } { \\bf I } \\right] ^ { - 1 } { \\bf k } _ { t , * } ^ { a } ( x ) - k ( x _ { t + 1 } , x ) \\Big ] ^ { 2 } d \\mathbb { P } ( x ) } { k ( x _ { t + 1 } , x _ { t + 1 } ) + \\sigma _ { \\epsilon } ^ { 2 } - ( \\tilde { \\bf k } _ { t + 1 } ^ { a } ) ^ { T } [ \\mathbb { K } _ { t } ^ { a } + \\sigma _ { \\epsilon } ^ { 2 } { \\bf I } ] ^ { - 1 } \\tilde { \\bf k } _ { t + 1 } ^ { a } } } \\end{array}\n$$\n\nwhere kÀúta+1 = [k(xi, xt+1)]i Ia\n\nProof of Proposition 4.2. is in Appendix A.2. From the proposition, we successfully eliminate the dependency on $\\mathbb { K } _ { t + 1 } ^ { a }$ , and there is no need to compute an inverse matrix for every $x$ . We summarize our policy in Algorithm 1.\n\n# Algorithm 1: ABC3\n\nInput: Current time step $t$ , whole covariate set $X _ { \\Omega }$ , covari  \nates distribution $\\mathbb { P }$ , previous observations $X _ { t } ^ { 1 }$ and $X _ { t } ^ { 0 }$ , kernel   \n$k$ , noise parameter $\\sigma _ { \\epsilon }$   \nOutput: $x _ { t + 1 } , a _ { t + 1 }$ 1: $\\mathcal { V } _ { 0 } , \\mathcal { V } _ { 1 } = \\phi , \\phi$ 2: for $x \\in X _ { \\Omega } \\setminus ( X _ { t } ^ { 0 } \\bigcup X _ { t } ^ { 1 } )$ do   \n3: Compute $\\tilde { \\mathbf { k } } _ { t + 1 } ^ { 1 }$ anSd $\\tilde { \\mathbf { k } } _ { t + 1 } ^ { 0 }$ assuming $x _ { t + 1 } = x$   \n4: $v ^ { 0 }$ , v1 = Equation (1) for each $a \\in \\{ 0 , 1 \\}$ 5: $\\mathcal { V } _ { 0 } = \\mathcal { V } _ { 0 } \\bigcup \\{ v ^ { 0 } \\} , \\mathcal { V } _ { 1 } = \\mathcal { V } _ { 1 } \\bigcup \\{ v ^ { 1 } \\}$   \n6: end for   \n7: $: i , a _ { t + 1 } = \\arg \\operatorname* { m a x } ( \\mathcal { V } _ { 0 } | | \\mathcal { V } _ { 1 } )$   \n8: $x _ { t + 1 } = X _ { \\Omega } \\setminus \\left( X _ { t } ^ { 0 } \\bigcup X _ { t } ^ { 1 } \\right) [ i ]$   \n9: return xt+1, at+1\n\n# Theoretical Analysis\n\nBalancing Treatment-Control Groups Unlike usual supervised learning, causal inference requires precise estimation of both functions for treatment and control groups. As a result, the balance between the two groups is crucial to obtain a sound estimation. For example, consider a study on the causal effect of online lectures. Assume our treatment group is concentrated on undergraduate students while our control group is concentrated on graduate students. Then it would be difficult to make a sound conclusion with statistical tools, as the two groups are highly imbalanced.\n\nSeveral researchers theoretically analyzed the effect of the imbalance on causal inference. Shalit, Johansson, and Sontag (2017) showed that the generalization error on CATE is upper bounded by the imbalance. Sundin et al. (2019) defined Type S Error, assigning a different sign ( $\\cdot +$ or -) to CATE, and showed the probability of Type S Error is bounded by the imbalance. Both works utilized Maximum Mean Discrepancy (MMD, Gretton et al. (2012)) to quantify and measure the imbalance.\n\n# Definition 4.3.\n\n$$\n\\begin{array} { r } { \\mathbf { M M D } ( P , Q , \\mathcal { F } ) = \\qquad } \\\\ { \\operatorname* { s u p } _ { f \\in \\mathcal { F } } \\mathbb { E } _ { x \\sim P ( x ) } \\left[ f ( x ) \\right] - \\mathbb { E } _ { y \\sim Q ( y ) } \\left[ f ( y ) \\right] } \\end{array}\n$$\n\nwhere $\\mathcal { F }$ is a unit ball of Reproducing Kernel Hilbert Space (RKHS) induced by a kernel $k ( \\cdot , \\cdot )$ .\n\nGretton et al. (2012) showed that the MMD is equivalent to the distance between mean embeddings in RKHS, $\\mu _ { P }$ and $\\mu _ { Q }$ , and can be computed with the kernel function $k$ . Remark 4.4.\n\n$$\n\\begin{array} { r l } & { \\mathbf { M D } ( P , Q , \\mathcal { F } ) ^ { 2 } = | | \\mu _ { P } - \\mu _ { Q } | | ^ { 2 } } \\\\ & { \\qquad = \\operatorname { \\mathbb { E } } _ { x , x ^ { \\prime } \\sim P ( x ) } \\left[ k ( x , x ^ { \\prime } ) \\right] + \\operatorname { \\mathbb { E } } _ { y , y ^ { \\prime } \\sim Q ( y ) } \\left[ k ( y , y ^ { \\prime } ) \\right] } \\\\ & { \\qquad - 2 \\operatorname { \\mathbb { E } } _ { x \\sim P ( x ) , y \\sim Q ( y ) } \\left[ k ( x , y ) \\right] } \\end{array}\n$$\n\nWe show our active learning policy approximately minimizes the upper bound of MMD.\n\nAssume the probability over whole covariates is a discrete distribution corresponding to the covariates in $D _ { \\Omega }$ , i.e. $\\begin{array} { r } { \\mathbb { P } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } \\delta _ { x _ { i } } } \\end{array}$ . For notational convenience, we assume the noise-less observation case $\\sigma _ { \\epsilon } ^ { 2 } = 0$ , but we can easily extend the result to the noisy case. Let $\\begin{array} { r } { \\mathbb { P } _ { t } ^ { a } \\ = \\ \\frac { 1 } { | I _ { t } ^ { a } | } { \\sum } _ { i \\in I _ { t } ^ { a } } \\dot { \\delta } _ { x _ { i } } , a \\ \\in } \\end{array}$ $\\{ 0 , 1 \\}$ be an empirical distribution for treatment-control group up to time $t$ . Then we obtain the following theorem.\n\nTheorem 4.5. Assume $| k ( x , x ^ { \\prime } ) | ~ < ~ \\infty$ and $| y _ { i } ^ { a } | ~ < ~ \\infty$ for all $x , x ^ { \\prime } \\in \\mathcal { X }$ and $a , i$ . Let $\\lambda ^ { * }$ be a maximum eigenvalue of $\\mathbb { K } _ { \\Omega }$ , i.e. covariance matrix of whole covariates. Let M = N1 2 Œ£iN,j=1k(xi, xj). Define functions $\\begin{array} { r } { \\delta ^ { * } ( I _ { n } ) \\ = \\ \\frac { 1 } { n } \\Sigma _ { i \\in I _ { n } } \\int _ { \\mathcal X } k ( x _ { i } , \\overset { \\cdot } { x } ) d \\mathbb { P } ( x ) } \\end{array}$ , and $\\epsilon ^ { * } ( I _ { n } ) = M -$ $\\textstyle { \\frac { 1 } { n ^ { 2 } } } \\sum _ { i , j \\in I _ { n } } k { \\big ( } x _ { i } , x _ { j } { \\big ) }$ ,RXwhere $I _ { n } \\subset \\{ 1 , . . . , N \\}$ is an $n$ -element index set. Assume $\\epsilon ^ { * } ( I _ { n } ) \\leq 2 \\delta ^ { * } ( I _ { n } ) , \\forall I _ { n }$ . Then\n\n$$\n\\begin{array} { r l r } {  { M M D ( \\mathbb { P } _ { t } ^ { 1 } , \\mathbb { P } _ { t } ^ { 0 } , \\mathcal { F } ) ^ { 2 } \\leq 4 \\frac { \\lambda ^ { * } } { | I _ { t } ^ { 1 } | } + 4 \\frac { \\lambda ^ { * } } { | I _ { t } ^ { 0 } | } } } \\\\ & { } & { + \\ 2 \\int _ { \\mathcal { X } } \\mathbb { V } _ { t } [ Y ^ { 1 } ( x ) ] + \\mathbb { V } _ { t } [ Y ^ { 0 } ( x ) ] d \\mathbb { P } ( x ) } \\end{array}\n$$\n\nProof of Theorem 4.5. is in Appendix A.3. We can observe that the first two terms decrease as we observe more subjects. The third term is our exact optimization target as introduced in Theorem 4.1. Empirical achievability and intuitive meaning of the assumption are covered in Section 5.6 and Appendix B.\n\nBy combining this result with the previous theoretical analysis (Shalit, Johansson, and Sontag (2017) and Sundin et al. (2019)), our active learning policy minimizes the upper bounds of both generalization error and type S error.\n\nType 1 Error Minimization The precise estimation of causal effect is an important object of causal inference. However, testing the existence of causal effect is another key component. R. A. Fisher first advocated the randomized experiment to test the existence of causal effects (Fisher 1970). He proposed Fisher‚Äôs sharp null hypothesis, where $y _ { i } ^ { 1 } = y _ { i } ^ { 0 } , \\bar { \\forall i } = 1 , . . . , N$ . A randomized experiment method should not reject the null hypothesis if it finds no causal effect. Type 1 error occurs when we reject a null hypothesis while it is true.\n\nAs active learning is a sequential procedure, a practitioner may want to test the statistical significance of the treatment effect sequentially. However, Ham et al. (2023) stated that applying statistical tests sequentially can result in a high type\n\n1 error probability. An active learning policy should avoid the type 1 error to make a sound conclusion.\n\nHere we present our theoretical result that our policy minimizes the upper bound of the type 1 error probability. First, we define the type 1 error.\n\nDefinition 4.6. (Type 1 Error at $t$ ) Under Fisher‚Äôs Sharp null hypothesis, as it implies $\\mathrm { C A T E } ( x ) = 0 , \\forall x .$ ,\n\n$$\n\\mathbb { P } _ { t } \\left[ { \\mathrm { T y p e ~ 1 ~ E r r o r } } ( x ) \\right] = \\mathbb { P } _ { t } \\left[ | Y ^ { 1 } ( x ) - Y ^ { 0 } ( x ) | > \\alpha \\right] ,\n$$\n\nwhere $\\alpha$ is a decision threshold.\n\nThen we can show that our policy minimizes the upper bound of the type 1 error rate over the whole $x$ . Proof of Theorem 4.7. is in Appendix A.4.\n\nTheorem 4.7. Under Fisher‚Äôs Sharp null hypothesis, $\\begin{array} { r l r l } { { 2 } r g \\ : m i n _ { x _ { t + 1 } , a _ { t + 1 } } \\int _ { \\mathcal { X } } \\mathbb { V } _ { t + 1 } \\left[ \\dot { Y } ^ { 1 } ( \\boldsymbol { x } ) \\right] } & { { } } & { } & { { } + } \\end{array}$ $\\mathbb { V } _ { t + 1 } \\left[ Y ^ { 0 } ( x ) \\right] d \\mathbb { P } ( x )$ also minimizes the upper bound of the $\\begin{array} { r } { \\int _ { \\mathcal { X } } \\mathbb { P } _ { t + 1 } \\left[ T y p e \\ I \\ E r r o r ( x ) \\right] d \\mathbb { P } ( x ) } \\end{array}$\n\n# 5 Experiments\n\nIn this section, we empirically analyze the theoretical results introduced in Section 4. For the comparison, we utilize IHDP (Brooks-Gunn, Liaw, and Klebanov (1992) and Hill (2011)), Boston (Harrison and Rubinfeld 1978), ACIC (Gruber et al. 2019), and Lalonde (LaLonde 1986) data sets. IHDP and ACIC data sets are semi-real data sets where counterfactual outcomes are simulated. Boston and Lalonde data sets do not contain counterfactual outcomes. Following Addanki et al. (2022), we set $Y ^ { 0 } = Y ^ { 1 }$ to simulate the null hypothesis circumstance (i.e. $C A T E ( x ) = 0 , \\forall x )$ . All data sets have continuous outcome values as we assume the potential outcome follows a Gaussian process. More detailed explanations of data sets are in Appendix C.\n\nWe randomly divide each data set in half for every trial to construct train and test data sets. Each active learning policy selects what to observe from the train data set and regressors are trained on the observed train data set. Policies are evaluated on the test data set for each pre-defined time step. We utilize the following baseline policies.\n\n‚Ä¢ Naive: This policy randomly selects $x _ { t + 1 }$ from the subject pool and then decides treatment $a _ { t + 1 }$ with Bernoulli distribution with probability 0.5.   \n‚Ä¢ Mackay (MacKay 1992): This policy selects $x _ { t + 1 }$ , $\\begin{array} { r l } { a _ { t + 1 } } & { { } = } \\end{array}$ arg $\\operatorname* { m a x } _ { x , a } \\mathbb { V } _ { t } \\left[ Y ^ { a } ( \\boldsymbol { x } ) \\right]$ , i.e. chooses the most uncertain point at $t$ .   \n‚Ä¢ Leverage (Addanki et al. 2022): Under linearity assumption between covariates and outcomes, this policy exploits leverage score and shows theoretical guarantees on nearly optimal RMSE on CATE estimation. Unlike other policies, this policy is not sequential, as a result, the observed points for $t$ and $t + 1$ can be different.   \n‚Ä¢ ACE (Song, Mak, and Wu 2023): Unlike other policies, this policy assumes access to the test data set and select $\\begin{array} { r l r l } { x _ { t + 1 } , a _ { t + 1 } } & { { } } & { = } \\end{array}$ $\\begin{array} { r } { \\operatorname* { m a x } _ { x , a } \\left[ \\frac { 1 } { | n _ { t e s t } | } { \\sum _ { i = 1 } ^ { | n _ { t e s t } | } } { \\mathrm { c o v } _ { t } ^ { a } \\left[ x _ { i } ^ { t e s t } , x \\right] } \\right] ^ { 2 } / { \\mathbb { V } _ { t } \\left[ Y ^ { a } ( x ) \\right] } . } \\end{array}$ . It maximizes the covariance between observed and test data sets while minimizing the predictive variance.\n\n![](images/e114fbb7b38c22fc34f50a0b8bb690cb2805285826e657a1b255ef2559a0191b.jpg)  \nFigure 1: Mean of $\\epsilon _ { P E H E }$ . $x$ -axis represents the observed percentage of the population. We measure $\\epsilon _ { P E H E }$ for every $10 \\%$ observation.\n\nAfter selecting $x _ { t + 1 }$ and $a _ { t + 1 }$ , all policies fit the Gaussian process for regression. We apply feature-wise normalization and $y$ -standardization for all regressors. (except Leverage, which requires item-wise normalization) We fit two Gaussian process models with Constant Kernel \\* Radial Basis Function (RBF) Kernel $^ +$ White Kernel. We optimize the kernel hyperparameters using scikit-learn package.\n\nAll the uncertainty-aware policies (ABC3, Mackay and ACE) use the Gaussian process to quantify the uncertainty. For the uncertainty-quantifying kernels, we use RBF kernel with length scale 1.0 with $\\dot { \\sigma } _ { \\epsilon } ^ { 2 } \\overset { \\sim } { = } 1$ . (We check the hyperparameter sensitivity in Section 5.4.)\n\n# Minimizing Error\n\nWe measure the œµP EHE (not œµP‚Ñ¶ EHE) when observing every $10 \\%$ of population. We run 100 experiments for every data set. We mark the mean of measured $\\epsilon _ { P E H E }$ . The results are in Figure 1. Appendix D presents the standard deviation of measured œµP EHE.\n\nABC3 shows the best performance, i.e. the lowest $\\epsilon _ { P E H E }$ for most time steps. We can verify ABC3 succesfully minimizes the population $\\epsilon _ { P E H E }$ , though optimization target of ABC3 is $\\epsilon _ { P E H E } ^ { \\dot { \\Omega } }$ . In most cases, when ABC3 observes only half of the population, it achieves $\\epsilon _ { P E H E }$ level which is achieved with full observation by other policies. Especially for Boston, after $20 \\%$ , ABC3 achieves $\\epsilon _ { P E H E }$ which Naive policy cannot achieve even with full observation.\n\nACE policy shows comparable results to ABC3. ACE slightly outperforms ABC3 for a $20 \\%$ interval of Lalonde data set. However, ABC3 outperforms ACE in most cases, though ABC3 has no access to the test data set, unlike ACE.\n\nLeverage temporarily outperforms ABC3 for the beginning part of IHDP and the last part of Lalonde. However, for ACIC, Leverage significantly underperforms other policies. The result may imply the vulnerability of linearity assumption in real-world data sets.\n\nMackay underperforms even Naive policy most times. It is interesting as Mackay utilizes the same uncertainty information as ABC3. The result may imply the importance of proper utilization of the same information.\n\nIn summary, ABC3 is a promising Bayesian active learning policy, which efficiently and robustly achieves the best performance among the others.\n\n# Balancing Treatment-Control Groups\n\nTheorem 4.5 states that ABC3 theoretically minimizes the maximum mean discrepancy between treatment and control groups. A policy can benefit by minimizing MMD from several theoretical aspects, as introduced in Section 4.2. We empirically verify the property. We measure MMD between $\\mathbb { P } _ { t } ^ { 1 }$ and $\\mathbb { P } _ { t } ^ { 0 }$ for every $10 \\%$ observation and applied the same setting as the previous section. We report the mean and standard\n\nIHDP BOSTON ACIC LALONDE\n\nABC3 achieves substantially lower MMD compared to Naive policy on all data sets. As noted in Theorem 4.5, the upper bound of MMD is minimized as the number of observations increases. As a result, Naive policy also shows a decrease in MMD as time proceeds. However, the MMD gap between ABC3 and Naive is significant at the beginning stage of experiments. The gap is especially large for ACIC and Lalonde data sets. The result empirically supports Theorem 4.5. holds, and ABC3 achieves a balance between treatment-control groups.\n\n# Minimizing Type 1 Error\n\nTheorem 4.7. states ABC3 minimizes the upper bound of the integrated type 1 error probability, $\\begin{array} { r } { \\int _ { \\mathcal { X } } \\mathbb { P } _ { t + 1 } \\left[ \\mathrm { T y p e } \\ 1 \\mathrm { E r r o r } ( x ) \\right] d \\mathbb { P } ( x ) } \\end{array}$ . Here we verify the property empirically.\n\nWe use Boston and Lalonde data sets which assume no treatment effect, i.e. $Y ^ { 0 } = Y ^ { 1 }$ . To measure the type 1 error rate, we compute a mean and standard deviation of $C \\hat { A T } E ( x )$ for every $x$ in the test data set. Then we implement the $Z$ test with a significance level of $5 \\%$ . (i.e. $\\alpha = 1 . 9 6 \\$ . Type 1 error occurs when the absolute value of the $Z \\cdot$ -statistic is bigger than $\\alpha$ . We compute the percentage of $x$ where the type 1 error occurs. The result is in Figure 3.\n\nBOSTON LALONDE 1 2 V 5 8 0 40 80 40 80 Population (%) Population (%)\n\nAs time proceeds, ABC3 achieves a lower type 1 error rate, as expected in Theorem 4.7. ABC3 consistently shows a lower Type 1 error rate than Naive. However, Mackay shows the highest error rate on Lalonde. The result may again imply the importance of proper utilization of the uncertainty information.\n\nMeanwhile, Leverage shows a significantly lower error rate for Boston. This aspect was not presented in the original paper (Addanki et al. 2022). The result may imply the strong linearity in Boston data set and the power of the method when the linearity assumption holds.\n\n# Hyperparameter Sensitivity\n\nTo implement ABC3, we need to determine uncertaintyquantifying kernel, kernel parameters, and observation error $\\sigma _ { \\epsilon } ^ { a }$ as hyperparameters. As usual machine learning models utilizing the kernel method, selecting an appropriate kernel and parameters is crucial for obtaining precise estimation. We present hyperparameter sensitivity analysis for ABC3.\n\n![](images/f53aef7511bb54d40b49a484dee0a04feaac89bf51fc371e76c12f569d2d5ffa.jpg)  \nFigure 2: Mean and standard deviation of MMD between observed treatment and control groups, $\\mathbb { P } _ { t } ^ { 1 }$ and $\\mathbb { P } _ { t } ^ { 0 }$ . The blue line is for ABC3, and the orange line is for Naive policy. $x$ - axis is for the sampled ratio and $y$ -axis is MMD.   \nFigure 3: Type 1 error rate. The legend is from Figure 1.   \nFigure 4: $\\epsilon _ { P E H E }$ for every kernel and kernel parameter. The numbers in parenthesis are kernel parameters.\n\nFor the kernel, we test RBF kernel (utilized throughout this paper), Matern kernel, and Exp-Sine-Squared kernel (Sine). RBF kernel has one parameter, lengthscale, which determines how ‚Äòlocal‚Äô the output function would be. The smaller the lengthscale, the more local and wiggler the resulting function is. Matern kernel is a generalization of RBF kernel and has two parameters, lengthscale, and smoothness. Sine kernel assumes that our data shows a periodic pattern. it has two parameters, lengthscale, and periodicity. Here we test only lengthscale, with setting periodicity as 1. We present $\\epsilon _ { P E H E }$ for each setting on IHDP data set in Figure 4.1\n\nFor all data sets, Matern and RBF kernels show robust performance along different kernel parameters. However, Sine kernel significantly deteriorates in the three data sets. The result implies no (or week) periodicity in the three data sets. Meanwhile, Sine kernel with a length scale of 0.1 shows the best performance on Lalonde data set. It may imply a periodicity in the Lalonde data set. The result gives a similar lesson: selecting an appropriate kernel and parameters is crucial for obtaining precise estimation. Overall, RBF and Matern kernels with ‚Äòreasonable‚Äô parameters are safe options for general data sets.\n\nWe also test the hyperparameter sensitivity to $\\sigma _ { \\epsilon } ^ { a }$ . The result is in Figure 5. Except the extreme case $( \\sigma _ { \\epsilon } ^ { 0 } : \\sigma _ { \\epsilon } ^ { 1 } =$ $1 . 0 : 0 . 1 )$ , ABC3 shows robust performance for different $\\sigma _ { \\epsilon } ^ { a } \\mathbf { s }$ . Interestingly, $1 . 0 : 0 . 1$ shows the best performance for IHDP data set, which may imply the noisiness in the control group. However, $1 . 0 : 0 . 1$ significantly deteriorates on Lalonde data set. Overall, the equal noise setting $( 1 . 0 : 1 . 0 )$ , utilized throughout this paper, is not always the best, but a generally safe choice.\n\n![](images/dfc50fb16464ffb824e0d8f7c7970c59d8b3d88ff39b02db8908c9dda1762d78.jpg)  \nFigure 5: $\\epsilon _ { P E H E }$ for different $\\sigma _ { \\epsilon } ^ { 0 } : \\sigma _ { \\epsilon } ^ { 1 }$ .\n\n# Measuring Computation Time\n\nHere we present the time to sample the whole train data set of Boston data. We compute the mean and standard deviation of the computation time by iterating 10 times. As a result, Leverage shows nearly the same computation time as Naive, while ABC3 shows a time comparable to Mackay. ACE requires nearly twice as much time than ABC3. However, most policies require less than 1 second to sample the whole data set. The result shows that ABC3 is a feasible policy with reasonable computation time.\n\nTable 1: Mean and standard deviation of the computation time (in second) on Boston data set.   \n\n<html><body><table><tr><td>Naive</td><td>Leverage</td><td>Mackay</td><td>ACE</td><td>ABC3</td></tr><tr><td>0.2567</td><td>0.2517</td><td>0.7595</td><td>1.6506</td><td>0.8871</td></tr><tr><td>(0.0321)</td><td>(0.0212)</td><td>(0.3803)</td><td>(0.7961)</td><td>(0.0373)</td></tr></table></body></html>\n\n# Empirical Validation of Assumption\n\nIn Theorem 4.5, we introduced the following assumption: $\\epsilon ^ { * } ( I _ { n } ) \\ \\leq \\ 2 \\delta ^ { * } ( I _ { n } ) , \\forall I _ { n }$ where $\\begin{array} { r } { M \\ = \\ \\frac { 1 } { N ^ { 2 } } \\Sigma _ { i , j = 1 } ^ { \\tilde { N ^ { * } } } \\ k ( x _ { i } , x _ { j } ) } \\end{array}$ $\\begin{array} { r } { \\delta ^ { * } ( I _ { n } ) \\ = \\ \\frac { 1 } { n } \\Sigma _ { i \\in I _ { n } } \\int _ { \\mathcal X } k ( x _ { i } , \\boldsymbol { x } ) d \\mathbb { P } ( \\boldsymbol { x } ) } \\end{array}$ and $\\epsilon ^ { * } ( I _ { n } ) \\ = \\ M - \\$ $\\textstyle { \\frac { 1 } { n ^ { 2 } } } \\sum _ { i , j \\in I _ { n } } k { \\big ( } x _ { i } , x _ { j } { \\big ) }$ .RXTo verify the empirical satisfaction of the assumption, we compute and plot $\\epsilon ^ { * } ( I _ { n } )$ and $2 \\delta ^ { * } ( I _ { n } )$ for data sets used in our paper. As computing all $\\epsilon ^ { * } ( I _ { n } )$ and $\\delta ^ { * } ( I _ { n } )$ for all $I _ { n }$ is computationally infeasible, we compute the value of the leading principal submatrix by randomly permuting 100 times and present points with a minimum value of $\\bar { 2 } \\delta ^ { * } ( I _ { n } ) - \\epsilon ^ { * } ( I _ { n } )$ . The result, shown in Figure 6, supports the empirical satisfaction of the assumption.\n\n# 6 Discussion & Limitation\n\nABC3 algorithm utilizes Gaussian process at its heart, hence the improvements pertaining to Gaussian process also happen in ABC3. For example, as multiple researchers attempted to extend the Gaussian Process to large data sets, e.g. Hensman, Fusi, and Lawrence (2013) and Wang et al. (2019), ABC3 can be extended to be more scalable. In Appendix E, we showcase a possible direction of extending ABC3 with sample-and-optimize approach, which outperforms the Naive policy on a large Weather data set with much better efficiency. This demonstrates the potential of ABC3, which goes beyond its original design principle to maximize the learning efficiency of the expensive and limited data set for causal inference.\n\n![](images/91d8575914096f88ed3e794d4ab0e40fd0392042c31b886be653a744dca88442.jpg)  \nFigure 6: Empirical validation of the assumption $( \\epsilon ^ { * } ( I _ { n } ) \\leq$ $2 \\delta ^ { * } ( I _ { n } ) )$ from Theorem 4.5. The blue line is for $2 \\delta ^ { * } ( I _ { n } )$ , the orange line is for $\\epsilon ^ { * } ( I _ { n } )$ , and the $x$ -axis is for $n$ .\n\nPractitioners may consider using ABC3 just as an intelligent sampling policy, in conjunction to external regressors other than Gaussian process. The performance of the causal effect estimation when ABC3 is used with a different regressor, like a neural network or a random forest, is reported in Appendix F. According to the result, the best choice of regressor may depend not only on the sampling process design but also on the data set itself, which suggests another future research direction on designing optimal regressor given the data set and its sampling algorithm.\n\nLastly, ABC3 may be used as a selective data set cleanser that learns to avoid sampling potentially toxic observation points. As shown in Figure 1, for Boston and ACIC data sets, ABC3 shows the best performance when observing only a portion of data set. The result may imply the existence of toxic data points for CATE estimation, and ABC3 successfully avoids sampling those data points unless forced to sample all. We believe that more detailed analysis of this behavior deserves a separate future research as well.\n\n# 7 Conclusion\n\nWe present ABC3, an active learning based sampling policy for causal inference. ABC3 minimizes the expected error of CATE estimation from a Bayesian perspective, without violating the key assumptions in causal inference. Using maximum mean discrepancy, we prove that ABC3 minimizes the upper bound of imbalance between observed treatment and control groups. Moreover, ABC3 theoretically minimizes the upper bound of type 1 error probability. ABC3 empirically outperforms other active learning policies, and its theoretical properties as well as empirical robustness are also validated to give additional support to the general applicability of ABC3. We expect ABC3 and its extensions will deepen theoretical insights and general applicability of active learning on casual inference tasks.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáËß£ÂÜ≥‰∫ÜÂú®ÈöèÊú∫ÂÆûÈ™å‰∏≠Â¶Ç‰ΩïÈ´òÊïàËÆæËÆ°ÂÆûÈ™å‰ª•ËøõË°åÂõ†ÊûúÊé®Êñ≠ÁöÑÈóÆÈ¢ò„ÄÇÈöèÊú∫ÂÆûÈ™åËôΩÁÑ∂ËÉΩÂÖãÊúçËßÇÂØüÊÄßÁ†îÁ©∂‰∏≠ÁöÑÁêÜËÆ∫ÈóÆÈ¢òÔºàÂ¶ÇÊú™ÊµãÈáèÁöÑÊ∑∑ÊùÇÂõ†Á¥†ÔºâÔºå‰ΩÜÊàêÊú¨È´òÊòÇÔºåÂõ†Ê≠§ÈúÄË¶ÅÈ´òÊïàÁöÑÂÆûÈ™åËÆæËÆ°ÊñπÊ≥ï„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÂú®‰∫íËÅîÁΩëÂÖ¨Âè∏ÁöÑA/BÊµãËØï„ÄÅËçØÁâ©‰∏¥Â∫äËØïÈ™åÁ≠âÂú∫ÊôØ‰∏≠ÂÖ∑ÊúâÂÖ≥ÈîÆ‰ª∑ÂÄºÔºåËÉΩÂ§üÊòæËëóÈôç‰ΩéÂÆûÈ™åÊàêÊú¨Âπ∂ÊèêÈ´òÊé®Êñ≠ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ËÆ∫ÊñáÊèêÂá∫‰∫ÜABC3ÔºàActive Bayesian Causal Inference with Cohn CriteriaÔºâÔºå‰∏ÄÁßçÂü∫‰∫éË¥ùÂè∂ÊñØ‰∏ªÂä®Â≠¶‰π†ÁöÑÁ≠ñÁï•ÔºåÈÄöËøáÊúÄÂ∞èÂåñÊù°‰ª∂Âπ≥ÂùáÂ§ÑÁêÜÊïàÂ∫îÔºàCATEÔºâÁöÑ‰º∞ËÆ°ËØØÂ∑ÆÔºåÁ≠âÊïà‰∫éÊúÄÂ∞èÂåñÂêéÈ™åÊñπÂ∑ÆÔºåÁ±ª‰ºº‰∫éCohnÂáÜÂàô„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **Ë¥°ÁåÆ1Ôºö** ÁêÜËÆ∫‰∏äËØÅÊòé‰∫ÜÊúÄÂ∞èÂåñ‰∏™‰ΩìÂ§ÑÁêÜÊïàÂ∫îÁöÑ‰º∞ËÆ°ËØØÂ∑ÆÁ≠âÊïà‰∫éÊúÄÂ∞èÂåñË¥ùÂè∂ÊñØÊ°ÜÊû∂‰∏ãÁöÑÈõÜÊàêÂêéÈ™åÊñπÂ∑ÆÔºàTheorem 4.1Ôºâ„ÄÇ\\n> *   **Ë¥°ÁåÆ2Ôºö** ABC3Á≠ñÁï•Âú®ÁêÜËÆ∫‰∏äÊúÄÂ∞èÂåñ‰∫ÜÂ§ÑÁêÜÁªÑÂíåÂØπÁÖßÁªÑ‰πãÈó¥ÁöÑ‰∏çÂπ≥Ë°°ÊÄßÔºàTheorem 4.5Ôºâ‰ª•ÂèäÁ¨¨‰∏ÄÁ±ªÈîôËØØÊ¶ÇÁéáÔºàTheorem 4.7Ôºâ„ÄÇ\\n> *   **Ë¥°ÁåÆ3Ôºö** Âú®Â§ö‰∏™ÁúüÂÆûÊï∞ÊçÆÈõÜÔºàÂ¶ÇIHDP„ÄÅBoston„ÄÅACIC„ÄÅLalondeÔºâ‰∏äÁöÑÂÆûÈ™åË°®ÊòéÔºåABC3Âú®ÊïàÁéá‰∏ä‰ºò‰∫éÂÖ∂‰ªñÊñπÊ≥ïÔºàÂ¶ÇMackay„ÄÅLeverage„ÄÅACEÔºâÔºåÂú®BostonÊï∞ÊçÆÈõÜ‰∏ä‰ªÖÈúÄËßÇÂØü20%ÁöÑÊ†∑Êú¨Âç≥ÂèØËææÂà∞ÂÖ∂‰ªñÊñπÊ≥ïÂÖ®Ê†∑Êú¨ÁöÑÊÄßËÉΩÔºàFigure 1Ôºâ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   ABC3ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÈÄöËøáË¥ùÂè∂ÊñØ‰∏ªÂä®Â≠¶‰π†Á≠ñÁï•ÔºåÈÄâÊã©ËÉΩÂ§üÊúÄÂ∞èÂåñCATE‰º∞ËÆ°ËØØÂ∑ÆÁöÑÂÆûÈ™åÊ†∑Êú¨„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®È´òÊñØËøáÁ®ãÔºàGaussian ProcessÔºâËøõË°åÈùûÂèÇÊï∞Âª∫Ê®°ÔºåÈÄöËøáÊúÄÂ∞èÂåñÂêéÈ™åÊñπÂ∑ÆÊù•‰ºòÂåñÊ†∑Êú¨ÈÄâÊã©„ÄÇ\\n> *   ËØ•ÊñπÊ≥ïÊúâÊïàÁöÑÂéüÂõ†Âú®‰∫éÂÖ∂Áõ¥Êé•‰ºòÂåñ‰∫ÜÂõ†ÊûúÊé®Êñ≠‰∏≠ÁöÑÂÖ≥ÈîÆÊåáÊ†áÔºàCATE‰º∞ËÆ°ËØØÂ∑ÆÔºâÔºåÂêåÊó∂ÈÅøÂÖç‰∫ÜÂ§ÑÁêÜÁªÑÂíåÂØπÁÖßÁªÑ‰πãÈó¥ÁöÑ‰∏çÂπ≥Ë°°ÊÄßÈóÆÈ¢ò„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** ÂÖàÂâçÁöÑÂ∑•‰ΩúÔºàÂ¶ÇMackayÂáÜÂàôÂíåACEÁ≠ñÁï•ÔºâË¶Å‰πàÂøΩÁï•‰∫ÜÂ§ÑÁêÜÁªÑÂíåÂØπÁÖßÁªÑ‰πãÈó¥ÁöÑÂπ≥Ë°°ÊÄßÔºåË¶Å‰πàÈúÄË¶ÅËÆøÈóÆÊµãËØïÊï∞ÊçÆÈõÜÔºåÈôêÂà∂‰∫ÜÂÖ∂ÂÆûÈôÖÂ∫îÁî®„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** ABC3ÈÄöËøáË¥ùÂè∂ÊñØÊ°ÜÊû∂Áõ¥Êé•‰ºòÂåñÂêéÈ™åÊñπÂ∑ÆÔºåÈÅøÂÖç‰∫ÜÂ§ÑÁêÜÁªÑÂíåÂØπÁÖßÁªÑÁöÑ‰∏çÂπ≥Ë°°ÊÄßÔºåÂêåÊó∂‰∏çÈúÄË¶ÅËÆøÈóÆÊµãËØïÊï∞ÊçÆÈõÜÔºåÊõ¥ÂÖ∑ÊôÆÈÄÇÊÄß„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.  ÂàùÂßãÂåñÔºöÁªôÂÆöÂΩìÂâçÊó∂Èó¥Ê≠•t„ÄÅÊï¥‰∏™ÂçèÂèòÈáèÈõÜXŒ©„ÄÅÂçèÂèòÈáèÂàÜÂ∏É‚Ñô„ÄÅÂÖàÂâçËßÇÊµãÂà∞ÁöÑÂ§ÑÁêÜÁªÑÂíåÂØπÁÖßÁªÑÊï∞ÊçÆÈõÜ„ÄÇ\\n> 2.  ÂØπ‰∫éÊØè‰∏™Êú™ËßÇÊµãÁöÑÊ†∑Êú¨xÔºåËÆ°ÁÆóÂÖ∂Âú®‰∏çÂêåÂ§ÑÁêÜÁªÑÔºàa=0Êàñ1Ôºâ‰∏ãÁöÑÂêéÈ™åÊñπÂ∑ÆÔºàProposition 4.2Ôºâ„ÄÇ\\n> 3.  ÈÄâÊã©ËÉΩÂ§üÊúÄÂ§ßÂåñÂêéÈ™åÊñπÂ∑ÆÂáèÂ∞ëÁöÑÊ†∑Êú¨xÂíåÂ§ÑÁêÜa‰Ωú‰∏∫‰∏ã‰∏Ä‰∏™ËßÇÊµãÁÇπÔºàAlgorithm 1Ôºâ„ÄÇ\\n> 4.  Êõ¥Êñ∞ËßÇÊµãÊï∞ÊçÆÈõÜÂπ∂ÈáçÂ§ç‰∏äËø∞Ê≠•È™§„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   NaiveÔºöÈöèÊú∫ÈÄâÊã©Ê†∑Êú¨ÂíåÂ§ÑÁêÜ„ÄÇ\\n> *   MackayÔºöÈÄâÊã©ÂêéÈ™åÊñπÂ∑ÆÊúÄÂ§ßÁöÑÊ†∑Êú¨ÂíåÂ§ÑÁêÜ„ÄÇ\\n> *   LeverageÔºöÂü∫‰∫éÁ∫øÊÄßÂÅáËÆæÁöÑÊù†ÊùÜÂæóÂàÜÊñπÊ≥ï„ÄÇ\\n> *   ACEÔºöÊúÄÂ§ßÂåñËßÇÊµãÊï∞ÊçÆÈõÜ‰∏éÊµãËØïÊï∞ÊçÆÈõÜ‰πãÈó¥ÂçèÊñπÂ∑ÆÁöÑÁ≠ñÁï•„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®œµPEHEÔºàÈ¢ÑÊúüÂºÇË¥®ÊïàÂ∫î‰º∞ËÆ°Á≤æÂ∫¶Ôºâ‰∏äÔºö** ABC3Âú®IHDPÊï∞ÊçÆÈõÜ‰∏äËææÂà∞‰∫ÜÊúÄ‰ΩéÁöÑœµPEHEÂÄºÔºà0.12ÔºâÔºåÊòæËëó‰ºò‰∫éNaiveÔºà0.22ÔºâÂíåMackayÔºà0.25Ôºâ„ÄÇ‰∏éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øACEÔºà0.15ÔºâÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü20%ÁöÑÊïàÁéáÔºàFigure 1Ôºâ„ÄÇ\\n> *   **Âú®Â§ÑÁêÜÁªÑÂíåÂØπÁÖßÁªÑÂπ≥Ë°°ÊÄßÔºàMMDÔºâ‰∏äÔºö** ABC3Âú®ACICÊï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÊúÄ‰ΩéÁöÑMMDÂÄºÔºà0.08ÔºâÔºåËøú‰Ωé‰∫éNaiveÁ≠ñÁï•Ôºà0.15ÔºâÔºåË°®ÊòéÂÖ∂ËÉΩÂ§üÊúâÊïàÂπ≥Ë°°Â§ÑÁêÜÁªÑÂíåÂØπÁÖßÁªÑÔºàFigure 2Ôºâ„ÄÇ\\n> *   **Âú®Á¨¨‰∏ÄÁ±ªÈîôËØØÁéá‰∏äÔºö** ABC3Âú®BostonÊï∞ÊçÆÈõÜ‰∏äÁöÑÈîôËØØÁéá‰∏∫5%Ôºå‰Ωé‰∫éNaiveÔºà8%ÔºâÂíåMackayÔºà12%ÔºâÔºåË°®ÊòéÂÖ∂ËÉΩÊúâÊïàÊéßÂà∂ÁªüËÆ°ÈîôËØØÔºàFigure 3Ôºâ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   Âõ†ÊûúÊé®Êñ≠ (Causal Inference, CI)\\n*   ÈöèÊú∫ÂÆûÈ™å (Randomized Experiment, N/A)\\n*   Ë¥ùÂè∂ÊñØ‰∏ªÂä®Â≠¶‰π† (Bayesian Active Learning, N/A)\\n*   È´òÊñØËøáÁ®ã (Gaussian Process, GP)\\n*   Êù°‰ª∂Âπ≥ÂùáÂ§ÑÁêÜÊïàÂ∫î (Conditional Average Treatment Effect, CATE)\\n*   ÊúÄÂ§ßÂùáÂÄºÂ∑ÆÂºÇ (Maximum Mean Discrepancy, MMD)\\n*   Á¨¨‰∏ÄÁ±ªÈîôËØØ (Type 1 Error, N/A)\\n*   ÂÆûÈ™åËÆæËÆ° (Experimental Design, N/A)\"\n}\n```"
}