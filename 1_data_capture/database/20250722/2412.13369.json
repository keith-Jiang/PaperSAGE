{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.13369",
    "link": "https://arxiv.org/abs/2412.13369",
    "pdf_link": "https://arxiv.org/pdf/2412.13369.pdf",
    "title": "Multiple Mean-Payoff Optimization under Local Stability Constraints",
    "authors": [
        "David Klaska",
        "A. Kucera",
        "Vojtech Kur",
        "Vít Musil",
        "V. Rehák"
    ],
    "categories": [
        "cs.AI"
    ],
    "publication_date": "2024-12-17",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Masaryk University"
    ],
    "paper_content": "# Multiple Mean-Payoff Optimization Under Local Stability Constraints\n\nDavid Klaˇska, Anton´ın Kuˇcera, Vojtˇech Ku˚ r, V´ıt Musil, Vojteˇch ˇReh´ak\n\nMasaryk University, Brno, Czechia {david.klaska, vojtech.kur}@mail.muni.cz, {tony, musil, rehak} $@$ fi.muni.cz\n\n# Abstract\n\nThe long-run average payoff per transition (mean payoff) is the main tool for specifying the performance and dependability properties of discrete systems. The problem of constructing a controller (strategy) simultaneously optimizing several mean payoffs has been deeply studied for stochastic and game-theoretic models. One common issue of the constructed controllers is the instability of the mean payoffs, measured by the deviations of the average rewards per transition computed in a finite “window” sliding along a run. Unfortunately, the problem of simultaneously optimizing the mean payoffs under local stability constraints is computationally hard, and the existing works do not provide a practically usable algorithm even for non-stochastic models such as two-player games. In this paper, we design and evaluate the first efficient and scalable solution to this problem applicable to Markov decision processes.\n\n# Code — https://gitlab.fi.muni.cz/formela/2025-aaai-mmp Extended version — https://arxiv.org/abs/2412.13369\n\n# Introduction\n\nMean payoff, i.e., the long-run average payoff per time unit, is the standard tool for specifying and evaluating the longrun performance of dynamic systems. The overall performance is typically characterized by a tuple of mean payoffs computed for multiple payoff functions representing expenditures, income, resource consumption, and other relevant aspects. The basic task of multiple mean-payoff optimization is to design a controller (strategy) jointly optimizing these mean payoffs. Efficient strategy synthesis algorithms have been designed for various models, such as Markov decision processes or two-player games (see Related work).\n\nA fundamental problem of the standard mean payoff optimization is the lack of local stability guarantees. For example, consider a payoff function modeling the expenditures of a company. Even if the long-run average expenditures per day (mean payoff) are $\\$ 1000$ , there are no guarantees on the maximal average expenditures per day in a bounded time horizon of, say, one month. It is possible that the company pays between $\\$ 800$ and $\\$ 1200$ per day every month, which is fine and sustainable. However, it may also happen that there are “good” and “bad” months with average daily expenditures of $\\$ 0$ and $\\$ 5000$ , respectively, where the second case is not so frequent but represents a critical cashflow problem that may ruin the company. The mean payoff does not reflect this difference; hence, optimizing the mean payoff (minimizing the overall long-run expenditures) may lead to disastrous outcomes in certain situations.\n\nThe lack of local stability guarantees has motivated the study of window mean payoff objectives, where the task is to optimize the average payoff in a “window” of finite length sliding along a run. The window size represents a bounded time horizon, and the task is to construct a strategy such that the average payoff computed for the states in the window stays within given bounds. Thus, one can express both longrun average performance and stability constraints. For a single payoff function, some technical variants of this problem are solvable efficiently, while others are PSPACE-hard. For multiple window mean payoffs, intractability results have been established for two-player games (see Related work).\n\nThe main concern of the previous works on window mean payoffs is classifying the computational complexity of constructing an optimal strategy. The obtained algorithms are based on reductions to other problems, and their complexity matches the established lower bounds. To the best of our knowledge, there have been no attempts to tackle the high complexity of strategy synthesis by designing scalable algorithms at the cost of some (unavoidable but acceptable) compromises. This open challenge and the problem’s high practical relevance are the primary motivations for our work.\n\nOur Contribution. We design the first efficient and scalable strategy synthesis algorithm for optimizing multiple window mean payoffs in a given Markov decision process.\n\nWe start by establishing the principal limits of our efforts by showing that the problem is NP-hard even for simple instances where the underlying MDP is a graph. Consequently, every efficient algorithm attempting to solve the problem must inevitably suffer from some limitations. Our algorithm trades efficiency for optimality guarantees, i.e., the computed solutions are not necessarily optimal. Nevertheless, our experiments show that the algorithm can construct highquality (and sometimes quite sophisticated) strategies for complicated instances of non-trivial size. Thus, we obtain the first practically usable solution to the problem of multiple window mean payoff optimization.\n\nMore concretely, the optimization objective is specified by a function Eval measuring the “badness” of the achieved tuple of window mean payoffs, and the task is to minimize the expected value of Eval (we refer to the next section for precise definitions). The Eval function can specify complex requirements on the tuple of achieved mean payoffs. This overcomes another limitation of the previous works, where it was only possible to specify the desired upper/lower bounds for each window mean payoff separately.\n\nThe core of our strategy synthesis algorithm is a novel procedure based on dynamic programming, computing the distribution of local mean payoffs for a given starting state. Remarkably, this procedure is differentiable, and the corresponding gradient can be calculated (in the backward pass) at essentially the same computational costs.\n\nA strategy maximizing the expected value of Eval may require memory. Our strategy synthesis algorithm produces finite-memory randomized strategies, where the memory size is a parameter. Using larger memory may produce better strategies but also substantially increases computational costs. From the scalability perspective, using randomization is essential because randomized strategies may achieve much better performance than deterministic strategies with the same amount of memory. This is demonstrated by the simple example below.\n\nExample 1. Consider a graph $D$ with two states $A , B$ and two payoff functions where $P a y _ { 1 } ( A ) { = } 1 , P a y _ { 1 } ( B ) { = } 0 .$ $P a y _ { 2 } ( A ) { = } 0$ , and $P a y _ { 2 } ( B ) { = } 8$ . We aim to construct a finitememory strategy such that the pair of expected window mean payoffs in a window of length 8 is positive in both components and as close to $( 1 , 1 )$ as possible with respect to $L _ { 1 }$ (Manhattan) distance. Formally, for a given pair of window mean-payoffs $( w m p _ { 1 } , w m p _ { 2 } )$ , the function Eval returns the $L _ { 1 }$ distance to the vector $( 1 , 1 )$ if both $w m p _ { 1 }$ and $w m p _ { 2 }$ are positive; otherwise, Eval returns a “penalty” equal to 5.\n\nSuppose that the available memory has $K$ different states. Then, for every $K < 7$ , there is a randomized strategy with $K$ memory states achieving a strictly better expected value of Eval than the best deterministic strategy with $K$ memory states. For $K = 1 , 2$ , such strategies are shown in Fig. 1 (for $K = 1$ , the best deterministic strategy achieves the window mean payoffs $( { \\textstyle { \\frac { 1 } { 2 } } } , 4 )$ in every window of length 8, and hence the expected $\\bar { L _ { 1 } } ^ { - }$ distance to $( 1 , 1 )$ is 3.5; for $K = 2$ , 2/3 of the windows have mean payoffs $\\textstyle { \\left( { \\frac { 5 } { 8 } } , 3 \\right) }$ , and $1 / 3$ (those beginning with $A A$ ) have $\\left( { \\frac { 3 } { 4 } } , 2 \\right)$ , hence $\\mathbb { E } [ E \\nu a l ] = 2 \\rangle$ . The optimal finite-memory strategy achieving the window mean payoffs $\\textstyle { \\left( { \\frac { 7 } { 8 } } , 1 \\right) }$ in every window of length 8 (i.e., the expected value of Eval equal to $\\frac { 1 } { 8 }$ ) requires 7 memory elements. □\n\nIn our experiments, we concentrate on the following:\n\n(1) Demonstrating the improvement achieved by the dynamic procedure described above, where the baseline is a “naive” DFS-based procedure.   \n(2) Evaluating the scalability of our algorithm and the quality of the constructed strategies.   \n(3) Analyzing the power of randomization to compensate for insufficient memory.\n\n![](images/ff3ddce103304c9ff5d9500af7eabad1aa65f81b0500ecf01fb96bac8749feed.jpg)  \nFigure 1: For a memory with $K$ states where $K \\in \\{ 1 , 2 \\}$ , the best deterministic strategy (left) is worse than a suitable randomized strategy (right). The optimal finite memory strategy requires 7 memory elements.\n\nIn (2), a natural baseline for evaluating the quality of a constructed strategy is the best value achievable by a finitememory strategy. However, there is no algorithm for computing the best value. This issue is overcome by constructing a parameterized family of graphs where the best achievable value and the amount of required memory can be determined by a careful manual analysis. The structure of our parameterized graphs is similar to the ones constructed in our NP-hardness proofs mentioned above. Thus, we avoid bias towards simple instances of the problem. In (3), we analyze the quality of randomized strategies constructed for memory of increasing size. Roughly speaking, our experiments show that even in scenarios where the number of memory elements is insufficient for implementing an optimal strategy, the quality of the obtained strategies is still relatively close to the optimum. Hence, randomization can “compensate” for insufficient memory. This is encouraging because the number of memory elements is one of the crucial parameters influencing the computational costs.\n\nRelated Work. Single mean-payoff optimization for Markov decision processes (MDPs) is a classical problem studied for decades (see, e.g., (Puterman 1994)). For multiple mean-payoff optimization in MDPs, a polynomial-time algorithm computing optimal strategies and approximating the Pareto curve has been given in (Bra´zdil et al. 2011), and the solution has been successfully integrated into the software tool PRISM (Bra´zdil et al. 2015).\n\nThe window mean payoff objectives have been first studied for a single payoff function and non-stochastic twoplayer games in (Chatterjee et al. 2015), and then for Markov decision processes in (Bordais, Guha, and Raskin 2019; Brihaye et al. 2020). The complexity of the strategy synthesis algorithms proposed in these works ranges from polynomial time to polynomial space, depending on the concrete technical variant of the problem. The multiple window mean payoff objectives have been considered for twoplayer games in (Chatterjee et al. 2015), where the problem of computing an optimal strategy is classified as provably intractable (EXPTIME-hard). The optimization objective studied in these works is a simple conjunction of upper/lower bounds imposed on the achieved window mean payoffs. The main focus is on classifying the computational complexity of the studied problems, where the upper bounds are typically obtained by reductions to other game-theoretic problems. To the best of our knowledge, our work gives the first scalable algorithm for multiple window mean payoff optimization applicable to Markov decision processes of considerable size.\n\nIn a broader context, a related problem of window parity objective has been studied in (Chatterjee and Henzinger 2006; Horn 2007; Chatterjee, Henzinger, and Horn 2009). An alternative approach to capturing local stability of mean payoff based on bounding the variance of relevant random variables has been proposed in (Bra´zdil et al. 2017).\n\n# The Model\n\nWe assume familiarity with basic notions of probability theory (probability distribution, expected value, conditional random variables, etc.) and Markov chain theory. The set of all probability distributions over a finite set $A$ is denoted by $D i s t ( A )$ . We use $\\mathbb { N }$ and $\\mathbb { Q } _ { + }$ do denote the set of nonnegative integers and non-negative rationals, respectively.\n\nMarkov chains. A Markov chain is a triple $\\begin{array} { r c l } { M } & { = } & { ( S , P r o b , \\mu ) } \\end{array}$ where $S$ is a finite set of states, $P r o b : S \\times S  [ 0 , 1 ]$ is a stochastic matrix where $\\begin{array} { r } { \\sum _ { t \\in S } P r o b ( s , t ) \\ = \\ 1 } \\end{array}$ for all $s \\in S$ , and $\\mu \\ \\in \\ D i s t ( S )$ is an initial distribution.\n\nA run in $M$ is an infinite sequence of states. We use $\\mathbb { P }$ to denote the standard probability measure over the runs of $M$ (see, e.g., (Norris 1998))\n\nA state $t$ is reachable from a state $s$ if $P r o b ^ { n } ( s , t ) > 0$ for some $n \\geq 1$ . We say that $M$ is strongly connected (or irreducible) if all states are mutually reachable from each other. For an irreducible $M$ , we use $\\mathbb { I } \\in D i s t ( S )$ to denote the unique invariant distribution satisfying $\\mathbb { I } = \\mathbb { I } \\cdot P r o b$ (note that $\\mathbb { I }$ is independent of $\\mu _ { . }$ ). By ergodic theorem (Norris 1998), $\\mathbb { I }$ is the limit frequency of visits to the states of $S$ along a run. More precisely, let $w = s _ { 0 } , s _ { 1 } , . . .$ be a run of $M$ . For every $s \\in S$ , let\n\n$$\nF r e q _ { s } ( w ) = \\operatorname* { l i m } _ { n \\to \\infty } \\frac { \\# s ( s _ { 0 } , \\ldots , s _ { n - 1 } ) } { n }\n$$\n\nwhere $\\# _ { s } ( s _ { 0 } , \\dots , s _ { n - 1 } )$ is the number of occurrences of $s$ in s0, . $s _ { 0 } , \\ldots , s _ { n - 1 }$ . If the above limit does not exist, we put $F r e q _ { s } ( w ) = 0$ . Furthermore, let $F r e q ( w ) : S  [ 0 , 1 ]$ be the vector of all $F r e q _ { s } ( w )$ where $s \\in S$ . The ergodic theorem says that $\\mathbb { P } [ F r e q = \\mathbb { I } ] = 1$ .\n\nA bottom strongly connected component (BSCC) of $M$ is a maximal $B \\subseteq S$ such that $B$ is strongly connected and closed under reachable states. Note that if $M$ is irreducible, then the set $S$ is the only BSCC of $M$ . Otherwise, $M$ can have multiple BSCCs, and each such $B$ can be seen as an irreducible Markov chain where the set of states is $B$ and the probability matrix is the restriction of $P r o b$ to $B \\times B$ .\n\nFor the rest of this section, we fix an irreducible Markov chain $M = ( S , P r o b , \\mu )$ .\n\nGlobal and Local Mean Payoff. Let $P a y : S  \\mathbb { N }$ be a payoff function. For every run $w = s _ { 0 } , s _ { 1 } , \\ldots$ , let\n\n$$\n\\mathrm { G M P } ( w ) = \\operatorname* { l i m } _ { n \\to \\infty } \\frac { 1 } { n } \\sum _ { i = 0 } ^ { n - 1 } P a y ( s _ { i } )\n$$\n\nbe the limit-average payoff per transition along the run $w$ . If the limit does not exist, we put $\\mathrm { G M P } ( w ) = 0$ . An immediate consequence of the ergodic theorem (see above) is that the defining limit of $\\mathrm { G M P } ( w )$ exists and takes the same value $\\begin{array} { r } { g m p \\ = \\ \\sum _ { s \\in S } \\mathbb { I } ( s ) \\cdot P a { \\dot { y } } ( s ) } \\end{array}$ for almost all runs, i.e., $\\mathbb { P } [ \\mathrm { G M P } { = } g m p ] = { \\bar { 1 } }$ . We refer to the (unique) value gmp of GMP as the global mean payoff.\n\nLet $d \\in \\mathbb { N }$ be a time horizon. For every $j \\in \\mathbb { N }$ , let\n\n$$\n\\operatorname { W M P } [ d , j ] ( w ) = { \\frac { 1 } { d } } \\sum _ { i = j } ^ { j + d - 1 } P a y ( s _ { i } )\n$$\n\nbe the average payoff computed for the sequence of $d$ consecutive states in $w$ starting with $s _ { j }$ . We refer to the value $\\mathrm { W M P } [ d , j ]$ as the window mean payoff after $j$ steps (assuming some fixed $d$ ).\n\nNote that as $d \\to \\infty$ , the value of $\\mathrm { W M P } [ d , j ]$ is arbitrarily close to gmp with arbitrarily large probability, independently of $j$ . However, for a given $d$ , the value of $\\mathrm { W M P } [ d , j ]$ depends on $j$ and can be rather different from gmp. Also observe that $\\mathrm { W M P } [ d , j ]$ is a discrete random variable, and the underlying distribution depends only on the state $s _ { j }$ . More precisely, for all $j , m \\in \\mathbb { N }$ and $s \\in S$ such that $\\mathbb { P } [ s _ { j } \\overset { \\cdot } { = } s ] > 0$ and $\\mathbb { P } [ s _ { m } = s ] > 0$ , we have that the conditional random variables $\\mathrm { W M P } [ d , j ] \\mid s _ { j } = s$ and $\\mathrm { W M P } [ d , m ] \\mid s _ { m } = s$ are identically distributed. In the following, we write just $\\mathrm { W M P } [ d , s ]$ instead of $\\mathrm { W M P } [ d , j ] \\mid s _ { j } = s$ where $\\mathbb { P } [ s _ { j } = s ] > 0$ .\n\nMean Payoff Objectives. Let $P a y _ { 1 } , \\dots , P a y _ { k } \\ : \\ S \\ \\to \\ \\mathbb { N }$ be payoff functions. The standard multi-objective optimization problem for Markov Decision Processes (see below) is to jointly maximize the global mean payoffs for $P a y _ { 1 } , \\ldots , P a y _ { k }$ . In this paper, we use a more general approach where the “badness” of the achieved mean payoffs is measured by a dedicated function $\\begin{array} { r } { E \\nu a l : \\mathbb { R } ^ { k } \\ \\overset { \\bullet } {  } \\ \\dot { \\mathbb { R } } } \\end{array}$ . A smaller value of Eval indicates more appropriate payoffs.\n\nFor example, the joint maximization of the mean payoffs can be encoded by $\\begin{array} { r } { E \\nu a l _ { \\mathrm { m a x } } ( \\vec { x } ) = \\sum _ { i = 1 } ^ { k } ( P a y _ { i } ^ { \\mathrm { m a x } } { - } \\vec { x } _ { i } ) , } \\end{array}$ where $P a y _ { i } ^ { \\operatorname* { m a x } } = \\operatorname* { m a x } _ { s \\in S } P a y _ { i } ( s )$ . ThePdefining sum can also be weighted to reflect certain priorities among the Pareto optimal solutions. In general, Eval can encode the preference of keeping the mean payoffs close to some values, within some interval, or even enforce some mutual dependencies among the mean payoffs.\n\nAs we shall see, our strategy synthesis algorithm works for an arbitrary Eval that is decomposable. Intuitively, the decomposability condition enables more efficient evaluation/differentiation of Eval by dynamic programming without substantially restricting the expressive power.\n\nNow we can define the global and local value of a run as follows:\n\n$$\n\\begin{array} { l } { { \\displaystyle G V A L ( w ) = E \\nu a l ( \\mathrm { G M P } _ { 1 } ( w ) , \\dots , \\mathrm { G M P } _ { k } ( w ) ) } } \\\\ { { \\displaystyle W V A L ( w ) = \\operatorname* { l i m } _ { n \\to \\infty } \\frac { 1 } { n } \\sum _ { j = 0 } ^ { n - 1 } E \\nu a l \\big ( \\mathrm { W M P } _ { 1 } [ d , j ] , \\dots , \\mathrm { W M P } _ { k } [ d , j ] \\big ) } } \\end{array}\n$$\n\nwhere ${ \\mathrm { G M P } } _ { i } ( w )$ and $\\mathrm { W M P } _ { i } [ d , j ]$ are the global and window mean payoff determined by $P a y _ { i }$ . Note that $W V A L ( w )$ corresponds to the “limit-average badness” of the window mean-payoffs along the run $w$ .\n\nClearly, $\\begin{array} { r l r } { \\mathbb { P } [ G V \\bar { A } L { = } g \\nu a l ] } & { { } = { } } & { 1 . } \\end{array}$ where gval $\\ l =$ $E \\nu a l ( g m p _ { 1 } , \\dots , g m p _ { k } )$ . Furthermore, by applying the ergodic theorem and the above observations leading to the definition of $\\mathrm { W M P } _ { i } [ d , s ]$ , we obtain that $\\mathbb { P } [ W V A L = w \\nu a l ] = 1$ where\n\n$$\nw \\nu a l = \\sum _ { s \\in S } \\mathbb { I } ( s ) \\cdot \\mathbb { E } \\big [ E \\nu a l ( \\operatorname { W M P } _ { 1 } [ d , s ] , \\dots , \\operatorname { W M P } _ { k } [ d , s ] ) \\big ]\n$$\n\nWe refer to gval and wval as the global and window mean payoff value. In this paper, we study the problem of minimizing wval in a given Markov decision process.\n\nMarkov decision processes. A Markov decision process $( M D P ) ^ { 1 }$ is a triple $\\textit { D } = \\textit { ( V , E , p ) }$ where $V$ is a finite set of vertices partitioned into subsets $( V _ { N } , V _ { S } )$ of nondeterministic and stochastic vertices, $E \\subseteq V \\times V$ is a set of edges s.t. each vertex has at least one out-going edge, and $p \\colon V _ { S } \\to D i s t ( V )$ is a probability assignment such that $p ( v ) ( v ^ { \\prime } ) { > } 0$ only if $( v , v ^ { \\prime } ) \\in E$ . We say that $D$ is a graph if $V _ { S } { = } \\emptyset$ .\n\nOutgoing edges in non-deterministic vertices are selected by a strategy. In this paper, we consider finite-memory randomized $( F R )$ strategies where the selection depends not only on the vertex currently visited but also on some finite information about the history of vertices visited previously.\n\nFR strategies. Let $D = ( V , E , p )$ be an MDP and $M { \\neq } \\emptyset$ a finite set of memory states that are used to “remember” some information about the history. For a given pair $( v , m )$ where $v$ is a currently visited vertex and $m$ a current memory state, a strategy randomly selects a new pair $( \\boldsymbol { v } ^ { \\prime } , \\boldsymbol { m } ^ { \\prime } )$ such that $( v , v ^ { \\prime } ) \\in E$ .\n\nFormally, let $\\alpha \\colon V \\to 2 ^ { M }$ be a memory allocation, and let $\\overline { { V } } ~ = ~ \\{ ( v , m ) ~ | ~ v ~ \\in ~ V , m ~ \\in ~ \\alpha ( v ) \\}$ be the set of augmented vertices. A finite-memory $( F R )$ strategy is a function $\\sigma \\colon \\overline { { V } }  D i s t ( \\overline { { V } } )$ such that for all $( v , m ) \\in \\overline { { V } }$ where $v \\in V _ { S }$ and every $( \\boldsymbol { \\dot { v } } , \\boldsymbol { \\dot { v ^ { \\prime } } } ) \\in E$ we have that\n\n$$\n\\sum _ { m ^ { \\prime } \\in \\alpha ( v ^ { \\prime } ) } \\sigma ( v , m ) ( v ^ { \\prime } , m ^ { \\prime } ) = p ( v ) ( v ^ { \\prime } ) .\n$$\n\nAn FR strategy is memoryless (or Markovian) if $M$ is a singleton. In the following, we use $\\overline { { v } }$ to denote an augmented vertex of the form $( v , m )$ for some $m \\in \\alpha ( v )$ .\n\nEvery FR strategy $\\sigma$ together with a probability distribution $\\overset { \\cdot } { \\mu } \\in \\mathit { D i s t } ( \\overline { { \\overline { { V } } } } )$ determine the Markov chain $D ^ { \\sigma } =$ $( \\overline { { V } } , P r o b , \\mu )$ where $P r o b ( \\overline { { v } } , \\overline { { u } } ) = \\sigma ( \\overline { { v } } ) ( \\overline { { u } } )$ .\n\n# The Optimization Problem\n\nIn this section, we define the multiple window mean-payoff optimization problem and examine the principal limits of its computational tractability.\n\nLet $D = ( V , E , p )$ be an MDP, $P a y _ { 1 } , \\ldots , P a y _ { k } : V {  } \\mathbb { N }$ payoff functions, and $E \\nu a l : \\mathbb { R } ^ { k }  \\mathbb { R }$ an evaluation function. Furthermore, let $d \\in \\mathbb { N }$ be a time horizon. The task is to construct an FR strategy $\\sigma$ and an initial augmented vertex so that the value of wval is minimized.\n\nMore precisely, for a given FR strategy $\\sigma$ , the function wval is evaluated as follows. Every $P a y _ { i }$ is extended to the augmented vertices of $D ^ { \\sigma }$ by defining $\\dot { P } a y _ { i } ( \\overline { { v } } ) = P a y _ { i } ( v )$ . Let $C _ { 1 } , \\ldots , C _ { n }$ be the BSCCs of the Markov chain $D ^ { \\sigma }$ . Recall that every $C _ { i }$ can be seen as an irreducible Markov chain. We use $w \\nu a l ( C _ { i } )$ to denote the window mean payoff value computed for $C _ { i }$ . The value of wval achieved by $\\sigma$ , denoted by $w \\nu a l ^ { \\sigma }$ , is defined as\n\n$$\nw \\nu a l ^ { \\sigma } ~ = ~ \\operatorname* { m i n } \\{ w \\nu a l ( C _ { i } ) \\mid 1 \\leq i \\leq n \\}\n$$\n\nRecall that the initial augmented vertex can be chosen freely, which is reflected in the above definition (the strategy $\\sigma$ can be initiated directly in the “best” BSCC and thus achieve the value $w \\nu a l ^ { \\sigma }$ ).\n\nA FR-strategy $\\sigma$ is $\\varepsilon$ -optimal for a given $\\varepsilon \\ge 0$ if\n\n$$\nw \\nu a l ^ { \\sigma } - \\varepsilon \\ \\leq \\ \\operatorname* { i n f } _ { \\pi } w \\nu a l ^ { \\pi }\n$$\n\nwhere $\\pi$ ranges over all FR strategies. A 0-optimal strategy is called optimal.\n\nThe next theorem shows that computing an optimal strategy is computationally hard, even for restricted subsets of instances.\n\nTheorem 1 Let $D = ( V , E , p )$ be a graph, $P a y _ { 1 } , \\ldots , P a y _ { k }$ payoff functions, $d \\leq | V |$ a time horizon, and Eval an evaluation function. The problem of whether there exists a $F R$ strategy $\\sigma$ such that wva $\\iota ^ { \\sigma } \\leq 0$ is NP-hard.\n\nFurthermore, the problem is NP-hard even if the set of eligible instances is restricted so that an optimal memoryless strategy is guaranteed to exist, and one of the following three conditions is satisfied:\n\nA. $k { = } 1$ (i.e., there is only one payoff function).   \nB. $k { = } 2$ , and there are thresholds $c _ { 1 } , c _ { 2 } \\geq 0$ such that $E \\nu a l ( \\kappa _ { 1 } , \\kappa _ { 2 } ) = 0$ iff $\\kappa _ { 1 } \\geq c _ { 1 }$ and $\\kappa _ { 2 } \\geq c _ { 2 }$ .   \n$C .$ . There is a constant $r \\in \\mathbb { N }$ such that $k \\ \\leq \\ | V | ^ { \\frac { 1 } { r } }$ (i.e., the number of payoff functions is “substantially smaller” than the number of vertices), and the co-domain of every $P a y _ { i }$ is $\\{ 0 , 1 \\}$ .\n\nIntuitively, (A) says that one payoff function is sufficient for NP-hardness, (B) says that for two payoff functions we have NP-hardness even if we just aim to push both window mean payoffs above certain thresholds, and (C) says that the range of all payoff functions can be restricted to $\\{ 0 , 1 \\}$ even if the number of payoff functions is at most $\\left| V \\right| ^ { \\frac { 1 } { r } }$ . Furthermore, the Eval function can be constructed so that it ranges over $\\{ - t , 0 \\}$ where $t ~ \\in ~ \\mathbb { N }$ is an arbitrary constant, and $w \\nu a l ^ { \\sigma } = 0$ iff $\\sigma$ is optimal. Consequently, an optimal (and even $t { - } 1$ -optimal) strategy cannot be constructed in polynomial time unless ${ \\mathsf { P } } = { \\mathsf { N P } }$ .\n\n# The Algorithm\n\nFor the rest of this section, we fix an MDP $D = ( V , E , p )$ , a time horizon $d$ , payoff functions $P a y _ { 1 } , \\ldots , P a y _ { k }$ , and an evaluation function $E \\nu a l$ .\n\nOur algorithm is based on optimizing wval by the methods of differentiable programming. The core ingredient is a dynamic procedure for computing the expected value of $E \\nu a l ( \\mathrm { W M P } _ { 1 } [ d , s ] , \\dotsc , \\mathrm { W M P } _ { k } [ d , s ] )$ (see (1)). The procedure is designed so that the gradient of wval can be computed using automatic differentiation. Then, we show how to incorporate this procedure into a strategy-improvement algorithm for wval. For the rest of this section, we fix a memory allocation α : V 2M .\n\nRepresenting FR Strategies. For every pair of augmented vertices $( \\overline { { v } } , \\overline { { u } } )$ such that $( { \\bar { v } } , u ) \\in E$ , we fix a real-valued parameter representing $\\sigma ( \\overline { { v } } ) ( \\overline { { u } } )$ . Note that if $\\overline { { v } }$ is stochastic, then the parameter actually represents the probability of selecting the memory state of $\\overline { { u } }$ . These parameters are initialized to random values, and we use the standard SOFTMAX function to transform these parameters into probability distributions. Thus, every function $F$ depending on $\\sigma$ becomes a function of the parameters, and we use $\\nabla F$ to denote the corresponding gradient.\n\nComputing wvalσ. Let $\\sigma$ be an FR strategy where $\\alpha$ is the memory allocation. We show how to compute $w \\nu a l ^ { \\sigma }$ interpreted as a function of the parameters representing $\\sigma$ .\n\nRecall that $w \\nu a l ^ { \\sigma } = \\operatorname* { m i n } \\{ w \\nu a l ( C _ { i } ) \\mid 1 \\leq i \\leq n \\}$ . Hence, the first step is to compute all BSCCs of $D ^ { \\sigma }$ by the Tarjan’s algorithm (Tarjan 1972). Then, for each BSCC $C$ , we compute $w { \\nu } a l ( C )$ in the following way.\n\nThe invariant distribution $\\mathbb { I } _ { C }$ is computed as the unique solution of the following system of linear equations: For every ${ \\overline { { v } } } \\in C$ , we fix a fresh variable $x _ { \\overline { { v } } }$ and the corresponding equation $\\begin{array} { r } { x _ { \\overline { { v } } } \\ = \\ \\sum _ { \\overline { { u } } \\in C } x _ { \\overline { { u } } } \\cdot \\sigma ( \\overline { { u } } ) ( \\overline { { v } } ) } \\end{array}$ . Furthermore, we add the equation $\\textstyle \\sum _ { { \\overline { { u } } } \\in C } x _ { \\overline { { u } } } = 1$ . Recall that $\\mathbb { I } _ { C }$ is the unique distribution satisfying $\\mathbb { I } _ { C } = \\mathbb { I } _ { C } \\cdot P r o b _ { C }$ , where $P r o b _ { C }$ is the probability matrix of $C$ . Hence, $\\mathbb { I } _ { C }$ is the unique solution of the constructed system.\n\nThe main challenge is to compute the expected value\n\n$$\n\\mathbb { E } \\big [ E \\nu a l ( \\mathrm { W M P } _ { 1 } [ d , \\overline { { v } } ^ { * } ] , \\dots , \\mathrm { W M P } _ { k } [ d , \\overline { { v } } ^ { * } ] ) \\big ]\n$$\n\nfor each $\\overline { { v } } ^ { * } \\in C$ . This is achieved by Algorithm 1 described below. Then, $w { \\nu } a l ( C )$ is calculated using (1).\n\nComputing the Expected Value of Eval by Dynamic Programming. In this section, we show how to compute (2) by dynamic programming for a given ${ \\overline { { v } } } ^ { * } \\in C$ . We use $\\mathbb { P }$ to denote the probability measure over the runs in $C$ , where the initial distribution assigns 1 to $\\overline { { v } } ^ { * }$ .\n\nLet $\\mathbb { N } ^ { C }$ be the set of vectors of non-negative integers indexed by the augmented vertices $\\overline { { v } } \\in C$ . For each $t \\in \\mathbb { N }$ , let $\\mathbb { N } _ { t } ^ { C } = \\left\\{ x \\in \\mathbb { N } ^ { \\breve { C } } \\mid \\ell _ { x } = t \\right\\}$ , where $\\begin{array} { r } { \\ell _ { x } = \\sum _ { \\overline { { u } } \\in C } x ( \\overline { { u } } ) } \\end{array}$ . For every $x \\in \\mathbb { N } ^ { C }$ , let $O c c _ { x }$ be an indicator assigning to every run $w = \\overline { { v } } _ { 0 } , \\overline { { v } } _ { 1 } , \\ldots$ of $C$ either 1 or 0 so that $O c c _ { x } ( w ) = 1$ iff $\\# _ { \\overline { { v } } } ( \\overline { { v } } _ { 0 } , \\ldots \\overline { { v } } _ { \\ell _ { x } - 1 } ) = x ( \\overline { { v } } )$ for every $\\overline { { v } } \\in C$ .\n\nFor every $\\boldsymbol { x } \\in \\mathbb { N } ^ { C }$ , let\n\n$$\n\\mathrm { W M P } _ { i } ( x ) = \\frac { \\sum _ { \\overline { { \\boldsymbol { v } } } \\in C } x ( \\overline { { \\boldsymbol { v } } } ) \\cdot P a y _ { i } ( \\overline { { \\boldsymbol { v } } } ) } { \\ell _ { x } } .\n$$\n\n# Algorithm 1: Evaluation via dynamic programming\n\nThen, (2) is equal to\n\n$$\n\\sum _ { x \\in \\mathbb { N } _ { d } ^ { C } } \\mathbb { P } [ O c c _ { x } = 1 ] \\cdot E \\nu a l ( \\mathrm { W M P } _ { 1 } ( x ) , \\dots , \\mathrm { W M P } _ { k } ( x ) ) .\n$$\n\nCalculating (3) directly is time-consuming. However, $E \\nu a l ( \\mathrm { W M P } _ { 1 } ( x ) , \\dots , \\mathrm { W M P } _ { k } ( x ) )$ is the same for many different $x \\in \\mathbb { N } _ { d } ^ { C }$ , and our dynamic algorithm avoids these redundant computations. The algorithm is applicable to a subclass of decomposable Eval functions defined below. The decomposability condition is not too restrictive, and it does not influence the NP-hardness of the considered optimization problem (the Eval functions constructed in the proof of Theorem 1 are decomposable).\n\nFor all x $, y \\in \\mathbb { N } ^ { C }$ and $\\overline { { v } } \\in C$ , we write $x  ^ { \\overline { { v } } } y$ if $y ( { \\overline { { v } } } ) =$ $x ( \\overline { { v } } ) + 1$ and $y ( \\overline { { u } } ) = x ( \\overline { { u } } )$ for all $\\overline { { u } } \\neq \\overline { { v } }$ . We say that Eval is decomposable if there is a set $R$ of representatives and efficiently computable functions $r \\colon \\mathbb { N } ^ { C } \\ { \\overset { \\cdot } { \\to } } \\ R$ , $e \\colon R  \\mathbb { R }$ and $m \\colon R \\times C \\to R$ satisfying the following conditions:\n\n• $E \\nu a l ( \\mathrm { W M P } _ { 1 } ( x ) , \\ldots , \\mathrm { W M P } _ { k } ( x ) ) = e ( r ( x ) )$ for every $x \\in \\dot { \\mathbb { N } } _ { d } ^ { C }$ , i.e., the value of Eval for a given $\\mathcal { x } \\in \\mathbb { N } _ { d } ^ { C }$ is efficiently computable by the function $e$ just from the representative of $x$ . • For each $x  ^ { \\overline { { v } } } y$ , we have that $r ( y ) = m ( r ( x ) , \\overline { { v } } )$ . That is, when a path is prolonged by a vertex $\\overline { { v } }$ , the representative can be efficiently updated by the function $m$ .\n\nA concrete example of a decomposable $E \\nu a l$ and the associated $r , e , m$ functions is given in a special subsection below.\n\nFor each $t \\in \\{ 1 , \\ldots , d \\}$ , let $R _ { t } \\hat { \\ } = \\ \\{ r ( x ) \\ | \\ x \\ \\in \\ \\mathbb { N } _ { t } ^ { C } \\}$ . Furthermore, for every representative $\\varrho \\in R _ { t }$ , let $\\mathbb { P } [ \\varrho ] =$ $\\textstyle \\sum _ { x \\in \\mathbb { N } _ { t } ^ { C } \\cap r ^ { - 1 } ( \\varrho ) } \\mathbb { P } [ O c c _ { x } = \\bar { 1 } ]$ . Then (3) can be rewritten into\n\n$$\n\\sum _ { \\varrho \\in { \\cal R } _ { d } } \\mathbb { P } [ \\varrho ] \\cdot e ( \\varrho ) .\n$$\n\nAlgorithm 1 computes $\\mathbb { P } [ \\varrho ]$ for all $t \\in \\{ 1 , \\ldots , d \\}$ and $\\varrho \\in R _ { t }$ by dynamic programming. Moreover, only reachable representatives (i.e., those with $\\mathbb { P } [ \\varrho ] > 0 )$ are considered during the computation. Thus, Algorithm 1 avoids the redundancies of the direct computation of (3).\n\nMore specifically, Algorithm 1 uses two associative arrays (such as $^ { C + + }$ unordered map), called $m a p _ { 0 }$ and $m a p _ { 1 }$ , to gather information about the representatives and the corresponding probabilities. In the $t$ -th iteration of the cycle, $m a p _ { 0 }$ contains items corresponding to all reachable $\\varrho \\in R _ { t }$ , and the items corresponding to all reachable $\\varrho \\in R _ { t + 1 }$ are gradually gathered in $m a p _ { 1 }$ . In particular, each $m a p _ { i }$ is indexed by the elements $( \\overline { { v } } , \\varrho ) \\in C \\times R$ . Intuitively, each isnuictihateldemine $\\overline { { v } } ^ { * }$ respurcehsetnhtas  the rse eoxfisatlsl $x \\in \\mathbb { N } _ { t + i } ^ { C }$ $w = \\overline { { v } } _ { 0 } , \\overline { { v } } _ { 1 } , \\ldots$ $r ( x ) = \\varrho$ , $O c c _ { x } ( w ) = 1$ , and $\\overline { { v } } _ { \\ell _ { x } - 1 } = \\overline { { v } }$ . The value associated to $( \\overline { { v } } , \\varrho )$ is the total probability of all such runs $w$ .\n\nAlgorithm 2: Evaluation via DFS   \n\n<html><body><table><tr><td colspan=\"2\">1:ifn<d then</td></tr><tr><td colspan=\"2\">2: foru∈Cdo</td></tr><tr><td>3:</td><td>DFS(u,p·o[u][u],n+1,payoff_vector+Pay(u))</td></tr><tr><td colspan=\"2\">4:else</td></tr><tr><td>5:</td><td>rsl+=p·Eval(payoff_vector)</td></tr></table></body></html>\n\nA Simple DFS Procedure. As a baseline for measuring the improvement achieved by the dynamic algorithm described in the previous paragraph, we use a simple DFS-based procedure of Algorithm 2. For simplicity, the vector $( P a y _ { 1 } \\hat { ( v ) } , \\ldots , P a y _ { k } ( \\overline { { v } } ) \\hat { ) }$ is denoted by $P a y ( \\overline { { v } } )$ .\n\nThe DFS procedure inputs the following parameters:\n\n• the current augmented vertex $\\overline { { v } }$ ;   \n• the probability $p$ of the current path;   \n• the length $n$ of the current path;   \n• the vector payoff vector of the individual payoffs accumulated along the path.\n\nThe procedure is called as $\\mathrm { D F S } ( \\overline { { v } } ^ { * } , 1 , 1 , P a y ( \\overline { { v } } ) )$ for each $\\overline { { v } } ^ { * }$ in the currently examined BSCC $C$ . At the end of the computation, the global variable rsl holds the value of (2).\n\nA Strategy Improvement Algorithm. In this section, we describe a strategy improvement algorithm WINMPSYNT that inputs an MDP $\\bar { \\textbf { \\textit { D } } } = \\mathbf { \\beta } ( V , \\bar { E } , p )$ , payoff functions $P a y _ { 1 } , \\ldots , P a y _ { k } : V  \\mathbb { N } .$ , a decomposable $\\dot { E } \\nu a l : \\mathbb { R } ^ { k }  \\mathbb { R }$ , and a time horizon $d \\in \\mathbb { N }$ , and computes an FR strategy $\\sigma$ with the aim of minimizing wvalσ.\n\nThe memory allocation function is a hyperparameter (by default, all memory states are assigned to every vertex). The algorithm proceeds by randomly choosing the parameters representing a strategy. The values are sampled from LOGUNIFORM distribution so that no prior knowledge about the solution is imposed. Then, the algorithm computes the BSCCs of $D ^ { \\sigma }$ and identifies a BSCC $C$ with the best $w { \\nu } a l ( C )$ . Subsequently, $w { \\nu } a l ( C )$ is improved by gradient descent. The crucial ingredient of WINMPSYNT is Algorithm 1, allowing to compute $w { \\nu } a l ( C )$ and its gradient by automatic differentiation. After that, the point representing the current strategy is updated in the direction of the steepest descent. The intermediate solutions and the corresponding $w { \\nu } a l ( C )$ values are stored, and the best solution found within STEPS optimization steps is returned (the value of STEPS is a hyper-parameter). Our implementation uses PYTORCH framework (Paszke et al. 2019) and its automatic differentiation with ADAM optimizer (Kingma and Ba 2015). Observe that WINMPSYNT is equally efficient for general MDPs and graphs. The only difference is that stochastic vertices generate fewer parameters.\n\n![](images/4324c13ab0a8952f7d9d0caf2d05ab75076f8569b421b50004ec2225c9c9ddae.jpg)  \nFigure 2: The graph $D _ { 6 }$ and the payoffs assigned to vertices.\n\nAn Example of a Decomposable Eval Function. Let Pay be a payoff function, and let $E \\nu a l : \\mathbb { R }  \\mathbb { R }$ where $E { \\nu } a l ( P )$ is either 0 or 1 depending on whether $P \\in [ 3 , 5 ]$ or not, respectively. Assume $d = 5$ . Then, we can put $\\bar { R ^ { \\prime } } = \\{ 0 , \\ldots , \\bar { 2 6 } \\}$ and define\n\n$$\n\\begin{array} { r c l } { { r ( x ) } } & { { = } } & { { \\displaystyle \\operatorname* { m i n } \\{ a , 2 6 \\} , \\mathrm { ~ w h e r e ~ } a = \\sum _ { \\overline { { { v } } } \\in C } x ( \\overline { { { v } } } ) \\cdot P a y ( \\overline { { { v } } } ) } } \\\\ { { } } & { { } } & { { } } \\\\ { { e ( \\varrho ) } } & { { = } } & { { \\displaystyle \\left\\{ \\begin{array} { l c } { { 0 } } & { { \\mathrm { i f ~ } \\varrho / d \\in [ 3 , 5 ] , } } \\\\ { { 1 } } & { { \\mathrm { o t h e r w i s e . } } } \\end{array} \\right. } } \\\\ { { m ( \\varrho , \\overline { { { v } } } ) } } & { { = } } & { { \\displaystyle \\operatorname* { m i n } \\{ \\varrho + P a y ( \\overline { { { v } } } ) , 2 6 \\} } } \\end{array}\n$$\n\nNote that as soon as the accumulated payoff exceeds 25, there is no reason to remember the exact value because Eval inevitably becomes one. Note that $R$ contains 27 elements independently of the size of $C$ , while the total number of all $x \\in \\mathbb { N } _ { 5 } ^ { C }$ such that $\\mathbb { P } [ O c c _ { x } = 1 ] > 0$ may exceed $\\binom { | C | } { 4 }$ , and the total number of paths, all of which are considered separately by the naive DFS-based algorithm, may reach $| C | ^ { 4 }$ .\n\n# Experiments\n\nWe perform our experiments on graphs to separate the probabilistic choice introduced by the constructed strategies from the internal probabilistic choice performed in stochastic vertices. The graphs are structurally similar to the ones constructed in the NP-hardness proof of Theorem 1. This avoids bias towards simple instances. Recall that the problem of constructing a (sub)optimal strategy for such graphs is NP-hard even if just one memory state is allocated to every vertex, there are only two payoff functions, and we aim at pushing the window mean payoffs above certain thresholds (see item B. in Theorem 1).\n\nThe graphs $D _ { \\ell }$ . For every $\\ell \\geq 2$ , we construct a directed ring $D _ { \\ell }$ with three “layers” where every vertex in the inner, middle, and outer layer is assigned a pair of payoffs $( 1 0 , 0 )$ , $( 2 , 2 )$ , and $( 0 , 1 0 )$ , respectively. The vertices are connected in the way shown in Fig. 2.\n\nThe Eval function. A scenario is a pair $( \\ell , d )$ where $\\ell , d$ are even integers in the interval [2, 20] representing $D _ { \\ell }$ and the window length $d$ . For every scenario, we aim to push both window mean payoffs simultaneously above a bound $b$ , where $b$ is as large as possible. For each $( \\ell , d )$ , the maximal bound achievable by an FR strategy is denoted by $b _ { \\ell , d }$ , and can be determined by a careful manual analysis, together with the least number of memory states $K _ { \\ell , d }$ required by an optimal FR strategy (we have that $K _ { \\ell , d } \\le 5$ for all $( \\ell , d ) )$ . Let us note that the manual analysis of $D _ { \\ell }$ is enabled by the regular structure of $D _ { \\ell }$ , but this regularity does not bring any advantage to WINMPSYNT.\n\nFor a given scenario $( \\ell , d )$ , we use the evaluation function $E \\nu a l _ { \\ell , d }$ defined as follows:\n\n$$\nE \\nu a l _ { \\ell , d } ( P _ { 1 } , P _ { 2 } ) = \\frac { \\operatorname* { m a x } \\{ 0 , b _ { \\ell , d } - P _ { 1 } \\} + \\operatorname* { m a x } \\{ b _ { \\ell , d } - P _ { 2 } \\} } { 2 \\cdot b _ { \\ell , d } } .\n$$\n\nBy the definition of $b _ { \\ell , d }$ , there always exists an optimal FR strategy $\\sigma$ for the scenario $( \\ell , d )$ achieving the bound $b _ { \\ell , d }$ , i.e., $w \\nu a l ^ { \\sigma } = 0$ . Due to the normalizing denominator, the maximal value of $E \\nu a l _ { \\ell , d }$ is bounded by 1, simplifying the comparison of strategies constructed for different scenarios.\n\nThe experiments. For all scenarios $( \\ell , d )$ and $\\textbf { \\textit { K } } \\in$ $\\{ 1 , \\ldots , 5 \\}$ , we invoked WINMPSYNT 100 times, where $K$ memory states are assigned to every vertex. The number of optimization steps is set to 1000. Thus, for all $( \\ell , d )$ and $K$ , we obtained a set $\\Sigma _ { \\ell , d , K }$ of 100 strategies and the corresponding values. We use $\\Sigma _ { \\ell , d }$ to denote $\\textstyle \\bigcup _ { K = 1 } ^ { 5 } \\sum _ { \\ell , d , K }$ , and Σ to denote the union of all Σℓ,d.\n\nThe quality of the obtained strategies. Due to the definition of $E \\nu a l _ { \\ell , d }$ , for every $\\sigma \\in \\Sigma _ { \\ell , d }$ , the value wvalσ can be interpreted as a normalized distance to the optimal strategy with value 0. The percentage of scenarios where the value of the best strategy found by WINMPSYNT is bounded by 0, 0.05, and 0.1 is $4 0 \\%$ , $5 2 \\%$ , and $1 0 0 \\%$ , respectively. Hence, the best strategies found by WINMPSYNT are of very high quality. Since the best strategy is selected out of 500 strategies constructed for a given scenario, a natural question is how good are these strategies “on average”. The percentage of all $\\sigma \\in \\Sigma$ whose value is bounded by 0, 0.1, and 0.3 is $6 \\%$ , $2 7 \\%$ , and $9 9 \\%$ , respectively. Hence, the quality of an “average” strategy is substantially worse, which is consistent with intuitive expectations (since the problem is computationally hard, obtaining a high-quality solution for nontrivial instances cannot be easy).\n\nThe roles of memory states and randomization are demonstrated in Fig. 3. The scenarios are split into five disjoint subsets with the same $K _ { \\ell , d }$ (horizontal axis). For each subset, we report the values achieved by strategies with $1 , \\ldots , 5$ memory states assigned to every vertex (indicated by different colors). Note that for the subset where $K _ { \\ell , d } = 2$ , an optimal strategy is computed when 2 or more memory states are available. For the subset where $K _ { \\ell , d } = 3$ , an optimal strategy is found only for 5 memory states. For all subsets, increasing the number of memory states decreases the average strategy value. Furthermore, even if the number of memory states is smaller than $K _ { \\ell , d }$ , the value of the constructed strategies is still relatively small on average. Hence, randomization effectively compensates for the lack of memory.\n\nThe improvement achieved by dynamic programming. The baseline for evaluating the efficiency improvement\n\n![](images/11ad835d49b3fea62a0d70d2875478801a3b7960ee9f8f49f2b0b8b59f5c5c68.jpg)  \nFigure 3: More memory states lead to better strategies.   \nFigure 4: Dynamic evaluation procedure outperforms the DFS-based one.\n\nAverage train step time (log scale) 4 5 timeout (20 s) 4 3 2 3 2 2 0   \n-2 -4   \n-86 EvalDuaPtion algorithmDFS   \n-10 4 6 8 10 12 14 16 18 20 22 24 26 28 30 Both window length (d) and ring size (l)\n\nachieved by the dynamic procedure of Algorithm 1 is the simple DFS-based procedure of Algorithm 2. For every instance $( \\ell , d )$ where $4 \\leq \\ell = d \\leq 3 0$ , we report the average running time of one training step for an FR strategy with $1 , \\ldots , 5$ memory states (different colors) using logarithmic scale. The timeout is set to 20 secs. For one memory state, the DFS-based procedure reaches the timeout for all scenarios $( \\ell , d )$ where $\\ell { = } d \\leq 2 0$ , whereas the dynamic procedure needs less than one second even for the (30, 30) scenario. Hence, the dynamic procedure substantially outperforms the DFS-based one, and the same holds when the number of memory states increases.\n\n# Conclusions\n\nWe have designed an efficient strategy synthesis algorithm for optimizing multiple window mean payoffs capable of producing high-quality solutions for instances of considerable size. An interesting question is whether the proposed approach is applicable to a larger class of window-based optimization objectives such as the window parity objectives.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了在马尔可夫决策过程（MDP）中，优化多个窗口平均收益（window mean payoffs）的问题，同时满足局部稳定性约束。\\n> *   该问题的重要性在于，传统的平均收益优化方法无法保证在有限时间窗口内的稳定性，可能导致实际应用中的灾难性后果（如公司现金流问题）。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种基于动态规划和可微分编程的高效策略合成算法，通过最小化评估函数（Eval）的期望值来优化窗口平均收益。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **首个高效且可扩展的算法：** 设计了第一个适用于MDP的多窗口平均收益优化算法，解决了现有方法无法处理的问题。\\n> *   **动态规划与梯度计算：** 提出了一种可微分的动态规划方法，能够高效计算局部平均收益的分布及其梯度。\\n> *   **实验验证：** 在结构化图上验证了算法的有效性，展示了随机化策略在内存不足时的补偿能力。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   通过动态规划计算局部平均收益的分布，并利用梯度下降优化策略，以最小化评估函数（Eval）的期望值。\\n> *   设计哲学在于通过可微分的动态规划方法，将复杂的多目标优化问题转化为可计算的梯度优化问题。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 先前工作主要集中在计算复杂性分类上，缺乏实际可用的高效算法。\\n> *   **本文的改进：** 提出了一个可扩展的动态规划方法，能够处理多窗口平均收益优化，并通过随机化策略弥补内存不足的问题。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **初始化策略参数：** 随机初始化策略参数，使用SOFTMAX函数将其转换为概率分布。\\n> 2.  **计算BSCCs：** 使用Tarjan算法计算马尔可夫链的强连通分量（BSCCs）。\\n> 3.  **动态规划计算：** 对每个BSCC，使用动态规划方法计算局部平均收益的分布及其梯度。\\n> 4.  **梯度下降优化：** 利用自动微分和ADAM优化器更新策略参数，以最小化评估函数的期望值。\\n\\n> **案例解析 (Case Study)**\\n> *   论文提供了一个简单图例（Example 1），展示了随机化策略在内存不足时如何优于确定性策略。例如，当内存状态数为1时，随机化策略的期望评估值为3.5，而确定性策略为2。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   DFS-based procedure（深度优先搜索基础方法）\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在计算效率上：** 本文方法的动态规划算法在窗口长度为30时，单次训练步骤时间小于1秒，显著优于DFS基础方法（超时20秒）。\\n> *   **在策略质量上：** 本文方法在40%的场景中找到了最优策略（评估值为0），在52%的场景中评估值小于0.05。\\n> *   **在内存利用上：** 随机化策略在内存状态数不足时，仍能接近最优策略的性能，例如在内存状态数为1时，评估值平均为0.3。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   窗口平均收益 (Window Mean Payoff, WMP)\\n*   马尔可夫决策过程 (Markov Decision Process, MDP)\\n*   动态规划 (Dynamic Programming, DP)\\n*   可微分编程 (Differentiable Programming, N/A)\\n*   策略合成 (Strategy Synthesis, N/A)\\n*   局部稳定性 (Local Stability, N/A)\\n*   随机化策略 (Randomized Strategy, N/A)\\n*   有限内存 (Finite Memory, N/A)\"\n}\n```"
}