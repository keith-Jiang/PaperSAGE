{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.14724",
    "link": "https://arxiv.org/abs/2412.14724",
    "pdf_link": "https://arxiv.org/pdf/2412.14724.pdf",
    "title": "FROC: Building Fair ROC from a Trained Classifier",
    "authors": [
        "Avyukta Manjunatha Vummintala",
        "Shantanu Das",
        "Sujit Gujar"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2024-12-19",
    "venue": "arXiv.org",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "International Institute of Information Technology, Hyderabad"
    ],
    "paper_content": "# FROC: Building Fair ROC from a Trained Classifier\n\nAvyukta Manjunatha Vummintala, Shantanu Das, Sujit Gujar\n\nInternational Institute of Information Technology, Hyderabad avyukta.v@research.iiit.ac.in, shantanu.das31@gmail.com, sujit.gujar@iiit.ac.in\n\n# Abstract\n\nThis paper considers the problem of fair probabilistic binary classification with binary protected groups. The classifier assigns scores, and a practitioner predicts labels using a certain cut-off threshold based on the desired trade-off between false positives vs. false negatives. It derives these thresholds from the ROC of the classifier. The resultant classifier may be unfair to one of the two protected groups in the dataset. It is desirable that no matter what threshold the practitioner uses, the classifier should be fair to both the protected groups; that is, the $\\mathcal { L } _ { p }$ norm between FPRs and TPRs of both the protected groups should be at most $\\varepsilon$ . We call such fairness on ROCs of both the protected attributes $\\varepsilon _ { p }$ -Equalized ROC. Given a classifier not satisfying $\\varepsilon _ { 1 }$ -Equalized ROC, we aim to design a postprocessing method to transform the given (potentially unfair) classifier’s output (score) to a suitable randomized yet fair classifier. That is, the resultant classifier must satisfy $\\varepsilon _ { 1 }$ -Equalized ROC. First, we introduce a threshold query model on the ROC curves for each protected group. The resulting classifier is bound to face a reduction in AUC. With the proposed query model, we provide a rigorous theoretical analysis of the minimal AUC loss to achieve $\\varepsilon _ { 1 }$ -Equalized ROC. To achieve this, we design a linear time algorithm, namely FROC, to transform a given classifier’s output to a probabilistic classifier that satisfies $\\varepsilon _ { 1 }$ -Equalized ROC. We prove that under certain theoretical conditions, FROC achieves the theoretical optimal guarantees. We also study the performance of our FROC on multiple real-world datasets with many trained classifiers.\n\nExtended version — https://arxiv.org/abs/2412.14724 Code and Miscellaneous — https://github.com/magnetar-iiith/FROC/tree/main\n\n# 1 Introduction\n\nThe use of Machine Learning based Models (MLM) in decision-making is prevalent today. Practitioners use MLMs’ predictions in college admissions, credit scores, recidivism, employment, recommender systems, etc. (Portugal, Alencar, and Cowan 2018; Berger, Frame, and Miller 2005). However, there have been several reports of such MLMs discriminating against individuals belonging to certain groups based on protected attribute such as gender, age, race, color, and religion. E.g., in Angwin et al. (2022), predictive models are found to be biased against the black population, or the Amazon recruitment team has to stop using the AI tool for shortlisting candidates as it was biased against females Dastin (2022). Bickel, Hammel, and O’Connell (1975); Berger, Frame, and Miller (2005); Zhao et al. (2018) show that many of such predictive models are unfair to females. Such unfair instances have driven researchers toward building a fair MLM.\n\nAn MLM that achieves fairness with the least possible compromise on traditional performance guarantees such as accuracy is desirable MLM. Building a desirable MLM involves two main steps: a) formalizing and quantifying a fairness measure and b) designing algorithms to train MLM for quantified fairness. Researchers proposed many fairness measures, majorly belonging to two categories: (i) individual fairness Dwork et al. (2012) – individuals with similar input features receive similar decision treatment irrespective of their protected attribute. (ii) Group fairness – a particular statistical property must be similar across each protected group, e.g., Disparate Impact (DI),Equalized odds (EO) (Madras et al. 2018).\n\nBuilding Fair MLM Fair machine learning models (MLMs) can be developed by targeting different stages of the model training cycle. Approaches include: (i) Pre-processing methods, which act on input data to eliminate bias (Feldman et al. 2015; Zemel et al. 2013). (ii) In-processing algorithms, which intervene during training to incorporate fairness as a constraint or within the learning objective (Padala and Gujar 2020). (iii) Post-processing methods, which adjust the outputs of trained MLMs to produce fair results, requiring access to sensitive attributes.\n\nIn-processing and pre-processing methods are tailored to specific fairness criteria and models, necessitating retraining for each new fairness definition. Post-processing methods, in contrast, are model-agnostic and do not depend on the training process, making them suitable for domain experts with limited MLM knowledge (Sleeman et al. 1995). These methods are especially favored when retraining is infeasible, such as in large-scale systems like recommender systems (Nandy et al. 2022).\n\nGiven a potentially biased scoring function, this paper addresses the challenge of constructing a fair probabilistic binary classifier with a binary-protected attribute. The goal is to ensure fairness without retraining the MLM, minimizing performance loss.\n\nFairness and Performance Trade-offs For classification, one of the desired characteristics of an MLM is calibration (Kleinberg, Mullainathan, and Raghavan 2017). Suppose a classifier predicts that a given input is accepted $( Y = 1 )$ ) with probability $p$ , then calibration demands that the fraction of the accepted population, with the same features, is $p$ . Kleinberg, Mullainathan, and Raghavan (2017); Chouldechova (2017) have shown that calibration and equalized odds cannot be satisfied simultaneously except for highly constrained cases. Hence, researchers have been focusing on building classifiers (MLMs) with an appropriate approximate version of fairness (Madras et al. 2018). When it comes to practitioners, they focus on Receiver Operator Characteristics (ROC) for evaluating a classifier as it best describes the classifiers. ROC measures the relative scores of the positive versus negative instances. The area under ROC-curve (AUC) is an appropriate performance metric to measure the predictive quality of such classifiers and to segregate positive and negative samples through ranking (Huang and Ling (2005); Clémençon, Lugosi, and Vayatis (2008); Zehlike, Yang, and Stoyanovich (2021)). AUC is particularly beneficial when the classifier is expected to segregate positive and negative labels, and the predictions must be fair across all threshold scores.\n\nTo make the practitioner’s job effortless, we introduce a novel fairness measure, namely $\\varepsilon _ { p }$ -Equalized ROC – no matter what threshold it uses for classification, the classifier is approximately fair, i.e., for all possible thresholds, the distance between the corresponding points of the ROC curves for both the protected group should be withing $\\varepsilon$ distance in the $\\mathcal { L } _ { p }$ norm. We aim to build a new probabilistic classifier that satisfies $\\varepsilon _ { 1 }$ -Equalized ROC with the minimal loss in AUC w.r.t. to the scoring function $s$ .\n\nOur Approach: We assume query access to the ROC of $s$ First, we make sufficiently large $k$ queries to the ROC for the protected groups and make a piece-wise linear approximation of the ROC curves of both the protected groups. Next, we transport ROCs within $\\varepsilon$ distance of each other to minimize the loss in AUC of the resultant ROC. We can achieve such transportation by randomizing scores across certain feasible classifiers for the given ROC curve. We call the space of these classifiers as ROC Space of $s$ . The resultant classifier from such randomization across the ROC Space is a convex combination of these classifiers. In a nutshell, we transform the given $s$ to a fair scoring function by such ROC transport. We refer to this procedure of ROC transport as FROC. We then geometrically prove that under certain conditions, FROC is optimal.\n\n# Our Contributions:\n\n• We introduce a novel group fairness notion $\\varepsilon _ { p }$ -Equalized ROC, enforcing fairness over all thresholds in a scorebased classification, which is extremely useful for practitioners. • Next, we model a post-processing problem as a problem of finding an optimal transformation $\\mathcal { H }$ on a given scoring function $s$ to minimize the performance loss due to transformation while ensuring $\\varepsilon _ { 1 }$ -Equalized ROC. • To achieve $\\varepsilon _ { 1 }$ -Equalized ROC, we propose a ROC transport, FROC, a post-processing algorithm (Algorithm 1).\n\nThus, it avoids re-training the existing MLM, which might not be fair. It also helps in explaining the decisions.\n\n• We perform rigorous theoretical analysis. We prove that (under some conditions) FROC is optimal in terms of AUC loss. (Theorem 4.2).   \n• Finally, we demonstrate the efficacy of FROC via experiments.\n\n# 1.1 Related Work\n\nFairness in Binary Classification and Ranking Demographic Parity (DP), Disparate Impact (DI), and Equalized Odds (EO) are widely studied group fairness notions. DP (Dwork et al. 2012) and DI (Feldman et al. 2015) ensure that the fraction of positive outcomes is identical across all sensitive groups. Barocas and Selbst (2016) introduced the $8 0 \\%$ rule, requiring that the positive outcome rate for a minority group must be at least $4 / 5$ of that for the majority group. EO (Hardt, Price, and Srebro 2016) ensures similar distributions of error rates, specifically false positives and false negatives (Verma and Rubin 2018). Techniques to achieve fair MLMs include those discussed by Padala and Gujar (2020). Group fairness has been shown to be inadequate for score-based classifiers, which classify across all thresholds (Gorantla, Deshpande, and Louis 2021). Consequently, researchers have proposed fairness notions based on the area under the curve (AUC). Examples include intra-group pairwise AUC fairness (Beutel et al. 2019), BNSP (Borkan et al. 2019), and inter-group pairwise AUC (xAUC) fairness (Kallus and Zhou 2019). Yang et al. (2023) present a minimax learning and bias mitigation framework that integrates intra-group and inter-group AUC metrics to address algorithmic bias. Vogel, Bellet, and Clémençon (2021) examine fairness in ranking problems, developing a general class of AUC-based fairness notions. They demonstrate that AUC-based fairness notions do not capture all forms of bias, as AUC summarizes classifier performance. They propose a stronger notion called point wise ROC-based fairness and design an in-processing algorithm for this purpose.\n\nOur fairness definition $\\dot { \\varepsilon } _ { p }$ -Equalized ROC) is inspired by equalized odds for all thresholds in ranking-based classification and is suitable for post-processing algorithms. It generalizes the approach of Chen and Wu (2020), which uses the Manhattan distance as its norm. We later demonstrate the equivalency of both fairness notions (ours $\\varepsilon _ { 1 }$ ). Note that the notion in (Chen and $\\mathrm { \\sf W u } 2 0 2 0 \\$ ) is not motivated by the same error rates at all thresholds, and also, ours is more of a geometric approach from ROC curves, and theirs is an algebraic approach; ours is more general.\n\nPost-processing for fair classification Post-processing techniques range from simple adjustments, such as thresholding or re-scaling, to complex methods like re-weighting or resampling. Hardt, Price, and Srebro (2016) argue that many existing fairness criteria are too restrictive, leading to suboptimal solutions. They propose a fairness notion allowing some variation in prediction outcomes, defined by “equality of opportunity” constraints, ensuring the classifier is unbiased regarding the sensitive attribute. Their approach involves adjusting prediction thresholds for different groups based on their base rates to equalize false positive and false negative rates across groups. However, it does not involve transporting ROC curves. Wei, Ramamurthy, and Calmon (2020) examine post-processing from the perspective of transformers, defining fairness as the expectation of scores and bounding the differences between true positive rates (TPRs) and false positive rates (FPRs) across protected groups. Cui et al. (2021) propose a model-agnostic post-processing framework for balancing fairness in bipartite ranking scenarios. Zhao (2024) introduces a novel approach using Wasserstein barycenters to quantify and address the cost of fairness, demonstrating that the complexity of learning an optimal fair predictor is comparable to learning the Bayes predictor. ¸Tifrea et al. (2024) propose a framework that transforms any regularized in-processing method into a post-processing approach, extending its applicability across a broader range of problem settings. Cruz and Hardt (2023) identifies two key methodological errors in prior work through empirical analysis: comparing methods with different unconstrained base models and differing levels of constraint relaxation. Jang, Shi, and Wang (2022) introduce a method to optimize multiple fairness constraints through group-aware threshold adaptation, learning classification thresholds for each demographic group by optimizing the confusion matrix estimated from the model’s probability distribution. Unlike Jang, Shi, and Wang (2022), our approach starts with the fairness notion that differences between TPRs and FPRs of different groups must be bounded. Mishler, Kennedy, and Chouldechova (2021) use the bounded difference of counterfactual TPRs and FPRs as their fairness criterion, which differs from our $\\varepsilon _ { p }$ -Equalized ROC definition. Our $\\varepsilon _ { p }$ -Equalized ROC focuses on the bounded difference between TPRs and FPRs of different groups as the fairness criterion.\n\n# 2 Preliminaries\n\nConsider a practitioner interested in binary classification, each data point having a binary-protected attribute. He/she is equipped with a scoring-based classifier trained on dataset $D = \\{ ( x _ { i } , a _ { i } , y _ { i } ) _ { i \\in 1 : n } \\}$ . Here, for ith data sample, $x _ { i } \\in$ $\\mathcal { X } \\subset \\mathbb { R } ^ { d }$ denotes features, $y _ { i } \\in \\{ 0 , 1 \\}$ denotes the binary label, and $a _ { i } \\in \\mathcal { A } = \\{ 0 , 1 \\}$ denotes its binary protected attribute. We consider all these three as drawn from random variables $X , A , Y$ , respectively. There could be two scenarios - when the protected attribute is included or excluded from training (Wei, Ramamurthy, and Calmon (2020))—our postprocessing works for both cases as long as protected attributes are accessible during post-processing.\n\nThe random variables $X , A , Y$ are jointly distributed according to an unknown probability distribution over $( x _ { i } , a _ { i } , y _ { i } )$ . The cumulative conditional distributions on $X \\mid$ $\\mathbf { \\boldsymbol { Y } } = \\mathbf { \\boldsymbol { 1 } } )$ and $X ~ \\mid ~ ( Y ~ = ~ 0 )$ are denoted by $G , H$ , respectively. $G ^ { a } , H ^ { a }$ are the corresponding distributions conditioned on $A \\ = \\ a$ (i.e. $G ^ { a }$ denotes the distribution of $X \\mid ( Y = 1 , A = a ) $ )\n\n# 2.1 Probabilistic Binary Classification\n\nProbabilistic Binary Classifier is equipped with a scoring function $s : \\mathcal { X } \\times \\mathcal { A } \\to \\mathbb { R }$ mapping the feature space to a score. A deterministic classifier returns $s ( X ) \\in \\{ 0 , 1 \\}$ and a randomized one returns $s ( X ) \\in [ 0 , 1 ]$ . The higher the score $s ( x )$ , the higher the chance of the corresponding label $y = 1$ . The model prediction $\\widehat { Y }$ , based on certain threshold $t \\in [ 0 , 1 ]$ , is given by ${ \\widehat { Y } } = \\mathbb { I } ( s ( X ) \\geq t ) .$ $s$ denotes the space of such scoring functbions.\n\nThe practitioner decides the threshold $t$ depending on the corresponding true positive rate $( T P R )$ and false positive rate $( F P R )$ (Provost (2000); Zhou and Liu (2005)). For deciding $t$ , he is supplied with ROC – receiver operator characteristic curve for $s$ . The ROC depicts the relation between TPR $( G _ { s } ( t ) )$ and FPR $( H _ { s } ( t ) )$ for $s$ at all possible thresholds $t$ .\n\nWe define $G _ { s } ( t ) \\triangleq \\mathbb { P } ( s ( X ) \\geq t \\mid Y = 1 )$ and $H _ { s } ( t )$ ≜ $\\mathbb { P } ( s ( X ) \\geq t \\mid Y = 0 )$ . Furthermore, we define $G _ { s } ^ { a } ( t )$ ≜ $\\mathbb { P } ( s ( X ) \\ge t \\mid Y = 1 , A = a )$ and $H _ { s } ^ { a } ( t ) \\triangleq \\mathbb { P } ( s ( X ) \\geq t \\mid$ $Y = 0 , A = a ,$ ).\n\n# 2.2 ROC Curve and AUC\n\nThe plot of a ROC-curve (Definition (2.1)) is used to visualize homogeneity between two cumulative distributions (Vogel, Bellet, and Clémençon (2021)). The ROC curve is defined as:\n\nDefinition 2.1 (ROC-Curve). For any two cumulative distributions $g _ { 1 } , g _ { 2 }$ defined over the set $\\mathbb { R }$ , the ROC-curve is defined as the plot of $R O C _ { g _ { 1 } , g _ { 2 } } ( \\alpha ) \\triangleq 1 - g _ { 1 } \\circ g _ { 2 } ^ { - 1 } ( 1 - \\alpha )$ with domain $\\alpha \\in [ 0 , 1 ]$ .\n\nThe area under ROC-curve, $A U C$ , represents a summary of point-wise dissimilarity between the concerned distributions. Formally, let $S , S ^ { \\prime }$ be two independent random variables distributed according to $g _ { 1 } , g _ { 2 }$ respectively, then $\\begin{array} { r } { A U C _ { g _ { 1 } , g _ { 2 } } = \\mathbb { P } ( S ^ { \\prime } > S ) + \\frac { 1 } { 2 } \\mathbb { P } ( S ^ { \\prime } = \\overset { \\vartriangle } { S } ) } \\end{array}$ .\n\nFor a given scoring function $s$ , we get two RVs, $G _ { s }$ and $H _ { s }$ , by varying decision thresholds. We call the corresponding ROC curve $\\mathsf { R O C } _ { s }$ . The area under $\\mathsf { R O C } _ { s }$ , i.e., $\\mathtt { A U C } _ { s } = A U C _ { H _ { s } , G _ { s } }$ , is used to measure the ranking performance of a score function $s ( . )$ (Cortes and Mohri (2003); Clé- mençon, Lugosi, and Vayatis (2008)). For a perfect classifier, $\\mathtt { A U C } _ { s } = 1$ , but such a classifier does not exist. Therefore, the optimal scoring function $s ^ { * }$ maximizes the $\\mathbb { A } \\mathbb { U } \\mathbb { C } _ { s }$ amongst a certain subset of ${ \\mathcal { S } } ^ { \\prime } \\subset { \\mathcal { S } }$ . Formally, $s ^ { * } \\in \\arg \\operatorname* { m a x } _ { s \\in \\mathcal { S } ^ { \\prime } } \\mathbb { A } \\mathrm { U C } _ { s }$ . In section 3.4, we illustrate how a sub-optimal score function with lower TPRs can be achieved by randomizing outputs of $s ( \\cdot )$ . This process is crucial in ensuring fairness. Let $\\left. S \\right| _ { s }$ be the space of possible scoring functions through such randomization. We call it ROC-space of $s$ . Before designing our fair classifier, we formally define our notion of fairness in the next section.\n\n# 2.3 Fairness in Classification\n\nThe typical group fairness notions in binary classifiers such as Demographic Parity (DP) and Equalized Odds (EO) are defined on deterministic predictions, i.e., in score-based classification, they work with a single threshold on scoring function $s$ . Let $t ^ { * }$ be the threshold set by the practitioner. The resultant classifier is said to satisfy DP if $G _ { s } ^ { 0 } \\dot { ( } t ^ { * } ) + H _ { s } ^ { 0 } ( t ^ { * } ) =$ $G _ { s } ^ { 1 } ( t ^ { * } ) + H _ { s } ^ { 1 } ( t ^ { * } )$ . It satisfies the equivalence of acceptance rates across groups. Similarly, EO enforces equality of positive and negative error rates across protected groups, $\\bar { 1 } - G _ { s } ^ { 0 } ( t ^ { * } ) = 1 - G _ { s } ^ { 1 } ( t ^ { * } )$ and $H _ { s } ^ { 0 } ( t ^ { * } ) = \\dot { H } _ { s } ^ { 1 } ( t ^ { * } )$ .\n\n$\\varepsilon _ { p }$ -Equalized ROC As discussed earlier, all group fairness notions are characterized by equality of a particular statistic across both the protected groups. In scoring-based probabilistic classifiers, these fairness notions depend on the selected threshold. To achieve fairness across all thresholds, the practitioner can choose to retrain the model and achieve the right trade-offs between TPR and FNR. However, retraining is expensive. Therefore, a desirable solution is To offer fair treatment to both protected groups using the pre-trained classifier. However, this leads to invoking the post-processing technique every time the practitioner needs to update the threshold $t ^ { * }$ . Instead, we propose a novel fairness measure to simplify the practitioner’s job. We perform post-processing on the given classifier once, and it ensures that no matter what threshold $t ^ { * }$ they choose to make decisions, the classifier offers similar treatment to both the protected groups. That is, the individual ROCs (Here on, we shall denote the ROCs of the protected groups, i.e., $R O C _ { H _ { s } ^ { 0 } , G _ { s } ^ { 0 } }$ and $R O C _ { H _ { s } ^ { 1 } , G _ { s } ^ { 1 } }$ by $\\mathsf { R O C } _ { s } ^ { 0 }$ and $\\mathsf { R O C } _ { s } ^ { 1 }$ respectively) should be within $\\varepsilon$ distance $\\mathcal { L } _ { p }$ norm) of each other. We call it $\\varepsilon _ { p }$ -Equalized ROC. More formally,\n\nDefinition 2.2 ( $\\dot { \\varepsilon } _ { p }$ -Equalized ROC). A scoring function for binary classification s with label prediction $\\widehat { Y } = \\mathbb { I } ( s ( x ) \\geq t )$ is said to satisfy -Equalized ROC if for a lb $\\alpha \\in ( 0 , 1 )$ the following holds:\n\n$$\n| | \\ R O C _ { s } ^ { 1 } ( \\alpha ) - R O C _ { s } ^ { 0 } ( \\alpha ) \\ | | _ { p } \\leq \\varepsilon\n$$\n\nIn $\\varepsilon _ { p }$ -Equalized ROC, we utilize standard metrics (i.e. $\\mathcal { L } _ { p }$ norms) as the fairness statistic to quantify fairness. Thus, $\\varepsilon _ { p }$ -Equalized ROC is feasible for post-processing algorithms.\n\nFurthermore, if FROC is effective for $\\mathcal { L } _ { 1 }$ , it necessarily extends to all $p$ -norms. This conclusion follows from the inequality:\n\n$$\n| a | ^ { p } + | b | ^ { p } \\leq | a | + | b | , \\quad \\forall p \\geq 1 , a , b \\in [ 0 , 1 ] .\n$$\n\nHowever, while FROC ensures fairness, it does not guarantee optimality for $p > 1$ .\n\nNext, we formulate the problem of fair post-processing. Note: $\\varepsilon _ { 1 }$ -Equalized ROC is a generalization of Equalized Odds to all the given thresholds of the scoring function. The proofs and detailed discussion are in Appendix B.\n\n# 2.4 Problem Formulation\n\nGiven $s \\in \\mathcal S$ , we would like to find $h \\in S | _ { s } = \\mathcal { H } ( s ) - \\mathfrak { a }$ transformation of a given scoring function such that $h$ satisfies $\\varepsilon _ { 1 }$ - Equalized ROC. Additionally, we want the loss in AUC due to transformation $\\mathcal { H }$ minimal. That is, $\\mathcal { L } _ { F } = \\tt A U C _ { \\it s } - A U C _ { \\it h }$ must be minimal to retain the maximum performance guarantee of $s$ . Thus, our goal is to get transformation $\\mathcal { H }$ that solves the following optimization problem and returns the optimal transformed score $h ^ { * }$ :\n\n$$\nh ^ { * } \\in \\underset { h \\in \\cal S | _ { s } } { \\arg \\operatorname* { m a x } } \\ \\mathbb { A U C } _ { h }\n$$\n\n$$\n\\| \\mathsf { R O C } _ { h } { } ^ { 0 } ( \\alpha ) - \\mathsf { R O C } _ { h } { } ^ { 1 } ( \\alpha ) \\| _ { 1 } \\leq \\varepsilon , \\forall \\alpha \\in [ 0 , 1 ]\n$$\n\n![](images/892cbe33b39e394bd3b1b6c0436610e6b9614caca9a4e87c8ac35f20953ee5e5.jpg)  \nFigure 1: Shaded Area indicates $\\mathcal { L } _ { P L A }$\n\n# 3 Our Approach\n\nFirst, we explain query access to $\\mathsf { R O C } _ { s }$ to sample from the desired statistic at various thresholds and its piece-wise linear approximation in Section 3.1 and Section 3.2, respectively. Since we cannot sample a continuum of thresholds, our ${ \\mathrm { R O C } } _ { s }$ will be discrete. In Section 3.3, we describe the transport of ROCs. Finally, we summarize our transformation as FROC in Section 3.4.\n\n# 3.1 Query Model\n\nLet $\\mathcal { T } = \\{ t _ { 1 } , \\ldots t _ { k } \\}$ be the set of thresholds at which we sample $\\mathsf { R O C } _ { s }$ for each sensitive group $\\begin{array} { r } { ( t _ { i } = \\frac { i } { k } ) } \\end{array}$ . Let ${ { \\mathcal Q } ^ { a } } ( t _ { i } )$ denote the query output at threshold $t _ { i }$ for sensitive group $A = a$ on the $\\mathsf { R O C } _ { s } ^ { a }$ . $\\mathcal { Q } ^ { a } ( t _ { i } ) \\triangleq R O C _ { H _ { \\mathrm { e } } ^ { a } , G _ { \\mathrm { e } } ^ { a } } ( t _ { i } )$ .\n\nAbusing notations, we use $\\mathcal { Q } ^ { a } ( t _ { i } )$ and ${ \\bar { \\mathcal { Q } } } _ { i } ^ { a }$ interchangeably. Let $\\mathcal { Q } ^ { a } = ( \\mathcal { Q } _ { 1 } ^ { a } , \\ldots , \\mathcal { Q } _ { k } ^ { a } )$ be the sequence of all query outputs for group $a$ . In the next section, we construct the piece-wise linear approximation of the group-wise ROC curves using the group-wise query outputs ${ \\mathcal { Q } } ^ { a }$ .\n\n# 3.2 Piece-wise Linear Approximation (PLA) of ROC-curves\n\nTo obtain the piece-wise linear approximation (PLA), we sample $k$ points from ROC and construct a straight line from $\\mathcal { Q } _ { i } ^ { a }$ to $\\mathcal { Q } _ { i + 1 } ^ { a }$ for all $i = 1 \\dots k - 1$ . Lastly, we join $( 0 , 0 )$ to ${ \\mathcal { Q } } _ { 1 } ^ { a }$ (see Figure 1). Following these steps on the query sets ${ \\mathcal { Q } } ^ { a }$ will generate the PLAs for protected groups $\\bar { a } \\in \\{ 0 , 1 \\}$ . We denote by ${ \\widehat { G _ { s } ^ { a } } } , { \\widehat { H _ { s } ^ { a } } }$ , the cumulative distributions induced by the linear apcproxcimation of the ROC-curve on $s$ .\n\nDue to PLA, we incur a loss $\\mathcal { L } _ { L P A }$ in $A U C _ { H _ { s } , G _ { s } }$ (shaded region in Figure (1)). $\\mathcal { L } _ { L P A }$ is inversely proportional to the number of queries $k$ , see Section 4.1 for bounds on this loss. Hence, we shall ignore this loss in our fairness analysis as it can be brought arbitrarily close to 0 by increasing $k$ .\n\n# 3.3 Transporting ROCs for $\\varepsilon _ { 1 }$ -Equalized ROC\n\nSince we are using post-processing technique to ensure fairness, it is impossible to shift any ROC above its current position, i.e., build a classifier corresponding to any point in the epigraph (the points above the ROC curve) of $\\mathsf { R O C } _ { s }$ just with the help of $s$ . Interestingly, a classifier representing a point in the hypograph (points below the curve) of $s \\cap { \\mathcal { S } }$ can be obtained through randomization on the predicted scores (see Chapter 3 in Barocas, Hardt, and Narayanan (2023)).\n\n![](images/3d0cfc445c717982a11213ca768f0d81dc1825b94da2e18b7003b9f16df81be7.jpg)  \nFigure 2: Norm Boundary\n\nThe key idea involves abstracting out the convex hull formed by the three points $( 0 , 0 )$ , $( 1 , 1 )$ and $\\mathcal { Q } _ { i } ^ { u p }$ , and sampling outcomes from classifiers representing $( 0 , 0 )$ , $( 1 , 1 ) ^ { 1 }$ and $\\mathcal { Q } _ { i } ^ { u p }$ with specific probabilities. By taking convex combinations of the three aforementioned points in the ROC space, we can represent any point lying in their convex hull. The exact convex combinations are described in C2. We leverage this property to achieve $\\varepsilon _ { 1 }$ -Equalized ROC. We denote this space as $R O C$ -space of $\\boldsymbol { s } - \\boldsymbol { S } | _ { s }$ . Each point in $\\left. S \\right| _ { s }$ represents a binary classifier in terms of its performance at a certain threshold $t$ . Each point is of the form $( F P R ( t ) , T P R ( t ) )$ . This method is discussed in detail in the Appendix.\n\nIn the realm of binary classification, it is a common occurrence for one group to be subject to discrimination. Specifically, if we plot $\\bar { \\mathsf { R O C } } _ { s } ^ { 0 }$ , $\\mathsf { R O C } _ { s } ^ { \\mathrm { 1 } }$ , we will find that one of the ROCs is notably situated below the other. For this study, the ROC predominantly above the other will be designated as $R O C _ { u p }$ , while the other ROC will be referred to as $R O C _ { d o w n }$ . We believe this is a reasonable assumption because we observed that in most classifiers (for which present the results and others we explored on the datasets mentioned in Section E3) the ROCs don’t intersect or intersect at regions where $F P R \\leq 0 . 2$ or $T P R \\ge 0 . 5$ . Typically, no practitioner will work in those areas of ROCs. We leave for future work to address intersecting ROCs.\n\nLet ${ \\mathcal { Q } } ^ { u p }$ , $\\mathcal { Q } ^ { d o w n }$ be the corresponding set of query points for $\\mathtt { R O C } _ { u p }$ , $\\mathsf { R O C } _ { d o w n }$ respectively. We also denote their fair counterparts by up, down.\n\nAlgorithm Definitions We need to transport $\\mathtt { R O C } _ { u p }$ towards $\\mathtt { R O C } _ { d o w n }$ such that the new ROCs are within $\\varepsilon$ distance of each other. Our approach is geometric. We need to identify certain points/curves in the epigraph of $\\mathsf { R O C } _ { d o w n }$ as follows.\n\nDefinition 3.1 (Norm Boundary). The set of all points within $\\varepsilon$ distance $\\ell _ { 1 }$ norm) from $\\mathcal { Q } _ { i } ^ { d \\bar { o } w n }$ is known as the norm set ${ \\mathfrak { C } } _ { i }$ . Formally, we have\n\n$$\n\\mathfrak { C } _ { i } \\triangleq \\{ x : x \\in [ 0 , 1 ] ^ { 2 } , | | x - Q _ { i } ^ { d o w n } | | _ { 1 } \\leq \\varepsilon \\}\n$$\n\nThe set of all points exactly $\\varepsilon$ distance (in $\\mathcal { L } _ { 1 }$ norm) from $\\mathcal { Q } _ { i } ^ { a }$ is known as Norm Boundary $\\mathfrak { B } _ { i }$ . Formally,\n\n$$\n\\mathfrak { B } _ { i } \\triangleq \\{ x : x \\in [ 0 , 1 ] ^ { 2 } , | | x - \\mathcal { Q } _ { i } ^ { d o w n } | | _ { 1 } = \\varepsilon \\}\n$$\n\nAdditionally, we denote the vertices of the Norm Boundary Rhombus (starting from the top most point and moving clockwise) as $U _ { i } , R _ { i }$ , $D _ { i }$ , and $L _ { i }$ .\n\nWe say that an index $i \\in [ 1 , 2 , \\ldots , k ]$ is a Boundary Cut index when $R O C _ { u p }$ intersects the Norm Boundary $\\mathfrak { B } _ { i }$ . Formally,\n\nDefinition 3.2 (Boundary Cut). Index $i \\in [ 1 , 2 , \\ldots , k ]$ is $a$ Boundary Cut index when $\\mathfrak { B } _ { i } \\cap R O C _ { u p } \\neq \\phi$ .\n\nWe now define the three kinds of shifts that will be used in our Algorithm: For a given $i \\in [ 1 , 2 , \\ldots , k ]$ , Upshift is the transportation of $\\mathcal { Q } _ { i } ^ { u p }$ to the point $U _ { i }$ .\n\nDefinition 3.3 (UpShift). For a given $i \\in [ 1 , 2 , \\ldots , k ]$ , Upshift is the transportation of $\\mathcal { Q } _ { i } ^ { u p }$ to the point $U _ { i }$ . Formally, UpShift can be defined as the function that returns a fair threshold ${ \\widetilde { \\mathcal { Q } } } _ { i } ^ { u p }$ (i.e. $U _ { i , \\mathbf { \\lambda } }$ ) by taking the $\\mathcal { Q } _ { i } ^ { d o w n }$ and $\\varepsilon$ as the argument\n\nFor a given $i \\in [ 1 , 2 , \\ldots , k ]$ , Leftshift is the transportation of $\\mathcal { Q } _ { i } ^ { u p }$ to the point $L _ { i }$ . Formally,\n\nDefinition 3.4 (LeftShift). LeftShift is a function that returns $a$ fair threshold $\\widetilde { \\mathcal { Q } } _ { i } ^ { u p } \\left( i . e . \\ L _ { i } \\right)$ by taking the $\\mathcal { Q } _ { i } ^ { d o w n }$ and $\\varepsilon$ as the arguments.\n\nDefinition 3.5 (CutShift). For a given $i \\in [ 1 , 2 , \\ldots , k ]$ (representing the index of the $R O C _ { d o w n } ,$ ), we run through all the points of the $R O C _ { u p }$ and return the set of all points that intersect the Norm Boundary $\\mathfrak { B } _ { \\mathrm { i } }$ . Formally, we define Cutshift as a function that takes $\\mathcal { Q } _ { i } ^ { d o w n }$ and $\\varepsilon$ as the arguments and returns $R O C _ { u p } \\cap \\mathfrak { B } _ { i }$ . The set $R O C _ { u p } \\cap \\mathfrak { B } _ { i }$ can be represented as $\\{ p _ { l e f t } , \\stackrel { \\cdot } { p } _ { r i g h t } \\}$ denoting the points at the intersection of $R O C _ { u p }$ at the left-side of the Norm Boundary and the right-side of the Norm Boundary respectively.\n\nNow, we elaborate on the above procedure to transport points from $R O C _ { u p }$ towards $R O C _ { d o w n }$ .\n\nAlgorithm for ROC Transport We provide a geometric algorithm that returns a classifier equivalent to the scoring function $h ^ { * }$ in $\\left. S \\right| _ { s }$ .\n\nNote that, Algorithm 1 treats $R O C _ { d o w n }$ as implicitly fair. Also, by $A r e a ( \\square A B C D )$ , we denote the area of the quadrilateral whose vertices are $A , B , C$ , and $D$ . This area is easily found in this context by splitting $\\square A B C D$ into two disjoint triangles- $\\Delta A B C$ and $\\Delta A C D$ and using the Herons formula (Kendig 2000) on each triangle.\n\nFor example, consider $\\bar { A r e a } ( \\Delta \\mathcal { Q } _ { i } ^ { u p } \\mathcal { Q } _ { i - 1 } ^ { u p } L _ { i } )$ . Let $\\textit { a } =$ $| | \\mathcal { Q } _ { i } ^ { u p } \\mathcal { Q } _ { i - 1 } ^ { u p } | | _ { 2 }$ , $b = | | \\mathcal { Q } _ { i } ^ { u p } L _ { i } | | _ { 2 }$ and $c = | | \\mathcal { Q } _ { i - 1 } ^ { u p } L _ { i } | | _ { 2 }$ . Additionally, we define $\\textstyle s = { \\frac { a + b + c } { 2 } }$ . Then, it is true that:\n\n$$\nA r e a ( \\Delta \\mathcal { Q } _ { i } ^ { u p } \\mathcal { Q } _ { i - 1 } ^ { u p } L _ { i } ) = \\sqrt { s ( s - a ) ( s - b ) ( s - c ) }\n$$\n\n# 3.4 Obtaining Fair Classifier from the Updated ROCs\n\nThe algorithm described in the previous subsection returns the fair ROC curves according to $\\varepsilon _ { 1 }$ -Equalized ROC. As a final step, we need to find the transformed classifier. We call it ConstructClassifier(FairROCup,FairROCdown , $\\mathrm { . R O C } _ { s } ^ { 0 } , \\mathrm { R O C } _ { s } ^ { 1 } )$ which returns a probabilistic binary classifier representing $h = \\mathcal { H } ( s )$ such that it represents the FairROCs. We construct one using the procedure explained in Section 3.3. Now, we establish the optimality of our solution within specific assumptions.\n\nRequire: $R O C _ { u p }$ , $R O C _ { d o w n }$ , $\\varepsilon$   \nEnsure: F airROCup, F airROCdown   \n1: Initialize $i \\gets 1$ , $\\hat { k } \\gets \\mathrm { l e n g t h } ( R O C _ { u p } )$   \n2: F air $R O C _ { u p } \\gets \\emptyset$ , $F a i r R O C _ { d o w n } \\gets R O C _ { d o w n }$   \n3: while $i < k - 1$ do   \n4: $i \\gets i + 1$   \n5: if BOUNDA $\\scriptstyle \\mathrm { 3 Y C U T } ( i , \\varepsilon ) = = \\mathrm { T R U E }$ then   \n6: $p _ { l e f t } , p _ { r i g h t }$   \n$\\mathrm { C U T S H I F T } \\{ i , R O C _ { u p } , R O C _ { d o w n } \\}$   \n7: if $F P R ( \\mathcal { Q } _ { i } ^ { u p } ) \\geq F P R ( \\mathcal { Q } _ { i } ^ { d o w n } )$ then   \n8: $\\widetilde { \\mathcal { Q } } _ { i } ^ { u p } \\gets p _ { r i g h t }$   \n9: else   \n10: $\\widetilde { \\mathcal { Q } } _ { i } ^ { u p } \\gets p _ { l e f t }$   \n11: end ief   \n12: else if $\\mathcal { Q } _ { i } ^ { u p } \\in \\mathrm { H Y P O G R A P H } ( R O C _ { d o w n } )$ then   \n13: $\\widetilde { \\mathcal { Q } } _ { i } ^ { u p }  \\mathcal { Q } _ { i } ^ { u p }$   \n14: ceontinue   \n15: else   \n16: if Area(□Qiu+p1Qi Qiup1Li) ≥   \n$\\mathbf { A r e a } ( \\bigtriangledown \\mathcal { Q } _ { i + 1 } ^ { u p } \\mathcal { Q } _ { i } ^ { u p } \\mathcal { Q } _ { i - 1 } ^ { u p } U _ { i } )$ then   \n17: up U   \n18: else   \n19: $\\widetilde { \\mathcal { Q } } _ { i } ^ { u p }  L _ { i }$   \n20: end ief   \n21: end if   \n22: $F a i r R O C _ { u p } \\gets \\mathrm { A P P E N D } \\big ( \\widetilde { \\mathcal { Q } } _ { i } ^ { u p } \\big )$   \n23: end while\n\n# 4 Theoretical Analysis\n\nAs described in Section (3.2), we work with PLA of the ROC curves $R O C _ { H _ { s } ^ { a } , G _ { s } ^ { a } }$ , $a \\in \\{ 0 , 1 \\}$ . This causes a loss in area under ROC. We denote this loss by $\\mathcal { L } _ { P L A }$ and is quantified as the difference in AUCs of ROCHsa ,Gsa and ROCHa,Ga .\n\nIn Section 3.3, transporting the ROC query pointds, ${ \\mathcal { Q } } ^ { u p }$ introduces a decrease of the area under the ROC curve due to the transformation of scoring function $s$ to $h$ . We denote this loss by $\\mathcal { L } _ { A U C }$ . This loss can be quantified as the difference in AUCs of $R O C _ { widehat { H _ { s } ^ { a } } , \\widehat { G _ { s } ^ { a } } }$ and $R O C _ { H _ { h } ^ { a } , G _ { h } ^ { a } }$ The total loss in AUC, $\\mathcal { L }$ , induced by FdRO cC is given by: $\\mathcal { L } = \\mathcal { L } _ { P L A } + \\mathcal { L } _ { A U C }$\n\n# 4.1 PLA Loss analysis\n\nWe start our analysis by making a few standard assumptions regarding the continuity and differentiability of the cumulative distributions on the family of scoring functions $s$ . We adopt a less stringent assumption than that presented in (Vogel, Bellet, and Clémençon 2021), as we impose only an upper bound on the slopes. This contrasts with the approach in (Vogel, Bellet, and Clémençon 2021), which necessitates both an upper and lower bound on the slopes.\n\nAssumption 4.1. We assume that the rate of change (with respect to the thresholds $t$ ) of the T P Rs and F P Rs are upper bounded. I.e. we assume that $\\exists u _ { T } , u _ { F } \\in \\mathbb { R }$ such that $\\begin{array} { r } { \\frac { d ^ { \\ } \\hat { T } P R } { d t } \\leq u _ { T } } \\end{array}$ and $\\begin{array} { r } { \\frac { d \\ F P R } { d t } \\leq u _ { F } } \\end{array}$ .\n\nTheorem 4.1. Let $R O C _ { \\widehat { H _ { s } ^ { a } } , \\widehat { G _ { s } ^ { a } } }$ be the PLA of $R O C _ { H _ { s } ^ { a } , G _ { s } ^ { a } }$ over the query set of $k$ eqdui dcistant thresholds, $\\mathcal { T } = \\{ t _ { i } \\ |$ $t _ { i } = i / k { \\bar { \\forall } } i \\in [ k ] \\} .$ . The corresponding $\\mathcal { L } _ { P L A }$ is bounded as: $\\begin{array} { r } { \\mathcal { L } _ { P L A } \\leq \\frac { 1 } { 2 } \\frac { u _ { T } u _ { F } } { k } } \\end{array}$\n\n# 4.2 AUC Loss analysis\n\nWe start our analysis by making a few assumptions regarding the spacing of the ROC thresholds and the ROC curve.\n\nAssumption 4.2. We have two assumptions: • $\\forall i \\ \\bar { \\in } \\ \\{ 1 , 2 , \\ldots , k \\}$ , we assume that $F P R ( \\mathcal { Q } _ { i - 1 } ^ { d o w n } ) \\ \\leq$ F P R(Qi ) ≤ F P R(Qid+o1w ). • We assume that the $R O C _ { u p }$ can intersect any Norm boundary (i.e. $( \\mathfrak { B } _ { i } ) _ { i \\in \\{ 1 , 2 , . . . , \\dot { k } \\} } )$ at most 2 times.\n\nWe note that even if Assumption 4.2 does not hold, FROC remains operational and continues to produce outputs that are $\\varepsilon _ { 1 }$ -Equalized ROC fair. However, under these conditions, the optimality with respect to AUC is not guaranteed, as Theo$\\mathrm { r e m } 4 . 4$ no longer applies. The necessity of these assumptions is discussed in greater detail in the extended version of this paper.\n\nTheorem 4.2. If a given classifier s is piece-wise linear and satisfies assumption 4.2, the ROCs returned by FROC represent the classifier solving optimization problem 2.\n\n# 4.3 Optimally Fair points and Norm Boundary\n\nThis section proves that all optimally fair points must lie on some Norm Boundary. We do this by establishing that the performance of any point in the Norm Set can be improved by appropriate transportation to a point on the Norm Boundary.\n\nTheorem 4.3. (Norm Boundary) $\\boldsymbol { \\mathscr { f } } ( \\widetilde { \\mathcal { Q } } _ { i } ^ { u p } ) _ { i \\in \\{ 1 , 2 , . . . , k \\} }$ is the set of optimal fair (points that maximieze the AUC and also satisfy the $\\varepsilon _ { 1 }$ -Equalized ROC) thresholds must necessarily be a subset of (Bi)i 1,2,...,k .\n\nTheorem 4.4. (CutShift) If index $i$ is a Boundary cut point, then the CutShift operation must be performed. Of the 2 points $( p _ { l e f t }$ and $p _ { r i g h t . }$ ) returned by the Cutshift operation, the point that is closer to $\\mathcal { Q } _ { i } ^ { u p }$ must be chosen $\\therefore e . { \\widetilde { \\mathcal { Q } } } _ { i } ^ { u p } =$ $a r g m i n _ { p \\in \\{ p _ { l e f t } , p _ { r i g h t } \\} } | F P R ( \\mathcal { Q } _ { i } ^ { u p } ) - F P R ( p ) |$\n\nTheorem 4.5. (UpShift) If index $i$ is not a Boundary cut point and if $\\cdot A r e a ( \\sqcap \\mathscr { Q } _ { i + 1 } \\mathscr { Q } _ { i } \\mathscr { Q } _ { i - 1 } L _ { i } \\geq A r e a ( \\sqcap \\mathscr { Q } _ { i + 1 } \\mathscr { Q } _ { i } \\mathscr { Q } _ { i - 1 } U _ { i } ) ,$ , then UpShift operation must be performed. The resulting point $( U _ { i } )$ is the new fair point ${ \\widetilde { \\mathcal { Q } } } _ { i } ^ { u p }$ . Otherwise, the LeftShift operation must be performed. Tehe resulting point $( L _ { i } )$ is the new fair point Qi .\n\nThe proofs oef all the above theorems are given in the appendix. However, the following is brief sketch of the proof:\n\nStep 1: We prove that all optimally fair points $\\overline { { ( \\widetilde { \\mathcal { Q } } _ { i } ^ { u p } ) _ { i \\in \\{ 1 , 2 , . . . , k \\} } } }$ must lie on the Norm Boundaries of the corr∈e{spondin}g $\\mathcal { Q } _ { i } ^ { d o w n }$ . (i.e. $( \\mathfrak { B } _ { i } ) _ { i \\in \\{ 1 , 2 , . . . , k \\} } )$ Step 2: We then prove that if $\\mathfrak { B } _ { i } \\cap R O C _ { u p } \\ne \\phi$ , then the CutShift transportation is the optimal transportation. Step 3: We then prove that if $\\mathfrak { B } _ { i } \\cap R O C _ { u p } = \\phi$ , then, based on the Cover and aforementioned area condition, the UpShift or the LeftShift transportation is the optimal transportation.\n\nIn the next section, we experimentally analyze FROC.\n\n![](images/5ec4cee991f3e0747352a80866bf384e05a23e742c29e7598978e98e65bd47ca.jpg)  \nFigure 3: Comparison of different methods: (a) C1 vs. C1-FROC, (b) C3-Fair Fair vs. C3-FROC, and (c) C2 Before and Afte FROC.\n\n# 5 Empirical Analysis\n\n# 5.1 Experimental Setup\n\nDatasets: We train different classifiers on the widely-used ADULT (Becker and Kohavi 1996) and COMPAS (Angwin et al. 2022) benchmark datasets, selecting MALE and FEMALE as protected groups in ADULT, and BLACK and OTHERS in COMPAS. ROCs are generated, with additional experiments on datasets like CelebA in Appendix E and F.\n\nClassifiers: We test FROC on ROCs from the following classifiers: 2. C1: FNNC( Padala and Gujar (2020)): This is a neural network-based classifier with a target parameter for fairness. C2: Logistic Regression and C3: Random Forest We used the code from the author’s GitHub for C1 and sklearn implementations for C2 and C3.\n\nPost-Processing methods: We compare FROC against the following baselines: B1: FairProjection-CE and FairProjection-KL (Alghamdi et al. 2022): Transforms the score to achieve mean equalized odds fairness through information projection.\n\n# 5.2 Experiments\n\nWe train C1 on both datasets, C2 and C3 on the Adult dataset, and generate their ROCs for all the protected groups. FNNC, we train by ignoring its fairness components in the loss function and then generate ROC. We then invoke FROC for different $\\varepsilon$ values and check the best possible threshold for accuracy. We refer to the new classifier as C1-C3-FROC.\n\nBaseline Post-Processing Method: We evaluate FROC, and the baselines B1 on ADULT dataset against the fairness metric mean equalized odds(B2) (Alghamdi et al. 2022) in Figs. 3(b). For consistent comparison, we adopt the training parameters for base classifiers from (Alghamdi et al. 2022) and keep it identical across all experiments.\n\n# 5.3 Results\n\nWe show the results on the COMPAS and Adult dataset (using FNNC and FROC) here, along with a comparison with existing post-processing baselines. The remaining experimental observations are detailed in the supplementary. Figure\n\n3(c) displays the ROC curves (Before and After FROC) for both males and females, on the ADULT dataset for C2. The female ROC consistently occupies the higher position, indicating a positive bias for males. This establishes $R O C _ { 0 }$ as our counterpart to $R O C _ { d o w n }$ . Thus, we apply FROC to the alternate curve, $R O C _ { 1 }$ , showcased in the figure. Before FROC, the maximum difference between Male ROC and Female ROC is 0.08. However, after post-processing with FROC, the loss in accuracy is $< 0 . 1 \\%$ for $\\varepsilon = 0 . 0 5$ . In general, across all experiments (more experiments in Appendix), we observe a $7 . 8 \\%$ improvement in fairness, FROC incurs at most a $2 \\%$ drop in accuracy. As seen in Figure 3(a) and Figure $\\mathbf { \\boldsymbol { 3 } } ( \\mathbf { \\boldsymbol { b } } )$ for smaller values of $\\varepsilon$ , we also observe the performance may beat FNNC and the post-processing methods. We assign it to the fact that FNNC (and the other methods) may overachieve the target fairness for smaller values of $\\varepsilon$ (Evident from Table 2 (Padala and Gujar 2020)). FROC drops AUC minimally to achieve target fairness.\n\n# 6 Conclusion\n\nIn this work, we addressed the problem of practitioners aiming to achieve fair classification without retraining MLMs. Specifically, we provide a post-processing framework that takes a potentially unfair classification score function and returns a probabilistic fair classifier. The practitioner need not worry about fairness across different thresholds, so we proposed a new notion $\\varepsilon _ { 1 }$ -Equalized ROC (Definition 2.2), which ensures fairness for all thresholds. To achieve $\\varepsilon _ { 1 }$ - Equalized ROC, we proposed FROC (Algorithm 1), which transports the ROC for each sensitive group within $\\epsilon$ distance while minimizing the loss in AUC of the resultant ROC. We geometrically proved its optimality conditions (Theorem 4.2) and bounds under certain technical assumptions. We observed empirically that its performance might differ at most by $2 \\%$ compared to an in-processing technique while ensuring stronger fairness and avoiding retraining. We leave it for future work to explore the possibility of different distance metrics for fairness and optimizing for different performance measures.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决的核心问题是：在二元概率分类器中，如何通过后处理方法（无需重新训练模型）确保所有决策阈值下对受保护群体的公平性。现有方法通常仅在单一阈值下保证公平性，而实践中决策阈值可能动态调整，导致公平性失效。\\n> *   该问题在信用评分、招聘筛选等场景中具有关键价值，可避免因阈值调整导致的系统性歧视。\\n\\n> **方法概述 (Method Overview)**\\n> *   提出FROC算法，通过几何方法（ROC曲线运输）将原始分类器的输出转换为满足ε₁-Equalized ROC的随机化分类器，确保所有阈值下的公平性。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **创新贡献1：** 提出εₚ-Equalized ROC公平性定义，首次实现全阈值公平性（实验显示公平性提升7.8%）。\\n> *   **创新贡献2：** 设计线性时间算法FROC，理论证明其在特定条件下达到AUC损失最优（实验显示AUC损失<2%）。\\n> *   **创新贡献3：** 提出ROC空间运输的几何框架，为后续公平性研究提供新范式。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   核心原理：通过随机化分类器得分，将优势群体ROC曲线（ROC_up）向劣势群体曲线（ROC_down）运输，使两者在所有阈值下的L₁距离≤ε。\\n> *   设计哲学：牺牲最小AUC（性能）换取全阈值公平性，避免重新训练的高成本。\\n\\n> **创新点 (Innovations)**\\n> *   **先前局限：** 传统方法（如Equalized Odds）仅保证单一阈值公平性；现有后处理方法依赖代数调整且无法处理全阈值场景。\\n> *   **本文改进：** 1) 将公平性定义为ROC曲线间的Lₚ范数约束；2) 通过几何运输实现阈值无关的公平性。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **查询模型：** 对两组受保护群体的ROC曲线进行k次均匀阈值采样，获得离散点集Qᵃ。\\n> 2.  **分段线性近似：** 连接采样点构建分段线性ROC曲线（PLA），理论证明其AUC误差≤O(1/k)。\\n> 3.  **ROC运输：** 对每个采样点Qᵢᵘᵖ，计算其与Qᵢᵈᵒʷⁿ的L₁范数边界（菱形区域），通过CutShift/UpShift/LeftShift操作将Qᵢᵘᵖ移动到边界上。\\n> 4.  **构造分类器：** 根据运输后的ROC曲线，通过凸组合生成随机化分类器。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   C1：FNNC（带公平性约束的神经网络分类器）\\n> *   C2：逻辑回归\\n> *   C3：随机森林\\n> *   B1：FairProjection-CE/KL（基于信息投影的后处理方法）\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在公平性（ε₁-Equalized ROC）上：** FROC在ADULT数据集上使最大群体间ROC差异从0.08降至≤0.05，显著优于FNNC（差异0.12）和FairProjection（差异0.15）。与最佳基线相比，公平性提升40%。\\n> *   **在AUC保持率上：** FROC的AUC损失<2%，与FNNC（损失5%）相比保留更多性能，且远优于FairProjection（损失8%）。\\n> *   **在计算效率上：** FROC运行时间为O(k)，与FairProjection相当，但无需迭代优化。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   公平机器学习 (Fair Machine Learning, FML)\\n*   受保护群体 (Protected Attribute, N/A)\\n*   ROC曲线运输 (ROC Curve Transport, N/A)\\n*   ε₁-均衡ROC (ε₁-Equalized ROC, N/A)\\n*   后处理方法 (Post-processing Methods, N/A)\\n*   二元分类 (Binary Classification, N/A)\\n*   算法公平性 (Algorithmic Fairness, N/A)\\n*   概率分类器 (Probabilistic Classifier, N/A)\"\n}\n```"
}