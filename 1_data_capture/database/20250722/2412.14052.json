{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.14052",
    "link": "https://arxiv.org/abs/2412.14052",
    "pdf_link": "https://arxiv.org/pdf/2412.14052.pdf",
    "title": "Neural Combinatorial Optimization for Stochastic Flexible Job Shop Scheduling Problems",
    "authors": [
        "Igor G. Smit",
        "Yaoxin Wu",
        "Pavel Troubil",
        "Yingqian Zhang",
        "Wim P.M. Nuijten"
    ],
    "categories": [
        "cs.AI",
        "cs.LG"
    ],
    "publication_date": "2024-12-18",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science",
        "Mathematics"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Eindhoven University of Technology",
        "Eindhoven Artificial Intelligence Systems Institute",
        "Delmia R&D",
        "Dassault SystÃ¨mes"
    ],
    "paper_content": "# Neural Combinatorial Optimization for Stochastic Flexible Job Shop Scheduling Problems\n\nIgor G. Smit1, 2, Yaoxin $\\mathbf { W _ { u } } 2 , 3 *$ , Pavel Troubil4, Yingqian Zhang2, 3, Wim P.M. Nuijten1, 2\n\n1Department of Mathematics and Computer Science, Eindhoven University of Technology 2Eindhoven Artificial Intelligence Systems Institute, Eindhoven University of Technology 3Department of Industrial Engineering and Innovation Sciences, Eindhoven University of Technology 4Delmia R&D, Dassault SystÃ¨mes i.g.smit@tue.nl, y.wu2 $@$ tue.nl, pavel.troubil $@$ 3ds.com, yqzhang@tue.nl, w.p.m.nuijten@tue.nl\n\n# Abstract\n\nNeural combinatorial optimization (NCO) has gained significant attention due to the potential of deep learning to efficiently solve combinatorial optimization problems. NCO has been widely applied to job shop scheduling problems (JSPs) with the current focus predominantly on deterministic problems. In this paper, we propose a novel attention-based scenario processing module (SPM) to extend NCO methods for solving stochastic JSPs. Our approach explicitly incorporates stochastic information by an attention mechanism that captures the embedding of sampled scenarios (i.e., an approximation of stochasticity). Fed with the embedding, the base neural network is intervened by the attended scenarios, which accordingly learns an effective policy under stochasticity. We also propose a training paradigm that works harmoniously with either the expected makespan or Value-at-Risk objective. Results demonstrate that our approach outperforms existing learning and non-learning methods for the flexible JSP problem with stochastic processing times on a variety of instances. In addition, our approach holds significant generalizability to varied numbers of scenarios and disparate distributions.\n\n# Code â€” https://github.com/ai-for-decision-makingtue/NCO-for-Stochastic-FJSP\n\n# Introduction\n\nCombinatorial optimization (CO) problems are prevalent in various application areas, posing significant challenges due to their complex nature and computational intractability. One critical CO problem is the flexible job shop scheduling problem (FJSP), which has wide applicability ranging from semiconductor manufacturing (Tamssaouet et al. 2022) to healthcare scheduling (Burdett and Kozan 2018) and aluminum production (Zhang, Yang, and Zhou 2016). Traditional approaches, such as constraint programming (Baptiste, Pape, and Nuijten 2001; Col and Teppan 2022), heuristics (Sels, Gheysen, and Vanhoucke 2012), or metaheuristics (Rooyani and Defersha 2019) have made great progress in solving these problems. However, these methods generally assume deterministic problems. In practice, oppositely, there is an abundance of uncertainty, leading to vulnerable plans produced by these optimization methods.\n\nIn recent years, increasing focus has been directed to stochastic optimization methods. However, these methods remain largely underrepresented, especially for more complex optimization problems such as the FJSP. Existing methods solving the stochastic FJSP, such as a simulationoptimization framework (Ghaedy-Heidary et al. 2024) or a meta-heuristic approach including Monte-Carlo sampling (Flores-GÃ³mez, Borodin, and DauzÃ¨re-PÃ©rÃ¨s 2023) add significant computational complexity to already expensive approaches, while also being tailored to specific use-cases and objectives, limiting their potential for adoption.\n\nA recent branch of optimization technology is constituted by neural combinatorial optimization (NCO) methods, leveraging deep (reinforcement) learning (DRL) to solve CO problems (Bengio, Lodi, and Prouvost 2021). They learn high-quality optimization policies that fit a set of problem instances, eliminating the need for handcrafted expert rules, while also being efficient. Current NCO methods mainly target deterministic optimization, overlooking their practical use in stochastic situations. It is highly beneficial to extend NCO to the stochastic optimization domain.\n\nSeveral existing works leverage learning-based methods for stochastic routing or scheduling. However, these are generally focused on online or dynamic scenarios. Conversely, in many practical cases, such as manufacturing plants, a one-time schedule is needed in advance. Moreover, existing methods only focus on implicitly learning stochastic dynamics through the Markov decision process (MDP). In stochastic optimization, on the contrary, the assumed distributions can be explicitly considered, allowing for better plans.\n\nWe address these issues by a novel attention-based scenario processing module (SPM) that extends deterministic NCO architectures to solve stochastic problems. The SPM, which is expressive yet easily adaptable to different base neural networks for different problems, effectively captures the representation of stochastic scenarios to favorably intervene the solution policy. We embed SPM in a novel training paradigm that can address various stochastic objectives and offers high-quality yet fast solution policies, using a sampling strategy commonly found in stochastic optimization methods. We combine SPM with an existing network architecture to form SPM-DAN and solve the stochastic FJSP. Results show our method outperforms existing methods consistently. Our main contributions are summarized as follows:\n\n1. We propose an attention-based SPM to solve stochastic optimization problems, which is expressive and modular, with a good transferability to different base neural networks.   \n2. We put forward an effective DRL training paradigm, which is used to optimize different stochastic objectives.   \n3. We apply SPM to solve the stochastic FJSP, which outperforms existing learning and non-learning methods.\n\n# Related Work\n\nNCO for FJSP Graph neural networks (GNNs) for solving a variety of scheduling problems have been developed in recent years (Zhang et al. 2020; Park et al. 2021; Lei et al. 2022; Kwon et al. 2021). We refer to a recent survey (Smit et al. 2025) for a complete outline. Song et al. (2022) first proposed a competitive end-to-end DRL algorithm to construct FJSP solutions. They used a heterogeneous graph and designed a heterogeneous GNN using different GAT (VelicË‡kovicÂ´ et al. 2018) layers to encode machine and operation nodes. In the follow-up work, Wang et al. (2023) proposed a recent state-of-the-art dual attention network (DAN) that comprises both self- and crossattention, which achieved superior performance over previous DRL approaches.\n\nNCO for Dynamic FJSP Multiple papers target the dynamic FJSP (e.g. Zhang et al. 2023; Lei et al. 2023), which involve different sources of uncertainty such as random arrivals, machine availability, or processing times. These papers mainly focus on developing training strategies and MDP formulations to fit the dynamic use cases. The network architectures are not specifically tailored to address uncertainties, which are implicitly handled by approximating the expected reward in MDP trajectories. Similarly, NCO methods for other problems also mainly target deterministic or dynamic cases, in which the realized values become known during solution construction (Schmitt-Ulms et al. 2022; Kwon et al. 2020; Joe and Lau 2020).\n\nNCO for Stochastic FJSP Similar to us, Infantes et al. (2024) consider a stochastic job shop scheduling problem (JSP) for which a full plan must be created before the realizations become known. They include three features in the state space to describe the assumed triangular distribution, and base the reward function on a single sampled realization for each instance to optimize the expected makespan. While promising, their method is limited to triangular processing time distributions and the expected makespan objective.\n\n# Preliminaries and Problem Formulation\n\nFlexible Job Shop Scheduling Problem The FJSP consists of a pair $( \\mathcal { I } , \\mathcal { M } )$ where $\\mathcal { I }$ and $\\mathcal { M }$ are jobs and machines, respectively. A job $J _ { i } \\in \\mathcal { I }$ consists of a set ${ \\mathcal { O } } _ { i } =$ $\\{ O _ { i 1 } , \\ldots , O _ { i n _ { i } } \\}$ of $n _ { i }$ operations to be performed in order. The total set of operations is $\\mathcal { O } = \\cup _ { i } \\bar { \\mathcal { O } } _ { i }$ . Each operation\n\n$O _ { i j }$ must be processed by a single machine, selected from the set of compatible machines $\\mathcal { M } _ { i j } \\subseteq \\mathcal { M }$ . The processing time to perform operation $O _ { i j }$ on machine $M _ { k } \\in \\mathcal { M } _ { i j }$ is given by $p _ { i j } ^ { k } > 0$ and each machine can only process one job at a time. A solution to the FJSP is a schedule, which assigns a compatible machine to each operation $O _ { i j }$ and determines the order of operations on each machine. The goal is to minimize the makespan $c _ { m a x } = \\operatorname* { m a x } _ { O _ { i j } \\in \\mathcal { O } } c _ { i j }$ , which is the maximum completion time $c _ { i j }$ of all operations.\n\nStochastic Flexible Job Shop Scheduling Problem In the stochastic FJSP, we assume that the processing times are random variables $P _ { i j } ^ { k }$ . The realized processing times $p _ { i j } ^ { k } \\sim$ $\\mathcal { P } _ { i j } ^ { k }$ follow the probability distributions $\\mathcal { P } _ { i j } ^ { k }$ and only become known after the schedule is created. As a result, the realized $c _ { i j }$ and $c _ { m a x }$ are also realizations of the random variables $C _ { i j }$ and $C _ { m a x }$ . The goal is to create a schedule that optimizes an objective function $f ( C _ { m a x } )$ , which can be the expected value $\\mathbb { E } \\left( C _ { m a x } \\right)$ , the Value-at-Risk $V a R _ { \\alpha } \\left( C _ { m a x } \\right) =$ $\\operatorname* { m i n } \\{ c _ { m a x } : \\mathbb { P } ( C _ { m a x } \\leq c _ { m a x } ) \\geq \\alpha \\}$ , or other functions of $C _ { m a x }$ . $V a R _ { \\alpha }$ indicates the value for which, with probability $\\alpha$ , our makespan is at most this high. As such, it considers the practically relevant robustness to the uncertainty of schedules. We set and keep $\\alpha = 9 5 \\%$ in this paper.\n\nAttention Mechanism The attention function aims to map a group of $n$ queries $Q \\in \\mathbb { R } ^ { n \\times d _ { q } }$ to their advanced representations, using $n _ { v }$ keys $K \\in \\mathbb { R } ^ { n _ { v } \\times d _ { q } }$ and $n _ { v }$ values $V \\in \\mathbb { R } ^ { n _ { v } \\times d _ { v } }$ , such that:\n\n$$\n\\mathrm { A t t } ( Q , K , V ) = \\mathrm { s o f t m a x } ( Q K ^ { T } / \\sqrt { d _ { q } } ) V\n$$\n\nwhere $Q K ^ { T }$ defines the similarity between $Q$ and $K$ , based on which weights are computed by the softmax function and then used to obtain a weighted sum of the values in $V$ .\n\nVaswani et al. (2017) extended the attention mechanism by introducing the multi-head attention. By setting $h$ heads, $Q , K$ , and $V$ are first projected to $h$ matrices, respectively. Then, the attention function is applied to every three matrices in each head. The advanced representations from all heads are concatenated and linearly transformed:\n\n$\\mathbf { M H A } ( Q , K , V ) = \\mathsf { c o n c a t } ( \\mathrm { h e a d } _ { 1 } , \\dots , \\mathrm { h e a d } _ { h } ) W ^ { O }$ where he $\\mathsf { \\Omega } _ { \\mathsf { M } _ { i } } = \\mathsf { A t t } ( Q W _ { i } ^ { Q } , K W _ { i } ^ { k } , V W _ { i } ^ { V } )$ . $W _ { i } ^ { Q } \\in \\mathbb { R } ^ { d _ { q } \\times d _ { q } ^ { \\prime } }$ , $W _ { i } ^ { K } \\in \\mathbb { R } ^ { d _ { q } \\times d _ { q } ^ { \\prime } }$ , and $W _ { i } ^ { V } \\in \\mathbb { R } ^ { d _ { v } \\times d _ { v } ^ { \\prime } }$ are learnable matrices. We follow a typical choice for the dimensions by setting $d _ { q } = d _ { v } = d$ and $d _ { q } ^ { \\prime } = d _ { v } ^ { \\prime } = d / h$ . The multi-head attention layer is often embedded in a block, alongside the normalization, skip-connection, and feed-forward layer. Given input matrices $\\boldsymbol { X } \\in \\mathbb { R } ^ { n \\times d }$ and $Y \\in \\mathbb { R } ^ { n _ { v } \\times d }$ a multi-head attention block is defined as:\n\n$\\mathbf { M H A B } ( X , Y ) = \\mathbf { L a y e r N o r m } ( Z + \\mathbf { F F } ( Z ) )$ where $Z = { \\mathrm { L a y e r N o r m } } ( X + { \\mathbf { M H A } } ( X , Y , Y )$ . FF is a fullyconnected feed-forward layer and LayerNorm is the layer normalization operation (Ba, Kiros, and Hinton 2016).\n\n# Methodology\n\nWe take the state-of-the-art FJSP method, i.e., the DAN from (Wang et al. 2023), for an example, and describe how our method SPM-DAN is developed from DAN. Our method is easily applicable to other NCO methods.\n\n# Markov Decision Process\n\nThe scheduling process can be considered as a sequential decision-making process of iteratively assigning operations to available machines. At every decision moment $t$ , an operation-machine pair $( O _ { i j } , M _ { k } )$ is selected such that $O _ { i j }$ can be assigned to $M _ { k }$ . In the MDP formulation, the agent at each step receives state $s _ { t }$ , representing the environment, and takes action $a _ { t } = ( O _ { i j } , M _ { k } ) \\in \\mathcal { A } ( t )$ from the set of eligible actions $\\mathbf { \\mathcal { A } } ( t )$ , which are the possible assignments of the first unscheduled operation in each job to a compatible machine. The environment feeds back reward $\\boldsymbol { r } _ { t }$ and new state $s _ { t + 1 }$ . The schedule is completed after $| \\mathcal { O } |$ actions.\n\nState Space The relevant operations $\\mathcal { O } _ { u } ( t ) \\subseteq \\mathcal { O }$ for the state $s _ { t }$ are all operations except those that already have a successor scheduled on the same machine and, thus, do not directly influence the schedule anymore. The relevant machines $\\mathcal { M } _ { u } ( t ) \\subseteq \\mathcal { M }$ are all machines on which any of the remaining operations can be scheduled. For the deterministic method proposed in (Wang et al. 2023), the state $s _ { t } = \\{ \\mathcal { H } _ { O } , \\mathcal { H } _ { M } , \\mathcal { H } _ { O M } \\}$ consists of the operation features $\\mathcal { H } _ { O } \\ = \\ \\{ h _ { O _ { i j } } \\ \\in \\ \\mathbb { R } ^ { 1 0 } | O _ { i j } \\ \\in \\ \\mathcal { O } _ { u } ( t ) \\}$ , machine features $\\mathcal { H } _ { M } = \\{ h _ { M _ { k } } \\in \\mathbb { R } ^ { 8 } | M _ { k } \\in \\mathcal { M } _ { u } ( t ) \\}$ , and operation-machine pair features $\\mathcal { H } _ { O M } \\ = \\ \\{ h _ { ( O _ { i j } , M _ { k } ) } \\in \\ \\bar { \\mathbb { R } } ^ { 8 } | ( O _ { i j } , M _ { k } ) \\in \\$ $\\boldsymbol { \\mathcal { A } } ( t ) \\boldsymbol  \\}$ . These features are dynamic and interdependent. We refer to the original paper for the detailed feature descriptions. We introduce an expansion of this state space, based on sample approximation. Concretely, we sample $n _ { s c n }$ independent scenarios, each representing a set of realized processing time values pikj âˆ¼ Pikj. For each scenario, we keep track of all the features during the scheduling process. In addition, we maintain the deterministic problem instance. In doing so, the expanded state captures both the deterministic and stochastic problem characteristics. Formally, our state at step $t$ is $\\ { \\bf \\bar { \\mu } } _ { s _ { t } } ~ = ~ \\{ s _ { t } ^ { d e t } , S _ { t } ^ { s t o c h } \\} $ , where $s _ { t } ^ { d e t } \\bar { \\mathbf { \\Psi } } = \\{ \\mathcal { H } _ { O } ^ { d e t } , \\mathcal { H } _ { M } ^ { d e t } , \\mathcal { H } _ { O M } ^ { \\bar { d e t } } \\}$ reptrese {s tthe dSetermi}nistic instance state, and $\\ddot { S } _ { t } ^ { s t o c h } = \\{ s _ { t } ^ { l } | 1 \\leq l \\leq n _ { s c n } \\}$ is the set of states representinSg the scen{arit|os $s _ { t } ^ { l } = \\{ \\mathcal { H } _ { O } ^ { l } , \\mathcal { H } _ { M } ^ { l } , \\mathcal { H } _ { O M } ^ { l } \\}$ Note that the sets $\\dot { \\mathcal { O } } _ { u } ( t ) , \\mathcal { M } _ { u } ( t )$ , and $\\mathbf { \\mathcal { A } } ( t )$ are the same in the deterministic and sampled scenarios.\n\nAn alternative to capture stochastic information is representing the distribution by statistical features (Infantes et al. 2024). However, these features are hard to define for complex distributions and may lead to high dimensionality. Moreover, the varied characteristics in the state formulation depend on both the scheduling logic and the processing times, which cannot be accurately described by the statistical features, so valuable information is inevitably lost. Our features of the deterministic instance and scenarios can favorably reflect the influence of stochasticity, and are applicable to different problems, feature definitions, and distributions. Using efficient multiprocessing or parallel GPU computations, the runtime overhead of processing the states of scenarios is negligible, thereby maintaining fast solution construction.\n\nAction Space The action space $\\mathbf { \\mathcal { A } } ( t )$ consists of all compatible operation-machine pairs of the first unscheduled operations per job. The same action is applied to the deterministic instance and all the sampled scenarios.\n\nState Transition Upon taking an action, the sets $\\mathcal { O } _ { u } ( t )$ , $\\mathcal { M } _ { u } ( t )$ , and $\\mathbf { \\mathcal { A } } ( t )$ are updated to reflect the new relevant operations, machines, and potential actions. Additionally, the features for $s _ { t } ^ { d e t }$ and all $s _ { t } ^ { l }$ are updated independently, resulting in a new state $s _ { t + 1 }$ .\n\nReward We generalize the reward defined in (Zhang et al. 2020) and (Wang et al. 2023). In this deterministic reward function, a lower bound $\\underline { { \\mathbf { C } } } ( O _ { i j } , s _ { t } )$ of the completion time is estimated for each operation $O _ { i j }$ at time step $t$ . This bound equals the scheduled completion time if the operation has been scheduled. For unscheduled operations, the lower bound is approximated using the recursion $\\underline { { \\mathbf { C } } } ( O _ { i j } , s _ { t } ) \\ =$ $\\begin{array} { r } { \\underline { { \\mathsf { C } } } ( O _ { i ( j - 1 ) } , s _ { t } ) + \\operatorname* { m i n } _ { k \\in \\mathcal { M } _ { i j } } p _ { i j } ^ { k } } \\end{array}$ . Then the reward at step $t$ is $\\begin{array} { r } { r _ { t } = \\operatorname* { m a x } _ { O _ { i j } \\in \\mathcal { O } } \\underline { { \\mathrm { C } } } ( O _ { i j } , s _ { t } ) - \\operatorname* { m a x } _ { O _ { i j } \\in \\mathcal { O } } \\underline { { \\mathrm { C } } } ( O _ { i j } , s _ { t + 1 } ) . } \\end{array}$ .\n\nFor the stochastic reward, we consider $\\scriptstyle n _ { r e w }$ scenarios that are independently sampled and do not overlap with the scenarios of the state, to facilitate the generalization. These scenarios require a small amount of computation for tracking their makespan lower bounds since they do not necessitate computing all features. Concretely, let $\\mathbf { \\widehat { s } } _ { t } ^ { l }$ represent the state of reward scenario $l$ , then $\\mathcal { C } _ { t } ~ =$ $\\{ \\operatorname* { m a x } _ { O _ { i j } \\in \\mathcal { O } } \\mathbb { C } ( O _ { i j } , \\hat { s } _ { t } ^ { l } ) | 1 \\leq l \\leq n _ { r e w } \\}$ are the makespan lower bounds of all scenarios at time $t$ . The reward is computed as $r _ { t } = f ( \\mathcal { C } _ { t } ) - f ( \\mathcal { C } _ { t + 1 } )$ , where $f$ is the objective function, which in our case is the $V a R _ { \\alpha }$ or mean makespan. However, other functions such as conditional $V a R _ { \\alpha }$ or median are also possible. If the discounting factor is $\\gamma = 1$ , the sum of all rewards naturally sums to |tO=|0âˆ’1 r t = $f ( \\mathcal { C } _ { 0 } ) - f ( \\mathcal { C } _ { | \\mathcal { O } | - 1 } )$ , where $\\mathcal { C } _ { | \\mathcal { O } | - 1 } = \\{ C _ { m a x } ^ { l } | 1 \\leq l \\leq n _ { r e w } \\}$ are the makespans of all scenarios. Hence, maximizing the cumulative reward corresponds to minimizing the objective.\n\n# Network Architecture\n\nOur neural network consists of the scenario processing modules (SPMs) that take the scenarios $S _ { t } ^ { s t o c h }$ and compute embeddings from them to represent stochasticity. These are combined with the deterministic state $s _ { t } ^ { d e t }$ and inputted to the base neural network. In doing so, we do not need to restructure the base neural network drastically, making our method easily applicable to different GNN architectures. An alternative would be to pass all scenarios through the GNN and do â€˜post-processingâ€™ before the actor and critic layers. However, this is significantly more computationally complex. Figure 1 outlines the proposed SPM and its application to the state-of-the-art DAN for FJSP (Wang et al. 2023).\n\nScenario Processing Module To efficiently and effectively extract embeddings from the scenario states to reflect the stochasticity, we design the SPM by a multi-head attention mechanism, which is inspired by (Lee et al. 2019). Consider a set $\\mathcal { H }$ of $n _ { s c n }$ scenario states, which are linearly transformed into d-dimensional embeddings H âˆˆ RnscnÃ—d. We expect to extract an embedding that represents the distribution of scenario states. To this end, we capture the interaction between scenarios using the multi-head attention. A straightforward way would be to apply the full selfattention $\\mathrm { M } \\mathrm { \\bar { H } A B } ( H , H )$ . However, self-attention complex\n\nğ’‰ğ’‰(ğŸğŸ) ğ’‰ğ’‰(ğŸğŸ) ğ’‰ğ’‰(ğ’ğ’ğ’”ğ’”ğ’”ğ’”ğ’”ğ’”) ğ’Š(ğŸğŸ) ğ’Šğ’Š(ğŸğŸ) ğ’Šğ’Š(ğ’ğ’) Ã— ğ“ğ“ğ’–ğ’– Ã— ğ“œğ“œğ’–ğ’– Ã— ğ“ğ“ ğ’‰ğ’‰(ğ‘¶ğŸğ‘¶ğ’ŠğŸğ’Šğ’Š)ğ’Š ğ’‰ğ’‰(ğ‘¶ğŸğ‘¶ğ’ŠğŸğ’Šğ’Š)ğ’Š (ğ’ğ’ğ’”ğ’”ğ’”ğ’”ğ’”ğ’”) ğ’‰ğ’‰(ğ‘´ğŸğ‘´ğŸğ’Œ)ğ’Œğ’‰ğ’‰(ğ‘´ğŸğ‘´ğŸğ’Œ)ğ’Œ ğ’‰ğ’‰(ğ‘´ğ’ğ‘´ ğ’ğ’Œğ’”ğ’Œğ’”ğ’”ğ’”ğ’”ğ’”) (ğŸğŸ) (ğŸğŸ) (ğ’ğ’ğ’”ğ’”ğ’”ğ’”ğ’”ğ’”) Linear Layer ğ’€ğ’€ ğ‘¿ğ‘¿ ğ‘¶ğ‘¶ğ’Šğ’Šğ’Šğ’Š (ğ‘¶ğ‘¶ğ’Šğ’Šğ’Šğ’Š,ğ‘´ğ‘´ğ’Œğ’Œ) (ğ‘¶ğ‘¶ğ’Šğ’Šğ’Šğ’Š,ğ‘´ğ‘´ğ’Œğ’Œ) (ğ‘¶ğ‘¶ğ’Šğ’Šğ’Šğ’Š,ğ‘´ğ‘´ğ’Œğ’Œ) ğ’€ğ’€ ğ‘¿ğ‘¿ ğ‘²ğ‘² ğ‘½ğ‘½ ğ‘¸ğ‘¸ SPMğ‘¶ğ‘¶ SPMğ‘´ğ‘´ SPMğ‘¶ğ‘¶ğ‘¶ğ‘¶   \nMulti-Head Attention Block Multi-Head Attention Layer ğ’‰ğ’‰â€²ğ‘¶ğ‘¶ğ’Šğ’Šğ’Šğ’Š ğ’‰ğ’‰ğ’…ğ’…ğ’…ğ’…ğ’…ğ’… ğ‘¶ğ‘¶ğ’Šğ’Šğ’Š ğ’‰ğ’‰â€²ğ‘´ğ‘´ ğ’‰ğ’‰ğ’…ğ‘´ğ’… ğ‘´ğ’…ğ’…ğ’…ğ’… ğ’‰ğ’‰(ğ’…ğ’…ğ‘¶ğ’…ğ‘¶ğ’…ğ’Šğ’…ğ’Š ğ’…ğ’Šğ’Š,ğ‘´ğ‘´ğ’Œğ’Œ) ğ’‰ğ’‰(ğ‘¶ğ‘¶ğ’Šğ’Šğ’Šğ’Š,ğ‘´ğ‘´ğ’Œğ’Œ) ğ’€ğ’€ ğ‘¿ğ‘¿ Concat Concat Concat Add and Layer Norm.   \nMulti-Head Attention Block Fully-Connected Layer ğ’‰ğ’‰ğ‘¶ğ‘¶ğ’Šğ’Šğ’Šğ’Š â€¦ Ã— ğ“ğ“ğ’–ğ’– ğ’‰ğ’‰ğ‘´ğ‘´ğ’Œğ’Œ â€¦ Ã— ğ“œğ“œğ’–ğ’– ğ’‰ğ’‰(ğ‘¶ğ‘¶ğ’Šğ’Šğ’Šğ’Š,ğ‘´ğ‘´ğ’Œğ’Œ) â€¦ Ã— ğ“ğ“ ğ’‰ğ’‰ğ’‰(ğŸğŸ) ğ’‰ğ’‰â€²(ğŸğŸ) ğ’‰ğ’‰ğ’‰(ğ’ğ’ğ’”ğ’”ğ’”ğ’”ğ’”ğ’”) Add and Layer Norm. Aggregation Dual Attention Network ğ‘¶ğ‘¶ğ‘¶ğ‘¶ğ‘¶ğ‘¶ ğğğ’‚ğ’‚ğŸğŸ ğğğ’‚ğ’‚ğŸğŸ â€¦ ğğğ’‚ğ’‚|ğ“ğ“| ğ’—ğ’— (a) Scenario Processing Module (SPM). (b) Applying SPM to DAN (Wang et al. 2023).\n\nity scales quadratically with $n _ { s c n }$ , making it too expensive for a large value $n _ { s c n }$ . Hence, we propose a trainable set of $\\textit { m d }$ -dimensional inducing point vectors $\\boldsymbol { I } \\in \\mathbb { R } ^ { m \\times d }$ to compute the cross-attention over $H \\in \\mathbb { R } ^ { n _ { s c n } \\times d }$ , thereby maintaining a linear complexity with $n _ { s c n }$ and resulting in the embeddings $J \\in \\mathbb { R } ^ { m \\times d }$ . We then apply the original set $H$ to perform cross-attention with $J$ , to get $\\mathbf { \\bar { \\boldsymbol { H } } ^ { \\prime } } \\in \\mathbb { R } ^ { \\breve { n } _ { s c n } \\times d }$ , which approximates MHAB $( H , H )$ . To extract a single embedding of the stochasticity, we must apply permutation invariant aggregation, for which we use mean aggregation. Formally, our scenario processing module (SPM) can be expressed by:\n\n$$\n\\mathrm { S P M } ( \\mathcal { H } ) = \\mathrm { A v g } ( \\mathbf { M H A B } ( H , \\mathbf { M H A B } ( I , H ) )\n$$\n\nSPM-DAN The DAN in (Wang et al. 2023) comprises $L$ operation message attention blocks (OMBs) and machine and critic network. Each $\\mathrm { O M B } _ { l }$ computes embeddings $h _ { O _ { i j } } ^ { ( l ) }$ by an attention mechanism:\n\n$$\n\\{ h _ { O _ { i j } } ^ { ( l ) } | O _ { i j } \\in \\mathcal { O } _ { u } \\} = \\mathbf { O M B } _ { l } \\left( \\{ h _ { O _ { i j } } ^ { ( l - 1 ) } | O _ { i j } \\in \\mathcal { O } _ { u } \\} \\right)\n$$\n\nSim $\\mathbf { M M B } _ { l }$ computes embeddings $h _ { M _ { k } } ^ { ( l ) }$ for all ma$M _ { k } \\in \\mathcal { M } _ { u }$ volving both the operation and machine features, such that:\n\n$$\n\\begin{array} { r } { \\{ h _ { M _ { k } } ^ { ( l ) } | M _ { k } \\in \\mathcal { M } _ { u } \\} = \\mathbf { M } \\mathbf { M } \\mathbf { B } _ { l } \\big ( \\{ h _ { M _ { k } } ^ { ( l - 1 ) } | M _ { k } \\in \\mathcal { M } _ { u } \\} , } \\\\ { \\{ h _ { O _ { i j } } ^ { ( l - 1 ) } | O _ { i j } \\in \\mathcal { O } _ { u } \\} \\big ) } \\end{array}\n$$\n\nThe actor network in the DAN is a multilayer perceptron (MLP) that calculates a score $\\mu _ { ( O _ { i j } , M _ { k } ) }$ for each action $( O _ { i j } , M _ { k } ) \\in \\mathcal { A }$ :\n\n$$\n\\mu _ { ( O _ { i j } , M _ { k } ) } = \\mathbf { M } \\mathbf { L } \\mathbf { P } _ { \\theta } \\left( [ h _ { O _ { i j } } ^ { ( L ) } | | h _ { M _ { k } } ^ { ( L ) } | | h _ { G } ^ { ( L ) } | | h _ { ( O _ { i j } , M _ { k } ) } ] \\right)\n$$\n\nwhere $\\begin{array} { r } { \\begin{array} { r } { h _ { G } ^ { ( L ) } = \\big [ \\frac { 1 } { | \\mathcal { O } _ { u } | } \\sum _ { \\ d { O } _ { i j } \\in \\mathcal { O } _ { u } } h _ { \\ d { O } _ { i j } } ^ { ( L ) } \\big | | \\frac { 1 } { | \\mathcal { M } _ { u } | } \\sum _ { \\ d { M } _ { k } \\in \\mathcal { M } _ { u } } h _ { \\ d { M } _ { k } } ^ { ( L ) } \\big ] } \\end{array} } \\end{array}$ is a graph embedding. Then, the DAN uses the softmax function over the scores to transform them into a probability distribution $\\pi _ { \\boldsymbol { \\theta } }$ , from which the action can be sampled. The critic network is an MLP for outputting a scalar value:\n\n$$\nv = \\mathbf { M L P } _ { \\phi } ( h _ { G } ^ { ( L ) } )\n$$\n\nwhich estimates the value of actions taken by the actor. We refer to (Wang et al. 2023) for details of each block.\n\nThe DAN requires three distinct types of input features, $h _ { O _ { i j } } \\ = \\ h _ { O _ { i j } } ^ { ( 0 ) }$ , $h _ { M _ { k } } \\ = \\ h _ { M _ { k } } ^ { ( 0 ) }$ , and $h _ { ( O _ { i j } , M _ { k } ) }$ . We compute these features based on our state definition. Specifically, we apply our SPM to process the scenario states $\\bar { S } ^ { s t o c h }$ and concatenate the resulting embeddings with their deterministic counterparts, such that:\n\n$$\n\\begin{array} { r l } & { h _ { O _ { i j } } = \\big [ h _ { O _ { i j } } ^ { d e t } \\big | \\big | \\mathrm { S P M } _ { O } \\big ( \\{ h _ { O _ { i j } } ^ { l } \\big | 1 \\leq l \\leq n _ { s c n } \\big \\} \\big ) \\big ] } \\\\ & { h _ { M _ { k } } = \\big [ h _ { M _ { k } } ^ { d e t } \\big | \\big | \\mathrm { S P M } _ { M } \\big ( \\{ h _ { M _ { k } } ^ { l } \\big | 1 \\leq l \\leq n _ { s c n } \\big \\} \\big ) \\big ] } \\\\ & { h _ { ( O _ { i j } , M _ { k } ) } = \\big [ h _ { ( O _ { i j } , M _ { k } ) } ^ { d e t } \\big | \\big | \\mathrm { S P M } _ { O M } \\big ( \\{ h _ { O _ { i j } , M _ { k } } ^ { l } \\} \\big | 1 \\leq l \\leq n _ { s c n } \\big \\} \\big ) \\big ] } \\end{array}\n$$\n\nIn summary, SPM-DAN uses the SPM to compute embeddings representing the stochasticity of the problem instance. Combined with the deterministic features and passed to DAN, they enable the learning of effective solution policies for solving the stochastic FJSP.\n\n# Training Procedure\n\nFollowing (Song et al. 2022) and (Wang et al. 2023), we train our policy by an actor-critic-based proximal policy optimization (PPO) algorithm (Schulman et al. 2017), shown in Algorithm 1. We extend the default PPO by keeping track of multiple independent scenarios per problem instance and applying the same actions. We compute the reward and state using these distinct scenarios and combine them into a single transition tuple reflecting all scenarios per problem instance per step. Hence, we maintain many scenarios, accommodating our state space to reflect the stochastic problem, without increasing the PPO batch sizes. The model is trained for $N _ { e p }$ episodes. We sample batches of $n _ { B }$ problem instances, with $n _ { s c n }$ scenarios to compute the state and $\\scriptstyle n _ { r e w }$ scenarios for the reward after $N _ { s }$ episodes. We validate on a fixed set of $n _ { v a l i }$ instances every $N _ { e v a l }$ episodes. As a common practice, we sample actions from the distribution $\\pi _ { \\boldsymbol { \\theta } }$ for training and infer greedy solutions for validation.\n\nAlgorithm 1: Training Procedure   \n\n<html><body><table><tr><td>Input:Neural network with randomly initialized parame- ters, fixed set of nvali evaluation instances 1:Sample batch of nB instances 2: for nep = 1,2,... Nep do 3: for b=1,2,...nB do 4: for t= 0,1,...,|O|-1 do 56 78 9: for l = 1,2,...,nrew do 10: Perform at,b and receive St+1,b 11: 12: 13: 14: 15: if nep mod Neval = O then 16: Validate the policy 17: if nep mod NB =O then 18: Compute rt,b</td><td>InParallel In Parallel InParallel Collect transition (St,b, at,b, rt,b, St+1,b) Compute generalized advantage estimates Compute PPO loss and update network parameters Sample new batch of nB training instances</td></tr></table></body></html>\n\n# Experiments\n\nIn this section, we numerically evaluate our method on various problem instances and compare the performance with several baseline methods.\n\nBaselines We use the first-in-first-out (FIFO), mostoperations-remaining (MOR), shortest-processing-time (SPT), and most-work-remaining (MWKR) priority dispatching rules (PDRs). In addition, we evaluate the deterministic OR-tools CP-SAT solver (Perron, Didier, and Gay 2023) with the median processing times, using the implementation by (Reijnen et al. 2023). We also extend the CP-SAT formulation to optimize over multiple sampled scenarios simultaneously, which we name CP-stoch. For these methods, we produce a plan using the deterministic information (or the used scenarios for CP-stoch) and evaluate on $\\scriptstyle n _ { r e w }$ independent scenarios. We set a 1-hour time limit for the CP-SAT solver. Based on preliminary tests, we set the number of scenarios for CP-stoch to 25 for synthetic $1 0 \\times 5$ and $2 0 \\times 5$ instances, and 10 for the other. Lastly, we evaluate the deterministic DAN policies on stochastic instances and use DAN with our reward mechanism but without SPM (DAN-stoch).\n\nDatasets As is common, we use synthetic datasets for training and evaluation. We use $\\mathrm { S D } _ { 1 }$ from (Wang et al. 2023). However, $\\mathrm { S D } _ { 1 }$ has processing times in the range $\\{ 1 , \\ldots , 2 0 \\}$ . Since it is common to round to integers, the stochasticity is limited, especially for small numbers (e.g., 1 with $50 \\%$ standard deviation will mostly be 1). We exclude $\\mathrm { S D _ { 2 } }$ from (Wang et al. 2023) as it has an unrealistic assumption of unrelated processing times $p _ { i j } ^ { k } \\in \\{ 1 , \\ldots , 9 9 \\}$ for an operation $O _ { i j }$ , drastically limiting the effects of stochasticity (e.g., $p _ { i j } ^ { 1 } \\ = \\ 1 0$ will mostly be favored over $p _ { i j } ^ { 2 } \\ = \\ 9 0$ regardless of the variance). Instead, we propose a more realistic $\\mathrm { S D _ { 3 } }$ for which we mimic the instance structure of $\\mathrm { S D _ { 2 } }$ , but sample processing times using $\\overline { { p } } _ { i j } \\sim U ( 1 , 9 9 )$ for each $O _ { i j }$ , after which we sample $p _ { i j } ^ { k } \\sim U ( 0 . 8 5 \\overline { { p } } _ { i j } , 1 . 1 5 \\overline { { p } } _ { i j } )$ . We use instance sizes $1 0 \\times 5$ , $2 0 \\times 5$ $5 , 1 5 \\times 1 0 , 2 0 \\times \\mathrm { { i } } 0 , 3 0 \\times \\mathrm { { i } } 0 .$ , and $4 0 \\times 1 0$ , with $n \\times m$ indicating $n$ jobs and $m$ machines.\n\nWe also evaluate our method using the mk (Brandimarte 1993), rdata, edata, and vdata instances (Hurink, Jurisch, and Thole 1994). Note, that the mk instances suffer from both limitations of $\\mathrm { S D } _ { 1 }$ and $\\mathrm { S D _ { 2 } }$ .\n\nCreating Stochastic Instances We assume the processing times of the deterministic instances to be the median value. For each pair $( O _ { i j } , M _ { k } )$ , we sample $C V _ { i j } ^ { k } \\sim U ( 0 . 1 , 0 . 5 )$ and set the standard deviation ${ \\sigma } _ { i j . } ^ { k } = C V _ { i j . } ^ { k } \\times m e d i a n _ { i j . } ^ { k }$ , as different machine-operation pairs have various uncertainty levels. There is no agreement on the best probability distributions in the literature. Unless specified otherwise, we assume lognormal distributions like (Caldeira and Gnanavelbabu 2021).\n\nConfigurations For a fair comparison, we keep the same training configurations $N _ { e p } = 1 0 0 0$ , $N _ { B } = 2 0$ , $N _ { s } = 2 0$ , $N _ { e v a l } = 1 0$ as (Wang et al. 2023). We also keep the same hyperparameters for the OMB and MMB blocks, the critic network, and the PPO algorithm, for which we refer to their paper. We increase the hidden layer size of the actor network to 128 to facilitate the expanded size of the input features $h _ { ( O _ { i j } , M _ { k } ) }$ . Based on preliminary tests, we use 4 attention heads and dimensions $d = 3 2$ for the SPMs. We set the number of inducing points $m = 1 6$ . For training, validation, and testing we set $n _ { r e w } = 1 0 0 0$ and use $n _ { s c n } = 1 0 0$ . For the synthetic data, we test 100 problem instances per instance type. We use an NVIDIA A100 GPU and 18-core Intel Xeon Platinum 8360Y CPU for DRL training and a 32-core AMD Rome 7H12 CPU for the CP-SAT solver. We train policies on $\\mathrm { S D } _ { 1 }$ and $\\mathrm { S D _ { 3 } }$ for the smallest four instance sizes, which takes between 1 and 14 hours. For inference, we use greedy solution construction and a sampling strategy with 100 samples per instance, conforming to the existing FJSP papers. Unless specified otherwise, we assume the $V a R _ { 9 5 \\% }$ of the makespan as the objective. We evaluate this metric using $n _ { r e w } = 1 0 0 0$ scenarios for each instance. We also report the inference time and the gap to the CP-SAT solutions, calculated as $\\begin{array} { r } { g a p = \\frac { f ( C _ { m a x } ) ^ { - } \\bar { f ( C _ { m a x } ) } _ { C P } } { f ( C _ { m a x } ) _ { C P } } } \\end{array}$ . The reported values are the average for each set of instances.\n\n# Synthetic Datasets Results\n\nTable 1 presents the performance of our methods using test instances generated from the same distributions as the training instances. These results show that our method consistently outperforms all PDRs. In addition, using sampling inference, our model outperforms the deterministic CP-SAT solver on all instance sets. The CP-stoch formulation is only competitive on the $1 0 \\times 5$ instances. In larger instances, the additional computational complexity outweighs the increased model accuracy. DAN and DAN-stoch are competitive with our model only on the smallest $\\mathrm { S D } _ { 1 }$ instances. For other instance sets, especially larger ones, our method performs considerably better. In our implementation, SPMDAN adds runtime of roughly $20 \\%$ for greedy inference and 3 times for sampling (3.14 vs. 2.63 and 10.50 vs. 3.80 seconds for $\\mathrm { S D _ { 3 } } ~ 2 0 { \\times } 1 0$ per instance), but is still relatively fast and leads to clear performance improvements. For example, greedy inference using SPM-DAN considerably outperforms sampling inference using the baseline models for $2 0 \\times 1 0$ instances, showcasing a better performance-runtime trade-off despite an initial runtime increase. The DAN-stoch models do not consistently outperform DAN, showcasing the value of synergizing our training procedure and SPM.\n\nTable 1: Results on synthetic instances of the same sizes as the instances used for training.   \n\n<html><body><table><tr><td colspan=\"2\">Size</td><td colspan=\"4\">PDRs FIFO MOR MWKR</td><td colspan=\"3\">Greedy DAN DAN-stoch SPM-DAN</td><td colspan=\"3\">Sample DAN</td><td colspan=\"2\">DAN-stoch SPM-DAN CP-SAT CP-stoch</td></tr><tr><td>10Ã—5</td><td>Obj Gap</td><td>160.66 14.27%</td><td>157.40 11.95%</td><td>155.01 10.25%</td><td>SPT 171.16 21.74%</td><td>149.03 150.99 6.00% 7.39%</td><td></td><td>149.58 6.39%</td><td>140.90 141.64 0.21% 0.74%</td><td></td><td>140.44 -0.11%</td><td>140.60 139.07 0.00% -1.09%</td></tr><tr><td>20Ã—5</td><td>Time (s) Obj Gap</td><td>0.06 270.60 7.70%</td><td>0.06 271.81 8.18%</td><td>0.06 270.93 7.83%</td><td>0.06 293.45 16.79%</td><td>0.69 0.69 270.14 261.82 7.51% 4.20%</td><td>0.84 255.47 1.68%</td><td>4.53 259.75 3.38%</td><td>4.59 265.59 5.70%</td><td>5.33 250.81 -0.18%</td><td>251.26 0.00%</td><td>260.14 3.53%</td></tr><tr><td>qs 15Ã—10</td><td>Time (s) Obj Gap</td><td>0.12 254.20 17.49%</td><td>0.12 241.74 11.73%</td><td>0.12 0.12 239.70 266.19 10.79% 23.03%</td><td>1.35 226.48 4.68%</td><td>1.39 228.12 5.44%</td><td>1.66 220.20 1.77%</td><td>9.10 216.17 -0.09%</td><td>9.15 218.61 1.04%</td><td>12.62 211.01 -2.47%</td><td>216.36 0.00%</td><td>224.62 3.82%</td></tr><tr><td>20Ã—10</td><td>Time (s) Obj Gap</td><td>0.19 308.87 11.74% 0.26</td><td>0.19 299.23 8.25% 0.27</td><td>0.19 296.27 7.18%</td><td>0.18 2.10 337.68 279.44 22.16% 1.09%</td><td>2.07 274.36 -0.75%</td><td>2.45 262.53 -5.02%</td><td>14.41 270.15 -2.27%</td><td>14.42 266.34 -3.65%</td><td>22.50 256.77 -7.11%</td><td>276.42 0.00%</td><td>291.56 5.48%</td></tr><tr><td>10Ã—5</td><td>Time (s) Obj Gap Time (s)</td><td>760.14 10.47% 0.06</td><td>755.55 9.80% 0.06</td><td>0.26 744.38 8.18% 0.06</td><td>0.26 2.77 827.24 722.77 20.22% 5.04% 0.65</td><td>2.83 736.05 6.97% 0.65</td><td>3.36 710.63 3.27% 0.79</td><td>19.84 688.74 0.09% 0.74</td><td>19.88 693.49 0.78%</td><td>34.47 677.24 -1.58%</td><td>688.10 0.00%</td><td>676.19 -1.73%</td></tr><tr><td>20Ã—5 qs</td><td>Obj Gap Time (s)</td><td>4.55% 0.12</td><td>6.08% 0.12</td><td>5.31% 0.12</td><td>0.06 1310.67 1329.94 1320.251427.00|1307.48 13.83% 4.29% 0.12 1.28</td><td>1290.02 2.90% 1.37</td><td>1279.70 2.08% 1.58</td><td>1272.08 1.47% 1.46</td><td>0.73 1257.08 0.27% 1.48</td><td>1.11 1243.75 -0.79%</td><td>1253.67 0.00%</td><td>1271.71 1.44%</td></tr><tr><td>15Ã—10</td><td>Obj Time (s) Gap</td><td>13.05% 0.18</td><td>10.07% 0.19</td><td>7.57% 21.97% 0.18 0.18</td><td>1215.141183.131156.25 1311.01|1111.55 3.41% 1.96</td><td>1129.99 5.13% 1.96</td><td>1090.92 1.49% 2.36</td><td>1066.60 -0.77% 2.69</td><td>1080.32 0.51%</td><td>2.99 1039.73 -3.27%</td><td>0.00%</td><td>1074.88 1105.90 2.89%</td></tr><tr><td>20Ã—10 Time (s)</td><td>Obj Gap</td><td>1449.91 7.28% 0.25</td><td>6.22% 0.26</td><td>5.14% 0.26 0.25</td><td>1435.71 1421.03 1640.16|1376.18 21.35% 1.82% 2.63</td><td>1379.80 2.09% 2.60</td><td>1289.77 -4.57%</td><td>1336.97 -1.08% 3.14 3.80</td><td>2.74 1346.62 -0.37%</td><td>6.51 1261.80 -6.64%</td><td>= 1351.58 0.00%</td><td>1406.68 4.08%</td></tr></table></body></html>\n\nTable 2: Results on large synthetic instances using the policies trained on size $2 0 \\times 1 0$ .   \n\n<html><body><table><tr><td rowspan=\"2\">Size</td><td rowspan=\"2\"></td><td rowspan=\"2\"></td><td colspan=\"3\">PDRs</td><td rowspan=\"2\"></td><td rowspan=\"2\">Greedy DAN-stoch</td><td rowspan=\"2\">SPM-DAN</td><td rowspan=\"2\">DAN</td><td rowspan=\"2\">Sample DAN-stoch</td><td rowspan=\"2\">SPM-DAN</td><td rowspan=\"2\">CP-SAT</td></tr><tr><td>FIFO</td><td>MOR MWKR</td><td>DAN</td></tr><tr><td rowspan=\"2\">30Ã—10</td><td>Obj Gap</td><td>412.05 8.70%</td><td>410.85 8.39%</td><td>410.41 8.27%</td><td>458.12 20.86%</td><td>386.09 1.85%</td><td>371.58 -1.97%</td><td>355.85 -6.12%</td><td>376.92 -0.56%</td><td>365.36 -3.61%</td><td>352.16 -7.10%</td><td>379.06 0.00%</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>46.804</td><td>334</td><td></td><td></td></tr><tr><td rowspan=\"2\">qs 40Ã—10</td><td>Timbjs) Gap</td><td>0.409 6.79%</td><td>0447 8.21%</td><td>0418 8.31%</td><td>07.0 18.42%</td><td>46 2.05%</td><td>44.158 -2.58%</td><td>435 -6.55%</td><td>0.18%</td><td>-3.68%</td><td>56.67 -7.07%</td><td>487.47</td></tr><tr><td>Time (s)</td><td>0.58</td><td>0.58</td><td>0.58</td><td>0.57</td><td>5.58</td><td>5.44</td><td>6.70</td><td>42.20</td><td>41.88</td><td>89.22</td><td>0.00% =</td></tr><tr><td rowspan=\"3\">30Ã—10</td><td>Obj</td><td>1957.33</td><td>1997.42</td><td>1992.04</td><td>2251.80</td><td>1917.87</td><td>1915.07</td><td>1776.47</td><td>1882.99</td><td>1884.82</td><td>1775.37</td><td>1885.18</td></tr><tr><td>Gap</td><td>3.83%</td><td>5.95%</td><td>5.67%</td><td>19.45%</td><td>1.73%</td><td>1.59%</td><td> -5.77%</td><td>-0.12%</td><td>-0.02%</td><td>-5.82%</td><td>0.00%</td></tr><tr><td>Time (s)</td><td>0.41</td><td>0.41</td><td>0.41</td><td>0.40</td><td>3.95</td><td>3.95</td><td>4.76</td><td>6.17</td><td>6.40</td><td>22.26</td><td>=</td></tr><tr><td rowspan=\"3\">qs 40Ã—10</td><td>Obj</td><td>2496.87</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>2.24%</td><td>2559.63 4.81%</td><td>2572.53 5.33%</td><td>2858.96</td><td>2474.38</td><td>2457.34</td><td>2294.33</td><td>2438.07</td><td>2432.05</td><td>2305.52</td><td>2442.27</td></tr><tr><td>Gap Time (s)</td><td>0.57</td><td>0.57</td><td>0.57</td><td>17.06% 0.56</td><td>1.31% 5.20</td><td>0.62% 5.26</td><td>-6.06% 6.29</td><td>-0.17% 8.64</td><td>-0.42% 8.63</td><td>-5.60% 37.64</td><td>0.00%</td></tr></table></body></html>\n\nTable 2 shows the generalization of our policies to larger instances, using the $2 0 \\times 1 0$ policies on $3 0 \\times 1 0$ and $4 0 \\times 1 0$ instances. We do not report CP-stoch, as memory limits are reached before finding sensible solutions for these instance sizes. In these larger instances, our method consistently and considerably outperforms all baselines, even achieving a $6 \\%$ improvement over CP-SAT and DAN using greedy sampling. DAN-stoch also beats DAN in these instances but remains far from SPM-DAN performance. These results show that SPM-DAN can scale to new instance sizes without the need for retraining.\n\n# Public Datasets Results\n\nWe assess the cross-distribution performance of our method on the public benchmark datasets, shown in Table 3. We report the policy trained on $1 5 { \\times } 1 0 \\mathrm { S D } _ { 3 }$ instances, but policies trained on $1 0 \\times 5 \\mathrm { S D _ { 3 } }$ , $1 5 \\times 1 0 \\mathrm { S D } _ { 1 }$ , and $1 0 \\times 5 \\mathrm { S D } _ { 1 }$ instances show similar results. We find that our method maintains performance across the benchmark instances. On the mk data, we see no performance improvement of our approach over the default DAN, caused by the previously mentioned instance limitations. On the other datasets, we maintain a clear performance improvement over the baseline neural methods and outperform CP-SAT up to $8 \\%$ on vdata instances.\n\nTable 3: Results on benchmark instances using policies trained on $\\mathrm { S D _ { 3 } }$ instances of size $1 5 \\times 1 0$ .   \n\n<html><body><table><tr><td rowspan=\"2\">Dataset</td><td rowspan=\"2\"></td><td colspan=\"4\">PDRs</td><td rowspan=\"2\"></td><td rowspan=\"2\">Greedy DAN-stoch SPM-DAN</td><td rowspan=\"2\"></td><td rowspan=\"2\">Sample</td><td colspan=\"4\"></td></tr><tr><td>FIFO</td><td>MOR</td><td>MWKR SPT</td><td>DAN</td><td>DAN</td><td>DAN-stoch SPM-DAN|CP-SAT CP-stoch</td><td></td><td></td></tr><tr><td rowspan=\"2\">mk</td><td>Obj</td><td>257.45</td><td>253.37</td><td>255.39</td><td>295.30</td><td>232.22</td><td>238.81</td><td>232.51</td><td>223.61</td><td>229.21</td><td>224.01</td><td>230.20</td><td>346.91</td></tr><tr><td>Gap</td><td>11.84%</td><td>10.07%</td><td>10.94%</td><td>28.28%</td><td>0.88%</td><td>3.74%</td><td>1.00%</td><td>-2.86%</td><td>-0.43%</td><td>-2.69%</td><td>0.00%</td><td>50.70%</td></tr><tr><td rowspan=\"3\"></td><td>Time (s)</td><td>0.18</td><td>0.18</td><td>0.18</td><td>0.18</td><td>1.98</td><td>1.94</td><td>2.37</td><td>14.89</td><td>14.96</td><td>21.71</td><td></td><td></td></tr><tr><td>Obj</td><td>1408.60 1397.46 1391.50 1568.94|1370.80</td><td></td><td></td><td></td><td></td><td>1366.73</td><td>1345.01</td><td>1318.70</td><td>1311.90</td><td>1294.62</td><td>1316.22</td><td>1549.72</td></tr><tr><td>Gap</td><td>7.02%</td><td>6.17%</td><td>5.72%</td><td>19.20%</td><td>4.15%</td><td>3.84%</td><td>2.19%</td><td>0.19%</td><td>-0.33%</td><td>-1.64%</td><td>0.00%</td><td>17.74%</td></tr><tr><td rowspan=\"3\"></td><td>Time (s)</td><td>0.19</td><td>0.19</td><td>0.19</td><td>0.19</td><td>2.12</td><td>2.07</td><td>2.46</td><td>16.06</td><td>16.08</td><td>23.98</td><td></td><td>=</td></tr><tr><td>Obj</td><td></td><td></td><td></td><td></td><td>1560.18 1695.89|1534.73</td><td>1544.10</td><td>1520.25</td><td>1467.36</td><td></td><td></td><td></td><td></td></tr><tr><td>Gap</td><td>1586.08 11.46%</td><td>1570.56 10.37%</td><td>9.64%</td><td>19.18%</td><td>7.85%</td><td>8.51%</td><td>6.83%</td><td>3.12%</td><td>1483.50 4.25%</td><td>1457.05 2.39%</td><td>1423.01 0.00%</td><td>1553.96 9.20%</td></tr><tr><td rowspan=\"3\">edata</td><td>Time (s)</td><td>0.19</td><td>0.19</td><td>0.19</td><td>0.19</td><td>2.10</td><td>2.11</td><td>2.49</td><td>15.96</td><td>16.09</td><td>24.11</td><td>-</td><td>=</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Obj</td><td></td><td></td><td></td><td></td><td>1295.16 1293.13 1286.46 1444.67|1281.67</td><td>1271.83</td><td>1227.05</td><td>1235.49</td><td>1225.49</td><td>1189.42</td><td>1300.97</td><td>1336.54</td></tr><tr><td rowspan=\"3\">vdata</td><td>Gap</td><td>-0.45%</td><td>-0.60%</td><td>-1.12%</td><td>11.05%</td><td>-1.48%</td><td>-2.24%</td><td>-5.68%</td><td>-5.03%</td><td>-5.80%</td><td>-8.57%</td><td>0.00%</td><td>2.73%</td></tr><tr><td>Time (s)</td><td>0.20</td><td>0.20</td><td>0.19</td><td>0.19</td><td>2.06</td><td>2.05</td><td>2.52</td><td>16.19</td><td>16.11</td><td>24.27</td><td></td><td></td></tr></table></body></html>\n\n<html><body><table><tr><td rowspan=\"2\">Size</td><td rowspan=\"2\"></td><td colspan=\"4\">PDRs</td><td rowspan=\"2\">DAN</td><td colspan=\"2\">Greedy</td><td colspan=\"4\">Sample</td><td rowspan=\"2\"></td></tr><tr><td>FIFO</td><td>MOR</td><td>MWKR</td><td>SPT</td><td></td><td>DAN-stoch SPM-DAN</td><td>DAN</td><td>DAN-stoch SPM-DAN</td><td></td><td>CP-SAT CP-stoch</td></tr><tr><td></td><td>Obj</td><td>646.99</td><td>643.20</td><td>634.07</td><td>709.00</td><td>616.62</td><td>621.76</td><td>619.44</td><td>595.87</td><td>596.10</td><td>591.32</td><td>586.89</td><td>591.72</td></tr><tr><td>10Ã—5</td><td>Gap</td><td>10.24%</td><td>9.59%</td><td>8.04%</td><td>20.81%</td><td>5.07%</td><td>5.94%</td><td>5.55%</td><td>1.53%</td><td>1.57%</td><td>0.75%</td><td>0.00%</td><td>0.82%</td></tr><tr><td></td><td>Time (s)</td><td>0.06</td><td>0.06</td><td>0.06</td><td>0.06</td><td>0.65</td><td>0.65</td><td>0.81</td><td>0.74</td><td>0.79</td><td>1.55</td><td></td><td></td></tr><tr><td></td><td>Obj</td><td></td><td></td><td></td><td>1172.19 1187.68 1181.30 1280.42</td><td>1169.24</td><td>1149.28</td><td>1142.68</td><td>1146.42</td><td>1145.32</td><td>1130.61</td><td>1120.84</td><td>1156.04</td></tr><tr><td>20Ã—5</td><td>Gap</td><td>4.58%</td><td>5.96%</td><td>5.39%</td><td>14.24%</td><td>4.32%</td><td>2.54%</td><td>1.95 %</td><td>2.28%</td><td>2.18%</td><td>0.87%</td><td>0.00%</td><td>3.14%</td></tr><tr><td></td><td>Time (s)</td><td>0.12</td><td>0.12</td><td>0.12</td><td>0.12</td><td>1.28</td><td>1.33</td><td>1.61</td><td>1.46</td><td>1.64</td><td>5.05</td><td>1</td><td>1</td></tr><tr><td></td><td>Obj</td><td></td><td>1074.56 1044.78</td><td>1021.75 1168.17</td><td></td><td>983.03</td><td>986.66</td><td>968.87</td><td>951.44</td><td>952.00</td><td>931.59</td><td>949.46</td><td>991.76</td></tr><tr><td>15Ã—10</td><td>Gap</td><td>13.18%</td><td>10.04%</td><td>7.61%</td><td>23.04%</td><td>3.54%</td><td>3.92%</td><td>2.04%</td><td>0.21%</td><td>0.27%</td><td>-1.88 %</td><td>0.00%</td><td>4.46%</td></tr><tr><td></td><td>Time (s)</td><td>0.18</td><td>0.19</td><td>0.18</td><td>0.18</td><td>1.96</td><td>2.00</td><td>2.43</td><td>2.69</td><td>2.96</td><td>11.19</td><td></td><td></td></tr><tr><td></td><td>Obj</td><td>1302.11</td><td>1288.43</td><td>1277.09</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>20Ã—10</td><td>Gap</td><td>7.28%</td><td>6.15%</td><td>5.22%</td><td>1485.581237.78 22.39%</td><td>1.98%</td><td>1231.46</td><td>1181.44 -2.66%</td><td>1209.97 -0.31%</td><td>1203.39 -0.86%</td><td>1171.56 -3.48%</td><td>1213.78</td><td>1286.98</td></tr><tr><td></td><td>Time (s)</td><td>0.25</td><td>0.26</td><td>0.25</td><td>0.26</td><td>2.63</td><td>1.46% 2.68</td><td>3.20</td><td>3.80</td><td>4.25</td><td>18.96</td><td>0.00%</td><td>6.03%</td></tr></table></body></html>\n\nTable 4: Results on synthetic $\\mathrm { S D _ { 3 } }$ instances using the expected makespan objective.\n\n# Flexibility to Different Objectives\n\nWe train and evaluate policies for optimizing the expected makespan to show that our method works for different objective functions. Table 4 shows the results of these policies for $\\mathrm { S D _ { 3 } }$ instances. We see a similar pattern as for the $V a R _ { 9 5 \\% }$ where there is a limited effect for the smallest instances but our policies outperform the other methods toward larger problem sizes. The overall improvement is slightly less compared to the $V a R _ { 9 5 \\% }$ objective. This is expected as the expected value and deterministic values tend to be more related to each other than the $V a R _ { 9 5 \\% }$ and deterministic values as the $V a R _ { 9 5 \\% }$ is more dependent on the distributional range of processing times. However, the improvements of our method are still considerable and for the larger instances we improve over CP-SAT, as well as the other baselines.\n\n# Resilience to Varying Probability Distributions\n\nAn important criterion for stochastic optimization methods is that they perform under different assumed probability distributions. Therefore, we create one instance set with beta distributions (B) similar to (Flores-GÃ³mez, Borodin, and DauzÃ¨re-PÃ©rÃ¨s 2023), one set with a mixture of log-normal and beta distributions (LB), and one set with a mixture of log-normal, beta, and gamma distributions (LBG). The results are shown in Table 5. It is clear that our method maintains performance across all instance sets. We see that DANstoch struggles to outperform DAN, as it cannot capture the dynamics that it observes through the rewards. Oppositely, our SPM-DAN model is aware of the types of stochasticity, which helps it distinguish different stochastic states. As a result, SPM-DAN achieves a gap compared to CP-SAT of $6 \\%$ and improves performance up to $5 \\%$ over the other models.\n\n# Trade-Off Performance and Number of Scenarios\n\nWe assess how $n _ { s c n }$ affects the performance of SPM-DAN. To this end, we evaluate models trained with 100 (SPMDAN) and 200 (SPM-DAN-200) scenarios using 50, 100, and 200 scenarios for inference. Table 6 shows that, at the cost of higher (lower) inference times, using a higher (lower) $n _ { s c n }$ to generate solutions generally improves (decreases) performance. The differences in performance gap are roughly between $0 . 5 \\%$ and $1 \\%$ . However, training using more scenarios does not per se lead to better performance, especially for smaller instances. Namely, SPM-DAN with 200 evaluation scenarios outperforms SPM-DAN-200 in some instance sets. In larger instances, the performance difference between SPM-DAN and SPM-DAN-200 is limited, although SPM-DAN-200 performs slightly better. Hence, computational requirements can be decreased by training with fewer scenarios at the cost of a limited potential performance loss. Then, in inference, $n _ { s c n }$ can be varied to trade off runtime and performance. The increased runtime of using a higher $n _ { s c n }$ is also limited by increasing parallelism.\n\nTable 5: Results on synthetic $\\mathrm { S D _ { 3 } }$ instances of size $2 0 \\times 1 0$ using different processing time distributions.   \n\n<html><body><table><tr><td rowspan=\"2\">Distribution</td><td rowspan=\"2\"></td><td colspan=\"4\">PDRs</td><td colspan=\"3\">Greedy</td><td colspan=\"3\">Sample</td><td rowspan=\"2\"></td></tr><tr><td>FIFO</td><td>MOR MWKR</td><td></td><td>SPT DAN</td><td></td><td>DAN-stoch SPM-DAN</td><td>DAN</td><td>DAN-stoch SPM-DAN|CP-SAT CP-stoch</td><td></td><td></td></tr><tr><td rowspan=\"3\">B</td><td>Obj</td><td></td><td>1267.59 1251.52 1236.12 1452.37|1186.82</td><td></td><td></td><td>1187.25</td><td>1161.30</td><td>1156.79</td><td>1159.41</td><td>1133.56</td><td></td><td>[1170.30 1233.08</td></tr><tr><td>Gap</td><td>8.31%</td><td>6.94%</td><td>5.62%</td><td>24.10% 1.41%</td><td>1.45%</td><td>-0.77%</td><td>-1.15%</td><td>-0.93%</td><td>-3.14%</td><td>0.00%</td><td>5.36%</td></tr><tr><td>Time (s)</td><td>0.25</td><td>0.26</td><td>0.26 0.25</td><td>2.70</td><td>2.66</td><td>3.40</td><td>5.84</td><td>5.85</td><td>18.88</td><td></td><td></td></tr><tr><td rowspan=\"3\">LB</td><td>Obj</td><td></td><td></td><td>1371.43 1356.26 1343.65 1559.23|1298.64</td><td></td><td>1303.83</td><td>1229.54</td><td>1254.58</td><td>1265.35</td><td>1194.14</td><td>1275.49</td><td>1322.15</td></tr><tr><td>Gap</td><td>7.52%</td><td>6.33%</td><td>5.34%</td><td>22.25%</td><td>2.22%</td><td>-3.60%</td><td>-1.64%</td><td>-0.79%</td><td>-6.38%</td><td>0.00%</td><td>3.66%</td></tr><tr><td>Time (s)</td><td>0.25</td><td>0.26</td><td>0.26 0.25</td><td>1.81% 2.77</td><td>2.70</td><td>3.25</td><td>5.84</td><td>5.81</td><td>19.17</td><td>1</td><td>-</td></tr><tr><td rowspan=\"3\">LBG</td><td></td><td></td><td></td><td></td><td></td><td>1278.59</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Obj</td><td></td><td>[1359.61 1344.19 1329.91</td><td></td><td>1546.49|1289.39</td><td></td><td>1228.24</td><td>1247.24</td><td>1242.22</td><td>1197.75</td><td>1263.08</td><td>1321.16</td></tr><tr><td>Gap Time (s)</td><td>7.64% 0.25</td><td>6.42% 0.26</td><td>5.29% 0.26</td><td>22.44% 0.25</td><td>2.08% 1.23% 2.74 2.66</td><td>-2.76% 3.25</td><td>-1.25% 5.50</td><td>-1.65% 5.87</td><td>-5.17% 19.17</td><td>0.00%</td><td>4.60%</td></tr></table></body></html>\n\nTable 6: Results on synthetic $\\mathrm { S D _ { 3 } }$ instances of models trained with different $n _ { s c n }$ using various numbers of $n _ { s c n }$ for inference   \n\n<html><body><table><tr><td rowspan=\"2\"></td><td rowspan=\"2\"></td><td colspan=\"4\">Greedy</td><td colspan=\"6\">Sample</td><td rowspan=\"2\" colspan=\"2\"></td></tr><tr><td colspan=\"2\">SPM-DAN</td><td colspan=\"3\">SPM-DAN-200</td><td colspan=\"2\">SPM-DAN</td><td colspan=\"3\">SPM-DAN-200</td></tr><tr><td rowspan=\"2\">Size</td><td>Obj</td><td>50 100 719.14</td><td>200</td><td>50</td><td>100</td><td>200</td><td>50</td><td>100</td><td>200</td><td>50 100</td><td>200</td><td></td><td>CP-SAT CP-stoch</td></tr><tr><td></td><td>710.63</td><td>709.58</td><td>713.64</td><td>709.69</td><td>712.11</td><td>674.51</td><td>677.24</td><td>672.29</td><td>673.36 671.75</td><td>675.07</td><td>688.10</td><td>676.19</td></tr><tr><td rowspan=\"3\">10Ã—5</td><td>Gap</td><td>4.51% 3.27% 0.79</td><td>3.12%</td><td>3.71%</td><td>3.14%</td><td>3.49%</td><td>-1.98%</td><td>-1.58%</td><td>-2.30% -2.14%</td><td>-2.38%</td><td>-1.89%</td><td>0.00%</td><td>-1.73%</td></tr><tr><td>Time (s)</td><td>0.80</td><td>0.81</td><td>0.80</td><td>0.81</td><td>0.78</td><td>1.10</td><td>1.11</td><td>2.64</td><td>1.12 1.55</td><td>1.52</td><td></td><td></td></tr><tr><td>Obj</td><td></td><td>1283.09 1279.70 1277.45 1312.16 1307.93 1297.82|1242.07 1243.75 1238.65 1268.96 1265.04 1266.04|1253.67</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>1271.71</td></tr><tr><td rowspan=\"3\">20Ã—5</td><td>Gap</td><td>2.35% 2.08%</td><td>1.90%</td><td>4.67%</td><td></td><td></td><td>-0.93%-0.79%-1.20%</td><td></td><td>1.22%</td><td>0.91%</td><td>0.99%</td><td>0.00%</td><td>1.44%</td></tr><tr><td>Time (s)</td><td>1.65</td><td>1.58 1.61</td><td>1.66</td><td>4.33%</td><td>3.52%</td><td>3.20</td><td>2.99</td><td>8.72</td><td>3.19</td><td>5.03</td><td>1</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td>1.62</td><td>1.57</td><td></td><td></td><td></td><td></td><td>4.87</td><td></td><td></td></tr><tr><td rowspan=\"3\">15Ã—10</td><td>Obj</td><td></td><td>1084.99 1090.921077.59 1092.26 1088.521089.67|1035.24 1039.731035.22 1041.44 1040.63 104455|1074.88 1105.90</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Gap</td><td>0.94%</td><td>1.49% 0.25%</td><td>1.62%</td><td>1.27%</td><td>1.38%</td><td></td><td>-3.69% -3.27% -3.69%</td><td></td><td>-3.11%</td><td>-3.19% -2.82%</td><td>0.00%</td><td>2.89%</td></tr><tr><td>Time (s)</td><td>2.41 2.36</td><td>2.47</td><td>2.43</td><td>2.47</td><td>2.37</td><td>6.64</td><td>6.51</td><td>16.50</td><td>6.65 11.22</td><td>11.17</td><td>1</td><td>-</td></tr><tr><td rowspan=\"3\">20Ã—10</td><td>Obj</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>-4.53% -4.57%</td><td>1290.38 1289.771285.83 1296.15 1295.711285.53|1261.14 1261.80 1256.07 1262.78 1259.25 1259.41|1351.58 1406.68</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Gap Time (s)</td><td>3.24</td><td>-4.86%</td><td></td><td>-4.10%-4.13%-4.89%</td><td></td><td>-6.69%</td><td>-6.64% -7.07%</td><td>-6.57%</td><td>-6.83%</td><td>-6.82%</td><td>0.00%</td><td>4.08%</td></tr><tr><td rowspan=\"3\"></td><td></td><td>3.14</td><td>3.33</td><td>3.24</td><td>3.30</td><td>3.17</td><td>11.10</td><td>10.50</td><td>28.22</td><td>11.09 19.09</td><td>18.61</td><td>1</td><td></td></tr><tr><td>Obj</td><td></td><td>1783.31 1776.471776.10 1788.87 1778.371775.00|1775.52 1775.371768.42 1765.85 1759.35 1754.91</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>1885.18</td><td></td></tr><tr><td>Gap</td><td>-5.40%</td><td>-5.77% -5.79%</td><td></td><td>-5.11% -5.67% -5.84%</td><td></td><td>-5.82%</td><td>-5.82% -6.19%</td><td>-6.33%</td><td></td><td>-6.67% -6.91%</td><td>0.00%</td><td></td></tr><tr><td rowspan=\"3\">30Ã—10</td><td>Time (s)</td><td>4.89 4.76</td><td>5.22</td><td>4.98</td><td>4.86</td><td>4.95</td><td>23.40</td><td>22.26</td><td>62.53</td><td>23.40 33.93</td><td>61.56</td><td></td><td>=</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Obj Gap</td><td>-5.94% -6.06%</td><td>-6.26%</td><td></td><td>2297.152294.332289.39 2303.53 2292.60 2286.712309.34 2305.522299.30 2283.102276.22 2269.88|2442.27 -5.68%-6.13% -6.37%</td><td></td><td>-5.44%</td><td>-5.60% -5.85%</td><td></td><td>-6.52%</td><td>-6.80% -7.06%</td><td>0.00%</td><td></td></tr></table></body></html>\n\n# Conclusion\n\nThis study proposes the scenario processing module SPM, a novel attention-based module to extend NCO methods to stochastic problems. SPM captures embeddings from sampled scenarios and feeds those to the base neural network, allowing it to learn effective policies under uncertainty. We integrate SPM into a training procedure that works well with different stochastic objectives and apply it to the stochastic FJSP, leading to SPM-DAN. We experimentally show that SPM-DAN outperforms existing learning and non-learning methods. SPM-DAN works well across a range of synthetic and benchmark instances while handling various processing time distributions. Future work may focus on developing and adapting sampling techniques for stochastic optimization to enhance the performance of constructive policies. Moreover, integrating data from historical schedules with their realizations or contextual factors influencing processing times is a worthwhile avenue for exploration.",
    "summary": "```json\n{\n  \"core_summary\": \"### ğŸ¯ æ ¸å¿ƒæ¦‚è¦\\n\\n> **é—®é¢˜å®šä¹‰ (Problem Definition)**\\n> *   è®ºæ–‡è§£å†³äº†éšæœºæŸ”æ€§ä½œä¸šè½¦é—´è°ƒåº¦é—®é¢˜ï¼ˆStochastic Flexible Job Shop Scheduling Problem, SFJSPï¼‰ï¼Œè¯¥é—®é¢˜åœ¨åŠå¯¼ä½“åˆ¶é€ ã€åŒ»ç–—è°ƒåº¦å’Œé“ç”Ÿäº§ç­‰å¤šä¸ªå·¥ä¸šé¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é’ˆå¯¹ç¡®å®šæ€§è°ƒåº¦é—®é¢˜ï¼Œå¿½ç•¥äº†å®é™…ç”Ÿäº§ä¸­çš„ä¸ç¡®å®šæ€§ï¼ˆå¦‚éšæœºå¤„ç†æ—¶é—´ï¼‰ï¼Œå¯¼è‡´ç”Ÿæˆçš„è°ƒåº¦è®¡åˆ’åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°è„†å¼±ã€‚\\n> *   è¯¥é—®é¢˜çš„é‡è¦æ€§åœ¨äºå…‹æœäº†ç°æœ‰ç¡®å®šæ€§ä¼˜åŒ–æ–¹æ³•åœ¨éšæœºç¯å¢ƒä¸‹è¡¨ç°è„†å¼±çš„ç“¶é¢ˆï¼Œæå‡äº†è°ƒåº¦æ–¹æ¡ˆçš„é²æ£’æ€§å’Œå®ç”¨æ€§ã€‚\\n\\n> **æ–¹æ³•æ¦‚è¿° (Method Overview)**\\n> *   æå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„åœºæ™¯å¤„ç†æ¨¡å—ï¼ˆScenario Processing Module, SPMï¼‰ï¼Œå°†ç¥ç»ç»„åˆä¼˜åŒ–ï¼ˆNeural Combinatorial Optimization, NCOï¼‰æ–¹æ³•æ‰©å±•åˆ°éšæœºä¼˜åŒ–é¢†åŸŸã€‚SPMé€šè¿‡å¤šæ³¨æ„åŠ›æœºåˆ¶æ•è·é‡‡æ ·åœºæ™¯çš„åµŒå…¥è¡¨ç¤ºï¼Œå¹²é¢„åŸºç¥ç»ç½‘ç»œçš„ç­–ç•¥å­¦ä¹ ï¼Œä»è€Œåœ¨éšæœºç¯å¢ƒä¸‹ç”Ÿæˆé²æ£’çš„è°ƒåº¦ç­–ç•¥ã€‚\\n\\n> **ä¸»è¦è´¡çŒ®ä¸æ•ˆæœ (Contributions & Results)**\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹1ï¼š** æå‡ºSPMæ¨¡å—ï¼Œå…¶æ€§èƒ½åœ¨å¤šç§å®ä¾‹ä¸Šä¼˜äºç°æœ‰å­¦ä¹ å’Œéå­¦ä¹ æ–¹æ³•ï¼ˆå¦‚CP-SATï¼‰ï¼Œåœ¨20Ã—10å®ä¾‹ä¸ŠVaRæŒ‡æ ‡æå‡7.11%ã€‚\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹2ï¼š** è®¾è®¡äº†ä¸€ç§å…¼å®¹ä¸åŒéšæœºç›®æ ‡ï¼ˆå¦‚æœŸæœ›å®Œå·¥æ—¶é—´ã€é£é™©ä»·å€¼ï¼‰çš„è®­ç»ƒèŒƒå¼ï¼Œæ”¯æŒçµæ´»çš„ç›®æ ‡å‡½æ•°ä¼˜åŒ–ã€‚\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹3ï¼š** å±•ç¤ºäº†æ–¹æ³•å¯¹åœºæ™¯æ•°é‡å’Œåˆ†å¸ƒå˜åŒ–çš„å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œä¾‹å¦‚åœ¨30Ã—10å®ä¾‹ä¸Šæ— éœ€é‡æ–°è®­ç»ƒå³å¯ä¿æŒæ€§èƒ½ï¼ŒVaR_{95%}ä¼˜äºCP-SATè¾¾6.12%ã€‚\",\n  \"algorithm_details\": \"### âš™ï¸ ç®—æ³•/æ–¹æ¡ˆè¯¦è§£\\n\\n> **æ ¸å¿ƒæ€æƒ³ (Core Idea)**\\n> *   SPMé€šè¿‡å¤šæ³¨æ„åŠ›æœºåˆ¶å»ºæ¨¡éšæœºåœºæ™¯çš„åˆ†å¸ƒç‰¹å¾ï¼Œå°†åœºæ™¯åµŒå…¥ä¸ç¡®å®šæ€§çŠ¶æ€æ‹¼æ¥åè¾“å…¥åŸºç½‘ç»œï¼ˆå¦‚DANï¼‰ï¼Œä½¿ç­–ç•¥å­¦ä¹ èƒ½å¤Ÿæ˜¾å¼åˆ©ç”¨éšæœºä¿¡æ¯ã€‚å…¶è®¾è®¡å“²å­¦æ˜¯é€šè¿‡åˆ†ç¦»åœºæ™¯å¤„ç†ä¸ç­–ç•¥å­¦ä¹ ï¼Œå®ç°æ¨¡å—åŒ–å’Œå¯è¿ç§»æ€§ã€‚\\n> *   è¯¥æ–¹æ³•æœ‰æ•ˆçš„åŸå› æ˜¯ï¼šé€šè¿‡å¤šåœºæ™¯é‡‡æ ·è¿‘ä¼¼éšæœºæ€§ï¼Œå¹¶åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶é«˜æ•ˆæå–åœºæ™¯é—´çš„äº¤äº’ä¿¡æ¯ï¼Œä»è€Œæ›´å¥½åœ°åæ˜ éšæœºæ€§å¯¹è°ƒåº¦é€»è¾‘çš„å½±å“ã€‚\\n\\n> **åˆ›æ–°ç‚¹ (Innovations)**\\n> *   **ä¸å…ˆå‰å·¥ä½œçš„å¯¹æ¯”ï¼š** ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚è’™ç‰¹å¡æ´›é‡‡æ ·ï¼‰è®¡ç®—å¤æ‚åº¦é«˜ï¼Œä¸”ä»…é€‚ç”¨äºç‰¹å®šåˆ†å¸ƒï¼›ç°æœ‰NCOæ–¹æ³•ï¼ˆå¦‚DANï¼‰ä»…å¤„ç†ç¡®å®šæ€§åœºæ™¯ã€‚\\n> *   **æœ¬æ–‡çš„æ”¹è¿›ï¼š** SPMé€šè¿‡è¯±å¯¼ç‚¹ï¼ˆInducing Pointsï¼‰é™ä½æ³¨æ„åŠ›è®¡ç®—å¤æ‚åº¦ï¼ˆçº¿æ€§è€Œéå¹³æ–¹ï¼‰ï¼Œå¹¶æ”¯æŒä»»æ„åˆ†å¸ƒçš„åœºæ™¯åµŒå…¥ã€‚\\n\\n> **å…·ä½“å®ç°æ­¥éª¤ (Implementation Steps)**\\n> 1.  **çŠ¶æ€æ‰©å±•ï¼š** å°†ç¡®å®šæ€§çŠ¶æ€ä¸n_scnä¸ªé‡‡æ ·åœºæ™¯çš„çŠ¶æ€ç»„åˆä¸ºæ‰©å±•çŠ¶æ€ã€‚\\n> 2.  **åœºæ™¯åµŒå…¥ï¼š** å¯¹æ¯ä¸ªæ“ä½œ-æœºå™¨å¯¹çš„ç‰¹å¾åº”ç”¨SPMï¼Œå…¬å¼ä¸ºï¼š\\n>     $$\\\\mathrm{SPM}(\\\\mathcal{H}) = \\\\mathrm{Avg}(\\\\mathbf{MHAB}(H, \\\\mathbf{MHAB}(I, H)))$$\\n>     å…¶ä¸­$I$ä¸ºå¯è®­ç»ƒçš„è¯±å¯¼ç‚¹ï¼Œ$H$ä¸ºåœºæ™¯ç‰¹å¾çŸ©é˜µã€‚\\n> 3.  **ç­–ç•¥å­¦ä¹ ï¼š** å°†åµŒå…¥è¾“å…¥DANçš„OMBæ¨¡å—ï¼Œé€šè¿‡PPOç®—æ³•ä¼˜åŒ–éšæœºç›®æ ‡å‡½æ•°ï¼ˆå¦‚VaRï¼‰ã€‚\\n\\n> **æ¡ˆä¾‹è§£æ (Case Study)**\\n> *   è®ºæ–‡æœªæ˜ç¡®æä¾›æ­¤éƒ¨åˆ†ä¿¡æ¯ã€‚\",\n  \"comparative_analysis\": \"### ğŸ“Š å¯¹æ¯”å®éªŒåˆ†æ\\n\\n> **åŸºçº¿æ¨¡å‹ (Baselines)**\\n> *   ä¼˜å…ˆçº§è°ƒåº¦è§„åˆ™ï¼ˆFIFO, MOR, SPT, MWKRï¼‰ã€ç¡®å®šæ€§CP-SATã€æ‰©å±•ç‰ˆCP-stochã€DANåŠå…¶éšæœºç‰ˆæœ¬DAN-stochã€‚\\n\\n> **æ€§èƒ½å¯¹æ¯” (Performance Comparison)**\\n> *   **åœ¨VaR_{95%}æŒ‡æ ‡ä¸Šï¼š** æœ¬æ–‡æ–¹æ³•åœ¨SD_3çš„20Ã—10å®ä¾‹ä¸Šè¾¾åˆ°256.77ï¼Œæ˜¾è‘—ä¼˜äºCP-SAT (276.42) å’ŒDAN-stoch (266.34)ã€‚ä¸CP-SATç›¸æ¯”ï¼Œæå‡7.11%ã€‚\\n> *   **åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šï¼š** åœ¨æœªè®­ç»ƒçš„30Ã—10å®ä¾‹ä¸Šï¼ŒSPM-DANçš„VaR_{95%}ä¸º355.85ï¼Œä¼˜äºCP-SAT (379.06) å’ŒDAN-stoch (376.92)ï¼Œå·®è·è¾¾6.12%ã€‚\\n> *   **åœ¨åˆ†å¸ƒé²æ£’æ€§ä¸Šï¼š** åœ¨æ··åˆåˆ†å¸ƒï¼ˆLBGï¼‰çš„20Ã—10å®ä¾‹ä¸Šï¼ŒSPM-DANçš„VaR_{95%}ä¸º1197.75ï¼Œæ¯”CP-SAT (1263.08) æå‡5.17%ã€‚\\n> *   **åœ¨æœŸæœ›å®Œå·¥æ—¶é—´ä¸Šï¼š** SPM-DANåœ¨15Ã—10å®ä¾‹ä¸Šè¾¾åˆ°968.87ï¼Œä¼˜äºCP-SATï¼ˆ949.46ï¼‰å’ŒDANï¼ˆ983.03ï¼‰ã€‚ä¸CP-SATç›¸æ¯”ï¼Œæå‡äº†2.04%ã€‚\",\n  \"keywords\": \"### ğŸ”‘ å…³é”®è¯\\n\\n*   éšæœºæŸ”æ€§ä½œä¸šè½¦é—´è°ƒåº¦ (Stochastic Flexible Job Shop Scheduling, SFJSP)\\n*   ç¥ç»ç»„åˆä¼˜åŒ– (Neural Combinatorial Optimization, NCO)\\n*   åœºæ™¯å¤„ç†æ¨¡å— (Scenario Processing Module, SPM)\\n*   å¤šå¤´éƒ¨æ³¨æ„åŠ›æœºåˆ¶ (Multi-Head Attention Mechanism, MHAM)\\n*   é£é™©ä»·å€¼ (Value at Risk, VaR)\\n*   æ·±åº¦å¼ºåŒ–å­¦ä¹  (Deep Reinforcement Learning, DRL)\\n*   åˆ¶é€ ä¸šè°ƒåº¦ (Manufacturing Scheduling, N/A)\"\n}\n```"
}