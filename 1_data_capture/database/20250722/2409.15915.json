{
    "source": "Semantic Scholar",
    "arxiv_id": "2409.15915",
    "link": "https://arxiv.org/abs/2409.15915",
    "pdf_link": "https://arxiv.org/pdf/2409.15915.pdf",
    "title": "Planning in the Dark: LLM-Symbolic Planning Pipeline without Experts",
    "authors": [
        "Sukai Huang",
        "N. Lipovetzky",
        "Trevor Cohn"
    ],
    "categories": [
        "cs.AI"
    ],
    "publication_date": "2024-09-24",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 2,
    "influential_citation_count": 1,
    "institutions": [
        "The University of Melbourne",
        "Google"
    ],
    "paper_content": "# Planning in the Dark: LLM-Symbolic Planning Pipeline Without Experts\n\nSukai Huang1, Nir Lipovetzky1 and Trevor $\\mathbf { C o h n } ^ { 1 , 2 * }$\n\n1The University of Melbourne 2 Google sukaih@student.unimelb.edu.au, {nir.lipovetzky, trevor.cohn}@unimelb.edu.au\n\n# Abstract\n\nLarge Language Models (LLMs) have shown promise in solving natural language-described planning tasks, but their direct use often leads to inconsistent reasoning and hallucination. While hybrid LLM-symbolic planning pipelines have emerged as a more robust alternative, they typically require extensive expert intervention to refine and validate generated action schemas. It not only limits scalability but also introduces a potential for biased interpretation, as a single expert’s interpretation of ambiguous natural language descriptions might not align with the user’s actual intent. To address this, we propose a novel approach that constructs an action schema library to generate multiple candidates, accounting for the diverse possible interpretations of natural language descriptions. We further introduce a semantic validation and ranking module that automatically filter and rank the generated schemas and plans without expert-in-the-loop. The experiments showed our pipeline maintains superiority in planning over the direct LLM planning approach. These findings demonstrate the feasibility of a fully automated end-to-end LLM-symbolic planner that requires no expert intervention, opening up the possibility for a broader audience to engage with AI planning with less prerequisite of domain expertise.\n\nCode — https://github.com/Sino-Huang/Official-LLMSymbolic-Planning-without-Experts Extended version — https://arxiv.org/abs/2409.15922\n\n# 1 Introduction\n\nThe advent of Large Language Models (LLMs) has opened new avenues for solving natural language-described planning tasks (Kojima et al. 2022). However, direct plan generation using LLMs, while seemingly straightforward, has been criticized for inconsistent reasoning and hallucination, which undermines their reliability in critical planning scenarios (Valmeekam et al. 2022, 2023; Huang et al. 2024). In response, researchers have advocated for more robust approaches that combine the flexibility of LLMs with the correctness of symbolic planning to solve planning tasks (Pallagani et al. 2024; Oswald et al. 2024). To improve the soundness of generated plans, a hybrid LLM-symbolic planning\n\nInitial state desc. Domain descriptions   \nGoal state desc. Predicate descriptions Action descriptions   \nBook 31 Book 2 1 Direct LLM Planning Table initial state Plan Candidates Large Language Model Book 12 Problem Spec. Book 3 Action Schema(s) Predicate List 1 Table Symbolic Planner goal state 2 LLM-Symbolic Planning Pipeline\n\npipeline has emerged. As shown in Figure 1, instead of relying solely on LLMs to generate sequences of action plans through in-context learning, this pipeline begins by leveraging LLMs to extract abstract symbolic action specifications from natural language descriptions, known as action schemas. These schemas define the essential components of an action in a structured format understandable by symbolic planners. Once these schemas are generated, a classical planner can take over to search for feasible plans that fulfill the task specifications (Liu et al. 2023; Silver et al. 2024; Guan et al. 2023; Kambhampati et al. 2024).\n\nYet, this method is brittle, as a single missing or contradictory predicate in an action schema can prevent the planner from finding a valid plan. Thus, current pipelines often require multiple iterations of expert intervention to refine and validate the generated action schemas. For instance, Guan et al. (2023) reported that the expert took 59 iterations to fix schema errors for a single task domain. This process demands substantial time and expertise, which significantly hinders the scalability of the method. More critically, due to budget constraints, often only one expert is involved in the process. This creates a critical vulnerability: the potential for interpretation mismatch between the expert and the user. Experts, while knowledgeable, inevitably bring their own subjective interpretations to the task descriptions, often formal\n\nDomain NLN-NLdL-e-dseescsccrriibeedd LLM bRSoleliypicrePmPslleanbntnaniotininloginsc Expert planning tasks 日 ((eRe..ge..,,pArcttieiosnescnhetemamata)i)ons 9 E Bottleneck: Review & Correction   \nLimitation 2: Subjective and Limited Interpretation User Domain 25 Mbanoaogkisng Expert ?   \nNew Books Shelve quickly, Categorize and any available space. arrange neatly.\n\nizing them in a single, specific way. This limits the system to a single perspective of the task. However, unlike formal language designed to have an exact, context-independent meaning, natural language inherently contains ambiguities that yield diverse valid interpretations of the same description. This ambiguity suggests that a straightforward, one-to-one mapping from natural to formal languages – a typical case when relying on a single expert – risks overlooking the interpretation that the user actually intended (Moravcsik 1983) (see Figure 2).\n\nRegarding the issue with reliance on expert intervention, we propose a novel pipeline that eliminates this dependency. Specifically, our approach introduces two key innovations:\n\n(1): We construct an action schema library to generate multiple candidates, a strategy that has been overlooked in prior work despite being a natural fit for capturing the inherent ambiguity in natural language. By leveraging this library, we also increase the likelihood of obtaining solvable action schema sets – those have at least one valid plan that can be found by a planner.\n\n(2): We leverages sentence encoders1 to automatically validate and filter generated action schemas. This module ensures that the generated schemas closely align with the task descriptions in the semantic space, effectively acting like expert feedback. Our experiments demonstrate that without expert intervention, our pipeline generates sound action plans competitive with direct LLM-based plan generation, even in short-horizon planning tasks. Importantly, our approach offers multiple schema sets and plan candidates, preserving the diversity of interpretations inherent in ambiguous natural language descriptions.\n\n# 2 Related Work\n\nDirect Plan Generation with LLMs: The use of LLMs for direct action plan generation has been explored across various domains, including embodied tasks (Wang et al.\n\n2023; Xiang et al. 2024), and other language grounding environments (Ahn et al. 2022; Huang et al. 2022). These approaches are built upon the idea that LLMs’ reasoning capabilities can be effectively elicited through in-context learning techniques, particularly the Chain-of-Thought (CoT) approach. CoT prompts the model to generate a series of intermediate reasoning steps before arriving at the final answer, resulting in more coherent and logically sound reasoning (Wei et al. 2022). Building upon CoT, Yao et al. (2024) proposed Tree-of-Thought (ToT) framework, which explores multiple reasoning pathways, generating diverse plans and ranking them based on self-verification heuristics. These heuristics are verbalized confidence scores produced by LLMs themselves, a method supported by studies showing that LLMs are effective as zero-shot ranking models (Lin et al. 2022; Hou et al. 2023; Zhuang et al. 2023).\n\nCriticism and Hybrid Planning: Despite the promising results, researchers have raised concerns about the reliability and soundness of LLM-generated plans (Valmeekam et al. 2022, 2023; Huang et al. 2024). A critical issue highlighted by Kambhampati et al. (2024) is that planning and reasoning tasks are typically associated with System 2 competency, which involves slow, deliberate, and conscious thinking (Sloman 1996; Kahneman 2011). However, LLMs, being essentially text generators, exhibit constant response times regardless of the complexity of the question posed. This behavior suggests that no first-principle reasoning is occurring, contradicting the expectations for true planning capabilities. To this end, researchers have explored hybrid approaches. For instance, Thought of Search (Katz et al. 2024) involves the generation of successor function and goal test code by LLMs, followed by their execution within an external execution environment. The approach we focus on involves utilizing LLMs to generate symbolic representations of tasks, which are then processed by external symbolic planners to search for feasible plans (Liu et al. 2023; Guan et al. 2023). However, existing pipelines emphasize the necessity of expert intervention for action schema validation and refinement. While Kambhampati et al. (2024) proposed using LLMs as semi-expert critics to assess output quality, this approach still necessitates expert involvement for final decision-making. In contrast, our work strives to reduce the dependency on expert intervention, offering a more accessible approach to hybrid LLM-symbolic planning that also addresses the inherent ambiguity in natural language descriptions.\n\n# 3 Problem Setting and Background\n\nWe consider a scenario where an agent generates action plans for natural language-described planning tasks. A task description typically consists of: (1) a domain description outlining general task information and possible high-level actions, and (2) a problem instance description specifying the initial and goal states. The study of LLM-symbolic planning pipelines is grounded in the formal framework of classical planning, which relies on symbolic representations of planning tasks. These representations are typically expressed using the Planning Domain Definition Language (PDDL) (Aeronautiques et al. 1998; Haslum et al. 2019).\n\nStep 1: 1   \nBuilding a Diverse Schema Library N M N 1 the same entity, but they are referring to PNlaLn-ndiensgcrTiabsekds LLMs generate SAchtieomna 1 encoded differently Z(D),P 4 N Task Entity O in symbolic in free form   \nStep 2: (Configured for High Diversity) Diverse action schema candidates   \nSemantic Coherence Filtering representation representation M N Score: 0.37 PDDL domain NatdueraslcrLipatnigounage Sentence PDDL Action Schema Encoder Score: 0.46 Score: 0.17 × Action Schema PSrcehdeimcateSeLtist SEenctteoendncecere Score: 0.15 ×   \nSPltaenp  S3:coring and Ranking Conformal Prediction (a)2y (0) (mi) seexpmeacntteidc tvoecotuotrpruetpsriemsielnatrations Similar semantic embeddings Symbolic Sentence Cumul. Score: 0.78 PDDL Planner Plalananssns Encoder Cumul. Score: 0.64 alanRnssanked PSrcehdeimcateSeLtist {π}K Cumul. Score: 0.47 Sorted Assumption in the filtering mechanism L (mi\n\nFigure 3: An overview of the proposed pipeline, it first constructs diverse action schema candidates to cover various interpretations of the natural language descriptions. Then, it filters out low-confidence candidates to ensure the generation candidates are semantically aligned with the descriptions. Lastly, it produces and ranks multiple plans using a symbolic planner. The filtering mechanism is grounded in the concept of semantic equivalence across different representations of the same content.\n\nIn brief, a PDDL description is defined by $\\langle \\mathcal { D } , \\Pi _ { \\mathcal { D } } \\rangle$ , where:\n\n• $\\mathcal { D } = \\langle \\mathcal { P } , \\mathcal { A } \\rangle$ is the domain specification: $\\mathcal { P }$ is the set of predicates that can either hold true or false, and $\\mathcal { A }$ is the set of action schemas. Each action schema $\\alpha \\in { \\mathcal { A } }$ is defined as a tuple $\\alpha = \\langle p a r , p r e , e f f \\rangle$ , where par details the parameters, and pre and $e f f$ are the preconditions and effects, respectively. Both pre and $e f f$ are typically expressed as conjunctive logical expressions using predicate logic. • $\\Pi _ { \\mathcal { D } } = \\langle \\mathcal { O } , \\mathcal { Z } , \\mathcal { G } \\rangle$ is the problem instance: $\\mathcal { O }$ is the set of objects to interact with, $\\boldsymbol { \\mathcal { T } }$ is the initial state, and $\\mathcal { G }$ is the goal state that the agent needs to achieve.\n\nA solution to the planning task is a sequence of grounded actions $( \\pi = ( a _ { 0 } , { \\bar { . . . , a _ { n } } } ) )$ that transforms the initial state $\\boldsymbol { \\mathcal { T } }$ to the goal state $\\mathcal { G }$ . Each grounded action $a _ { i }$ is an instantiation of an action schema $\\alpha \\in { \\mathcal { A } }$ and predicates, where the parameters in $\\alpha$ are replaced with specific objects from $\\mathcal { O }$ .\n\nTo bridge natural language descriptions and formal planning representations, we introduce a natural language proxy layer, denoted as $\\mathcal { Z } ( \\cdot )$ , for these task specifications. For example, $\\mathcal { Z } ( \\mathcal { D } )$ represents the natural language equivalent of the domain specification $\\mathcal { D }$ . The two approaches, direct LLM planning and LLM-symbolic planning, can then be expressed in Eq 1 and Eq 2, respectively:\n\n$$\n\\begin{array} { r } { \\pi \\sim P _ { \\mathrm { L L M } } ( \\cdot \\mid \\mathcal { Z } ( \\mathcal { D } ) , \\mathcal { Z } ( \\Pi _ { \\mathcal { D } } ) ) \\qquad } \\\\ { \\hat { \\mathcal { A } } \\sim P _ { \\mathrm { L L M } } ( \\cdot \\mid \\mathcal { Z } ( \\mathcal { D } ) ) ; \\Pi _ { \\mathcal { D } } \\sim P _ { \\mathrm { L L M } } ( \\cdot \\mid \\mathcal { Z } ( \\Pi _ { \\mathcal { D } } ) ) } \\\\ { \\pi = f \\left( \\langle \\mathcal { P } , \\hat { \\mathcal { A } } \\rangle , \\Pi _ { \\mathcal { D } } \\right) \\qquad } \\end{array}\n$$\n\nIn these equations, $P _ { \\mathrm { L L M } } ( \\cdot )$ represents the generation process of LLMs, and $f$ is the symbolic planner that search for sound plans. While we largely adhere to the problem setting of previous research (e.g., Liu et al. (2023), Guan et al. (2023)), we introduce a crucial refinement by specifying a precise predicate set $( \\mathcal { P } )$ for each domain descriptions. This controlled setting addresses a key challenge in evaluating across different methodologies. Without a standardized predicate set, variations in domain understanding can lead to diverse and potentially incomparable outputs, hindering meaningful evaluation.\n\n# 4 Methodology\n\nAs illustrated in Figure 3, the proposed pipeline stands in contrast to existing expert-dependent approaches and consists of three key steps: (1) Building a Diverse Schema $L i$ - brary $( \\ S 4 . I ) ,$ , (2) Semantic Coherence Filtering $( \\ S \\ 4 . 2 )$ and (3) Plan Scoring and Ranking $( \\ S \\ 4 . 4 )$ .\n\n# 4.1 Building a Diverse Schema Library\n\nA key challenge in translating natural language descriptions into symbolic action schemas is the inherent ambiguity of language itself. Different interpretations of the same description can lead to variations in action schemas, impacting the downstream plan generation process. To ensure we explore a wide range of interpretations and effectively cover the user’s intent, we utilize multiple LLM instances, denoted as $\\{ P _ { \\mathrm { L L M } } ^ { 1 } , P _ { \\mathrm { L L M } } ^ { 2 } , . . . , P _ { \\mathrm { L L M } } ^ { N } \\}$ , and set their temperature hyperparameter high to encourage diverse outputs. Each will then generate its own set of action schemas $\\bar { \\mathcal { A } } _ { i } \\sim P _ { \\mathrm { L L M } } ^ { i } ( \\cdot \\ |$ $\\mathcal { Z } ( \\mathcal { D } ) )$ , where $\\hat { \\mathcal { A } } _ { i } = ( \\hat { \\alpha } _ { i 1 } , \\hat { \\alpha } _ { i 2 } , . . . , \\hat { \\alpha } _ { i M } )$ . Here, $\\hat { \\alpha } _ { i j }$ , where $i \\in [ \\dot { 1 } , . . N ]$ and $j \\in [ 1 , . . . , M ]$ , represents the generated action schema of $j$ -th action in the domain by the $i$ -th LLM instance.\n\nThe generated schemas $\\hat { \\alpha } _ { i j }$ from all models are then aggregated into a single library. Since each domain comprises $M$ actions, a “set” of action schemas refers to a complete collection where each action in the domain is associated with one corresponding schema. Therefore, all possible combination of action schemas within the library can generate approximately $\\binom { N } { 1 } ^ { M }$ different sets of action schemas.\n\nIn addition, existing pipelines rely heavily on expert intervention, partly because individual LLMs struggle to generate solvable sets of schemas – those that a planner can successfully use to construct a plan. This reliance becomes even more pronounced as the number of actions increases, with the probability of obtaining a solvable set of schemas from a single LLM diminishing exponentially. In contrast, our approach, by constructing a diverse pool of action schema sets, substantially improves the probability of finding a solvable set. Our analysis (detailed in Appendix A) demonstrates that, under reasonable assumptions, this probability can increase from less than $0 . 0 0 0 1 \\%$ with a single LLM to over $9 5 \\%$ when using multiple LLM instances.\n\nNote that the solvability of a set of action schemas can be efficiently verified by leveraging the completeness feature of modern symbolic planners. If a plan can be found for a given problem using the generated schemas, the set is deemed solvable. Importantly, modern symbolic planners have advanced capabilities that allow them to efficiently reject unsolvable schema sets. This is achieved by the ability to prove delete-free reachability in polynomial time (Bonet and Geffner 2001). Furthermore, modern planners are designed to operate efficiently on multithread CPU and the efficiency of the process should not be a cause for concern. See Appendix D for more details.\n\n# 4.2 Semantic Coherence Filtering\n\nThe previous method alone faces two limitations. First, as task complexity grows, the “brute-force” approach of combining and evaluating all possible sets becomes increasingly inefficient. Second, solvability does not guarantee semantic correctness – schemas may not accurately reflect the task descriptions, potentially leading to incorrect or nonsensical plans. Therefore, it is crucial to implement a filtering mechanism that autonomously assesses the semantic correctness of individual action schemas, filtering out low-quality candidates before they enter the combination process.\n\nOur approach is grounded in the concept of semantic equivalence across different representations of the same content, as discussed by Weaver (1952) in his memorandum “Translation.” Weaver emphasized that the most effective way to translate between languages is to go deeper to uncover a shared “common base of meaning” between language representations, illustrating this by noting that “a Russian text is really written in English, but it has been encoded using different symbols.” This principle is crucial in our context, where task descriptions in natural language and their corresponding structured symbolic representations should exhibit high semantic similarity, reflecting the same shared meaning despite different syntactic forms (see right side of Figure 3).\n\nRecent developments in language models as code assistants (Chen, Tworek et al. 2021; Rozie\\`re, Gehring et al. 2024) further support this assumption, demonstrating that these models can decode the underlying semantics of structured symbolic representations. Inspired by this, we propose a filtering step that leverages a sentence encoder $E ( \\cdot )$ to generate embeddings for both the action descriptions $\\dot { E } ( \\mathcal { Z } ( \\alpha ) )$ and the generated schemas $E ( \\hat { \\alpha } )$ . Then, we compute the cosine similarity between these embeddings to quantify semantic relatedness and filter out action schemas with low scores.\n\nSpecifically, we employ a conformal prediction (CP) framework (see Appendix B) to statistically guarantee that true positive action schema candidates have a high probability of being preserved while minimizing the size of the filtered set (Sadinle et al. 2019). In this process, a threshold $\\hat { q }$ will be calculated based on a user-specified confidence level $1 - \\epsilon$ . Action schemas with cosine similarity scores below this threshold are filtered out from the library.\n\nThis process (illustrated in step 2 of Figure 3) significantly reduces the number of candidate sets of action schemas to $\\Pi _ { i = 1 } ^ { M } ( m _ { i } )$ , where $m _ { i }$ is the number of action schemas that pass the semantic validation for the $i$ -th action. This prefiltering approach not only reduces the computational load on the symbolic planner, increasing efficiency, but also ensures that generated schemas closely align with the semantic meaning of the task descriptions.\n\n# 4.3 Finetuning with Manipulated Action Schemas\n\nHard negative samples have been shown to enhance representation learning by capturing nuanced semantic distinctions (Robinson et al. 2023). In our context, we found that structured action schemas are particularly ideal for generating hard negatives. By manipulating predicates in the precondition or effect expressions of true action schemas, we create hard negatives with subtle differences. During finetuning, a triplet loss function is employed, where each training sample consists of a triplet: the natural language description of an action $\\scriptstyle ( { \\mathcal { Z } } ( \\alpha ) )$ , the true action schema $( \\alpha )$ , and a negative sample $( \\alpha ^ { \\mathrm { n e g } } )$ (see Figure 4). A negative sample is of three types – (1) Easy Negatives: action schemas from other planning domains (inter-domain mismatch); (2) SemiHard Negatives: action schemas from the same domain but referring to different actions (intra-domain mismatch); and (3) Hard Negatives: As shown in Table 1, we employ four types of manipulations – swap, negation, removal, and addition – to manipulate the reference2 action schema of the domain.\n\neasy negative a\b\t\nc aneg   \ndesc. of action   \nplace-on-table 0 ne qneg Q hard negative semi-hard positive (\u0001\u0002\u0003i\u0004\u0005lat\u0006\u0007) negative place-on-table pl\nc\f-\n\u0003-\u000eh\f\u000ff\n\nTable 1: Types of Manipulations for Generating Synthesized Hard Negative Action Schemas in Training Data. Mutexes are predicates that cannot be true simultaneously, e.g., one cannot hold a book and have it on a table simultaneously.   \n\n<html><body><table><tr><td>Manipulation Type Description</td><td></td></tr><tr><td>Swap</td><td>Exchanges a predicate between preconditions and effects</td></tr><tr><td>Negation</td><td>precotesitipredieate inether</td></tr><tr><td>Removal</td><td>Removes a predicate from either preconditions or effects</td></tr><tr><td>Addition</td><td>Adds mutually exclusive (mutex) predicates to preconditions or effects (Helmert 2009)</td></tr></table></body></html>\n\nThrough this process, the sentence encoder learns to embed natural language descriptions closer to their corresponding action schemas while distancing them from negative samples in the semantic space.\n\n# 4.4 Plan Generation and Ranking\n\nAction schemas that more accurately represent the intended tasks described in natural language are likely to yield higherquality, more reliable plans. Leveraging this causal relationship, we assess and rank the generated plans based on the cumulative semantic similarity scores of their constituent action schemas. Specifically, we feed each solvable set of action schemas into a classical planner, which generates a calculated as $\\textstyle \\sum _ { i = 1 } ^ { M } { \\frac { E ( { \\mathcal { Z } } ( \\alpha _ { i } ) ) \\cdot E ( \\hat { \\alpha _ { i } } ) } { \\| E ( { \\mathcal { Z } } ( \\alpha _ { i } ) ) \\| \\| E ( \\hat { \\alpha _ { i } } ) \\| } }$ , where $\\mathcal { Z } ( \\alpha _ { i } )$ is the $i$ main, $\\hat { \\alpha _ { i } }$ is the corresponding generated action schema and $E ( \\cdot )$ is already defined in Sec 4.2. It ensures that the structured symbolic model comprising the plans are semantically aligned with the descriptions of the planning domain (see step $3$ in Figure 3). Furthermore, this approach allows for optional lightweight expert intervention as a final, noniterative step. By presenting the ranked schema sets and their corresponding plans, experts can determine the most appropriate one, providing a balance between autonomy and expert guidance.\n\nOverall, our pipeline bridges the gap between ambiguous task descriptions and the precise requirements of symbolic planners. By generating a diverse pool of action schemas and leveraging semantic similarity for validation and ranking, we achieve two key advancements. First, we reduce the dependency on expert intervention, making the process more accessible and efficient. Second, we preserve the inherent ambiguity of natural language, offering users multiple valid interpretations of the task and their corresponding plans.\n\n# 5 Experiments\n\nOur experiments test the following hypotheses: (H1) Semantic equivalence across different representations, as discussed by Weaver, holds true in our context. $( \\mathbf { H } 2 )$ Ambiguity in natural language descriptions leads to multiple interpretations. (H3) Our pipeline produces multiple solvable candidate sets of action schemas and plans without expert intervention, providing users with a range of options. $\\mathbf { \\left( H 4 \\right) }$ Our pipeline outperforms direct LLM planning approaches in plan quality, demonstrating the advantage of integrating LLM with symbolic planning method. See Appendix for other experiments outside the scope of these hypotheses.\n\n# 5.1 Experimental Setup\n\nTask and Model Setup. We introduces several key enhancements that distinguish it from previous work. (1) Novel Test Domains: We carefully selected three test domains ensuring they are unfamiliar to LLMs – Libraryworld: a modified version of the classic Blockworld domain; Minecraft: resource gathering and crafting domain inspired by the game Minecraft; and Dungeon: a domain originally proposed by Chrpa et al. (2017). This approach addresses a significant issue: many $\\mathrm { I P C } ^ { 3 }$ domains have likely been leaked into LLM training data (see Appendix C). For training and calibration of the sentence encoder, we used domains from IPC and PDDLGym (Silver and Chitnis 2020). (2) LLM Selection: We use the open-source GLM (Hou et al. 2024) over proprietary models like GPT-4, aligning with our commitment to accessible planning systems. (3) Ambiguity Examination: We tested our pipeline on two types of task descriptions to assess the impact of ambiguity – (a) detailed descriptions following the established style of Guan et al. (2023), and (b) layman descriptions provided by five non-expert participants4 who, unfamiliar with PDDL, described the domains and actions based on reference PDDL snippets. (4) Symbolic Planner: We used DUAL-BWFS (Lipovetzky and Geffner 2017) planner for plan generation as well as checking if the generated schema sets are solvable. (5) LLM Prompt Engineering: We use the CO-STAR and CoT framework to guide LLMs in generating outputs (see Appendix E).\n\nBaselines. We evaluate our pipeline against two key baselines: (1) The previous LLM-symbolic planning pipeline proposed by Guan et al. (2023), which involves expert intervention for action schema validation and refinement; and $( 2 ) D i$ - rect LLM-based planning using Tree-of-Thought (ToT) (Yao et al. 2024), which generates multiple plans and ranks them based on self-verification heuristics.\n\n![](images/e9b4891fe1347282921ce7e9c4c0387bf02a5b7dbc6cb303086c821edbdcbd7e.jpg)  \nFigure 5: The sentence encoder enhances the identification of mismatched pairs by fine-tuning with negative samples.\n\n# 5.2 Semantic Equivalence Analysis\n\nTo investigate H1, we initially assessed the cosine similarity of sentence embeddings for pairs of action schemas and their corresponding natural language descriptions, both when they were matched and when they were mismatched. We employed two pre-trained, extensive sentence encoders: text-embedding-3-large and sentence-t5-xl. These models, without any fine-tuning, demonstrated higher cosine similarity for matched pairs compared to mismatched ones. This finding suggests that the ability to detect such equivalence is an inherent feature of high-quality sentence embedding models, not merely an artifact of fine-tuning. However, OpenAI text-embedding-3-large model is bad for its accessibility, a lightweight encoder all-roberta-large- $\\cdot \\nu I$ allows for better speed and improved accuracy through finetuning, which is good in practice. The performance of the fine-tuned roberta model is shown in Figure 5. The substantial improvement in the model’s capacity to identify hard negatives – mismatched pairs with subtle differences – is a direct result of our dedicated training weights allocation. We deliberately designed our training data selection to include a ratio of easy, semi-hard, and hard negatives as [0.0, 0.4, 0.6], respectively (see Appendix E.7). This ratio was strategically chosen to concentrate on hard negatives, as LLMs are more likely to make hard-negative mistakes when generating action schemas. By prioritizing hard negatives in our training dataset, we aimed to enhance the model’s ability to filter out low-quality action schemas during the semantic coherence filtering step.\n\n# 5.3 Pipeline Performance and Efficiency\n\nOur pipeline’s performance and efficiency are highlighted through several key observations. Firstly, the use of action schema library effectively produces solvable action schema sets without requiring expert-in-the-loop. Notably, deploying 10 LLM instances is sufficient to generate solvable schema sets for all test domains, supporting H3. Secondly,\n\nTable 2: Contrasts Our Pipeline with Existing Works. Note that the property of generating sound (logical correct) plans has been highlighted as a feature of the hybrid planner in prior work (Liu et al. 2023; Guan et al. 2023). However, there is no guarantee that the schemas are fully correct w.r.t. what the user actually wants. Thus, we are weakening the property to soundness w.r.t. schemas.   \n\n<html><body><table><tr><td>Model</td><td>Mech.</td><td>Expuert</td><td>Heyistic</td><td>w.t.dhemas</td></tr><tr><td>Tree-of-Thought (Yao et al. 2024)</td><td>LLM</td><td>0</td><td>Self Verification</td><td>No</td></tr><tr><td>Guan et al.(2023) Hybrid</td><td></td><td>~59</td><td>Expert Validation</td><td>Yes</td></tr><tr><td>Ours</td><td>Hybrid</td><td>≤1</td><td>Sim.anties</td><td>Yes</td></tr></table></body></html>\n\nFigure 6 reveals a clear pattern: when confronted with inherently ambiguous layman descriptions from non-expert participants, our pipeline generates a significantly increased number of distinct solvable schema sets (e.g., from 3419 to 8039 when $\\mathrm { L L M } \\# = 1 0 ~ \\mathrm { w } / \\mathrm { o } ~ \\mathrm { C P } )$ , thereby supporting H2. This increase is primarily attributed to the diverse selection of predicates within the action schemas. Each predicate selection reflects a different interpretation of the problem, with each schema set emphasizing distinct features deemed critical for planning.\n\nFor instance, in the Libraryworld domain, we observed that some schema sets generated by some LLM instances take into account the ‘category’ property of books when constructing actions such as stacking books on a shelf. This means that, according to these schema sets, only books within the same category can be stacked together, which is a more organized way of arranging books. Consequently, this leads to different planning outcomes that reflect the varied interpretations of the user query at hand, which are a direct result of the ambiguity present in the layman’s description and the flexibility it provides to LLMs in making such choices.\n\nThe pipeline’s ability to generate a range of potential interpretations in response to ambiguous inputs is a critical advantage. It ensures that all intended aspects of the user’s description can be captured, even when the description is imprecise or incomplete.\n\nThirdly, the integration of conformal prediction in the filtering step demonstrates a significant improvement in efficiency, as evidenced by Figure 6. With the confidence level $1 - \\epsilon$ set to 0.8, the pipeline filtered out a large number of candidates, reducing the total number of combinations to $3 . 3 \\%$ of the original (1051 out of 31483) but meanwhile, the ratio of solvable schemas (verified by the planner) increased from $1 0 . 9 \\%$ to $23 . 0 \\%$ . This result strongly supports H3, highlighting the pipeline’s ability to efficiently generate solvable and semantically coherent schema sets. See Table 2 for a comprehensive comparison of our pipeline with existing LLM-based planning approaches. Notably, the initial low ratio of solvable schema sets $( 1 0 . 9 \\% )$ underscores the challenge faced within the LLM-symbolic planning paradigm, which may explain why expert intervention has been a common practice in the past.\n\n![](images/75ac661f3b05ebdf925ebf4fdfc512a1217f4d28a52a9cb7f3a7f0dc5a6c12ca.jpg)  \nTotal vs. Viable Combinations (With CP Filtering, $1 - \\epsilon = 0 . 8$ )   \nFigure 6: With CP, a large number of candidates are pruned, thereby improving efficiency.\n\nTable 3: Blind plan ranking eval.: Four assessors compared the top two plans from each approach to gold plans.   \n\n<html><body><table><tr><td></td><td>Rank 1st</td><td>Rank 2nd</td><td>Rank 3rd</td><td>Rank 4th</td><td>Rank 5th</td><td>Avg.</td></tr><tr><td>Gold</td><td>14</td><td>4</td><td>4</td><td>1</td><td>1</td><td>1.79</td></tr><tr><td>Ours</td><td>4</td><td>18</td><td>11</td><td>5</td><td>10</td><td>2.97</td></tr><tr><td>ToT</td><td>6</td><td>2</td><td>9</td><td>18</td><td>13</td><td>3.62</td></tr></table></body></html>\n\n# 5.4 Human Evaluation on Plan Quality\n\nTo further validate our approach, we conducted a human evaluation comparing the top two plan candidates generated by our pipeline against those from the ToT framework and a gold-standard plan derived from the reference PDDL domain model. Four expert assessors with extensive PDDL experience ranked the plans based on their feasibility in solving the given problems. The results, summarized in Table 3, clearly support H4.\n\nFor a deeper insight into our pipeline’s capabilities, we specifically tested the Sussman Anomaly, a well-known planning problem that requires simultaneous consideration of multiple subgoals, as solving them in the wrong order can undo previous progress (see Figure 1). Our results showed that ToT-style approaches using various LLMs, including state-of-the-art models like GPT-4o, consistently fail to solve this problem. The failure arises from the mistaken assumption that the first subgoal mentioned (i.e., placing book 1 on top of book 2) should be addressed first, leading to incorrect plans. Interestingly, GPT-3.5 and GPT4o exhibited different behaviors when faced with this problem. While GPT-3.5 consistently, yet incorrectly, asserted it had completed the problem, GPT-4o occasionally exhibited awareness of the plan’s incompleteness. However, even with this heightened awareness, GPT-4o was unable to identify the correct path within the given depth limit. Notably, ToTstyle approaches reveals a critical limitation where high verbalized confidence scores does not necessarily translate to plan validity. In contrast, our pipeline generates a range of plans, including suboptimal ones, but excels at identifying and prioritizing the most promising candidates through its ranking process that is based on the cumulative cosine similarity scores of generated action schemas. By strictly adhering to semantic alignment between these schemas and natural language descriptions, and by using a symbolic planner, the system avoids being misled by the tendency – observed in both humans and LLMs – to reason in a linear manner. This tendency involves prioritizing subgoals based on their order of appearance rather than considering their underlying logical dependencies. Such linear reasoning can lead to noninterleaved planning, where subgoals are tackled in the order they are presented and each must be fully completed before addressing the next one, which is a pitfall in complex planning problems like the Sussman Anomaly.\n\n# 5.5 Failure Case Analysis\n\nSchema Set with No Plan Found: We encountered instances where no solvable action schema set was generated, primarily due to limitations in the LLM’s reasoning capabilities. The use of open-source LLMs, while more accessible, may result in a lower success rate compared to more advanced proprietary models like GPT-4o. Specifically, with 7 LLM instances, we observed occasional failures of generating solvable sets action schemas for the libraryworld and minecraft domains. Nevertheless, solvable schema sets were consistently obtained across all domains when the number of LLM instances was increased to 10 (see Appendix F for a breakdown of schema set yield by LLM instance count).\n\nUnexpected Preference: In the Dungeon domain, human assessors unexpectedly preferred ToT-generated plans over both the reference plan and the proposed pipeline’s plans. Further analysis revealed that the ToT plans consistently included a step: grabbing a sword. Interestingly, grabbing a sword was not a necessary step for solving the given problem. Consequently, symbolic planners, focused on optimal pathfinding, excluded this step from their plans. However, this “unnecessary” step of acquiring a sword aligns with common strategies in Dungeon games, where players typically prioritize preparedness. Thus, this action strongly appealed to human assessors, causing them to rank the ToTgenerated plans higher.\n\n# 6 Conclusion\n\nOur work presents a 3-step pipeline that learn symbolic PDDL models over ambiguous natural language descriptions and generated multiple ranked plan candidates. Our findings demonstrate that a full end to end hybrid planner is possible without expert intervention, paving the way for democratizing planning systems for a broader audience. One limitation in this work is the lack of direct evaluation methods for assessing the quality of generated action schema sets. Metrics like “bisimulation” (Coulter et al. 2022) or “heuristic domain equivalence” (Oswald et al. 2024) require the generated schema sets to have the same action parameters as a predefined reference set. This approach doesn’t suit our context, where action parameters are flexible and inferred in real-time from natural language descriptions.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了大型语言模型（LLMs）在自然语言描述的规划任务中直接使用时出现的推理不一致和幻觉问题。现有混合LLM-符号规划管道虽然更稳健，但需要大量专家干预来细化和验证生成的动作模式，这不仅限制了可扩展性，还引入了潜在的偏见解释。\\n> *   该问题的重要性在于，它克服了现有方法对专家干预的依赖，使得规划系统更加自动化和可扩展，适用于更广泛的用户群体。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种新颖的管道，通过构建动作模式库生成多个候选方案，并引入语义验证和排名模块，自动过滤和排名生成的模式和计划，无需专家干预。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **贡献1：** 构建了一个多样化的动作模式库，生成多个候选方案，以捕捉自然语言描述的多样性解释。实验表明，使用10个LLM实例可以将可解模式集的概率从不到0.0001%提高到超过95%。\\n> *   **贡献2：** 引入了语义验证和排名模块，自动过滤和排名生成的动作模式和计划，无需专家干预。通过置信度0.8的过滤，候选方案数量减少到3.3%，但可解模式集的比例从10.9%提高到23.0%。\\n> *   **贡献3：** 通过符号规划器生成并排名多个计划，在专家评估中平均排名为2.97，优于ToT框架的3.62。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   该方法的核心思想是通过构建多样化的动作模式库来捕捉自然语言描述的多样性解释，并通过语义验证和排名模块自动过滤和排名生成的模式和计划。\\n> *   该方法有效的原因在于它利用了LLMs的多样性生成能力和语义相似性验证，确保生成的模式和计划既多样又语义正确。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 先前的工作需要大量专家干预来验证和细化生成的动作模式，限制了可扩展性并引入了潜在的偏见解释。例如，Guan et al. (2023)报告称，专家需要59次迭代来修复单个任务域的模式错误。\\n> *   **本文的改进：** 本文通过构建动作模式库和引入语义验证模块，完全消除了对专家干预的依赖，同时保留了自然语言描述的多样性解释。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> *   **步骤1：** 使用多个LLM实例生成多样化的动作模式库。每个LLM实例生成自己的动作模式集，温度参数设置为高以鼓励多样性输出。\\n> *   **步骤2：** 使用句子编码器计算动作描述和生成模式之间的语义相似性，过滤掉低置信度的候选方案。采用符合预测框架，基于用户指定的置信水平计算阈值。\\n> *   **步骤3：** 将可解的动作模式集输入符号规划器，生成并排名多个计划。排名基于生成的动作模式的累积语义相似性分数。\\n\\n> **案例解析 (Case Study)**\\n> *   在Libraryworld域中，一些模式集考虑了书籍的“类别”属性，导致只有同一类别的书籍可以堆叠在一起，反映了对用户查询的不同解释。\\n> *   在Dungeon域中，ToT生成的计划包含“抓取剑”的步骤，虽然不必要，但符合人类玩家的策略偏好，导致评估者意外偏好ToT计划。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   Guan et al. (2023)提出的混合LLM-符号规划管道，需要专家干预。\\n> *   直接LLM规划方法，使用Tree-of-Thought (ToT)框架生成和排名计划。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在规划质量上：** 本文方法生成的计划在专家评估中排名优于ToT框架生成的计划，平均排名为2.97，而ToT框架的平均排名为3.62。\\n> *   **在效率上：** 本文方法通过构建动作模式库和语义验证模块，显著提高了生成可解模式集的概率，从不到0.0001%提高到超过95%。\\n> *   **在语义验证上：** 使用置信度0.8的过滤，候选方案数量减少到3.3%，但可解模式集的比例从10.9%提高到23.0%。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n> **提取与格式化要求**\\n> *   大型语言模型 (Large Language Models, LLMs)\\n> *   符号规划 (Symbolic Planning, N/A)\\n> *   动作模式库 (Action Schema Library, N/A)\\n> *   语义验证 (Semantic Validation, N/A)\\n> *   自然语言处理 (Natural Language Processing, NLP)\\n> *   规划任务 (Planning Tasks, N/A)\\n> *   多样性解释 (Diverse Interpretations, N/A)\\n> *   自动化规划 (Automated Planning, N/A)\"\n}\n```"
}