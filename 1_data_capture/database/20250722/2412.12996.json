{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.12996",
    "link": "https://arxiv.org/abs/2412.12996",
    "pdf_link": "https://arxiv.org/pdf/2412.12996.pdf",
    "title": "Neural Control and Certificate Repair via Runtime Monitoring",
    "authors": [
        "Emily Yu",
        "Dorde Zikelic",
        "T. Henzinger"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "2024-12-17",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Institute of Science and Technology Austria",
        "Singapore Management University"
    ],
    "paper_content": "# Neural Control and Certificate Repair via Runtime Monitoring\n\nEmily $\\mathbf { Y } \\mathbf { u } ^ { 1 }$ , Ðord¯e ˇZikelic´2, Thomas A. Henzinger1\n\n1Institute of Science and Technology Austria, Klosterneuburg, Austria 2Singapore Management University, Singapore, Singapore emily.yu $@$ ist.ac.at, dzikelic@smu.edu.sg, tah@ist.ac.at\n\n# Abstract\n\nLearning-based methods provide a promising approach to solving highly non-linear control tasks that are often challenging for classical control methods. To ensure the satisfaction of a safety property, learning-based methods jointly learn a control policy together with a certificate function for the property. Popular examples include barrier functions for safety and Lyapunov functions for asymptotic stability. While there has been significant progress on learning-based control with certificate functions in the white-box setting, where the correctness of the certificate function can be formally verified, there has been little work on ensuring their reliability in the black-box setting where the system dynamics are unknown. In this work, we consider the problems of certifying and repairing neural network control policies and certificate functions in the black-box setting. We propose a novel framework that utilizes runtime monitoring to detect system behaviors that violate the property of interest under some initially trained neural network policy and certificate. These violating behaviors are used to extract new training data, that is used to re-train the neural network policy and the certificate function and to ultimately repair them. We demonstrate the effectiveness of our approach empirically by using it to repair and to boost the safety rate of neural network policies learned by a state-of-the-art method for learning-based control on two autonomous system control tasks.\n\n# Introduction\n\nThe rapid advance of machine learning has sparked interest in using it to solve hard problems across various application domains, and autonomous robotic systems control is no exception. Learning-based control methods obtain data through repeated interaction with an unknown environment, which is then used to learn a control policy. A popular example is (deep) reinforcement learning algorithms, which learn neural network policies (Mnih et al. 2015; Sutton and Barto 2018). However, the complex and often uninterpretable nature of learned policies poses a significant barrier to their safe and trustworthy deployment in safety-critical applications such as self-driving cars or healthcare devices (Amodei et al. 2016; García and Fernández 2015).\n\nIn control theory, a classical method to certify the correctness of a control policy concerning some property of interest is to compute a certificate function for that property (Dawson, Gao, and Fan 2023). A certificate function is a mathematical object which proves that the system under the control policy indeed satisfies the property. Common examples of certificate functions include Lyapunov functions for stability (Khalil 2002) and barrier functions for safety set invariance (Prajna and Jadbabaie 2004). While certificate functions are a classical and well-established concept in control theory, earlier methods for their computation are based either on polynomial optimization or rely on manually provided certificates, which makes them restricted to polynomial systems and typically scalable to low-dimensional environments (Ahmadi and Majumdar 2016; Dawson, Gao, and Fan 2023; Srinivasan et al. 2021).\n\n![](images/16deab21bbdb63cce68a117530a49000d7471ed96298a97f782b694f1237fed6.jpg)  \nFigure 1: The monitor-learner framework.\n\nRecent years have seen significant progress in overcoming these limitations, by utilizing machine learning to compute certificates for non-polynomial environments with neural network control policies (Abate et al. 2021b; Chang, Roohi, and Gao 2019; Dawson et al. 2021; Qin, Sun, and Fan 2022; Qin et al. 2021; Richards, Berkenkamp, and Krause 2018). The key idea behind these methods is to jointly learn a policy together with a certificate function, with both parametrized as neural networks. This is achieved by defining a loss function that encodes all defining properties of the certificate function. The learning algorithm then incurs loss whenever some of the defining certificate conditions are violated at any system state explored by the learning algorithm. Hence, the training process effectively guides the learning algorithm towards a control policy that admits a correct certificate function. A survey of the existing approaches and recent advances can be found in (Dawson, Gao, and Fan 2023).\n\nWhile the above approach guides the learning algorithm towards a correct certificate, it does not guarantee that the output control policy or the certificate function is correct. To address this issue, a number of recent works have considered the white-box setting which assumes complete knowledge of the environment dynamics and formally verifies the correctness of the certificate function. Notable examples include methods for learning and verifying neural Lyapunov functions (Abate et al. 2021b; Chang, Roohi, and Gao 2019; Richards, Berkenkamp, and Krause 2018) and neural barrier functions (Abate et al. 2021a; Zhang et al. 2023; Zhao et al. 2020), also extended to stochastic settings (Abate et al. 2023; Ansaripour et al. 2023; Chatterjee et al. 2023; Lechner et al. 2022; Mathiesen, Calvert, and Laurenti 2023; Mazouz et al. 2022; Zhi et al. 2024; Zikelic et al. 2023a,b). While there are several methods that learn control policies together with certificates in the black-box setting, e.g. (Dawson et al. 2021; Qin, Sun, and Fan 2022; Qin et al. 2021), to the best of our knowledge no prior method considers the problem of analyzing and certifying their correctness in the black-box setting, without assuming any knowledge on the dynamics. In this setting, one cannot guarantee the correctness of certificate functions at every system state as we do not have access to the system model at each state. However, we may still use more lightweight techniques to evaluate the control policy and the certificate and to repair them in cases when they are found to be incorrect.\n\nOur contributions. In this work, we propose a method for analyzing and repairing neural network policies and certificates in the black-box setting. Our method uses runtime monitoring to detect system behaviors that violate the property of interest or the certificate defining conditions. These behaviors are used to extract additional training data, which is then used by the training algorithm to re-train and ultimately repair the neural policy and the certificate. Our method consists of two modules – called the monitor and the learner, which are composed in a loop as shown in Fig. 1. The loop is executed until the monitor can no longer find property or certificate condition violating behaviors, upon which our algorithm concludes that the neural policy and the certificate have been repaired.\n\nWhile the idea of using runtime monitors to identify incorrect behaviors is natural, it introduces several subtle challenges whose overcoming turns out to be highly non-trivial. The first and obvious approach to runtime monitoring of a control policy in isolation is to simply test it, flag runs that violate the property, and add visited states to the training set. However, such a simple monitor suffers from two significant limitations. First, it can only be used to monitor violations of properties that can be observed from finite trajectories such as safety, but not of properties that are defined by the limiting behavior such as stability. Second, the simple monitor detects property violations only after they happen, meaning that it cannot identify states and actions that do not yet violate the property but inevitably lead to property violation. In contrast, in practice we are often able to predict in advance when some unsafe scenario may occur. For instance, if we drive a car in the direction of a wall at a high speed, then even before hitting the wall we can predict that a safety violation would occur unless we change the course of action.\n\nTo overcome these limitations, we advocate the runtime monitoring of neural network policies together with certificate functions and propose two monitoring algorithms. The first algorithm, called Certificate Policy Monitor (CertPM), issues a warning whenever either the property or some of the defining conditions of the certificate are violated. The second algorithm, called Predictive Policy Monitor (PredPM), goes a step further by estimating the remaining time until the property or some certificate condition may be violated, and issues a warning if this time is below some tolerable threshold. Experimental results demonstrate the ability of CertPM and PredPM to detect the property or certificate condition violating behaviors. Furthermore, we show that CertPM and PredPM allow our method to repair and significantly improve neural policies and certificates computed by a stateof-the-art learning-based control theory method.\n\nOur contributions can be summarized as follows:\n\n1. Runtime monitoring of policies and certificates. We design two novel algorithms for the runtime monitoring of control policies and certificates in the black-box setting.   \n2. Monitoring-based policy and certificate repair. By extracting additional training data from warnings issued by our monitors, we design a novel method for automated repair of neural network control policies and certificates.   \n3. Empirical evaluation. Our prototype successfully repairs neural network control policies and certificates computed by a state-of-the-art learning-based control method.\n\nRelated work. Related works on learning-based control with certificate functions, as well as on formal verification of learned control policies and certificates in the white-box setting, have been discussed above, so we omit repetition. In what follows, we discuss some other related lines of work.\n\nConstrained reinforcement learning (RL) algorithms consider constrained Markov decision processes (CMDPs) (Altman 1999; Geibel 2006) and are also concerned with learning policies under safety constraints. Notable examples of algorithms for solving CMDPs include Constrained Policy Optimization (CPO) (Achiam et al. 2017) or the method of (Chow et al. 2018) which uses a Lyapunov based approach. While these algorithms perform well, they are empirical in nature and do not provide any mechanism for checking correctness of learned policies.\n\nShielding (Bloem et al. 2015; Alshiekh et al. 2018) is a runtime enforcement approach (Könighofer et al. 2020; Pranger et al. 2021; Carr et al. 2023). They use a monitor to detect if the system is close to reaching an unsafe set of states and thus violating the safety property. In such cases, the system switches to a safe back-up policy. However, there are two fundamental differences between our method and shielding. First, we do not assume a backup policy. Second, our method can also be used to repair a control policy, in cases when it is determined to be incorrect. Apart from shielding, runtime monitoring has been extensively used in the field of runtime verification as a more lightweight alternative to formal verification or when errors are only triggered at runtime (Falcone and Pinisetty 2019; Renard et al. 2019; Zhou et al. 2020). Runtime monitoring and repair of neural network policies, but without using certificates, was considered in (Bauer-Marquart et al. 2022; Lyu et al. 2023).\n\n# Preliminaries\n\nWe consider a (deterministic, continuous-time) dynamical system $\\Sigma = ( X , U , f )$ , where $\\boldsymbol { \\mathcal { X } } \\subseteq \\mathbb { R } ^ { n }$ is the state space, $\\boldsymbol { u } \\subseteq \\mathbb { R } ^ { m }$ is the control action space and $f : \\mathcal { X } \\times \\mathcal { U }  \\mathcal { X }$ is the system dynamics, assumed to be Lipschitz continuous. The dynamics of the system are defined by $\\dot { \\boldsymbol { x } } ( t ) ~ =$ $f ( x ( t ) , u ( t ) )$ , where $t \\in \\mathbb { R } _ { > 0 }$ is time, $x : \\mathbb { R } _ { \\geq 0 } \\to \\mathcal { X }$ is the state trajectory, and $u : \\bar { \\mathbb { R } } _ { \\geq 0 } \\to \\mathcal { U }$ is the control input trajectory. The control input trajectory is determined by a control policy $\\pi : \\mathcal { X }  \\mathcal { U }$ . We use $\\mathcal { X } _ { 0 } \\subseteq \\mathcal { X }$ to denote the set of initial states of the system. For each initial state $x _ { 0 } \\in \\mathcal { X } _ { 0 }$ , the dynamical system under policy $\\pi$ gives rise to a unique trajectory $x ( t )$ with $x ( 0 ) = x _ { 0 }$ . Control tasks are concerned with computing a control policy $\\pi : \\mathcal { X }  \\mathcal { U }$ such that, under the control input $u ( t ) \\ = \\ \\pi ( x ( t ) )$ , the state trajectory induced by ${ \\dot { x } } ( t ) = f ( x ( t ) , \\pi ( x ( t ) )$ satisfies a desired property for every initial state in $\\mathcal { X } _ { 0 }$ . In this work, we consider two of the most common families of control properties:\n\n1. Safety. Given a set of unsafe states $\\mathcal { X } _ { u } \\subseteq \\mathcal { X }$ , the dynamical system satisfies the safety property under policy $\\pi$ with respect to $\\mathcal { X } _ { u }$ , if it never reaches $\\mathcal { X } _ { u }$ , i.e., if for each initial state in $\\mathcal { X } _ { 0 }$ we have $x ( t ) \\notin \\mathcal { X } _ { u }$ for all $t \\geq 0$ .   \n2. Stability. Given a set of goal states $\\mathcal { X } _ { g } \\subseteq \\mathcal { X }$ , the dynamical system satisfies the (asymptotic) stability property under policy $\\pi$ with respect to $\\mathcal { X } _ { g }$ , if it asympotically converges to $\\mathcal { X } _ { g }$ , i.e., if $\\begin{array} { r } { \\operatorname* { l i m } _ { t  \\infty } \\operatorname* { i n f } _ { x _ { g } \\in \\mathcal { X } _ { g } } | | x ( \\bar { t } ) - x _ { g } | | = 0 } \\end{array}$ for each initial state in $\\mathcal { X } _ { 0 }$ .\n\nWe also consider the stability-while-avoid property, which is a logical conjunction of stability and safety properties. To ensure that the dynamical system satisfies the property of interest, classical control methods compute a certificate function for the property. In this work, we consider barrier functions (Ames, Grizzle, and Tabuada 2014) for proving safety and Lyapunov functions (Khalil 2002) for proving stability. The stability-while-avoid property is then proved by computing both certificate functions.\n\nProposition 1 (Barrier functions) Suppose that there exists a continuously differentiable function $B : \\mathcal { X }  \\mathbb { R }$ for the dynamical system $\\Sigma$ under a policy $\\pi$ with respect to the unsafe set ${ { \\mathcal { X } } _ { u } } .$ , that satisfies the following conditions:\n\n$\\boldsymbol { { \\mathit { 1 } } }$ . Initial condition. $B ( x ) \\geq 0$ for all $x \\in \\mathcal { X } _ { 0 }$ .   \n2. Safety condition. $B ( x ) < 0$ for all $x \\in \\mathcal { X } _ { u }$ .   \n3. Non-decreasing condition. $L _ { f } \\mathcal { B } ( x ) + \\mathcal { B } ( x ) \\ge 0$ for all $x \\in \\{ x \\mid B ( x ) \\geq 0 \\}$ , where $\\begin{array} { r } { L _ { f } B = \\frac { \\partial B } { \\partial x } f ( x , u ) } \\end{array}$ is the Lie derivative of $\\boldsymbol { B }$ with respect to $f$ .\n\nThen, $\\Sigma$ satisfies the safety property under $\\pi$ with respect to ${ { \\mathcal { X } } _ { u } } ,$ , and we call $\\textit { B a }$ barrier function.\n\nProposition 2 (Lyapunov functions) Suppose that there exists a continuously differentiable function $\\mathcal { V } : \\mathcal { X }  \\mathbb { R }$ for the dynamical system $\\Sigma$ under policy $\\pi$ with respect to the goal set $\\mathcal { X } _ { g }$ , that satisfies the following conditions:\n\n1. Zero upon goal. $\\gamma ( x _ { g } ) = 0$ for all $x _ { g } \\in \\mathcal { X } _ { g }$ . 2. Strict positivity away from goal. $\\mathcal { V } ( x ) > 0$ for all $x \\in$ $\\mathcal { X } \\backslash \\mathcal { X } _ { g }$ . 3. Decreasing condition. $L _ { f } \\mathcal { V } < 0$ for all $x \\in \\mathcal { X } \\backslash \\mathcal { X } _ { g }$ . Then, $\\Sigma$ satisfies the stability property under $\\pi$ with respect to $\\mathcal { X } _ { g }$ , and we call $\\nu a$ Lyapunov function.\n\nAssumptions. We consider the black-box setting, meaning that the state space $\\mathcal { X }$ , the control action space $\\mathcal { U }$ as well as the goal set $\\mathcal { X } _ { g }$ and/or the unsafe set $\\mathcal { X } _ { u }$ (depending on the property of interest) are known. However, the system dynamics function $f$ is unknown and we only assume access to an engine which allows us to execute the dynamics function.\n\nProblem statement. Consider a dynamical system $\\Sigma$ defined as above. Suppose we are given one of the above properties, specified by the unsafe set $\\mathcal { X } _ { u }$ and/or the goal set $\\mathcal { X } _ { g }$ . Moreover, suppose that we are given a control policy $\\pi$ and a certificate function $\\boldsymbol { B }$ for the property.\n\n1. Policy certification and repair. Determine whether the system $\\Sigma$ under policy $\\pi$ satisfies the property. If not, repair the policy $\\pi$ such that the property is satisfied.   \n2. Certificate certification and repair. Determine whether $\\boldsymbol { B }$ is a good certificate function which shows that the dynamical system $\\Sigma$ under policy $\\pi$ satisfies the property. If not, repair the certificate function $\\boldsymbol { B }$ such that it becomes a correct certificate function.\n\n# Runtime Monitoring Policies and Certificates\n\nWe now present our algorithms for runtime monitoring of a policy by monitoring it together with a certificate function. These algorithms present the first step in our solution to the two problems defined above. The second step, namely policy and certificate repair, will follow in the next section. The runtime monitoring algorithms apply to general policies and certificate functions, not necessarily being neural networks.\n\nMotivation for certificate monitoring. Given a dynamical system $\\Sigma = ( \\mathcal { X } , \\mathcal { U } , f )$ and a property of interest, a monitor is a function $\\mathcal { M } : \\mathcal { X } ^ { + }  \\mathbf { \\bar { \\{ 0 , 1 \\} } }$ that maps each finite sequence of system states to a verdict on whether the sequence violates the property. This means that the monitor can only detect violations with respect to properties whose violations can be observed from finite state trajectories. This includes safety properties where violations can be observed upon reaching the unsafe set, but not infinite-time horizon properties like stability which requires the state trajectory to asymptotically converge to the goal set in the limit. In contrast, monitoring both the control policy and the certificate function allows the monitor to issue a verdict on either (1) property violation, or (2) certificate violation, i.e. violation of one of the defining conditions in Proposition 1 for barrier functions or Proposition 2 for Lyapunov functions. By Propositions 1 and 2, we know that the barrier function or the Lyapunov function being correct provides a formal proof of the safety or the stability property. Hence, in order to show that there are no property violations and that the property of interest is satisfied, it suffices to show that there are no certificate violations which can be achieved by monitoring both the control policy and the certificate function.\n\n# Certificate Policy Monitor\n\nWe call our first monitor the certificate policy monitor (CertPM). For each finite sequence of observed states $x _ { 0 } , x _ { 1 } , \\ldots , x _ { n }$ , the monitor $\\mathcal { M } _ { \\sf C e r t P M }$ issues a verdict on whether a property or a certificate violation has been observed. If we are considering a safety property, the property violation verdict is issued whenever $x _ { n } \\in \\mathcal { X } _ { u }$ , and the certificate violation verdict is issued whenever one of the three defining conditions in Proposition 1 is violated at $x _ { n }$ . If we are considering a stability property, the property violation verdict cannot be issued, however, the certificate violation verdict is issued whenever one of the defining conditions in Proposition 2 is violated at $\\scriptstyle { x _ { n } }$ . In the interest of space, in what follows we define the monitor $\\mathcal { M } _ { \\sf C e r t P M }$ for a safety property specified by the unsafe set of states $\\mathcal { X } _ { u }$ . The definition of $\\mathcal { M } _ { \\sf C e r t P M }$ for a stability property is similar, see the extened versio. Let $\\pi$ be a policy and $\\boldsymbol { B }$ be a barrier function:\n\n• The safety violation verdict is issued if $x _ { n } \\in \\mathcal { X } _ { u }$ . We set $\\mathcal { M } _ { \\sf C e r t P M } ( x _ { 0 } , x _ { 1 } , \\dots , x _ { n } ) = 1$ .   \n• The certificate violation verdict for the Initial condition in Proposition 1 is issued if $x _ { n } \\in \\mathcal { X } _ { 0 }$ is an initial state but $B ( x _ { n } ) < 0$ . We set $\\mathcal { M } _ { \\sf C e r t P M } ( x _ { 0 } , x _ { 1 } , \\dots , x _ { n } ) = 1$ .   \n• The certificate violation verdict for the Safety condition in Proposition 1 is issued if $B ( x _ { n } ) ~ < ~ 0$ . We set $\\mathcal { M } _ { \\sf C e r t P M } ( x _ { 0 } , x _ { 1 } , \\dots , x _ { n } ) = 1$ .   \n• Checking the Non-decreasing condition in Proposition 1 is more challenging since the dynamical system evolves over continuous-time and the non-decreasing condition involves a Lie derivative. To address this challenge, we approximate the non-decreasing condition by considering the subsequent observed state $x _ { n + 1 }$ and approximating the Lie derivative via\n\n$$\n\\widehat { L _ { f } B } ( x _ { n } ) = \\frac { \\displaystyle B ( x _ { n + 1 } ) - B ( x _ { n } ) } { \\displaystyle t _ { n + 1 } - t _ { n } } .\n$$\n\nThis requires the monitor to wait for at least one new observation be‘fore issuing the verdict. The approximation satisfies $\\widehat { | L _ { f } B ( x ) - L _ { f } B ( x ) | } \\ \\leq \\ \\epsilon$ for all $x \\in \\mathcal { X }$ where $\\begin{array} { r } { \\epsilon = \\frac 1 2 \\Delta _ { t } ( \\mathcal { C } _ { B } \\mathcal { L } _ { f } + \\mathcal { C } _ { f } \\mathcal { L } _ { B } ) \\mathcal { C } _ { f } } \\end{array}$ , with $\\mathcal { L } _ { B }$ and $\\mathcal { L } _ { f }$ being the Lipschitz constants of $\\boldsymbol { B }$ and $f$ bounded by constants $\\mathcal { C } _ { B } , \\mathcal { C } _ { f } \\in \\mathbb { R } _ { > 0 }$ (Nejati et al. 2023). This suggests we can achieve good precision by using sufficiently small time intervals for monitoring. The certificate violation verdict for the Non-decreasing condition in Proposition 1 is issued if $B ( x _ { n } ) \\geq 0$ but $\\widehat { L _ { f } B } ( x _ { n } ) + \\bar { B } ( x _ { n } ) < 0$ . We set $\\mathcal { M } _ { \\sf C e r t P M } ( x _ { 0 } , x _ { 1 } , \\dots , x _ { n } ) = 1$ . • Otherwise, no violation verdict is issued. We set $\\mathcal { M } _ { \\sf C e r t P M } ( x _ { 0 } , x _ { 1 } , \\dots , x _ { n } ) = 0$ .\n\n1. $v _ { U }$ is an estimate of the time until $\\mathcal { X } _ { u }$ is reached and hence the safety property is violated (or, if $x _ { n } \\in \\mathcal { X } _ { u }$ already, $v _ { U }$ is an estimate of the time until the safe part $\\mathcal { X } \\backslash \\mathcal { X } _ { u }$ is reached).   \n2. $v _ { S }$ is an estimate of the time until the set $\\{ x \\in \\mathcal { X } \\mid$ $B ( x ) < 0 \\}$ may be reached and hence the Safety condition in Proposition 1 may be violated;   \n3. $v _ { N }$ is an estimate of the time until the Non-dec. condition in Proposition 1 may be violated.\n\nNote that these three values are estimates on the remaining time until some set $S \\subseteq { \\mathcal { X } }$ is reached, where $S = \\mathcal { X } _ { u }$ (or $S \\ = \\ \\mathcal { X } \\backslash \\mathcal { X } _ { u }$ if $x _ { n } \\ \\in \\ \\mathcal { X } _ { u } ,$ ) for computing $v _ { U }$ , $S \\ = \\ \\{ x \\ \\in$ $\\chi \\mid B ( x ) < 0 \\}$ for computing $v _ { S }$ , and $S ~ = ~ \\{ x ~ \\in ~ \\mathcal { X } ~ \\mid$ $\\widehat { L _ { f } B } ( x ) + B ( x ) < 0 \\}$ for computing $v _ { N }$ . PredPM computes each of the three values by solving the following problem:\n\n$$\n\\begin{array} { c } { \\displaystyle \\operatorname* { m i n } _ { a } \\mathcal { T } \\mathrm { s . t . } \\frac { d x } { d t } = v ( t ) , \\frac { d v } { d t } = a ( t ) , | a ( t ) | \\leq a _ { m a x } , } \\\\ { \\forall t \\in [ 0 , \\mathcal { T } ] ; x ( \\mathcal { T } ) \\in S , x ( 0 ) = x _ { n } , v ( 0 ) = v _ { 0 } . } \\end{array}\n$$\n\nHere, $\\boldsymbol { v } ( t )$ is the velocity and $a ( t )$ is the acceleration of the trajectory $x ( t )$ , with $a _ { m a x }$ being the maximum allowed acceleration. Intuitively, solving this optimization problem results in the acceleration that the controller should use at each time such that the set $S$ is reached in the shortest time possible. To make the computation physically more realistic, we assume a physical bound $a _ { m a x }$ on the maximum acceleration that the controller can achieve at any time. To compute an approximate solution to this problem and hence estimate $v _ { U } , v _ { S }$ and $v _ { N }$ , PredPM discretizes the time $[ 0 , \\mathcal { T } ]$ and for each discrete time point it uses stochastic gradient descent to select the next acceleration within the range $[ 0 , a _ { m a x } ]$ .\n\nOnce PredPM computes $v _ { U } , v _ { S } , v _ { N }$ , it checks if any of the values exceed the thresholds $\\xi _ { U } , \\xi _ { S } , \\xi _ { N }$ that are assumed to be provided by the user. The monitor issues the verdict $\\mathcal { M } _ { \\sf P r e d P M } ( x _ { 0 } , x _ { 1 } , \\dots , x _ { n } ) = 1$ if either $v _ { U } > \\xi _ { U }$ or $v _ { S } > \\xi _ { S }$ or $v _ { N } > \\xi _ { N }$ . Otherwise, it issues the verdict 0. Having quantitative assessments allows the user to specify how faulttolerant the monitor should be. For highly safety-critical systems, one can set the thresholds to be positive such that the monitor signals a warning in advance, i.e., when it predicts a dangerous situation and before it happens.\n\n# Predictive Policy Monitor\n\nCertPM checks if the property or one of the certificate defining conditions is violated at the states observed so far. Our second monitor, which we call the predictive policy monitor (PredPM), considers the case of safety properties and estimates the remaining time before the property or the certificate violation may occur in the future. PredPM then issues a verdict based on whether any of the estimated remaining times are below some pre-defined thresholds. Thus, our second monitor aims to predict future behavior and issue property and certificate violation verdicts before they happen.\n\nSince PredPM is restricted to safety properties, let $\\mathcal { X } _ { u }$ be the set of unsafe states. Let $\\pi$ be a policy and $\\boldsymbol { B }$ be a barrier function. For each finite sequence of observed states $x _ { 0 } , x _ { 1 } , \\ldots , x _ { n }$ , PredPM computes three quantitative assessments $\\big [ v _ { U } , v _ { S } , v _ { N } \\big ]$ , where:\n\n# Neural Policy and Certificate Repair\n\nOur monitors in the previous section are able to flag finite state trajectories that may lead to property or certificate violations. We now show how the verdicts issued by our monitors can be used to extract additional training data that describe the property or certificate violations. Hence, if the control policy and the certificate are both learned as neural networks, we show how they can be retrained on this new data and repaired. In the interest of space, we consider the case of safety properties where certificates are barrier functions. Suppose that $\\mathcal { X } _ { u }$ is a set of unsafe states in a dynamical system $\\Sigma = ( \\mathcal { X } , \\mathcal { U } , f )$ . The pseudocode of our monitoringbased repair algorithm is shown in Algorithm 1. Our method can be easily modified to allow Lyapunov function repair and we provide this extension in the extended version $\\mathrm { \\Delta \\Psi _ { Y u } }$ , Zikelic, and Henzinger 2024).\n\n<html><body><table><tr><td></td><td>work policy and certificate repair for safety properties.</td></tr><tr><td rowspan=\"8\"></td><td>Input: policy π,barrier function B,number D,</td></tr><tr><td>time points to=O<t1<·<tN,</td></tr><tr><td>(optional) thresholds £u,£s ξN for PredPM</td></tr><tr><td>1:M ← BUILDMONITOR(X)</td></tr><tr><td>>initializinga monitor, either CertPMor PredPM</td></tr><tr><td>2: DNew-data ← @ > new training data collected 3:X ← D states randomly sampled from Xo</td></tr><tr><td>4: for xo ∈ X do</td></tr><tr><td>x(t) ← state trajectory from x(O) = xo</td></tr><tr><td>6: 7:</td><td>for n ∈{0,1,...,N} do Xn ← observed state at time point tn</td></tr><tr><td>8:</td><td>..,xn）=1then if M(xo,x1,.</td></tr><tr><td>9:</td><td>DNew-data ← DNew-data U {xn}</td></tr><tr><td>10:</td><td>end if</td></tr><tr><td>11:</td><td>end for</td></tr><tr><td>12:</td><td>end for</td></tr><tr><td></td><td>Drepair,Depair Drepair</td></tr><tr><td>13:</td><td>Init， Safe Non-dec ←DNew-data  Xo,</td></tr><tr><td></td><td>DNew-dataXu,DNew-data  {x |B(x) ≥ O}</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>14: if policy certification and repair then</td></tr><tr><td></td><td></td></tr><tr><td>15:</td><td></td></tr><tr><td></td><td>π,B ← repair with loss function in eq.(1) and train-</td></tr><tr><td></td><td></td></tr><tr><td></td><td>ing dataD，Ddcc</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>17:</td><td>16:else if certificate certification and repair then</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>B ← repair with loss function in eq.(1) and training</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>data Drepair,Drepair</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Drepair</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Safe Non-dec</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>18:end if</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>Init，</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></table></body></html>\n\nLearning-based control with certificates. Before showing how to extract new training data and use it for policy and certificate repair, we first provide an overview of the general framework for jointly learning neural network policies and barrier functions employed by existing learning-based control methods (Zhao et al. 2020). Two neural networks are learned simultaneously, by minimizing a loss function which captures each of the defining conditions of barrier functions in Proposition 1. That way, the learning process is guided towards learning a neural control policy that admits a barrier function and hence satisfies the safety property. The loss function contains one loss term for each defining condition:\n\n$$\n\\mathcal { L } ( \\theta , \\nu ) = \\mathcal { L } _ { \\mathrm { I n i t } } ( \\theta , \\nu ) + \\mathcal { L } _ { \\mathrm { S a f e } } ( \\theta , \\nu ) + \\mathcal { L } _ { \\mathrm { N o n - d e c } } ( \\theta , \\nu ) ,\n$$\n\nwhere $\\theta$ and $\\nu$ are vectors of parameters of neural networks $\\pi _ { \\boldsymbol { \\theta } }$ and $B _ { \\nu }$ , respectively, and\n\n$$\n\\begin{array} { r } { \\mathcal { L } _ { \\mathrm { l n i t } } ( \\theta , \\nu ) = \\displaystyle \\frac { 1 } { | D _ { \\mathrm { l n i t } } | } \\sum _ { x \\in D _ { \\mathrm { l n i t } } } \\operatorname* { m a x } ( - \\mathcal { B } _ { \\nu } ( x ) , 0 ) ; } \\\\ { \\mathcal { L } _ { \\mathrm { s a t e } } ( \\theta , \\nu ) = \\displaystyle \\frac { 1 } { | D _ { \\mathrm { S a f e } } | } \\sum _ { x \\in D _ { \\mathrm { S a f e } } } \\operatorname* { m a x } ( \\mathcal { B } _ { \\nu } ( x ) , 0 ) ; } \\\\ { \\mathcal { L } _ { \\mathrm { N o n - d e c } } ( \\theta , \\nu ) = \\displaystyle \\frac { 1 } { | D _ { \\mathrm { N o n - d e c } } | } \\cdot } \\\\ { \\mathcal { L } _ { \\mathrm { c o n - d e c } } ( \\theta , \\nu ) \\operatorname* { m a x } \\Big ( - \\overbrace { L _ { f \\theta } \\mathcal { B } _ { \\nu } } ( x ) - \\mathcal { B } _ { \\nu } ( x ) , 0 \\Big ) . } \\end{array}\n$$\n\nIn words, $D _ { \\mathrm { I n i t } }$ , $D _ { \\mathrm { S a f e } }$ , $D _ { \\mathrm { N o n - d e c } } \\subseteq \\mathcal { X }$ are the training sets of system states used for each loss term that incurs loss whenever the defining condition in Proposition 1 is violated. The term $\\widehat { L _ { f _ { \\theta } } B _ { \\nu } } ( x )$ in ${ \\mathcal { L } } _ { \\mathrm { N o n - d e c } }$ is approximated by executing the system dynamics from state $x$ for a small time interval. Since we are interested in repair and not the initialization of $\\pi _ { \\boldsymbol { \\theta } }$ and $B _ { \\nu }$ , we omit the details on how this training data is collected and refer the reader to (Qin, Sun, and Fan 2022).\n\nMonitoring-based neural policy and certificate repair. We now show how the verdicts of our monitors are used to obtain new training data that describes the property or certificate violations, and how this new training data is used for the policy and certificate repair. Algorithm 1 takes as input the neural network control policy $\\pi$ and neural network barrier function $\\boldsymbol { B }$ , trained as above. It also takes as input a finite set of time points $t _ { 0 } = 0 < t _ { 1 } < \\cdots < t _ { N }$ at which the monitor observes new states, as well as the number $D$ of state trajectories that it monitors. In addition, if the PredPM monitor is to be used, the algorithm also takes as input the three thresholds $\\xi _ { U }$ , $\\xi _ { S }$ , and $\\xi _ { N }$ .\n\nAlgorithm 1 first initializes the monitor $\\mathcal { M }$ by constructing either the CertPM or the PredPM (line 1), the set of new training data $D _ { \\mathrm { N e w - d a t a } }$ which is initially empty (line 2) and a set $\\bar { \\tilde { \\mathcal { X } } } _ { 0 } ~ \\subseteq ~ \\mathcal { X } _ { 0 }$ of $D$ initial states obtained via sampling from the initial set $\\mathcal { X } _ { 0 }$ (line 3). Then, for each initial state $\\boldsymbol { x } _ { 0 } \\in \\tilde { \\mathcal { X } } _ { 0 }$ , it executes the dynamical system to obtain a state trajectory $x ( t )$ from $x ( 0 ) = x _ { 0 }$ (line 5). For each time point $t _ { n }$ , a new state $x _ { n }$ is observed (line 7) and the monitor verdict $\\mathcal { M } ( x _ { 0 } , x _ { 1 } , \\dots , x _ { n } )$ is computed (line 8). If $\\mathcal { M } ( x _ { 0 } , x _ { 1 } , \\ldots , x _ { n } ) = 1$ , i.e. if the monitor issues a property or certificate violation verdict, the new state $\\scriptstyle { x _ { n } }$ is added to the new training dataset $D _ { \\mathrm { N e w - d a t a } }$ (line 8). Once the new data is collected, it is used to initialize the new training datasets $D _ { \\mathrm { I n i t } } ^ { \\mathrm { r e p a i r } } ~ = ~ D _ { \\mathrm { N e w - d a t a } } \\cap \\mathcal { X } _ { 0 }$ , $D _ { \\mathrm { S a f e } } ^ { \\mathrm { r e p a i r } } ~ = ~ D _ { \\mathrm { N e w - d a t a } } \\cap \\mathcal { X } _ { u }$ and rNeopna-irdec = DNew-data ∩ {x | B(x) ≥ 0} (line 9). Finally, the neural network policy $\\pi$ and the neural network barrier function $\\boldsymbol { B }$ are repaired by being retrained on the loss function in eq. (1) with the new training datasets (line 11). If we are only interested in repairing the barrier function $\\boldsymbol { B }$ for a fixed control policy $\\pi$ , only the network $\\boldsymbol { B }$ is repaired while keeping the parameters of $\\pi$ fixed (line 13).\n\n# Experimental Evaluation\n\nWe implemented a prototype of our method in Python 3.6 as an extension of the SABLAS codebase (Qin, Sun, and Fan 2022). SABLAS is a state-of-the-art method and tool for learning-based control of neural network control policies with certificate functions. In order to evaluate our method, we consider the benchmarks from the SABLAS codebase, together with policies and certificate functions learned by the SABLAS method for these benchmarks. We then apply our method to these control policies and certificate functions in order to experimentally evaluate the ability of our method to detect incorrect behaviors and to repair them. Our goal is to answer the following three research questions (RQs):\n\n• RQ1: Is our method able to detect violating behavior and repair neural network policies and certificate functions? This RQ pertains to Problem 1. • RQ2: Is our method able to detect incorrect behavior and repair neural network certificate functions? This RQ pertains to Problem 2 in the Preliminaries Section.\n\n<html><body><table><tr><td></td><td>#DNEw SR(%) BR(%) NDR(%)</td></tr><tr><td>Initialized</td><td>1 93.99 87.03 45.38</td></tr><tr><td>Baseline</td><td>878 96.61 -</td></tr><tr><td>CertPM</td><td>548 99.13 100.00 90.66</td></tr><tr><td>PredPM [0.0,-1]</td><td>146 99.06 100.00 90.11</td></tr><tr><td>PredPM [0,1,-5]</td><td>355 98.67 100.00 90.12</td></tr><tr><td>PredPM[2.2.0]</td><td>1000 99.09 100.00 91.67</td></tr></table></body></html>\n\nTable 1: Results on repairing the control policy and the barrier function for DroneEnv. For PredPM, we evaluate it with three threshold configurations $[ \\xi _ { U } , \\xi _ { S } , \\xi _ { N } ]$ (we chose three configurations for which we observed different numbers $D _ { \\mathrm { N E W } }$ of property or certificate violations; see the extended version (Yu, Zikelic, and Henzinger 2024) for results on more thresholds configurations). Column $D _ { \\mathrm { N E W } }$ shows the total number of property or certificate violations found by each method. The safety rate (SR) is the proportion of time during which the agent stays away from the unsafe set calculated by $\\begin{array} { r } { \\frac { 1 } { T } \\int _ { 0 } ^ { T } [ x ( t ) \\notin \\mathcal { X } _ { u } ] \\mathop { d t } } \\end{array}$ , and BR is the proportion of time during which the system is within the invariant set $\\{ x \\mid B ( x ) \\geq 0 \\}$ , and NDR is the proportion of time during which the barrier function satisfies the non-decreasing condition. The best results for each column are in bold. All results are averaged over 50 further executions.\n\n• RQ3: How strong is predictive power of PredPM monitor, that is, can it predict safety violations ahead of time?\n\nBenchmarks. We consider two benchmarks that are available in the SABLAS codebase (Qin, Sun, and Fan 2022), originally taken from (Fossen 2000; Qin et al. 2021). The first benchmark concerns a parcel delivery drone flying in a city (called the active drone), among 1024 other drones that are obstacles to be avoided. Only the active drone is controlled by the learned policy, whereas other drones move according to pre-defined routes. The state space of this environment is 8-dimensional and states are defined via 8 variables $x = [ x , y , z , v _ { x } , v _ { y } , v _ { z } , \\theta _ { x } , \\theta _ { y } ]$ : three coordinate variables in the 3D space, three velocity variables, and two variables for row and pitch angles. The actions produced by the policy correspond to angular accelerations of $\\theta _ { x } , \\theta _ { y }$ , as well as the vertical thrust. The drone is completing a delivery task at a set destination, hence the property of interest in this task is a stability-while-avoid property.\n\nThe second benchmark ShipEnv concerns a ship moving in a river among 32 other ships. The state space of this environment is 6-dimensional and states are defined via 6 variables $[ x , y , \\theta , u , v , w ]$ . The first two variables specify the 2D coordinates of the ship, $\\theta$ is the heading angle, $u , v$ are the velocities in each direction, and $w$ is the angular velocity of the heading angle. The property of interest in this task is also a stability-while-avoid property with obstacles being collisions with other ships.\n\nExperimental setup. We consider the black-box setting, hence the dynamics are unknown to the monitor and repair algorithm. For each monitored execution, $N \\ : = \\ : 2 0 0 0$ and $N = 1 2 0 0$ states are observed for ShipEnv and DroneEnv, respectively, spaced out at time intervals of $\\Delta _ { t } = 0 . 1 s$ . In each environment, the states of the eight nearest obstacles (i.e. drones or ships) are given as observations to the policy, as well as the certificate functions. In our implementation of the PredPM monitor, we employ Adam (Kingma and $\\mathbf { B a }$ 2014) for approximating the assessments $[ v _ { U } , v _ { S } , v _ { N } ]$ .\n\n![](images/d11034d55585148ca3f260a3f7152e9d8274d65f68a554cf4ac45ca1a9163034.jpg)  \nFigure 2: The change in the number of certificate violations for the Safety condition and the Non-decreasing conditions of barrier functions, for the ship benchmark after one round of repair and after a second round of repair. The xaxis represents the number of runs. The first round monitors $D = 1 5 0 0 0$ system executions. The second round monitors an additional $D = 2 0 0 0 0$ executions. For better readability, we plot only the executions (out of 50) for which at least one certificate violation is detected.\n\nResults: RQ1. We observed that the control policy learned by SABLAS for DroneEnv does not satisfy the stabilitywhile-avoid property on all runs – its safety rate is $9 3 . 9 9 \\%$ whereas it leads to collision with other drones $6 . 0 1 \\%$ of the time, initialized with $1 0 ^ { 4 }$ state samples. We applied our repair method to the control policy and the barrier function learned by SABLAS with $D = 1 0 0 0$ . To evaluate the importance of monitoring both neural policies and certificates, we also compare our method against the baseline approach. The baseline is the simple monitor described in the Introduction, which only monitors a neural policy, flags traces that reach an unsafe state and adds these states to re-training data. Our results are summarized in Table 1. As we can see, both CertPM and PredPM monitors are able to effectively repair the control policy and the barrier function and lead to significantly higher safety rates (SR), reaching $9 9 . 1 3 \\%$ . The proportions of time at which the Safety and the Nondecreasing conditions of barrier functions are satisfied (BR and NDR in Table 1) go up from $8 7 . 0 3 \\%$ to $1 0 0 . 0 0 \\%$ , and from $4 5 . 3 8 \\%$ to $9 1 . 6 7 \\%$ . This demonstrates the advantage of monitoring both the control policy and the certificate toward effective and successful repair. Finally, the comparison between CertPM and PredPM shows that CertPM is slightly better in repairing the control policy, but PredPM is more effective for repairing the barrier function. Due to its predictive nature, PredPM identifies a larger number of property and certificate violations with the right choice of thresholds, resulting in larger re-training data $D _ { \\mathrm { N E W - D A T A } }$ . We conducted an analogous experiment with a certificate function consisting both of a barrier and a Lyapunov function and again observed significant improvements upon repair. Further details, including results on additional threshold configurations of PredPM beyond the three configurations discussed in Table 1, can be found in the extended version (Yu, Zikelic, and Henzinger 2024).\n\n![](images/5e7261bf58b71bca245cb85bbe065b95d0f8c29e642d851c9f924b79c295dec0.jpg)  \nFigure 3: Number of excution steps ( $\\mathbf { \\bar { X } }$ -axis) vs. verdicts (yaxis). Estimates $v _ { U }$ , $v _ { S }$ , and $v _ { N }$ for two different systems executions computed by the PredPM in both environments.\n\nResults: RQ2. We observed that the policy learned by SABLAS for the ship benchmark already achieves SR close to $1 0 0 . 0 0 \\%$ , however, the learned barrier function provides significantly lower BR and NDR. Hence, we use the ship benchmark to answer RQ2 which is concerned with the repair of a certificate function for a given control policy. In this case, a good certificate function acts as a proof of correctness that allows more trustworthy policy deployment. Figure 2 shows what the number of certificate violations for the barrier function looks like before and after repair, for the ship benchmark with CertPM used as a monitor. The results demonstrate that there are significantly fewer certificate violations upon repair. Additionally, we conducted the same experiment for the drone benchmark, and also observed significant level of improvement in BR and NDR upon repair. We refer to the extened version for more results.\n\nResults: RQ3. Recall that PredPM considers safety properties and monitors the control policy together with the barrier function. Upon each new observation, it computes an estimate $v _ { U }$ on the remaining time before the safety property may be violated, $v _ { S }$ on the remaining time before the Safety condition of barrier functions may be violated, and $v _ { N }$ on the remaining time before the Non-decreasing condition of barrier functions may be violated. Figure 3 shows how these estimates change along two different executions for DroneEnv and ShipEnv. It can be seen that $v _ { S }$ becomes negative before $v _ { U }$ , meaning the system is estimated to violate the Safety condition of barrier functions before it reaches the unsafe region. Hence, by tracking $v _ { S }$ , PredPM can predict unsafe behaviors and raise warnings before they happen. In comparison to the others, there are more estimated violations of the Non-decreasing condition. This means that $\\xi _ { N }$ can be set to a lower value, as we tend to consider the other two violations to be more severe. Overall, the experiments suggest PredPM can be particularly well-suited for runtime use, in addition to repairing neural networks.\n\nSummary of results. Our experimental results empirically justify the following claims: (i) Our method is able to successfully repair neural network control policies and certificate functions. Using either CertPM or PredPM for repair leads to significant improvements over the initial policy; (ii) Our method is able to successfully repair neural network certificate functions in the setting where the control policies are fixed; (iii) Using PredPM allows predicting safety property violations before they happen, hence showing potential for practical safety deployment even in the runtime setting.\n\nPractical considerations and limitations. To conclude, we also discuss two practical aspects that one should take into account before the deployment of our method: (i) As highlighted in the Introduction, our method provides no guarantees on the correctness of repaired policies. This means that, in principle, one could end up with a policy whose performance is suboptimal compared to the initial policy. However, we did not observe a single case of such a behavior in our experiments. (ii) It was observed by (Zikelic et al. 2022) that methods for jointly learning policies and certificates rely on a good policy initialization. Hence, our repair method is also best suited for cases when the policy is well initialized by some off-the-shelf method (e.g. with SR at least $90 \\%$ ).\n\n# Conclusion\n\nIn this work, we propose a method for determining the correctness of neural network control policies and certificate functions and for repairing them by utilizing runtime monitoring. Our method applies to the black-box setting and does not assume knowledge of the system dynamics. We present two novel monitoring algorithms, CertPM and PredPM. Our experiments demonstrate the advantage of monitoring policies together with certificate functions and are able to repair neural policies and certificates learned by a state-ofthe-art learning-based control method. Interesting directions of future work would be to consider the repair problem for stochastic systems and multi-agent systems. Another interesting direction would be to explore the possibility of deploying predictive monitors towards enhancing the safety of learned controllers upon deployment, i.e., at runtime.\n\n# Acknowledgments\n\nThis work was supported in part by the ERC project ERC2020-AdG 101020093.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决的核心问题是在黑盒设置（系统动力学未知）下，如何验证和修复神经网络控制策略及其证书函数（如屏障函数和Lyapunov函数）的正确性。\\n> *   该问题的重要性在于：现有方法主要关注白盒设置（动态已知），而黑盒设置下的验证和修复缺乏有效方法，这在安全关键应用（如自动驾驶、医疗设备）中尤为关键。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种基于运行时监控的框架，通过检测违反目标属性或证书条件的行为，提取新的训练数据，并重新训练策略和证书函数以修复它们。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **贡献1：** 设计了两种运行时监控算法（CertPM和PredPM），用于检测策略和证书函数的违规行为。实验表明，CertPM和PredPM能够有效检测违反属性或证书条件的行为。\\n> *   **贡献2：** 提出了一种基于监控的修复方法，通过重新训练策略和证书函数提升其正确性。实验显示，修复后的策略在安全率（SR）上从93.99%提升到99.13%，证书函数的满足率（BR和NDR）也从87.03%和45.38%提升到100%和90.66%。\\n> *   **贡献3：** 展示了PredPM的预测能力，能够提前估计安全属性或证书条件可能被违反的时间，从而在运行时提供早期警告。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   该方法的核心思想是通过运行时监控检测控制策略和证书函数的违反行为，并利用这些违反行为生成新的训练数据，通过重新训练来修复策略和证书函数。这种方法有效利用了监控反馈来迭代改进策略和证书的可靠性。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 先前工作主要集中在白盒设置下验证证书函数的正确性，或仅监控策略而忽略证书函数。这些方法无法在黑盒设置下提供可靠的验证和修复机制。\\n> *   **本文的改进：** 本文提出同时监控策略和证书函数，并利用监控结果生成新的训练数据。CertPM直接检测违反属性或证书条件的行为，而PredPM进一步预测未来可能发生的违反行为，从而更早地触发修复。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> *   1. **初始化监控器：** 构建CertPM或PredPM监控器，用于检测违反属性或证书条件的行为。\\n> *   2. **执行监控：** 在多个初始状态下运行系统，监控器观察状态序列并检测违反行为。\\n> *   3. **提取训练数据：** 将检测到的违反行为对应的状态添加到新的训练数据集。\\n> *   4. **重新训练：** 使用新的训练数据重新训练神经网络策略和证书函数，优化损失函数以满足证书定义条件。\\n> *   5. **迭代修复：** 重复监控和重新训练，直到监控器无法检测到更多违反行为。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   论文中提到的基线方法是一个简单的监控器，仅监控策略并标记到达不安全状态的轨迹。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在安全率（SR）上：** 本文方法在DroneEnv上达到了99.13%，显著优于基线模型（96.61%）。与表现最佳的基线相比，提升了2.52个百分点。\\n> *   **在证书函数满足率（BR和NDR）上：** 本文方法的BR和NDR分别达到100%和90.66%，远高于初始策略的87.03%和45.38%。\\n> *   **在预测能力上：** PredPM能够提前估计安全属性或证书条件可能被违反的时间，从而在运行时提供早期警告，这是基线方法无法实现的。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n> **提取与格式化要求**\\n> *   基于学习的控制 (Learning-based Control, N/A)\\n> *   证书函数 (Certificate Functions, N/A)\\n> *   运行时监控 (Runtime Monitoring, N/A)\\n> *   黑盒设置 (Black-box Setting, N/A)\\n> *   安全关键系统 (Safety-critical Systems, N/A)\\n> *   屏障函数 (Barrier Functions, N/A)\\n> *   李雅普诺夫函数 (Lyapunov Functions, N/A)\\n> *   自动驾驶 (Autonomous Driving, N/A)\"\n}\n```"
}