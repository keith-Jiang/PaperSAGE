{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.11777",
    "link": "https://arxiv.org/abs/2412.11777",
    "pdf_link": "https://arxiv.org/pdf/2412.11777.pdf",
    "title": "Fast and Slow Gradient Approximation for Binary Neural Network Optimization",
    "authors": [
        "Xinquan Chen",
        "Junqi Gao",
        "Biqing Qi",
        "Dong Li",
        "Yiang Luo",
        "Fangyuan Li",
        "Pengfei Li"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "2024-12-16",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Harbin Institute of Technology",
        "Shanghai Artificial Intelligence Laboratory"
    ],
    "paper_content": "# Fast and Slow Gradient Approximation for Binary Neural Network Optimization\n\nXinquan Chen1, Junqi Gao1,2, Biqing $\\mathbf { Q _ { i } ^ { \\bullet 2 * } }$ , Dong ${ \\bf L i } ^ { 1 , 2 }$ , Yiang Luo1, Fangyuan $\\mathbf { L i } ^ { 1 }$ , Pengfei Li1\n\n1Harbin Institute of Technology, Harbin, P.R.China, 2Shanghai Artificial Intelligence Laboratory, Shanghai, P.R.China {xinquanchen0117,gjunqi97,qibiqing7}@gmail.com, arvinlee826,normanluo668,jacklee19900212,lipengfei0208 $@$ gmail.com\n\n# Abstract\n\nBinary Neural Networks (BNNs) have garnered significant attention due to their immense potential for deployment on edge devices. However, the non-differentiability of the quantization function poses a challenge for the optimization of BNNs, as its derivative cannot be backpropagated. To address this issue, hypernetwork based methods, which utilize neural networks to learn the gradients of non-differentiable quantization functions, have emerged as a promising approach due to their adaptive learning capabilities to reduce estimation errors. However, existing hypernetwork based methods typically rely solely on current gradient information, neglecting the influence of historical gradients. This oversight can lead to accumulated gradient errors when calculating gradient momentum during optimization. To incorporate historical gradient information, we design a Historical Gradient Storage (HGS) module, which models the historical gradient sequence to generate the first-order momentum required for optimization. To further enhance gradient generation in hypernetworks, we propose a Fast and Slow Gradient Generation (FSG) method. Additionally, to produce more precise gradients, we introduce Layer Recognition Embeddings (LRE) into the hypernetwork, facilitating the generation of layerspecific fine gradients. Extensive comparative experiments on the CIFAR-10 and CIFAR-100 datasets demonstrate that our method achieves faster convergence and lower loss values, outperforming existing baselines.\n\nCode — https://github.com/two-tiger/FSG\n\n# Introduction\n\nIn recent years, Deep Neural Networks (DNNs) have demonstrated remarkable achievements across various computer vision tasks (Girshick et al. 2014; He et al. 2016; LeCun, Bengio, and Hinton 2015; Everingham et al. 2015). However, the increasing number of layers and parameters in deep learning models poses significant challenges. These challenges often result in substantial model sizes and elevated computational demands, hindering deployment on resource-constrained devices such as smartphones, cameras, and drones. To address these limitations, a plethora of compression techniques have emerged, including network pruning (Ding et al. 2021; Luo, Wu, and Lin 2017), low-rank approximation (Denton et al. 2014; Hayashi et al. 2019), architectural redesign (Chen et al. 2020; Iandola et al. 2016), and network quantization (Banner et al. 2018; Ajanthan et al. 2021). Among these, network quantization—which employs reduced-precision data types for weight storage and computations—holds significant promise. For instance, Binary Neural Networks (BNNs), which restrict weights to binary values ( $- 1$ and $+ 1$ ) (Courbariaux, Bengio, and David 2015), can achieve remarkable efficiency by saving approximately 32 times in memory and boosting inference speed by a factor of 58 compared to full-precision networks (Rastegari et al. 2016).\n\nDue to the non-differentiable nature of the sign function, conventional backpropagation is hindered in updating gradients for BNNs. BinaryConnect (Courbariaux, Bengio, and David 2015) addresses this challenge by introducing the Straight-Through Estimator (STE), a technique used to approximate the gradient of the sign function during backpropagation. This method has become the fundamental paradigm for BNN optimization due to its effectiveness. However, a significant issue remains: the discrepancy between the binary quantization applied during forward propagation and the gradient estimation during backpropagation leads to a gradient mismatch. This mismatch introduces inherent estimation errors, which impede the convergence and optimization of BNNs. As gradient propagation continues, these errors accumulate, further limiting the effectiveness of BNNs optimization.\n\nTo address the aforementioned challenges, a variety of methods have been developed. These include redesigned straight-through estimators (Qin et al. 2020; Xu et al. 2021; Wu et al. 2023; Lin et al. 2020) and hypernetwork based methods, which utilize differentiable neural networks (Chen, Wang, and Pan 2019; Liu et al. 2020) to directly learn the derivatives of non-differentiable functions, converting nondifferentiable parts into differentiable operations for optimization. However, the addition of neural networks introduces more complex operations and higher computational costs, which has led to the potential of hypernetwork based methods being overlooked. In fact, the ultimate goal of BNNs is to deploy on resource limited devices, and the training process for obtaining a quantized network version is not constrained by the computational limitations of the target device. Moreover, despite careful design, straightthrough estimators cannot perfectly approximate the derivative of the binary thresholding function, resulting in inherent estimation errors and gradient mismatch. Hypernetwork based methods offer a solution by dynamically adjusting the gradient approximation during training, thereby continually reducing estimation errors. This capability allows BNNs to optimize more flexibly and achieve higher performance. Nonetheless, the current hypernetwork based methods (Chen, Wang, and Pan 2019) use the current weight gradient and the full precision weight to generate the final gradient. This method of generating gradients uses limited information and ignores the guiding role of historical gradients. Yet the optimization objective of the hypernetwork aligns with that of the final task, rather than the objective of generating a more suitable weight gradient, and does not match the optimization goal of generating a more appropriate weight gradient.\n\nMotivated by the concept of momentum in Stochastic Gradient Descent with Momentum (SGD-M), which uses historical gradients to inform the current gradient, we have designed a Historical Gradient Storage (HGS) module. Momentum stabilizes the direction of gradient descent and accelerates optimization. Our HGS module stores a portion of historical gradients, treating them as time-series data that encapsulate gradient change information. These historical gradient sequences are then input into the hypernetwork to generate weight gradients. Compared to using only the current gradient for learning, incorporating historical gradients provides additional context, enabling the network to generate gradients that more appropriate for BNN optimization. This approach enhances the optimization process for BNNs, facilitating more effective and efficient training.\n\nTo generate gradients more precisely, we have designed the Fast and Slow gradient Generation (FSG) method, inspired by the optimization method of SGD-M. This method employs two hypernetworks, referred to as fast-net and slow-net. Slow-net utilizes models with strong longsequence modeling capabilities, such as Mamba (Gu and Dao 2023) and LSTM (Hochreiter and Schmidhuber 1997), to capture the essence of historical gradient sequences in order to generate gradients consistent with the concept of momentum. This process requires summarizing accumulated historical data, similar to slow evolution. While fastnet utilizes a Multi-Layer Perceptron (MLP) to extract highdimensional features from the current gradient, which can quickly generate the current weight gradient. The final gradient is obtained by combining these two components. Gradient generation is layer-specific, so in order to further guide slow-net to generate appropriate momentum, we add Layer Recognition Embeddings (LRE). Specifically, we add a learnable layer embedding vector as the recognition of layer information. Compared with previous methods using a single hypernetwork, FSG enriches the process of gradient generation, allowing historical gradients to participate in gradient generation and providing richer information for gradient generation.\n\nWe conduct comprehensive experiments on the CIFAR10 and CIFAR-100 datasets. The results demonstrate that our method outperforms various competitive benchmarks. In summary, our contributions are as follows:\n\n• Inspired by the idea of momentum, we examine the impact of historical gradients on gradient generation. To this end, we propose a HGS module for storing historical gradient sequences. By modeling these sequences, we generate gradient momentum, which guides the network in producing more suitable gradients. • We design a FSG method to guide the gradient generation process, fully leveraging the information from both historical and current gradients to produce more refined gradients. Additionally, we propose LRE to assist the hypernetwork in learning layer-specific gradients. • Extensive experiments on the CIFAR-10 and CIFAR100 datasets demonstrate that our proposed FSG method achieves superior performance and faster convergence.\n\n# Related Work\n\nThe origins of BNNs can be traced back to Courbariaux’s groundbreaking work (Courbariaux et al. 2016), where they employ a sign function to binarize network weights. To address the challenge of the sign function’s non-differentiability during backpropagation, BinaryConnect (Courbariaux, Bengio, and David 2015) introduces the STE as an effective approximation technique. The outstanding performance of STE has since become the cornerstone of optimization strategies in this field.\n\nLater work primarily focuses on constraining the key aspect of BNN optimization: the derivative of the sign function. For instance, IR-Net (Qin et al. 2020) employs an Error Decay Estimator (EDE) in a two-stage process to gradually approximate the sign function and reduce gradient error. RBNN (Lin et al. 2020) introduces a training-aware approximation of the sign function for gradient computation during backpropagation. FDA (Xu et al. 2021) utilizes combinations of sine functions in the Fourier frequency domain to estimate the gradient of the sign function during BNN training and employe neural networks to learn the approximation error. ReSTE (Wu et al. 2023) leverages power functions to design a correction estimator that balances estimation error and gradient stability.\n\nDespite efforts to design approximation functions, inherent estimation errors persist between these approximations and the sign function’s derivatives, which are fundamentally uneliminable. Consequently, some recent approaches employ hypernetworks to transform the non-differentiable components of binary neural networks into differentiable operations. For instance, MetaQuant (Chen, Wang, and Pan 2019) proposes incorporating a neural network into the backpropagation process to learn the non-differentiable gradients. Similarly, QuantNet (Liu et al. 2020) utilizes a differentiable subnetwork to directly binarize full-precision weights, avoiding reliance on the STE or any learnable gradient estimator. Hypernetwork based methods (Andrychowicz et al. 2016) offer the advantage of adaptively adjusting gradient approximations during training, thereby reducing approximation errors. Unlike manually designed STE, these methods can dynamically refine the gradient approximations. However, they are limited in their ability to generate precise gradients. While the network learns based on the final loss, the optimization objective of hypernetworks is to generate better gradients for the sign function, leading to differing optimization goals.\n\n# Preliminaries\n\n# Definition of BNNs\n\nBNNs are often derived from fully accurate networks (Rastegari et al. 2016). For a neural network model $f$ with $n$ layers, parameterized by ${ \\mathcal { W } } = [ W _ { 1 } , W _ { 2 } , \\dots , W _ { n } ]$ , where $W _ { i }$ represents the weight of the $i$ -th layer, we define a pre-processing function $\\boldsymbol { \\mathcal { A } } ( \\cdot )$ and a quantization function $Q ( \\cdot )$ to binarize the weights. This article investigates the pre-processing and quantization methods of DoReFa (Zhou et al. 2016). The pre-processing function $\\boldsymbol { \\mathcal { A } } ( \\cdot )$ normalizes the weights, while the quantization function $Q ( \\cdot )$ converts each value in the weight matrix to $\\{ - 1 , + 1 \\}$ . We have,\n\n$$\n\\begin{array} { r l } & { \\hat { \\mathcal { W } } = \\mathcal { A } ( \\mathcal { W } ) = \\cfrac { \\operatorname { t a n h } ( \\mathcal { W } ) } { 2 \\operatorname* { m a x } ( | \\operatorname { t a n h } ( \\mathcal { W } ) | ) } + \\frac { 1 } { 2 } , } \\\\ & { \\mathcal { W } _ { b } = Q ( \\hat { \\mathcal { W } } ) = 2 \\cfrac { \\operatorname { r o u n d } \\big [ ( 2 ^ { k } - 1 ) \\hat { \\mathcal { W } } \\big ] } { 2 ^ { k } - 1 } - 1 . } \\end{array}\n$$\n\nFor a training set $\\{ \\mathbf { x } , \\mathbf { y } \\}$ with $\\mathbf { N }$ instances, we incorporate quantization in the forward process. Consequently, the loss function for training a binary neural network is given by $\\ell ( f ( Q ( A ( \\mathcal { W } ) ) ; \\mathbf { x } ) , \\bar { \\mathbf { y } } )$ . During the backpropagation process, we compute the derivative of $\\ell$ with respect to $\\mathcal { W }$ ,\n\n$$\n\\frac { \\partial \\ell } { \\partial \\mathcal { W } } = \\frac { \\partial l } { \\partial Q ( \\mathcal { A } ( \\mathcal { W } ) ) } \\frac { \\partial Q ( \\mathcal { A } ( \\mathcal { W } ) ) } { \\partial \\mathcal { A } ( \\mathcal { W } ) } \\frac { \\partial \\mathcal { A } ( \\mathcal { W } ) } { \\partial \\mathcal { W } } ,\n$$\n\nwhere ∂Q∂(A((W))) is the Dirac delta function. This introduces a discontinuity in the backpropagation process, causing it to be interrupted at this point.\n\n# Hypernetwork Based Method\n\nHypernetwork based methods (Chen, Wang, and Pan 2019) introduce a shared neural network $\\mathcal { M }$ , which is trained simultaneously with the binary neural network. During the backpropagation process, the hypernetwork $\\mathcal { M }$ receives two inputs, $\\begin{array} { r } { g _ { \\mathcal { W } } = \\frac { \\partial \\ell } { \\partial \\mathcal { W } } } \\end{array}$ and $\\hat { \\mathcal { W } }$ , and generates the derivative of $\\ell$ to the quantization function $Q ( { \\bar { \\mathcal { A } } } ( \\cdot ) )$ , i.e.\n\n$$\n\\frac { \\partial \\ell } { \\partial A ( \\cdot ) } = \\mathcal { M } _ { \\phi } ( g _ { \\mathcal { W } _ { b } } , \\hat { \\mathcal { W } } ) ,\n$$\n\nthe pre-processing function is differentiable, so the final derivative is,\n\n$$\n\\frac { \\partial \\ell } { \\partial \\mathcal { W } } = \\mathcal { M } _ { \\phi } ( g _ { \\mathcal { W } } , \\hat { \\mathcal { W } } ) \\frac { \\partial \\mathcal { A } ( \\mathcal { W } ) } { \\partial \\mathcal { W } } ,\n$$\n\nplease note that in the first iteration, the hypernetwork is not working as the gradient has not yet been generated. At this point, the STE function will be temporarily used as a substitute.\n\n# Methodology Motivation: Optimization Requires Historical Gradients\n\nGradient descent, a cornerstone optimization algorithm as described in (Saad 1998), relies on the gradient of the objective function with respect to the parameters. However, its reliance on instantaneous gradient can lead to premature convergence in local minima, hindering escape. To address this limitation, the momentum method incorporates exponential weighting from moving averages. By averaging past and current gradients with exponentially decaying weights, the method imparts inertia to the update direction. This inertia helps the algorithm to escape local minima more effectively, leading to improved convergence. SGD-M has the following update formula,\n\n$$\n\\begin{array} { r } { \\left\\{ \\begin{array} { l l } { \\mathbf { v } _ { t + 1 } = \\beta \\mathbf { v } _ { t } - \\alpha \\nabla f ( \\mathbf { x } _ { t } ) , } \\\\ { \\mathbf { x } _ { t + 1 } = \\mathbf { x } _ { t } + \\mathbf { v } _ { t + 1 } , } \\end{array} \\right. } \\end{array}\n$$\n\nwhere $\\mathbf { v } _ { t + 1 }$ is the momentum, $\\beta$ is the momentum constant, $\\alpha$ is the step size, also known as the learning rate, $\\nabla f ( \\mathbf { x } _ { t } )$ is the gradient of $f$ at $\\mathbf { x } _ { t }$ , and the momentum $\\mathbf { v } _ { t + 1 }$ can be expanded to:\n\n$$\n\\begin{array} { r l } & { \\mathbf { v } _ { t + 1 } = \\beta \\mathbf { v } _ { t } - \\alpha \\nabla f ( \\mathbf { x } _ { t } ) } \\\\ & { \\qquad = \\beta ^ { 2 } \\mathbf { v } _ { t - 1 } - \\beta \\alpha \\nabla f ( \\mathbf { x } _ { t - 1 } ) - \\alpha \\nabla f ( \\mathbf { x } _ { t } ) } \\\\ & { \\qquad = \\beta ^ { 3 } \\mathbf { v } _ { t - 2 } - \\beta ^ { 2 } \\alpha \\nabla f ( \\mathbf { x } _ { t - 2 } ) - \\beta \\alpha \\nabla f ( \\mathbf { x } _ { t - 1 } ) } \\\\ & { \\qquad - \\alpha \\nabla f ( \\mathbf { x } _ { t } ) \\dots } \\end{array}\n$$\n\nFrom the above expansion of momentum, we can see that momentum is a linear combination of historical gradients, which means that the direction and magnitude of historical gradients can guide the direction of the current update and the step size in optimization. Therefore, it can be considered that when optimizing non-differentiable functions, it is also possible to benefit from historical gradients. We focuses on the role of historical gradients and introduces a module for storing historical gradients, called the Historical Gradient Storage (HGS) module, which saves the most recent $l$ step gradients.\n\nWe treat the saved historical gradient sequence as a timeseries data containing gradient change information. Mamba is a state-space model with a selection mechanism that can effectively filter out noise in historical gradient sequences, efficiently integrate historical gradients and provide more accurate momentum predictions. Therefore, we use a shared Mamba block (Gu and Dao 2023) to complete the modeling of the gradient sequence of each layer. We explain in detail the reasons for choosing Mamba as the momentum generator in the Appendix based on the principles of Mamba, and compare the performance of different sequence modeling models as momentum generators in Ablation Experiment.\n\n# Fast and Slow Gradient Generation Method\n\nHistorical Gradient Storage Module. In order to save the historical gradient, we introduce a module for storing historical gradients, termed the HGS module, and denote as $\\mathcal { H } = \\{ h _ { 1 } , h _ { 2 } , . . . , h _ { n } \\}$ , where $h _ { i }$ represents the historical gradient snapshot of the $i$ -th layer. For simplicity, we assume\n\n![](images/4cae13ab780b33ebc3457a95be51b601f16dd995bf1d64a00656cd56553ea415.jpg)  \nFigure 1: Fast and Slow Gradient Generation Illustration. Take ResNet as an example. During the backpropagation, the weight gradients from the previous iteration step are fed into the HGS and fast-net. Fast-net uses MLP to learn the scale of the weights, thereby obtaining the fast grad. The slow-net receives the historical gradient sequence from HGS and adds a LRE vector at the front of the sequence, then uses the mamba block to generate the slow grad. Ultimately, the slow grad and fast grad are combined through a weighted sum to generate the final gradients, replacing the non-differentiable parts (indicated by the blue dashed arrows). The forward process will be explained in Section Training of FSG.\n\nthat $n$ layers of convolutional layers need to be generated, and the weights of the neural network are represented as $\\mathcal { W } = [ W _ { 1 } , W _ { 2 } , . . . , W _ { n } ]$ , where $W _ { i } \\in \\mathbb { R } ^ { C _ { o u t } ^ { i } \\times C _ { i n } ^ { \\bar { i } } \\times K ^ { i } \\times K ^ { i } }$ is the weight of the $i$ -th convolutional layer has a corresponding gradient $g _ { W _ { i } } \\in \\mathbb { R } ^ { C _ { o u t } ^ { i } \\times C _ { i n } ^ { i } \\times K ^ { i } \\times \\dot { K ^ { i } } }$ . We flatten the gradient to one dimension and save it as $g _ { w _ { i } }  \\overline { { g } } _ { w _ { i } } \\in \\mathbb { R } ^ { \\xi ^ { i } \\times 1 }$ , where $\\xi ^ { i } = { C _ { o u t } ^ { i } } \\cdot { C _ { i n } ^ { i } } \\cdot { K ^ { i } } \\cdot { K ^ { i } }$ , to control memory usage, we only retain the gradients from the most recent $l$ steps,\n\n$$\n\\begin{array} { r } { h _ { i } ^ { t } = \\operatorname { C o n c a t } [ \\overline { { g } } _ { w _ { i } } ^ { t - l + 1 } , . . . , \\overline { { g } } _ { w _ { i } } ^ { t - 1 } , \\overline { { g } } _ { w _ { i } } ^ { t } ] \\in \\mathbb { R } ^ { \\xi ^ { i } \\cdot l \\times 1 } , } \\end{array}\n$$\n\nwhere Concat denotes the concatenation along the appropriate dimension to form a vector of length $\\xi ^ { i } \\cdot l , \\overline { { g } } _ { W _ { i } } ^ { t }$ is the flattened gradient of the weight $W _ { i }$ at step $t$ .\n\nFast and Slow Gradient Generation Mechanism. As shown in the Fig. 1, we introduce two hypernetworks shared by each layer $\\mathcal { M } _ { f }$ and $\\mathcal { M } _ { s }$ , named fast-net and slow-net respectively. Fast-net receives the full-precision weights $\\hat { W } _ { i } ^ { t }$ of the previous step and the gradients $g _ { W _ { i } } ^ { t }$ of the previous step to generate the gradients $g _ { \\mathcal { W } } ^ { t + 1 }$ for updating. Slow-net receives the historical gradient sequence $\\bar { h _ { i } ^ { t } }$ provided by the HGS module to generate the gradient momentum $\\mathbf { v } _ { t + 1 }$ for updating. Therefore, there is the following update form, for the $i$ -th layer,\n\n$$\n\\mathbf { W } _ { i } ^ { t + 1 } = \\mathbf { W } _ { i } ^ { t } - \\alpha \\mathcal { M } _ { f } ( g _ { W _ { i } } ^ { t } , \\hat { W } _ { i } ^ { t } ) + \\beta \\mathcal { M } _ { s } ( h _ { i } ^ { t } )\n$$\n\nSlow-net is responsible for modeling historical gradient sequences to generate gradients, which is consistent with the gradient momentum composed of historical gradients. The generation of momentum requires the accumulation of historical information, similar to a slow evolution. Fast-net, on the other hand, learns the high-dimensional features of the current gradient and is a fast gradient generation method.\n\nLayer Recognition Embedding. To enable the Mamba block to discern the layer-specific information and prevent the gradients from confusing each other during the training process, inspired by ViM (Zhu et al. 2024) and BERT (Devlin et al. 2018), we propose LRE, which initializing a learnable embedding vector for each layer of the model $\\mathbb { E } \\in \\mathbb { R } ^ { n \\times d }$ , $d$ is the dimension of the embedding space. For a given layer index $i$ , LRE will return the corresponding embedding vector, which we define as $\\mathbf { t } _ { i } \\in \\mathbb { R } ^ { d }$ . Define $\\boldsymbol { \\hbar } \\stackrel { - } { = } [ \\hbar _ { 1 } , \\hbar _ { 2 } , . . . , \\hbar _ { n } ]$ , where $\\hbar _ { i }$ is the input vector formed by concatenating the vector $\\mathbf { t } _ { i }$ with the projected historical gradient.\n\n$$\n\\begin{array} { r l } & { \\quad \\hbar _ { i } ^ { t } = [ \\mathbf { t } _ { i } ; \\overline { { g } } _ { w _ { i } } ^ { t - l } \\mathbf { W } _ { a } , . . . , \\overline { { g } } _ { w _ { i } } ^ { t - 1 } \\mathbf { W } _ { a } , \\overline { { g } } _ { w _ { i } } ^ { t } \\mathbf { W } _ { a } ] } \\\\ & { \\quad \\quad = [ \\mathbf { t } _ { i } ; h _ { i } \\mathbf { W } _ { a } ] \\in \\mathbb { R } ^ { ( \\xi ^ { i } \\cdot l + 1 ) \\times d } } \\end{array}\n$$\n\nwhere $\\mathbf { W } _ { a } \\in \\mathbb { R } ^ { 1 \\times d }$ is a projection matrix that maps each flattened gradient $\\overline { { g } } _ { w _ { i } } ^ { t - k }$ onto a $d$ dimensional space. Next, we input $\\hbar _ { i } ^ { t }$ into the Mamba block, we slice the output vector, take the last $\\xi ^ { i }$ sequences, and project them to a 1- dimensional vector,\n\n$$\n\\overline { { g } } _ { Q } = \\mathbf { M a m b a } ( \\hbar _ { i } ^ { t } ) [ : , - \\xi ^ { i } : , : ] \\mathbf { W } _ { b } \\in \\mathbb { R } ^ { \\xi ^ { i } \\times 1 } ,\n$$\n\nwhere $\\mathbf { W } _ { b } \\in \\mathbb { R } ^ { d \\times 1 }$ is a projection matrix. We reshape the output vector $\\overline { { g } } _ { Q }  g _ { Q } \\in \\mathbb { R } ^ { C _ { o u t } ^ { i } \\times C _ { i n } ^ { i } \\times K ^ { i } \\times K ^ { i } }$ and obtain the derivative of quantized function. Compared to using only the current gradient information to generate gradients, this approach uses historical gradients to guide the generation of the current gradient, fully utilizing the information.\n\n# Training of FSG\n\nSimilar to (Chen, Wang, and Pan 2019), we will incorporate shared fast-net $\\mathcal { M } _ { f }$ and slow-net $\\mathcal { M } _ { s }$ into the forward process of the model, as shown in Fig. 1. Eq. 9 is used to merge the gradients generated by FSG and the weight $W _ { i }$ of $i$ -th layer. Then we have the forward process in the $i$ -th convolutional layer,\n\n$$\n\\begin{array} { l } { \\hat { \\boldsymbol { W } } _ { i } ^ { t + 1 } = \\boldsymbol { \\mathcal { A } } ( \\boldsymbol { W } _ { i } ^ { t + 1 } ) } \\\\ { \\displaystyle \\quad = \\boldsymbol { \\mathcal { A } } \\left[ \\boldsymbol { W } _ { i } ^ { t } - \\alpha \\mathcal { M } _ { f } ( \\boldsymbol { \\mathcal { G } } _ { \\boldsymbol { W } _ { i } } ^ { t } , \\hat { \\boldsymbol { W } } _ { i } ^ { t } ) \\frac { \\partial \\boldsymbol { \\mathcal { A } } ( \\boldsymbol { W } _ { i } ^ { t } ) } { \\partial \\boldsymbol { W } _ { i } ^ { t } } + \\beta \\mathcal { M } _ { s } ( \\boldsymbol { \\hat { h } } _ { i } ^ { t } ) \\right] } \\end{array}\n$$\n\nAt the same time, we register the generated gradient $\\begin{array} { r } { \\mathcal G _ { F S G } = \\beta \\mathcal M _ { s } ( \\hbar _ { i } ^ { t } ) - \\alpha \\mathcal M _ { f } ( \\mathcal G _ { W _ { i } } ^ { t } , \\hat { W } _ { i } ^ { t } ) \\frac { \\partial \\mathcal A ( W _ { i } ^ { t } ) } { \\partial W _ { i } ^ { t } } } \\end{array}$ in the optimizer to replace $\\begin{array} { r } { g _ { \\mathcal { W } } = \\frac { \\partial \\ell } { \\partial \\mathcal { W } } } \\end{array}$ , and we calculate the cross entropy loss $\\mathcal { L }$ based on the results generated by quantifying weights:\n\n$$\n\\mathcal { L } = \\ell \\left( f ( Q ( \\hat { W } _ { i } ^ { t + 1 } ) ; \\mathbf { x } ) , \\mathbf { y } \\right) ,\n$$\n\ntherefore, $\\mathcal { M } _ { f }$ and $\\mathcal { M } _ { s }$ are associated with the final loss and can be used for backpropagation to update parameters. We assume that $\\mathcal { M } _ { f }$ and $\\mathcal { M } _ { s }$ are parametered by $\\phi _ { f }$ and $\\phi _ { s }$ , respectively, during the backpropagation process,\n\n$$\n\\begin{array} { r l } & { \\cfrac { \\partial \\ell } { \\partial \\phi _ { f } ^ { t + 1 } } = \\cfrac { \\partial \\ell } { \\partial \\hat { \\mathcal { W } } ^ { t + 1 } } \\cfrac { \\partial \\hat { \\mathcal { W } } ^ { t + 1 } } { \\partial \\phi _ { f } ^ { t + 1 } } = - \\alpha \\cdot \\mathcal { G } _ { F S G } \\cfrac { \\partial \\mathcal { M } _ { f } ( \\mathcal { G } _ { W _ { i } } ^ { t } , \\hat { \\mathcal { W } } _ { i } ^ { t } ) } { \\partial \\phi _ { f } ^ { t + 1 } } } \\\\ & { \\cfrac { \\partial \\ell } { \\partial \\phi _ { s } ^ { t + 1 } } = \\cfrac { \\partial \\ell } { \\partial \\hat { \\mathcal { W } } ^ { t + 1 } } \\cfrac { \\partial \\hat { \\mathcal { W } } ^ { t + 1 } } { \\partial \\phi _ { s } ^ { t + 1 } } = \\beta \\cdot \\mathcal { G } _ { F S G } \\cfrac { \\partial \\mathcal { W } ^ { t + 1 } } { \\partial A ( \\mathcal { W } ^ { t + 1 } ) } \\cfrac { \\partial \\mathcal { M } _ { s } ( \\hat { h } _ { t } ) } { \\partial \\phi _ { s } ^ { t + 1 } } } \\end{array}\n$$\n\nAfter the hypernetwork updates, we use the registered gradient $\\mathcal { G } _ { F S G }$ to update all parameters with the optimizer.\n\n# Convergence Analysis of FSG\n\nIn this section, we analyze the convergence of the proposed fast and slow gradient generation mechanism under the general convex function condition. Randomly sample a dataset $\\mathcal { D } = \\{ ( a _ { 1 } , b _ { 1 } ) , ( a _ { 2 } , b _ { 2 } ) , \\dots , ( a _ { N } , b _ { N } ) \\}$ . The task is to predict the label $b$ given the input $a$ , i.e. determine an optimal function $\\phi$ that minimizes the expected risk $\\mathbb { E } [ L ( \\phi ( a ) , b ) ]$ , where $L ( \\cdot , \\cdot )$ represents the loss function and the function $\\phi$ is the prediction function in a certain function space.\n\nIn practice, in order to reduce the range of the objective function, it is necessary to parameterize $\\phi ( \\cdot )$ to $\\phi ( \\cdot ; x )$ , where $x$ is a parameter. Using empirical risk to approximate expected risk requires solving the following minimization problem:\n\n$$\n\\operatorname* { m i n } _ { x } \\quad \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } L ( \\phi ( a _ { i } ; x ) , b _ { i } ) = \\mathbb { E } _ { ( a , b ) \\sim \\hat { P } } [ L ( \\phi ( a ; x ) , b ) ] ,\n$$\n\ndefine $f _ { i } ( x ) = L ( \\phi ( a _ { i } ; x ) , b _ { i } )$ , then we only need to consider the following stochastic optimization problem:\n\n$$\n\\operatorname* { m i n } _ { x \\in \\mathbb { R } ^ { n } } \\quad f ( x ) \\stackrel { \\mathrm { d e f } } { = } \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } f _ { i } ( x )\n$$\n\nTheorem 1. (Convergence of FSG) Let formula 9 run t iterations. By setting $\\begin{array} { r } { \\alpha = \\frac { C ^ { \\supset } } { \\sqrt { t + 1 } } } \\end{array}$\n\n$$\n\\begin{array} { r l } & { \\quad \\mathbb { E } [ f ( \\widehat { \\mathbf { x } } _ { t } ) - f ( \\mathbf { x } ^ { * } ) ] } \\\\ & { \\le \\displaystyle \\frac { \\beta } { ( 1 - \\beta ) ( t + 1 ) } ( f ( \\mathbf { x } _ { 0 } ) - f ( \\mathbf { x } ^ { * } ) ) + \\frac { ( 1 - \\beta ) \\| \\mathbf { x } _ { 0 } - \\mathbf { x } _ { * } \\| ^ { 2 } } { 2 C \\Omega \\kappa \\sqrt { t + 1 } } } \\\\ & { \\quad + \\displaystyle \\frac { C \\Theta \\rho ( G ^ { 2 } + \\delta ^ { 2 } ) } { 2 \\Omega \\kappa ( 1 - \\beta ) \\sqrt { t + 1 } } } \\end{array}\n$$\n\nWhere $C , \\Omega , \\kappa , \\Theta , \\rho$ is a normal number, $\\mathbf { x } ^ { * }$ is the optimal solution and $\\textstyle \\widehat { \\mathbf { x } } _ { t } = \\sum _ { k = 0 } ^ { t } \\mathbf { x } _ { k } / ( t + 1 )$ .\n\nRemark. Theborem 1 describes the convergence of FSG, i.e. the expected error with the optimal solution decreases as the number of iterations increases for the iterative formula $t$ times, and the convergence order is $1 / \\sqrt { t + 1 }$ . The convergence speed is slow at the beginning of the iteration. As the number of iterations increases, the convergence speed will increase. It can almost guarantee convergence.\n\nThe proof is given in the Appendix.\n\n# Experiment\n\nIn this section, we follow the setting of (Chen, Wang, and Pan 2019) and explore whether FSG can effectively learn the derivatives of quantization functions on the CIFAR-10/100 dataset. Then we follow the setting of (Chen, Wang, and Pan 2019; Qin et al. 2020) to compare it with several State-OfThe-Art (SOTA) methods.\n\n# Experiment Setup\n\nIn this work, we use CIFAR-10 and CIFAR-100 as benchmark datasets. CIFAR-10 (Krizhevsky, Hinton et al. 2009) and CIFAR-100 (Krizhevsky, Hinton et al. 2009) are commonly used datasets for image classification, consisting of 50000 training images and 10000 test images, with 10 and 100 categories respectively. Each image has a size of $3 2 \\times 3 2$ and includes an RGB color channel. For CIFAR-10, we conduct experiments using ResNet-20/32/44 architecture, and for CIFAR-100, we conduct experiments using ResNet56/110 architecture. For fair comparison, except for the first convolutional layer and the last fully connected layer, other layers are binarized in this experiment. See the Appendix for more experimental settings and details.\n\n# Experimental Results and Analysis\n\nIn this experiment, we aim to investigate whether the FSG can produce effective gradients. To this end, we compare FSG with the STE (Zhou et al. 2016), FCGrad (Chen, Wang, and Pan 2019) and LSTMFC (Chen, Wang, and Pan 2019). Our experiments are conducted on both SGD and Adam optimizers.\n\nTab. 1 shows the experimental results of STE, FCGrad, LSTMFC and FSG using different optimization methods on the CIFAR-10 dataset. The results show that the accuracy of FSG is significantly better than other types of hypernetworks, and it has a smaller loss, with ResNet-32 having the smallest difference of only $0 . 8 1 \\%$ compared to the results of full precision inference.\n\nTable 1: Results on CIFAR-10 dataset. Loss is the cross-entropy loss value. FP Acc is the result of the full precision version of the backbone on this dataset. The bold font is utilized to denote the optimal results, the underline is applied to indicate the suboptimal outcomes.   \n\n<html><body><table><tr><td>Backbone</td><td>Forward</td><td>Optimization</td><td>Backward</td><td>Train Acc (%)</td><td>Test Acc (%)</td><td>Loss</td><td>FP Acc (%)</td></tr><tr><td rowspan=\"3\"></td><td rowspan=\"3\"></td><td rowspan=\"3\">SGD</td><td>STE FCGrad</td><td>55.469 ± 3.342 92.084±1.103</td><td>46.81 ± 4.893 88.54 ± 0.291</td><td>1.451 ± 0.395 0.220±0.021</td><td rowspan=\"6\"></td></tr><tr><td>LSTMFC</td><td>92.070 ±1.012</td><td>88.28±0.810</td><td>0.228 ± 0.027</td></tr><tr><td>FSG(ours)</td><td>92.302 ± 1.247</td><td>88.67 ± 0.633</td><td>0.218 ± 0.012</td></tr><tr><td rowspan=\"3\"></td><td rowspan=\"3\"></td><td rowspan=\"3\">Adam</td><td>STE FCGrad LSTMFC</td><td>93.138 ± 0.976 94.331 ± 0.986</td><td>87.96 ± 0.173 89.96 ± 0.068</td><td>0.151 ± 0.042 0.148 ± 0.027</td><td rowspan=\"3\"></td></tr><tr><td>FSG(ours)</td><td>94.768 ± 1.023 94.926 ± 0.747</td><td>89.98 ±0.103 91.00 ± 0.804</td><td>0.148 ± 0.029 0.143 ± 0.039</td></tr><tr><td>STE</td><td></td><td></td><td></td></tr><tr><td rowspan=\"3\">ResNet-32</td><td rowspan=\"3\"></td><td rowspan=\"3\">SGD</td><td>FCGrad</td><td>55.469 ± 2.948 95.377 ± 1.588</td><td>47.12 ± 4.683 89.93 ± 0.246</td><td>1.443 ± 0.397 0.154 ± 0.024</td><td rowspan=\"3\">92.13</td></tr><tr><td>LSTMFC</td><td>95.807 ± 1.643</td><td>90.40 ± 0.149</td><td>0.150 ± 0.021</td></tr><tr><td>FSG(ours) STE</td><td>96.898 ± 1.476</td><td>90.53 ± 0.192</td><td>0.145 ± 0.019</td></tr><tr><td rowspan=\"5\"></td><td rowspan=\"5\"></td><td rowspan=\"5\">Adam</td><td>FCGrad LSTMFC</td><td>95.288 ± 1.648 95.140±1.779</td><td>89.47 ± 0.097 90.28 ± 0.068 90.15 ± 0.074</td><td>0.124 ± 0.023 0.900 ± 0.018 0.907 ±0.023</td><td rowspan=\"5\"></td></tr><tr><td>FSG(ours)</td><td>97.632 ±1.310</td><td>91.42 ± 0.140</td><td>0.083 ± 0.016</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>STE</td><td>59.375 ± 3.322</td><td>49.73 ± 4.628</td><td>1.240 ± 0.081</td></tr><tr><td>FCGrad LSTMFC</td><td>92.228 ± 2.029</td><td>89.93 ± 0.246 90.40 ± 0.149</td><td>0.711 ± 0.024 0.723 ±0.022</td></tr><tr><td rowspan=\"5\">ResNet-44</td><td rowspan=\"5\">DoReFa</td><td rowspan=\"5\">Adam</td><td>FSG(ours)</td><td>94.548 ± 1.947 98.916 ± 1.398</td><td>91.91 ± 0.122</td><td>0.031 ± 0.015</td><td rowspan=\"5\">93.55</td></tr><tr><td>STE</td><td>97.656 ± 0.996</td><td>90.21 ± 0.075</td><td></td></tr><tr><td>FCGrad</td><td>90.584 ± 1.573</td><td></td><td>0.083 ±0.012</td></tr><tr><td></td><td></td><td>90.98 ± 0.068</td><td>0.083 ±0.022</td></tr><tr><td>LSTMFC FSG(ours)</td><td>90.147 ± 1.392 99.262 ± 1.322</td><td>90.95 ± 0.074 92.78 ± 0.053</td><td>0.082 ±0.023 0.022 ± 0.011</td></tr></table></body></html>\n\nTab. 2 shows the experimental results of STE, FCGrad, LSTMFC, and FSG on the CIFAR-100 dataset using different optimization methods. Similar to CIFAR-10, FSG demonstrates superior performance over LSTMFC across a range of scenarios, achieving a higher accuracy rate of $6 8 . 0 4 \\%$ compared to LSTMFC’s $6 6 . 3 0 \\%$ . This indicates that FSG’s incorporation of historical gradient information significantly improves the optimization process by providing more accurate estimates of the gradients needed for learning quantization schemes.\n\n# Convergence Analysis\n\nIn this comparative study, we aim to evaluate the convergence speed of different optimizers, namely STE, FCGrad, LSTMFC, and FSG, by analyzing their loss curves throughout the training process, as depicted in Fig 2. The loss curves are plotted to visualize the performance of each method during training, using the DoReFa quantization function and the ResNet44 network architecture on the CIFAR-10 dataset. The loss curves reveal that FSG exhibit superior convergence characteristics, with a notably faster descent rate and consistently lower loss values compared to FCGrad, LSTMFC and STE. Furthermore, compared to traditional SGD optimizer, the Adam optimizer exhibits a more stable and consistent optimization process, which reflects that our momentum generation gradient strategy is more suitable for the Adam optimizer.\n\n# Ablation Experiment\n\nIn this section, we investigate the effects of various sequence models as slow-nets on performance and examine the influence of the balancing parameter $\\alpha$ , which governs the blending of fast and slow gradients within the FSG method, alongside the impact of the hyperparameter $l$ , which represents the length of the historical gradient memory fed into the slow network.\n\nUsing different slow-net. In this experiment, we use two sequence modeling models, including LSTM and Mamba, as slow-nets. For the CIFAR10 and CIFAR100 datasets, we employed ResNet-44 and ResNet-56 as the respective backbone architectures. As evident from Table 3, utilizing Mamba as a momentum generator exhibited superior performance, indicating that Mamba possesses enhanced capabilities in modeling long sequences. Its distinctive selection mechanism was found to aid in the filtering of noise within the historical gradient sequence.\n\nInfluence of $\\beta$ . In this experiments, we explore the sensitivity of the FSG method to the combination parameter $\\beta$ , selecting values of 0.9, 0.7, 0.5, 0.3, and 0.1 for evaluation on the CIFAR-100 dataset. We utilize the ResNet56 architecture and the DoReFa quantization function for our analysis. From the result in the Fig. 2, it can be found that the highest accuracy is achieved when $\\beta$ is set to 0.3. This indicates that the current gradient generated by fast-net dominates the update, which helps to respond quickly to changes in gradient and speed up convergence. At the same time, the gradient momentum generated by the historical gradient sequence can still reduce the gradient oscillation caused by noise to a certain extent.Based on these results, we recommend starting with a smaller value of $\\beta$ (around 0.3).\n\nInfluence of l. For $l$ , we select 3, 4, 5, 6, and 7 to conduct experiments on CIFAR-100, using ResNet56 as the network architecture and DoRefa as the quantization function. From the result in the Fig. 2, it can be found that the highest accuracy is achieved when $l$ is set to 6. This indicates that historical gradient memory requires an appropriate length, and shorter memory lengths cannot accumulate enough sequence information. A longer memory length in\n\n<html><body><table><tr><td>Backbone</td><td>Forward</td><td>Optimization</td><td>Backward</td><td>Train Acc (%)</td><td>Test Acc (%)</td><td>Loss</td><td>FP Acc (%)</td></tr><tr><td rowspan=\"2\">ResNet-56</td><td rowspan=\"2\">DoReFa</td><td>SGD</td><td>STE FCGrad LSTMFC FSG(ours)</td><td>51.126 ± 7.374 73.934 ±1.639 73.934 ± 1.454 74.239 ± 1.691</td><td>42.27 ± 8.143 63.25±0.935 62.95 ± 2.183</td><td>1.890 ± 0.178 0.861 ±0.012 0.901 ± 0.010</td><td rowspan=\"2\"></td></tr><tr><td></td><td>STE FCGrad LSTMFC</td><td>81.250 ± 0.642 79.944 ± 0.923 79.336 ± 0.931</td><td>64.02 ± 1.344 64.78 ± 0.533 66.56 ± 0.351 66.30± 0.793</td><td>0.822 ± 0.012 0.618 ± 0.032 0.684 ± 0.022</td></tr><tr><td rowspan=\"2\">ResNet-110</td><td rowspan=\"2\">DoReFa</td><td>SGD</td><td>STE FCGrad LSTMFC</td><td>53.159 ± 9.793 82.379 ± 1.553 81.074 ±1.457</td><td>43.42 ± 8.902 65.15 ± 2.490 64.75± 2.850</td><td>1.783 ± 0.211 0.602 ±0.023 0.619±0.027</td><td rowspan=\"2\"></td></tr><tr><td></td><td>FSG(ours) STE</td><td>84.733 ± 2.773 85.156 ± 1.344</td><td>65.23 ± 0.822</td><td>0.601 ± 0.012</td></tr><tr><td></td><td></td><td>Adam</td><td>FCGrad LSTMFC FSG(ours)</td><td>85.299 ± 1.284 85.699 ± 1.273 85.738 ± 1.031</td><td>66.84 ±1.205 68.74±0.363 67.14 ± 1.286</td><td>0.548 ± 0.016 0.483 ± 0.014</td><td>72.54</td></tr></table></body></html>\n\n(a)Loss Convergence (b)Loss Convergence Analysis (SGD) Analysis (Adam) (c)Accuracy by Beta (d)Accuracy by Length FcGrad 自 6867.50 67.59 67.35 FSG(ours)   \n0.25   \n0.00 20 40 60 80 0.00 20 40 60 80 650.9 0.7 0.5 0.3 0.1 65 3.0 4.0 5.0 6.0 7.0 Epoch Epoch Beta Length\n\nTable 3: Performance comparison of different sequence modeling models as slow-net.   \n\n<html><body><table><tr><td>Backbone</td><td>Optimization</td><td>Slow-net</td><td>Test Acc (%)</td><td>FP Acc (%)</td></tr><tr><td rowspan=\"2\">ResNet-44</td><td rowspan=\"2\">CIFAR10</td><td>LSTM</td><td>92.07 ± 0.066</td><td rowspan=\"2\">93.55</td></tr><tr><td>Mamba</td><td>92.63 ± 0.052</td></tr><tr><td rowspan=\"2\">ResNet-56</td><td rowspan=\"2\">CIFAR100</td><td>LSTM</td><td>67.60 ± 0.670</td><td rowspan=\"2\">71.22</td></tr><tr><td>Mamba</td><td>68.04 ± 0.948</td></tr></table></body></html>\n\ntroduces more historical gradient noise, which is not conducive to the generation of gradient momentum.\n\n# Comparison with SOTA Methods\n\nIn order to verify the performance of FSG, we conducted performance studies on it compared to other BNN optimization methods. We follow the experimental setup of (Chen, Wang, and Pan 2019; Qin et al. 2020). A series of SOTA methods are compared on the CIFAR-10 and CIFAR-100 datasets to validate performance, including DoReFa (Zhou et al. 2016), LSTMFC (Chen, Wang, and Pan 2019), ReSTE (Wu et al. 2023), RBNN (Lin et al. 2020), IR-Net (Qin et al. 2020). The experimental results are shown in Tab. 4. From the table, it can be seen that our method performs well, surpassing all other methods in terms of final accuracy. On CIFAR-100, it exceeds the second-best baseline by $0 . 5 4 \\%$ .\n\n<html><body><table><tr><td>Dataset</td><td>CIFAR-10</td><td>CIFAR-10</td><td>CIFAR-100</td></tr><tr><td>Backbone</td><td>ResNet-20</td><td>ResNet-44</td><td>ResNet-56</td></tr><tr><td>FP</td><td>91.70</td><td>93.55</td><td>71.22</td></tr><tr><td>DoReFa</td><td>87.96 ±0.173</td><td>90.21 ±0.075</td><td>64.78 ±0.533</td></tr><tr><td>ReSTE</td><td>89.26 ±0.884</td><td>90.25 ±0.783</td><td>65.59±0.733</td></tr><tr><td>LSTMFC</td><td>90.03±0.299</td><td>91.91 ± 0.538</td><td>66.48±0.808</td></tr><tr><td>IR-Net</td><td>90.80 ± 0.971</td><td>92.37 ±0.532</td><td>68.94 ±0.967</td></tr><tr><td>RBNN</td><td>90.89±0.921</td><td>91.59 ± 0.874</td><td>67.10±0.933</td></tr><tr><td>FSG(ours)</td><td>91.00 ± 0.804</td><td>92.78 ± 0.053</td><td>69.48 ± 0.979</td></tr></table></body></html>\n\nTable 4: Performance comparison with SOTA methods. FP is the full-precision version of the backbone. The bold font is utilized to denote the optimal results, the underline is applied to indicate the suboptimal outcomes.\n\n# Conclusion\n\nThis work investigated the generation of non-differentiable function gradients based on hypernetwork methods in BNN optimization. We re-examined the process of gradient generation from the inspiration of momentum, modeled the historical gradient sequence to generate the first-order momentum required for optimization, and introduced the FSG method, using a combination of gradient and momentum to generate the final gradient. In addition, we had added LRE to the slow-net to help generate layer specific fine gradients. The experiments on the CIFAR-10 and CIFAR-100 datasets showed that our method has faster convergence speed, lower loss values, and better performance than competitive baselines.\n\n# Acknowledgments\n\nThis work is Supported by Shanghai Artificial Intelligence Laboratory.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   二进制神经网络（BNNs）因其在边缘设备上的部署潜力而受到广泛关注。然而，量化函数的不可微性导致梯度无法反向传播，从而阻碍了BNNs的优化。现有基于超网络的方法虽然能够自适应地学习不可微量化函数的梯度，但通常仅依赖当前梯度信息，忽略了历史梯度的影响，导致优化过程中梯度误差累积。\\n> *   该问题的重要性在于，BNNs的高效部署依赖于优化的准确性，而梯度误差累积会显著降低模型的收敛速度和最终性能。\\n\\n> **方法概述 (Method Overview)**\\n> *   本文提出了一种快速和慢速梯度生成（FSG）方法，通过结合历史梯度信息和当前梯度信息，生成更精确的梯度，从而优化BNNs的训练过程。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **历史梯度存储模块（HGS）**：设计了一个模块用于存储历史梯度序列，生成优化所需的一阶动量，显著减少了梯度误差累积。在CIFAR-10上，测试准确率提升至91.00%，优于基线模型STE（87.96%）和FCGrad（89.96%）。\\n> *   **快速和慢速梯度生成（FSG）方法**：通过结合快速网络（fast-net）和慢速网络（slow-net），充分利用历史和当前梯度信息，生成更精细的梯度。在CIFAR-100上，准确率提升至69.48%，超过第二佳基线0.54%。\\n> *   **层识别嵌入（LRE）**：引入可学习的层嵌入向量，帮助超网络生成层特定的精细梯度，进一步提升了优化效果。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   本文的核心思想是通过结合历史梯度信息和当前梯度信息，生成更精确的梯度，从而优化BNNs的训练过程。历史梯度序列被建模为时间序列数据，用于生成一阶动量，而当前梯度信息则用于快速响应梯度变化。\\n> *   该方法有效的原因在于，历史梯度信息能够提供额外的上下文，帮助网络生成更适合BNNs优化的梯度，从而减少梯度误差累积。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比**：现有基于超网络的方法通常仅依赖当前梯度信息，忽略了历史梯度的影响，导致梯度误差累积。\\n> *   **本文的改进**：本文提出HGS模块存储历史梯度序列，并设计FSG方法结合历史和当前梯度信息，生成更精确的梯度。此外，引入LRE帮助生成层特定的精细梯度。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> *   1. **历史梯度存储（HGS）**：存储最近l步的梯度序列，作为时间序列数据输入慢速网络（slow-net）。\\n> *   2. **快速和慢速梯度生成（FSG）**：快速网络（fast-net）使用MLP从当前梯度提取高维特征，生成快速梯度；慢速网络（slow-net）使用Mamba或LSTM等模型从历史梯度序列生成慢速梯度。\\n> *   3. **层识别嵌入（LRE）**：为每一层初始化可学习的嵌入向量，帮助慢速网络生成层特定的梯度。\\n> *   4. **梯度组合**：将快速梯度和慢速梯度加权组合，生成最终梯度用于优化。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   STE（Straight-Through Estimator）\\n> *   FCGrad（基于超网络的方法）\\n> *   LSTMFC（基于LSTM的超网络方法）\\n> *   ReSTE\\n> *   RBNN\\n> *   IR-Net\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在测试准确率上**：本文方法在CIFAR-10数据集上达到了91.00%，显著优于基线模型STE（87.96%）和FCGrad（89.96%）。与表现最佳的基线相比，提升了1.04个百分点。\\n> *   **在测试准确率上**：本文方法在CIFAR-100上达到了69.48%，优于基线模型LSTMFC（66.30%）和IR-Net（68.94%）。与表现最佳的基线IR-Net相比，提升了0.54个百分点。\\n> *   **在收敛速度上**：本文方法的损失曲线下降更快，且最终损失值更低，表明其具有更快的收敛速度和更优的优化效果。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   二进制神经网络 (Binary Neural Networks, BNNs)\\n*   梯度生成 (Gradient Generation, N/A)\\n*   历史梯度存储 (Historical Gradient Storage, HGS)\\n*   快速和慢速梯度生成 (Fast and Slow Gradient Generation, FSG)\\n*   层识别嵌入 (Layer Recognition Embeddings, LRE)\\n*   超网络 (Hypernetwork, N/A)\\n*   优化算法 (Optimization Algorithm, N/A)\\n*   边缘设备 (Edge Devices, N/A)\"\n}\n```"
}