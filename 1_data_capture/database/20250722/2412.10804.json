{
    "source": "Semantic Scholar",
    "arxiv_id": "2412.10804",
    "link": "https://arxiv.org/abs/2412.10804",
    "pdf_link": "https://arxiv.org/pdf/2412.10804.pdf",
    "title": "Medical Manifestation-Aware De-Identification",
    "authors": [
        "Yuan Tian",
        "Shuo Wang",
        "Guangtao Zhai"
    ],
    "categories": [
        "cs.CV",
        "cs.AI"
    ],
    "publication_date": "2024-12-14",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 0,
    "influential_citation_count": 0,
    "institutions": [
        "Shanghai AI Laboratory",
        "Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University"
    ],
    "paper_content": "# Medical Manifestation-Aware De-Identification\n\nYuan Tian1, Shuo Wang2, Guangtao Zhai2\\*\n\n1 Shanghai AI Laboratory 2 Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University tianyuan168326@outlook.com, zhaiguangtao $@$ sjtu.edu.cn\n\n# Abstract\n\nFace de-identification (DeID) has been widely studied for common scenes, but remains under-researched for medical scenes, mostly due to the lack of large-scale patient face datasets. In this paper, we release MeMa, consisting of over 40,000 photo-realistic patient faces. MeMa is re-generated from massive real patient photos. By carefully modulating the generation and data-filtering procedures, MeMa avoids breaching real patient privacy, while ensuring rich and plausible medical manifestations. We recruit expert clinicians to annotate MeMa with both coarse- and fine-grained labels, building the first medical-scene DeID benchmark. Additionally, we propose a baseline approach for this new medical-aware DeID task, by integrating data-driven medical semantic priors into the DeID procedure. Despite its conciseness and simplicity, our approach substantially outperforms previous ones.\n\nDataset and Code — https://github.com/tianyuan168326/MeMa-Pytorch\n\n# Introduction\n\nThe public sharing of large-scale image datasets has facilitated the rapid progress in Artificial Intelligence (AI). However, this also poses great privacy concerns, especially for facial images, which are widely used for identity authentication. To address this issue, many de-identification (DeID) algorithms (Cao et al. 2021; Maximov, Elezi, and Leal-Taixe´ 2020; Gu et al. 2020; Li et al. 2023; Cai et al. 2024) have been continuously proposed for protecting the facial identity, achieving promising results on common-scene facial datasets (Karras, Laine, and Aila 2019; Karras et al. 2017).\n\nHowever, rare researches are conducted for the medical scenes, although patient privacy leakage is a big concern in the medical AI era (Price and Cohen 2019). Research on medical-aware DeID (Med-DeID) mainly faces two obstacles. First, there are few medical-scene facial datasets available, due to the difficulty in accessing patients compared to healthy individuals. Moreover, it is often not acceptable to package real patient faces as datasets and make them publicly downloadable. Second, the current DeID approaches may not be appropriate for protecting medical facial images, due to not particularly preserving the disease manifestations of the origin image. This leads to the lost of diagnosisnecessary disease signs, deteriorating the medical utilities.\n\n![](images/5c0cef6c180031697f045b835e6fdefac151891c783b681e026d115101e73618.jpg)  \nFigure 1: (a) Common DeID approaches, focus on removing identity. (b) Our medical-aware DeID (Med-DeID), also considers preserving the diagnosis-necessary medical information. (c) Our MeMa, a large-scale patient face dataset. (d) Our MeMa-Seg, the tumor segmentation subset of MeMa.\n\nIn this paper, we release a Medical Manifestation-rich patient face dataset, termed MeMa, containing over 40,000 photo-realistic virtual patient images. To construct MeMa, we obtained permission from the hospital’s medical ethics committee to photograph patients. Then, these patient photos are annotated by expert physicians, before being used to train a specialized generative model. By carefully modulating the sampling procedure of the generative model and filtering the generated data, we created a diverse, high-quality, and real-world-like patient face dataset.\n\nFurthermore, we propose a baseline medical semanticspreserved DeID approach, termed MedSem-DeID, to eliminate the patient identity from the facial image, at the premise of preserving the medical utility. Concretely, we first condense the rich medical priors within the MeMa into a medical semantics encoder, and then adopt it to (1) enhance the medical knowledge of the features within the DeID pipeline, and (2) minimize the medical-aware distortion of the deidentified images. Despite its simplicity, our approach easily outperforms previous DeID approaches for medical scenes, thanks to the rich medical manifestation knowledge embodied in the MeMa dataset. Our main contributions are:\n\n• We release, to the best of our knowledge, the first largescale patient face dataset of rich medical manifestations, MeMa, which is expected to facilitate research in the field of medical-scene privacy protection.\n\n![](images/87329cf5d56474db992a88ed5fd3f877f4b652cd37d11cfef4ea909573707778.jpg)  \nFigure 2: Examples and the distribution characteristics of the proposed MeMa dataset.   \nFigure 3: MeMa building pipeline. (a) Training patient face generation model on real patient data. (b) Rich-condition patient face sampling. $P ( a g e )$ and $P ( g e n d e r )$ denote the age and gender distributions, which are statistically derived from the real patients. ‘SD’ denotes the stable diffusion model.\n\n• We propose a baseline approach for this novel medical DeID problem, which particularly preserves the disease signs during the DeID procedure, by making full use of the rich medical priors within MeMa. • We build the first medical-scene DeID benchmark, by comprehensively evaluating the proposed baseline and other recent DeID approaches on MeMa. Our approach is consistently superior in various aspects.\n\n# Related Work\n\nFacial Datasets. Amounts of large-scale face datasets (Karras, Laine, and Aila 2019; Karras et al. 2017) have been proposed, but they primarily feature healthy individuals, limiting their use for medical-scene DeID. In contrast, we introduce a large-scale patient face dataset with rich medical manifestations. Our dataset also includes rich medical annotations, facilitating face DeID field in medical scenes.\n\nFace De-identification. Early De-ID methods (Jourabloo, Yin, and Liu 2015) used the K-same algorithm. Recent approaches (Hukkel˚as, Mester, and Lindseth 2019; Maximov, Elezi, and Leal-Taixe´ 2020) leverage generative models to remove facial identity, while often compromising utility. More recent methods (Wen et al. 2023; Cai et al. 2024; Ren, Lee, and Ryoo 2018) aim to preserve more facial attributes and better serve common utilities such as gaze detection and image/video recognition (Kong and $\\mathrm { F u } 2 0 2 2$ ; Tian et al. 2022, 2020, 2021, 2019; Yan et al. 2023; Gao et al. 2024; Tan et al. 2024; Che et al. 2021), but not specifically medical signs (Chen et al. 2024a). In contrast, our approach leverages medical manifestation representations learned from real patient photos, preserving medical attributes during DeID. Additionally, it is reversible, similar to (Gu et al. 2020; Cao et al. 2021; Li et al. 2023), enabling reversal for medical audits.\n\nSemantic Representation. Effectively modeling semantic information is crucial for modifying facial images, while maintaining perceptual quality (Min et al. 2024; Yi et al. 2021; Duan et al. 2022; Chen et al. 2024b; Li et al. 2024; Gao et al. 2022, 2021; Yi, Jiang, and Zhou 2024) and preserving medical utility. Previous approaches have leveraged contrastive learning (Tian et al. 2024b, 2023b) and masked image modeling (Tian et al. 2023a; Tian, Lu, and Zhai\n\n2024) for self-supervised learning of image semantics. Recent studies have shown that pre-trained visual foundation models, such as stable diffusion (Rombach et al. 2022), exhibit even stronger semantic representations (Zhang et al. 2024; Tian, Lu, and Zhai 2025; Hedlin et al. 2024). In this work, we present the first adaptation of diffusion modelextracted semantics to the medical DeID problem.\n\nMedical-scene Face Privacy Protection. Progress on this problem has been slow, often relying on simple methods like blurring or replacing faces with 3D masks (Yang et al. 2022), which discard critical disease signs. The progress gap is attributed to the lack of large-scale medical-scene facial datasets. Our work aims to address this gap.\n\n# Approach\n\nWe first build a new patient face dataset, termed MeMa. It addresses the lack of medical-scene facial datasets. MeMa is synthesized from real patient photos. Its synthetic nature avoids potential ethical problems. Expert physicians recognize its validity. Further, we propose a baseline model for the medical-aware facial DeID (Med-DeID) problem.\n\nBCC/Conj/Pto SD 雲 G a   \nsis/SCC/TAO... (a)   \nFFHQ P(age) 1 P(gender) Real-guided Feature Sampling Extractor Filter/ Annotate +   \nDisease Ran d0.2. V.alue Minht IP-Adapter   \nBCC/Conj/Pto   \nsis/SCC/TAO... Prompt \"A face with   \nDegree Generate {BCC}.{mid) SD   \nslight/mid... (b)\n\n# MeMa Dataset\n\nThe overview of MeMa is shown in Fig. 2, which consists of 42,307 synthetic patient face images. MeMa closely mimics real patients in both visual appearance and statistics. Patient age and gender are estimated using the DeepFace framework (Serengil 2020; Serengil and Ozpinar 2021). We describe the main steps for assembling MeMa as follows.\n\nDisease Categories: We take the eye clinic as an exemplar scene, since most eye diseases show typical external facial manifestations. In our study, we included patients with seven eye diseases. These are Basal Cell Carcinoma (BCC), Conjunctivitis (Conj), Uveitis, Ptosis, Squamous Cell Carcinoma (SCC), Strabismus (Strab), and Thyroid Associated Ophthalmopathy (TAO). We also included clinically Normal cases. Examples are shown in Fig. 2(a). The detailed manifestations of the above diseases can be found in the MSD medical manual (Merck & Co. 2024).\n\nReal Patient Data Collection: We collected 39,323 photos of 12,467 real patients. They attended the Eye Clinic at Shanghai Ninth People’s Hospital(SNPH) between January 2020 and June 2023. The photo-taking procedure was approved by the hospital’s ethics committee. The patients’ diagnosis results were collected from their medical records.\n\nGenerating MeMa from Real Data: As shown in Fig. 3, we first train a medical-aware generative model with the collected patient data. Then, we sample the virtual patients from the model by using proper conditions, aiming to generate safe and diverse samples. Finally, we recruit expert physicians to filter the images of bad medical quality, then annotate the filtered images. The steps are detailed as follows.\n\nStep1: Medical-aware Generative Model Training: We first translate the disease type into the prompt caption ‘A face, eye with $\\{ { \\mathrm { d i s e a s e ~ n a m e } } \\} ^ { \\prime }$ . With the paired data of the real patient photographs and the disease type caption, we fine-tune the diffusion model (Rombach et al. 2022), producing the patient face generation model. As compared in Fig. 4 (a) and (b), after fine-tuning the SD model on our real patient dataset, the generated image shows typical medical signs and manifestations, while the vanilla SD model can not effectively generate images with reasonable medical manifestations, due to its limited medical knowledge.\n\nStep2: Rich-Condition Patient Face Synthesis: Directly sampling from the real-patient generation model with the simple prompt ‘A face, eye with disease name ’ is not enough, which shows two problems. First, identity leakage: the identity of most sampled patients can be found in the training dataset, potentially leaking the privacy of real patients. Second, mode collapse: the samples tend to be less diverse, with collapsed medical manifestation modes.\n\nTo address the identity leakage problem, we propose injecting facial attributes from public faces into the generation process. Specifically, we randomly sample face images from the FFHQ dataset and use the IP-Adapter (Ye et al. 2023) to inject these attributes. As shown in Tab. 1, this substantially reduces the average identity leakage percentage from $7 1 . 8 \\%$ to $1 . 2 7 \\%$ , when being evaluated with multiple face recognition models, i.e., SphereFace (Liu et al. 2017), ArcFace (Deng et al. 2019), and CosFace (Wang et al. 2018).\n\nTable 1: Effectiveness of injecting public face attribute for reducing the identity leakage percentage.   \n\n<html><body><table><tr><td></td><td>SphereFace</td><td>ArcFace</td><td>CosFace</td><td>Average</td></tr><tr><td>Direct Sample</td><td>73.45%</td><td>76.72%</td><td>65.34%</td><td>71.83%</td></tr><tr><td>Face Injection</td><td>1.62%</td><td>0.97%</td><td>1.24%</td><td>1.27%</td></tr></table></body></html>\n\n![](images/689375da5d004f2e108f65c4a20f7c0c5a71d4f80ab4afb500364c01964fc0d3.jpg)\n\nFigure 4: Comparison of different image generation strategies. We take the Basal Cell Carcinoma (BCC) disease as an example. ‘SD’ denotes the Stable Diffusion.   \nTable 2: Wasserstein distance (Ru¨schendorf 1985) between the generated and the real patient image distributions. Smaller distance indicates more real-world alike generation.   \n\n<html><body><table><tr><td>Sampling Strategy</td><td>Disease</td><td>Age</td><td>Gender</td></tr><tr><td>Random sampling</td><td>0.256</td><td>0.143</td><td>0.157</td></tr><tr><td>Real-guided sampling</td><td>0.003</td><td>0.002</td><td>0.002</td></tr></table></body></html>\n\nTo mitigate the mode collapse problem, we first randomly sample the public face injection weight from the range [0.2, 0.4], instead of using a fixed weight. This leads to better feature fusion flexibility and improves output diversity. Second, we enhance the text prompt with severity descriptions, e.g., ‘A face, eye with disease name , slight/mid/heavy - level’. This further improves diversity, even though no disease severity is annotated in the collected patient captions. The reason may be that the base SD model has learned a large dictionary of word semantics and can automatically connect the common ‘severity’ description words to the image generation process. As compared in Fig. 4 (b) and (c), with rich conditions injected, both the quality and diversity of the generated images are substantially improved.\n\nTo ensure the generated dataset’s statistical characteristics match those of real patients, we calculate the distributions of real patient disease types, ages, and genders. We control the generated images to follow the above distributions. To control the disease type, we simply modify the disease name of the prompt. To control the age and gender of the generated images, we label FFHQ images using an age and gender estimation model (Serengil 2020), then select images based on this metadata for attribute injection. As shown in Tab. 2, the real distribution-guided sampling strategy produces a dataset with similar statistical characteristics to real patients.\n\nStep3: Filtering and Annotation: After generating the images, we remove those with small identity feature distance to the original real patient set, ensuring the privacy of the real patients will not be leaked. Then, the physicians filter out the images with low medical utility quality. Finally, these physicians label the per-image disease information of the filtered dataset. Moreover, considering that lesion segmentation is another representative medical imaging task. We also ask the physicians to segment the tumor mask of the subset SCC images, producing the MeMa-Seg subset. The annotation procedure is assisted by the SAM model (Kirillov et al. 2023) and then refined by the physicians.\n\n# A Baseline Approach for Med-DeID\n\nWe propose a baseline approach to incorporate the rich medical manifestation knowledge within MeMa into the DeID procedure, which consists of two sub-modules: medical semantics encoding and medical semantics-preserved DeID.\n\nMedical Semantics Encoding. The Med-DeID task requires preserving as much medical information as possible while obfuscating other identifying details. This necessitates a semantic encoder that recognizes local medical semantics.\n\nMotivated by the strength of diffusion models in extracting fine-grained local semantics (Tian et al. $2 0 2 4 \\mathrm { a }$ ; Tang et al. 2023), we train another diffusion model on the proposed MeMa dataset to learn the medical semantics. We adopt its first several blocks as the medical encoder $E n c _ { m e d }$ , instead of the whole network, for reducing the computational cost.\n\nIt should be mentioned that the roles of the diffusion models in the previous section and here are fundamentally different: the previous one is for high-quality image generation, whereas the one here is for extracting rich medical semantics. Our approach is very flexible, and the semantic encoder can be other choices, as analyzed in the experiment section. Medical Semantics-Preserved DeID (MedSem-DeID). As illustrated in Fig. 5, our approach leverages the medical encoder $E n c _ { s e m }$ to inject medical knowledge into the feature extraction procedure, as well as regularize the medical utility of the de-identified image.\n\nGiven the original image $X$ , where $H$ and $W$ denote its height and width, an image encoder transforms $X$ into the facial feature $f _ { f a c e } ~ \\in ~ \\bar { \\mathbb { R } } ^ { 5 1 2 \\times \\frac { H } { 3 2 } \\times \\frac { W } { 3 2 } }$ . Meanwhile, we use Encmed to extract the medical feature fmed ∈ R320× 1H6 × 1W6 . The $f _ { m e d }$ is downscaled and concatenated with $f _ { f a c e }$ , passing through three consecutive residual blocks (He et al. 2016), producing $f$ . Then, we employ a group of Transformer blocks (Vaswani et al. 2017), termed ID-Encryptor, to encrypt the ID information within $f$ . Specifically, we flatten the spatial dimension of $f$ , concatenate it with the password vector $\\boldsymbol { P } \\in \\mathbb { R } ^ { 5 1 2 }$ , and feed the concatenated vector into ID-Encryptor, producing the encrypted feature $f _ { e n c } . \\ f _ { e n c }$ is passed through an image decoder network to result in the encrypted image $X _ { e n c }$ . Please refer to the supplementary material for the network architecture details.\n\n![](images/269243fc191ef0501a72af16f784aaf6ea40c78788d21c87ed5d2b36e149b612.jpg)  \nFigure 5: Overview of the proposed baseline model MedSem-DeID. The snow icon indicates the $E n c _ { \\mathrm { m e d } }$ is frozen during training DeID networks. The image decoder after the ID-decryptor is omitted for briefness. $\\oplus$ denotes the channel-wise concatenation operation.\n\nIn medical contexts, it is often necessary to rigorously recheck results with expert physicians on the original image. Moreover, the Good Clinical Practice (GCP) guideline (Guideline 2001) mandates that all medical materials involved in the diagnosis process must be traceable. Therefore, we design our method to be reversible, enabling the recovery of the original image from the encrypted features. Given the original password $P$ , $f _ { e n c }$ can be decrypted back to $\\hat { f }$ , by another group of Transformer blocks termed IDDecryptor. Then, $\\hat { f }$ is reconstructed as the original image $\\hat { X }$ by the image decoder. When an incorrect password is used, $f _ { e n c }$ is reconstructed into a wrong image $X _ { w r o n g }$ .\n\nLearning Objectives. The learning objective of the proposed MedSem-DeID is formulated as follows, $\\mathcal { L } = \\mathcal { L } _ { d e i d } +$ Lrev id $+ \\ \\mathcal { L } _ { w r o n g } \\ + \\ \\lambda _ { m e d } \\mathcal { L } _ { m e d } \\ + \\ \\lambda _ { r e v } \\mathcal { L } _ { r e v } \\ + \\ \\mathcal { L } _ { G A N }$ . $\\mathcal { L } _ { d e i d } = \\cos ( \\phi ( X ) , \\phi ( X _ { \\mathrm { e n c } } ) )$ enforces the identity of the encrypted image apart from the original image, where $\\phi$ denotes the pre-trained identity recognition network ArcFace (Deng et al. 2019), cos denotes the cosine similarity. $\\mathcal { L } _ { r e v - i d } = - \\cos ( \\phi ( X ) , \\phi ( \\hat { X } ) )$ enforces the identity of reversibly decrypted image is the same as the original image. $\\mathcal { L } _ { w r o n g } = \\cos ( \\phi ( X ) , \\phi ( X _ { e n c } ^ { w r o n g } ) )$ enforces the identity of the image decrypted by the wrong password far away from the original image. $\\mathcal { L } _ { m e d } = \\ell _ { 2 } ( f _ { m e d } , E n c _ { m e d } ( X _ { \\mathrm { e n c } } ) )$ facilitate the encrypted image is similar to the original image in terms of medical semantics. $\\mathcal { L } _ { r e v } = \\ell _ { 1 } ( X , \\hat { X } )$ regularizes the appearance of the recovered image by right password is similar to the original one. $\\ell _ { 1 }$ and $\\ell _ { 2 }$ denote the mean absolute error (MSE) and the mean squared error (MSE) functions, respectively. The $\\mathcal { L } _ { \\mathrm { G A N } }$ is the adversarial generative network (GAN) loss, enforcing the photo-realism of all images. $\\lambda _ { m e d }$ and $\\lambda _ { r e c }$ denote the balancing weights.\n\n# Experiments\n\nDatasets. MeMa: the proposed MeMa dataset consists of 42,307 images in total, which is split into a training set (34,000 images), a hyper-parameter selection set (3,729 images), and a validation set (4,578 images). All images are labeled with the disease category. MeMa-Seg: for the BCC (basal cell carcinoma) disease type, we randomly select 600 images from the training set and 150 images from the validation set of MeMa, annotating the tumor masks for these images. This results in the MeMa-Seg dataset, which can be used to evaluate the fine-grained medical performance of different DeID approaches. Real-ECXHCSU: we also collaborate with Eye Center of Xiangya Hospital of Central South University (ECXHCSU), enrolling 129 patients to conduct a real-world clinical trial. This aims to validate whether our algorithm, trained on the synthetic MeMa dataset, remains effective for real-world patients. Moreover, ECXHCSU is geographically distant from SNPH used to develop the MeMa dataset. This aims to further emphasize the generalization capability of our approach.\n\n![](images/fb7f18a05bdee8a6bb9867f26af113eb86a57b1a34409a79f2d1e19f34470d40.jpg)\n\nFigure 6: Qualitative results of different methods on the MeMA validation set.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Rev</td><td rowspan=\"2\">Utility Prior</td><td colspan=\"9\">Normasi Paoton S)C</td><td colspan=\"2\">SDicentation1</td></tr><tr><td>All</td><td>BCC</td><td>Conj</td><td></td><td></td><td></td><td>Strab</td><td>TAO</td><td>Uveitis</td><td></td><td></td></tr><tr><td>DeepPrivacy</td><td>X</td><td>Landmark</td><td>40.80</td><td>91.20</td><td>4.52</td><td>80.38</td><td>69.10</td><td>0.95</td><td>3.66</td><td>59.23</td><td>17.32</td><td>0.0041</td><td>0.0021</td></tr><tr><td>CIAGAN</td><td>×</td><td>Landmark</td><td>41.36</td><td>30.96</td><td>3.85</td><td>96.93</td><td>49.36</td><td>13.52</td><td>7.33</td><td>31.81</td><td>97.11</td><td>0.1280</td><td>0.0757</td></tr><tr><td>Disguise</td><td>X</td><td>Landmark+Gaze</td><td>76.01</td><td>57.19</td><td>95.32</td><td>98.63</td><td>90.68</td><td>58.29</td><td>37.00</td><td>90.69</td><td>80.31</td><td>0.2751</td><td>0.1984</td></tr><tr><td>Password</td><td>√</td><td>Unet</td><td>54.21</td><td>43.99</td><td>95.99</td><td>87.37</td><td>47.53</td><td>52.38</td><td>5.93</td><td>79.26</td><td>21.22</td><td>0.6336</td><td>0.5192</td></tr><tr><td>Personal</td><td>√</td><td>Face Attributes</td><td>30.17</td><td>67.17</td><td>9.69</td><td>61.95</td><td>31.99</td><td>4.19</td><td>2.09</td><td>33.39</td><td>30.90</td><td>0.0136</td><td>0.0073</td></tr><tr><td>RiDDLE</td><td>√</td><td>StyleGAN</td><td>30.01</td><td>29.27</td><td>0.00</td><td>74.40</td><td>58.32</td><td>0.00</td><td>0.00</td><td>4.04</td><td>74.02</td><td>0.0031</td><td>0.0017</td></tr><tr><td>Ours</td><td>√</td><td>Med-Knowledge</td><td>86.70</td><td>96.45</td><td>99.83</td><td>98.98</td><td>92.32</td><td>87.05</td><td>63.70</td><td>97.72</td><td>57.56</td><td>0.6775</td><td>0.5453</td></tr></table></body></html>\n\nTable 3: Comparison of various DeID methods on medical tasks. Classification and Segmentation tasks are evaluated on validation sets of MeMa and MeMa-Seg, respectively. ‘Rev’ denotes if the method is reversible.\n\nImplementation Details. For training the patient face generator model, we fine-tune Stable Diffusion v1-5 (Rombach et al. 2022) using the low-rank adaptation (LoRA) (Hu et al. 2021) technique, with the real patient data. The rank number is set to 64. We use the Adam optimizer (Kingma 2014) with $\\beta _ { 1 } = 0 . 9$ and $\\beta _ { 2 } ~ = ~ 0 . 9 9$ . The learning rate starts at $1 \\times 1 0 ^ { - 4 }$ and follows a cosine decay schedule. The batch size is 32, and the model is trained for ten epochs. It takes about five days to train the model on a machine equipped with two Nvidia A6000 GPUs. For training the medical semantics encoder, we use the same training strategy as above, except that the training data comes only from the MeMa training set. For training the MedSem-DeID model, we use the Adam optimizer with $\\beta _ { 1 } = 0 . 5$ and $\\beta _ { 2 } = 0 . 9 9$ . The initial learning rate is $2 \\times 1 0 ^ { - 4 }$ and is halved after 150,000 iterations. The total iteration number is 300,000. The batch size is 16. Training takes approximately two days on a machine equipped with four Nvidia 4090 GPUs.\n\nBenchmark Methods. For DeepPrivacy (Hukkela˚s, Mester, and Lindseth 2019), Password (Gu et al. 2020), CIAGAN (Maximov, Elezi, and Leal-Taixe´ 2020), and RiDDLE (Li et al. 2023), we adopt their officially released codes and models. For Disguise (Cai et al. 2024) and Personal (Cao et al. 2021), we request the materials from the authors.\n\nEvaluation Protocol and Metrics. Medical utility: for the disease classification task, we fine-tune the DiNov2 model (Oquab et al. 2023) on the MeMa training set. We evaluate its Top1 accuracy on the MeMa validation set processed by various DeID approaches. For the tumor segmentation task, we use the nnU-Net (Isensee et al. 2021) to evaluate different methods on MeMa-Seg, adopting the Dice score (Kamnitsas et al. 2017) and Jaccard index (Fletcher, Islam et al. 2018) as metrics. Real-word clinical utility: we recruit three physicians to manually diagnose the images in Real-ECXHCSU, that are de-identified by various DeID approaches. Each image is diagnosed by all three physicians, and the final diagnosis is determined by a majority voting strategy. We use Cohen’s Kappa $( k )$ (Banerjee et al. 1999) to measure the diagnosis consistency between the original and the de-identified images. $k$ is a common metric for evaluating clinical trial outcomes in the medical field. Identity protection: following recent works (Cao et al. 2021; Wen et al. 2023), we use Euclidean distance between the identity features of de-identified and original faces, denoted as ‘ID-Dis’, to quantitatively evaluate the effectiveness of identity protection. Identity features are extracted by FaceNet (Schroff, Kalenichenko, and Philbin 2015) trained on CASIA (Yi et al. 2014), FaceNet trained on VGGFace2 (Cao et al. 2018), and SphereFace (Liu et al. 2017), which are not used in the training procedure. Other utilities: following previous methods (Li et al. 2023; Cai et al. 2024), we adopt the Dlib (King 2009) and L2CS-Net (Abdelrahman et al. 2023) to evaluate the landmark detection and gaze estimation performances. Reversibility: we compare our method against the previous reversible methods, in terms of ID similarity, medical results, and visual quality of the reconstructed original image.\n\n# Results\n\nMedical Utility. As shown in Tab. 3, Our method achieves the best overall classification accuracy, outperforming the second-best method, Disguise, by more than $10 \\%$ . For SCC disease, our approach outperforms DeepPrivacy, CIAGAN, Disguise, Password, Personal, and RiDDLE by $8 6 . 1 0 \\%$ , $7 3 . 5 3 \\%$ , $2 7 . 7 6 \\%$ , $34 . 6 7 \\%$ , $8 2 . 8 6 \\%$ , and $8 7 . 0 5 \\%$ , respectively. On the more challenging tumor segmentation task, our approach also performs best, achieving the highest Dice (0.6775) and Jaccard (0.5453) scores.\n\nMoreover, we train Password and Disguise models on our MeMa dataset, improving their classification accuracy to $5 4 . 6 7 \\%$ and $7 7 . 1 2 \\%$ , respectively, but still lagging behind our $8 6 . 7 0 \\%$ . This indicates that MeMa can enhance the efficacy of various DeID methods in medical contexts, and its full potential can be realized through specialized medicalscene methods like our MedSem-DeID.\n\nIn Tab. 3 (3rd column), we summarize the priors employed by different methods. Landmark priors (DeepPrivacy and CIAGAN) and high-level common priors (face attributes/StyleGAN adopted by Personal/RiDDLE) perform poorly in medical applications, i.e., less than $50 \\%$ accuracy and 0.2 segmentation Dice score. The gaze prior (Disguise) is effective for coarse-grained classification $( 7 6 . 0 1 \\% )$ but fails in fine-grained segmentation task (0.2751). Password, using a U-Net to preserve high-frequency signals, excels in low-level segmentation (0.6336) but fails in high-level classification task $( 5 4 . 2 1 \\% )$ . This also introduces visual artifacts (Fig. 6, 4rd column). In contrast, our method, leveraging the medical semantics within MeMa, achieves superior performance in both classification $( 8 6 . 7 0 \\% )$ and segmentation (0.6775) without handcrafted designs such as landmark.\n\nReal-World Clinical Utility. We conduct a clinical trial on the Real-ECXHCSU cohort. As shown in Tab. 4, our method largely outperforms the recent DeID methods (Disguise and Password) across all five disease categories, achieving near-perfect consistency in the clinical outcomes, i.e., $k { > } 0 . 8 1$ . Moreover, our approach is more flexible and effective than a recent hand-crafted DeID approach that is delicately designed for eye diseases, namely, digital mask (DM) (Yang et al. 2022). On complex diseases, such as BCC and Eyelid Nevus (EyelidN), DM does not work ( $k$ $= 0 . 0 5 6 6 / 0 . 0 9 8 8 )$ while our approach achieves satisfactory results $\\left( k = 0 . 8 2 4 5 / 0 . 8 3 4 6 \\right)$ ). Moreover, this real-world evaluation introduces additional disease categories not seen during training, i.e., Entropion and EyelidN, highlighting the robustness and generalizability of our method. The diagnosis accuracy is provided in the supplementary material.\n\nTable 4: Comparison of different DeID methods in terms of the diagnosis outcomes, on the real-world cohort RealECXHCSU. $k { > } 0 . 8 1$ indicates perfect clinical consistency.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"4\">Cohen's Kappa (k)↑</td></tr><tr><td>BCC</td><td>TAO Ptosis</td><td>Entropion</td><td>EyelidN</td></tr><tr><td>DM</td><td>0.0566</td><td>0.8159 0.8276</td><td>0.1879</td><td>0.0988</td></tr><tr><td>Disguise</td><td>0.7534</td><td>0.5824 0.7134</td><td>0.2467</td><td>0.1387</td></tr><tr><td>Password</td><td>0.4657 0.2758</td><td>0.4289</td><td>0.1329</td><td>0.0459</td></tr><tr><td>RiDDLE</td><td>0.1201</td><td>0.0751 0.0826</td><td>0.0937</td><td>0.0811</td></tr><tr><td>Ours</td><td>0.8245</td><td>0.8278 0.8302</td><td>0.8256</td><td>0.8346</td></tr></table></body></html>\n\nTable 5: Comparison of different methods on the MeMa validation set. The higher the ID-Dis, the better de-identified. Bold and italic indicates the best and the second-best result.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Rev</td><td colspan=\"3\">ID-Dis ↑</td></tr><tr><td>FaceNetvGGFace2</td><td>FaceNetcAsIA</td><td>Sphere</td></tr><tr><td>DeepPrivacy</td><td>X</td><td>1.1548</td><td>1.1831</td><td>1.1818</td></tr><tr><td>CIAGAN</td><td>X</td><td>1.2843</td><td>1.2566</td><td>1.2881</td></tr><tr><td>Disguise</td><td>X</td><td>1.3976</td><td>1.3607</td><td>1.3128</td></tr><tr><td>Password</td><td>√</td><td>1.3380</td><td>1.3139</td><td>1.2629</td></tr><tr><td>Personal</td><td>√</td><td>1.2819</td><td>1.2944</td><td>1.2351</td></tr><tr><td>RiDDLE</td><td>√</td><td>1.4278</td><td>1.3694</td><td>1.3583</td></tr><tr><td>Ours</td><td>√</td><td>1.4007</td><td>1.3609</td><td>1.3601</td></tr></table></body></html>\n\nTable 6: Face matching rate on Real-ECXHCSU.   \n\n<html><body><table><tr><td>Method</td><td>DeepPrivacy Password Disguise RiDDLE Ours</td><td></td><td></td></tr><tr><td>Rate(%)↓</td><td>5.76</td><td>7.76 2.89</td><td>2.13 1.76</td></tr></table></body></html>\n\nDe-Identification Performance. As shown in Tab. 5, our method achieves the best DeID performance of ID-Dis value 1.3601, when evaluated with the SphereFace face recognition model. With the FaceNetVGGFace2 and $\\mathrm { F a c e N e t _ { C A S I A } }$ models, our approach outperforms all methods except RiDDLE. RiDDLE maps person images into the very low-dimensional StyleGAN (Karras, Laine, and Aila 2019) latent space and selects a sample with the maximum identity distance from this space. While this over-dimension-reduction operation benefits identity protection, it sacrifices much original face information, resulting in poor downstream utilities, as evidenced in Tab. 3 and Tab. 4.\n\nFurthermore, we evaluate our approach on the LFW dataset (Huang et al. 2008). With the SphereFace facial recognition network, our approach achieves a face verification accuracy close to random guessing $( 5 0 \\% )$ .\n\nMoreover, we simulate a real-world identity authentication system. We use ID-card photos of Real-ECXHCSU patients as the identity database. We then match the deidentified clinical photos within the ID photo database. Note that the ID photo may be a long time away from the clinical photo. As shown in Tab. 6, our approach achieves the lowest successful face matching rate of $1 . 7 6 \\%$ , compared to other approaches such as Disguise $( 2 . 8 9 \\% )$ and RiDDLE $( 2 . 1 3 \\% )$ . Qualitative Results. As shown in Fig. 6, our approach uniquely preserves both coarse- and fine-grained medical cues, such as drooping eyelids and conjunctival redness. In contrast, DeepPrivacy masks and replaces the original face, Password retains color but distorts shapes. Disguise, Personal, and RiDDLE sacrifice medical cues for privacy. Besides, our approach demonstrates good visual quality.\n\nTable 7: Comparison of different DeID methods, in terms of common utilities, on the MeMa validation set. Landmark error is calculated as the averaged pixel distance between the original and the de-identified image. Bold and italic indicates the best and the second-best performance.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Rev</td><td colspan=\"2\">LandmarkError</td><td colspan=\"2\">Gaze Error↓</td></tr><tr><td>All</td><td>Eye Mouth Nose</td><td></td><td>Pitch Yaw</td></tr><tr><td>DeepPrivacy CIAGAN</td><td>X X X</td><td>193.91</td><td>5.86 155.02 33.02 313.36 23.87 215.11 74.37</td><td>7.72 13.00</td><td>7.06 7.78</td></tr><tr><td>Disguise Password Personal RiDDLE</td><td>√ √</td><td>94.09 65.90 109.46 136.79</td><td>5.78 67.29 4.38 42.52 5.87 71.20 5.73 91.28</td><td>21.01 18.98 32.38 39.77</td><td>4.85 5.71 5.34 9.97 7.24 7.45 7.01 7.93</td></tr></table></body></html>\n\nOther Downstream Utilities. As shown in Tab. 7, our approach shows competitive or best results on facial landmark detection and gaze detection tasks. For example, our approach achieves an eye landmark detection error of 2.94, much lower than the second-best approach, Password, which achieves 4.38. This is due to our particularly preserved eyerelated medical semantics. For gaze detection, although Disguise explicitly introduces the gaze detection loss, it still obtains a larger gaze error of $4 . 8 5 ~ \\nu . s . ~ 3 . 9 6$ , proving the power of our semantics learned on MeMa.\n\nReversibility. As shown in Tab. 8, compared with other reversible methods, our method performs better in terms of identity recovery, image fidelity, and perceptual quality, achieving ID-Dis, Peak Signal-to-Noise Ratio (PSNR), and LPIPS (Zhang et al. 2018) values of 0.5642, 27.02dB, and 0.2098, respectively. Moreover, our approach exhibits the best disease classification accuracy of $8 9 . 3 4 \\%$ , while the second-best Personal only obtains $7 3 . 0 8 \\%$ .\n\nWe provide the qualitative results in Fig. 7. Only our approach precisely preserves the clinical diagnosis necessary sign, i.e., the discolored left eye iris. RiDDLE generates high-quality facial textures, while obsoleting medical details. Password and Personal can not generate sharp details.\n\n# Model Analysis\n\nAblation Study for MedSem-DeID Model. Recalling that MedSem-DeID enhances the medical knowledge of the\n\n<html><body><table><tr><td>Method</td><td>ID-Dis↓</td><td>PSNR↑</td><td>LPIPS↓</td><td>Med-Class↑</td></tr><tr><td>Password</td><td>0.6382</td><td>26.29dB</td><td>0.2752</td><td>67.99%</td></tr><tr><td>Personal</td><td>0.5723</td><td>25.92dB</td><td>0.2240</td><td>73.08%</td></tr><tr><td>RiDDLE</td><td>0.8593</td><td>14.00dB</td><td>0.3732</td><td>47.46%</td></tr><tr><td>Ours</td><td>0.5642</td><td>27.02dB</td><td>0.2098</td><td>89.34%</td></tr></table></body></html>\n\nTable 8: Comparison of the recovered image by various reversible approaches on MaMa validation set. Lower ID-Dis indicates the recovered identity is more similar to the original. Lower LPIPS indicates better perceptual quality.\n\n![](images/5297b792033e727afe343e6f9a3fe81a6e5e1a88015557d1fdb704be5dd3a443.jpg)  \nFigure 7: Qualitative results of the recovered image of different reversible DeID approaches. The ocular region is zoomed-in for a more clear comparison.\n\nTable 9: Framework ablation Study. Both the MeMa dataset and the utilization of medical priors are useful. ID-Dis is calculated with the SphereFace network. Med-Class denotes the disease classification accuracy.   \n\n<html><body><table><tr><td>Model</td><td>Dataset</td><td>fmed</td><td>Lmed</td><td>ID-Dis↑</td><td>Med-Class</td></tr><tr><td>M1</td><td>FFHQ</td><td>X</td><td>X</td><td>1.3609</td><td>42.13%</td></tr><tr><td>M2</td><td>MeMa</td><td>X</td><td>X</td><td>1.3609</td><td>46.82%</td></tr><tr><td>M3</td><td>MeMa</td><td>√</td><td>X</td><td>1.3602</td><td>69.94%</td></tr><tr><td>M4</td><td>MeMa</td><td>X</td><td>√</td><td>1.3601</td><td>71.35%</td></tr><tr><td>Ours</td><td>MeMa</td><td>√</td><td>√</td><td>1.3601</td><td>86.70%</td></tr></table></body></html>\n\nDeID pipeline in both the feature extraction procedure $( f _ { m e d } )$ and the loss function $( \\mathcal { L } _ { m e d } )$ , we verify the effectiveness of both strategies. As shown in Tab. 9, when trained on the common-scene facial dataset FFHQ without using any medical prior, the resulting model (M1) achieves an IDDis score of 1.3689 and a disease classification accuracy of $4 2 . 1 3 \\%$ . When the training dataset is replaced with MeMa, the medical accuracy of the resulting model (M2) improves to $4 6 . 8 2 \\%$ without compromising the DeID performance. After further introducing medical priors, no matter the $f _ { m e d }$ or the $\\mathcal { L } _ { m e d }$ , the resulting models M3 and M4 show an obvious improvement in classification accuracy, i.e., $6 9 . 9 4 \\%$ and $7 1 . 3 5 \\%$ , while slightly compromising the DeID results. When combining both strategies, the final model achieves a strong medical performance of $8 6 . 7 0 \\%$ .\n\nDifferent Medical Semantic Encoders. Our method is flexible, not relying on the typical implementation of the medical encoder. To verify this, we trained two other semantic encoders on MeMa. We fine-tuned a pre-trained ViT model (Sharir, Noy, and Zelnik-Manor 2021) using supervised and self-supervised learning strategies, specifically the\n\nTable 10: Impact of various medical semantic encoders.   \n\n<html><body><table><tr><td></td><td>ViT(Supervised)</td><td>ViT(MAE)</td><td>Diffusion</td></tr><tr><td>Med-Class↑</td><td>82.97%</td><td>85.26%</td><td>86.70%</td></tr><tr><td>ID-Dis↑</td><td>1.3600</td><td>1.3601</td><td>1.3601</td></tr></table></body></html>\n\nMedicalLossWeight ReversibleLossWeight 28 1.325日 M70 Med-Class PS-DiS 1.25 1.300 0.1 1 5 10 0.01 0.1 1 10 λmed 入rev\n\nmasked auto-encoder (MAE) (He et al. 2022). As shown in Tab. 10, all three variants achieved decent performance. The supervised ViT performed the poorest due to the sparse disease category label for supervision. The diffusion model outperformed ViT(MAE) with $8 6 . 3 8 \\%$ vs. $8 5 . 2 6 \\%$ . This is likely because the LAION-5B (Schuhmann et al. 2022) dataset used for pre-training the base stable diffusion model is much larger than the pre-training dataset for the base ViT. Different Loss Weights. As shown in Fig. 8 (left), increasing the weight of medical loss $( \\lambda _ { m e d } )$ consistently improves disease classification accuracy, due to the enhanced medical information. However, this also makes the de-identified images more similar to the originals, compromising the DeID performance, i.e., the reduced ID-Dis. We set $\\lambda _ { m e d } = 5$ to achieve the best trade-off between medical accuracy and DeID. For the weight controlling reversible reconstruction $( \\lambda _ { r e v } )$ , a similar trade-off between the reconstructed image quality and DeID performance is observed, as shown in Fig. 8 (right). We set $\\lambda _ { r e v } = 0 . 1$ to achieve optimal results.\n\n# Conclusion and Limitation\n\nWe have released a large-scale patient face dataset, MeMa, to facilitate research on medical privacy protection. Expert physicians validated and annotated MeMa. On this dataset, we established a comprehensive benchmark for medicalscene de-identification, also proposing a new baseline approach that outperforms previous approaches. A limitation is that the current dataset focuses on eye disease-related manifestations. Future work will expand the dataset to include other facial diseases, such as facial paralysis.\n\n# Acknowledgments\n\nThis work is supported by Strategic Research and Consulting Project of Chinese Academy of Engineering (2024-XBZD-18), National Natural Science Foundation of China (62225112), Shanghai Artificial Intelligence Laboratory, National Natural Science Foundation of China (62101326), National Natural Science Foundation of China (82388101), National Natural Science Foundation of China (72293585), and National Natural Science Foundation of China (72293580). We thank Min Zhou (Doctor of Medicine) and Xuefei Song (Doctor of Medicine) for their invaluable assistance with patient data collection, data annotation, and medical knowledge support.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了医疗场景中面部去识别（DeID）的问题，特别是在保护患者隐私的同时保留诊断必要的医学表现。现有方法在医疗场景中表现不佳，主要因为缺乏大规模患者面部数据集和未能保留疾病特征。\\n> *   该问题的重要性在于医疗AI时代患者隐私泄露的严重性，以及现有方法在保留医学效用方面的不足。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一个医学表现丰富的患者面部数据集MeMa，包含42,307张照片级真实感的虚拟患者图像，由专家医师标注。\\n> *   提出了基线方法MedSem-DeID，通过将医学语义先验集成到DeID过程中，保留医学效用。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **贡献1：** 发布了首个大规模患者面部数据集MeMa，包含超过40,000张照片级真实感的虚拟患者图像，由专家医师标注。\\n> *   **贡献2：** 提出了基线方法MedSem-DeID，在疾病分类任务上达到86.70%的准确率，显著优于现有方法（如Disguise的76.01%）。\\n> *   **贡献3：** 在肿瘤分割任务上，MedSem-DeID的Dice得分为0.6775，优于其他方法（如Password的0.6336）。\\n> *   **贡献4：** 在真实世界临床评估中，MedSem-DeID的Cohen's Kappa值达到0.8245，显著优于其他方法（如Disguise的0.7534）。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   MedSem-DeID的核心思想是通过医学语义编码器（Enc_med）将医学知识注入DeID流程，同时在损失函数中引入医学语义损失（L_med），以保留医学效用。\\n> *   该方法有效的原因在于其能够同时保护患者隐私和保留诊断必要的医学特征，通过可逆设计满足医疗审计需求。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 现有DeID方法（如DeepPrivacy、CIAGAN）主要关注通用场景，未能保留医学特征，导致医学效用下降。\\n> *   **本文的改进：** MedSem-DeID通过医学语义编码器和医学语义损失，针对性解决了医学特征保留的问题，同时在可逆性设计上优于其他方法。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> *   1. **医学语义编码：** 使用在MeMa上训练的扩散模型提取医学特征f_med。\\n> *   2. **特征融合：** 将面部特征f_face和医学特征f_med融合，通过残差块生成f。\\n> *   3. **身份加密：** 使用ID-Encryptor对f进行加密，生成加密特征f_enc。\\n> *   4. **图像生成：** 通过图像解码器生成加密图像X_enc。\\n> *   5. **可逆设计：** 使用ID-Decryptor和原始密码P恢复原始图像X。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   DeepPrivacy、CIAGAN、Disguise、Password、Personal、RiDDLE。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在疾病分类准确率上：** 本文方法在MeMa验证集上达到了86.70%，显著优于基线模型Disguise（76.01%）和Password（54.21%）。与表现最佳的基线相比，提升了10.69个百分点。\\n> *   **在肿瘤分割Dice得分上：** 本文方法的Dice得分为0.6775，远高于Password（0.6336），同时与Disguise（0.2751）相比有显著提升。\\n> *   **在身份保护（ID-Dis）上：** 本文方法在SphereFace评估下的ID-Dis值为1.3601，优于大多数基线（如Password的1.2629），仅次于RiDDLE（1.3583）。\\n> *   **在真实世界临床评估上：** 本文方法的Cohen's Kappa值达到0.8245，显著优于Disguise（0.7534）和Password（0.4657）。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   面部去识别 (Face De-Identification, DeID)\\n*   医学表现保留 (Medical Manifestation Preservation, N/A)\\n*   扩散模型 (Diffusion Model, DM)\\n*   医学语义编码 (Medical Semantic Encoding, N/A)\\n*   患者隐私保护 (Patient Privacy Protection, N/A)\\n*   可逆去识别 (Reversible De-Identification, N/A)\\n*   医学数据集 (Medical Dataset, N/A)\\n*   医学人工智能 (Medical Artificial Intelligence, MAI)\"\n}\n```"
}