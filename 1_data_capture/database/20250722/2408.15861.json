{
    "source": "Semantic Scholar",
    "arxiv_id": "2408.15861",
    "link": "https://arxiv.org/abs/2408.15861",
    "pdf_link": "https://arxiv.org/pdf/2408.15861.pdf",
    "title": "Fusing Pruned and Backdoored Models: Optimal Transport-based Data-free Backdoor Mitigation",
    "authors": [
        "Weilin Lin",
        "Li Liu",
        "Jianze Li",
        "Hui Xiong"
    ],
    "categories": [
        "cs.CR",
        "cs.LG"
    ],
    "publication_date": "2024-08-28",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 0,
    "institutions": [
        "The Hong Kong University of Science and Technology (Guangzhou)",
        "Shenzhen Research Institute of Big Data",
        "The Chinese University of Hong Kong, Shenzhen"
    ],
    "paper_content": "# Fusing Pruned and Backdoored Models: Optimal Transport-based Data-free Backdoor Mitigation\n\nWeilin Lin1, Li Liu1\\*, Jianze $\\mathbf { L i } ^ { 2 , 3 }$ , Hui Xiong\n\n1The Hong Kong University of Science and Technology (Guangzhou) 2Shenzhen Research Institute of Big Data 3The Chinese University of Hong Kong, Shenzhen\n\n# Abstract\n\nBackdoor attacks present a serious security threat to deep neuron networks (DNNs). Although numerous effective defense techniques have been proposed in recent years, they inevitably rely on the availability of either clean or poisoned data. In contrast, data-free defense techniques have evolved slowly and still lag significantly in performance. To address this issue, different from the traditional approach of pruning followed by fine-tuning, we propose a novel data-free defense method named Optimal Transport-based Backdoor Repairing (OTBR) in this work. This method, based on our findings on neuron weight changes (NWCs) of random unlearning, uses optimal transport (OT)-based model fusion to combine the advantages of both pruned and backdoored models. Specifically, we first demonstrate our findings that the NWCs of random unlearning are positively correlated with those of poison unlearning. Based on this observation, we propose a random-unlearning NWC pruning technique to eliminate the backdoor effect and obtain a backdoor-free pruned model. Then, motivated by the OT-based model fusion, we propose the pruned-to-backdoored OT-based fusion technique, which fuses pruned and backdoored models to combine the advantages of both, resulting in a model that demonstrates high clean accuracy and a low attack success rate. To our knowledge, this is the first work to introduce OT and model fusion techniques to the backdoor defense. Extensive experiments show that our method successfully defends against all seven backdoor attacks across three benchmark datasets, outperforming both state-of-the-art (SOTA) data-free and data-dependent methods.\n\n# Code — https://github.com/linweiii/OTBR Extended version — https://arxiv.org/pdf/2408.15861\n\n# Introduction\n\nOver the past decade, deep neural networks (DNNs) have become a crucial technology in various applications, including image recognition (Parmar and Mehta 2014; He et al. 2016a), speech processing (Gaikwad, Gawali, and Yannawar 2010; Maas et al. 2017), and natural language processing (Chowdhary and Chowdhary 2020), etc. However, as the deployment of DNNs in sensitive and critical domains\n\nNWCs of Unlearning (BadNets) NWCs of Unlearning (Blended) Clean Unlearning Clean Unlearning Random Unlearning Random Unlearning NWC values 中 小 NWCs of Poison Unlearning NWCs of Poison Unlearning becomes more widespread, concerns regarding their security cannot be ignored. Among the numerous threats to DNNs, backdoor attacks (Gu et al. 2019; Li et al. 2021a; Wu et al. 2023a) are particularly concerning. In these attacks, the attackers manipulate a small portion of the training data to implant a stealthy backdoor into a DNN, resulting in a backdoored model. During inference, the backdoored model behaves anomalously when the input contains a pre-defined trigger pattern; otherwise, it performs normally. This phenomenon is termed the backdoor effect. Such attacks may pose hidden security issues to real-world applications, such as unauthorized access to a system when a company develops its software using a third-party pre-trained model.\n\nIn recent years, as backdoor attack methods have evolved, backdoor defense (Wu et al. 2023b) techniques have also seen significant growth. Various important techniques have been developed for backdoor defense, including pruning (Wu and Wang 2021; Zheng et al. 2022b), unlearning (Zeng et al. 2021a), and fine-tuning (Zhu et al. 2023), etc. However, most of these techniques rely on the availability of clean or poisoned data, which restricts their applicability to the aforementioned scenarios. Recent insights reveal a promising direction (Lin et al. 2024): using neuron weight changes (NWCs) of clean unlearning1 to categorize the neurons into backdoor-related ones and clean ones2, based on an observation that the NWCs of unlearning clean and poisoned data are positively correlated. In this work, our extended findings reveal that using random noise for unlearning, termed as random unlearning, brings a new similar insight: the NWCs of random unlearning exhibit a positive correlation with those of poison unlearning (as shown in Figure 1). This motivates us to adopt NWCs for datafree backdoor mitigation using only the generated random noise. Normally, after identifying backdoor-related neurons, pruning followed by fine-tuning is employed to eliminate the backdoor effect and restore the lost performance (Liu, Dolan-Gavitt, and Garg 2018). However, this is infeasible in data-free scenarios since the subsequent fine-tuning requires clean data. If we only perform pruning using NWCs and simply skip the fine-tuning, the clean accuracy (ACC) is prone to decrease by more than $10 \\%$ (Lin et al. 2024). Therefore, after pruning, it is necessary to develop a new data-free technique for performance recovery.\n\n![](images/9df2cbca142f7afffcc0cb839c3826e40f9d064a1462966f697c063332ee2343.jpg)  \nFigure 2: OT-based model fusion for backdoor defense. The pruned model is aligned with the backdoored model layerby-layer using OT. Then the models are fused through a weighted averaging operation.\n\nRecently, model fusion (Li et al. 2023a) has received increasing attention. It combines the weights of multiple models to integrate their capabilities into a single network. As one of the most representative works, OTFusion (Singh and Jaggi 2020) employs optimal transport (OT) to align model weights layer-by-layer before fusing two models through averaging. Following it, Intra-Fusion (Theus et al. 2024) employs OT to integrate the functionality of pruned neurons with the remaining ones, aiming to maintain great performance after pruning. It can be seen that the aforementioned methods both demonstrate OT’s inherent ability to preserve critical information during the fusion process.\n\nMotivated by the above advancements in model fusion, in this work, we explore its potential to combine the high ACC of the backdoored model with the low attack success rate (ASR) of the pruned model in a data-free manner. Building on the foundation of NWC pruning and OTbased model fusion, we propose a novel data-free defense strategy called Optimal Transport-based Backdoor Repairing (OTBR), which fuses pruned and backdoored models. OTBR consists of two stages: random-unlearning\n\nNWC pruning and pruned-to-backdoored OT-based fusion. In the first stage, we calculate the NWCs based on random unlearning of the backdoored model, and then prune the topranking $\\gamma$ neurons to eliminate the backdoor effect. In the second stage, we align the weights of the pruned model with those of the backdoored model layer-by-layer using OT, and then fuse them into a single model. This process effectively dilutes the backdoor effect while preserving the clean performance. The fusion process is shown in Figure 2.\n\nOur main contributions can be summarized as follows:\n\n• We provide a new data-free pruning insight by revealing the positive correlation between NWCs when unlearning random noise and poisoned data. • We propose a novel data-free defense strategy that combines the high ACC of the backdoored model with the low ASR of the pruned model, using the OT-based model fusion. To our knowledge, this is the first work to apply OT and model fusion techniques to backdoor defense. • Experiments across various attacks, datasets, and experimental setups validate the effectiveness of our proposed OTBR method. Specifically, OTBR significantly outperforms both state-of-the-art (SOTA) data-free methods and SOTA data-dependent ones, consistently achieving successful defense performance against all tested attacks.\n\n# Related Work\n\n# Backdoor Attack\n\nIn the literature, various backdoor attacks on DNNs have been proposed, which can be generally categorized into two types: data-poisoning attacks and training-controllable attacks. For data-poisoning attacks, adversaries have access to the training dataset. BadNets (Gu et al. 2019), as one of the earliest examples, was proposed to implant a trigger pattern into the bottom-right corner of a small subset of the training images and reassign the labels to a specific target one. To enhance the stealthiness of the trigger, Blended (Chen et al. 2017) was proposed to blend the trigger onto the selected data with adjustable opacity. Recently, more sophisticated strategies have been proposed to enhance the trigger, including but not limited to SIG (Barni, Kallas, and Tondi 2019), label-consistent attacks (Shafahi et al. 2018; Zhao et al. 2020), and SSBA (Li et al. 2021a). Meanwhile, the second type, training-controllable attacks, is also rapidly evolving. In these attacks, adversaries have access to the training process, enabling more advanced attack strategies. Representative examples of this category include WaNet (Nguyen and Tran 2021) and Input-aware (Nguyen and Tran 2020), which incorporate an injection function into the training process to generate unique triggers for each input data. These innovative tactics make it more challenging to detect the triggers and conduct an effective defense.\n\n# Backdoor Defense\n\nIn general, backdoor defense methods can be categorized into three types: pre-training, in-training, and post-training defenses. Among them, Post-training Defense has received the most attention, where the defenders aim to mitigate the backdoor effect of a well-trained backdoored model. FP (Liu, Dolan-Gavitt, and Garg 2018), as one of the seminal defense methods, prunes the less-activated neurons and then fine-tunes the model, based on the observation that poisoned and clean data activate different neurons; ANP (Wu and Wang 2021) detects and prunes backdoorrelated neurons by applying adversarial perturbations to neuron weights; Building on this, RNP (Li et al. 2023b) refines the perturbation technique using clean unlearning, and performs pruning based on a learned mask. In addition to these pruning-based techniques, some other important defense techniques exist. NC (Wang et al. 2019) proposes recovering the trigger to improve backdoor removal; NAD (Li et al. 2021c) pioneers the use of model distillation to train a benign student model; i-BAU (Zeng et al. 2021a) uses adversarial attacks to identify potential triggers and then performs poison unlearning to mitigate the backdoor effect.\n\n![](images/f7e045db35b82848d9f2b67abece711f0b227676729b2c065dfc5b552270fa16.jpg)  \nFigure 3: Overview of the proposed OTBR framework.\n\nDifferent from the above defenses, which are all datadependent, CLP (Zheng et al. 2022a) is the first datafree defense method, which identifies and prunes potential backdoored neurons based on channel Lipschitzness; ABD (Hong et al. 2023) designs a plug-in defensive technique specialized for data-free knowledge distillation; DHBE (Yan et al. 2023) proposes a competing strategy between distillation and backdoor regularization to distill a clean student network without data.\n\nAlthough several data-dependent techniques have already been proposed in the literature, the scarcity of data-free defense techniques still limits the applicability of backdoor defenses in real-world scenarios. In this paper, we will focus on addressing this issue, and develop a novel effective data-free defense method by using random-unlearning NWCs and the OT-based model fusion technique.\n\n# Preliminary\n\n# Threat Model\n\nIn this work, we address threats from both data-poisoning and training-controllable attacks. The attackers aim to poison a small portion of the training data so that the trained model predicts a target class when presented with data containing a pre-defined trigger, while otherwise performing normally. The weights of a $L$ -layer backdoored model are denoted as $\\pmb { \\theta } _ { b d } = \\overline { { \\{ \\pmb { \\theta } _ { b d } ^ { ( l ) } \\} _ { 1 \\leq l \\leq L } } }$ , where $\\pmb { \\theta } _ { b d } ^ { ( l ) }$ represents the weights for the ${ l ^ { t h } }$ layer, consisting of $m ^ { ( l ) }$ neurons.\n\n# Defense Setting\n\nWe focus on the post-training scenario, aiming to mitigate the backdoor effect of a well-trained backdoored model while minimizing the negative impact on ACC. Different from most previous works (Liu, Dolan-Gavitt, and Garg 2018; Wu and Wang 2021; Zeng et al. 2021a), which assumes access to $5 \\%$ of clean data for defense, we adopt a more stringent approach that relies only on the backdoored model, without access to any clean data (Zheng et al. 2022a).\n\n# Method\n\n# Overview of Our Method\n\nThe complete data-free defense process of our proposed OTBR strategy is illustrated in Figure 3, which consists of two stages as follows:\n\n• In Stage 1, referred to as random-unlearning NWC pruning, we aim to obtain a backdoor-free model. Specifically, we first conduct random unlearning on the backdoored model for $I$ iterative steps. During each step, a mini-batch of random noise with random labels is generated and used for unlearning. Then, we calculate the NWC for each neuron based on their weights from both the backdoored and unlearned models. Finally, we prune the top-ranking $\\gamma$ of neurons, based on their NWCs, from the backdoored model to eliminate its backdoor effect.\n\n• In Stage 2, referred to as pruned-to-backdoored OTbased fusion, when obtaining a sub-optimal pruned model, we aim to combine its low ASR with the high ACC of the original backdoored model by repairing the backdoor-related neurons using OT-based model fusion. Specifically, we propose NWC-informed OT to align the weights of the pruned model with those of the backdoored model layer-by-layer, taking into account the backdoor importance as determined by NWCs. For each layer, starting with the earliest pruned one, we initialize the probability mass on neuron weights using a uniform distribution for the pruned model and an NWC distribution for the original backdoored model. This strategy discriminatively transfers clean functionality to the backdoor-related neurons. The cost matrix $\\boldsymbol { C } ^ { ( l ) }$ is calculated based on the Euclidean distance between neuron weights from the two models. Using this cost matrix, we then derive the optimal transport map $\\mathbf { T } ^ { ( l ) }$ and employ it to transport the weights of the pruned model. After aligning all layers, we perform a simple weight averaging to fuse the transported and backdoored models, resulting in an effective defense.\n\nNext, we will present more detailed formulations and provide further insights.\n\n# Stage 1: Random-Unlearning NWC Pruning\n\nRandom Unlearning. Unlearning is a reverse training process designed to maximize the loss value on a given dataset (Li et al. 2023b). In this work, we define random unlearning as the process of unlearning a DNN model $f$ using a generated random dataset $\\textstyle { \\mathcal { D } } _ { r }$ . More precisely, random unlearning on the backdoored model $\\pmb { \\theta } _ { b d }$ is formulated as:\n\n$$\n\\operatorname* { m a x } _ { \\pmb { \\theta } _ { b d } } \\mathbb { E } _ { ( \\pmb { x } _ { r } , \\pmb { y } _ { r } ) \\in \\mathcal { D } _ { r } } \\left[ \\mathcal { L } ( f ( \\pmb { x } _ { r } ; \\pmb { \\theta } _ { b d } ) , \\pmb { y } _ { r } ) \\right] ,\n$$\n\nwhere the loss function $\\mathcal { L }$ is chosen to be a cross-entropy loss, and the generated random dataset $\\mathcal { D } _ { r }$ contains $I \\times B$ pairs of random noises $\\pmb { x } _ { r } \\in [ 0 , 1 ] ^ { A \\times H \\times \\dot { W } }$ and random×labels $y _ { r } \\in \\{ 0 , 1 , \\ldots , G \\}$ . Here, $I$ is the number of iterative steps; $B$ is the batch size; $A , H$ and $W$ represent the generated noise size; and $G$ is the largest class label.\n\nNWC Pruning. We follow the NWC definition from (Lin et al. 2024) to quantify the weight changes for each neuron during unlearning. Specifically, for the $j$ -th neuron in the $l .$ - th layer, the NWC is defined as:\n\n$$\n\\begin{array} { r } { \\mathrm { N W C } ^ { ( l ) j } \\stackrel { \\mathrm { d e f } } { = } \\| \\pmb { \\theta } _ { u l } ^ { ( l ) j } - \\pmb { \\theta } _ { b d } ^ { ( l ) j } \\| _ { 1 } , } \\end{array}\n$$\n\nwhere $\\pmb { \\theta } _ { u l }$ denotes the unlearned backdoored model, $j \\in$ $\\{ 1 , \\ldots , m ^ { ( l ) } \\}$ and $l \\in \\{ 1 , \\ldots , L \\}$ . To eliminate the backdoor effect, we sort all calculated NWCs in descending order and prune the top-ranking $\\gamma$ of neurons from the original backdoored model. The pruned model is denoted as $\\pmb { \\theta } _ { p n }$ .\n\n# Stage 2: Pruned-to-Backdoored OT-based Fusion\n\nOptimal Transport. OT is a mathematical framework to find the most economical way to transport mass from one distribution to another. Suppose we have two discrete probability distributions in the space $\\mathcal { X } ~ = ~ \\{ x _ { i } \\} _ { i = 1 } ^ { n }$ and $y =$ $\\{ y _ { j } \\} _ { j = 1 } ^ { m }$ , i.e., the source distribution $\\textstyle \\mu : = \\sum _ { i = 1 } ^ { n } \\alpha _ { i } \\cdot \\delta ( x _ { i } )$ and the target distribution $\\nu : = \\textstyle \\sum _ { j = 1 } ^ { m } \\beta _ { j } \\cdot \\delta ( y _ { j } )$ , where $\\textstyle \\sum _ { i = 1 } ^ { n } \\alpha _ { i } = \\sum _ { j = 1 } ^ { m } \\beta _ { j } = 1$ and $\\delta ( \\cdot )$ is the Dirac delta function. The OT problem can be formulated as a linear programming problem as follows:\n\n$$\n\\begin{array} { r } { \\mathrm { O T } ( \\mu , \\nu ; C ) \\stackrel { \\mathrm { d e f } } { = } \\operatorname* { m i n } \\langle \\mathbf { T } , C \\rangle , } \\\\ { \\mathrm { s . t . } , \\mathbf { T } \\mathbf { 1 } _ { m } = \\alpha , \\mathbf { T } ^ { \\top } \\mathbf { 1 } _ { n } = \\beta } \\end{array}\n$$\n\nwhere $\\mathbf { T } \\in \\mathbb { R } _ { + } ^ { n \\times m }$ is the transport map that determines the optimal transport amount of mass from $\\chi$ to $y$ , and $C$ is the cost matrix quantifying the cost of moving each unit of mass.\n\nNWC-informed OT. In our approach, we align the NWCpruned model, which contains only clean functionality, with the original backdoored model to achieve more effective fusion. The goal is to dilute the backdoor effect with the least influence on clean performance. To achieve this, we focus more on the backdoor-related neurons during weight transport by employing NWC-informed initialization for the target distribution $\\nu$ (backdoored model), while using a uniform distribution for the source distribution $\\mu$ (pruned model). For the $l$ -th layer, we denote the probability mass as:\n\n$$\n\\pmb { \\alpha } ^ { ( l ) } \\equiv \\left\\{ \\frac { 1 } { n ^ { ( l ) } } \\right\\} _ { i = 1 } ^ { n ^ { ( l ) } } , \\beta ^ { ( l ) } \\overset { \\mathrm { d e f } } { = } \\left\\{ \\frac { \\mathrm { N W C } ^ { ( l ) j } } { \\sum _ { j = 1 } ^ { m ^ { ( l ) } } \\mathrm { N W C } ^ { ( l ) j } } \\right\\} _ { j = 1 } ^ { m ^ { ( l ) } } ,\n$$\n\nwhere $n ^ { ( l ) }$ and $m ^ { ( l ) }$ denote the neuron numbers of pruned and backdoored models, respectively. Then, based on the distributions $\\mu ^ { ( l ) }$ and $\\nu ^ { ( l ) }$ , and the cost matrix $C ^ { ( l ) }$ , we can derive the optimal transport map $\\mathbf { T } ^ { ( l ) }$ by equation (3).\n\nModel Fusion. Inspired by the OTFusion (Singh and Jaggi 2020), we align and fuse the pruned and backdoored models layer-by-layer, using OT in equation (3) and our defined distributions in equation (4). The details of the entire fusion process are shown in Algorithm 1. Note that we start the fusion process from the first pruned layer $p$ , rather than the second layer. For the $l .$ -th layer, $\\boldsymbol { n } ^ { ( l ) }$ and $m ^ { ( l ) }$ represent the neuron number of ${ \\pmb \\theta } _ { p n } ^ { ( l ) }$ and $\\dot { \\pmb { \\theta } } _ { b d } ^ { ( l ) }$ , respectively.\n\nIn Algorithm 1, for each layer $l$ , we first align the incoming edge weights using the OT map $\\mathbf { T } ^ { ( l - 1 ) }$ and probability mass $\\beta ^ { ( l - 1 ) }$ from the previous layer:\n\n$$\n\\widehat { \\pmb { \\theta } } _ { p n } ^ { ( l ) }  \\pmb { \\theta } _ { p n } ^ { ( l ) } \\mathbf { T } ^ { ( l - 1 ) } \\mathrm { d i a g } ( 1 / \\beta ^ { ( l - 1 ) } ) .\n$$\n\nThen, we get the distributions $\\mu ^ { ( l ) }$ and $\\nu ^ { ( l ) }$ of the current layer and compute the cost matrix $C ^ { ( l ) }$ using Euclidean distance between neuron weights: $C _ { i j } ^ { ( l ) } \\stackrel { \\mathrm { d e f } } { = } \\| \\pmb { \\theta } _ { p n } ^ { ( l ) i } - \\pmb { \\theta } _ { b d } ^ { ( l ) j } \\| ^ { 2 }$ . Finally, as in equation (3), using $\\mu ^ { ( l ) }$ , $\\nu ^ { ( l ) }$ and $C ^ { ( l ) }$ , the OT\n\nInput: Pruned model $\\pmb { \\theta } _ { p n }$ , backdoored model $\\pmb { \\theta } _ { b d }$ , random-unlearning NWC values for each neuron, balance coefficient $\\lambda$ , the first pruned layer $p$ .\n\nOutput: Clean model $\\pmb { \\theta } ^ { * }$\n\n$$\n\\begin{array} { c } { { : \\alpha ^ { ( p - 1 ) }  \\{ 1 / n ^ { ( p - 1 ) } \\} _ { i = 1 } ^ { n ^ { ( p - 1 ) } } } } \\\\ { { \\beta ^ { ( p - 1 ) }  \\{ 1 / m ^ { ( p - 1 ) } \\} _ { j = 1 } ^ { m ^ { ( p - 1 ) } } } } \\end{array}\n$$\n\n$$\n\\begin{array} { r l } & { \\hat { \\theta } _ { p n } ^ { ( i ) } \\gets \\theta _ { p n } ^ { ( i ) } \\mathbf { Y } ^ { ( i - 1 ) } \\mathrm { d i a g } ( \\mathrm { 1 } / \\beta ^ { ( i - 1 ) } ) } \\\\ & { \\alpha ^ { ( i ) } \\gets \\left\\{ 1 / n ^ { ( i ) } \\right\\} _ { i = 1 } ^ { n ^ { ( i ) } } } \\\\ & { \\beta ^ { ( i ) } \\gets \\left\\{ \\mathrm { N W C } ^ { ( i ) } / \\sum _ { j = 1 } ^ { m ^ { ( i ) } } \\mathrm { N W C } ^ { ( i ) j } \\right\\} _ { j = 1 } ^ { m ^ { ( i ) } } } \\\\ & { \\mu ^ { ( i ) } , \\nu ^ { ( i ) } \\gets \\mathrm { G e l D i s t i n t i o n } ( \\alpha ^ { ( i ) } , \\beta _ { b j } ^ { ( i ) } ) } \\\\ & { C ^ { ( i ) } \\gets \\mathrm { C o m p u t c o s t } ( \\theta _ { p n } ^ { ( i ) } , \\theta _ { b j } ^ { ( i ) } ) } \\\\ & { \\mathbf { T } ^ { ( i ) } \\gets \\mathrm { O T } ( \\mu ^ { ( i ) } , \\nu ^ { ( i ) } , C ^ { ( i ) } ) } \\\\ & { \\hat { \\theta } _ { p n } ^ { ( i ) } \\gets \\mathrm { d i a g } ( \\mathrm { 1 } / \\beta ^ { ( i ) } ) \\mathbf { T } ^ { ( i ) ^ { \\top } } \\hat { \\theta } _ { p n } ^ { ( i ) } } \\\\ & { \\theta ^ { ( i ) } \\gets \\lambda \\hat { \\theta } _ { p n } ^ { ( i ) } + ( 1 - \\lambda ) \\theta _ { b j } ^ { ( i ) } } \\end{array}\n$$\n\n13: Obtain clean model $\\pmb { \\theta } ^ { * }$\n\nmap $\\mathbf { T } ^ { ( l ) }$ of the current layer can be derived, and the transported pruned model $\\widetilde { \\pmb { \\theta } } _ { p n } ^ { ( l ) }$ can be obtained as:\n\n$$\n\\widetilde { \\pmb { \\theta } } _ { p n } ^ { ( l ) }  \\mathrm { d i a g } ( 1 / \\beta ^ { ( l ) } ) { \\bf T } ^ { ( l ) ^ { \\top } } \\widehat { \\pmb { \\theta } } _ { p n } ^ { ( l ) } ,\n$$\n\nwhich has been aligned with the backdoored model.\n\nAfter alignment, the transported model is then fused with the backdoored model to obtain the final defense model, which can be formulated as:\n\n$$\n\\pmb { \\theta } ^ { * }  \\lambda \\widetilde { \\pmb { \\theta } } _ { p n } + ( 1 - \\lambda ) \\pmb { \\theta } _ { b d } ,\n$$\n\nwhere $\\boldsymbol { \\theta } ^ { * }$ represents the fused clean model and $\\lambda$ is the balance coefficient.\n\nWhy Can OT-based Fusion Mitigate Backdoor Effect? We now offer a possible explanation for the effectiveness of OT-based model fusion in mitigating backdoor effects. Based on previous work (Lin et al. 2024), the NWC-pruned model can be made backdoor-free, i.e., the ASR dropping to zero, by selecting a suitable pruning threshold. Therefore, by aligning the pruned model with the original backdoored model using NWC-informed OT, we can transport the clean functionality of the remaining neurons to the nearest backdoored positions, as determined by NWCs and Euclidean distance. Then, further fusion based on the transported model can be viewed as a dilution operation to weaken the backdoored effect of the original backdoored model while preserving its clean functionality, thanks to the inherent ability of OT (Singh and Jaggi 2020; Theus et al. 2024). This is consistent with the previous insights that the backdoor task is easier and encoded in much fewer neurons than the clean task (Li et al. 2021b; Cai et al. 2022).\n\n![](images/d40440d57331e2f91deb96ed4d9410ce9d9d3cafedcecfc0f071b5731c906249.jpg)  \nFigure 4: Illustration of neuron-level weight norms for the backdoored, pruned, and transported models during OTbased fusion.\n\nA practical example of the BadNets-attacked PreActResNet18 (He et al. 2016b) is illustrated in Figure 4. From the perspective of weight norm, the larger the difference in a neuron’s weight between the pruned and transported models, the more it is transported by the OT. We observe a consistent, slight decrease in the unpruned neurons that are intensively transported to some specific pruned neurons, resulting in their rapid recovery. This outcome reflects the effect of transporting from a uniform source distribution to a NWC-informed target distribution. By fusing the transported (green) and backdoored (blue) models, we can discriminately modify the neuron functionality, e.g., recovering more in low-NWC neurons, to effectively mitigate the backdoor effect while preserving high performance.\n\n# Experiment\n\n# Experimental Setup\n\nFor a fair comparison, all experiments, including the code implementation of our proposed method, are conducted using the default settings in BackdoorBench (Wu et al. 2022).\n\nDatasets. Similar to previous works (Zhu et al. 2023; Wei et al. 2023), our experiments are conducted on three benchmark datasets, including CIFAR-10 (Krizhevsky, Hinton et al. 2009), Tiny ImageNet (Le and Yang 2015), and CIFAR-100 (Krizhevsky, Hinton et al. 2009).\n\nAttack Setup. We evaluate the effectiveness of all defense methods using seven SOTA backdoor attacks: BadNets (Gu et al. 2019), Blended (Chen et al. 2017), Inputaware (Nguyen and Tran 2020), LF (Zeng et al. 2021b), SSBA (Li et al. 2021a), Trojan (Liu et al. 2018) and WaNet (Nguyen and Tran 2021). All attacks are conducted using the default settings in BackdoorBench (Wu et al. 2022). For example, we set the target label to 0, the poisoning ratio to $1 0 \\%$ , and the tested model to PreActResNet18 (He et al. 2016b).\n\nTable 1: Performance comparison with the SOTA defenses on CIFAR-10, Tiny ImageNet, and CIFAR-100 $( \\% )$ .   \n\n<html><body><table><tr><td rowspan=\"2\">Datasets</td><td rowspan=\"2\">Attacks</td><td colspan=\"2\">No Defense</td><td colspan=\"2\"></td><td colspan=\"8\">Data-Dependent</td><td colspan=\"3\">Data-Free OTBR</td></tr><tr><td></td><td>ASR</td><td>FP ACC</td><td>ASR</td><td>NAD ACC</td><td>ASR</td><td>ACC</td><td>ASR</td><td>i-BAU ACC</td><td>ASR</td><td>RNP ACC ASR</td><td>ACC</td><td>CLP ASR</td><td>ACC</td><td>ASR</td></tr><tr><td rowspan=\"8\">CIFAR-10</td><td>BadNets</td><td>ACC 91.32</td><td>95.03</td><td>91.31</td><td>57.13</td><td>89.87</td><td>2.14</td><td>90.94</td><td>5.91</td><td>89.15</td><td>1.21</td><td>89.81 24.97</td><td>90.06</td><td>77.50</td><td>90.11</td><td>1.08</td></tr><tr><td>Blended</td><td>93.47</td><td>99.92</td><td>93.17 99.26</td><td>92.17</td><td>97.69</td><td>93.00</td><td>84.90</td><td>87.00</td><td>50.53</td><td>88.76</td><td>79.74</td><td>91.32</td><td>99.74</td><td>92.01</td><td>1.64</td></tr><tr><td>Input-aware</td><td>90.67</td><td>98.26</td><td>91.74 0.04</td><td></td><td>93.18 1.68</td><td>91.04</td><td>1.32</td><td>89.17</td><td>27.08</td><td>90.52</td><td>1.84</td><td>90.30</td><td>2.17</td><td>86.52</td><td>0.37</td></tr><tr><td>LF</td><td>93.19</td><td>99.28</td><td>92.90 98.97</td><td>92.37</td><td>47.83</td><td>92.83</td><td>54.99</td><td>84.36</td><td>44.96</td><td>88.43</td><td>7.02</td><td>92.84</td><td>99.18</td><td>87.68</td><td>9.69</td></tr><tr><td>SSBA</td><td>92.88</td><td>97.86</td><td>92.54 83.50</td><td>91.91</td><td>77.40</td><td>92.67</td><td>60.16</td><td>87.67</td><td>3.97</td><td>88.60</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>93.42</td><td>100.00</td><td>92.46</td><td></td><td>91.88</td><td>92.97</td><td></td><td>90.37</td><td></td><td></td><td>17.89</td><td>91.38</td><td>68.13</td><td>85.27</td><td>9.54</td></tr><tr><td>Trojan WaNet</td><td>91.25</td><td></td><td>71.17</td><td></td><td>3.73</td><td></td><td>46.27 2.22</td><td>89.49</td><td>2.91 5.21</td><td>90.89</td><td>3.59</td><td>92.98</td><td>100.00</td><td>90.62</td><td>7.50</td></tr><tr><td></td><td>92.31</td><td>89.73 97.15</td><td>91.46 92.23</td><td>1.09 58.74</td><td>93.17 92.08</td><td>22.98</td><td>91.32</td><td></td><td></td><td></td><td>90.43 0.96</td><td>81.91</td><td>78.42</td><td>88.12</td><td>10.93</td></tr><tr><td rowspan=\"6\">Tiny ImageNet</td><td>Average</td><td>56.23</td><td></td><td></td><td></td><td>36.21</td><td>92.11</td><td>36.54</td><td>88.17</td><td>19.41</td><td>89.63</td><td>19.43</td><td>90.11</td><td>75.02</td><td>88.62</td><td>5.82</td></tr><tr><td>BadNets</td><td>57.45</td><td>100.00</td><td>51.73 99.99</td><td>46.37</td><td>0.27</td><td>50.55</td><td>7.74</td><td>51.48</td><td>97.36</td><td>21.91</td><td>0.00</td><td>55.94</td><td>100.00</td><td>54.13</td><td>0.00</td></tr><tr><td>Input-aware</td><td></td><td>98.85 55.28</td><td>62.92</td><td>47.91</td><td>1.86</td><td>53.17</td><td>0.17</td><td>52.48</td><td>72.98</td><td>15.57</td><td>0.00</td><td>57.75</td><td>99.58</td><td>51.40</td><td>0.02</td></tr><tr><td>SSBA</td><td>55.22</td><td>97.71</td><td>50.47 88.87</td><td></td><td>45.32 57.32</td><td>52.83</td><td>91.44</td><td>49.86</td><td>81.90</td><td>37.64</td><td>0.00</td><td>55.17</td><td>97.65</td><td>54.15</td><td>3.89</td></tr><tr><td>Trojan</td><td>55.89</td><td>99.98</td><td>50.22 8.82</td><td></td><td>48.48 0.83</td><td>50.37</td><td>1.40</td><td>52.65</td><td>98.49</td><td>46.27</td><td>0.00</td><td>55.86</td><td>8.39</td><td>53.85</td><td>0.14</td></tr><tr><td>WaNet</td><td>56.78</td><td>99.49 53.84</td><td>3.94</td><td></td><td>46.98 0.43</td><td>53.87</td><td>0.75</td><td>53.71</td><td>75.23</td><td>20.50</td><td>0.00</td><td>56.21</td><td>98.50</td><td>55.64</td><td>0.03</td></tr><tr><td rowspan=\"5\">CIFAR-100</td><td>Average</td><td>56.31</td><td>99.21</td><td>52.31</td><td>52.91</td><td>47.01</td><td>12.14 52.16</td><td>20.30</td><td>52.04</td><td>85.19</td><td>28.38</td><td>0.00</td><td>56.19</td><td>80.82</td><td>53.83</td><td>0.82</td></tr><tr><td>BadNets</td><td>67.22</td><td>87.43</td><td>64.55</td><td>0.42</td><td>66.37</td><td>0.06</td><td>63.65 0.00</td><td>60.37</td><td>0.04</td><td>55.68</td><td>0.00</td><td>65.40</td><td>81.95</td><td>66.81</td><td>0.00</td></tr><tr><td>Input-aware</td><td>65.24</td><td>98.61</td><td>67.82 2.34</td><td>69.25</td><td>31.11</td><td>58.99</td><td>0.00</td><td>65.21</td><td>85.14</td><td>55.66</td><td>0.01</td><td>65.22</td><td>99.81</td><td>59.64</td><td>4.21</td></tr><tr><td>SSBA</td><td>69.06</td><td>97.22</td><td>61.60 14.02</td><td>67.38</td><td>89.51</td><td>64.35</td><td>39.60</td><td>63.09</td><td>28.91</td><td>68.44</td><td>92.80</td><td>65.39</td><td>97.52</td><td>66.89</td><td>1.28</td></tr><tr><td>WaNet</td><td>64.04</td><td>97.72 68.07</td><td>10.29</td><td>68.46</td><td>0.55</td><td>60.05</td><td>0.05</td><td>65.31</td><td>43.96</td><td>49.48</td><td>0.00</td><td>25.90</td><td>83.49</td><td>62.91</td><td>8.18</td></tr><tr><td></td><td>Average</td><td>66.39 95.25</td><td>65.51</td><td>6.77</td><td>67.87</td><td>30.31</td><td>61.76</td><td>9.91</td><td>63.50</td><td>39.51</td><td>57.32</td><td>23.20</td><td>55.48</td><td>90.69</td><td>64.06</td><td>3.42</td></tr><tr><td></td><td>AverageACCDrop (smallerisbetter)</td><td></td><td>↓1.51</td><td></td><td>1</td><td>↓2.64</td><td>1 ↓2.55</td><td>-</td><td>↓3.87</td><td>1</td><td></td><td>↓12.17</td><td>↓3.73</td><td>1</td><td>↓2.97</td><td>，</td></tr><tr><td colspan=\"3\">Average ASR Drop (larger is better) Successful Defense Count</td><td>8/16</td><td>↓53.39</td><td></td><td>↓70.11 9/16</td><td></td><td>↓72.51 10/16</td><td></td><td>↓52.33 5/16</td><td>1</td><td>↓83.02 7/16</td><td>- 2/16</td><td>↓16.57</td><td>16/16</td><td>↓93.66</td></tr></table></body></html>\n\nDefense Setup. We compare our proposed OTBR method with six SOTA defense methods: Fine-pruning (FP) (Liu, Dolan-Gavitt, and Garg 2018), NAD (Li et al. 2021c), ANP (Wu and Wang 2021), i-BAU (Zeng et al. 2021a), RNP (Li et al. 2023b), and CLP (Zheng et al. 2022a). Note that only CLP can be conducted in a data-free manner similar to OTBR, while the other five defenses are all datadependent. Therefore, we follow the common setting in the post-training scenario that $5 \\%$ clean data is provided for those methods.\n\nEvaluation Metrics. We use two common metrics to evaluate performance: ACC and ASR. They measure the proportion of correct predictions on clean data (the higher, the better) and the rate of incorrect predictions for the target label on poisoned data (the lower, the better), respectively. A defense is usually considered successful against an attack if the ASR is reduced to below $20 \\%$ (Qi et al. 2023; Xie et al. 2024). In this paper, to consider both ACC and ASR, we consider a defense to be successful (marked with green in all tables) only if it achieves both of the following criteria: ACC decreases by less than $10 \\%$ and ASR falls below $20 \\%$ . Otherwise, it is considered unsuccessful. The best average results are boldfaced in all tables within this section.\n\n# Compared with Previous Works\n\nThe main defense performance, compared with the six baseline methods, is shown in Table 1. We observe that our OTBR successfully defends against all 16 attacks across three benchmark datasets, achieving the largest drop in average ASR $( 9 3 . 6 6 \\% )$ with an acceptable average ACC reduction $( 2 . 9 7 \\% )$ . Moreover, OTBR outperforms both datafree and data-dependent defenses, achieving the lowest average ASR on CIFAR-10 and CIFAR-100, and the secondbest ASR on Tiny ImageNet. Notably, the best ASR on Tiny ImageNet, achieved by RNP, comes with a significant drop in ACC. For the performances of baseline methods, we observe that the data-dependent approaches have clear advantages over the data-free CLP, which succeeds in only 2 out of 16 defenses. ANP achieves the best results with 10 out of 16 successful defenses; however, it consistently fails against SSBA attacks, a shortcoming also observed in NAD. Despite its failures on CIFAR-10, FP performs well on CIFAR-100 and consistently achieves high ACCs across all three datasets, validating the effectiveness of fine-tuning. i-BAU fails completely on Tiny ImageNet, exposing its limitations when dealing with different data complexities. Although RNP achieves good performance in ASR, it fails with significant drops in ACC on Tiny ImageNet. In contrast, OTBR performs the best, consistently achieving superior results across various attacks and datasets.\n\nTable 2: Comparison of different fusion schemes $( \\% ) . \\ \\mathbf { V } _ { 1 }$ : no fusion; $\\mathbf { V } _ { 2 }$ : vanilla fusion; ${ \\bf V } _ { 3 }$ : OT-based fusion.   \n\n<html><body><table><tr><td rowspan=\"2\">Attacks</td><td colspan=\"2\">V1</td><td colspan=\"2\">V2</td><td colspan=\"2\">V3 (Ours)</td></tr><tr><td>ACC</td><td>ASR</td><td>ACC</td><td>ASR</td><td>ACC</td><td>ASR</td></tr><tr><td>BadNets</td><td>84.98</td><td>1.56</td><td>91.3</td><td>81.13</td><td>90.11</td><td>1.08</td></tr><tr><td>Blended</td><td>69.08</td><td>4.71</td><td>92.06</td><td>16.76</td><td>92.01</td><td>1.64</td></tr><tr><td>LF</td><td>54.15</td><td>0.83</td><td>90.97</td><td>60.4</td><td>87.68</td><td>9.69</td></tr><tr><td>SSBA</td><td>40.58</td><td>0.02</td><td>89.84</td><td>40.57</td><td>85.27</td><td>9.54</td></tr></table></body></html>\n\n# Ablation Studies\n\nEffectivness of OT-based Model Fusion. To evaluate the effectiveness of OT-based model fusion, we keep Stage 1 unchanged and modify Stage 2 to generate three different versions for comparison. (1) $\\mathbf { V } _ { 1 }$ : no fusion is conducted in Stage 2; instead, we evaluate the performance of the pruned model from Stage 1; (2) $\\mathbf { V } _ { 2 }$ : implement vanilla fusion by directly fusing the pruned model with the backdoored model using equation (5); (3) ${ \\bf V } _ { 3 }$ (Ours): the final version, where the full procedures of both Stage 1 and 2 are conducted. Table 2 shows the performances of these three versions on CIFAR10 across four different attacks. The results validate the ef\n\n100(a) Impact of  and I on ACC 100(b) Impact of  and I on ASR 100 (c) Impact of 100 (d) Impact of G 80 80 80 80   \n460 ACC-I-520 ASR (%) 460 ASR-I-520 60 ASCRC 460 ASCRC ACC-I-40 ASR-I-80 A 20 20 20 20 ACC-I-80 0 1 5 10 15 20 25 30 0 10 15 20 25 30 00.10.20.30.4 0.6 0.8 1.0 0 1 2 3 4 5 6 7 8 9 Pruning Ratio  (%) Pruning Ratio  (%) Balance Coefficient Largest Label G\n\n![](images/f0274778e7cbb19b187d027f43925c5357eba9c34b5b5953ef9a576a335f4c41.jpg)  \nFigure 5: Impact of different factors on performance. (a) and (b) show the impact of $\\gamma$ and $I$ on ACC and ASR, respectively, with “ACC-I-5” representing ACC when $I = 5$ ; (c) shows the impact of $\\lambda$ ; and (d) shows the impact of $G$ .   \nFigure 6: Comparison of different OT distributions. $\\mathtt { u 2 u }$ : uniform to uniform transport; $\\mathbf { u } 2 \\mathbf { r } .$ : uniform to random transport; $\\mathbf { u } 2 \\mathbf { n } ( \\mathbf { o u r s } )$ : uniform to NWC transport.\n\nfectiveness of aligning neuron weights using OT, where the pruned model inherently achieves a low ASR (or even better) while the high ACC is kept. Although the vanilla fusion $( \\mathbf { V } _ { 2 } )$ better preserves ACC, it fails in effectively mitigating the backdoor effect.\n\nEffectiveness of NWC-informed OT. To verify the important role of NWC-informed OT in achieving optimal defense performance, we fix the source distribution as a uniform distribution and compare three different initialization schemes for the target distribution in OT. Specifically, we consider three types of distributions: uniform distribution, random distribution, and NWC distribution. These schemes are labeled as $^ { \\mathrm { \\scriptsize { e } } \\mathfrak { e } } \\mathrm { u } 2 \\mathrm { u } ^ { \\mathrm { \\tiny { , } \\mathfrak { e } } }$ , “u2r”, and ”u2n”(Ours), respectively. The results are presented in Figure 6. We observe that only $^ { \\mathrm { { e } } } \\mathrm { { u } } 2 \\mathrm { { n } } ^ { \\mathrm { { , } } }$ , i.e., NWC-informed OT, consistently achieves strong performance across different attacks. It can be explained w.r.t. the weight transport as in Figure 4. In contrast, since uniform and random distributions are unrelated to the backdoor functionality, they fail to effectively guide neuron weight transport, resulting in poor fusion performance.\n\n# Parameter Analysis\n\nImpact of Different Factors. We aim to investigate the impact of various factors on the performance of OTBR. These factors include the number of iterative steps $I$ , the pruning ratio $\\gamma$ , the largest label $G$ , and the balance coefficient $\\lambda$ . The experiments are conducted using default settings on CIFAR-10 and BadNets with a $10 \\%$ poisoning ratio. The results are shown in Figure 5. Firstly, in subfigures (a) and (b), we present the results of varying the pruning ratio $\\gamma$ from $1 \\%$ to $30 \\%$ across four numbers of iterative steps $I$ (5, 20, 40, and 80 steps). The two subfigures, showing ACC and ASR respectively, demonstrate that OTBR performs well across different settings. A larger pruning ratio tends to require more iterative steps for effective random unlearning. In our setting $\\left( I \\right) = 2 0 ,$ ), $\\gamma$ is insensitive in the range of $5 \\%$ to $2 5 \\%$ , resulting in successful defense. Secondly, in subfigure (c), we show the impact of the balance coefficient $\\lambda$ ranging from 0.1 to 1.0. A higher value of $\\lambda$ means a more important role the transported model plays in the fusion. We observe that there exists a trade-off between the high ACC from the backdoored model and the low ASR from the transported model, as we assumed before. It suggests setting the $\\lambda$ between 0.4 and 0.8 for a successful defense. Lastly, in subfigure (d), we evaluate the performances with different largest labels $G$ for random data generation to test the impact of class number. Note that 9 is the largest label, in which case the model is trained. The results reveal that performance remains consistently good across $G$ values from 4 to 9, while a smaller class number may fail. Overall, our OTBR proves to be a robust defense method across various hyperparameter settings.\n\n# Conclusion\n\nIn this work, we propose a novel data-free backdoor defense method, OTBR, using OT-based model fusion. Notably, we provide a new data-free pruning insight by revealing the positive correlation between NWCs when unlearning random noise and poisoned data. This insight enables us to effectively eliminate the backdoor effect using pruning guided by NWCs in a data-free manner. Then, we propose to combine the high ACC of the backdoored model with the low ASR of the pruned model using the OT-based model fusion. Furthermore, we provide possible explanations for the success of both NWC pruning and OT-based fusion. Extensive experiments across various attacks and datasets confirm the effectiveness of our OTBR method. A current limitation of this work is its reliance on NWC, which applies only to the post-training scenario. In future work, we plan to explore the potential of OT-based model fusion for more scenarios.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文针对深度神经网络（DNNs）中的后门攻击问题，提出了一种无需数据的防御方法。后门攻击通过在训练数据中植入隐蔽的触发器，使得模型在特定输入下表现异常，而现有防御方法大多依赖干净或中毒数据，限制了其在实际场景中的应用。\\n> *   该问题的重要性在于，无需数据的防御方法可以广泛应用于第三方预训练模型的安全验证，尤其是在数据不可获取或隐私敏感的场景中。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种基于最优传输（Optimal Transport, OT）的模型融合方法（OTBR），通过结合剪枝模型和带后门模型的优势，实现高清洁准确率（ACC）和低攻击成功率（ASR）。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **创新贡献点1：** 揭示了随机遗忘（random unlearning）与中毒遗忘（poison unlearning）的神经元权重变化（NWCs）之间的正相关性，为无需数据的剪枝提供了新思路。\\n> *   **创新贡献点2：** 首次将最优传输和模型融合技术引入后门防御领域，提出了一种两阶段防御策略：随机遗忘NWC剪枝和剪枝-后门OT融合。\\n> *   **创新贡献点3：** 提出NWC-informed OT，在融合过程中优先传输与后门相关的神经元权重，从而更有效地稀释后门效应。\\n> *   **效果：** 在三个基准数据集（CIFAR-10、Tiny ImageNet、CIFAR-100）上，OTBR成功防御了所有七种后门攻击，平均ASR降低93.66%，同时ACC仅下降2.97%。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   通过随机遗忘生成的噪声数据计算NWCs，识别并剪枝与后门相关的神经元，消除后门效应。随后，利用OT将剪枝模型的清洁功能与后门模型的高ACC融合，稀释后门效应。\\n> *   **设计哲学：** 剪枝阶段通过NWCs定位后门神经元，OT融合阶段通过权重对齐和平均操作保留清洁性能。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 传统剪枝方法依赖后续微调（需干净数据），而OTBR完全无需数据。\\n> *   **本文的改进：** 提出NWC-informed OT，在融合过程中优先传输与后门相关的神经元权重，从而更有效地稀释后门效应。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **随机遗忘NWC剪枝：** 生成随机噪声数据，通过随机遗忘计算NWCs，剪枝排名前γ的神经元。\\n> 2.  **剪枝-后门OT融合：** 使用NWC-informed OT对齐剪枝模型和后门模型的权重，通过加权平均融合两者。\\n> *   **关键公式：**  \\n>     - NWC定义：$\\\\mathrm{NWC}^{(l)j} \\\\stackrel{\\\\mathrm{def}}{=} \\\\| \\\\pmb{\\\\theta}_{ul}^{(l)j} - \\\\pmb{\\\\theta}_{bd}^{(l)j} \\\\|_1$\\n>     - OT问题：$\\\\mathrm{OT}(\\\\mu, \\\\nu; C) \\\\stackrel{\\\\mathrm{def}}{=} \\\\operatorname*{min} \\\\langle \\\\mathbf{T}, C \\\\rangle$\\n>     - 融合公式：$\\\\pmb{\\\\theta}^{*} \\\\stackrel{\\\\mathrm{def}}{=} \\\\lambda \\\\widetilde{\\\\pmb{\\\\theta}}_{pn} + (1-\\\\lambda) \\\\pmb{\\\\theta}_{bd}$\\n\\n> **案例解析 (Case Study)**\\n> *   论文通过BadNets攻击的PreActResNet18模型展示了OT融合的效果。从权重范数的角度看，OT融合使得剪枝模型中未被剪枝的神经元的权重范数略有下降，而特定被剪枝的神经元的权重范数迅速恢复，从而有效稀释了后门效应。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   Fine-pruning (FP)、NAD、ANP、i-BAU、RNP、CLP。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在攻击成功率（ASR）上：** OTBR在CIFAR-10上平均ASR降至5.82%，显著优于数据依赖方法ANP（36.54%）和数据无关方法CLP（75.02%）。与表现最佳的基线RNP相比，ASR降低了70.11个百分点。\\n> *   **在清洁准确率（ACC）上：** OTBR在CIFAR-100上ACC为64.06%，仅下降3.42%，优于FP（↓12.17%）和CLP（↓16.57%）。\\n> *   **在防御成功率上：** OTBR成功防御了所有16次攻击（100%），而最佳数据依赖基线ANP仅成功10次（62.5%）。\\n> *   **在Tiny ImageNet上：** OTBR的ASR为0.82%，仅次于RNP（0.00%），但RNP的ACC下降显著（↓85.19%），而OTBR的ACC仅下降0.82%。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   后门攻击 (Backdoor Attack, N/A)\\n*   深度神经网络 (Deep Neural Network, DNN)\\n*   最优传输 (Optimal Transport, OT)\\n*   模型融合 (Model Fusion, N/A)\\n*   神经元权重变化 (Neuron Weight Changes, NWCs)\\n*   数据无关防御 (Data-free Defense, N/A)\\n*   剪枝 (Pruning, N/A)\\n*   随机遗忘 (Random Unlearning, N/A)\"\n}\n```"
}