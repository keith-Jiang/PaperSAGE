{
    "source": "Semantic Scholar",
    "arxiv_id": "2407.15820",
    "link": "https://arxiv.org/abs/2407.15820",
    "pdf_link": "https://arxiv.org/pdf/2407.15820.pdf",
    "title": "On shallow planning under partial observability",
    "authors": [
        "Randy Lefebvre",
        "Audrey Durand"
    ],
    "categories": [
        "cs.AI"
    ],
    "publication_date": "2024-07-22",
    "venue": "AAAI Conference on Artificial Intelligence",
    "fields_of_study": [
        "Computer Science"
    ],
    "citation_count": 1,
    "influential_citation_count": 1,
    "institutions": [
        "Universite¬¥ Laval",
        "Canada CIFAR AI Chair"
    ],
    "paper_content": "# On Shallow Planning Under Partial Observability\n\nRandy Lefebvre1, Audrey Durand1,2\n\n1Universite¬¥ Laval 2Canada CIFAR AI Chair randy.lefebvre.1@ulaval.ca, audrey.durand@ift.ulaval.ca\n\n# Abstract\n\nFormulating a real-world problem under the Reinforcement Learning framework involves non-trivial design choices, such as selecting a discount factor for the learning objective (discounted cumulative rewards), which articulates the planning horizon of the agent. This work investigates the impact of the discount factor on the bias-variance trade-off given structural parameters of the underlying Markov Decision Process. Our results support the idea that a shorter planning horizon might be beneficial, especially under partial observability.\n\n# Introduction\n\nReinforcement Learning (RL) has had tremendous success on Atari games (Mnih et al. 2013), yet applications of RL in the real-world remain limited (Dulac-Arnold et al. 2021). This complexity is due to many challenges such as sample efficiency of RL methods (Yu 2018), risk/safety issues (Gu et al. 2023) and partial observability (Sondik 1978; Francois-Lavet et al. 2019; Kaelbling, Littman, and Cassandra 1998). Formulating a real-world problem under the RL framework also involves several non-trivial decisions such as selecting a state/action space (especially when these are continuous), formulating a reward function, and formulating the learning objective (Hare 2019; Devidze et al. 2021). The learning objective usually corresponds to the discounted cumulative rewards, which depends on a discount factor articulating the considered planning horizon when attributing values to states and actions (Sutton and Barto 2018). This objective is useful since it can reduce the search space intuitively by giving less credit to future rewards and actions. In toy and simulated environments, early practitioners tend to use large discount factor values often found in the RL literature on Atari (Kaiser et al. 2024; Mnih et al. 2013). This equates to considering very long planning horizons. On the other hand, real-world applications tend to formulate sequential decision-making problems under the contextual bandit setting (i.e. with a myopic agent w.r.t. the planning horizon) in response to low data regimes (Bastani and Bayati 2020; Ding, Li, and Liu 2019; Durand et al. 2018).\n\nThe impact of reducing the planning horizon has been studied previously, and bounds on the resulting biasvariance trade-off on the state value functions have been proposed (Amit, Meir, and Ciosek 2020; Jiang et al. 2015). Unfortunately, these results provide loose bounds that do not consider the structure of the underlying Markov Decision Process (MDP) and thus fail to capture its impact on the optimal planning horizon. The optimal planning horizon can be defined using the discount factor $\\gamma$ which minimizes the planning loss (see Eq. 2), i.e. the one which can extract the best policy possible considering the limited amount of data. Distinct results involving the structure of MDPs (Jiang, Singh, and Tewari 2016; Gheshlaghi Azar, Munos, and Kappen 2013; Wu, He, and Gu 2023; He, Zhou, and Gu 2021) have been achieved separately, but these insights have never been brought together.\n\nContributions In this work, we introduce new results on the bias-variance trade-off that explicitly depend on highlevel structural parameters of the underlying MDP. More importantly, our results touch on Partially Observable MDPs (POMDPs), providing the first insights supporting the advantage of considering short horizons in the learning objective for practical applications under partial observability. We support and illustrate the theory with experiments, hoping that this can open the door to choices in learning objectives that are better adapted to real-world RL applications. Finally, we provide the open-source code1 for all our experiments to ensure reproducibility and offer a framework that practitioners can modify to better understand the impact of partial observability on their specific applications.\n\n# Fully Observable Setting\n\nAn MDP can be described as a tuple $( \\mathcal { S } , \\mathcal { A } , P , R )$ , where $s$ is a finite state space, $\\mathcal { A }$ is a finite action space, $P :$ $s \\times \\mathcal { A } \\times \\mathcal { S } \\mapsto [ 0 , 1 ]$ is a transition function, and $R :$ $S \\times \\mathcal { A } \\mapsto [ 0 , R _ { \\mathrm { { m a x } } } ]$ is a reward function, with $R _ { \\mathrm { m a x } }$ denoting the maximal reward obtainable in the MDP. On each time step $t \\in  { \\mathbb { N } } _ { 0 }$ , the current state $S _ { t } \\in \\mathcal S$ is observed, an action $A _ { t } \\in { \\mathcal { A } }$ is played, the environment transitions into next state $S _ { t + 1 }$ (using $P$ ) and generates an observed reward $R _ { t + 1 }$ (using $R$ ). Given an MDP (tuple) $M$ , the value of state $s \\in { \\mathcal { S } }$ under policy $\\pi : { \\mathcal { S } } \\mapsto { \\mathcal { A } }$ is the expected sum of discounted rewards obtained by selecting actions according to policy $\\pi$\n\nfrom state $s$ :\n\n$$\nV _ { M , \\gamma } ^ { \\pi } ( s ) = \\mathbb { E } _ { \\pi } \\left[ \\sum _ { k = 0 } ^ { \\infty } \\gamma ^ { k } R _ { t + k + 1 } | S _ { t } = s \\right] ,\n$$\n\nwhere the discount factor $\\gamma \\in [ 0 , 1 ]$ controls the planning horizon $1 / ( 1 - \\gamma )$ , i.e. the credit assigned to action $A _ { t }$ for future rewards. The goal of a learning agent is to find the optimal policy $\\pi _ { M , \\gamma } ^ { * }$ that maximizes $V _ { M , \\gamma } ^ { \\pi } ( s )$ for all states . We consider a setting where this policy is unique. We use $V _ { M , \\gamma } ^ { \\pi } \\in \\mathbb { R } ^ { | S | }$ to denote the vector of state values. A notation table can be found in Appendix A.\n\nBlackwell Discount Factor Practitioners often believe using a higher discount factor will result in a better policy. While this is true with an infinite amount of data, it is rarely the case when using RL in practice. It has even been shown such that for any previously that there always exists a discount factor $\\gamma \\geq \\gamma _ { B w }$ , we have = VMœÄ,MŒ≥,Œ≥Bw when $\\vert s \\vert < \\infty$ and $| { \\mathcal { A } } | < \\infty$ (Grand-Cle¬¥ment and Petrik (2023), Thm. 3.2). We refer to $\\gamma _ { B w }$ as the Blackwell discount factor and we say that optimal policies with discount factor $\\gamma \\geq \\gamma _ { B w }$ are optimal according to the Blackwell optimality criterion. This concept closely resembles the idea of effective planning horizon by Laidlaw, Russell, and Dragan (2023), with the effective horizon defined as $1 / ( 1 - \\hat { \\gamma _ { B w } } )$ instead of a number of look ahead steps.\n\nPlanning Loss The planning loss captures the impact of using $\\gamma < \\gamma _ { B w }$ given that the planning is performed in an approximate model of the environment2 $\\hat { M }$ with $\\widehat { M } \\approx M$ :\n\n$$\n\\begin{array} { r l } & { \\| \\boldsymbol { V } _ { M , \\gamma _ { B w } } ^ { \\pi _ { M , \\gamma _ { B w } } ^ { * } } - \\boldsymbol { V } _ { M , \\gamma _ { B w } } ^ { \\pi _ { \\bar { M } , \\gamma } ^ { * } } \\| _ { \\infty } } \\\\ & { = \\| \\boldsymbol { V } _ { M , \\gamma _ { B w } } ^ { \\pi _ { M , \\gamma _ { B w } } ^ { * } } - \\boldsymbol { V } _ { M , \\gamma _ { B w } } ^ { \\pi _ { M , \\gamma } ^ { * } } + \\boldsymbol { V } _ { M , \\gamma _ { B w } } ^ { \\pi _ { M , \\gamma } ^ { * } } - \\boldsymbol { V } _ { M , \\gamma _ { B w } } ^ { \\pi _ { \\bar { M } , \\gamma } ^ { * } } \\| _ { \\infty } } \\\\ & { \\leq \\underbrace { \\big | \\big | \\boldsymbol { V } _ { M , \\gamma _ { B w } } ^ { \\pi _ { M , \\gamma _ { B w } } ^ { * } } - \\boldsymbol { V } _ { M , \\gamma _ { B w } } ^ { \\pi _ { M , \\gamma } ^ { * } } \\big | _ { \\infty } } _ { \\mathrm { b i a s } } + \\underbrace { \\big | \\big | \\boldsymbol { V } _ { M , \\gamma _ { B w } } ^ { \\pi _ { M , \\gamma } ^ { * } } - \\boldsymbol { V } _ { M , \\gamma _ { B w } } ^ { \\pi _ { \\bar { M } , \\gamma } ^ { * } } \\big | _ { \\infty } } _ { \\mathrm { v a r i a n c e } } . } \\end{array}\n$$\n\nThis decomposition offers insight into two components which can affect the quality of the policy obtained when planning on an approximate model of the environment using a shallow planning horizon $( \\gamma < \\gamma _ { B w } )$ . The bias denotes the loss in value function (evaluated on the true MDP $M$ and with the Blackwell planning horizon) when using a policy that is optimal with a shallow planning horizon instead of using a policy that is optimal with $\\gamma _ { B w }$ . On the other hand, the variance captures the impact of optimizing the policy under an approximate model $\\hat { M }$ with a shallow planning horizon and will tend to 0 with more data. This decomposition is different from previous work (Jiang et al. 2015) and has the advantage of being interpretable as a bias-variance trade-off from the PAC-learning literature. We can compare the bias to the approximation error and the variance to the estimation error (Shalev-Shwartz and Ben-David 2014).\n\nStructural parameters have been introduced previously to characterize the difficulty of an MDP.\n\nDefinition 1 (Value-function variation (Jiang, Singh, and Tewari 2016)). Given an MDP M and discount factor $\\gamma _ { ; }$ , the value function variation captures the largest difference between the values of two different states when following the optimal policy:\n\n$$\n\\kappa _ { M , \\gamma } = \\operatorname* { m a x } _ { s , s ^ { \\prime } \\in S } \\left. V _ { M , \\gamma } ^ { \\pi _ { M , \\gamma } ^ { * } } ( s ) - V _ { M , \\gamma } ^ { \\pi _ { M , \\gamma } ^ { * } } ( s ^ { \\prime } ) \\right. \\leq \\frac { R _ { m a x } } { 1 - \\gamma } .\n$$\n\nNote that the value-function is evaluated with the same discount factor as used by the policy.\n\nThe value-function variation provides insight on the impact of starting in certain states over others in the MDP. A low value indicates that the starting state is not important to consider for predicting future rewards (Jiang, Singh, and Tewari 2016).\n\nDefinition 2 (Action variation (Jiang, Singh, and Tewari 2016)). Given an MDP M with transition probabilities $P$ , the action variation captures how much actions can impact state transitions:\n\n$$\n\\delta _ { M } = \\operatorname* { m a x } _ { s \\in { \\cal S } } \\operatorname* { m a x } _ { a , a ^ { \\prime } \\in { \\cal A } } \\| P ( \\cdot | s , a ) - P ( \\cdot | s , a ^ { \\prime } ) \\| _ { 1 } .\n$$\n\nIf the action variation is equal to 0, the agent cannot influence the state transitions (and therefore future rewards). In this case, we would expect the problem to be safely (and efficiently) formulated under the contextual bandit setting, which corresponds to using a myopic agent $( \\gamma = 0 )$ ). The maximal value for this $L _ { 1 }$ distance is 2, which often happens under deterministic settings. This is validated by our numerical experiments below.\n\nUsing Definitions 1 and 2, the following bound on the bias was introduced (Jiang, Singh, and Tewari 2016):\n\n$$\n| | { \\cal V } _ { M , \\gamma _ { B w } } ^ { \\pi _ { M , \\gamma _ { B w } } ^ { * } } - { \\cal V } _ { M , \\gamma _ { B w } } ^ { \\pi _ { M , \\gamma } ^ { * } } | | _ { \\infty } \\leq \\frac { \\delta _ { M } / 2 \\cdot \\kappa _ { M , \\gamma } ( \\gamma _ { B w } - \\gamma ) } { ( 1 - \\gamma _ { B w } ) ( 1 - \\gamma _ { B w } ( 1 - \\delta _ { M } / 2 ) ) } .\n$$\n\nUnfortunately, the action variation is not sensitive to the planning horizon of the agent compared with the Blackwell planning horizon. Moreover, unlike the bias, there is no current bounds for the variance. We address these limitations with Lem. 1 and Eq. 3, which will allow us to obtain a new bound on the planning loss (upcoming Thm. 1).\n\n# Improving the Bias Bound\n\nIn order to consider the planning horizon in the existing bias bound (Eq. 3), we introduce the following definitions:\n\nDefinition 3 (Discordant state-action pairs). The set of stateaction pairs in an MDP M where policies $\\pi$ and $\\pi ^ { \\prime }$ differ:\n\n$$\n\\mathcal { Z } _ { M } ( \\pi \\neq \\pi ^ { \\prime } ) = \\{ ( s , a ) \\in \\mathcal { S } \\times \\mathcal { A } : \\pi ( s ) \\neq \\pi ^ { \\prime } ( s ) , \\pi ^ { \\prime } ( s ) = a \\} .\n$$\n\nThis new set will be used to capture the impact of a shallow-horizon policy on the action variation:\n\nDefinition 4 (Horizon-sensitive action variation). The most important difference in transition probabilities induced by using discount factor $\\gamma$ instead of $\\gamma _ { B w }$ on an MDP $M$ :\n\n$$\n\\delta _ { M , \\gamma } = \\operatorname* { m a x } _ { \\substack { ( s , a ) \\in \\mathcal { Z } _ { M } ( \\pi _ { M , \\gamma } ^ { * } \\neq \\pi _ { M , \\gamma B w } ^ { * } ) } } \\Vert P ( \\cdot | s , \\pi _ { M , \\gamma } ^ { * } ( s ) ) - P ( \\cdot | s , a ) \\Vert _ { 1 } .\n$$\n\nThe implementation of the action variations in proofs is to bound the difference in transition probabilities between two different policies. The highest possible bound is given by prior results (Def. 2), but we tighten this result by simply considering states for which the policies are unequal instead of all states. This has the benefit of being 0 when the policy is evaluated with a discount factor above the Blackwell. By building on previous analysis (Jiang, Singh, and Tewari 2016), we can obtain the following result to characterize the impact of optimizing the policy with a shallow horizon on $k$ -steps transition probabilities.\n\nProposition 1 (Horizon-sensitive transition probabilities distance). Given an MDP M, let PsœÄ,k denote the vector of the transition probabilities from state $s \\in { \\mathcal { S } }$ to every possible states when following policy $\\pi$ for $k \\geq 1$ time steps. The transition probabilities when following the policy that is optimal for a shallow planning horizon $( \\gamma < \\gamma _ { B w } )$ instead of following the policy that is optimal for the Blackwell planning horizon is bounded by:\n\n$$\n\\| P _ { s , k } ^ { \\pi _ { M , \\gamma } ^ { * } } - P _ { s , k } ^ { \\pi _ { M , \\gamma _ { B w } } ^ { * } } \\| _ { 1 } \\leq 2 - 2 ( 1 - \\delta _ { M , \\gamma } / 2 ) ^ { k } .\n$$\n\nProp. 1 can be used to extend Eq. 3 using the horizonsensitive structural parameters:\n\n$$\n\\begin{array} { r l } & { | | \\underbrace { V _ { M , \\gamma _ { B w } } ^ { \\pi _ { M , \\gamma _ { B w } } ^ { * } } - V _ { M , \\gamma _ { B w } } ^ { \\pi _ { M , \\gamma } ^ { * } } } _ { \\mathrm { b i a s } } | | _ { \\infty } \\leq } \\\\ & { \\qquad \\frac { \\delta _ { M , \\gamma } / 2 \\cdot \\kappa _ { M , \\gamma } ( \\gamma _ { B w } - \\gamma ) } { ( 1 - \\gamma _ { B w } ) \\left( 1 - \\gamma _ { B w } ( 1 - \\delta _ { M , \\gamma } / 2 ) \\right) } , } \\end{array}\n$$\n\nwhere we use the term $\\delta _ { M , \\gamma }$ (instead of $\\delta _ { M }$ ) to capture the divergence between the shallow and optimal-horizon policies. Since the set offered by Def. 3 is smaller than the set of all state-action pairs, Eq. 4 is tighter than Eq. 3. See Appendix $\\mathbf { B }$ for the complete proofs.\n\n# Controlling the Variance\n\nWe now introduce new definitions and results to bound the variance (in Eq. 2). Recall that the variance captures the impact of learning an optimal policy on an (empirical) approximation $\\hat { M }$ of a true MDP $M$ when using a shallow planning horizon $( \\gamma < \\gamma _ { B w } )$ . To this end, it is convenient to isolate the variance that does not depend on the shallow planning.\n\nDefinition 5 (Variance due to model approximation). The maximum difference in value-function due to the approximate model $\\hat { M }$ :\n\n$$\n\\begin{array} { r } { \\hat { \\epsilon } = \\| V _ { M , \\gamma } ^ { \\pi _ { M , \\gamma } ^ { * } } - V _ { M , \\gamma } ^ { \\pi _ { \\widehat { M } , \\gamma } ^ { * } } \\| _ { \\infty } . } \\end{array}\n$$\n\nThis term can be upper-bounded into the following results by using known settings in the PAC literature (Gheshlaghi Azar, Munos, and Kappen 2013; Wu, He, and Gu 2023;\n\nHe, Zhou, and $\\mathrm { G u } \\ 2 0 2 1 \\$ ). We can also use the discordant state-action pairs (Def. 3) to capture the action variation resulting from having optimized the policy on an approximation $\\hat { M }$ of a true MDP $M$ .\n\nDefinition 6 (Empirical action variation). The most important difference in transition probabilities when following the policy optimal on an MDP $M$ vs the policy optimal on an approximate model MÀÜ:\n\n$$\n\\hat { \\delta } _ { M , \\gamma } = \\operatorname* { m a x } _ { \\substack { ( s , a ) \\in \\mathcal { Z } _ { M } ( \\pi _ { M , \\gamma } ^ { * } \\neq \\pi _ { \\hat { M } , \\gamma } ^ { * } ) } } \\left\\| P ( \\cdot | s , \\pi _ { M , \\gamma } ^ { * } ( s ) ) - P ( \\cdot | s , a ) \\right\\| _ { 1 } .\n$$\n\nThe improvement over the action variation (Def. 2) is that it will tend towards 0 as $\\hat { M } \\approx M$ which is desirable in a bound on the variance. As was done previously in Definition 4, we can also build on previous analysis (Jiang, Singh, and Tewari 2016) to obtain the following result to characterize the impact of optimizing the policy with an approximate model $\\hat { M }$ on $k$ -steps transition probabilities.\n\nProposition 2 (Empirical transition probabilities distance). Given an MDP M and an approximate model $\\hat { M }$ . Let $P _ { s , k } ^ { \\pi }$ denote the vector of the transition probabilities from state $s \\in { \\mathcal { S } }$ to every possible states when following policy $\\pi$ for $k \\geq 1$ time steps. For a given planning horizon, the transition probabilities when following the optimal policy on MÀÜ instead of following the optimal policy on $M$ is bounded by:\n\n$$\n\\| P _ { s , k } ^ { \\pi _ { M , \\gamma } ^ { * } } - P _ { s , k } ^ { \\pi _ { \\hat { M } , \\gamma } ^ { * } } \\| _ { 1 } \\leq 2 - 2 ( 1 - \\hat { \\delta } _ { M , \\gamma } / 2 ) ^ { k } .\n$$\n\nProp. 2 can be used with Def. 3 and 6 to obtain the following bound on the variance (see Appendix C).\n\nLemma 1 (Variance). Consider optimal policies computed with a shallow planning horizon $( \\gamma < \\gamma _ { B w } )$ on an MDP $M$ and an approximate model $\\hat { M }$ . The difference between their value-functions evaluated on $M$ with discount factor $\\gamma _ { B w }$ is bounded by:\n\n$$\n\\begin{array} { r l } & { \\| V _ { M , \\gamma _ { B w } } ^ { \\pi _ { M , \\gamma } ^ { * } } - V _ { M , \\gamma _ { B w } } ^ { \\pi _ { \\bar { M } , \\gamma } ^ { * } } \\| _ { \\infty } } \\\\ & { \\leq \\hat { \\epsilon } \\left( \\displaystyle \\frac { 1 - \\gamma } { 1 - \\gamma _ { B w } } \\right) + \\frac { \\hat { \\delta } _ { M , \\gamma } / 2 \\cdot \\kappa _ { M , \\gamma } ( \\gamma _ { B w } - \\gamma ) } { ( 1 - \\gamma _ { B w } ) \\left( 1 - \\gamma _ { B w } \\left( 1 - \\hat { \\delta } _ { M , \\gamma } / 2 \\right) \\right) } . } \\end{array}\n$$\n\nThis bound is interesting because it becomes tighter when the empirical action variation $\\hat { \\delta } _ { M , \\gamma }$ or the value function variation $\\kappa _ { M , \\gamma }$ decrease. We can then deduce that a problem with a low value in these structural parameters lowers both the bias (Eq. 4) and the variance. Finally, the use of the empirical action variation (Def. 6) gives rise to a bound that is coherent in convergence, as it will tend towards 0 as the amount of data increases.\n\n# A New Bound on the Planning Loss\n\nBy combining the extended bias bound (Eq. 4) with our novel bound on the variance (Lem. 1), we obtain the following bound on the planning loss (Eq. 2). See Appendix D for the complete proof.\n\n![](images/a5e84603342ac5ecbe60df0ceaa07a5030d26260eef26ec28bc0208b38e75fe2.jpg)  \nFigure 1: Proportion of randomly sampled MDPs where Eq. 5 is true given a discount factor $\\gamma$ .\n\nTheorem 1 (Planning loss). Given an MDP $M$ , its Blackwell discount factor $\\gamma _ { B w }$ , and an approximate model $\\hat { M }$ . The planning loss is bounded by:\n\n$$\n\\begin{array} { r l } & { \\| V _ { M , \\gamma _ { B w } } ^ { \\pi _ { M , \\gamma _ { B w } } ^ { * } } - V _ { M , \\gamma _ { B w } } ^ { \\pi _ { M , \\gamma } ^ { * } } \\| _ { \\infty } } \\\\ & { \\leq \\kappa _ { M , \\gamma } \\left( \\frac { \\gamma _ { B w } - \\gamma } { 1 - \\gamma _ { B w } } \\right) \\left( \\frac { \\delta _ { M , \\gamma } / 2 } { 1 - \\gamma _ { B w } \\left( 1 - \\delta _ { M , \\gamma } / 2 \\right) } \\right) } \\\\ & { \\quad + \\kappa _ { M , \\gamma } \\left( \\frac { \\gamma _ { B w } - \\gamma } { 1 - \\gamma _ { B w } } \\right) \\left( \\frac { \\delta _ { M , \\gamma } / 2 } { 1 - \\gamma _ { B w } \\left( 1 - \\delta _ { M , \\gamma } / 2 \\right) } \\right) } \\\\ & { \\quad + \\hat { \\epsilon } \\left( \\frac { 1 - \\gamma } { 1 - \\gamma _ { B w } } \\right) . } \\end{array}\n$$\n\nThis result provides insight into how structural parameters affect not only the bias, but also the variance. For instance, a problem with action variation $\\delta _ { M } \\approx 0$ has low variance due to the limited impact of the policy over the state value (agent actions do not impact transition probabilities). Similarly to prior work (Jiang et al. 2015), although the applicability of this result is limited by not having access to the true model $M$ , it remains a helpful guide to design heuristics and better understand how one could decide a discount factor. For example, it justifies framing recommender systems as contextual bandits when the outcome of future recommendations do not depend on current recommendations, which translates into a low value-function variation $\\kappa _ { M , \\gamma }$ . Thm. 1 is tighter than the current existing bound (Jiang et al. 2015) under the following condition on the quality of the model approximation $\\hat { M }$ (see Appendix E):\n\n$$\n\\begin{array} { r l } & { \\hat { \\epsilon } \\leq \\displaystyle \\frac { R _ { m a x } } { 1 - \\gamma } - \\kappa _ { M , \\gamma } \\bigg ( \\frac { \\delta _ { M , \\gamma } / 2 } { 1 - \\gamma _ { B w } \\left( 1 - \\delta _ { M , \\gamma } / 2 \\right) } } \\\\ & { \\quad \\quad \\quad \\quad \\quad + \\frac { \\hat { \\delta } _ { M , \\gamma } / 2 } { 1 - \\gamma _ { B w } \\left( 1 - \\hat { \\delta } _ { M , \\gamma } \\right) } \\bigg ) . } \\end{array}\n$$\n\nFig. 1 supports the idea that Thm. 1 becomes tighter than prior results when the variance due to model approximation (Def. 5) is low or when $\\frac { R _ { m a x } } { 1 - \\gamma }$ is dominant ( $\\langle \\gamma$ close to 1).\n\n# Bias Under Partial Observability\n\nWe now look at how partial observability impacts the structural parameters to better understand its impact on the bias.\n\nThis is important since most practical problems suffer from a form of partial observability (Dulac-Arnold et al. 2021). We consider a discrete-time POMDP (Sondik 1978) described by the MDP tuple extended with two elements: a finite set of possible observations $\\Omega$ and the probabilities of receiving each observation given a state, $\\mathbb { O } : \\mathcal { S } \\times \\Omega \\mapsto [ 0 , 1 ]$ . On each time step $t \\in  { \\mathbb { N } } _ { 0 }$ , the current state $S _ { t } \\in \\mathcal { S }$ leads the agent to receive an observation $O _ { t } \\in \\Omega$ (using $\\mathbb { O }$ ), an action $\\bar { A _ { t } } \\in \\mathcal { A }$ is played, the environment transitions into the next (unknown) state $S _ { t + 1 }$ (using $P$ ) and generates an observed reward $R _ { t + 1 }$ (using $R$ ).\n\nWhen facing a partially observable setting, an effective way of approximating a solution is to use a policy defined on compressed histories (Francois-Lavet et al. 2019). Let $\\mathcal { H } _ { t } = \\bar { \\Omega } \\times ( \\boldsymbol { A } \\times \\boldsymbol { R } \\times \\Omega ) ^ { t }$ denote the set of histories observed up to time $t$ and let $\\textstyle { \\mathcal { H } } = \\bigcup _ { t = 0 } ^ { \\infty } { \\mathcal { H } } _ { t }$ denote the space of all possible histories. The bel ef state $b ( s | H )$ is a vector where the $i$ -th component $( i \\in \\{ 1 , \\dots , | S | \\} )$ corresponds to $\\mathbb { P } ( s = i | H )$ , for history $H \\in { \\mathcal { H } }$ . One can define a mapping $\\phi : { \\mathcal { H } } \\mapsto \\phi ( { \\mathcal { H } } )$ , where the set compressed history $\\phi ( \\mathcal { H } ) = \\{ \\phi ( H ) | H \\in \\mathcal { H } \\}$ is finite, which can be used as input to a policy $\\pi : \\phi ( { \\mathcal { H } } ) \\mapsto { \\mathcal { A } }$ . We consider POMDPs with sufficient mappings such that $\\mathbb { P } ( s | H ) = \\mathbb { P } ( s | \\phi ( H ) )$ , which defines an MDP on state space $\\phi ( \\mathcal { H } )$ .\n\nGiven a POMDP (extended tuple) $M$ and any given distribution $\\mathcal { D } _ { H }$ over histories, one can define the expected return obtained over an infinite time horizon from a given history $H$ , with $A _ { t } \\sim \\pi ( \\phi ( H _ { t } ) )$ :\n\n$$\n\\begin{array} { r l } { V _ { M , \\gamma } ^ { \\pi } ( \\phi ( H ) ) = } & { \\underset { H ^ { \\prime } \\sim \\mathcal { D } _ { H } : } { \\mathbb { E } } \\left[ V _ { M } ^ { \\pi } ( H ^ { \\prime } | \\phi ) \\right] } \\\\ { \\phi ( H ^ { \\prime } ) { = } \\phi ( H ) } & { } \\end{array}\n$$\n\nWith $V _ { M , \\gamma } ^ { \\pi } ( H ^ { \\prime } | \\phi )$ given by\n\n$$\nV _ { M , \\gamma } ^ { \\pi } ( H ^ { \\prime } | \\phi ) = \\mathbb { E } _ { \\pi } \\left[ \\sum _ { k = 0 } ^ { \\infty } \\gamma ^ { k } R _ { t + k + 1 } \\Big | S _ { t } \\sim b ( \\cdot | H _ { t } = H ^ { \\prime } ) , \\right] .\n$$\n\nFor a given sufficient mapping œï, the optimal policy œÄ‚àóM,Œ≥ maximizes $V _ { M , \\gamma } ^ { \\pi } ( \\phi ( H ) )$ for all histories $H \\in { \\mathcal { H } }$ .\n\n# Extending Structural Parameters\n\nWe can extend Definitions 1 and 2 to the POMDP setting by applying them to compressed histories rather than the actual states in the underlying MDP:\n\n$$\n\\begin{array} { r l } & { \\kappa _ { M , \\gamma } ^ { \\phi } = \\underset { \\sigma , \\sigma ^ { \\prime } \\in \\phi ( \\mathcal { H } ) } { \\operatorname* { m a x } } \\left| V _ { M , \\gamma } ^ { \\pi _ { M , \\gamma } ^ { * } } ( \\sigma ) - V _ { M , \\gamma } ^ { \\pi _ { M , \\gamma } ^ { * } } ( \\sigma ^ { \\prime } ) \\right| \\eqno ( 6 } \\\\ & { \\delta _ { M } ^ { \\phi } = \\underset { \\sigma \\in \\phi ( \\mathcal { H } ) } { \\operatorname* { m a x } } \\underset { a , a ^ { \\prime } \\in \\mathcal { A } } { \\operatorname* { m a x } } \\sum _ { \\sigma ^ { \\prime } \\in \\phi ( \\mathcal { H } ) } \\left| \\mathbb { P } ( \\sigma ^ { \\prime } | \\sigma , a ) - \\mathbb { P } ( \\sigma ^ { \\prime } | \\sigma , a ^ { \\prime } ) \\right| . } \\end{array}\n$$\n\nWe introduce the following result showing how the structural parameters in the POMDP relate to the structural parameters of the underlying MDP (see Appendix F):\n\nTheorem 2. Given a POMDP M, let $\\kappa _ { M , \\gamma } ^ { S }$ and $\\delta _ { M } ^ { S }$ denote the structural parameters (Definitions $^ { 1 }$ and 2) evaluated on the underlying state space. Let $\\begin{array} { r } { \\mathcal { H } ( s ) = \\bigcup _ { t = 0 } ^ { \\infty } \\{ H _ { t } : } \\end{array}$ $b ( s | H _ { t } ) ~ > ~ 0 , H _ { t } ~ \\in ~ \\mathcal { H } _ { t } \\}$ denote the set of all histories which can lead to being in state s at any time $t$ , and $\\mathcal { D } _ { H } ( s )$ denote its probability distribution. Define $\\Delta _ { \\epsilon } ^ { \\phi }$ s.t. $\\begin{array} { r } { \\left. V _ { M , \\gamma } ^ { \\pi _ { M , \\gamma } ^ { * } } ( s ) - V _ { M , \\gamma } ^ { \\pi _ { \\phi } } ( s ) \\right. \\leq \\Delta _ { \\epsilon } ^ { \\phi } \\forall s \\in \\mathcal { S } } \\end{array}$ , define $\\dot { \\pi } _ { \\phi } ( s ) ~ = ~ \\operatorname * { \\mathbb { E } } _ { H \\sim \\mathcal { D } _ { H } ( s ) } \\pi _ { M , \\gamma } ^ { * } ( \\phi ( H ) )$ as the optimal policy on compression of histories when executed over the underlying state space and define $b ( s | \\sigma ) = \\mathbb { E } \\qquad b ( s | H ) f o r$ $\\begin{array} { r } { H { \\sim } \\mathcal { D } _ { H } ( s ) , \\sigma { = } \\phi ( H ) } \\end{array}$ $\\sigma \\in \\phi ( \\mathcal { H } )$ . We have that:\n\n$$\n\\delta _ { M } ^ { \\phi } \\leq \\delta _ { M } ^ { S }\n$$\n\n$$\n\\kappa _ { M , \\gamma } ^ { \\phi } \\leq \\operatorname* { m a x } _ { \\sigma , \\sigma ^ { \\prime } \\in \\phi \\left( \\mathcal { H } \\right) } \\frac { \\| b ( \\cdot | \\sigma ) - b ( \\cdot | \\sigma ^ { \\prime } ) \\| _ { 1 } } { 2 } \\left( \\kappa _ { M , \\gamma } ^ { S } + \\Delta _ { \\epsilon } ^ { \\phi } \\right) .\n$$\n\nThe first inequality confirms that partial observability impacts negatively the ability for the agent to control state transitions. The second inequality implies that if the policy on the partially observable domain remains accurate on the underlying state space $( \\Delta _ { \\epsilon } ^ { \\phi } \\approx 0 )$ , then the state-value variation observed by the agent is lower (since the $L _ { 1 }$ distance is bounded by 2), which could make the learning task easier and as efficient with a low discount factor. This $L _ { 1 }$ distance often has a value of 2, which makes the bound quite loose, but they illustrate the idea that the values of structural parameters decrease when taking a convex combination over states. In fact the maximal value of the expectation of a random variable happens if all the mass is concentrated on the maximal value of the support. This is explained further in appendix B. By considering that the horizon-sensitive action variation (Def. 4) is upper-bounded by the action variation (Def. 2), we can observe from Eq. 4 that the bias in the underlying MDP upper-bounds the bias of the POMDP when the optimal policy under partial observability is accurate on the true state space. This extends the ideas from Abel, Hershkowitz, and Littman (2016) where abstractions can make a problem much easier to learn while retaining good performance and in our case, lower the bias. We can get a better understanding of this trade-off in the state-abstraction setting (Abel, Hershkowitz, and Littman 2016), as shown in our numerical experiments below.\n\n# Numerical Experiments\n\nWe now conduct experiments3 to highlight the relationships between the planning horizon, the partial observability, and the structural parameters of the underlying MDP.\n\nRandom MDPs We consider the simulated environment of Jiang, Singh, and Tewari (2016). We use 2-action MDPs, with $F i x e d ( | S | , d )$ denoting a randomly generated MDP with $d \\geq 1$ next states reachable from each state. MDPs are sampled using the following procedure: 1) each state-action pair is assigned $d$ possible next states; 2) transition probabilities to these states are sampled uniformly in $[ 0 , 1 ]$ , then normalized; 3) rewards are assigned to state-action pairs by sampling uniformly in $[ 0 , 1 ]$ .\n\nExtension to Partial Observability We consider the state-abstraction setting (Abel, Hershkowitz, and Littman 2016), which corresponds to a specific case of partial observability where the history compressor $\\phi ( \\mathcal { H } )$ returns only the last observation and where $\\mathbb { O }$ is a one-hot vector on an observation from $\\Omega$ . For simplicity, we make sure that each observation is connected to at least one state. Using Bayes‚Äô theorem to recover the belief that the agent is in state $s$ given observation $\\omega$ , we get a constant uniform distribution on every state $s$ which maps onto this observation:\n\n$$\nb ( s | \\omega ) = \\frac { 1 } { | \\{ s \\in \\mathcal { S } : \\mathbb { O } ( o , s ) > 0 \\} | } \\forall s \\in \\mathcal { S } : \\mathbb { O } ( \\omega , s ) > 0 ,\n$$\n\nand a belief of 0 otherwise. From this special case of POMDP, we can extract an abstract MDP $\\begin{array} { l l } { { M _ { A } } } & { { = } } \\end{array}$ $\\langle S _ { A } , \\mathcal { A } , P _ { A } , R _ { A } , \\gamma \\rangle$ from the underlying MDP $\\begin{array} { r l } { M } & { { } = } \\end{array}$ $\\langle S , \\mathcal { A } , P , R , \\gamma \\rangle$ (Abel, Hershkowitz, and Littman 2016):\n\n$$\n\\begin{array} { l } { { R _ { A } ( \\omega , a ) = \\displaystyle \\sum _ { s \\in \\mathcal { S } } b ( s | \\omega ) R ( s , a ) , \\mathrm { a n d } } } \\\\ { { { \\cal P } _ { A } ( \\omega , a , \\omega ^ { \\prime } ) = \\displaystyle \\sum _ { s \\in \\mathcal { S } } \\displaystyle \\sum _ { s ^ { \\prime } \\in \\mathcal { S } } P ( s , a , s ^ { \\prime } ) b ( s | \\omega ) \\mathbb { O } ( \\omega ^ { \\prime } , s ^ { \\prime } ) . } } \\end{array}\n$$\n\nFor our experiments under partial observability, we start by sampling an MDP from $F i \\bar { x e d } ( | S | , d )$ . Then, we can map it onto abstracted MDPs (POMDPs) with different number of observations as $| \\Omega |$ . The number of obervations encodes the level of partial observability akin to what happens in discretization. For $\\left| \\Omega \\right| = \\left| S \\right|$ , the problem is fully observable. For $| \\Omega | \\ = \\ 1$ (and $\\vert s \\vert ~ > ~ 1 )$ , the agent is completely blind to the state. We sample $1 0 ^ { 4 }$ MDPs with $F i x e d ( 1 0 , 3 )$ and abstract each MDP into 6 configurations: $| \\Omega | \\in \\left\\{ 1 0 , 8 , 6 , 4 , 2 , 1 \\right\\}$ . The Blackwell discount factors are computed by iterating from $\\gamma = 1$ to $\\gamma = 0$ with step size of 0.01 until the optimal policy changes.\n\nFig. 2 (left) shows that the mass of the Blackwell planning horizon tends to decrease as the observability decreases. Since the bias is null when planning with a discount factor larger than $\\gamma _ { B w }$ , we only cumulate variance above that point. When $| \\Omega | = 1$ , the myopic agent $( \\gamma = 0 )$ ) enjoys the optimal planning horizon, which corresponds to the bandit setting. We also evaluate the normalized bias\n\n$$\n\\operatorname* { m a x } _ { s \\in S } \\left( V _ { M , \\gamma _ { B w } } ^ { \\pi _ { M , \\gamma _ { B w } } ^ { * } } ( s ) - V _ { M , \\gamma _ { B w } } ^ { \\pi _ { M , \\gamma } ^ { * } } ( s ) \\right) / V _ { M , \\gamma _ { B w } } ^ { \\pi _ { M , \\gamma _ { B w } } ^ { * } } ( s )\n$$\n\nfor different planning horizons, averaged over different levels of partial observability. Fig. 2 (right) shows that, although the bias decreases when the planning horizon $( 1 / ( 1 -$ $\\gamma )$ )) increases, this effect attenuates as the observability decreases. Since many real-world problems are partially observable, this supports the need to consider shallow planning more seriously.\n\nWe normalize parameters for each abstract MDP by dividing structural parameters by those of the true $F i x e d ( \\cdot )$ MDP, i.e., $\\kappa _ { M , \\gamma } ^ { \\phi } / \\kappa _ { M , \\gamma } ^ { S }$ for the normalized value function variation and $\\delta _ { M } ^ { \\phi } / \\delta _ { M } ^ { S }$ for the normalized action variation.\n\n![](images/697edd7d936770d70763746225e9171549615dc837561b3f3c2037de03e6c97c.jpg)  \nFigure 2: Left: Distribution of Blackwell discount factors over $1 0 ^ { 4 }$ POMDPs given the number of observations. Right: Average normalized bias given the discount factor and number of observations.\n\n![](images/6803f8bfa8eb9ac1e837f31ed03db208a9052caa8f0517c3e77954aeac3473f0.jpg)  \nFigure 3: Left: Distribution of normalized $\\kappa _ { M , \\gamma } ^ { \\phi }$ over $1 0 ^ { 4 }$ POMDPs given the number of observations and the discount factor used. Right: Distribution of normalized $\\delta _ { M } ^ { \\phi }$ given the number of observations.\n\nFig. 3 (left) highlights the tradeoff given by Thm. 2. We see that there is a downards trend, but for high values of gamma, the error $\\Delta _ { \\epsilon } ^ { \\phi }$ in some cases is so high that the valuefunction variation can be higher under partial observability. Fig. 3 (right) highlights the strict inequality offered by Thm. 2 on the action variation of the POMDP vs the underlying MDP. By observing Thm. 1 and Fig. 3, the reduction in structural parameters offers insight into why the bias decreases under partial observability. It also points to the fact that there might exist a bound on the Blackwell discount factor using structural parameters much tighter than GrandCle¬¥ment and Petrik (2023) or even improve upon the results of Laidlaw, Russell, and Dragan (2023) using partial observability.\n\n# Impact of Partial Observability on Deep RL\n\nWe explore the interaction of shallow planning and partial observability in the Cartpole-v1 environment (Towers et al. 2024). In Cartpole, an agent uses two discrete actions (left or right) to balance a pole on a moving cart. The agent‚Äôs state includes the cart‚Äôs position and velocity, along with the pole‚Äôs angle and angular velocity. A reward of $+ 1$ is given for each time step before episode termination or reaching 500 time steps.\n\nWe consider the widely used PPO (Schulman et al. 2017) agent policy with the recommended hyperparameters for this task (Raffin et al. 2021). We then consider different discount factors $\\gamma \\in \\{ 0 , 0 . 2 4 , 0 . 4 9 , 0 . 7 3 , 0 . 9 8 \\}$ , with the largest $\\gamma ~ = ~ 0 . 9 8$ from the baseline (Raffin et al. 2021). For the partially observable component, we simulate noisy sensors, which are common in real life. These are simulated by injecting noise into the state. The noise is sampled from a multivariate normal distribution $\\mathcal { N } ( \\mathbf { 0 } , I \\sigma ^ { 2 } )$ parameterized by a diagonal covariance matrix with value $\\sigma ^ { 2 }$ on the diagonal. We consider $\\sigma \\in \\{ 0 , 0 . 1 , 1 \\}$ . We train 10 agents for each combination of $( \\gamma , \\sigma )$ , resulting in 150 models, and evaluate each of these models on 100 unseen seeds.\n\nFig. 4 displays the average reward obtained by running the 10 models associated with each $( \\gamma , \\sigma )$ configuration on the 100 environment seeds. Without surprise, planning with the longest horizon (largest discount factor) maximizes rewards (as expected on this task). However, we also observe that, as the environment becomes partially observable (e.g. $\\sigma = 0 . 1$ ), shallow planning can be better than long planning since the bias is lowered from the partial observability. The Blackwell discount factor could be lowered, supporting our previous results and thesis. As expected, when observability becomes too low (e.g. $\\sigma = 1$ ), the agent cannot understand the environment anymore and is unable to learn the task no matter the discount factor considered.\n\n![](images/1f9f3aae1974c560f8e5372226c4dca6f34d306ae194b70c1034f0b5f86bcbff.jpg)  \nFigure 4: Average reward and standard deviation obtained by running 10 models on 100 environment seeds given the noise level and discount factor.\n\n# Related Work\n\nEffective Planning Horizon When it comes to using RL in practical applications, there is a trend towards a better understanding of the planning horizon on the optimality of the policy learned. Grand-Cle¬¥ment and Petrik (2023) explore the Blackwell optimality criterion and proves the existence of a planning horizon above which the policy learned cannot be improved. Similarly, Laidlaw, Russell, and Dragan (2023) explore the minimal horizon (in number of time steps forward) for which the optimal policy can be retrieved with high probability. Our work builds on top of these frameworks to better understand the bias-variance trade-off that arises from using a shallow planning horizon.\n\nPlanning Loss The proposed decomposition of the planning loss (Eq. 2) enables its interpretation in terms of bias and variance, unlike the previous decomposition of Jiang et al. (2015). The bias term had been studied previously (Jiang, Singh, and Tewari 2016), borrowing inspiration on the approximation term from the PAC bound literature in machine learning (Shalev-Shwartz and Ben-David 2014). However, the variance term remained unbounded. By connecting ideas from previous works Jiang et al. (2015); Jiang, Singh, and Tewari (2016), we are now able to understand not only the bias as a function of the structural parameters, but also part of the variance.\n\nPartial Observability Most practical problem are under some form of partial observability (Dulac-Arnold et al. 2021). Different approximation schemes are explored through the literature like memoryless policies (Mu¬®ller and\n\nMontu¬¥far 2021) or compressions of histories (FrancoisLavet et al. 2019). There is also plenty of work on properties of POMDPs such that approximations are more likely to yield good results (Liu et al. 2022). Our work extends these results to a better understanding of how these approximation schemes can impact not only the sample complexity or the performance guarantees, but the planning horizon that should be considered.\n\nIn this direction, the literature on state abstraction aims to tackle some of the challenges of practical RL through a simpler state-space (Abel, Hershkowitz, and Littman 2016; Allen et al. 2021; Abel et al. 2018). This approximation scheme is well understood, and lots of useful properties on policy performance or sample complexity have been found. These results have not yet been connected to the planning horizon that should be used under these regimes. We attempt to bridge this gap by providing insight into the behaviour of the bias under partial observability.\n\n# Conclusion\n\nWe extend existing structural parameters to consider the planning horizon (Def. 4) and the model approximation (Def. 6). This allow us to extend an existing bound on the bias (Eq. 4) and propose a new bound on the variance (Lem. 1), which result in a new bound on the planning loss (Thm. 1). We finally extend the structural parameters to POMDPs (Eq. 6 and 7) and show that these are controlled by their fully observable counterparts (Thm. 2). This complements previous results (Abel, Hershkowitz, and Littman 2016; Francois-Lavet et al. 2019) by considering the impact of the planning horizon on the bias when shallow planning under partial observability.\n\nLimitations and Future Work Our work motivates further research on understanding when shallow planning might be beneficial by providing new theory. However, our results are limited to a finite state space. Also, the reduction in $\\kappa _ { M , \\gamma _ { . } } ^ { S }$ (Thm. 2) that comes from partial observability can be much larger than what is captured by the $L _ { 1 }$ distance; the bound could be improved by using better mathematical tools. Even with these limitations, practitioners can use our analysis and our experiments to better understand how the nature of their problem might impact the bias-variance tradeoff, and select a better planning horizon. Future work could improve upon our results by using deep-learning theory. It could also be possible to develop automatic discount factor selection algorithms, as previous research papers have indicated that even naive form of this strategy can be very impactful (Franc¬∏ois-Lavet, Fonteneau, and Ernst 2015). Statespecific discount factors are also interesting. It is easy to construct an MDP such that the Blackwell discount factor is very high because of only a handful of states in the state space. A better understanding of the local structure around a state means we could create heuristics to adapt the discount factor automatically (Pitis 2019; Yoshida, Uchibe, and Doya 2013). Finally, a better understanding of the process which could lead to the downward trend in $\\gamma _ { B w }$ observed in Fig. 2 through our analysis could provide better insight into how to prevent over-fitting.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáÁ†îÁ©∂‰∫ÜÂú®ÈÉ®ÂàÜÂèØËßÇÊµãÁéØÂ¢É‰∏ãÔºåÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâ‰∏≠ÊäòÊâ£Âõ†Â≠êÔºàdiscount factorÔºâÂØπÂÅèÂ∑Æ-ÊñπÂ∑ÆÊùÉË°°ÁöÑÂΩ±Âìç„ÄÇÊ†∏ÂøÉÈóÆÈ¢òÊòØÔºöÂ¶Ç‰ΩïÈÄâÊã©ÈÄÇÂΩìÁöÑËßÑÂàíËßÜÈáéÔºàplanning horizonÔºâ‰ª•‰ºòÂåñÁ≠ñÁï•Â≠¶‰π†ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈÉ®ÂàÜÂèØËßÇÊµãÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ãÔºàPOMDPsÔºâ‰∏≠„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éÔºåÁé∞ÊúâÊñπÊ≥ïÂú®ÈÉ®ÂàÜÂèØËßÇÊµãÁéØÂ¢É‰∏ãÂæÄÂæÄÂøΩÁï•ËßÑÂàíËßÜÈáéÁöÑÈÄâÊã©ÔºåÂØºËá¥Á≠ñÁï•Â≠¶‰π†ÊïàÁéá‰Ωé‰∏ãÊàñÊÄßËÉΩ‰∏ç‰Ω≥„ÄÇÊú¨ÊñáÈÄöËøáÁêÜËÆ∫ÂàÜÊûêÂíåÂÆûÈ™åÈ™åËØÅÔºåÊè≠Á§∫‰∫ÜÁü≠ËßÑÂàíËßÜÈáéÂú®ÈÉ®ÂàÜÂèØËßÇÊµãÁéØÂ¢É‰∏ãÁöÑ‰ºòÂäø„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁêÜËÆ∫Ê°ÜÊû∂ÔºåÈÄöËøáÊâ©Â±ïÁªìÊûÑÂèÇÊï∞ÔºàÂ¶Ç‰ª∑ÂÄºÂáΩÊï∞ÂèòÂºÇÂíåÂä®‰ΩúÂèòÂºÇÔºâÊù•ÈáèÂåñËßÑÂàíËßÜÈáéÂØπÂÅèÂ∑ÆÂíåÊñπÂ∑ÆÁöÑÂΩ±ÂìçÔºåÂπ∂Êé®ÂØº‰∫ÜÊñ∞ÁöÑËßÑÂàíÊçüÂ§±ÁïåÈôêÔºàThm. 1Ôºâ„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **Ë¥°ÁåÆ1Ôºö** ÊèêÂá∫‰∫ÜÊñ∞ÁöÑÂÅèÂ∑Æ-ÊñπÂ∑ÆÂàÜËß£ÔºàEq. 2ÔºâÔºåÈ¶ñÊ¨°Â∞ÜËßÑÂàíÊçüÂ§±ÊòéÁ°ÆÂàÜ‰∏∫ÂÅèÂ∑ÆÂíåÊñπÂ∑Æ‰∏§ÈÉ®ÂàÜÔºåÂπ∂ÂàÜÂà´Êé®ÂØº‰∫ÜÁïåÈôêÔºàEq. 4ÂíåLem. 1Ôºâ„ÄÇ\\n> *   **Ë¥°ÁåÆ2Ôºö** Êâ©Â±ï‰∫ÜÁªìÊûÑÂèÇÊï∞ÔºàDef. 4ÂíåDef. 6ÔºâÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊçïÊçâËßÑÂàíËßÜÈáéÂíåÊ®°ÂûãËøë‰ººÁöÑÂΩ±Âìç„ÄÇ\\n> *   **Ë¥°ÁåÆ3Ôºö** Âú®POMDPs‰∏≠È™åËØÅ‰∫ÜÁü≠ËßÑÂàíËßÜÈáéÁöÑÊúâÊïàÊÄßÔºàFig. 2ÂíåFig. 4ÔºâÔºåÂÆûÈ™åË°®ÊòéÂú®ÈÉ®ÂàÜÂèØËßÇÊµãÁéØÂ¢É‰∏ãÔºåÁü≠ËßÑÂàíËßÜÈáéÂèØ‰ª•ÊòæËëóÈôç‰ΩéÂÅèÂ∑Æ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   ËÆ∫ÊñáÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÈÄöËøáÈáèÂåñËßÑÂàíËßÜÈáéÂØπÂÅèÂ∑ÆÂíåÊñπÂ∑ÆÁöÑÂΩ±ÂìçÔºå‰∏∫ÈÉ®ÂàÜÂèØËßÇÊµãÁéØÂ¢É‰∏ãÁöÑRLÊèê‰æõÁêÜËÆ∫ÊåáÂØº„ÄÇÂÖ∂ËÆæËÆ°Âì≤Â≠¶ÊòØÔºöÂú®Êï∞ÊçÆÊúâÈôêÊàñÁéØÂ¢ÉÈÉ®ÂàÜÂèØËßÇÊµãÊó∂ÔºåÁü≠ËßÑÂàíËßÜÈáéÂèØ‰ª•Êõ¥Â•ΩÂú∞Âπ≥Ë°°ÂÅèÂ∑ÆÂíåÊñπÂ∑Æ„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** ÂÖàÂâçÂ∑•‰ΩúÔºàÂ¶ÇJiang et al. 2015ÔºâÁöÑÂÅèÂ∑Æ-ÊñπÂ∑ÆÁïåÈôêËæÉ‰∏∫ÊùæÊï£Ôºå‰∏îÊú™ËÄÉËôëPOMDPsÁöÑÁªìÊûÑÁâπÊÄß„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** Êú¨ÊñáÈÄöËøáÂºïÂÖ•‚ÄúËßÜÈáéÊïèÊÑüÂä®‰ΩúÂèòÂºÇ‚ÄùÔºàDef. 4ÔºâÂíå‚ÄúÁªèÈ™åÂä®‰ΩúÂèòÂºÇ‚ÄùÔºàDef. 6ÔºâÔºåÂ∞ÜÁªìÊûÑÂèÇÊï∞‰∏éËßÑÂàíËßÜÈáéÂíåÊ®°ÂûãËøë‰ººÁõ¥Êé•ÂÖ≥ËÅîÔºå‰ªéËÄåÊé®ÂØºÂá∫Êõ¥Á¥ßÁöÑÁïåÈôêÔºàThm. 1Ôºâ„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> *   **Ê≠•È™§1Ôºö** ÂÆö‰πâMDPÂíåPOMDPÁöÑÂü∫Êú¨ÁªìÊûÑÔºàÂ¶ÇÁä∂ÊÄÅÁ©∫Èó¥„ÄÅÂä®‰ΩúÁ©∫Èó¥„ÄÅËΩ¨ÁßªÂáΩÊï∞Á≠âÔºâ„ÄÇ\\n> *   **Ê≠•È™§2Ôºö** ÂºïÂÖ•BlackwellÊäòÊâ£Âõ†Â≠êÔºàŒ≥_BwÔºâ‰Ωú‰∏∫ËßÑÂàíËßÜÈáéÁöÑÂü∫ÂáÜ„ÄÇ\\n> *   **Ê≠•È™§3Ôºö** ÂàÜËß£ËßÑÂàíÊçüÂ§±‰∏∫ÂÅèÂ∑ÆÂíåÊñπÂ∑ÆÔºàEq. 2ÔºâÔºåÂπ∂ÂàÜÂà´Êé®ÂØºÁïåÈôêÔºàEq. 4ÂíåLem. 1Ôºâ„ÄÇ\\n> *   **Ê≠•È™§4Ôºö** Âú®POMDPs‰∏≠Êâ©Â±ïÁªìÊûÑÂèÇÊï∞ÔºàEq. 6ÂíåEq. 7ÔºâÔºåÂπ∂ÈÄöËøáÂÆûÈ™åÈ™åËØÅÁêÜËÆ∫ÔºàFig. 2ÂíåFig. 4Ôºâ„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÂàóÂá∫Âü∫Á∫øÊ®°ÂûãÔºå‰ΩÜÂÆûÈ™åÈÉ®ÂàÜÂØπÊØî‰∫Ü‰∏çÂêåËßÑÂàíËßÜÈáéÔºàŒ≥ÂÄºÔºâÂíåÈÉ®ÂàÜÂèØËßÇÊµãÊ∞¥Âπ≥Ôºà|Œ©|ÂÄºÔºâ‰∏ãÁöÑÊÄßËÉΩÂ∑ÆÂºÇ„ÄÇ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®ËßÑÂàíÊçüÂ§±‰∏äÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®ÈöèÊú∫ÁîüÊàêÁöÑMDPsÔºàFixed(10,3)Ôºâ‰∏äÈ™åËØÅ‰∫ÜThm. 1ÁöÑÁ¥ßËá¥ÊÄßÔºàFig. 1ÔºâÔºåË°®ÊòéÂú®‰ΩéÊ®°ÂûãËøë‰ººËØØÂ∑ÆÔºàŒµÃÇÔºâÊó∂ÔºåÊñ∞ÁïåÈôê‰ºò‰∫éÂÖàÂâçÂ∑•‰ΩúÔºàJiang et al. 2015Ôºâ„ÄÇ\\n> *   **Âú®ÈÉ®ÂàÜÂèØËßÇÊµãÁéØÂ¢É‰∏ãÔºö** Êú¨ÊñáÊñπÊ≥ïÂú®Cartpole-v1ÁéØÂ¢É‰∏≠ÔºàœÉ=0.1ÔºâÊòæÁ§∫ÔºåÁü≠ËßÑÂàíËßÜÈáéÔºàŒ≥=0.49ÔºâÁöÑÂπ≥ÂùáÂ•ñÂä±‰∏éÈïøËßÑÂàíËßÜÈáéÔºàŒ≥=0.98ÔºâÁõ∏ÂΩìÔºå‰ΩÜÂú®È´òÂô™Â£∞ÔºàœÉ=1ÔºâÊó∂ÔºåÊâÄÊúâËßÑÂàíËßÜÈáéÂùáÂ§±ÊïàÔºàFig. 4Ôºâ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n> **ÊèêÂèñ‰∏éÊ†ºÂºèÂåñË¶ÅÊ±Ç**\\n> *   Âº∫ÂåñÂ≠¶‰π† (Reinforcement Learning, RL)\\n> *   ÈÉ®ÂàÜÂèØËßÇÊµãÈ©¨Â∞îÂèØÂ§´ÂÜ≥Á≠ñËøáÁ®ã (Partially Observable Markov Decision Process, POMDP)\\n> *   ÊäòÊâ£Âõ†Â≠ê (Discount Factor, N/A)\\n> *   ÂÅèÂ∑Æ-ÊñπÂ∑ÆÊùÉË°° (Bias-Variance Trade-off, N/A)\\n> *   ËßÑÂàíËßÜÈáé (Planning Horizon, N/A)\\n> *   ÁªìÊûÑÂèÇÊï∞ (Structural Parameters, N/A)\\n> *   ÁªèÈ™åÂä®‰ΩúÂèòÂºÇ (Empirical Action Variation, N/A)\"\n}\n```"
}