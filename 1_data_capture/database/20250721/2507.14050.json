{
    "source": "ArXiv (Semantic Scholaræœªæ”¶å½•)",
    "arxiv_id": "2507.14050",
    "link": "https://arxiv.org/abs/2507.14050",
    "pdf_link": "https://arxiv.org/pdf/2507.14050.pdf",
    "title": "Foundation Models as Class-Incremental Learners for Dermatological Image Classification",
    "authors": [
        "Mohamed Elkhayat",
        "Mohamed Mahmoud",
        "Jamil Fayyad",
        "Nourhan Bayasi"
    ],
    "categories": [
        "cs.CV"
    ],
    "publication_date": "æœªæ‰¾åˆ°æäº¤æ—¥æœŸ",
    "venue": "æš‚æœªå½•å…¥Semantic Scholar",
    "fields_of_study": "æš‚æœªå½•å…¥Semantic Scholar",
    "citation_count": "æš‚æœªå½•å…¥Semantic Scholar",
    "influential_citation_count": "æš‚æœªå½•å…¥Semantic Scholar",
    "institutions": [
        "Cairo University",
        "University of Victoria",
        "University of British Columbia"
    ],
    "paper_content": "# Foundation Models as Class-Incremental Learners for Dermatological Image Classification\n\nMohamed Elkhayat $^ 1$ \\*, Mohamed Mahmoud $^ 1$ \\*, Jamil Fayyad $^ { 2 \\dagger }$ â€ , and Nourhan Bayasi $^ 3$ â€ \n\n1 Cairo University, Giza, Egypt 2 University of Victoria, Victoria, BC, Canada 3 University of British Columbia, Vancouver, BC, Canada {Mohammed.Khayyat02, muhammad.mahmoud01}@eng-st.cu.edu.eg\n\nAbstract. Class-Incremental Learning (CIL) aims to learn new classes over time without forgetting previously acquired knowledge. The emergence of foundation models (FM) pretrained on large datasets presents new opportunities for CIL by offering rich, transferable representations. However, their potential for enabling incremental learning in dermatology remains largely unexplored. In this paper, we systematically evaluate frozen FMs pretrained on large-scale skin lesion datasets for CIL in dermatological disease classification. We propose a simple yet effective approach where the backbone remains frozen, and a lightweight MLP is trained incrementally for each task. This setup achieves state-of-the-art performance without forgetting, outperforming regularization, replay, and architecture-based methods. To further explore the capabilities of frozen FMs, we examine zero-training scenarios using nearest-mean classifiers with prototypes derived from their embeddings. Through extensive ablation studies, we demonstrate that this prototype-based variant can also achieve competitive results. Our findings highlight the strength of frozen FMs for continual learning in dermatology and support their broader adoption in real-world medical applications. Our code and datasets are available here.\n\nKeywords: Class-Incremental Learning Â· Continual Learning Â· Foundation Models Â· Dermatological Image Classification Â· Dermatology\n\n# 1 Introduction\n\nReal-world clinical applications rarely offer the luxury of independent and identically distributed (i.i.d.) data [15]. In dermatology, new disease classes or imaging variations may appear gradually as data is collected over time from different sources. Conventional models trained in a static setting often fail under these changing conditions [16], showing a sharp drop in performance on previously learned tasks when updated with new data; a problem known as catastrophic forgetting [26,4]. Continual learning (CL) aims to address this challenge by allowing models to learn new information while preserving past knowledge. Several setups within CL include Class-Incremental Learning (CIL) is one of the most challenging. In CIL, new classes are introduced over time, and the model must learn them without access to data from earlier tasks, making this a relevant setting in clinical workflows where storing or replaying patient data is often restricted due to privacy and ethical concerns.\n\nTo avoid forgetting without storing or replaying old patient data [11], researchers have explored regularization- and architecture-based strategies. Regularization methods penalize changes to important parameters [32], while architecturebased approaches expand the model or allocate task-specific components [36,5,10]. While effective in controlled settings, these methods face key limitations in clinical practice: regularization requires reliable importance estimates, which are often difficult to obtain in data-scarce environments, and architecture-based techniques introduce complexity and memory overhead. In contrast, foundation models (FM) trained on large-scale datasets have reshaped the landscape of CIL, offering robust, transferable features that generalize well with minimal finetuning [12]. Recent work in natural image domains shows that simply leveraging frozen FMs can significantly boost performance and reduce forgetting [19,20].\n\nMotivated by these findings, we turn to dermatology and ask: Can frozen FMs pretrained on large-scale dermatological data support CIL for skin lesion classification, or are specialized CL methods still necessary? To answer this, we present the first comprehensive evaluation of frozen dermatology FMs for continual skin lesion classification. Our setup is deliberately simple: the backbone remains frozen, and a lightweight MLP classifier is incrementally trained for each task. Surprisingly, this approach outperforms prior CIL methods, including regularization, replay, and architectural techniques, without requiring any finetuning. We also explore zero-training setups using prototype-based classifiers derived from FM embeddings, and through extensive ablation studies, demonstrate that variations of this method can significantly outperform existing approaches. Our results suggest that future dermatology-based CL research should start with FMs, rather than designing methods from scratch.\n\n# 2 Related Work\n\nClass-Incremental Learning for Medical Imaging. CIL has recently received growing attention in medical imaging, driven by the need for models that can learn new disease categories without forgetting prior knowledge, all while preserving patient privacy. This has spurred data-free methods that synthesize prior class representations instead of storing raw images. For example, Ayromlou et al. [1] use gradient inversion and novel loss functions to preserve class discriminability. Bayasi et al. [7,6,9] introduce a pruning-based approach that builds independent subnetworks to eliminate forgetting and support fair and generalizable CL. Others rely on regularization: Chee et al. [13] expand network capacity while retaining prior knowledge, and Chen et al. [14] use contrastive learning and distillation for class- and domain-incremental segmentation.\n\nFoundation Models in Continual Learning. Traditional CL methods often train feature extractors from scratch, making them vulnerable to catastrophic forgetting. Recent work has shown that leveraging frozen FMs can improve both stability and efficiency. In vision, methods like DualPrompt [30] and L2P [31] use prompt tuning on frozen backbones, while Janson et al. [19] showed that simple classifiers on frozen features can rival or outperform complex methods. In medical imaging, Yang et al. [34] used fixed encoders with Gaussian mixtures, Zhang et al. [35] introduced adapter modules, and Bayasi et al. [8] leveraged frozen model ensembles. Yet, the role of FMs in continual dermatology classification remains unexplored, leaving an important gap in the field.\n\n# 3 Methodology\n\n# 3.1 Problem Setup\n\nLet $\\mathcal { D } = \\{ ( \\mathbf { x } _ { i } , y _ { i } ) \\} _ { i = 1 } ^ { N }$ denote a dataset of skin lesion images, where $\\mathbf { x } _ { i } \\in \\mathbb { R } ^ { H \\times W \\times C }$ is an input image and $y _ { i } \\in \\{ 1 , 2 , \\ldots , C \\}$ is its corresponding class label. In the class-incremental learning (CIL) setup, the complete set of classes ${ \\mathcal { C } } =$ $\\{ 1 , 2 , . . . , C \\}$ is partitioned into $T$ disjoint subsets, ${ \\mathcal { C } } _ { 1 } , { \\mathcal { C } } _ { 2 } , . . . , { \\mathcal { C } } _ { T }$ , such that new classes are introduced sequentially over $T$ tasks. At each time step $t \\in \\{ 1 , \\ldots , T \\}$ , the model receives access only to a task-specific dataset $\\mathcal { D } _ { t } = \\{ ( \\mathbf { x } _ { i } , y _ { i } ) \\mid y _ { i } \\in \\mathcal { C } _ { t } \\}$ . No access is granted to prior task data $\\mathcal { D } _ { < t }$ , and storage of past examples is not allowed. The model must update its classification capabilities to accommodate new classes in $\\mathit { \\check { C } } _ { t }$ while preserving performance on all previously learned classes $\\textstyle { \\mathcal { C } } _ { < t } = \\bigcup _ { j = 1 } ^ { t - 1 } { \\mathcal { C } } _ { j }$ .\n\nLet $\\mathcal { F } _ { \\theta }$ be a dermatology FM with frozen parameters $\\theta$ , pretrained on a largescale skin lesion images. The parameters $\\theta$ remain fixed and are never updated during the continual learning (CL) process. For an input image $\\mathbf { x }$ , the model produces a feature embedding:\n\n$$\n\\mathbf { z } = \\mathcal { F } _ { \\boldsymbol { \\theta } } ( \\mathbf { x } ) \\in \\mathbb { R } ^ { d } .\n$$\n\nOur goal is to evaluate two CL baselines built on top of these frozen embeddings. The first one uses an MLP-based classifier, where a lightweight multi-layer perceptron is incrementally trained on top of the frozen embeddings for each new task. The second one adopts a prototype-based nearest-mean classifier (NMC), which requires no training. Instead, it computes a mean feature vector (prototype) for each class using the frozen features of the labeled training samples.\n\n# 3.2 Baseline 1: MLP-Based Class-Incremental Learning\n\nIn this baseline, we keep the FM frozen and train a lightweight MLP classifier incrementally across tasks.\n\n3.2.1. Training Phase. At each task $t$ , a new MLP head $h _ { t } : \\mathbb { R } ^ { d }  \\mathbb { R } ^ { | \\mathcal { C } _ { t } | }$ is trained on the frozen embeddings from $\\mathcal { D } _ { t }$ . The MLP has two hidden layers with ReLU activation and a softmax output:\n\n$$\nh _ { t } ( { \\bf z } ) = \\mathrm { S o f t m a x } \\left( W _ { 3 } \\cdot \\mathrm { R e L U } \\left( W _ { 2 } \\cdot \\mathrm { R e L U } ( W _ { 1 } { \\bf z } + { \\bf b } _ { 1 } ) + { \\bf b } _ { 2 } \\right) + { \\bf b } _ { 3 } \\right) ~ .\n$$\n\nTo support all seen classes, we concatenate the outputs of all MLPs learned up to task $t$ : $h ( \\mathbf { z } ) = \\operatorname { C o n c a t } ( h _ { 1 } ( \\mathbf { z } ) , \\dots , h _ { t } ( \\mathbf { z } ) )$ .\n\n3.2.2. Inference Phase. At test time, input image $\\mathbf { x }$ is passed through the frozen encoder and all MLP heads. The final prediction is made by taking the class with the highest probability across all tasks: $\\hat { y } = \\arg \\operatorname* { m a x } _ { c \\in \\mathcal { C } _ { \\leq t } } h ( \\mathcal { F } _ { \\theta } ( \\mathbf { x } ) ) _ { c }$ .\n\n# 3.3 Baseline 2: Prototype-Based Nearest Mean Classifier (NMC)\n\nThis baseline avoids training by using class-wise mean embeddings (prototypes) computed from frozen features.\n\n3.3.1. Training Phase. For each class $c \\in { \\mathcal { C } } _ { t }$ , we compute a class prototype $\\mu _ { c }$ by averaging the frozen embeddings of all class-wise training samples in $\\mathcal { D } _ { t }$ :\n\n$$\n\\mu _ { c } = \\frac { 1 } { | \\mathcal { D } _ { c } | } \\sum _ { ( \\mathbf { x } _ { i } , y _ { i } ) \\in \\mathcal { D } _ { t } , y _ { i } = c } \\mathcal { F } _ { \\theta } ( \\mathbf { x } _ { i } ) .\n$$\n\nThese prototypes are stored in a memory bank: $\\mathcal { M } _ { t } = \\{ \\mu _ { c } \\ | \\ c \\in \\mathcal { C } _ { t } \\}$ .\n\n3.3.2. Inference Phase. Given a test image $\\mathbf { x }$ , we extract its embedding $\\mathbf { z } =$ $\\mathcal { F } _ { \\boldsymbol { \\theta } } ( \\mathbf { x } )$ , then classify it by assigning the label of the nearest prototype across all seen classes: $\\begin{array} { r } { \\hat { y } = \\arg \\operatorname* { m i n } _ { c \\in \\mathcal { C } _ { \\leq t } } \\| \\mathbf { z } - \\mu _ { c } \\| _ { 2 } } \\end{array}$ .\n\n# 4 Experiments and Results\n\nWe evaluate our two FM-based CL baselines on the task of skin lesion classification under the CIL setting, where new sets of classes are introduced sequentially without access to previous data. Details are given next.\n\n# 4.1 Experimental Setup\n\nDatasets. Our experiments are conducted on three publicly available dermatology datasets: HAM10000 (HAM) [28], Dermofit (DMF) [2], and Derm7pt (D7P) [21], comprising 10,015 1,211, and 963 dermoscopic images, respectively. These datasets were collected from diverse clinical sources and span a subset of seven skin lesion classes. To simulate a CIL scenario, each dataset is partitioned into $T$ tasks with mutually exclusive class labels. We adopt the dataset splits and experimental protocol from [8] to ensure fair and consistent comparison.\n\nImplementation Details. We evaluate our baselines using two dermatologybased FMs: the Google Derm model [18], a publicly released FM trained on over 400 skin conditions, and PanDerm [33], a large-scale FM pre-trained on millions of clinical and dermoscopic dermatology images. Both models are used as frozen feature extractors throughout the continual learning process, with no fine-tuning. The MLP-based classifier is trained using the Adam optimizer (learning rate 0.001, batch size 200) with cross-entropy loss. Training runs for up to 200 epochs per task, with early stopping based on validation accuracy to mitigate overfitting. Reference Methods and Competitors. We compare our baselines with three standard reference methods: SINGLE, which trains separate models for different tasks and deploys a specific model for each task during inference; JOINT, which aggregates the data from all tasks as a consolidated dataset to jointly train a single model (aka. multitask learning); and SeqFT, which fine-tunes a single model on the current task, without any countermeasure to forgetting. We compare against several CL competitors, including two regularization-based methods: EWC [22] and LwF [23]; two generative-based method: DGM [25] and BIR [29]; two replay-based method: iCaRL [27] and RM [3] and a frozen pretrained model-based method: Continual-Zoo [8].\n\nEvaluation Metrics. We report the balanced accuracy (BAAC), which accounts for class imbalance by averaging the recall across all classes, ensuring that each class contributes equally to the final score. Also, we report the forgetting measure $( \\mathbf { F } )$ , which quantifies how much the model forgets previously learned tasks: $\\begin{array} { r } { { \\bf F } = \\frac { 1 } { T - 1 } \\sum _ { i = 1 } ^ { T - 1 } \\operatorname* { m a x } _ { k \\in \\{ 1 , \\dots , T - 1 \\} } a _ { k , i } - a _ { T , i } } \\end{array}$ , where $a _ { k , i }$ is the accuracy on task $i$ after training on task $k$ , and $\\boldsymbol { a } _ { T , i }$ is the final accuracy on task $i$ after training on all $T$ tasks. A higher value of $\\mathbf { F }$ indicates more forgetting.\n\n# 4.2 Results and Analysis\n\nMain Results. Table 1 summarizes the performance of our two FM-based baselines across three skin lesion benchmarks. Our MLP-based models (Google Derm and PanDerm) consistently achieve state-of-the-art balanced accuracy (BAAC) while exhibiting zero forgetting ( $\\mathbf { F } = 0$ ), outperforming all existing CL methods including regularization, replay, and architecture-based approaches. On the HAM dataset, PanDerm with MLP achieves a BAAC of $9 2 . 2 5 \\%$ , surpassing even the upper-bound SINGLE model $( 8 8 . 3 5 \\%$ ) and strongly outperforming replaybased methods like RM. Similar trends are observed on DMF, where PanDerm with MLP reaches $9 3 . 1 1 \\%$ , exceeding the best non-foundation continual learning method, Continual-Zoo, by over 20 percentage points. On the D7P dataset, PanDerm again leads with a BAAC of $7 7 . 8 0 \\%$ , outperforming all methods.\n\nInterestingly, our NMC-based FM baselines, particularly with Google Derm, achieve comparable, and sometimes superior, results relative to other competing techniques. For example, NMC with Google Derm on D7P surpasses all CL methods and even JOINT. However, their performance lag behind their MLP counterparts, reflecting their inability to adapt to complex or overlapping class boundaries typical of medical imaging and skin lesion data. By contrast, MLPs can learn more flexible decision boundaries in the embedding space, better leveraging the rich features of the frozen FM. These results suggest that the choice of classifier plays an important role in realizing the full potential of frozen foundation features in the CIL setting. Motivated by this, we explore enhancements for NMC-based models in the subsequent ablation studies.\n\nTable 1. Performance evaluation of our FM-based baselines and existing methods on three skin lesion classification benchmarks in the CIL setting. Numbers in parentheses next to replay- or generative-based methods indicate the number of stored or generated samples per old class, respectively. Green and blue cells denote the best and secondbest results, respectively.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">HAM</td><td colspan=\"2\">DMF</td><td colspan=\"2\">D7P</td></tr><tr><td>BAAC (â†‘)</td><td>F (â†“)</td><td>BAAC (â†‘)</td><td>F(â†“)</td><td>BAAC (â†‘)</td><td>F(â†“ï¼‰</td></tr><tr><td colspan=\"7\">ReferenceMethods</td></tr><tr><td>SINGLE</td><td>88.35</td><td>-</td><td>85.01</td><td>1</td><td>73.74</td><td></td></tr><tr><td>JOINT</td><td>82.13</td><td>=</td><td>80.66</td><td></td><td>68.32</td><td></td></tr><tr><td>SeqFT</td><td>51.54</td><td>50.76</td><td>37.21</td><td>55.25</td><td>36.51</td><td>52.18</td></tr><tr><td colspan=\"7\">CompetingMethods</td></tr><tr><td>EWC LWF DGM BIR</td><td>59.84 61.22 75.97</td><td>32.29 33.70 19.27 17.85</td><td>50.86 49.87 64.99 61.47</td><td>43.72 40.69 25.47</td><td>42.73 40.67 61.24</td><td>38.51 35.66 22.38</td></tr><tr><td>iCaRL (50) iCaRL (100)</td><td>74.39 70.80 73.27</td><td>18.44 14.97</td><td>64.32 68.49</td><td>19.18 20.17 18.27</td><td>62.90 60.84</td><td>19.65 24.78</td></tr><tr><td>RM (50)</td><td>73.61</td><td>16.83</td><td>63.73</td><td>16.73</td><td>63.72 63.05</td><td>19.28 22.57</td></tr><tr><td>RM (100)</td><td>76.32 78.15</td><td>15.92 11.09</td><td>70.14 72.51</td><td>15.22 14.21</td><td>65.87 68.04</td><td>20.17 17.58</td></tr><tr><td colspan=\"7\">Continual-Zoo Ours (Baseline1:FMwith MLP)</td></tr><tr><td>Google Derm</td><td>89.26</td><td>0</td><td>91.35</td><td>0</td><td></td><td></td></tr><tr><td>PanDerm</td><td>92.25</td><td>0</td><td>93.11</td><td>0</td><td>74.59 77.80</td><td>0 0</td></tr><tr><td colspan=\"7\">Ours (Baseline 2: FM with NMC)</td></tr><tr><td>Google Derm PanDerm</td><td>64.75 57.95</td><td>0 0</td><td>67.56 49.27</td><td>0 0</td><td>68.74 44.51</td><td>0 0</td></tr></table></body></html>\n\nAblation Studies. We conduct ablation studies to understand design choices in our approach: (1) exploring variants of the NMC classifier, and (2) evaluating the impact of replacing dermatology-specific FMs with general-purpose alternatives.\n\n1. NMC Classifier Variants. Table 2 reports the performance of several variants of the base NMC evaluated on HAM, DMF and D7P benchmarks. We begin with a straightforward yet effective enhancement: applying $\\ell _ { 2 }$ normalization to embeddings prior to centroid computation. This standardization consistently improves accuracy by better aligning the embedding space for distance-based decisions. For example, on DMF with Google Derm, accuracy increases from $6 7 . 5 6 \\%$ to $6 9 . 4 6 \\%$ . Next, we explore projection-based variants that transform embeddings before classification. Random projection [24] into a higher-dimensional Euclidean space yields limited gains; however, when combined with normalization, modest improvements are observed; for instance, PanDerm accuracy on DMF increases from $4 9 . 5 7 \\%$ to $5 4 . 3 8 \\%$ . The most substantial improvements arise from our learnable hyperbolic projection [17], which maps embeddings onto a hyperbolic manifold whose parameters are optimized during training. This projection explicitly captures hierarchical and relational structures among classes, adapting the embedding geometry to improve clustering and distance-based decision boundaries. The impact is significant: on HAM, accuracy rises from 64.75% to $8 1 . 4 1 \\%$ with the Google Derm model and from 57.95% to $8 0 . 2 4 \\%$ with PanDerm. Further, combining the hyperbolic projection with normalization boosts\n\nTable 2. Performance evaluation (balanced accuracy $\\%$ ) of different variations of the NMC classifier across three skin lesion classification benchmarks. Green cells denote the best results.   \n\n<html><body><table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">HAM</td><td colspan=\"2\">DMF</td><td colspan=\"2\">D7P</td></tr><tr><td>Derm</td><td>PanDerm</td><td>Derm</td><td>PanDerm</td><td>Derm</td><td>PanDerm</td></tr><tr><td>Base NMC (from Table1)</td><td>64.75</td><td>57.95</td><td>67.56</td><td>49.27</td><td>68.74</td><td>44.51</td></tr><tr><td>Base NMC+ Norm.</td><td>66.60</td><td>61.03</td><td>69.46</td><td>54.89</td><td>66.43</td><td>47.94</td></tr><tr><td>Random Projection</td><td>67.29</td><td>57.43</td><td>66.99</td><td>49.57</td><td>68.74</td><td>40.72</td></tr><tr><td>Random Projection + Norm.</td><td>66.11</td><td>62.06</td><td>66.20</td><td>54.38</td><td>68.46</td><td>47.27</td></tr><tr><td>Hyperbolic Projection</td><td>81.41</td><td>80.24</td><td>63.79</td><td>43.21</td><td>65.28</td><td>59.59</td></tr><tr><td>Hyperbolic Projection + Norm.</td><td>80.15</td><td>80.15</td><td>60.08</td><td>53.91</td><td>64.77</td><td>63.73</td></tr><tr><td>PCA</td><td>64.75</td><td>56.67</td><td>67.26</td><td>50.23</td><td>68.74</td><td>44.51</td></tr><tr><td>PCA+ Norm.</td><td>66.60</td><td>59.96</td><td>69.46</td><td>54.89</td><td>66.10</td><td>47.94</td></tr><tr><td>LDA</td><td>51.88</td><td>78.91</td><td>69.38</td><td>37.12</td><td>45.23</td><td>38.56</td></tr></table></body></html>\n\nTable 3. Performance evaluation (balanced accuracy $\\%$ ) of our FM-based baselines using a general-purpose foundation model (CLIP ViT-L/14). Green and blue cells denote the best and second-best results, respectively.   \n\n<html><body><table><tr><td>Method</td><td>HAM</td><td>DMF</td><td>ä¸€ D7P</td></tr><tr><td colspan=\"4\">OurBaselines</td></tr><tr><td>FMwithMLP</td><td>88.38</td><td>90.43</td><td>71.19</td></tr><tr><td>FM with NMC</td><td>53.53</td><td>70.13</td><td>46.01</td></tr><tr><td></td><td>NMC Classifier Variants</td><td></td><td></td></tr><tr><td>Base NMC + Norm. RandomProjection</td><td>55.11 52.15</td><td>71.26 69.99</td><td>46.52 43.79</td></tr><tr><td>Random Projection + Norm.</td><td>51.56</td><td>69.87</td><td>43.63</td></tr><tr><td>Hyperbolic Projection</td><td>80.05</td><td>53.50</td><td>59.59</td></tr><tr><td>Hyperbolic Projection + Norm.</td><td>80.05</td><td>57.20</td><td>60.10</td></tr><tr><td>PCA</td><td>53.53</td><td>70.13</td><td>45.86</td></tr><tr><td>PCA_+ Norm.</td><td>55.10</td><td>71.25</td><td>46.37</td></tr><tr><td>LDA</td><td>73.04</td><td></td><td></td></tr><tr><td></td><td></td><td>61.82</td><td>28.57</td></tr></table></body></html>\n\nPanDerm accuracy on D7P from $4 4 . 5 1 \\%$ to $6 3 . 7 3 \\%$ , yielding the strongest NMC results overall. While both the hyperbolic projection and the MLP classifier involve learnable parameters, they differ fundamentally. The MLP learns flexible, general mappings from embeddings to class predictions, requiring more extensive training. In contrast, the hyperbolic projection embeds data in a geometric space that models hierarchies, enhancing clustering and interpretability with fewer parameters and less risk of overfitting. We finally assess classical dimensionality reduction techniques. Principal component analysis (PCA), which preserves variance without explicitly optimizing class separability, does not improve performance, whereas Linear discriminant analysis (LDA), designed to maximize between-class variance, delivers mixed, unstable results: while it achieves $7 8 . 9 1 \\%$ on HAM with PanDerm, its performance deteriorates on other datasets due to the high intra-class variance. In summary, we conclude that normalization (as a non-learnable enhancement) and the hyperbolic projection (as a learnable enhancement) provide the most effective improvements to the NMC, each helping to narrow the gap to the MLP classifiers reported in Table 1 on different datasets. 2. General-Purpose vs. Domain-Specific FMs. To assess the importance of domain specialization, we repeat our experiments using a general-purpose FMâ€”CLIP ViT-L/14 pretrained on natural images, replacing the dermatologyspecific model. Results are shown in Table 3. Despite lacking domain-specific pretraining, CLIP embeddings remain highly effective for parametric classifiers: the MLP achieves $8 8 . 3 8 \\%$ , $9 0 . 4 3 \\%$ , and 71.19% on HAM, DMF, and D7P, respectively, outperforming all prior CL methods. This supports our central claim: strong, transferable FM features, regardless of domain, can improve performance in CIL. In contrast, NMC variants suffer significant degradation. The base NMC achieves only $5 3 . 5 3 \\%$ on HAM and $4 6 . 0 1 \\%$ on D7P, far below its dermatologyinitialized counterpart. Interestingly, while normalization and hyperbolic projection again improve performance (e.g., HAM jumps from $5 3 . 5 3 \\%$ to $8 0 . 0 5 \\%$ ), they cannot fully bridge the gap, and their gains are inconsistent across datasets. Hyperbolic projection $^ +$ normalization achieves a strong $6 0 . 1 0 \\%$ on D7P but still trails the MLP by more than $1 1 \\%$ . LDA continues to show erratic behavior: while it produces $7 3 . 0 4 \\%$ on HAM (competitive with more structured NMC variants), it collapses entirely on D7P (28.57%), underscoring its sensitivity to class imbalance and feature distributions. Overall, these findings reinforce two observations: (1) parametric models like MLPs can extract meaningful decision boundaries from general-purpose FMs, making them highly effective for CIL; and (2) for other approaches like NMC that lack task-specific adaptation, alignment between the pretraining and target domain remains crucial.\n\n# 5 Conclusions\n\nThis work demonstrates the clear advantage of leveraging frozen foundation models as class-incremental learners in dermatological image classification. Through systematic evaluation across three skin lesion benchmarks, we show that a simple approach, which is training a lightweight MLP on top of a frozen dermatologyspecific backbone, can surpass upper-bound reference methods, without requiring complex regularization, replay, or architectural modifications. Remarkably, this MLP-based strategy maintains strong performance when built on generalpurpose models like CLIP ViT-L/14, further reinforcing the value of rich, pretrained features in CL for medical applications. These findings yield three key insights. First, foundation models should be considered the default starting point for future research in CL. Second, nearest-mean classifiers still benefit substantially from domain-specific pretraining due to their limited representational flexibility. Third, our results emphasize the importance of aligning model design with the geometric properties of the embedding space. Specifically, incorporating inductive biases, such as learnable hyperbolic projections, can significantly close the gap between simple prototype-based classifiers and other learnable models while offering greater simplicity and interpretability. Taken together, we hope this work encourages the community to rethink the foundations of CL; i.e., shifting from building methods from scratch toward designing smarter, lighter learning systems that build on the strengths of powerful pretrained models. A promising future direction is to explore dynamic backbone adaptation and taskaware prompt tuning to further improve flexibility while retaining the benefits of strong pretrained representations.",
    "summary": "```json\n{\n  \"core_summary\": \"### ğŸ¯ æ ¸å¿ƒæ¦‚è¦\\n\\n> **é—®é¢˜å®šä¹‰ (Problem Definition)**\\n> *   è®ºæ–‡è§£å†³çš„æ ¸å¿ƒé—®é¢˜æ˜¯çš®è‚¤ç—…å˜åˆ†ç±»ä¸­çš„ç±»å¢é‡å­¦ä¹ ï¼ˆClass-Incremental Learning, CILï¼‰é—®é¢˜ï¼Œå³åœ¨ä¸æ–­å¼•å…¥æ–°ç–¾ç—…ç±»åˆ«çš„åŒæ—¶ï¼Œé¿å…å¯¹å·²å­¦ä¹ çŸ¥è¯†çš„é—å¿˜ã€‚\\n> *   è¯¥é—®é¢˜åœ¨ä¸´åºŠåº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œå› ä¸ºç°å®ä¸­çš„çš®è‚¤ç—…æ•°æ®å¾€å¾€æ˜¯åŠ¨æ€å¢åŠ çš„ï¼Œä¸”ç”±äºéšç§å’Œä¼¦ç†é™åˆ¶ï¼Œæ— æ³•å­˜å‚¨æˆ–é‡æ”¾æ—§æ‚£è€…æ•°æ®ã€‚\\n\\n> **æ–¹æ³•æ¦‚è¿° (Method Overview)**\\n> *   è®ºæ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ï¼ˆFoundation Model, FMï¼‰ä½œä¸ºå†»ç»“çš„ç‰¹å¾æå–å™¨ï¼Œå¹¶åœ¨å…¶ä¸Šå¢é‡è®­ç»ƒä¸€ä¸ªè½»é‡çº§MLPåˆ†ç±»å™¨æˆ–ä½¿ç”¨åŸå‹åˆ†ç±»å™¨ï¼ˆNMCï¼‰ã€‚\\n\\n> **ä¸»è¦è´¡çŒ®ä¸æ•ˆæœ (Contributions & Results)**\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹1ï¼š** å±•ç¤ºäº†å†»ç»“çš„åŸºç¡€æ¨¡å‹åœ¨çš®è‚¤ç—…å˜åˆ†ç±»çš„CILä»»åŠ¡ä¸­å¯ä»¥è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ— éœ€å¤æ‚çš„æ­£åˆ™åŒ–æˆ–é‡æ”¾æœºåˆ¶ã€‚\\n>     *   å…³é”®æ•°æ®ï¼šåœ¨HAM10000æ•°æ®é›†ä¸Šï¼ŒPanDerm+MLPçš„å¹³è¡¡å‡†ç¡®ç‡ï¼ˆBAACï¼‰è¾¾åˆ°92.25%ï¼Œæ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹2ï¼š** æ¢ç´¢äº†é›¶è®­ç»ƒçš„åŸå‹åˆ†ç±»å™¨ï¼ˆNMCï¼‰ï¼Œå¹¶é€šè¿‡å½’ä¸€åŒ–å’ŒåŒæ›²æŠ•å½±ç­‰æ”¹è¿›ï¼Œä½¿å…¶æ€§èƒ½æ¥è¿‘MLPåˆ†ç±»å™¨ã€‚\\n>     *   å…³é”®æ•°æ®ï¼šåœ¨HAM10000æ•°æ®é›†ä¸Šï¼ŒåŒæ›²æŠ•å½±+NMCçš„BAACä»57.95%æå‡åˆ°80.24%ã€‚\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹3ï¼š** éªŒè¯äº†é€šç”¨åŸºç¡€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨ç¼ºä¹é¢†åŸŸç‰¹å®šé¢„è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä»èƒ½é€šè¿‡MLPåˆ†ç±»å™¨å®ç°é«˜æ€§èƒ½ã€‚\\n>     *   å…³é”®æ•°æ®ï¼šCLIP+MLPåœ¨HAM10000æ•°æ®é›†ä¸Šçš„BAACä¸º88.38%ï¼Œä¼˜äºå¤§å¤šæ•°åŸºçº¿æ–¹æ³•ã€‚\\n> *   **åˆ›æ–°è´¡çŒ®ç‚¹4ï¼š** é€šè¿‡æ¶ˆèå®éªŒï¼Œè¯æ˜äº†å½’ä¸€åŒ–å’ŒåŒæ›²æŠ•å½±å¯¹NMCæ€§èƒ½çš„æå‡ä½œç”¨ã€‚\\n>     *   å…³é”®æ•°æ®ï¼šåœ¨DMFæ•°æ®é›†ä¸Šï¼ŒGoogle Derm+NMCçš„BAACä»67.56%æå‡åˆ°69.46%é€šè¿‡å½’ä¸€åŒ–ã€‚\",\n  \"algorithm_details\": \"### âš™ï¸ ç®—æ³•/æ–¹æ¡ˆè¯¦è§£\\n\\n> **æ ¸å¿ƒæ€æƒ³ (Core Idea)**\\n> *   è®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ï¼ˆFMï¼‰æä¾›çš„å¼ºå¤§ä¸”é€šç”¨çš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ï¼Œé€šè¿‡å†»ç»“å…¶å‚æ•°é¿å…ç¾éš¾æ€§é—å¿˜ï¼Œå¹¶åœ¨å…¶åŸºç¡€ä¸Šå¢é‡å­¦ä¹ è½»é‡çº§åˆ†ç±»å™¨ã€‚\\n> *   è¿™ç§æ–¹æ³•æœ‰æ•ˆçš„åŸå› æ˜¯FMåœ¨å¤§å‹æ•°æ®é›†ä¸Šé¢„è®­ç»ƒåï¼Œå·²ç»å­¦ä¹ åˆ°äº†ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤ºï¼Œèƒ½å¤Ÿæ³›åŒ–åˆ°æ–°çš„ç±»åˆ«å’Œä»»åŠ¡ã€‚\\n\\n> **åˆ›æ–°ç‚¹ (Innovations)**\\n> *   **ä¸å…ˆå‰å·¥ä½œçš„å¯¹æ¯”ï¼š** ä¼ ç»Ÿçš„CILæ–¹æ³•é€šå¸¸éœ€è¦å¤æ‚çš„æ­£åˆ™åŒ–ã€é‡æ”¾æˆ–æ¶æ„æ‰©å±•ï¼Œè€Œè¿™äº›æ–¹æ³•åœ¨ä¸´åºŠç¯å¢ƒä¸­å¯èƒ½ä¸åˆ‡å®é™…æˆ–æ•ˆç‡ä½ä¸‹ã€‚\\n> *   **æœ¬æ–‡çš„æ”¹è¿›ï¼š** æœ¬æ–‡æå‡ºäº†ä¸€ç§æç®€çš„æ–¹æ³•ï¼Œä»…éœ€å†»ç»“FMå¹¶å¢é‡è®­ç»ƒä¸€ä¸ªè½»é‡çº§MLPæˆ–ä½¿ç”¨åŸå‹åˆ†ç±»å™¨ï¼Œé¿å…äº†å­˜å‚¨æ—§æ•°æ®æˆ–è®¾è®¡å¤æ‚æœºåˆ¶çš„éœ€æ±‚ã€‚\\n\\n> **å…·ä½“å®ç°æ­¥éª¤ (Implementation Steps)**\\n> 1.  **ç‰¹å¾æå–ï¼š** ä½¿ç”¨é¢„è®­ç»ƒçš„FMï¼ˆå¦‚Google Dermæˆ–PanDermï¼‰ä½œä¸ºå†»ç»“çš„ç‰¹å¾æå–å™¨ï¼Œè¾“å…¥å›¾åƒ$x$å¾—åˆ°åµŒå…¥$z = F_\\\\theta(x)$ã€‚\\n> 2.  **MLPåˆ†ç±»å™¨è®­ç»ƒï¼š** å¯¹äºæ¯ä¸ªæ–°ä»»åŠ¡$t$ï¼Œè®­ç»ƒä¸€ä¸ªè½»é‡çº§MLPåˆ†ç±»å™¨$h_t$ï¼Œå…¶è¾“å‡ºä¸º$h_t(z) = \\\\text{Softmax}(W_3 \\\\cdot \\\\text{ReLU}(W_2 \\\\cdot \\\\text{ReLU}(W_1 z + b_1) + b_2) + b_3)$ã€‚\\n> 3.  **åŸå‹åˆ†ç±»å™¨ï¼ˆNMCï¼‰ï¼š** å¯¹äºæ¯ä¸ªæ–°ç±»åˆ«$c$ï¼Œè®¡ç®—å…¶åŸå‹$\\\\mu_c$ä½œä¸ºè¯¥ç±»æ‰€æœ‰è®­ç»ƒæ ·æœ¬åµŒå…¥çš„å¹³å‡å€¼ï¼Œåˆ†ç±»æ—¶é€‰æ‹©æœ€è¿‘çš„åŸå‹$\\\\hat{y} = \\\\arg\\\\min_{c} \\\\|z - \\\\mu_c\\\\|_2$ã€‚\\n> 4.  **æ”¹è¿›NMCï¼š** é€šè¿‡åµŒå…¥å½’ä¸€åŒ–ã€éšæœºæŠ•å½±æˆ–åŒæ›²æŠ•å½±ç­‰æŠ€å·§æå‡NMCçš„æ€§èƒ½ã€‚\\n\\n> **æ¡ˆä¾‹è§£æ (Case Study)**\\n> *   è®ºæ–‡æœªæ˜ç¡®æä¾›æ­¤éƒ¨åˆ†ä¿¡æ¯ã€‚\",\n  \"comparative_analysis\": \"### ğŸ“Š å¯¹æ¯”å®éªŒåˆ†æ\\n\\n> **åŸºçº¿æ¨¡å‹ (Baselines)**\\n> *   è®ºæ–‡å¯¹æ¯”äº†å¤šç§åŸºçº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬SINGLEï¼ˆæ¯ä¸ªä»»åŠ¡å•ç‹¬è®­ç»ƒæ¨¡å‹ï¼‰ã€JOINTï¼ˆå¤šä»»åŠ¡è”åˆè®­ç»ƒï¼‰ã€SeqFTï¼ˆé¡ºåºå¾®è°ƒï¼‰ã€æ­£åˆ™åŒ–æ–¹æ³•ï¼ˆEWCã€LwFï¼‰ã€ç”Ÿæˆæ–¹æ³•ï¼ˆDGMã€BIRï¼‰ã€é‡æ”¾æ–¹æ³•ï¼ˆiCaRLã€RMï¼‰ä»¥åŠContinual-Zooï¼ˆåŸºäºå†»ç»“æ¨¡å‹çš„é›†æˆæ–¹æ³•ï¼‰ã€‚\\n\\n> **æ€§èƒ½å¯¹æ¯” (Performance Comparison)**\\n> *   **åœ¨å¹³è¡¡å‡†ç¡®ç‡ï¼ˆBAACï¼‰ä¸Šï¼š** æœ¬æ–‡æ–¹æ³•ï¼ˆPanDerm+MLPï¼‰åœ¨HAM10000æ•°æ®é›†ä¸Šè¾¾åˆ°äº†92.25%ï¼Œæ˜¾è‘—ä¼˜äºæœ€ä½³åŸºçº¿Continual-Zooï¼ˆ78.15%ï¼‰å’Œé‡æ”¾æ–¹æ³•RMï¼ˆ76.32%ï¼‰ã€‚ä¸è¡¨ç°æœ€ä½³çš„åŸºçº¿ç›¸æ¯”ï¼Œæå‡äº†14.1ä¸ªç™¾åˆ†ç‚¹ã€‚\\n> *   **åœ¨é—å¿˜åº¦é‡ï¼ˆFï¼‰ä¸Šï¼š** æœ¬æ–‡æ–¹æ³•ï¼ˆPanDerm+MLPï¼‰çš„é—å¿˜åº¦é‡ä¸º0ï¼Œè¡¨æ˜å®Œå…¨æ²¡æœ‰é—å¿˜ï¼Œè€Œæ­£åˆ™åŒ–æ–¹æ³•EWCçš„é—å¿˜åº¦é‡ä¸º32.29ï¼Œé‡æ”¾æ–¹æ³•RMçš„é—å¿˜åº¦é‡ä¸º15.92ã€‚\\n> *   **åœ¨é›¶è®­ç»ƒåœºæ™¯ä¸‹ï¼š** æœ¬æ–‡çš„NMC+åŒæ›²æŠ•å½±åœ¨HAMæ•°æ®é›†ä¸Šè¾¾åˆ°äº†81.41%çš„BAACï¼Œä¼˜äºéšæœºæŠ•å½± (67.29%) å’ŒPCA (64.75%)ã€‚\\n> *   **åœ¨é€šç”¨åŸºç¡€æ¨¡å‹ä¸Šï¼š** CLIP+MLPåœ¨HAM10000æ•°æ®é›†ä¸Šçš„BAACä¸º88.38%ï¼Œä¼˜äºå¤§å¤šæ•°åŸºçº¿æ–¹æ³•ã€‚\",\n  \"keywords\": \"### ğŸ”‘ å…³é”®è¯\\n\\n*   ç±»å¢é‡å­¦ä¹  (Class-Incremental Learning, CIL)\\n*   æŒç»­å­¦ä¹  (Continual Learning, CL)\\n*   åŸºç¡€æ¨¡å‹ (Foundation Model, FM)\\n*   çš®è‚¤ç—…å˜åˆ†ç±» (Dermatological Image Classification, N/A)\\n*   åŸå‹åˆ†ç±»å™¨ (Nearest-Mean Classifier, NMC)\\n*   åŒæ›²æŠ•å½± (Hyperbolic Projection, N/A)\\n*   åŒ»ç–—å½±åƒåˆ†æ (Medical Image Analysis, N/A)\\n*   å½’ä¸€åŒ– (Normalization, N/A)\"\n}\n```"
}