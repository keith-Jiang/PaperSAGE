{
    "source": "ArXiv (Semantic Scholar未收录)",
    "arxiv_id": "2507.14032",
    "link": "https://arxiv.org/abs/2507.14032",
    "pdf_link": "https://arxiv.org/pdf/2507.14032.pdf",
    "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models",
    "authors": [
        "Lam Nguyen",
        "Erika Barcelos",
        "Roger French",
        "Yinghui Wu"
    ],
    "categories": [
        "cs.AI"
    ],
    "publication_date": "未找到提交日期",
    "venue": "暂未录入Semantic Scholar",
    "fields_of_study": "暂未录入Semantic Scholar",
    "citation_count": "暂未录入Semantic Scholar",
    "influential_citation_count": "暂未录入Semantic Scholar",
    "institutions": [
        "Case Western Reserve University"
    ],
    "paper_content": "# KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models\n\nLam Nguyen, Erika Barcelos, Roger French, and Yinghui Wu\n\nCase Western Reserve University, Cleveland OH 44106, USA {ltn18,eib14,rxf131,yxw1650}@case.edu\n\nAbstract. Ontology Matching (OM) is a cornerstone task of semantic interoperability, yet existing systems often rely on handcrafted rules or specialized models with limited adaptability. We present KROMA, a novel OM framework that harnesses Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG) pipeline, to dynamically enrich the semantic context of OM tasks with structural, lexical, and definitional knowledge. To optimize both performance and efficiency, KROMA integrates a bisimilarity-based concept matching and a lightweight ontology refinement step, which prune candidate concepts and substantially reduce the communication overhead from invoking LLMs. Through experiments on multiple benchmark datasets, we show that integrating knowledge retrieval with context-augmented LLMs significantly enhances ontology matching—outperforming both classic OM systems and cutting-edge LLM-based approaches—while keeping communication overhead comparable. Our study highlights the feasibility and benefit of the proposed optimization techniques (targeted knowledge retrieval, prompt enrichment, and ontology refinement) for ontology matching at scale. Code is available at: https://anonymous.4open. science/r/kroma/\n\nKeywords: Ontology Matching $\\cdot \\cdot$ Large Language Models · Retrieval Augmented Generation\n\n# 1 Introduction\n\nOntologies have been routinely developed to unify and standardize knowledge representation to support data-driven applications. They allow researchers to harmonize terminologies and enhance knowledge and sharing in their fields. Ontologies can be classified according to their level of specificity, ranging from more abstract, general-purpose ontologies such as Basic Formal Ontology (BFO)[38] or DOLCE[6] down to more domain-oriented ‘mid-level” ones, such as CheBi [14] in chemistry, Industrial Ontology Foundy (IOF) [17] or Common Core Ontology(CCO) [27]. Domain ontologies are data-driven, task-specific “low-level” ontologies, containing concepts from domain-specific data, such as Materials Data Science Ontology (MDS-Onto) [46]. To achieve broad usability, ontologies need to be aligned for better interoperability via ontology matching.\n\nOntology matching has been studied to find correspondence between terms that are semantically equivalent [52]. It is a cornerstone task to ensure semantic interoperability among terms originated from different sources. Ontology matching methods can be categorized to rule-based or structural-based (graph pattern or path-based) matching [10], matching with semantic similarity, machine learning-based approaches and hybrid methods. Linguistic (terminological) methods are often used for ontology matching tasks, ranging from simple string matching or embedding learning to advance counterparts based on natural language processing (NLP). Nevertheless, conventional rule- or structural-based methods are often restricted to certain use cases and hard to be generalized for new or unseen concepts. Learning-based approaches may on the other hand require high re-training process, for which abundant annotated or training data remains a luxury especially for e.g., data-driven scientific research.\n\nMeanwhile, the emerging Large Language Models (LLMs) have demonstrated remarkable versatility for NL understanding. LLMs are trained on vast and diverse corpora, endowing them with an understanding of language nuances and contextual subtleties. Extensive training enables them to capture semantic relationships and abstract patterns that are critical for aligning concepts across different data sources. A missing yet desirable opportunity is to investigate whether and how LLMs can be engaged to automate and improve ontology matching.\n\nThis paper introduces KROMA, a novel framework that exploits LLMs to enhance ontology matching. Unlike existing LLM-based methods that typically “outsource” ontology matching to LLMs with direct prompting (which may have low confidence and risk of hallucination), KROMA maintain a set of conceptually similar groups that are co-determined by concept similarity and LLMs, both guided by their “context” obtained via a runtime knowledge retrieval process. Moreover, the groups are further refined by a global ontological equivalence relation that incorporate structural equivalence.\n\nContributions. Our main contributions are summarized below.\n\n(1) We propose a formulation of semantic equivalence relation in terms of a class of bisimilar equivalence relation, and formally define the ontology structure, called concept graph, to be maintained (Sections 2 and 3). We justify our formulation by showing the existence of an “optimal” concept graph with minimality and uniqueness guarantee, subject to the bisimilar equivalence.\n\n(2) We introduce KROMA, an LLM-enhanced ontology matching framework (Section 4). KROMA fine-tunes LLMs with prompts over enriched semantic contexts. Such contexts are obtained from knowledge retrieval, referencing highquality, external knowledge resources.\n\n(3) KROMA supports both offline and online matching, to “cold-start” from scratch, and to digest terms arriving from a stream of data, respectively. We introduce efficient algorithms to correctly construct and maintain the optimal concept graphs (Section 5). (a) The offline refinement performs a fast grouping process guided by the bisimilar equivalence, blending concept equivalence tests co-determined by semantic closeness and LLMs. (b) The online algorithm effectively incrementalizes its offline counterpart with fast delay time for continuous concept streams. Both algorithms are in low polynomial time, with optimality guarantee on the computed concept graphs.\n\n(4) Using benchmarking ontologies and knowledge graphs, we experimentally verify the effectiveness and efficiency of KROMA (Section 6). We found that KROMA outperforms existing LLM-based methods by $1 0 . 9 5 \\%$ on average, and the optimization of knowledge retrieval and refinement improves its accuracy by $6 . 6 5 \\%$ and $2 . 6 8 \\%$ , respectively.\n\nRelated Work. We summarize related work below.\n\nLarge Language Models. Large language models (LLMs) have advanced NLP by enabling parallel processing and capturing complex dependencies [53,48], which have scaled from GPT-1’s 117M [44], GPT-2’s 1.5B [45] to GPT-3’s 175B [35] and GPT-4’s 1.8T parameters [36]. Open-source models like Llama have grown to 405B [32], with Mistral Large (123B) [33] and DeepSeek V3 (671B) [12] also emerging. Recent advances in specialized reasoning LLMs (RLLMs) such as OpenAI’s O1 and DeepSeek R1 have further propelled Long Chain-of-Thought reasoning—shifting from brief, linear Short CoT to deeper, iterative exploration, and yielded substantial gains in multidisciplinary tasks [37,13,47,50,8,42,56,30]. Ontology Matching with LLMs. Several methods have been developed to exploit LLMs for ontology matching. Early work focused on direct prompting LLMs for ontology matching. For example, [39] frame product matching as a yes/no query, and [34] feed entire source and target ontologies into ChatGPT—both achieving high recall on small OAEI conference-track tasks but suffering from low precision. Beyond these “direct-prompt” approaches, state-of-the-art LLM-OM systems fall into two main categories: (1) retrieval-augmented pipelines, which first retrieve top- $k$ candidates via embedding-based methods and then refine them with LLM prompts (e.g. LLM4OM leverages TF–IDF and SBERT retrievers across concept, concept-parent, and concept-children representations [20], while MILA adds a prioritized depth-first search step to confirm high-confidence matches before any LLM invocation [49]); and (2) prompt-engineering systems, which generate candidates via a high-precision matcher or inverted index and then apply targeted prompt templates to LLMs in a single step (e.g. OLaLa embeds SBERT candidates into MELT’s prompting framework with both independent and multi-choice formulations [25], and LLMap uses binary yes/no prompts over concept labels plus structural context with Flan-T5-XXL or GPT-3.5 [23]).\n\n# 2 Ontologies and Ontology Matching\n\nOntologies. An ontology $O$ is a pair $( C , E )$ , where $C$ is a finite set of concept names, and $E \\subset C \\times C$ is a set of relations. An ontology has a graph representation with a set of concept nodes $C$ , and a set of edges $E$ . We consider ontologies as directed acyclic graphs (DAGs).\n\nIn addition, each concept node (or simply “node”) $c$ in $O$ carries the following auxiliary structure. (1) The rank of a node $c \\in C$ is defined as: (a) $r ( c ) = 0$ if $c$ has no child, otherwise, (b) $r ( c ) = \\operatorname* { m a x } ( r ( c ^ { \\prime } ) + 1 )$ for any child $c ^ { \\prime }$ of $c$ in $O$ .\n\n<html><body><table><tr><td>Notation</td><td>Description</td></tr><tr><td>0 = (C,E)</td><td>ontology O, C: set of concepts,E:set of relations</td></tr><tr><td>[0|</td><td>size of ontology O;|O|= |C] +E</td></tr><tr><td>r(c)</td><td>rank of concept node c</td></tr><tr><td>c.I</td><td>ground set of concept node c</td></tr><tr><td>Os=(Cs,Es),Ot=(Ct,Et)</td><td>sourceand target ontology,respectively</td></tr><tr><td>R~</td><td>ontological equivalence relation</td></tr><tr><td>C</td><td>equivalence partition of concept set Cs U Ct</td></tr><tr><td>9o = (Vo,Eo)</td><td>concept graph Go,Vo:set of nodes,Eo:set of edges</td></tr><tr><td>△g</td><td>newlyarrive edges;edgeupdates in g</td></tr><tr><td>[c] ∈Vo</td><td>an equivalence class in C</td></tr><tr><td>M</td><td>a language model</td></tr><tr><td>M</td><td>set of Large Language Model(s)</td></tr><tr><td>q</td><td>aprompt query</td></tr><tr><td>q(M)</td><td>a natural language answer from language model M</td></tr><tr><td>F(c,c')</td><td>concept similarity between two concepts c and c'</td></tr><tr><td>q(c,c',M)</td><td>anaturallanguage answer to Masking ifc~c'</td></tr><tr><td>α∈(0,1]</td><td>threshold for asserting concept similarity</td></tr><tr><td>W=(Os,Ot,F,M,α)</td><td>configuration input for ontology matching</td></tr><tr><td>Zc</td><td>embedding of concept c</td></tr><tr><td>S</td><td>set of candidate concept pairs with similarity scores</td></tr><tr><td>C</td><td>top-k pairs with highest similarity scores</td></tr></table></body></html>\n\nTable 1: Summary of Notations.\n\n(2) A ground set $c . \\mathcal { Z }$ , refers to a set of entities from e.g., external ontologies or knowledge bases that can be validated to belong to the concept $c$ .\n\nOntology Matching. Given a source ontology $O _ { s } = ( C _ { s } , E _ { s } )$ and a target ontology $O _ { t } = ( C _ { t } , E _ { t } )$ , an ontological equivalence relation $R _ { \\simeq } \\subseteq C _ { s } \\times C _ { t }$ is an equivalence relation that contains pairs of nodes $( c , c ^ { \\prime } )$ that are considered to be “semantically equivalent”. The relation $R _ { \\simeq }$ induces an equivalence partition $\\boldsymbol { \\mathcal { C } }$ of the concept set $C _ { s } \\cup C _ { t }$ , such that each partition is an equivalence class that contains a set of pairwise equivalent concepts in $C _ { s } \\cup C _ { t }$ .\n\nConsistently, we define a concept graph $\\mathcal { G } _ { O } = ( V _ { O } , E _ { O } )$ as a DAG with a set of nodes $V _ { O }$ , where each node $[ c ] \\in V _ { O }$ is an equivalence class in $\\mathcal { C }$ , and there exists an edge $( [ c ] , [ c ^ { \\prime } ] ) \\in E _ { O }$ if and only if there exists an edge $( c , c ^ { \\prime } )$ in $E _ { s }$ or in $E _ { t }$ . A concept graph $\\scriptstyle { \\mathcal { G } } _ { O }$ can be a multigraph: there may exist multiple edges of different relation names between two equivalent classes.\n\nGiven $O _ { s }$ and $O _ { t }$ , the task of ontology matching is to properly formulate $R _ { \\simeq }$ and compute $\\boldsymbol { \\mathscr { C } }$ induced by $R _ { \\simeq }$ over $O _ { s }$ and $O _ { t }$ ; or equivalently, compute the concept graph $\\scriptstyle { \\mathcal { G } } _ { O }$ induced by $R _ { \\simeq }$ .\n\nExample 1. Figure 1 depicts two ontologies $O _ { s }$ and $O _ { t }$ as DAGs, involving in total 9 concept nodes. An equivalence relation may suggest that “mammal”, “animal”, “organism” and “vertebrate” are pairwise similar; and similarly for the sets {“house pet”, “carnivora” and “canine” $\\left. \\begin{array} { r l } \\end{array} \\right\\} \\qquad \\mathrm { ~ }$ , and {“wolfdog, “coyote”}”. This induces a concept graph $\\mathcal { G } _ { o }$ as shown on the top right as a result of ontology matching, with three equivalence classes.\n\n![](images/055ddc5315677015e72a9997aefa0413b95789a001b879affd873bd10f355b17.jpg)  \nFig. 1: Ontologies with ground sets, Ontology Matching and Concept Graphs\n\nLanguage Models for Ontology Matching. A language model $M$ takes as input a prompt query $q$ , and generate an answer $q ( M )$ , typically an NL statement, for downstream processing. Large language models (LLMs) are foundation models that can effectively learn from a handful of in-context examples, included in a prompt query $q$ , that demonstrate input–output distribution [54].\n\nPrompt query. A prompt query $q$ is in a form of NL statements that specifies (1) a task definition $\\tau$ with input and output statement; (2) a set of in-context examples $\\mathcal { D }$ with annotated data; (3) a statement of query context $Q$ , which describe auxiliary query semantics; and optionally (4) specification on output format, and (5) a self-evaluation of answer quality such as confidence. An evaluation of a prompt query $q$ invokes an LLM $M$ to infer a query result $q ( M )$ .\n\nWe specify a prompt query $q$ and LLMs for ontology matching. A prompt query $q$ is in the form of $q ( c , c ^ { \\prime } )$ , which asks “are $c$ and $c ^ { \\prime }$ semantically equivalent?” An LLM $M$ can be queried by $q ( c , c ^ { \\prime } )$ and acts as a Boolean “oracle” with “yes/no” answer. An LLM is deterministic, if it always generate a same answer for the same prompt query. We consider deterministic LLMs, as in practice, such LLMs are desired for consistent and robust performance.\n\n# 3 Ontology Matching with LLMs\n\nIn this section, we provide a pragmatic characterization for the ontological equivalence relation $R _ { \\simeq }$ . We then formulate the ontology matching problem.\n\n# 3.1 Semantic Equivalence: A Characterization\n\nConcept similarity. A variety of methods have been proposed to decide if two concepts are equivalent [7]. KROMA by default uses a Boolean function $F$ defined by a weighted combination of a semantic closeness metric $\\sin ^ { 1 }$ and the result from a set of LLMs $\\mathcal { M }$ (see Section 4).\n\n$$\nF ( c , c ^ { \\prime } ) = \\gamma \\mathsf { s i m } ( c , c ^ { \\prime } ) + ( 1 - \\gamma ) q ( c , c ^ { \\prime } , \\mathcal { M } )\n$$\n\nwhere $q$ is a prompt query that specifies the context of concept equivalence for LLMs, and $\\gamma$ be a configurable parameter. KROMA supports a built-in library of semantic similarity functions sim, including (a) string similarity, feature and information measure [40], or normalized distances (NGDs) [28]; and (b) a variety of LLMs such as GPT-4o Mini (OpenAI) [36], LLaMA-3.3 (Meta AI) [31], and Qwen-2.5 (Qwen Team) [43].\n\nOntological Bisimilarity. We next provide a specification of the ontological equivalence relation, notably, ontological bisimilarity, denoted by the same symbol $R _ { \\simeq }$ for simplicity. Given a source ontology $O _ { s } = ( C _ { s } , E _ { s } )$ , and a target ontology $O _ { t } = ( C _ { t } , E _ { t } )$ , we say a pair of nodes $c _ { s } \\in C _ { s }$ and $c _ { t } \\in C _ { t }$ are ontologically bisimilar, denoted as $( c _ { s } , c _ { t } ) \\in R _ { \\simeq }$ , if and only if there exists a non-empty binary relation $R _ { \\simeq }$ , such that: (1) $c _ { s }$ and $c _ { t }$ are concept similar, i.e., $F ( c _ { s } , c _ { t } ) \\ge \\alpha$ , for a threshold $\\alpha$ ; (2) for every edge $( c _ { s } ^ { \\prime } , c _ { s } ) \\in E _ { s }$ , there exists an edge $( c _ { t } ^ { \\prime } , c _ { t } ) \\in E _ { t }$ , such that $( c _ { s } ^ { \\prime } , c _ { t } ^ { \\prime } ) \\in R _ { \\simeq }$ , and vice versa; and (3) for every edge $( c _ { s } , c _ { s } ^ { \\prime \\prime } ) \\in E _ { s }$ , there exists an edge $( c _ { t } , c _ { t } ^ { \\prime \\prime } ) \\in E _ { t }$ , such that $( c _ { s } ^ { \\prime \\prime } , c _ { t } ^ { \\prime \\prime } ) \\in R _ { \\simeq }$ .\n\nOne can verify the following result.\n\nLemma 1. The ontological bisimilar relation $R _ { \\simeq }$ is an equivalence relation.\n\nWe can prove the above results by verifying that $R _ { \\simeq }$ is reflexive, symmetric and transitive over the concept set $C _ { s } \\cup C _ { t }$ , ensured by the transitivity of concept similarity and by definition. Observe that two nodes that are concept similar may not be ontologically bisimilar. On the other hand, two ontologically bisimilar entities must be concept similar, by definition.\n\n# 3.2 Problem Statement\n\nWe now formulate our ontological matching problem. Given a configuration $W =$ $( O _ { s } , O _ { t } , F , \\mathcal { M } , \\alpha )$ that specifies as input a source ontology $O _ { s }$ , a target ontology $O _ { t }$ , a Boolean function $F$ and threshold $\\alpha \\in ( 0 , 1 ]$ that asserts concept similarity, and a set of LLMs $\\mathcal { M }$ , the problem is to compute a smallest concept graph $\\scriptstyle { \\mathcal { G } } _ { O }$ induced by the ontologically bisimilar equivalence $R _ { \\simeq }$ .\n\nWe justify the above characterization by proving that there exists an “optimal”, invariant solution encoded by a concept graph $\\scriptstyle { \\mathcal { G } } _ { O }$ . To see this, we provide a minimality and uniqueness guarantee on $\\scriptstyle { \\mathcal { G } } _ { O }$ .\n\n![](images/3f377688c8b6c0df31059186c453d64c5ebc1ab9c363485ddb4f792ea2d8b465.jpg)  \nFig. 2: KROMA Framework Overview: Major Components and Dataflow\n\nLemma 2. Given a configuration $W$ and semantic equivalence specified by the ontological bisimilar relation $R _ { \\simeq }$ , there is a unique smallest concept graph $\\scriptstyle { \\mathcal { G } } _ { O }$ that capture all semantically equivalent nodes in terms of $R _ { \\simeq }$ .\n\nProof sketch: We show that the above result holds by verifying the following. (1) There is a unique, maximum ontological bisimilar relation $R _ { \\simeq }$ for a given configuration $W = ( O _ { s } , O _ { t } , F , \\mathcal { M } , \\alpha )$ , where any LLM in $\\mathcal { M }$ is a deterministic model for the same prompt query $q$ generated consistently from $W$ . This readily follows from Lemma 1, which verifies that $R _ { \\simeq }$ is an equivalence relation. (2) Let the union of $O _ { s }$ and $O _ { t }$ be a graph $O _ { s t } = \\{ C _ { s } \\cup C _ { t } , E _ { s } \\cup E _ { t } \\}$ . By setting $\\scriptstyle { \\mathcal { G } } _ { O }$ as the quotient graph induced by the largest ontological bisimilar relation $R _ { \\simeq }$ over $O _ { s t }$ , $\\scriptstyle { \\mathcal { G } } _ { O }$ contains the smallest number of equivalent classes (nodes) and edges. This can be verified by proof by contradiction. (3) The uniqueness of the solution can be verified by showing that $R _ { \\simeq }$ induces only one unique partition $\\boldsymbol { \\mathcal { C } }$ and results in a concept graph $\\scriptstyle { \\mathcal { G } } _ { O }$ up to graph isomorphism. In other words, for any two possible concept graphs induced by $R _ { \\simeq }$ , they are isomorphic to each other. $\\sqcup$\n\nThe above analysis suggests that for a configuration $W$ , it is desirable to compute such an optimal concept graph as an invariant, stable result with guarantees on sizes and uniqueness on topological structures. We next introduce KROMA and efficient algorithms to compute such optimal concept graphs.\n\n# 4 KROMA Framework\n\n# 4.1 Framework Overview\n\nGiven a configuration $W = ( O _ { s } , O _ { t } , F , \\mathcal { M } )$ where $\\mathcal { M }$ is a set of pre-trained LLMs $\\mathcal { M }$ , KROMA has the following major functional modules that enables a multisession ontology matching process.\n\nConcept Graph Initialization Upon receiving two ontologies $O _ { s } = ( C _ { s } , E _ { s } )$ and $O _ { t } = ( C _ { t } , E _ { t } )$ , KROMA initializes the concept graph $\\mathcal { G } _ { O } = ( V , E )$ with $V =$\n\n$C _ { S } \\cup C _ { T }$ and $E = E _ { S } \\cup E _ { T }$ . For each concept $c \\in V$ , KROMA computes the rank $r ( c )$ as described in Section 2.\n\nKnowledge Retrieval. To assemble a rich, yet compact, context for each concept $c \\in V$ , we perform: (1) Neighborhood Sampling: traverse up to its two hops in $\\scriptstyle { \\mathcal { G } } _ { O }$ to collect parents, children, and “sibling” concepts of $c$ , creating a node induced subgraph centered at $c$ ; (2) subgraph parameterization: Sample and substitute constant values from the subgraph with variables to create SPARQL queries $S _ { q }$ ; and (3) ground set curation: process $S _ { q }$ onto external knowledge bases to augment its ground set $c . \\mathcal { Z }$ with auxiliary information (e.g., relevant entities, definition, labels, etc.).\n\nEmbedding Generation. In this phase, KROMA consults a built-in library of semantic similarity functions $\\sin ( \\cdot , \\cdot )$ and embedding functions embd $( \\cdot )$ . For each concept $c \\in V$ , KROMA computes:\n\n$$\nz _ { c } = \\alpha \\mathsf { e m b d } _ { g r a p h } ( c ) + ( 1 - \\alpha ) \\mathsf { e m b d } _ { t e x t } ,\n$$\n\nwhere embdgraph (e.g. node2vec [22]) captures the $c$ ’s topology information and $\\mathsf { e m b d } _ { t e x t }$ (e.g. SciBERT [5]) encodes $c$ ’s textual context. After obtaining the necessary embeddings $( z _ { c } \\forall c \\in V )$ , KROMA computes pairwise concept similarity between source and target ontologies using sim functions:\n\n$$\n\\mathbb { S } = \\{ ( c _ { s } , c _ { t } , s c o r e _ { s , t } ) | c _ { s } \\in C _ { S } , c _ { t } \\in C _ { T } , s c o r e _ { s , t } = \\operatorname { s i m } ( z _ { c _ { s } } , z _ { c _ { t } } ) \\}\n$$\n\nFrom ${ \\mathbb S }$ , we select the top- $k$ pairs with the highest similarity scores (i.e. in descending order of $s c o r e _ { s , t }$ ), yielding the candidate list $\\mathbb { C }$ .\n\nExample 2. We revisit Example 1. (1) A knowledge retrieval for node “house pet” samples its neighbors and issues a set of “star-shaped” SPARQL queries centered at “house pet” to query an underlying knowledge graph $K G$ . This enriches its ground set with a majority of herbivore or omnivorous pets that are not “carnivore”. Similarly, the ground set of “carnivore” is enriched by “wolf”, unlikely a house pet. Despite “coyote” and “wolfdog” (house pet) alone are less similar, the ground set of “coyote” turns out to be a set of canine pets e.g., “husky” that are “coyote-like”, hence similar with “wolfdog”. (2) The embedding generation phase incorporates enriched ground sets and generate embeddings accordingly, which scores that distinguishes “house pet” from “carnivore” due to embedding difference, and assert “coyote” and “wolfdog” to be concept similar, hence a candidate pair to be “double checked” by LLMs in the next phase.\n\nPrompt Querying LLMs. For each candidate pair $( c _ { s } , c _ { t } ) \\in \\mathbb { C }$ , KROMA automatically generates an NL prompt that includes: (1) Task description $\\tau$ (e.g. “Given two ontology concepts and their contextual metadata, decide if they are related or not.”), (2) In-context examples $\\mathcal { D }$ containing both positive and negative matches, (3) Query context for $c _ { s }$ and $c _ { t }$ including their ground sets, (4) Output format and confidence (e.g., “Answer Yes or No, and provide a confidence score between 0 and 10.”). It then calls (a set of) LLMs $\\mathcal { M }$ to obtain a match decision with confidence. Low-confidence or conflicting outputs are routed to validation module. A query template and a generated example is illustrated in Figure 3.\n\nTask Description Task Description Giventwoconcepts:sourceandtarget.Determineif theyare Giventwoontologyconcepts:heatwaveanddrought.Determine semanticallyequivalentornot. iftheyaresemanticallyequivalentornot. Examples Examples Example1:urban flooding (parents $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ [hydrological Example1:sourcel (+metadata) issemantically hazard],...)is semantically equivalent to city equivalent totarget1 (+metadata) flood (parents=[floodevent]....) Example2:source2 (+metadata) isnot semantically Example2:soil erosion (parent $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ [landdegradation], equivalent to target2 (+ metadata)   \n3moreexamples... (parents=[masswasting],..)   \n3moreexamples... Query Context QueryContext Sourcecontext:heatwave (parents $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ [extremeweather], + Sourcecontext: source (parents=[...],children $\\mathbf { \\tau } = \\left[ \\ \\dots \\right] ,$ childrer $\\mathbf { \\sigma } = \\mathbf { \\sigma }$ [publichealthmergency],synonyms=[etm synonyms=[...],definition=..\") heat],iio $\\mathbf { \\omega } = \\mathbf { \\omega } _ { \\cdots } \\mathbf { \\omega } _ { \\mathbf { \\omega } }$ Targetcontext:target (parent=[..],children=[..], Targetcontext:drought (parent=[climaticanomaly], synonyms=[..],efinition=..\") children=[cropfailure],synonyms=[prolongeddryness], definition=\"...\") OutputFormat OutputFormat <ans>Yes/No</ans> <ans>Yes/No</ans> Confidence Confidence <conf>0-10</conf> <conf>0-10</conf>\n\nOntology Refinement $\\boldsymbol { \\mathcal { E } }$ Expert Validation. KROMA next invokes a refinement process, OfflineRefine (Algorithm 1), to “cold-start” the construction of $\\scriptstyle { \\mathcal { G } } _ { O }$ , or OnlineRefine (Algorithm 2), to incrementally refine $\\scriptstyle { \\mathcal { G } } _ { O }$ for any unseen, newly arrived concept nodes or edges. Any pair of nodes whose structural ranks remain in “conflict” is routed into a set of queries for expert validation (see Algorithm 2, Section 4); once approved, are integrated back into $\\scriptstyle { \\mathcal { G } } _ { O }$ . An active sampling strategy is applied, to select pairs of nodes that have low confidence from LLMs for expert validation, to reduce the manual effort.\n\nExample 3. Continuing with Example 2, as “golden retriever” and “coyote” are asserted by the function $F$ that combines the descision of semantic similarity function sim and LLMs, an equivalence class is created in $\\mathcal { G } _ { O } ^ { \\prime }$ . As “house pet” and “carnivore” has quite different embedding considering the features from themselves and their ground sets, “carnivore” is separated from the group of “house pet”. This suggests further that “organism” now has a different context that distinguish it from the group $\\{$ {mammal, animal, vertebrate}, by the definition of bisimilarity equivalence. This leads to a finer-grained concept graph $\\mathcal { G } _ { O } ^ { \\prime }$ .\n\n# 5 Ontology Refinement\n\nWe next describe our ontology refinement algorithms. KROMA supports ontology refinement in two modes. The offline mode assumes that the source ontology $O _ { s }$ and the target ontology $O _ { t }$ are known, and performs a batch processing to compute the concept graph $\\scriptstyle { \\mathcal { G } } _ { O }$ from scratch. The online refinement incrementally maintains $\\scriptstyle { \\mathcal { G } } _ { O }$ upon a sequence of (unseen) triples (edges) from external resources.\n\nThe offline refinement algorithm is outlined as Algorithm 1. (1) It starts by initializing $\\scriptstyle { \\mathcal { G } } _ { O }$ (lines 1-6) as the union of $O _ { s }$ and $O _ { t }$ , followed by computing the\n\n# Algorithm 1: Offline Refinement\n\nInput: Source ontology $O _ { S } = ( C _ { S } , E _ { S } )$ , target ontology $O _ { T } = ( C _ { T } , E _ { T } )$ Output: Concept graph $\\boldsymbol { \\mathcal { G } } _ { O }$ . 1 set $V  C _ { S } \\cup C _ { T }$ ; set $E  E _ { S } \\cup E _ { T }$ ; 2 Initialize concept graph $\\mathcal { G } _ { O } = ( V , E )$ ; 3 foreach $c \\in V$ do 4 compute rank $r ( c )$ as in Section 2; 5 $\\rho \\gets \\operatorname* { m a x } _ { c \\in V } r ( c )$ ; 6 for $i \\gets 0$ to $\\rho$ do ${ \\begin{array} { r l } { { \\boldsymbol { \\mathsf { 7 } } } } & { { } { \\boldsymbol { \\mathsf { \\ L } } } _ { i } \\gets \\{ c : r ( c ) = i \\} ; } \\end{array} }$ 8 $P \\gets \\{ B _ { 0 } , \\ldots , B _ { \\rho } \\}$ ; 9 for $i \\gets 0$ to $\\rho$ do 10 $D _ { i } \\gets \\{ X \\in P : X \\subseteq B _ { i } \\} ;$ ： 11 foreach $X \\in D _ { i }$ do 12 1 $G \\gets \\mathsf { c o l l a p s e } ( G , X ) ;$ 13 foreach $c \\in B _ { i }$ do 14 foreach $C \\in P$ with $C \\subseteq \\textstyle \\bigcup _ { j > i } B _ { j }$ do 15 Split $C$ into $C _ { 1 } , C _ { 2 }$ by adjacency to $c$ ; 16 $P  ( P \\setminus \\{ C \\} ) \\cup \\{ C _ { 1 } , C _ { 2 } \\}$ ;\n\n17 return $\\mathscr { G } _ { O }$ ;\n\nnode ranks. At each rank, it initializes a “bucket” $B _ { i }$ (as a single node set) that simply include all the concept nodes at rank $i$ (lines 7-8), and initialize a partition $\\mathcal { P }$ with all the buckets (line 9). It then follows a “bottom-up” process to refine the buckets iteratively, by checking if two concepts $c$ and $c ^ { \\prime }$ in a same bucket $B _ { i }$ are concept similar (as asserted by LLM and embedding similarity), and have all the neighbors that satisfy the requirement of ontological bisimilar relation by definition (lines 10-13). If not, a procedure collapse is invoked, to (1) split the bucket $B _ { i }$ into three fragments: $B _ { i } ^ { 1 } = B _ { i } \\setminus \\{ c , c ^ { \\prime } \\}$ , $B _ { i } ^ { 2 } = \\{ c \\}$ , and $B _ { i } ^ { 3 } { = } \\{ c ^ { \\prime } \\}$ , followed by a “merge” check to test if $c$ and $c ^ { \\prime }$ can be merged to $B _ { i } ^ { 1 }$ ; and (2) propagate this change to further “split-merge” operators to affected buckets at higher ranks (lines 14-17). This process continues until no change can be made.\n\nCorrectness. Algorithm 1 correctly terminates at obtaining a maximum bisimilar ontological equivalence relation $R _ { \\simeq }$ , with two variants. (1) As ontologies are DAGs, it suffices to perform a one-pass, bottom-up splitting of equivalence classes following the topological ranks; (2) the collapse operator ensures the “mergable” cases to reduce unnecessary buckets whenever a new bucket is separated. This process simulates the correct computation of maximum bisimulation relation in Kripke structures (a DAG) [15], optimized for deriving ontology matching determined by LLM-based concept similarity and bisimilar equivalence.\n\nTime Cost. The initialization of concept graph $\\scriptstyle { \\mathcal { G } } _ { O }$ is in $O ( | O _ { s } | + | O _ { t } | )$ . Here $| O _ { s } | { = } | V _ { s } | + | E _ { s } |$ ; and $| O _ { t } |$ is defined similarly. The iterative collapse (lines 10-17) takes in $O ( | O _ { s } | + | O _ { t } | )$ time as the number of buckets (resp. edges) is at most $| C _ { s } | + | C _ { T } |$ (resp. $\\left| E _ { s } \\right| + \\left| E _ { t } \\right| ,$ ). The overall cost is in $O ( | O _ { s } | + | O _ { t } | )$ .\n\nOverall Cost. We consider the cost of the entire workflow of KROMA. (1) The knowledge checking takes $O ( ( | C _ { s } | + | C _ { T } | ) | K G | )$ time, where $| K G |$ refers to the size of the external ontology or knowledge graph that is referred to by the knowledge retrieval via e.g., SPARQL access. Note here we consider SPARQL queries with “star” patterns, hence the cost of query processing (to curate ground sets) is in quadratic time. (2) The total cost of LLM inference is in $O ( | C _ { s } | | C _ { t } | T _ { I } )$ , for a worst case that any pair of nodes in $C _ { s } \\times C _ { t }$ are concept similar in terms of $\\sim$ . Here $T _ { I }$ is the unit cost of processing a prompt query. Putting these together, the total cost is in $O ( | C _ { s } | | C _ { t } | T _ { I } + ( | C _ { s } | + | C _ { t } | ) | K G | + ( | O _ { s } | + | O _ { t } | ) )$ time.\n\nOnline Refinement. We next outline the online matching process. In this setting, KROMA receives new ontology components as an (infinite) sequence of triples (edges), and incrementally maintain a concept graph $\\scriptstyle { \\mathcal { G } } _ { O }$ by processing the sequence input in small batched updates $\\varDelta \\mathcal { G }$ . For each newly arrived concept (node) $c$ in $\\varDelta \\mathcal { G }$ , KROMA conducts knowledge retrieval to curate $c . \\mathcal { Z }$ ; and consult LLMs to decide if $c$ is concept similar to any node in $\\scriptstyle { \\mathcal { G } } _ { O }$ . It then invokes online refinement algorithm to enforce the ontological bisimilar equivalence.\n\nThe algorithm (with pseudoscope reported in [1]) first updates the buckets in $\\scriptstyle { \\mathcal { G } } _ { O }$ by incorporating the nodes from $\\varDelta \\mathcal { G }$ that are verified to be concept similar, as well as their ranks. It then incrementally update the buckets to maintain the bisimilar equivalence consistently via a “bottom-up” split-merge process as in Algorithm 1 (lines 6-15). Due to the unpredictability of the “unseen” concepts, the online refinement defers the processing of two “inconsistent” cases for experts’ validation: (1) When a concept $c$ is determined to be concept similar by function $\\sim$ but not LLMs with high confidence; or (2) whenever for a new edge $( c , c ^ { \\prime } ) \\in$ $\\varDelta \\mathcal { G }$ , $( c , [ c _ { 1 } ] ) \\in R _ { \\simeq }$ , $( c ^ { \\prime } , [ c _ { 2 } ] ) \\in R _ { \\simeq }$ , yet $r ( c _ { 1 } ) < r ( c _ { 2 } )$ in $\\scriptstyle { \\mathcal { G } } _ { O }$ . Both require domain experts’ feedback to resolve. These cases are cached into a query set $\\mathcal { Q }$ to be further resolved in the validation phase (see Section 4). We cache these cases into a query set by using an auxiliary data structure (e.g., priority queue ranked by LLM confidence score) to manage their processing.\n\nAnalysis. The correctness of online refinement carries over from its offline counterpart, and that it correctly incrementalize the split-merge operator for each newly arrived edges. For time cost, for each batch, it takes a delay time to update $\\scriptstyle { \\mathcal { G } } _ { O }$ in $O ( | \\mathcal { G } _ { O } | + | \\Delta G | \\log | \\Delta G | + | \\Delta G | \\log | V _ { O } | )$ time. This result verifies that online refinement is able to response faster than offline maintenance that recomputes the concept graph from scratch. We present the detailed analysis in [1].\n\n# 6 Experimental Study\n\nWe investigated the following research questions. [RQ1]: How well KROMA improves state-of-the-art baselines with different LLMs? [RQ2]: How can knowledge retrieval and ontology refinement enhance matching performance? and [RQ3]: What are the impact of alternative LLM reasoning strategies?\n\n# 6.1 Experimental Setting\n\nBenchmark Datasets. We selected five tracks from the OAEI campaign [41], covering various domain tracks. For each track, we selected two representative ontologies as a source ontology $O _ { s }$ and a target ontology $O _ { t }$ . The selected tracks include Anatomy [16] (Mouse-Human), Bio-LLM [24] (NCIT-DOID), CommonKG (CKG) [19] (Nell-DBpedia, YAGO-Wikidata), BioDiv [29] (ENVO-SWEET), and MSE [26] (MI-MatOnto). To ensure a fair and comprehensive evaluation of KROMA, we adopted the standard benchmarks from the Ontology Alignment Evaluation Initiative (OAEI), enabling direct comparison with prior LLM-based methods. Despite the high cost of LLM inference, we tested KROMA across five diverse tracks to demonstrate its robustness across domains.\n\nLLMs selection. To underscore KROMA ’s ability to achieve strong matching performance even with smaller or lower-performance LLMs, we selected models with relatively modest Chatbot Arena MMLU scores [9]: Gemma-2B (51.3%) and Llama-3.2-3B $( 6 3 . 4 \\% )$ , compared to the baseline systems Flan-T5-XXL $( 5 5 . 1 \\%$ ) and MPT- 7B $( 6 0 . 1 \\% )$ . We have selected a diverse set of LLMs, ranging from ultra-lightweight to large-scale—to demonstrate KROMA’s compatibility with models that can be deployed on modest hardware without sacrificing matching quality. Our core evaluations use DeepSeek-R1-Distill-Qwen-1.5B [51] (1.5B), GPT-4o-mini [36] (8B), and Llama-3.3 [31] (70B), each chosen in a variant smaller than those employed by prior OM-LLM baselines. To further benchmark our framework, we include Gemma-2B [11] (2B), Llama-3.2-3B [3] (3B), Mistral-7B [4] (7B), and Llama-2-7B [2] (7B) in our ablation studies. To run inference on the aforementioned models, we make use of TogetherAI and OpenAI APIs as off-the-shelf inference services on hosted, pretrained models, and we are not performing any fine-tuning or weight updates.\n\nLLMs Determinism & Reproducibility. To enforce determinism and ensure reproducibility, all LLM calls are issued with a fixed temperature of 0.3, striking a balance between exploration and exploitation; and a constant random seed is used for sampling.\n\nConfidence Calibration. Our selection of LLMs is justified by a calibration test with their confidence over ground truth answers. The self-evaluated confidence by LLMs align well with performance: over $8 0 \\%$ of correct matches fall in the top confidence bins (9–10), while fewer than $5 \\%$ of errors are reported. Gemma-2B shows almost no errors above confidence 8, and both Llama-3.2-3B and Llama3.3-70B maintain $\\geq 9 5 \\%$ precision at confidence thresholds of 9 or greater. We thus choose a confidence threshold to be 8.5 for all LLMs to accept their output.\n\nTest sets generation. Following [23,21], for each dataset, we arbitrarily designate one concept as the “source” for sampling and its target counterpart. We remark that the source and target roles are interchangeable w.l.o.g given our theoretical analysis, algorithms and test results. We randomly sample 20 matched concept pairs from the ground truth mappings. For each source ontology concept, we select an additional 24 unmatched target ontology concepts based on their cosine similarity scores, thereby creating a total of 25 candidate mappings (including the ground truth mapping). Finally, we randomly choose 20 source concepts that lack a corresponding target concept in the ground truth and generate 25 candidate mappings for each. Each subset consists of 20 source ontology concepts with a match and 20 without matches, with each concept paired with 25 candidate mappings, totaling 1000 concept pairs per configuration.\n\nConcepts and entities not selected as test sets are treated as external knowledge base for knowledge retrieval. All models operate with SciBERT [5] for Embedding Generation, leveraging its strength in scientific data embedding. Baselines. Our evaluation considers four state-of-the-art LLM-based ontology matching methods: LLM4OM [20], MILA [49], OLaLa [25], and LLMap [23]. For RQ2, we also developed KROMA-NR, which skip knowledge retrieval, and KROMA-NB, which disables the bisimilarity-based clustering (hence clusters are determined by concept similarity alone).\n\nNaming Convention. Each configuration uses a pattern [Method][Optional Suffix][LLM Version], where the initials (K, M, O, L) specify the method (KROMA, MILA, OLaLa, LLM4OM). Suffixes “NKR” and “NR” denote “no knowledge retrieval” and “no ontology refinement”, respectively, and the trailing version number (e.g., 3.3, 2.0, 4mini) specifies the underlying LLM release. Some examples are: KL3.3 is the full KROMA using LLaMA-3.3; KNR3.3 is KROMA without Knowledge Retrieval using LLaMA-3.3; and KNB3.3 is KROMA without Ontology Refinement using LLaMA-3.3. The convention applies similarly to other LLMs (e.g., Gemma-2B, Mistral-7B, GPT-4o-mini) and LLM-based ontology matching methods (MILA, OLaLa, LLM4OM).\n\n# 6.2 Experimental Results\n\nExp-1: Effectiveness (RQ1). In this set of tests, we evaluate the performance of KROMA compared with baselines, and the impact of factors such as test sizes.\n\nTable 2: KROMA Performance vs. Baselines.   \n\n<html><body><table><tr><td>Model</td><td>MH ND NDB YW 97.08 95.54</td><td>ES</td><td>MM</td></tr><tr><td>KL3.3 KNB3.3</td><td rowspan=\"5\">94.9498.63 90.91 96.02</td><td rowspan=\"10\"></td><td>61.25 59.95 55.00</td></tr><tr><td>KNR3.391.25 95.0194.26 91.98 86.59</td><td>93.58 89.74</td></tr><tr><td>KL3.1 94.50 98.24</td><td>91.43</td></tr><tr><td>KL2.0 93.24 KG2 85.53</td><td>85.06</td></tr><tr><td>KM7</td><td></td></tr><tr><td>ML3.1</td><td>92.2094.80</td><td>92.98 83.70 32.97</td></tr><tr><td>OL2.0</td><td>90.20 96.00</td><td>51.10</td></tr><tr><td>L4G3.5</td><td>89.11 83.01 94.26</td><td></td></tr><tr><td>K4mini</td><td>72.10</td><td>93.18</td></tr><tr><td>LLFT</td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td>L4L2</td><td>92.19</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td>L4M7 L4MPT</td><td></td><td>55.09 32.97</td></tr></table></body></html>\n\nKROMA vs. Baselines: Overall Performance. Across all six datasets in Table 2 (abbreviated by their first capitalized letters), the full KROMA configuration (KL3.3) achieves the highest $F _ { 1 }$ on every task, substantially outperforming competing baselines. For Mouse–Human, KL3.3 delivers 94.94 $F _ { 1 }$ , eclipsing MILA’s best (ML3.1) at 92.20, OLaLa (OL2.0) at 90.20, and LLM4OM (L4G3.5) at 89.11. On NCIT–DOID, KL3.3 reaches 98.63 versus 94.80 for MILA, 83.01 for LLM4OM, and 72.10 for Flan-T5-XXL. Similar gaps appear on Nell–DBpedia (97.08 vs. 96.00/94.26), YAGO–Wikidata (95.54 vs. 92.19/93.33), ENVO–SWEET (93.18 vs. 92.98/85.06), and MI–MatOnto (61.25 vs. 59.05/32.97). These verify the effectiveness of KROMA over representative benchmark datasets. More details (e.g., Precision and Recall) are reported in [1].\n\nImpact of different LLMs. Across six ontology-matching tracks, KROMA equipped with Qwen2.5-1.5B outperforms the best existing baseline on five out of the six datasets (see Table 3). In the Anatomy’s Mouse–Human track, Qwen2.5- 1.5B achieves $F _ { 1 } = 8 3 . 5 8$ (versus 92.20 for the OM-LLM baseline), while GPT4o-mini reaches $F _ { 1 } = 9 1 . 9 6$ . On NCIT–DOID both models surpass the baseline with $F _ { 1 }$ of 97.44 and 97.56 (baseline: 94.80), and similar gains appear on Nell–DBpedia (95.02, 95.67 vs. 96.00), YAGO–Wikidata (95.45, 95.44 vs. 92.19), and MI–MatOnto (61.25, 60.88 vs. 32.97). Only on ENVO–SWEET does the smallest model dip below the baseline (79.80 vs. 83.70), whereas GPT-4o-mini (93.18) and Llama-3.3-70B (91.95) still lead. These results show that KROMA (with both knowledge retrieval and ontology refinement) delivers strong performance even on relatively “small” LLMs, and scales further with larger LLMs.\n\nTable 3: KROMA Performance with different LLMs.   \n\n<html><body><table><tr><td>Dataset</td><td>Qwen2.5 GPT-4o-mini Llama-3.3</td><td></td><td></td></tr><tr><td>Mouse-Human</td><td>83.6</td><td>92.0</td><td>94.9</td></tr><tr><td>NCIT-DOID</td><td>97.4</td><td>97.6</td><td>98.6</td></tr><tr><td>Nell-DBpedia</td><td>95.0</td><td>95.7</td><td>97.1</td></tr><tr><td>YAGO-Wikidata</td><td>95.5</td><td>95.4</td><td>95.5</td></tr><tr><td>ENVO-SWEET</td><td>79.8</td><td>93.2</td><td>92.0</td></tr><tr><td>MI-MatOnto</td><td>61.3</td><td>60.9</td><td>62.2</td></tr></table></body></html>\n\nImpact of Test sizes. We next report the impact of ontology sizes to the performance of KROMA in performance (detailed results reported in [1]). For each dataset, we varied test sizes from 200 (xsmall) to 1,000 (full) pairs. KROMA’s performance is in general insensitive to the change of test sizes. For example, its $F _ { 1 }$ stays within a 1.90 variance on NCIT–DOID and a 1.10-point range on YAGO–Wikidata. This verifies the robustness of KROMA in maintaining desirable performance for large-scale ontology matching tasks.\n\nExp-2: Ablation Analysis (RQ2). In this test, we perform ablation analysis, comparing KROMA with its two variants, KROMA-NKR and KROMA-NR, removing knowledge retrieval and ontology refinement, respectively.\n\nImpact of Ontology Refinement. Table 4 (Left) shows that by incorporating Ontology Refinement (KROMA vs. KROMA-NR), KROMA improves $F _ { 1 }$ score across\n\nTable 4: KROMA Performance with/without Ontology Refinement (Left); and with/without Knowledge Retrieval (Right).   \n\n<html><body><table><tr><td>Dataset</td><td>Model</td><td>P</td><td>R</td><td>F1</td><td>Dataset</td><td>Model</td><td>P</td><td>R</td><td>F1</td></tr><tr><td rowspan=\"2\">Mouse-Human</td><td>KL3.3</td><td>90.78</td><td>99.50</td><td>94.94</td><td rowspan=\"2\">Mouse-Human</td><td>KL3.3</td><td>90.78</td><td>99.50</td><td>94.94</td></tr><tr><td>KNR3.3</td><td>92.34</td><td>90.16</td><td>91.25</td><td>KNKR3.3</td><td>100.00</td><td>76.36</td><td>86.59</td></tr><tr><td rowspan=\"2\">NCIT-DOID</td><td>KL3.3</td><td>97.59</td><td>99.69</td><td>98.63</td><td rowspan=\"2\">NCIT-DOID</td><td>KL3.3</td><td>97.59</td><td>99.69</td><td>98.63</td></tr><tr><td>KNR3.3</td><td>93.20</td><td>96.90</td><td>95.01</td><td>KNKR3.3</td><td></td><td>83.33 100.00</td><td>90.91</td></tr><tr><td rowspan=\"2\">Nell-DBpedia</td><td>KL3.3</td><td>94.32 100.00 97.08</td><td></td><td></td><td rowspan=\"2\">Nell-DBpedia</td><td>KL3.3</td><td>94.32</td><td>100.00 97.08</td><td></td></tr><tr><td>KNR3.3</td><td>97.46</td><td>91.27</td><td>94.26</td><td>KNKR3.3</td><td>91.17</td><td>96.12</td><td>93.58</td></tr><tr><td rowspan=\"2\">YAGO-Wikidata</td><td>KL3.3</td><td>91.80</td><td>99.60</td><td>95.54</td><td rowspan=\"2\">YAGO-Wikidata KL3.3</td><td></td><td>91.80</td><td>99.60</td><td>95.54</td></tr><tr><td>KNR3.3</td><td>93.50</td><td>90.50</td><td>91.98</td><td>KNKR3.3</td><td>90.50</td><td>89.00</td><td>89.74</td></tr><tr><td rowspan=\"2\">ENVO-SWEET</td><td>K4mini</td><td>87.23 100.00 93.18</td><td></td><td></td><td rowspan=\"2\">ENVO-SWEET</td><td>K4mini</td><td>87.23</td><td>100.00 93.18</td><td></td></tr><tr><td>KNR4mini 86.90</td><td></td><td>98.00</td><td>92.12</td><td>KNKR4mini</td><td>82.00</td><td>88.00</td><td>84.89</td></tr><tr><td rowspan=\"2\">MI-MatOnto</td><td>KL3.3</td><td>45.74</td><td>92.69</td><td>61.25</td><td rowspan=\"2\">MI-MatOnto</td><td>KL3.3</td><td>45.74</td><td>92.69</td><td>61.25</td></tr><tr><td>KNR3.3</td><td>44.80</td><td>91.40</td><td>59.95</td><td>KNKR3.3</td><td>42.00</td><td>85.00</td><td>55.00</td></tr></table></body></html>\n\n6 datasets: Mouse–Human (+3.69), NCIT–DOID (+3.62), Nell–DBpedia (+2.82), YAGO–Wikidata $( + 3 . 5 6 )$ , ENVO–SWEET (+1.06), MI–MatOnto (+1.30). By pruning false candidate pairs via offline and online refinement strategies on the concept graph, ontology refinement retains true semantic connections, by filtering out noise while preserving true alignments.\n\nImpact of Knowledge Retrieval. Table 4 (Right) shows that ablating Knowledge Retrieval (KROMA-KNR vs. KROMA) results in significant $F _ { 1 }$ drop across all six benchmark datasets: Mouse–Human (–8.35), NCIT–DOID (–7.72), Nell–DBpedia (–3.50), YAGO–Wikidata (–5.80), ENVO–SWEET (–8.29), MI–MatOnto (–6.25). By enriching concepts with external, useful semantic context that are overlooked by other baselines, knowledge retrieval helps improving the performance.\n\nExp-3: Alternative LLM Reasoning Strategies. We evaluate KROMA’s performance with two alternative LLM reasoning strategies: “Debating” and “Deep reasoning” to understand their impact to ontology matching.\n\nTable 5: KROMA Performance with/without “Debating” (Left); and with/without “Deep reasoning” (Right).\n\n<html><body><table><tr><td>Dataset</td><td>Model</td><td>P</td><td>R</td><td>F1</td><td colspan=\"5\"></td></tr><tr><td rowspan=\"4\">Mouse-Human</td><td>D2A3R D2A5R</td><td>100</td><td>70</td><td>82.35</td><td rowspan=\"2\">Dataset</td><td rowspan=\"2\">Model</td><td rowspan=\"2\">P</td><td rowspan=\"2\">R</td><td rowspan=\"2\">F1</td></tr><tr><td></td><td>100</td><td>74</td><td>85.06 79.55</td></tr><tr><td>D4A3R</td><td>100</td><td>66</td><td></td><td rowspan=\"2\">NCIT-DOID</td><td>L3.3Short 97.59</td><td></td><td>99.69</td><td>98.63</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>L3.3Long</td><td>97.66</td><td>96.31</td><td>96.98</td></tr><tr><td rowspan=\"4\">Nell-DBpedia</td><td>D2A3R</td><td>97.83</td><td>90</td><td>93.75</td><td rowspan=\"2\">Nell-DBpedia</td><td></td><td></td><td></td><td></td></tr><tr><td>D2A5R</td><td>95.55</td><td>9</td><td>94.55</td><td rowspan=\"2\"></td><td>L3.3Lonrt 94.32</td><td></td><td>100.00</td><td>97.08</td></tr><tr><td>D4A5R</td><td>93.88</td><td>92</td><td>92.93</td><td rowspan=\"2\"></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan=\"3\">YAGO-Wikidata</td><td></td><td></td><td></td><td></td><td rowspan=\"3\">YAGO-Wikidata</td><td>L3.3Lonrt 91.80</td><td></td><td>99.60</td><td>95.54</td></tr><tr><td>D2A3R</td><td>100</td><td></td><td>98</td><td rowspan=\"2\"></td><td></td><td></td><td></td></tr><tr><td>D4A5R</td><td>100</td><td>90 94.73</td><td></td><td></td><td></td><td></td></tr></table></body></html>\n\nCan “Debating” help? We implemented an LLM Debate ensemble [18], where multiple agents propose alignments with their chain-of-thought and then iteratively critique one another. To bound context size, we drop $5 0 \\%$ of historic turns and keep only each agent’s latest reply. Due to its expense, we tested this on Mouse-Human, Nell-DBpedia, and YAGO-Wikidata using four permutation of configurations: 2 or 4 agents over 3 or 5 debate rounds, denoted as D[Number Of Agent]A[Number Of Round]R. From Table 5 (Left), on Mouse–Human dataset, extending rounds in the 2-agent setup raised F1 from 82.35 to 85.06, whereas adding agents degraded performance (4 agents: 79.52 at 3 rounds, 80.95 at 5). Similar trends follow for Nell-DBpedia and YAGO-Wikidata. This interestingly indicates that “longer debates” help small ensembles converge to accurate matches, but larger groups introduce too much conflicting reasoning. In contrast, a single-agent, single-round KROMA pass attains $\\mathrm { F 1 } = 9 0 . 7 8$ , underscoring that a well-tuned solo model remains the most efficient and reliable choice.\n\nCan “Deep reasoning” help? Building on DeepSeek-R1’s “Aha Moment” [13], we extended KROMA to include a forced long chain-of-thought for self-revision. Noting that vanilla DeepSeek-R1 often emits empty “<think>\\n” tags (i.e. “<think>\\n\\n</think>”), we altered our prompt so every response must start with “<think>\\n”, ensuring the model spells out its reasoning. We then compared standard Llama-3.3 (70B) (“short” reasoning, noted as L3.3Short) to DeepSeek-R1-Distill-Llama-3.3 (70B) (“long” reasoning, noted as L3.3Long), finding that the added \"<think>\\n\" tag inflated inputs by $2 0 \\%$ and outputs by $6 0 0 \\%$ but delivered only tiny precision gains (e.g., NCIT–DOID: $9 7 . 5 9 \\% $ $9 7 . 6 6 \\%$ ) while slashing recall $( 9 9 . 6 9 \\% \\to 9 6 . 3 1 \\%$ ), dropping F1 by 1.65–2.01 points, based on Table 5 (Right). This suggests that verbose self-reflection may improve transparency but consumes crucial context and can overly filter valid matches—especially in binary tasks, where lengthy chains of thought have been shown to harm recall, which has been consistently observed in [55].\n\n# 7 Conclusion\n\nWe have presented KROMA, an ontology matching framework that exploits the semantic capabilities of LLMs within a bisimilar-based ontology refinement process. We show that KROMA computes a provably unique minimized structure that captures semantic equivalence relations, with efficient algorithms that can significantly reduces LLMs communication overhead, while achieving state-ofthe-art performance across multiple OAEI benchmarks. Our evaluation has verified that LLMs, when guided by context and optimized prompting, can rival or surpass much larger models in performance. A future topic is to extend KROMA with more LLM reasoning strategies and ontology engineering tasks.\n\nSupplemental Material Statement. Our code and experimental data has been made available at https://anonymous.4open.science/r/kroma/, including an extended version of the paper [1].",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决的核心问题是本体匹配（Ontology Matching, OM）中的语义互操作性问题。现有系统通常依赖于手工规则或专用模型，适应性有限，难以应对新概念或未见过的场景。\\n> *   该问题在语义网、知识图谱集成等领域具有关键价值，特别是在需要跨领域或跨数据源的知识对齐时。\\n\\n> **方法概述 (Method Overview)**\\n> *   提出KROMA框架，通过检索增强生成（RAG）管道动态整合大语言模型（LLMs）与结构化知识，结合双相似性概念匹配和轻量级本体优化步骤，显著提升匹配精度并降低计算开销。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **理论贡献**：形式化定义了基于双相似性（bisimilarity）的语义等价关系，并证明其诱导的概念图具有最小性和唯一性保证（Lemma 2）。\\n> *   **算法贡献**：提出离线和在线两种优化算法，时间成本分别为线性（$O(|O_s|+|O_t|)$）和对数级（$O(|\\\\mathcal{G}_O|+|\\\\Delta G|\\\\log|\\\\Delta G|)$）。\\n> *   **实验效果**：在OAEI基准测试中，KROMA平均优于现有LLM方法10.95%，其中知识检索和本体优化分别贡献6.65%和2.68%的准确率提升。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   通过LLMs的语义理解能力增强本体匹配，但避免直接提示导致的幻觉风险，转而结合结构相似性（双相似性）和检索增强的上下文（如邻居采样、SPARQL查询）来动态构建概念图。\\n\\n> **创新点 (Innovations)**\\n> *   **与传统方法对比**：传统规则或结构匹配泛化性差，学习型方法需大量标注数据；KROMA通过LLMs的零样本能力减少数据依赖。\\n> *   **与现有LLM方法对比**：现有LLM-OM系统（如LLM4OM、MILA）仅用嵌入检索或简单提示；KROMA引入双相似性等价关系（Section 3.1）和全局优化步骤（Section 5）。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **概念图初始化**：合并源/目标本体为DAG，计算节点秩（rank）和地面集（ground set）。\\n> 2.  **知识检索**：对每个概念进行两跳邻居采样，生成SPARQL查询丰富语义上下文（Example 2）。\\n> 3.  **嵌入生成**：混合图嵌入（如node2vec）和文本嵌入（如SciBERT）：$z_c = \\\\alpha \\\\mathsf{embd}_{graph}(c) + (1-\\\\alpha)\\\\mathsf{embd}_{text}$。\\n> 4.  **LLM提示查询**：生成包含任务描述、上下文示例和置信度要求的模板（Figure 3），调用LLMs验证候选对。\\n> 5.  **本体优化**：离线算法（Algorithm 1）按秩分桶，通过分裂-合并操作维护双相似等价；在线算法增量处理新边（Section 5）。\\n\\n> **案例解析 (Case Study)**\\n> *   如图1所示，通过比较“house pet”与“carnivore”的地面集（前者包含非肉食物种），结合LLMs验证，最终将它们分到不同等价类，而“coyote”和“wolfdog”因共享“canine”特征被归为同类。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   LLM4OM（TF-IDF + SBERT检索器）\\n> *   MILA（优先深度优先搜索 + LLM）\\n> *   OLaLa（SBERT + MELT提示框架）\\n> *   LLMap（二元提示 + Flan-T5-XXL）\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在F1分数上**：KROMA（KL3.3配置）在NCIT-DOID数据集上达到98.63，显著优于MILA（94.80）和LLM4OM（83.01），提升最高达15.62个百分点。\\n> *   **在计算效率上**：KROMA的在线算法延迟时间为$O(|\\\\mathcal{G}_O|+|\\\\Delta G|\\\\log|\\\\Delta G|)$，与轻量级模型LLMap相当，但F1分数高出后者32.97%（MI-MatOnto数据集）。\\n> *   **在知识检索贡献上**：移除检索模块（KROMA-NKR）导致F1下降7.72%（NCIT-DOID），验证了外部知识对LLM决策的关键作用。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   本体匹配 (Ontology Matching, OM)\\n*   大语言模型 (Large Language Models, LLMs)\\n*   检索增强生成 (Retrieval-Augmented Generation, RAG)\\n*   双相似性 (Bisimilarity, N/A)\\n*   语义互操作性 (Semantic Interoperability, N/A)\\n*   概念图 (Concept Graph, N/A)\\n*   知识图谱 (Knowledge Graph, KG)\"\n}\n```"
}