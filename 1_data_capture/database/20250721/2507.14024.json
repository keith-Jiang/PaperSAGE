{
    "source": "ArXiv (Semantic Scholar未收录)",
    "arxiv_id": "2507.14024",
    "link": "https://arxiv.org/abs/2507.14024",
    "pdf_link": "https://arxiv.org/pdf/2507.14024.pdf",
    "title": "Moodifier: MLLM-Enhanced Emotion-Driven Image Editing",
    "authors": [
        "Jiarong Ye",
        "Sharon X. Huang"
    ],
    "categories": [
        "cs.CV"
    ],
    "publication_date": "未找到提交日期",
    "venue": "暂未录入Semantic Scholar",
    "fields_of_study": "暂未录入Semantic Scholar",
    "citation_count": "暂未录入Semantic Scholar",
    "influential_citation_count": "暂未录入Semantic Scholar",
    "institutions": [
        "The Pennsylvania State University"
    ],
    "paper_content": "# Moodifier: MLLM-Enhanced Emotion-Driven Image Editing\n\nJiarong Ye The Pennsylvania State University jxy225@psu.edu\n\nSharon X. Huang The Pennsylvania State University suh972@psu.edu\n\n# Abstract\n\nBridging emotions and visual content for emotion-driven image editing holds great potential in creative industries, yet precise manipulation remains challenging due to the abstract nature of emotions and their varied manifestations across different contexts. We tackle this challenge with an integrated approach consisting of three complementary components. First, we introduce MoodArchive, an $^ { \\delta M + }$ image dataset with detailed hierarchical emotional annotations generated by LLaVA and partially validated by human evaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned on MoodArchive to translate abstract emotions into specific visual attributes. Third, we propose Moodifier, a training-free editing model leveraging MoodifyCLIP and multimodal large language models (MLLMs) to enable precise emotional transformations while preserving content integrity. Our system works across diverse domains such as character expressions, fashion design, jewelry, and home d´ecor, enabling creators to quickly visualize emotional variations while preserving identity and structure. Extensive experimental evaluations show that Moodifier outperforms existing methods in both emotional accuracy and content preservation, providing contextually appropriate edits. By linking abstract emotions to concrete visual changes, our solution unlocks new possibilities for emotional content creation in real-world applications. Project website: this https URL.\n\n# 1. Introduction\n\nImagine transforming a cocktail dress from elegant to joyful with a single click, or visualizing how a pendant might evoke different emotions through subtle material changes. As demonstrated in Fig. 1, tools for precise emotional manipulation could significantly enhance creative workflows1. Beyond aesthetic exploration, such capabilities would create tangible value across domains: animation artists could generate emotional sequences while preserving character identity; e-commerce platforms could tailor product imagery to different customer segments; marketing teams could repurpose assets across emotional campaigns without additional photoshoots, reducing production time and costs.\n\nHowever, existing image editing tools lack sophisticated emotion-driven controls. The challenge lies in how emotions manifest differently across contexts; for example, “excitement” translates to vibrant patterns in fashion, specific facial expressions in characters, and intricate textures in jewelry. Existing methods (e.g. [1–3]) either provide simplistic adjustments, such as generic filters, or introduce unwanted artifacts that compromise design integrity (see Fig. 4), forcing creators to rely on time-consuming iterations rather than enabling seamless creative exploration.\n\nHere we propose Moodifier to address previous limitations by bridging abstract emotional concepts with precise visual modifications through a two-stage approach, as illustrated in Fig. 3. First, given a desired emotion, MLLMs analyze the source image to generate context-specific editing instructions and spatial attention maps, highlighting where emotional modifications should be applied. To translate these instructions into visual changes, Moodifier uses MoodifyCLIP, a vision-language model fine-tuned specifically for emotional understanding. Unlike standard CLIP models trained on brief, factual captions from datasets like COCO [4] and Flickr [5]), MoodifyCLIP utilizes our MoodArchive dataset, which contains millions of affective images (See Fig. 1 in supp.) with hierarchical emotional annotations as shown in Fig. 2.\n\nSecond, using MoodifyCLIP-processed prompts and spatial attention maps, Moodifier performs attentioncontrolled editing. It extracts the latent representation of the source image and directs the diffusion model to modify only emotion-relevant attributes while preserving the overall content. The visual results in Fig. 1 demonstrate Moodifier’s versatility: character faces express diverse emotions while maintaining identity, dresses adapt to different emotional tones while retaining their original design, and jewelry and de´cor shift in mood without altering their fundamental structure.\n\nTo summarize, we introduce a comprehensive framework that advances emotion-driven image editing at the intersection of computer vision and affective computing:\n\n![](images/0ac97edd7a09914cd178f20fe6c42c13ac0a36ddfe3c8110c33e0883dbbcb8c6.jpg)  \nFigure 1. Our emotion-driven Moodifier image editing system performs consistently across diverse scenarios such as product mockups, e-commerce, fashion, jewelry, and home de´cor. It precisely modifies target objects while preserving the overall structure and integrity of the scene. The target objects selected for editing are depicted as clip arts in the leftmost column. In non-facial contexts, emotions manifest as semantic qualities (e.g., “Excitement” as vibrant designs, “Sad” as drooping elements, “Anger” as bold styling). Zoom in for better view.\n\n# 2. Related Works\n\n1. MoodArchive: The largest dataset of its kind, MoodArchive consists of over 8 million images with hierarchical emotional annotations generated by LLaVA-NeXT and partially validated by human evaluators. This dataset establishes a new benchmark for training models to understand nuanced emotional attributes in visual content.\n\n2. MoodifyCLIP: A vision-language model fine-tuned on MoodArchive that extends the capabilities of existing CLIP models by enabling fine-grained emotional reasoning and translation of abstract emotions into specific, context-aware visual attributes.\n\n3. Moodifier: A novel training-free image editing system that seamlessly integrates MoodifyCLIP’s emotional intelligence with MLLMs and diffusion models. Moodifier enables precise, targeted emotional transformations while preserving structural and semantic integrity.\n\nVisual Emotion Datasets. The discrete categorical system proposed by Ekman [6] identifies a set of universally recognized affective states, such as happiness, sadness, anger, and fear. These states are considered discrete categories that capture different emotional experiences. Numerous datasets have been constructed to study people’s emotional reactions to images [7–11]. However, these existing datasets often fail to capture complex or nuanced emotions, and their limited range of emotions can reduce their effectiveness in downstream tasks. Therefore, there is a need for more comprehensive datasets that encompass a broader spectrum of emotional experiences beyond basic emotions.\n\nContrastive Language-Image Pre-training (CLIP) [12] leverages contrastive learning between text-image pairs, demonstrating robust zero-shot capabilities across detection [13, 14], segmentation [15, 16], video understanding [17, 18], and image synthesis [19–22]. Despite its versatility, recent studies [23, 24] highlight CLIP’s limitations in fine-grained feature extraction and these token-region alignment methods remain ineffective for complex, longer captions. Synthetic data has improved vision-language pretraining in numerous works [25–28], leveraging text-to-image models and LLMs to generate high-quality image-text pairs that address issues like misalignment and harmful content in web-scraped datasets. While recent approaches [29, 30] use multimodal LLMs for better caption generation, they mainly focus on short, descriptive captions rather than capturing the rich emotional and affective dimensions that longer, more nuanced captions could provide.\n\nImage Editing and Attention Control. Text-guided image editing has evolved from limited GAN frameworks [31, 32] to diffusion models [33–36] with cross-modal attention control [37–40]. This progression has improved the accessibility of visual content manipulation, allowing users to transform images through simple textual descriptions rather than intricate manual editing. While traditional mask-based techniques [41–45] require precise region selection, newer attention-based methods such as Prompt-to-Prompt [38], Plug-and-Play [46], and MasaCtrl [47] enable zero-shot editing by preserving structural integrity or conceptual elements without explicit boundary definition. These innovations strategically modify attention between text and visual features for coherent transformations. Though instructiontuned models [2, 3] have advanced, they struggle with complex emotional transformations. Our approach leverages MLLMs for explicit emotional guidance and enhanced attention control.\n\n# 3. Methodology\n\n# 3.1. MoodArchive\n\nLanguage-image pre-training faces a key limitation: existing datasets lack detailed captions that effectively capture the complex emotional dimensions of visual content, hindering models’ ability to develop nuanced affective understanding. To address this, we introduce MoodArchive, a dataset with over 8 million images featuring diverse emotional content. The data collection steps are as follows:\n\n# Comprehensive list of 27 emotions in 4 contexts.\n\nAs showcased in Fig. 1A in the supplementary material (Supp.), to ensure our dataset surpasses existing datasets [7–11] in both scale and the granularity of emotion classes, we adopted the emotion list from GoEmotions [48] (27 emotions2). For each emotion, we aim to collect images in four distinct contexts: facial expressions, natural scenery, urban scenery, object classes.\n\nCaption: Caption:   \nThis image captures a woman with a joyful expression, giving a This image captures the dramatic beauty of a   \ndouble thumbs-up, exuding positivity and optimism. lightning strike illuminating a serene landscape   \n(+) (Beautiful hair) Her beautiful hair cascades in soft, natural with mountains and fields under a stormy sky.   \ncurls, radiating warmth and charm. (+) (Majestic Mountain) Majestic Mountain, a   \n(+) (Radiant smile) Her eyes sparkle with warmth and towering sentinel of nature's grandeur, stands as   \nkindness, her mouth is curved into a genuine smile, revealing testament to the power and beauty of the earth   \nher white teeth, and her lips are slightly parted, adding to the (+) (Fearsome Lightning) Awe-inspiring Lightning, a spectacle of raw and overall positive emotion evoked. untamed energy, strikes the sky with ferocity.   \n(+) (Positive thumbs up gesture) The thumbs up gesture (+) (Vast Field.) Vast field, a picturesque scene of nature's artistry, captivates the further emphasizes her approval and satisfaction with her soul and stirs the senses.   \nAppearance.   \n(Overall emotion evoked) The overall emotion evoked in this image is nervous, (Overall emotion evoked) The image captures a moment of pure joy and fear, excitement, as the viewer is drawn into the raw and untamed beauty of the contentment natural world   \nCaption: Caption:   \nThis image captures the haunting beauty of an This image captures the poignant beauty of a wilting rose drooping abandoned landscape, where nature begins to vase against a somber, dark green background.   \nreclaim forgotten structures.   \n(+) (Wilting Rose) The wilting Rose, its petals curling and head   \n(+) (Abandoned building) The abandoned Bowing in graceful surrender, symbolizes the fleeting nature of   \nbuilding, with its crumbling facade and broken beauty and life's fragile cycle.   \nwindows, stands as a silent testament to a bygone era, evoking a sense of (+) (Amber Vase) The amber Vase, with its structured geometric sadness and nostalgia. pattern catching subtle light, provides a sturdy vessel for   \n(+) (Desolate landscape) The desolate landscape, devoid of life and activity, impermanence and decay.   \nadds to the melancholic atmosphere. (+) (Dark Background) The dark Background, a shadowy canvas of muted (+) (Forlorn railroad tracks.) The forlorn railroad tracks, once a symbol of green, creates a somber stage that heightens the dramatic isolation of the dying connection and progress, now lie dormant, further emphasizing the sense of flower   \nloss and decay.   \n(Overall emotion evoked) The overall emotion evoked in this image is (Overall emotion evoked) The image evokes a deep sense of melancholy and melancholic contemplation, drawing the viewer into a quiet meditation on reflection, stirring thoughts of forgotten stories and lost histories. mortality, beauty in decline, and the dignity found in life's natural conclusion.\n\nEmotion Context Expansion. We used ChatGPT to decompose abstract emotions into specific descriptors, addressing the challenge of retrieving relevant images with broad terms like ”anger.” This yielded precise descriptors such as ’pursed lips’ and ’glaring eyes’ for facial expressions, and ’fierce wildfire’ for natural scenes (See Fig. 1 in supp for additional details). These refined phrases served as search queries to retrieve images from multiple sources3.\n\n#\n\nPost-processing. We filtered NSFW content and corrupt files, then generated structured annotations using LLaVA-NeXT [49]. Each image received: (1) a global summary, (2) three emotional stimuli identifying visual triggers, and (3) an overall emotion assessment (Fig. 2). Finally, we filtered out the bottom $20 \\%$ of images based on CLIP score similarity to emotion-related phrases.\n\nHuman Validation Study. To evaluate the quality and reliability of our dataset, we conducted a comprehensive human validation study on Amazon Mechanical Turk. We randomly selected 10k images from MoodArchive and asked workers to compare the original web-collected alttext with our LLaVA-generated detailed captions. The validation interface (Fig. 2 in Supp.) enforced strict evaluation criteria, requiring workers to assess both content accuracy and emotional interpretation. Captions were rejected if any component in the structured annotation (Fig. 2) was inaccurate, ensuring a rigorous standard. Results of the human evaluation showed that $8 5 \\%$ of the LLaVA-generated captions were selected by workers as better describing the images than the original web-collected alt-text, while the remaining $1 5 \\%$ were rejected in favor of either the original captions or indicated as inadequate, in which cases workers provided specific feedback citing issues such as inappropriate word choices, inaccurate emotion detection, overly dramatic descriptions, or misalignment between the detected emotions and cultural interpretations. This study demonstrates that our pipeline produces annotations that align well with human judgment while maintaining scalability. By integrating automated annotation generation with human verification, we create a large-scale emotional dataset that balances quality with scalability.\n\n# 3.2. MoodifyCLIP\n\nAlthough our human validation study confirms the decent quality of LLaVA-generated captions, we acknowledge that hallucinations are inevitable at this scale. However, the fine-grained emotional context these rich descriptions provide is invaluable, capturing nuances that shorter captions cannot. To handle longer descriptions, we adopt a positional embedding interpolation strategy [50]. To balance accuracy and detail, MoodifyCLIP incorporates: (1) concise summary captions to offset potential inaccuracies in detailed descriptions, (2) fine-grained alignment to precisely link image regions with emotional annotations, and (3) optimal transport loss to filter mismatches between visual and textual features. Our experimental results confirm that the emotional understanding gained far outweighs occasional inaccuracies.\n\nGlobal Contrastive Loss (GL). We adopt CLIP’s visual encoder $E _ { i }$ and text encoder $E _ { t }$ to map inputs into a unified embedding space: $I ^ { f } = E _ { i } ( I ) , \\quad T ^ { f } = E _ { t } ( T ^ { f } )$ . While detailed captions capture specific emotional triggers and nuanced affective dimensions missing from existing datasets, concise summary captions remain crucial for holistic image comprehension. To balance granular details with global understanding, we add $T ^ { s } = E _ { t } ( T ^ { s } )$ . Note that $T ^ { f }$ contains the complete five-sentence caption from our LLaVA annotations, while $T ^ { s }$ only includes the first and last sentences (i.e. summary caption and emotion assessment, see Fig. 2). As outlined in Algorithm 1 in the Supp., we obtain the text-to-vision loss via InfoNCE contrastive learning between visual representations $I ^ { f }$ and textual embeddings $T ^ { f }$ as: $\\mathcal { L } _ { f } = ( \\mathcal { L } _ { t 2 v } ^ { f } + \\mathcal { L } _ { v 2 t } ^ { f } ) / 2$ with explicit formulation :\n\n$$\n\\begin{array} { r l } & { \\mathcal { L } _ { t 2 v } ^ { f } = - \\displaystyle \\sum _ { i = 1 } ^ { N } \\log \\frac { \\exp ( \\cos ( I _ { i } ^ { f } , T _ { i } ^ { f } ) / \\tau ) } { \\sum _ { j = 1 } ^ { N } \\exp ( \\cos ( I _ { j } ^ { f } , T _ { i } ^ { f } ) / \\tau ) } } \\\\ & { \\mathcal { L } _ { v 2 t } ^ { f } = - \\displaystyle \\sum _ { i = 1 } ^ { N } \\log \\frac { \\exp ( \\cos ( T _ { i } ^ { f } , I _ { i } ^ { f } ) / \\tau ) } { \\sum _ { j = 1 } ^ { N } \\exp ( \\cos ( T _ { j } ^ { f } , I _ { i } ^ { f } ) / \\tau ) } } \\end{array}\n$$\n\nSimilarly, we add an objective for the global summary caption and emotion asessment: $\\mathcal { L } _ { s } = ( \\mathcal { L } _ { t 2 v } ^ { s } + \\mathcal { L } _ { v 2 t } ^ { s } ) / 2$ .\n\nFine-grained Loss (FG). Next, to fully utilize our hierarchical annotations, where each emotional stimulus prompt is carefully engineered to describe distinct affective elements, we align each with its corresponding regions of interest within the image. We first compute cross-attention weights between fine-grained text embeddings $( T ^ { f g } )$ and image patch embeddings $( I ^ { f g } )$ , yielding a similarity matrix $\\mathcal { W } = \\{ w _ { i , j } \\}$ . Applying these weights ensures that each emotional stimulus description is primarily focused on its relevant visual regions while differentiating itself from others. The attention-weighted visual representation for each stimulus j is computed as Ijfg = PmM=1 w3 i,wji,j If g. We implement a contrastive objective for such region visual representations as:\n\n$$\n\\begin{array} { r l r } {  { \\mathcal { L } _ { t 2 v } ^ { f g } = - \\sum _ { i = 1 } ^ { N } \\sum _ { j = 1 } ^ { 3 } \\log \\frac { \\exp ( \\cos ( I _ { i , j } ^ { f g } , T _ { i , j } ^ { f g } ) / \\tau ) } { \\sum _ { m = 1 } ^ { M } \\exp ( \\cos ( I _ { i , m } ^ { f g } , T _ { i , j } ^ { f g } ) / \\tau ) } } } \\\\ & { } & { \\mathcal { L } _ { v 2 t } ^ { f g } = - \\sum _ { i = 1 } ^ { N } \\sum _ { j = 1 } ^ { 3 } \\log \\frac { \\exp ( \\cos ( T _ { i , j } ^ { f g } , I _ { i , j } ^ { f g } ) / \\tau ) } { \\sum _ { m = 1 } ^ { M } \\exp ( T _ { i , j } ^ { f g } , \\cos ( I _ { i , m } ^ { f g } ) / \\tau ) } } \\end{array}\n$$\n\nSymmetrically we get $\\mathcal { L } _ { f g } = ( \\mathcal { L } _ { t 2 v } ^ { f g } + \\mathcal { L } _ { v 2 t } ^ { f g } ) / 2$ . This fine-grained alignment mechanism helps develop a sophisticated understanding of which visual elements trigger specific emotional responses (see implementation details in Algorithm 1 and Fig. 2 in Supp.).\n\nOptimal Transport Loss (OT). Now that we have the fine-grained loss for specific connections (zoom-in), such as matching a smiling face region with text about happiness or tearful eyes with sadness, we take a bigger-picture (zoom-out) via optimal transport [51]. Mathematically, it solves the matching problem by finding the most efficient overall assignment between all image regions and all text descriptions while respecting their similarity as in the matrix $\\mathbf { S } = \\langle \\mathbf { I } _ { \\mathrm { f g } } , \\mathbf { T } _ { \\mathrm { f g } } \\rangle _ { \\mathrm { b a t c h } }$ , which translates into transportation costs (distance matrix) as: $\\mathbf { W } = \\mathbf { 1 } - \\mathbf { S }$ . Then, by asking the question, ”What’s the most efficient way to match all the emotional elements in these images with all the emotional concepts in these texts?”, we use the Sinkhorn algorithm to find the optimal global assignment while respecting local emotional nuances.\n\n$$\n\\begin{array} { r l } & { \\mathbf { T } _ { o t } = \\sinh \\mathrm { k h o r n } ( \\mathbf { W } ) } \\\\ & { \\qquad = \\arg \\underset { \\mathbf { P } \\in \\Pi ( \\mathbf { a } , \\mathbf { b } ) } { \\operatorname* { m i n } } \\langle \\mathbf { W } , \\mathbf { P } \\rangle - \\epsilon H ( \\mathbf { P } ) } \\end{array}\n$$\n\nwhere a and b are uniform distributions over image regions and text features respectively, while $\\Pi ( \\mathbf { a } , \\mathbf { b } )$ is the set of all possible transport plans with these marginals, and $H ( \\mathbf { P } )$ is an entropy regularization term.\n\nHere higher values in $T$ represent preferred transport paths (where it’s “cheaper” to move mass), hence amplifying signals from high-similarity pairs while downplaying mismatches. This is particularly valuable for emotions, which are often ambiguous and overlapping. The final OT loss is computed as $\\mathcal { L } _ { o t } = \\mathrm { C r o s s E n t r o p y ( T _ { \\mathrm { o t } } \\odot S , I ) } .$ , where I is the identity matrix (see implementation details in Algorithm 1 in the Supp.).\n\nOur total loss combines all components as:\n\n$$\n{ \\mathcal { L } } _ { \\mathrm { m o o d i f y c l i p } } = \\lambda _ { f } \\cdot { \\mathcal { L } } _ { f } + \\lambda _ { s } \\cdot { \\mathcal { L } } _ { s } + \\lambda _ { f g } \\cdot { \\mathcal { L } } _ { f g } + \\lambda _ { o t } \\cdot { \\mathcal { L } } _ { o t }\n$$\n\n# 3.3. Moodifier\n\nWith MoodifyCLIP fine-tuned on MoodArchive, we propose Moodifier, a training-free, emotion-driven image editing system. The Moodifier workflow is illustrated in Fig. 3 and Algorithm 1. It first extracts visual features $f _ { V } ~ = ~ \\operatorname { E n c } _ { \\mathrm { v i s } } ( V )$ from the source image, enabling MLLM (in our case we applied LLaVA-NeXT) to generate emotion-specific prompts $P _ { E } = \\mathbf { M L M } ( f _ { V } , E )$ and attention maps $M _ { E }$ that identify where modifications should occur (see Supp. for instruction prompts). These outputs then guide the diffusion process to produce the emotionally transformed image. More specificially, we adopt the noniterative inversion approach from [52] to obtain the latent representation $z$ . Inspired by [38], we then manipulate the cross-attention mechanisms within the diffusion process to achieve precise emotional transformations. We operate on the key insight that cross-attention maps define the relationship between spatial image features and textual concepts.\n\nAlgorithm 1 Moodifier: MLLM-Enhanced Emotional Editing   \n\n<html><body><table><tr><td></td><td>Require:Source image V, target emotion E</td></tr><tr><td>Ensure: Emotionally edited image V'</td><td></td></tr><tr><td>1:fv ←Encvis(V)</td><td></td></tr><tr><td>2:PE,ME ←MLLM(fv,E))</td><td></td></tr><tr><td>3:PE ←MoodifyCLIP(PE)</td><td></td></tr><tr><td>4:z ←Invdm(V)</td><td></td></tr><tr><td>5:Initialize z*← z</td><td></td></tr><tr><td>6:fort=T,T-1,...,1do</td><td></td></tr><tr><td>7: Zt-1,Mt ←DM(zt,0,t)</td><td></td></tr><tr><td></td><td></td></tr><tr><td>8: M*←DM(z*,PE,t)</td><td></td></tr><tr><td>9: Mt ←BlendMaps(Mt,M*,ME,t)</td><td></td></tr><tr><td>10: 2-1←DM(2*,PE,t){M←Mt}</td><td></td></tr><tr><td>11:</td><td>2-1←(1-1Mε>0)①zt-1+IMε>0① z-1</td></tr><tr><td>12: end for</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>13: return z*</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></table></body></html>\n\nIn text-conditioned diffusion models, each diffusion step computes attention maps $M _ { t }$ through: $\\begin{array} { r l } { M _ { t } } & { { } = } \\end{array}$ Softmax $\\left( \\frac { Q K ^ { T } } { \\sqrt { d } } \\right)$ where $Q ~ = ~ \\ell _ { Q } ( \\phi ( z _ { t } ) )$ represents visual feature queries, and $K = \\ell _ { K } ( \\psi ( P ) )$ represents textual feature keys. The cell $M _ { i , j }$ defines the influence of the $j$ -th textual token on the $i$ -th pixel. For emotional editing, we need precise control over which image regions should change and how. Our MLLM generates not only detailed prompts $P _ { E }$ but also spatial attention maps $M _ { E }$ that identify emotion-relevant regions. As shown in Algorithm 1, DM (Diffusion Model) refers to a single step of the diffusion process that denoises the latent representation while computing cross-attention maps between visual and textual features. Specifically, $\\mathrm { D M } ( { \\boldsymbol { z } } _ { t } , P _ { E } , t )$ computes one denoising step from noise level $t$ to $t - 1$ conditioned on prompt $P$ , producing both the denoised latent $z _ { t - 1 }$ and the attention maps $M _ { t }$ . We blend attention maps from the source image with those generated for the target emotion: $\\tilde { M } _ { t } = \\mathrm { B l e n d M a p s } ( M _ { t } , M _ { t } ^ { \\ast } , M _ { E } , t )$ . This blending function applies attention control to preserve structural information from the source image while enabling targeted emotional modifications guided by the auto-generated attention maps $M _ { E }$ :\n\n$$\n\\tilde { M } _ { t } = \\left\\{ \\begin{array} { l l } { \\mathrm { R e f i n e } ( M ^ { \\mathrm { s r c } } , M ^ { \\mathrm { t g t } } , M _ { E } ) } & { t \\geq \\tau _ { c } } \\\\ { M ^ { \\mathrm { t g t } } } & { t < \\tau _ { c } } \\end{array} \\right.\n$$\n\n$$\n\\mathrm { R e f i n e } ( M ^ { \\mathrm { s r e } } , M ^ { \\mathrm { t g t } } , M _ { E } ) _ { i , j } = \\left\\{ \\begin{array} { l l } { ( M ^ { \\mathrm { t g t } } ) i , j } & { \\mathrm { i f } \\ ( i , j ) \\in M _ { E } } \\\\ { ( M ^ { \\mathrm { s r c } } ) _ { i , j } } & { \\mathrm { o t h e r w i s e } } \\end{array} \\right.\n$$\n\nwhere $\\tau _ { c }$ controls attention strength. At early steps $( t \\geq \\tau _ { c } )$ , refined attention is used to establish structure while preserving identity, whereas in later steps $( t < \\tau _ { c , } )$ ), target attention maps enhance emotional details. Finally, the emotion stimulus maps $M _ { E }$ smoothly blend source and target latents only in regions that should express the target emotion, while preserving the rest of the image.\n\n# 4. Experiments\n\n# 4.1. Experiment Settings\n\nWe evaluate our approach from two perspectives: (1) the zero-shot representation learning performance of the MoodifyCLIP model, and (2) the image editing performance of the Moodifier framework.\n\nMoodifyCLIP Evaluation. We evaluate MoodifyCLIP on zero-shot emotion classification and image-text retrieval using 5-fold cross-validation. For classification, we test on Emotion6 [8], EmoSet [9], and Emotic [10], reporting Top1/2/3 accuracy against three CLIP variants in both B/16 and $\\mathrm { L } / 1 4$ sizes. For retrieval, we evaluate on SentiCap [53], Affection [54], and MoodArchive- ${ \\displaystyle \\langle { \\cal I } | _ { \\mathrm { L } } }$ (5k human-verified pairs held out from training), reporting $\\mathbf { R } @ 1$ and $\\operatorname { R @ 5 }$ for both retrieval directions.\n\nMoodifier Evaluation. Evaluating emotion-driven image editing presents a fundamental challenge: the absence of standardized ground truth. Unlike tasks with welldefined objectives (e.g., “add a cat”), emotional transformations lack clear reference images. Traditional fidelity metrics such as PSNR and SSIM remain essential for ensuring that edited images preserve content integrity, preventing cases where emotional accuracy is achieved at the cost of extreme distortion or identity loss.\n\nTo balance structure-preserving evaluations with emotion-specific assessments, we employ a two-pronged approach for evaluating Moodifier: (1) quantitative metrics measuring both preservation and transformation aspects, and (2) human evaluation to assess visual appeal and emotional accuracy. Our metrics are computed hierarchically: using 240 images (30 images per category across 8 categories: face, bag, bracelet, clothes, earring, necklace, ring, and vase from MoodArchive, no overlap with MoodifyCLIP training set) as source, first at the source-synthesized pair level for each emotion, then averaged across all 27 emotions per source image, then across all 30 images per category, and finally across all 8 categories to obtain overall model-level performance. We measure structural distance between the source and synthesized images’ structural features; background preservation through PSNR, LPIPS, MSE, and SSIM; emotional transformation via the CLIP Similarity score between synthesized images and target emotions; and human evaluation through MTurk assessments (see more details in Fig. 4 and Sec. 5.3 in Supp.). While CLIP may overlook subtle nuances and human evaluation is inherently subjective, their combination provides complementary insights, ensuring both accuracy and practical usability. As shown in Table 4, we compare our Moodifier against several state-of-the-art image editing methods [1–3, 46, 47, 52, 55–58].\n\n![](images/6fc099d0b9d3ef81fa6bb202b3fddc647e23d458dbcb41a86cfa6aae4bf01cfe.jpg)  \nFigure 3. Our MLLM-enhanced Moodifier (right panel (3)) integrates ideas from instruction-based and attention control approaches, converting simple commands into rich emotional prompts for targeted editing. We applied LLaVA-NeXT for the MLLM. The bottom of panel (3) demonstrates that our system offers both fully automated processing and optional region selection, while ensuring natural transitions by modifying latent representations instead of using inpainting. The left panels (1) and (2) summarize existing paradigms from previous works for comparison.\n\n# 4.2. Result Analysis\n\nMoodifyCLIP Performance. Tables 1 and 2 show MoodifyCLIP consistently outperforms baseline CLIP models across emotion classification and retrieval tasks. Using 5- fold cross-validation, we observe significant improvements, especially on challenging datasets like Emotic. While all models perform better on MoodArchive- $ \\displaystyle \\langle 5 { \\bf k } \\$ due to caption format similarity, improvements on SentiCap and Affection confirm generalization capability. Table 3 shows both loss components contribute: fine-grained alignment identifies emotion-specific elements, while optimal transport optimizes global emotional matching, with their combination delivering strongest results.\n\nMoodifier Image Editing Performance. Moodifier’s two-pronged evaluation, combining quantitative metrics and human assessment, highlights its strength in achieving high emotional accuracy (as evidenced by CLIP scores and human preference) while preserving structural integrity. This balance is essential for practical applications, ensuring accurate emotional transformations without compromising content recognition.\n\nQuantitatively, results in Table 4 demonstrate this balance, with Moodifier achieving competitive performance on structural metrics while outperforming significantly in emotional accuracy measures. Methods focusing on pixel-level fidelity (Plug-and-Play, Inversion-Free Editing) struggle with emotional expression, whereas those making bolder changes (Instruct-Diffusion) sacrifice structural integrity. Moodifier strikes an optimal balance, achieving the best LPIPS score (94.9) and CLIP similarity (16.13) while preserving structural integrity. Moreover, MTurk human evaluations confirm that users prioritize effective emotional transformation while valuing original integrity. The high overall preference score validates our balanced approach.\n\nQualitatively, the comparisons in Fig. 4 show the limitations of competing methods, including identity change (ControlNet), artifacts (InstructPix2Pix, Instruct\n\nTable 1. Results of MoodifyCLIP for zero-shot classification across three emotional datasets (Emotion6, EmoSet, and Emotic). Top-1, Top-2, and Top-3 accuracy $( \\% )$ are reported. Bold numbers indicate the best performance. Details of specific CLIP variants in Supp.   \n\n<html><body><table><tr><td rowspan=\"2\"></td><td rowspan=\"2\"></td><td colspan=\"3\">Emotion6</td><td colspan=\"3\">EmoSet</td><td colspan=\"3\">Emotic</td></tr><tr><td>Top-1</td><td>Top-2</td><td>Top-3</td><td>Top-1</td><td>Top-2</td><td>Top-3</td><td>Top-1</td><td>Top-2</td><td>Top-3</td></tr><tr><td rowspan=\"4\">B/16</td><td>CLIPCommon Pool</td><td>41.82±1.72</td><td>68.99±1.80</td><td>83.13±0.79</td><td>40.07±0.24</td><td>62.73±0.26</td><td>74.43±0.34</td><td>27.59±0.44</td><td>39.67±0.75</td><td>48.74±0.57</td></tr><tr><td>CLIP DataComp</td><td>47.98±1.53</td><td></td><td>86.57±1.92</td><td>45.88±0.35</td><td>65.33±0.32</td><td>76.73±0.25</td><td>24.28±0.87</td><td>36.98±0.44</td><td>46.21±0.43</td></tr><tr><td>CLIP LAION</td><td>50.30±1.05</td><td>71.924101</td><td>84.80±3.11</td><td>49.43±0.43</td><td>70.74±0.40</td><td>81.96±0.20</td><td>23.94±0.61</td><td>35.70±0.57</td><td>44.01±0.39</td></tr><tr><td>MoodifyCLIP (Ours)</td><td>55.86±0.84</td><td>77.93±1.72</td><td>90.66±1.50</td><td>46.96±1.31</td><td>69.58±1.93</td><td>81.22±1.32</td><td>38.35±0.76</td><td>53.49±0.84</td><td>61.95±0.56</td></tr><tr><td rowspan=\"4\">L/14</td><td>CLIPCommon Pool</td><td>49.95 ±0.92</td><td>74.75±2.31</td><td>87.02±1.73</td><td>46.62±0.25</td><td>68.71±0.31</td><td>80.49±0.31</td><td>27.55±0.49</td><td>41.55±0.52</td><td>50.34±0.64</td></tr><tr><td>CLIP DataComp</td><td>51.77±2.48</td><td>73.94±1.52</td><td>87.63±1.13</td><td>48.54±0.38</td><td>67.43±0.28</td><td>78.63±0.27</td><td>26.02±0.75</td><td>39.20±0.83</td><td>47.95±0.67</td></tr><tr><td>CLIP LAION</td><td>55.76±0.29</td><td>79.39±2.27</td><td>89.80±1.72</td><td>49.98±0.40</td><td>70.60±0.40</td><td>81.83±0.30</td><td>21.53±0.26</td><td>33.07±0.20</td><td>41.53±0.32</td></tr><tr><td>MoodifyCLIP (Ours)</td><td>54.83±2.32</td><td>80.51±0.98</td><td>91.11±0.79</td><td>56.33±0.29</td><td>72.49±0.27</td><td>83.56±0.26</td><td>38.40±0.79</td><td>57.09±0.78</td><td>67.12±0.82</td></tr></table></body></html>\n\nTable 2. Results of MoodifyCLIP for zero-shot image-text retrieval across three datasets (SentiCap, Affection, and MoodArchive-5k) with both Image-to-Text and Text-to-Image settings. Details of specific CLIP variants in Supp.   \n\n<html><body><table><tr><td rowspan=\"3\"></td><td rowspan=\"3\"></td><td colspan=\"4\">SentiCap Image-to-Text</td><td colspan=\"4\">Affection Image-to-Text</td><td colspan=\"4\">MoodArchive-5k</td></tr><tr><td>R@1</td><td>R@5</td><td colspan=\"2\">Text-to-Image R@1</td><td colspan=\"2\">R@1</td><td colspan=\"2\">Text-to-Image</td><td colspan=\"2\">Image-to-Text</td><td colspan=\"2\">Text-to-Image</td></tr><tr><td>29.98±0.98</td><td>65.08±1.61</td><td>28.08±1.02</td><td>R@5 63.68±1.96</td><td>26.70±0.41</td><td>R@5</td><td>R@1 25.36±1.09</td><td>R@5</td><td>R@1 79.22±1.02</td><td>R@5</td><td>R@1</td><td>R@5 89.26±0.92</td></tr><tr><td rowspan=\"5\">B/16</td><td>CLIPCommonPool CLIP DataComp</td><td>41.08±1.45</td><td>78.46±0.93</td><td>39.74±0.81</td><td>76.22±0.58</td><td>40.30±1.55</td><td>54.02±0.83 66.98±0.50</td><td>37.74±0.92</td><td>52.18±0.55 63.96±1.45</td><td>82.90±0.98</td><td>94.56±0.73 98.42±0.25</td><td>69.60±0.89</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>77.44±0.92</td><td>96.18±0.57</td></tr><tr><td>CLIPLAION</td><td>42.54±0.57</td><td>79.24±1.03</td><td>41.24±0.36</td><td>77.80±1.05</td><td>40.78±0.69</td><td>68.20±1.16</td><td>39.66±0.56</td><td>66.84±1.17</td><td>83.22±0.69</td><td>98.24±0.34</td><td>78.40±1.14</td><td>96.62±0.44</td></tr><tr><td>MoodifyCLIP (Ours)</td><td>41.10±1.30</td><td>78.46±0.50</td><td>40.40±0.81</td><td>78.54±1.37</td><td>42.58±1.41</td><td>70.60±1.20</td><td>43.70±0.73</td><td>72.58±1.05</td><td>84.52±0.46</td><td>99.02±0.51</td><td>81.20±0.71</td><td>97.96±0.45</td></tr><tr><td>CLIPCommonPool</td><td>41.94±0.33</td><td>78.54±0.59</td><td>41.08±1.13</td><td>77.40±0.62</td><td>41.64±1.23</td><td>69.16±1.05</td><td>40.94±0.72</td><td>67.44±1.30</td><td>83.46±1.08</td><td>98.48±0.32</td><td>77.96±1.21</td><td>95.94±0.50</td></tr><tr><td rowspan=\"3\">L/14</td><td>CLIP DataComp</td><td>43.72±1.04</td><td>80.08±0.97</td><td>42.46±0.30</td><td>79.18±0.47</td><td>44.14±0.87</td><td>70.96±1.32</td><td>42.84±1.77</td><td>69.10±1.04</td><td>84.04±0.96</td><td>98.62±0.50</td><td>79.92±0.82</td><td>96.84±0.45</td></tr><tr><td>CLIPLAION</td><td>43.94±0.52</td><td>80.38±1.22</td><td>42.40±0.89</td><td>79.62±1.46</td><td>43.90±1.44</td><td>70.34±1.24</td><td>44.44±0.29</td><td>69.66±1.74</td><td>85.02±0.77</td><td>98.68±0.29</td><td>81.28±0.15</td><td>97.20±0.49</td></tr><tr><td>MoodifyCLIP(Ours)</td><td>44.72±1.10</td><td>80.46±1.14</td><td>44.90±0.86</td><td>82.18±0.70</td><td>46.34±0.66</td><td>74.02±0.82</td><td>48.56±0.91</td><td>76.56±1.22</td><td>86.16±0.37</td><td>98.84±0.38</td><td>85.18±0.54</td><td>98.96±0.29</td></tr></table></body></html>\n\nTable 3. Ablation study of different CLIP losses added to MoodifyCLIP. We use ViT-L/14 as the image backbone.   \n\n<html><body><table><tr><td colspan=\"2\"></td><td colspan=\"3\">Text Retrieval</td><td colspan=\"3\">Image Retrieval</td><td colspan=\"3\">Classification</td></tr><tr><td>CLIP Losses</td><td></td><td>SentiCap R@1</td><td>Affection</td><td>MoodArchive-5k</td><td>SentiCap</td><td>Affection</td><td>MoodArchive-5k</td><td>Emotion6</td><td>EmoSet</td><td>Emotic</td></tr><tr><td>FG ×</td><td>OT ×</td><td>41.14±0.84</td><td>R@1 40.68±1.30</td><td>R@1 83.56±1.13</td><td>R@1 40.38±1.20</td><td>R@1 42.76±1.59</td><td>R@1 84.08±0.48</td><td>Acc.(%) 48.03±2.32</td><td>Acc.(%) 52.56±0.13</td><td>Acc.(%) 34.97±0.31</td></tr><tr><td>√</td><td>×</td><td>42.46±0.89</td><td>42.64±0.96</td><td>84.36±0.90</td><td>40.46±1.40</td><td>44.00±0.75</td><td>84.58±0.52</td><td>51.82±1.40</td><td>54.10±0.28</td><td>37.85±0.62</td></tr><tr><td></td><td>√</td><td>41.56±0.59</td><td>41.28±0.71</td><td>84.20±0.99</td><td>39.64±0.97</td><td></td><td>84.30±0.55</td><td>48.78±1.45</td><td>52.93±0.22</td><td></td></tr><tr><td>× √</td><td>√</td><td>44.72±1.10</td><td>46.34±0.66</td><td>86.16±0.37</td><td>44.90±0.86</td><td>48.56±0.91 42.16±1.04</td><td>85.18±0.54</td><td>54.83±2.32</td><td>56.33±0.29</td><td>38.40±0.79 35.50±0.76</td></tr></table></body></html>\n\nTable 4. Comparison of image editing methods using emotion-based descriptive prompts. Emotions were adapted to category-appropriate attributes for each domain (see details in the Supp.). $^ { * } { \\mathrm { C L I P } }$ score is measured via torchmetrics using pretrained OpenAI CLIP-ViT-LargePatch14, not our MoodifyCLIP, to ensure unbiased evaluation. Human preference metrics represent hit rates $( \\% )$ from MTurk evaluations where participants selected their top 3 most-preferred models for each image, showing strong preference for Moodifier.   \n\n<html><body><table><tr><td>Method</td><td>Structure</td><td colspan=\"4\">Background Preservation</td><td>CLIP Score*</td><td colspan=\"3\">Human Preference (%)</td></tr><tr><td></td><td>Distancex103</td><td>PSNR ↑</td><td>LPIPSx103↓</td><td>MSEx104↓</td><td>SSIMx102</td><td>Similarity ↑</td><td>Visual Appeal ↑</td><td>Emotion Acc ↑</td><td>Overall ↑</td></tr><tr><td>ControlNet</td><td>21.9</td><td>22.01</td><td>102.3</td><td>104.8</td><td>80.3</td><td>12.80</td><td>28.9</td><td>24.7</td><td>26.8</td></tr><tr><td>Instruct-Pix2Pix</td><td>22.0</td><td>23.17</td><td>112.6</td><td>100.2</td><td>81.0</td><td>13.27</td><td>38.5</td><td>35.8</td><td>34.3</td></tr><tr><td>Instruct-Diffusion</td><td>34.5</td><td>18.55</td><td>175.8</td><td>227.6</td><td>74.2</td><td>13.24</td><td>19.3</td><td>20.5</td><td>19.8</td></tr><tr><td>MasaCtrl</td><td>18.4</td><td>21.52</td><td>144.2</td><td>76.2</td><td>79.4</td><td>12.80</td><td>27.4</td><td>26.3</td><td>24.9</td></tr><tr><td>Plug-and-Play</td><td>13.9</td><td>22.02</td><td>123.5</td><td>72.3</td><td>80.5</td><td>12.85</td><td>36.2</td><td>33.5</td><td>34.8</td></tr><tr><td>Edit Friendly</td><td>15.1</td><td>21.39</td><td>130.7</td><td>104.9</td><td>78.9</td><td>13.47</td><td>28.6</td><td>26.9</td><td>27.5</td></tr><tr><td>StyleDiffusion</td><td>20.4</td><td>17.42</td><td>159.7</td><td>219.0</td><td>70.9</td><td>13.42</td><td>22.7</td><td>23.4</td><td>21.3</td></tr><tr><td>Null-text Inversion</td><td>16.3</td><td>23.04</td><td>152.1</td><td>81.9</td><td>80.0</td><td>13.76</td><td>32.5</td><td>37.2</td><td>36.0</td></tr><tr><td>Direct Inversion</td><td>18.5</td><td>23.34</td><td>134.2</td><td>78.6</td><td>81.1</td><td>13.76</td><td>30.8</td><td>33.7</td><td>31.6</td></tr><tr><td>Inversion-Free Editing</td><td>14.6</td><td>24.11</td><td>96.8</td><td>80.1</td><td>82.9</td><td>14.23</td><td>45.6</td><td>47.9</td><td>43.7</td></tr><tr><td>Moodifier (Ours)</td><td>17.4</td><td>22.19</td><td>94.9</td><td>127.3</td><td>82.2</td><td>16.13</td><td>66.7</td><td>73.3</td><td>80.0</td></tr></table></body></html>\n\nDiffusion), weak emotional conveyance (StyleDiffusion, EditFriendly), or overly subtle modifications (MasaCtrl, Plug-n-Play). Moodifier, built upon MoodArchive and MoodifyCLIP, effectively addresses these limitations and it introduces meaningful emotional elements while maintaining strong structural integrity across diverse objects and emotions.\n\nThe ablation study (Fig. 5, Table 5) demonstrates that using prompts alone introduces emotional elements but sacrifices structural integrity, whereas using masks alone pre\n\n![](images/8882d2f4b3c9d81e54ed2cf75f36683fa16d1efb17266cd9f636f54419d422c0.jpg)  \nFigure 4. Qualitative comparisons with other editing methods. Zoom in for better view.\n\n<html><body><table><tr><td rowspan=\"2\">CLIPModel</td><td colspan=\"2\">Edit Method</td><td rowspan=\"2\">Structure</td><td colspan=\"4\">Background Preservation</td><td rowspan=\"2\">CLIP Score Similarity↑</td></tr><tr><td>Detail Prompts (PE)</td><td>AttnMask(Mε)</td><td>Distancex103↓ PSNR↑</td><td>LPIPSx103↓</td><td>MSEx104↓</td><td>SSIMx102↑</td></tr><tr><td rowspan=\"4\">CLIP-ViT-L-14</td><td>×</td><td>×</td><td>23.33</td><td>18.81</td><td>150.2</td><td>155.1</td><td>61.40</td><td>15.03</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>8.32</td><td></td></tr><tr><td>>×</td><td>×></td><td>22.30</td><td>19.01</td><td>146.2</td><td>1524</td><td></td><td>15.15</td></tr><tr><td>√</td><td>√</td><td>18.32</td><td>20.58</td><td>116.1</td><td>132.9</td><td>82.23</td><td>15.64</td></tr><tr><td rowspan=\"2\">MoodifyCLIP-ViT-L-14</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>></td><td>×></td><td>21.90</td><td>19.95</td><td>148.7</td><td>147.2</td><td>85.6</td><td>16.07</td></tr></table></body></html>\n\nTable 5. Ablation study of Moodifier variants. Note that no facial images are included in this study due to their fixed editing regions; this explains differences in the last row compared to Table 4. Bold values indicate best performance; † values indicate second-best performance.\n\n![](images/c075d2bb0289d696b43b3518f6293bcaa728c4289c1c68e2df6158630104691f.jpg)  \nFigure 5. A qualitative example for ablation. Ours allows faithful and rich semantic changes as well as better structural integrity perseveration. (See notation reference from Fig. 3, where $P _ { E }$ represents the detailed MLLM-generated prompt and $M _ { E }$ denotes the emotion stimulus masks). Zoom in for better view.\n\nserves structure but results in minimal emotional transformation. Combining both components yields optimal results, confirming that emotional transformation requires guidance on both what to change (masks) and how to change it (emotion-rich prompts).\n\n# 5. Conclusion and Discussion\n\nIn conclusion, we introduce an integrated framework consisting of MoodArchive (a dataset of $^ { 8 \\mathbf { M } + }$ images paired with emotion-relevant captions), MoodifyCLIP (a CLIP model specialized in the emotion dimension), and Moodifier (an MLLM-enhanced emotion-driven image editor). Our approach bridges abstract emotions and concrete visual changes while preserving content integrity. Extensive evaluations demonstrate our framework’s ability to achieve high emotional accuracy while maintaining structural integrity, empowering creators to efficiently visualize emotional variations across diverse applications.\n\nIn our future work, we will continue to increase the percentage of image-caption pairs in MoodArchive verified by human evaluators. We also recognize that the lack of standardized benchmarks for evaluating emotion-driven editing remains an open challenge. To address this, we plan to develop comprehensive datasets featuring images paired with their emotionally transformed versions, validated by human evaluators.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决了情感驱动图像编辑中的核心挑战：如何将抽象情感概念精确转化为具体的视觉属性修改，同时保持图像内容的完整性。现有方法（如[1-3]）要么提供过于简化的调整（如通用滤镜），要么引入不希望的伪影，破坏设计完整性（见图4）。\\n> *   该问题在创意产业（如时尚设计、动画制作、电子商务）中具有重要价值，能够显著提升创意工作流程的效率和质量，例如：动画艺术家可生成情感序列同时保持角色身份，电子商务平台可为不同客户群体定制产品图像。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一个集成框架，包括三个互补组件：1) MoodArchive（一个带有层次情感注释的8M+图像数据集），2) MoodifyCLIP（基于MoodArchive微调的视觉语言模型），3) Moodifier（利用MLLM和MoodifyCLIP的无训练情感驱动图像编辑系统）。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **MoodArchive数据集**：包含8,000,000+图像，带有由LLaVA-NeXT生成的层次情感注释（85%通过人工验证）。在Emotic数据集上，MoodifyCLIP的Top-1准确率达38.40±0.79%，显著优于基线CLIP-L/14（27.55±0.49%）。\\n> *   **MoodifyCLIP模型**：通过细粒度对齐损失（FG）、最优传输损失（OT）和全局对比损失（GL）的组合，在MoodArchive-5k上实现R@1=86.16±0.37%，优于基线CLIP-L/14（83.46±1.08%）。\\n> *   **Moodifier系统**：在CLIP相似性得分上达16.13（vs. Inversion-Free Editing的14.23），人类评估总体偏好达80%，LPIPS=94.9×10³（结构保留最佳）。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   Moodifier通过两阶段流程桥接抽象情感与具体视觉修改：1) MLLM（LLaVA-NeXT）分析源图像生成上下文相关的编辑指令和空间注意力图（$M_E$）；2) MoodifyCLIP将指令转化为视觉变化，通过注意力控制扩散模型仅修改情感相关属性。\\n> *   设计哲学：情感在不同上下文中表现各异（如“兴奋”在时尚中表现为鲜艳图案，在珠宝中表现为复杂纹理），需通过细粒度对齐和全局最优传输联合建模。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 传统CLIP模型（如COCO/Flickr训练）无法处理复杂情感描述；现有编辑方法（如Prompt-to-Prompt[38]）缺乏情感特定指导。\\n> *   **本文的改进：** 1) MoodArchive提供情感层次化标注；2) MoodifyCLIP通过$\\\\mathcal{L}_{fg}+\\\\mathcal{L}_{ot}$实现情感-视觉对齐；3) Moodifier通过混合注意力图$\\\\tilde{M}_t=\\\\text{Refine}(M^{src}, M^{tgt}, M_E)$实现精确编辑。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **特征提取**：$f_V = \\\\text{Enc}_{vis}(V)$。\\n> 2.  **MLLM生成指令**：$P_E, M_E = \\\\text{MLLM}(f_V, E)$（指令示例见Supp）。\\n> 3.  **MoodifyCLIP嵌入**：$P_E \\\\leftarrow \\\\text{MoodifyCLIP}(P_E)$。\\n> 4.  **潜在表示提取**：$z = \\\\text{InvDM}(V)$（非迭代反转[52]）。\\n> 5.  **扩散控制**：通过$\\\\tilde{M}_t$混合源/目标注意力，其中$\\\\tau_c$控制编辑强度（早期步骤保留结构，后期增强情感细节）。\\n\\n> **案例解析 (Case Study)**\\n> *   如图1所示，将鸡尾酒裙从“优雅”编辑为“欢乐”时：1) MLLM生成“增加鲜艳色彩和流动褶皱”指令；2) $M_E$聚焦于裙摆区域；3) MoodifyCLIP确保修改符合情感语义；4) 扩散模型仅修改目标区域，保留模特身份和背景。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   对比方法包括ControlNet、Instruct-Pix2Pix、Instruct-Diffusion、MasaCtrl、Plug-and-Play、Edit Friendly、StyleDiffusion、Null-text Inversion、Direct Inversion和Inversion-Free Editing。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在CLIP相似性得分上：** Moodifier达16.13，显著优于Inversion-Free Editing（14.23）和Instruct-Pix2Pix（13.27），相对最佳基线提升13.4%。\\n> *   **在结构距离上：** Moodifier为17.4×10³，优于Instruct-Diffusion（34.5×10³），与Plug-and-Play（13.9×10³）相当但情感准确性更高（人类偏好80% vs. 34.8%）。\\n> *   **在人类评估上：** Moodifier的情感准确率（73.3%）和视觉吸引力（66.7%）均远超Null-text Inversion（37.2%和32.5%）。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   情感驱动图像编辑 (Emotion-Driven Image Editing, EDI)\\n*   多模态大语言模型 (Multimodal Large Language Model, MLLM)\\n*   视觉语言预训练 (Vision-Language Pretraining, VLP)\\n*   对比学习 (Contrastive Learning, N/A)\\n*   最优传输 (Optimal Transport, OT)\\n*   扩散模型 (Diffusion Model, DM)\\n*   注意力控制 (Attention Control, N/A)\\n*   创意产业 (Creative Industries, N/A)\"\n}\n```"
}