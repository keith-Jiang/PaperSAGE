{
    "source": "ArXiv (Semantic Scholar未收录)",
    "arxiv_id": "2507.14111",
    "link": "https://arxiv.org/abs/2507.14111",
    "pdf_link": "https://arxiv.org/pdf/2507.14111.pdf",
    "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
    "authors": [
        "Xiaoya Li",
        "Xiaofei Sun",
        "Albert Wang",
        "Jiwei Li",
        "Chris Shum"
    ],
    "categories": [
        "cs.AI",
        "cs.DC",
        "cs.LG"
    ],
    "publication_date": "未找到提交日期",
    "venue": "暂未录入Semantic Scholar",
    "fields_of_study": "暂未录入Semantic Scholar",
    "citation_count": "暂未录入Semantic Scholar",
    "influential_citation_count": "暂未录入Semantic Scholar",
    "institutions": [
        "DeepReinforce Team"
    ],
    "paper_content": "# CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning\n\nXiaoya Li, Xiaofei Sun, Albert Wang, Jiwei Li and Chris Shum\n\nDeepReinforce Team research@deepreinforce.ai\n\nProject Page\n\n# Abstract\n\nThe exponential growth in demand for GPU computing resources, driven by the rapid advancement of Large Language Models (LLMs), has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current state-of-the-art models (e.g. DeepSeek-R1, OpenAI-o1) achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning (RL) framework for CUDA optimization. The core of CUDA-L1 is a contrastive RL model, a newly-designed RL system to enhance optimization through comparative learning. Different from performing gradient updates in isolation as in previous RL models, contrastive RL performs comparative analysis of previously generated CUDA variants alongside their execution performance, enabling the model to improve by distinguishing between effective and ineffective CUDA optimization strategies.\n\nCUDA-L1 achieves unprecedented performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of $\\mathbf { \\times 1 7 . 7 }$ across all 250 CUDA kernels of KernelBench, with peak speedups reaching $\\times 4 4 9$ . Furthermore, the model also demonstrates excellent portability across GPU architectures, achieving average speedups of $\\mathbf { \\times 1 7 . 8 }$ on H100, $\\mathbf { \\nabla \\times 1 9 . 0 }$ on RTX 3090, $\\mathbf { \\times 1 6 . 5 }$ on L40, $\\mathbf { \\times 1 4 . 7 }$ on H800, and $\\mathbf { \\nabla \\times 1 3 . 9 }$ on H20 despite being optimized specifically for A100. Beyond these benchmark results, CUDA-L1 demonstrates several remarkable properties: (1) It autonomously discovers a variety of CUDA optimization techniques and learns to combine them strategically to achieve optimal performance; (2) It automatically uncovers fundamental principles of CUDA optimization, such as the multiplicative nature of optimizations and how certain \"gatekeeper\" techniques must be applied first to unlock the effectiveness of others; (3) It identifies non-obvious performance bottlenecks (such as CPU-GPU synchronization dominating compute optimizations) and rejects seemingly beneficial optimizations that actually harm performance.\n\nThe capabilities of CUDA-L1 demonstrate that, reinforcement learning can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. The trained RL model can successfully identify CUDA optimization patterns, discovers new techniques, synthesizes them to achieve speedups, and more importantly, extend the acquired reasoning abilities to new kernels. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources.\n\n![](images/f1f1c4304a2c0024efcfe54f0657a010dbe30d1020f83ed86a45550b7c886a61.jpg)  \nFigure 1: Average speedup achieved by CUDA-L1 across different architectures on KernelBench.\n\n![](images/1111c1f00eebe49a4f20b10df249af05c0a37ef6057e4d440a5e4a92f92412d3.jpg)  \nFigure 2: Overview of the CUDA-L1 training pipeline. The approach consists of three progressive stages: (1) Stage 1: Supervised Fine-tuning with Data Augmentation – We augment the training dataset with CUDA code variants generated by LLMs and fine-tune the base model on executable and correct implementations to establish foundational CUDA knowledge. (2) Stage 2: Self-supervised Learning – The model iteratively generates CUDA kernels, validates their correctness and executability, and trains on successfully validated examples, enabling autonomous improvement without human supervision. (3) Stage 3: Contrastive Reinforcement Learning – We employ contrastive learning with execution-time rewards, training the model to distinguish between faster and slower CUDA implementations, ultimately optimizing for superior performance.\n\n# Introduction\n\nThe exponential growth in demand for GPU computing resources, driven primarily by the rapid advancement and deployment of Large Language Models (LLMs), has created an urgent need for highly efficient CUDA optimization strategies. Traditionally, CUDA optimization has been a highly manual and time-intensive process, where skilled engineers must meticulously analyze memory access patterns, experiment with different thread block configurations, and iteratively profile their code through extensive trial-and-error cycles.\n\nRecent advances in LLMs [25, 24, 26, 7, 32, 9, 11, 15, 19], especially those powered with RL [10, 8, 27, 17], have demonstrated remarkable capabilities in code generation and algorithm design. RL-powered LLMs hold significant potential to revolutionize the CUDA optimization process: CUDA optimization provides a uniquely clear reward signal—execution speed—which could be directly leveraged to automatically train reinforcement learning models. By treating performance improvements as rewards, RL-powered LLMs could iteratively generate, test, and refine CUDA optimizations without human intervention. This approach would not only automate the labor-intensive optimization process, potentially saving countless engineering hours, but also opens possibilities for discovering novel speedup algorithms that may surpass human-designed solutions. Unlike human engineers who are constrained by existing knowledge and conventional approaches, these systems could explore unconventional optimization combinations and potentially discover counterintuitive strategies that deliver significant performance improvements, offering new possibilities for advancing GPU computing efficiency.\n\nDespite the promise, current performance remains limited. State-of-the-art LLM models such as DeepSeek-R1 [8] and OpenAI-o1 [10] achieve low success rates in generating optimized CUDA code (only approximately $1 5 \\%$ on KernelBench [20]), which is primarily due to the scarcity of CUDA code in training datasets. To address these limitations and unlock the potential of LLMs for automated CUDA optimization, in this work, we propose CUDA-L1, an LLM framework powered by contrastive reinforcement learning for CUDA optimization. CUDA-L1 is a pipelined framework, the core of which is a newly-designed contrastive RL framework.\n\nDifferent from previous RL models [31, 23, 22] , contrastive RL performs comparative analysis of previously generated CUDA variants alongside their execution performance, enabling the model to improving through distinguishing between effective and ineffective optimization strategies. Contrastive-RL simultaneously optimizes the foundation model through gradientbased parameter updates while fulfilling the maximum potential from the current model through contrastive analysis from high-performance CUDA variants, creating a co-evolutionary dynamic that drives superior CUDA optimization performance.\n\nCUDA-L1 delivers unprecedented improvements on the CUDA optimization task: trained on NVIDIA A100, it achieves an average speedup of $\\mathbf { \\times 1 7 . 7 }$ across all 250 KernelBench CUDA kernels, with maximum speedups reaching $\\times 4 4 9$ . Furthermore, the CUDA codes optimized specifically for A100 demonstrate excellent portability across GPU architectures, achieving average speedups of $\\mathbf { \\nabla \\times 1 7 . 8 }$ on H100, $\\mathbf { \\nabla \\times 1 9 . 0 }$ on RTX 3090, $\\mathbf { \\times 1 6 . 5 }$ on L40, $\\mathbf { \\times 1 4 . 7 }$ on H800, and $\\mathbf { \\nabla \\times 1 3 . 9 }$ on H20. In addition to benchmark results, CUDA-L1 demonstrates several remarkable properties:\n\n• Automatic Discovery of Optimization Techniques: It automatically discovers a variety of CUDA optimization techniques, e.g., memory layout optimization, operation fusion, loop unrolling, memory coalescing etc. While some of these techniques are already widely adopted in the CUDA optimization community, others remain underutilized.   \n• Optimal Combination Selection: Upon the discovery of these techniques, CUDA-L1 can identify the optimal combination of them to achieve maximum speedup for different CUDA tasks.   \n• Uncovering Fundamental Principles: CUDA-L1 is able to uncover fundamental principles of CUDA optimization, such as the multiplicative nature of optimizations and how certain “gatekeeper” techniques must be applied first to unlock the effectiveness of others.   \n• Identifying Hidden Bottlenecks: It identifies non-obvious performance bottlenecks and rejects seemingly beneficial optimizations that actually harm performance.\n\nBeyond, CUDA-L1 reveals a remarkable capability of RL in autonomous learning for CUDA optimization:\n\n1. Even starting with a foundation model with poor CUDA optimization ability, by using code speedups as RL rewards and proper contrastive RL training techniques, we can still train an RL system capable of generating CUDA optimization codes with significant speedups.   \n2. Without human prior knowledge or guidance, RL systems can independently discover CUDA optimization techniques, learn to combine them strategically, and more importantly, extend the acquired CUDA reasoning abilities to unseen kernels with significant speedups. This capability unlocks the potential for a variety of automatic CUDA optimization tasks, e.g., kernel parameter tuning, memory access pattern optimization, and different hardware adaptations, offering substantial promises to enhance GPU utilization during this period of unprecedented computational demand.\n\n# 2 CUDA-L1\n\n# 2.1 Overview\n\nExisting large language models [8, 32, 7] demonstrate significant limitations in generating executable and correct CUDA code with speedup, as reported in prior research [20]. This deficiency likely stems from the insufficient representation of CUDA code in the training datasets of these models. To address this fundamental gap, we introduce a three-stage pipelined training strategy for CUDA-L1, aiming to progressively enhances the model’s CUDA programming capabilities:\n\n1. Supervised fine-tuning via data augmentation, which aims to expand the model’s exposure to CUDA patterns and programming constructs, with the primary goal of producing correct and executable CUDA code.   \n2. Self-supervised learningm which focuses on enabling models to develop a deeper understanding of CUDA semantics and programming principles, primarily aiming to achieve significant improvements in executability and correctness, while providing moderate speedup gains.   \n3. Contrastive reinforcement learning, which is designed to significantly optimize code execution speed, with the goal of generating high-performance CUDA implementations that deliver substantial speedup.\n\nBefore we delve into the details of each stage, we provide key definitions adopted throughout the rest of this paper:\n\n1. Executability: A CUDA code is executable if it successfully compiles, launches, and executes to completion within $1 0 0 0 \\times$ the runtime of the reference implementation. Code exceeding this runtime threshold is considered unexecutable.1   \n2. Correctness: A CUDA code is correct if it produces equivalent outputs to the reference implementation across 1000 random test inputs.2   \n3. Success: A CUDA code is successful if it is both executable and correct.\n\n# 2.2 SFT via Data Augmentation\n\nIn the SFT stage, we collect a dataset by using existing LLMs to generate CUDA code snippets and selecting successful one. This dataset is directly used to fine-tune the model. Throughout this paper, we use deepseek-v3-671B [15] as the model backbone.\n\nData Collection To expand the model’s exposure to CUDA patterns, we begin with data augmentation based on reference code from 250 tasks in KernelBench, which provides the official implementations used in PyTorch. To generate executable and correct CUDA code efficiently, we leverage six existing LLM models: GPT-4o, OpenAI-o1, DeepSeek-R1, DeepSeek V3, Llama 3.1-405B Instruct, and Claude 3.7. For each model, we construct prompts using the one-shot strategy, where the prompt contains the reference code (denoted by $q _ { i } , i \\in [ 1 , 2 5 0 ] )$ and asks the LLM to generate an alternative speedup implementation. We employ multiple models to maximize the diversity of successful CUDA code generation. The detailed prompt structure is provided in Table 2. For each of the six models, we iterate through all 250 tasks. Each task allows up to 20 trials and terminates early if we successfully collect 2 trials that are both executable and correct. Notably, some tasks may fail to produce any successful code across all trials. The successful code is denoted by $d _ { i , j }$ , where $j \\in \\{ 1 , 2 , \\dots , n _ { i } \\}$ , and $n _ { i }$ denotes the number of successful code snippets for the reference code $q _ { i }$ . Through this process, we collected 2,105 successful CUDA code snippets. Now we have collected the dataset $D = \\{ ( q _ { i } , \\{ d _ { i , j } \\} _ { j = 1 } ^ { n _ { i } } ) \\} _ { i }$ .\n\nThe collected dataset $D$ is used to finetune the fundation model. The instruction to the model is the same as the prompt for dataset generation, where the reference code $q _ { i }$ is included in the instruction and the model is asked to generate an improved version. The model is trained to predict each token in $d _ { i , j }$ given the instruction.\n\n# 2.3 Self-supervised Learning\n\nNow we are presented with the finetuned model after the SFT stage, where the model can potentially generate better CUDA code with higher success rates than the original model without finetuning. We wish to further improve the model’s ability to generate successful CUDA code by exposing it to more code snippets generated by itself.\n\nWe achieve this iteratively by sampling CUDA code from the model, evaluating it for executability and correctness, removing the unsuccessful trials and keeping the successful ones. Successful ones are batched and used to update the model parameters. Using the updated model, we repeat the process: generating code, evaluating it, and retraining the model. The psudo code for the algorithm is shown in Table 2.\n\nThe self-supervised learning strategy can be viewed as a special case of the REINFORCE algorithm [31], a typical policy gradient reinforcement learning method, where the reward is set to 1 for successful trials and 0 for unsuccessful trials, without applying any baseline. Interestingly, we find this adopted training strategy to be more stable than the REINFORCE variant with baseline applied. We conjecture that this stability arises because during the self-supervised learning stage, a significant proportion of generated instances remain unsuccessful. This approach avoids the potential instability caused by applying negative updates to unsuccessful samples when using a baseline.\n\nIt is worth noting that during the self-supervised learning stage, we focus exclusively on the executability and correctness of the generated code, without considering speed as a metric. This design choice reflects our primary objective of establishing reliable code generation before optimizing for performance.\n\n# Data Augmentation Prompt — Used in Supervised fine-tuning\n\n# Task for CUDA Optimization\n\nYou are an expert in CUDA programming and GPU kernel optimization. Now you’re tasked with developing a high-performance cuda implementation of Softmax. The implementation must:\n\n• Produce identical results to the reference PyTorch implementation.   \n• Demonstrate speed improvements on GPU.   \n• Maintain stability for large input values.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   随着大型语言模型（LLMs）的快速发展，GPU计算资源需求呈指数级增长，迫切需要自动化的CUDA优化策略。当前最先进的LLM模型（如DeepSeek-R1、OpenAI-o1）在CUDA优化任务上的成功率较低（约15%），主要原因是训练数据集中CUDA代码的稀缺性。\\n> *   该问题的重要性在于，CUDA优化通常是一个高度手动且耗时的过程，自动化优化可以显著节省工程时间，并可能发现超越人工设计的优化策略。\\n\\n> **方法概述 (Method Overview)**\\n> *   本文提出CUDA-L1，一种基于对比强化学习（Contrastive RL）的自动化CUDA优化框架。其核心是通过对比分析生成的CUDA变体及其执行性能，区分有效和无效的优化策略，从而提升优化效果。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **性能提升：** 在NVIDIA A100上训练后，CUDA-L1在KernelBench的250个CUDA内核上实现了平均`×17.7`的加速，峰值加速达到`×449`。\\n> *   **跨架构可移植性：** 在H100、RTX 3090、L40、H800和H20等不同GPU架构上，平均加速分别为`×17.8`、`×19.0`、`×16.5`、`×14.7`和`×13.9`。\\n> *   **自动化优化能力：** 能够自主发现CUDA优化技术（如内存布局优化、操作融合、循环展开、内存合并等），并战略性地组合它们以实现最佳性能。\\n> *   **发现优化原则：** 揭示了CUDA优化的基本原则，如优化的乘法性质以及某些“门控”技术必须首先应用以解锁其他技术的有效性。\\n> *   **识别隐藏瓶颈：** 能够识别非显而易见的性能瓶颈（如CPU-GPU同步主导计算优化）并拒绝看似有益但实际上损害性能的优化。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   CUDA-L1的核心思想是通过对比强化学习，利用执行时间作为奖励信号，训练模型区分高效和低效的CUDA优化策略。其设计哲学是通过对比分析，使模型能够从历史生成的CUDA变体中学习优化模式。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 传统RL模型（如REINFORCE）通过孤立地执行梯度更新，缺乏对优化策略的对比分析能力。\\n> *   **本文的改进：** CUDA-L1引入对比RL框架，通过同时执行梯度更新和对比分析，实现模型与高性能CUDA变体的协同进化。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  **监督微调（SFT）阶段：** 通过数据增强生成CUDA代码变体，并微调基础模型（deepseek-v3-671B）以生成可执行且正确的CUDA代码。\\n> 2.  **自监督学习阶段：** 模型迭代生成CUDA内核，验证其正确性和可执行性，并使用成功验证的样本进行训练。\\n> 3.  **对比强化学习阶段：** 利用执行时间作为奖励信号，训练模型区分快速和慢速的CUDA实现，最终优化性能。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   DeepSeek-R1\\n> *   OpenAI-o1\\n> *   GPT-4o\\n> *   DeepSeek V3\\n> *   Llama 3.1-405B Instruct\\n> *   Claude 3.7\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在CUDA优化成功率上：** 本文方法在KernelBench数据集上实现了显著高于基线模型的成功率（具体数值未明确提供）。\\n> *   **在执行速度上：** 本文方法在NVIDIA A100上实现了平均`×17.7`的加速，峰值加速达到`×449`，远高于基线模型的性能。\\n> *   **在跨架构可移植性上：** 本文方法在H100、RTX 3090等不同GPU架构上均实现了显著的加速效果，展示了优异的可移植性。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   CUDA优化 (CUDA Optimization, N/A)\\n*   对比强化学习 (Contrastive Reinforcement Learning, CRL)\\n*   大型语言模型 (Large Language Model, LLM)\\n*   GPU计算效率 (GPU Computing Efficiency, N/A)\\n*   自动化代码生成 (Automated Code Generation, N/A)\\n*   性能加速 (Performance Speedup, N/A)\\n*   跨架构可移植性 (Cross-architecture Portability, N/A)\\n*   强化学习 (Reinforcement Learning, RL)\"\n}\n```"
}