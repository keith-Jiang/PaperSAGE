{
    "source": "ArXiv (Semantic Scholar未收录)",
    "arxiv_id": "2507.13998",
    "link": "https://arxiv.org/abs/2507.13998",
    "pdf_link": "https://arxiv.org/pdf/2507.13998.pdf",
    "title": "ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies",
    "authors": [
        "Itay Katav",
        "Aryeh Kontorovich"
    ],
    "categories": [
        "cs.LG"
    ],
    "publication_date": "未找到提交日期",
    "venue": "暂未录入Semantic Scholar",
    "fields_of_study": "暂未录入Semantic Scholar",
    "citation_count": "暂未录入Semantic Scholar",
    "influential_citation_count": "暂未录入Semantic Scholar",
    "institutions": [
        "Ben-Gurion University of the Negev"
    ],
    "paper_content": "# ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies\n\nItay Katav Department of Computer Science Ben-Gurion University of the Negev Beer Sheva, Israel katavit@post.bgu.ac.il\n\nAryeh Kontorovich Department of Computer Science Ben-Gurion University of the Negev Beer Sheva, Israel karyeh@cs.bgu.ac.il\n\n# Abstract\n\nModern multivariate time series forecasting primarily relies on two architectures: the Transformer with attention mechanism and Mamba. In natural language processing, an approach has been used that combines local window attention for capturing short-term dependencies and Mamba for capturing long-term dependencies, with their outputs averaged to assign equal weight to both. We find that for time-series forecasting tasks, assigning equal weight to long-term and short-term dependencies is not optimal. To mitigate this, we propose a dynamic weighting mechanism, ParallelTime Weighter, which calculates interdependent weights for long-term and short-term dependencies for each token based on the input and the model’s knowledge. Furthermore, we introduce the ParallelTime architecture, which incorporates the ParallelTime Weighter mechanism to deliver state-of-the-art performance across diverse benchmarks. Our architecture demonstrates robustness, achieves lower FLOPs, requires fewer parameters, scales effectively to longer prediction horizons, and significantly outperforms existing methods. These advances highlight a promising path for future developments of parallel Attention-Mamba in time series forecasting. The implementation is readily available at: ParallelTime GitHub .\n\n# 1 Introduction\n\nForecasting is one of the most important tasks in time series analysis. To address this challenge, various architectures have been proposed. The Transformer architecture [Vaswani et al., 2017], which has achieved remarkable success in natural language processing [Brown et al., 2020] and computer vision [Dosovitskiy et al., 2021], has also shown promise in time series forecasting [Nie et al., 2023]. Another successful architecture introduced in recent years is the State Space Model (SSM) [Gu et al., 2022, Smith et al., 2023]. SSM-based models, such as Mamba [Gu and Dao, 2023], have demonstrated strong performance in time series forecasting [Wang et al., 2024] and other domains.\n\nEach approach has its distinct advantages. Mamba, through its parameter initialization, produces a summary of long-term dependencies [Gu et al., 2020]. The latter allows for extraction of the leading features for forecasting, while filtering out the noise in the time series. Attention models, such as the transformer, are highly accurate and excel at capturing complex patterns and interactions across the sequence, enabling robust forecasting performance [Nie et al., 2023]. Moreover, in cases of channel independence, where each variable in a multivariate time series is processed separately using the same model weights, attention models demonstrate superior performance on datasets with similar variates series [Nie et al., 2023]. In contrast, Mamba models, such as those proposed in Wang et al. [2024], achieve better results on datasets with heterogeneous variates series.\n\nIn this paper, we propose a novel method that combines the strengths of Mamba and the attention mechanism by computing both Mamba, which captures long-term dependencies, and a small local window attention, which focuses on short-term dependencies. Recent papers in natural language processing [Dong et al., 2024] tackle this problem by computing the mean of the values and assigning equal weight to both components. In contradistinction, our approach weights each component of each token separately. In cases where more long-range dependencies are needed for the prediction, the ParallelTime Weighter gives more weight to the Mamba component. When more short-term dependency predictions are required, more weight is given to the window attention component. Additionally, we leverage registers as domain-specific global context, providing a persistent reference that captures information beyond the input series. We demonstrate that our method is robust and significantly outperforms existing approaches, on almost every benchmark dataset.\n\n![](images/5018120cd1849bcc348b191937eef8902ff692bc6ac44d1c9f26f84756bb44b1.jpg)  \nFigure 1: $\\textcircled{8}$ High level visualization of ParallelTime module. $\\textcircled{8}$ Diagram of the attention map of ParallelTime, integrating global registers, local window attention, and Mamba components.\n\nOur contributions. The main contributions of this paper are three-fold:\n\n• We propose a novel ParallelTime Weighter that selects the contributions of short-term, long-term, and global memory for each time series patch, implemented via window-based attention, Mamba, and registers, respectively, to improve the accuracy of long-term forecasting.   \n• We demonstrate that parallel Mamba-Attention architecture is currently the most effective approach for long-term time series forecasting, highlighting a promising direction for future advancements in time series forecasting models.   \n• Our model, ParallelTime, achieves SOTA performance on real-world benchmarks, delivering better results from previews models with fewer parameters and lower computational cost, a characteristic highly critical for real-time forecasting applications.\n\n# 2 Related Work\n\n![](images/cdf0c4a9e3a7322be2d8c85268a82f2677b35491680d643516278f8cf07fbe96.jpg)  \nFigure 2: Comparison of five neural network block architectures: Transformer Blocks with Registers, Transformer Blocks, (Ours) Parallel Mamba-Attention with dynamic weighting mechanism , Mamba Blocks, and Hybrid Sequential Mamba-Attention Architecture.\n\nTransformer Vaswani et al. [2017], leveraging causal self-attention layers and feed-forward networks, has laid a powerful foundation for time series forecasting [Zhou et al., 2021, 2022]. A standout example is PatchTST [Nie et al., 2023], which achieves SOTA performance by utilizing channel independence to process each variable in a multivariate time series separately. By feeding contiguous time series patches as tokens into a standard self-attention mechanism, PatchTST outperforms many previous models. In standard self-attention, each token attends to all preceding tokens to capture global dependencies. To focus on local patterns, windowed attention variants, such as those in\n\nLongFormer [Beltagy et al., 2020a] and Swin Transformer [Liu et al., 2021], restrict each token to attend only to the most recent S tokens, as illustrated in Figure 1 (B).\n\nRegisters are model parameters that function as tokens, concatenated with input tokens to provide additional global domain-specific information. They serve as a persistent reference for the model, capturing information not explicitly present in the input tokens. Registers have shown considerable promise in natural language processing, as demonstrated by Burtsev et al. [2020], and in computer vision, as explored by Darcet et al. [2024], where they enhance model performance by leveraging task-specific memory.\n\nMamba [Gu and Dao, 2023] is a State Space Model (SSM) [Gu et al., 2022, Smith et al., 2023] designed for efficient [Waleffe et al., 2024] and high-performance sequence modeling. At the core of the Mamba architecture is the HIPPO matrix [Gu et al., 2020], which prioritizes recent tokens by assigning them greater influence in the state representation while compressing older tokens into a compact, approximated summary. This approach effectively captures a condensed representation of long-range dependencies, making it well-suited for time series forecasting. S-Mamba [Wang et al., 2024] has demonstrated competitive performance across several time series forecasting benchmarks.\n\nHybrid models which combine Mamba and Attention layers in a sequential stack, have gained prominence in natural language processing, as demonstrated by models such as Jamba [Team et al., 2024] and Samba [Ren et al., 2024]. In time-series forecasting, Heracles [Patro et al., 2024] showcases the versatility and effectiveness of this approach. However, sequential stacking may introduce information bottlenecks [Dong et al., 2024] and poses challenges in determining the optimal placement of each component, potentially compromising forecasting accuracy.\n\nParallel architectures where Mamba and attention mechanisms process the same input simultaneously and their outputs are combined in some way, have recently been proposed in natural language processing. For instance, Hymba [Dong et al., 2024] proposed aggregating Mamba and attention outputs via simple averaging. However, in time series forecasting, where window attention mechanisms capture short-term dependencies at each layer, and Mamba is responsible for summarizing long-term dependencies, assigning equal weights to both long-term and short-term dependencies may not optimally capture the right amount of each dependency needed for each prediction, especially when time series variates differ significantly. To the best of our knowledge, no prior work has applied parallel Mamba-Attention models to long-term time series forecasting. We demonstrate that our novel weighted aggregation approach, ParallelTime Weighter, outperforms naive combinations, leveraging the strengths of both components to achieve state-of-the-art performance.\n\n# 3 ParallelTime\n\nProblem definition. In multivariate long-term time series forecasting, the task is to predict future values of multiple interdependent variables based on historical data. Given a multivariate time series ${ \\bf X } = ( { \\bf x } _ { 1 } , . . . , \\dot { \\bf x } _ { T } ) \\in \\mathbb { R } ^ { \\bar { N } \\times T }$ , where $N$ is the number of variables and $T$ is the number of timestamps, the goal is to forecast $H$ future values $\\mathbf { Y } = ( \\mathbf { x } _ { T + 1 } , \\dots , \\mathbf { x } _ { T + H } ) \\in \\mathbb { R } ^ { N \\times H }$ . Each $\\mathbf { x } _ { t } \\in \\mathbb { R } ^ { N }$ represents the observations of $N$ variables at time $t$ .\n\n# 3.1 Overall Architecture\n\nThe ParallelTime architecture is illustrated in Figure 3. Our model begins by decomposing the multivariate time series input into $N$ univariate series, leveraging the channel independence framework [Nie et al., 2023]. This approach enables all model weights to learn more than one variant, enhancing robustness during testing. To address distribution shifts across different time series, we apply instance normalization $( { \\mathrm { R e V I n } } )$ [Kim et al., 2022] to the input. Subsequently, a patching mechanism divides each univariate series into non-overlapping patches, treating each patch as a \"token\" with features derived from the univariate time series values. We tried overlapping patches, but they increased computational cost without improving accuracy, so they were not used.\n\nTo effectively extract both global trend and local trends from each patch, we employ a dual embedding strategy. A linear layer aggregates global information by mixing all data points within the patch, while a Conv1D layer [O’Shea and Nash, 2015] captures local trends within the patch. The global and local representations are then combined through summation to form the final ${ \\bf x } _ { e }$ patch embedding.\n\n![](images/96a572b872f62dd6b5d187433ebca1dc2cbab4373fa613849722a7087e59898a.jpg)  \nFigure 3: The architecture of ParallelTime. The input time series $\\textcircled{1}$ is sliced into non-overlapping patches. $\\textcircled{2}$ Each patch is embedded and augmented with positional encoding. The resulting tokens are processed through $N$ stacked ParallelTime blocks. Each block first normalizes the input, applies Mamba computation and $\\textcircled{3}$ windowed attention with a register in parallel, $\\textcircled{4}$ weights their outputs using a ParallelTime Weighter mechanism, and then applies normalization followed by a nonlinear feedforward layer. Finally, the output is normalized $\\textcircled{5}$ and processed through an expand-compressprojection mechanism to generate the horizon prediction.\n\nTo capture sequential order in attention models, which function as a bag of words without positional encoding [Vaswani et al., 2017], we incorporate absolute positional encoding, defined as ${ \\bf x } _ { d } = { \\bf \\Phi }$ $\\mathbf { x } _ { e } + \\mathbf { x } _ { p o s } \\in \\mathbb { R } ^ { P \\times d i m }$ , where $\\mathbf { x } _ { e }$ represents the input embedding, $\\mathbf { x } _ { p o s }$ denotes the positional encoding, and $\\mathbf { x } _ { d }$ is the resulting encoded representation.\n\n# 3.2 ParallelTime Decoder Block\n\nOur approach builds upon a decoder-only transformer architecture [Vaswani et al., 2017] As illustrated in Figure 3, the decoder is composed of a stack of $N$ identical layers, with each layer comprising two sublayers. The first sublayer integrates parallel Mamba and attention mechanisms, with their outputs processed by the ParallelTime Weighter, which dynamically allocates weights to the Mamba and attention outputs for each patch or token. The second sublayer is a non-linear feed-forward network with SiLU activation. Each sublayer begins with LayerNorm [Ba et al., 2016] and is enclosed by residual connections [He et al., 2016].\n\n# 3.3 Mamba and Windowed Attention with Global Registers\n\nMamba mechanism. To achieve high accuracy with low memory requirements, we added a Mamba block [Gu and Dao, 2023], which leverages a state-space model. Mamba’s strength lies in its high accuracy [Wang et al., 2024] and constant memory usage, making it ideal for long time series\n\nforecasting. The core operation of Mamba is defined as:\n\n$$\n\\mathbf { h } _ { t } = \\mathbf { A } \\mathbf { h } _ { t - 1 } + \\mathbf { B } \\mathbf { x } _ { t } , \\quad \\mathbf { y } _ { t } = \\mathbf { C } \\mathbf { h } _ { t } ,\n$$\n\nwhere $\\mathbf { x } _ { t } \\in \\mathbb { R } ^ { d i m }$ is the input at time $t$ , $\\mathbf { h } _ { t } \\in \\mathbb { R } ^ { d i m }$ is the hidden state, and $\\mathbf { A } , \\mathbf { B } , \\mathbf { C }$ are learnable parameters of the state-space model. The output:\n\n$$\n\\mathbf { x } _ { m a m b a } = \\mathbf { M a m b a } ( \\mathbf { x } _ { d } ) ,\n$$\n\neffectively captures long-range dependencies in the input sequence $\\mathbf { x } _ { d } \\in \\mathbb { R } ^ { P \\times \\mathrm { d i m } }$ .\n\nWindowed Attention Mechanism. To capture local interactions efficiently within each layer, we utilize a causal multi-head windowed self-attention mechanism Beltagy et al. [2020b]. This approach allows us to restrict attention to a fixed window. We select a small window size, set at a $1 : 9$ ratio relative to the number of input sequence patches, ensuring that the attention mechanism focuses solely on short-term dependencies while delegating long-term dependencies to Mamba.\n\nGlobal Registers. To incorporate global context, we introduce global register tokens, denoted as $\\mathbf { W } _ { \\mathrm { r e g } } \\in \\mathbb { R } ^ { \\breve { R } \\times d i m }$ , where $R$ is the number of registers and dim is the embedding dimension. These tokens serve as a compact repository of domain-specific global information, providing the model with access to broader contextual cues. The input sequence $\\mathbf { x } _ { d } \\in \\mathbb { R } ^ { P \\times d i m }$ , is concatenated with the global registers to form: $\\mathbf { x } _ { \\mathrm { c a t } } = \\mathrm { C o n c a t } ( \\mathbf { W } _ { \\mathrm { r e g } } , \\mathbf { x } _ { d } ) \\in \\mathbb { R } ^ { ( R + P ) \\times d i m }$ . This concatenated sequence is then processed by the causal multi-head windowed attention mechanism, yielding:\n\n$$\n\\begin{array} { r } { { \\bf x } _ { a t t } = \\mathrm { W i n A t t } ( { \\bf x } _ { \\mathrm { c a t } } ) . } \\end{array}\n$$\n\n# 3.4 ParallelTime Weighter\n\nConsidering the Mamba $\\mathbf { x } _ { m a m b a }$ , which encapsulates both short-term and long-term dependencies, and the window attention $\\mathbf { x } _ { a t t }$ , which reflects short-term dependencies alongside global dependencies obtained from the registers, to make accurate prediction for different inputs, some inputs need to have more long-term dependency and some need more global, and short-term dependencies, giving a weight to each representation isn’t enough, we want the weights to be in respect to each other, so we created the novel ParallelTime Weighter.\n\nThe attention and Mamba outputs, $\\mathbf { x } _ { \\mathrm { a t t } }$ and $\\mathbf { x } _ { \\mathrm { m a m b a } }$ , are first normalized using RMSNorm [Zhang and Sennrich, 2019] to address their differing scales. Each output is then processed by a dedicated linear transformation that compresses the dimensionality from dim to $\\sqrt { \\dim }$ , preserving essential features:\n\n$$\n\\mathbf { x } _ { \\mathrm { a t t } } ^ { \\prime } = \\mathbf { R M S N o r m } ( \\mathbf { x } _ { \\mathrm { a t t } } ) \\mathbf { W } _ { \\mathrm { a t t } } \\in \\mathbb { R } ^ { P \\times \\sqrt { \\mathrm { d i m } } } ,\n$$\n\n$$\n\\mathbf { x } _ { \\mathrm { m a m b a } } ^ { \\prime } = \\mathbf { R M S N o r m } ( \\mathbf { x } _ { \\mathrm { m a m b a } } ) \\mathbf { W } _ { \\mathrm { m a m b a } } \\in \\mathbb { R } ^ { P \\times \\sqrt { \\mathrm { d i m } } } .\n$$\n\nThese specialized linear layers effectively tailor the compression to the unique characteristics of the attention and Mamba outputs. The compressed representations are then concatenated to form a unified feature set:\n\n$$\n\\mathbf { x } _ { \\mathrm { c a t } } ^ { \\prime } = \\mathrm { C o n c a t } ( \\mathbf { x } _ { \\mathrm { a t t } } ^ { \\prime } , \\mathbf { x } _ { \\mathrm { m a m b a } } ^ { \\prime } ) \\in \\mathbb { R } ^ { P \\times 2 \\sqrt { \\mathrm { d i m } } } ,\n$$\n\nFollowing concatenation, the compressed features from the attention and Mamba branches are processed through a two-layer transformation to capture complex interactions. Inspired by the kernel trick [Hearst et al., 1998], this approach leverages higher-dimensional spaces to reveal patterns not readily discernible in lower dimensions, this step generates adaptive weights:\n\n$$\n\\begin{array} { r } { \\mathbf { x } _ { \\mathrm { w e i g h t s } } = \\sigma ( \\mathbf { R e L U } ( \\mathbf { x } _ { \\mathrm { c a t } } ^ { \\prime } \\mathbf { W } _ { 1 } ) \\mathbf { W } _ { 2 } ) \\in \\mathbb { R } ^ { P \\times 2 } , \\mathbf { W } _ { 1 } \\in \\mathbb { R } ^ { 2 \\sqrt { \\mathrm { d i m } } \\times \\mathrm { d i m } \\cdot \\mathrm { h } } , \\mathbf { W } _ { 2 } \\in \\mathbb { R } ^ { \\mathrm { d i m } - \\mathrm { h } \\times 2 } } \\end{array}\n$$\n\nwhere dim-h denotes a dimension higher than $2 { \\sqrt { \\dim } }$ , and $\\sigma$ represents the sigmoid function. We attempted to replace the sigmoid function with softmax, but it yielded suboptimal results. This observation aligns with other weight mechanisms, such as the Squeeze-and-Excitation approach [Hu et al., 2018], which also performed better with sigmoid. The weight vector is defined as $\\mathbf { x } _ { \\mathrm { w e i g h t s } } = [ \\mathbf { x } _ { \\mathrm { w e i g h t } } ^ { \\mathrm { a t t } } , \\mathbf { x } _ { \\mathrm { w e i g h t } } ^ { \\mathrm { m a m b a } } ]$ . The final output is computed as a weighted sum of the original attention and Mamba outputs:\n\n$$\n\\mathbf { x } _ { \\mathrm { o u t } } = \\mathbf { x } _ { \\mathrm { a t t } } \\cdot \\mathbf { x } _ { \\mathrm { w e i g h t } } ^ { \\mathrm { a t t } } + \\mathbf { x } _ { \\mathrm { m a m b a } } \\cdot \\mathbf { x } _ { \\mathrm { w e i g h t } } ^ { \\mathrm { m a m b a } } ,\n$$\n\nThis architecture enables the weights to dynamically balance the contributions of each branch, leading to superior performance, as demonstrated in Table 1.\n\nFollowing the decoder layers, we apply Layer Normalization (LayerNorm). Unlike standard time series forecasting architectures that simply flatten and project data, our Expand-Compress-Project approach is more efficient. We first expand the data to a higher dimension than the input $\\mathrm { d i m } \\times$ higher-dim) and then compress it to a significantly smaller dimension (dim $\\div$ some-dim) than the input dimension. This approach reduces millions of parameters while maintaining comparable performance (see Appendix 5 for details). The projection output forms our model’s prediction, which we then de-normalize using ReVIn [Kim et al., 2022].\n\n# 3.5 Loss and Optimizer\n\nWe employed the Huber loss function [Huber, 1992, Wen et al., 2019], chosen for its enhanced robustness to outliers and contribution to improved training stability. The Huber loss is defined as:\n\n$$\n\\mathcal { L } _ { \\mathrm { h u b e r } } \\left( \\boldsymbol { x } _ { t } , \\hat { \\boldsymbol { x } } _ { t } \\right) = \\left\\{ \\begin{array} { l l } { \\left( \\boldsymbol { x } _ { t } - \\hat { \\boldsymbol { x } } _ { t } \\right) ^ { 2 } \\frac { 1 } { 2 } , } & { \\mathrm { i f ~ } \\left| \\boldsymbol { x } _ { t } - \\hat { \\boldsymbol { x } } _ { t } \\right| \\leq \\delta . } \\\\ { \\delta \\times \\left( \\left| \\boldsymbol { x } _ { t } - \\hat { \\boldsymbol { x } } _ { t } \\right| - \\frac { 1 } { 2 } \\times \\delta \\right) , } & { \\mathrm { e l s e } . } \\end{array} \\right.\n$$\n\nwhere $\\boldsymbol { x } _ { t }$ is the true value, $\\hat { x } _ { t }$ is the predicted value, and $\\delta$ is a hyperparameter modulating the balance between L1 and L2 loss characteristics.\n\nFor optimization, we used the Adam optimizer [Kingma and Ba, 2015], which provides efficient adaptive learning rate adjustments. Further details regarding the training process and specific hyperparameter settings can be found in Appendix 8.2.\n\n# 4 Evaluations\n\n# 4.1 Baselines and Experimental Setup\n\nTo assess the performance of our proposed ParallelTime, we compare it against several SOTA models for long time series forecasting. These include Transformer-based models such as PatchTST [Nie et al., 2023] and FEDFormer [Zhou et al., 2022], Linear models like DLinear [Zeng et al., 2023] and N-BEATS [Oreshkin et al., 2020], foundational models including Moment [Goswami et al., 2024], GPT4TS [Zhou et al., 2023], and TimesNet [Wu et al., 2023], as well as ensemble models LightTS [Campos et al., 2023] and Stationary. We evaluate all models on eight widely used datasets: Electricity, Weather, Illness, Traffic, and four ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2). For detailed dataset descriptions, see Appendix 8.1.\n\nWe adopt standard evaluation protocols with prediction horizons of $T \\in \\{ 2 4 , 3 6 , 4 8 , 6 0 \\}$ for the Illness dataset and $T \\in \\{ 9 6 , 1 9 \\bar { 2 } , 3 3 6 , 7 2 0 \\}$ for all other datasets. Performance metrics for baseline models are obtained from Goswami et al. [2024], while our model’s results are newly computed. A look-back window of $L = 5 1 2$ is used for all models, except DLinear, which employs an optimized input length of $L = 9 6$ to enhance performance.\n\n# 4.2 Main Results\n\nComprehensive forecasting results are listed in Table 1, with the best performance highlighted in red and the second best underlined. A lower Mean Squared Error (MSE) and Mean Absolute Error (MAE) indicate more accurate predictions. Our proposed model, ParallelTime, demonstrates exceptional performance across a diverse set of datasets and prediction horizons, consistently outperforming a range of state-of-the-art models, ParallelTime achieves the best forecasting accuracy in a significant number of scenarios, particularly excelling in datasets such as Weather, ETTh1, ETTh2, ETTm2, Electricity, Traffic, and Illness.\n\nOur model, ParallelTime, surpasses SOTA models, including PatchTST [Nie et al., 2023] and Moment [Goswami et al., 2024], in long-term time series forecasting. Although PatchTST remains a strong contender, ranking as the second-best performer, and Moment excels on the ETTm2 dataset (likely due to its training data), ParallelTime achieves superior performance with significantly fewer parameters and lower computational complexity, compared to PatchTST (see Table 2). Specifically, ParallelTime reduces MSE by an average of $4 . 2 5 \\%$ and MAE by $4 . 3 1 \\%$ relative to PatchTST. This\n\nTable 1: Full results of in-domain forecasting experiments. A lower MSE or MAE indicates a better prediction. The results obtained from [Goswami et al., 2024]. Red: the best, Underline: the 2nd best.   \n\n<html><body><table><tr><td colspan=\"2\">Methods</td><td colspan=\"2\">ParallelTime</td><td colspan=\"2\">MOMENT</td><td colspan=\"2\">GPT4TS</td><td colspan=\"2\">PatchTST</td><td colspan=\"2\">DLinear</td><td colspan=\"2\">TimesNet</td><td colspan=\"2\">FEDFormer</td><td colspan=\"2\">Stationary</td><td colspan=\"2\">LightTS</td><td colspan=\"2\">N-BEATS</td></tr><tr><td colspan=\"2\">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE MAE</td></tr><tr><td rowspan=\"4\">Weather</td><td>96</td><td>0.145</td><td>0.189</td><td>0.154</td><td>0.209</td><td>0.162</td><td>0.212</td><td>0.149</td><td>0.198</td><td>0.176</td><td>0.237</td><td>0.172</td><td>0.220</td><td>0.217</td><td>0.296</td><td>0.173</td><td>0.223 0.182</td><td>0.242</td><td>0.152</td><td>0.210</td></tr><tr><td></td><td>0.1492</td><td> 8.33</td><td>0.197</td><td>0.248</td><td>0.204</td><td>0.248</td><td>0.5</td><td>0.41</td><td>0.220</td><td>0.289</td><td>0.219</td><td>0.261</td><td>0.276</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>192</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.336</td><td>0.245</td><td>0.285</td><td>0.227</td><td>0.287</td><td>0.199</td><td>0.260</td></tr><tr><td>720</td><td>0.323</td><td>0.331</td><td>0.315</td><td>0.336</td><td>0.326</td><td>0.337</td><td>0.314</td><td>0.334</td><td>0.333</td><td>0.362</td><td>0.365</td><td>0.359</td><td>0.403 0.428</td><td>0.414</td><td>0.410</td><td>0.352</td><td>0.386</td><td>0.331</td><td>0.359</td></tr><tr><td rowspan=\"4\">ETTh1</td><td></td><td>0.365</td><td>0.398</td><td>0.387</td><td>0.410</td><td>0.376</td><td>0.397</td><td>0.370</td><td>0.399</td><td>0.375</td><td>0.399</td><td>0.384</td><td>0.402</td><td>0.376 0.419</td><td>0.513</td><td>0.491</td><td>0.424</td><td>0.432</td><td>0.399</td><td>0.428</td></tr><tr><td>96</td><td>0.399</td><td>0.415</td><td>0.410</td><td></td><td>0.446</td><td>0.418</td><td>0.42</td><td>0.421</td><td>0.405</td><td>0416</td><td>0.436</td><td></td><td>0.420 0.448</td><td></td><td>0.504</td><td></td><td>0.462</td><td></td><td></td></tr><tr><td>192</td><td></td><td></td><td></td><td>0.426</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.429</td><td></td><td>0.534</td><td></td><td>0.475</td><td></td><td>0.451</td><td>0.4604</td></tr><tr><td>720</td><td>0.420</td><td>0.443</td><td>0.454</td><td>0.472</td><td>0.477</td><td>0.456</td><td>0.447</td><td>0.466</td><td>0.472</td><td>0.490</td><td>0.521</td><td>0.500 0.506</td><td>0.507</td><td>0.643</td><td>0.616</td><td>0.547</td><td>0.533</td><td>0.608</td><td>0.573</td></tr><tr><td rowspan=\"3\">ETTh2</td><td></td><td>0.262</td><td>0.328</td><td>0.288</td><td>0.345</td><td>0.285</td><td>0.342</td><td>0.274</td><td>0.336</td><td>0.289</td><td>0.353</td><td>0.340 0.374</td><td>0.358</td><td>0.397</td><td>0.476</td><td>0.458</td><td>0.397</td><td>0.437</td><td>0.327</td><td>0.387</td></tr><tr><td>96 192</td><td>0.322</td><td>0.368</td><td>0.349</td><td>0.386</td><td>0.354</td><td>0.389</td><td>0.339</td><td>0.379</td><td>0.383</td><td>0.418</td><td>0.402</td><td>0.414</td><td>0.429</td><td>0.439 0.512</td><td></td><td>0.493 0.520</td><td>0.504</td><td>0.400</td><td>0.435</td></tr><tr><td>336</td><td>0.312</td><td>0.370</td><td>0.369</td><td>0.408</td><td>0.373</td><td>0.407</td><td>0.329</td><td>0.380</td><td>0.448</td><td>0.465</td><td>0.452</td><td>0.452 0.496</td><td>0.487</td><td>0.552</td><td>0.551</td><td>0.626</td><td>0.559</td><td>0.747</td><td>0.599</td></tr><tr><td rowspan=\"4\">ETTm1</td><td>720</td><td>0.399</td><td>0.434</td><td>0.403</td><td>0.439</td><td>0.406</td><td>0.441</td><td>0.379</td><td>0.422</td><td>0.605</td><td>0.551</td><td>0.462 0.468</td><td>0.463</td><td>0.474</td><td>0.562</td><td>0.560</td><td>0.863</td><td>0.672</td><td>1.454</td><td>0.847</td></tr><tr><td>96</td><td>0.284</td><td>0.337</td><td>0.293</td><td>0.349</td><td>0.292</td><td>0.346</td><td>0.290</td><td>0.342</td><td>0.299</td><td>0.343</td><td>0.338</td><td>0.375</td><td>0.379</td><td>0.419 0.386</td><td>0.398</td><td>0.374</td><td>0.400</td><td>0.318</td><td>0.367</td></tr><tr><td>192</td><td>0.329</td><td>0.366</td><td>0.326</td><td>0.368</td><td>0.332</td><td>0.372</td><td>0.332</td><td>0.369</td><td>0.335</td><td>0.365</td><td>0.374</td><td>0.387</td><td>0.426 0.441</td><td>0.459</td><td>0.444</td><td>0.400</td><td>0.407</td><td>0.355</td><td>0.391</td></tr><tr><td>336</td><td>0.365</td><td>0.391</td><td>0.352</td><td>0.384</td><td>0.366</td><td>0.394</td><td>0.366</td><td>0.392</td><td>0.369</td><td>0.386</td><td>0.410</td><td>0.411 0.445</td><td>0.459</td><td>0.495</td><td>0.464</td><td>0.438</td><td>0.438</td><td>0.401</td><td>0.419</td></tr><tr><td rowspan=\"4\">ETTm2</td><td>720</td><td>0.424</td><td>0.430</td><td>0.405</td><td>0.416</td><td>0.417</td><td>0.421</td><td>0.416</td><td>0.420</td><td>0.425</td><td>0.421</td><td>0.478 0.450</td><td>0.543</td><td>0.490</td><td>0.585</td><td>0.516</td><td>0.527</td><td>0.502</td><td>0.448</td><td>0.448</td></tr><tr><td>96</td><td>0.162</td><td>0.252</td><td>0.170</td><td>0.260</td><td>0.173</td><td>0.262</td><td>0.165</td><td>0.255</td><td>0.167</td><td>0.269</td><td>0.187</td><td>0.267</td><td>0.203 0.287</td><td>0.192</td><td></td><td>0.274 0.209</td><td>0.308</td><td>0.197</td><td></td></tr><tr><td>192</td><td>0.218</td><td>0.291</td><td>0.227</td><td>0.297</td><td>0.229</td><td>0.301</td><td>0.220</td><td>0.292</td><td>0.224</td><td>0.303</td><td>0.249</td><td>0.309 0.269</td><td>0.328</td><td>0.280</td><td>0.339</td><td>0.311</td><td>0.382</td><td>0.285</td><td>0.271 0.328</td></tr><tr><td>336 0.276</td><td></td><td>0.327</td><td>0.275</td><td>0.328</td><td>0.286</td><td>0.341</td><td>0.274</td><td>0.329</td><td>0.281</td><td>0.342</td><td>0.321 0.351</td></table></body></html>\n\n<html><body><table><tr><td rowspan=\"3\"></td><td colspan=\"2\">MSE</td><td>MAE</td><td>Fwd FLOPs</td><td colspan=\"2\">Fwd+Bwd FLOPs</td><td colspan=\"2\">#Params</td></tr><tr><td colspan=\"2\">Pred Len|ParallelTime PatchTST</td><td colspan=\"2\"></td><td colspan=\"2\">ParallelTime PatchTST|ParallelTime PatchTST|ParallelTime PatchTST</td><td colspan=\"2\">ParallelTime PatchTST</td></tr><tr><td>0.349 (↓3.1%)</td><td>0.360</td><td>0.231 (↓7.2%) 0.249</td><td>8.41G (↓35%) 13.1G</td><td>25.2G (↓36%) 39.5G</td><td></td><td>614k (↓48%)</td><td>1194k</td></tr><tr><td>96 192</td><td>0.371 (↓2.1%)</td><td>0.379</td><td>0.240 (↓6.3%) 0.256</td><td>8.42G (↓37%)</td><td>13.5G</td><td>25.2G (↓37%）40.5G</td><td>651k (↓67%)</td><td>1980k</td></tr><tr><td>336</td><td>0.388 (↓1.0%)</td><td>0.392</td><td>0.250 (↓5.3%) 0.264</td><td>8.45G (↓39%)</td><td>14.0G 25.3G (↓39%)</td><td>42.1G</td><td>707k (↓77%)</td><td>3160k</td></tr><tr><td>720</td><td>0.429 (↓0.7%)</td><td>0.432</td><td>0.274 (↓4.2%) 0.286</td><td>8.51G (↓44%)</td><td>15.4G</td><td>25.5G (↓44%) 46.3G</td><td>855k (↓86%)</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>6306k</td></tr></table></body></html>\n\nTable 2: Comparison of ParallelTime and PatchTST on the Traffic dataset. The table reports MSE, MAE, forward (Fwd) FLOPs (i.e., inference FLOPs), forward and backward $( \\mathrm { F w d + B w d } )$ FLOPs (i.e., training FLOPs), and the number of parameters (#Params). Bold values indicate superior performance. $\\downarrow$ indicates that lower values are better. The improvement percentages for ParallelTime over PatchTST are shown in parentheses.\n\ncombination of high accuracy, computational efficiency, and reduced resource requirements highlights the versatility and effectiveness of ParallelTime, positioning it as a leading solution for real-world time series forecasting challenges.\n\n# 5 Model Analysis\n\n# 5.1 Patch-Level Weight Analysis\n\nTo illustrate how our model allocates short-term and long-term dependencies for each token (patch), we analyze a sample from the Traffic dataset at prediction lengths of 96 and 192. We extract the weights assigned by our ParallelTime Weighter and present them in Figure 4. Looking at the input and the first block at each prediction length, when the previous patch (from left to right) exhibits a high value, our model assigns greater weight to Mamba, prioritizing long-term dependencies to reduce overfitting to potential noise. Similarly, in the second block for each prediction length, when consecutive patches are similar, the model leverages Mamba to emphasize long-term dependencies, capturing a broader range of historical behaviors rather than focusing solely on recent patterns. Conversely, when preceding patches differ significantly, the model assigns more weight to the attention mechanism to prioritize short-term dependencies. Notably, for the second blocks, longer prediction lengths exhibit a stronger emphasis on long-term dependencies. For an additional result, see Appendix 9.3.\n\n![](images/3d6d208bc6913d288703f9c0a37555908f88d5943c235b11e162f533b4105c7a.jpg)  \nFigure 4: Visualization of input series and the weight distribution for prediction length 96, 192 per patch in sample from Traffic dataset, for each of the first and second ParallelTime blocks.   \nFigure 5: Mean weight of tokens (patches) per layer in the ParallelTime model, highlighting varying requirements for short-term and long-term dependencies across different datasets and prediction horizons.\n\n# 5.2 Dynamic Weighting Analysis\n\nTo evaluate the performance of our dynamic weighting mechanism across various datasets, we computed the mean weight of all tokens (patches) for each layer in our ParallelTime, as shown in Figure 5. The analysis includes the Weather, Electricity, ETTh1, and Traffic datasets.\n\nThe results demonstrate that, in the setting where the Attention-Mamba weights of each patch are averaged across all patches, each dataset emphasizes a different balance between short-term and long-term dependencies. Notably, across all datasets, the second layer consistently assigns more weight to the window attention mechanism compared to the first layer. For example, in the Weather dataset, when the prediction lengths are 192 and 336, the model relies more heavily on long-term dependencies, which are captured by the Mamba mechanism in the first layer. Conversely, for prediction lengths of 96 and 720, short-term dependencies are prioritized via the attention mechanism. In the second layer, attention receives a larger share of the weights regardless of the prediction length.\n\nWeather Electricity Etth1 Traffic 1.00 11.00 1.00 1.00 0.50 0.50 0.50 0.50 0.0 L1L2 L1L2 L1L2 L1L2 0.0 L1L2 L1L2 L1L2 L1L2 0.0 L1L2 L1L2 L1L2 L1L2 0.0 L1L2 L1L2L1L2L1L2 96 192 336 720 96 192 336 720 96 192 336 720 96 192 336 720 Prediction length Prediction length Prediction length Prediction length Layer1 Attention Layer1Mamba Layer2 Attention Layer2 Mamba\n\n# 6 Ablation study\n\n# 6.1 Weighting Strategy for Attention and Mamba\n\nWe assess the impact of our proposed ParallelTime weighting methodology. We compare multiple strategies, including mean weighting, as in [Dong et al., 2024], and sum weighting. To ensure compatibility, Attention and Mamba outputs are normalized prior to weighting to address their differing scales. Our results, as shown in Figure 6, confirm the effectiveness of this approach across all datasets. Additional results for other datasets are provided in Appendix 9.1.\n\n![](images/a5a61fef9762c36c7ad8ff1b4cec39d19a8cc497a61c11a6edaf8a89bdc1592e.jpg)  \nFigure 6: Ablation study of various weighting strategies - Mean, Sum and our ParallelTime Weighter for combining Attention and Mamba outputs.\n\n# 6.2 Model Efficiency Analysis\n\nTable 2 presents a comparison of MSE and MAE, Floating-Point Operations (FLOPs) for both training and inference and number of parameters, of our model against PatchTST across various prediction lengths using the Traffic dataset. The results show that our model requires significantly fewer FLOPs for both training and inference, achieves higher accuracy, and scales better with larger prediction lengths. This efficiency makes our model particularly well-suited for real-time long-term forecasting applications, where computational resources and speed are critical. For results on additional datasets, refer to Appendix 7.\n\n# 7 Conclusion and Future Work\n\nIn this work, we present ParallelTime, a novel decoder-only architecture that integrates local window attention and Mamba in parallel to effectively capture short-term and long-term dependencies, respectively. The outputs of these components are processed by our innovative ParallelTime Weighter, which adaptively assigns weights to each component for accurate predictions. Our approach achieves state-of-the-art performance across multiple real-world benchmarks while requiring fewer parameters and lower computational costs. This work establishes a foundation for future advancements in parallel Attention-Mamba architectures, poised to enhance long-term time series forecasting.\n\nA key limitation of this study is that, due to limited GPU resources, we could not scale our model to include a large number of layers, which constrained our ability to train on larger datasets or more diverse tasks. Future research can explore the model’s potential as a foundation for time series analysis with minimal adjustments. Specifically, efforts can focus on fine-tuning the model for diverse tasks, such as anomaly detection, classification, and multi-step forecasting, across various domains.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   现代多元时间序列预测主要依赖于Transformer和Mamba两种架构。在自然语言处理中，通常采用局部窗口注意力捕捉短期依赖和Mamba捕捉长期依赖，并将它们的输出平均分配权重。然而，在时间序列预测任务中，这种等权重分配并不最优。\\n> *   该问题的重要性在于，时间序列预测在金融、气象、交通等领域具有广泛应用，优化依赖关系的权重分配可以显著提升预测精度和效率。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出了一种动态权重机制ParallelTime Weighter，根据输入和模型知识为每个token计算长期和短期依赖的权重，并引入ParallelTime架构，结合该机制实现最先进的性能。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **贡献1：** 提出ParallelTime Weighter，动态选择短期、长期和全局记忆的贡献，通过窗口注意力、Mamba和寄存器实现，提升长期预测的准确性。\\n> *   **贡献2：** 证明并行Mamba-Attention架构是目前最有效的时间序列预测方法，为未来时间序列预测模型的发展指明了方向。\\n> *   **贡献3：** ParallelTime在多个真实世界基准测试中达到SOTA性能，参数更少、计算成本更低，平均MSE降低4.25%，MAE降低4.31%。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   ParallelTime的核心思想是通过动态权重机制平衡短期和长期依赖关系，利用窗口注意力捕捉局部模式，Mamba捕捉全局趋势，寄存器提供全局上下文信息。\\n> *   该方法有效的原因在于它能够根据输入数据的特性动态调整权重，从而更灵活地适应不同时间序列的预测需求。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 先前工作如Hymba采用简单的平均权重分配，无法适应时间序列中不同依赖关系的需求。\\n> *   **本文的改进：** ParallelTime Weighter通过动态权重分配，解决了等权重分配的局限性，显著提升了预测精度。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  输入时间序列分解为单变量序列，应用实例归一化（ReVIn）。\\n> 2.  使用非重叠分块机制将每个单变量序列分块，每个块作为“token”。\\n> 3.  通过线性层和Conv1D层分别提取全局和局部信息，结合形成最终嵌入。\\n> 4.  加入绝对位置编码，捕捉序列顺序。\\n> 5.  并行处理Mamba和窗口注意力机制，输出通过ParallelTime Weighter动态加权。\\n> 6.  使用Huber损失函数和Adam优化器进行训练。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   Transformer-based models: PatchTST, FEDFormer\\n> *   Linear models: DLinear, N-BEATS\\n> *   Foundational models: Moment, GPT4TS, TimesNet\\n> *   Ensemble models: LightTS, Stationary\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在MSE上：** 本文方法在Weather数据集上达到了0.145，显著优于基线模型Moment (0.154) 和PatchTST (0.149)。与表现最佳的基线相比，提升了6.2%。\\n> *   **在MAE上：** 本文方法在ETTh1数据集上达到了0.415，显著优于基线模型Moment (0.426) 和PatchTST (0.421)。与表现最佳的基线相比，提升了1.4%。\\n> *   **在计算效率上：** 本文方法的FLOPs为8.41G，远低于PatchTST的13.1G，同时参数数量减少48%。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   时间序列预测 (Time Series Forecasting, N/A)\\n*   动态权重机制 (Dynamic Weighting Mechanism, N/A)\\n*   Mamba (Mamba, N/A)\\n*   窗口注意力 (Windowed Attention, N/A)\\n*   并行架构 (Parallel Architecture, N/A)\\n*   状态空间模型 (State Space Model, SSM)\\n*   多元时间序列 (Multivariate Time Series, N/A)\\n*   长期依赖 (Long-Term Dependencies, N/A)\"\n}\n```"
}