{
    "source": "ArXiv (Semantic ScholarÊú™Êî∂ÂΩï)",
    "arxiv_id": "2507.14121",
    "link": "https://arxiv.org/abs/2507.14121",
    "pdf_link": "https://arxiv.org/pdf/2507.14121.pdf",
    "title": "Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective",
    "authors": [
        "Pankaj Yadav",
        "Vivek Vijay"
    ],
    "categories": [
        "cs.LG",
        "cs.AI"
    ],
    "publication_date": "Êú™ÊâæÂà∞Êèê‰∫§Êó•Êúü",
    "venue": "ÊöÇÊú™ÂΩïÂÖ•Semantic Scholar",
    "fields_of_study": "ÊöÇÊú™ÂΩïÂÖ•Semantic Scholar",
    "citation_count": "ÊöÇÊú™ÂΩïÂÖ•Semantic Scholar",
    "influential_citation_count": "ÊöÇÊú™ÂΩïÂÖ•Semantic Scholar",
    "institutions": [
        "Êú™ÊâæÂà∞Êú∫ÊûÑ‰ø°ÊÅØ"
    ],
    "paper_content": "# Kolmogorov Arnold Networks (KANs) for Imbalanced Data - An Empirical Perspective\n\nPankaj Yadav, Vivek Vijay\n\n# Abstract\n\nKolmogorov Arnold Networks (KANs) are recent architectural advancement in neural computation that offer a mathematically grounded alternative to standard neural networks. This study presents an empirical evaluation of KANs in context of class imbalanced classification, using ten benchmark datasets. We observe that KANs can inherently perform well on raw imbalanced data more effectively than Multi-Layer Perceptrons (MLPs) without any resampling strategy. However, conventional imbalance strategies fundamentally conflict with KANs mathematical structure as resampling and focal loss implementations significantly degrade KANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from prohibitive computational costs without proportional performance gains. Statistical validation confirms that MLPs with imbalance techniques achieve equivalence with KANs $\\vert d \\vert < 0 . 0 8$ across metrics) at minimal resource costs. These findings reveal that KANs represent a specialized solution for raw imbalanced data where resources permit. But their severe performance-resource tradeoffs and incompatibility with standard resampling techniques currently limits practical deployment. We identify critical research priorities as developing KAN specific architectural modifications for imbalance learning, optimizing computational efficiency, and theoretical reconciling their conflict with data augmentation. This work establishes foundational insights for next generation KAN architectures in imbalanced classification scenarios.\n\nKeywords: Kolmogorov Arnold Networks, KANs, Imbalanced Data, Classification, Tabular Data, Empirical Study\n\n# 1. Introduction\n\nImbalanced data remains a prevalent and critical challenge in supervised learning, irrespective of model complexity, from simple MLPs to advance Transformer architectures. This skewness, where one or more dominant classes often outnumbered minority classes, severely degrades model performance by biasing towards the majority [1]. The problem highlights significantly in multi-class scenarios beyond binary classification. Models face learning difficulties where inter class relationships and scarcity of minority instances become crucial [2]. While conventional deep learning models like MLPs, Convolutional Neural Networks and Transformers offer high approximation capacity, operating as black boxes [3]. They generally rely on fixed, predefined activation functions like sigmoid, ReLu for non linearity and obscuring the explicit internal mechanism behind their predictions. This lack of interpretability is compounded by their massive parameter counts, hindering model diagnosis and refinement.\n\nRecently, KANs have emerged as a promising interpretable neural network [4]. Its foundation grounded in the Kolmogorov Arnold Representation Theorem which states that any multivariate continuous function can be represented as a finite composition of univariarte continuous functions [5]. KANs replace traditional linear weights with learnable univariate functions parameterized as splines. This architecture inherently prioritizes interpretability of these models. The learned univariate functions and their connections can be visualized and semantically analyzed, revealing input feature interactions and functional relationships. Furthermore, KANs demonstrate competitive or superior accuracy with significantly fewer parameters than MLPs in small to moderate datasets, constrained by computational complexity [6]. However, while the theoretical foundation of KANs provide existence guarantees for representation, its practical use for complex, high dimensional functions applications in real world machine learning remains an active area of investigation.\n\nDespite KANs advantage in interpretability and efficient function approximation, their behavior and efficacy on real-world tabular data remains largely unexplored. Tabular data in high stake domains like finance, healthcare, and cybersecurity presents unique structural challenges. Crucially, the intersection of KANs interpretable architecture and the critical need for robust models in imbalance tabular classification constitutes a significant research void. Existing literature on KANs focuses primarily on synthetic datasets, standard benchmarks or physics-informed tasks, leaving their potential for mitigating bias in practical, skewed tabular classification unvalidated [7], [8]. This article rigorously investigates the capability of KANs to address the challenges of interpretability and predictive performance in multi-class imbalanced tabular data classifications. This article thoroughly evaluates KANs for binary imbalanced tabular data through:\n\n‚Ä¢ Benchmark KANs against MLPs on imbalanced tabular data using minority sensitive metrics ‚Ä¢ KANs architecture to explain imbalance effects- Synergy or degradation of performance. ‚Ä¢ Computational penalties KANs incur versus MLPs when implementing different resampling techniques\n\nThrough comprehensive benchmarking across 10 datasets, we demonstrate KANs intrinsically handle raw imbalance better than MLPs but suffer significant degradation when conventional resampling techniques are applied. Crucially, KANs incur prohibitive computational costs, without proportional gains under mitigation. These findings reveal fundamental capabilities between KANs architecture and standard imbalance handling, establishing clear applicability boundaries while directing future research toward KAN specific solutions.\n\n# 2. Kolmogorov Arnold Networks (KANs)\n\n# 2.1. Kolmogorov Arnold Representation Theorem\n\nKolmogorov Arnold Networks (KANs) derive their theoretical robustness from the Kolmogorov-Arnold Representation Theorem, established by Andrey Kolmogorov and Valdimir Arnold in 1957 [9]. This profound mathematical result states that any continuous multivariate function can be representated as a finite superposition of continuous univariate functions. Formally, for an arbitrary continuous function\n\n$$\nf ( \\mathbf { x } ) : [ 0 , 1 ] ^ { n } \\to \\mathbb { R } , \\quad \\mathbf { x } = ( x _ { 1 } , x _ { 2 } , \\ldots , x _ { n } )\n$$\n\ndefined on the $n$ -dimensional unit hypercube, there exist continuous univariate functions $\\phi _ { i j } : [ 0 , 1 ] \\to \\mathbb { R }$ and $\\psi _ { i } : \\mathbb { R }  \\mathbb { R }$ such that [10]\n\n$$\nf ( \\mathbf { x } ) = \\sum _ { i = 1 } ^ { 2 n + 1 } \\psi _ { i } \\left( \\sum _ { j = 1 } ^ { n } \\phi _ { i j } x _ { j } \\right)\n$$\n\nfor all $\\mathbf { x } \\in [ 0 , 1 ] ^ { n }$ . This decomposition exhibits a fundamental two-layer structure: an inner transformation layer computing $n ( 2 n + 1 )$ univariate mappings $\\phi _ { i j } ( x _ { j } )$ , followed by linear aggregations $\\textstyle \\zeta _ { i } = \\sum _ { j = 1 } ^ { n } \\phi _ { i j } ( x _ { j } )$ , and an outer transform layer applying $\\psi _ { i } ( \\cdot )$ to each aggregated value.\n\nThe theorem achieves significant dimensionality reduction through the inner sums $\\begin{array} { r } { \\zeta _ { i } = \\sum _ { j = 1 } ^ { n } \\phi _ { i j } ( x _ { j } ) } \\end{array}$ , which project $n$ dimensional inputs onto scala values [11]. Crucially, the minimal width $2 n + 1$ of the hidden layer was proven optimal by Arnold, establishing that multivariate continuity reduces to\n\n$$\nf ( \\mathbf { x } ) \\equiv k \\left( \\{ \\phi _ { i j } \\} , \\{ \\psi _ { i } \\} \\right) = \\sum _ { i = 1 } ^ { 2 n + 1 } \\underbrace { \\psi _ { i } \\circ \\zeta _ { i } } _ { \\mathrm { u n i v a r i a t e c o m p o } }\n$$\n\nThis representation resolves Hilbert‚Äôs thirteenth problem by demonstrating that multivariate functions require no inherently multidimensional operations [12]. This is because all complexities are aggregated in univariate functions and addition, thereby avoiding combinatorial explosion through superposition.\n\nThe Kolmogorov-Arnold foundation ensures existence guarantees for exact representation of continuous functions on compact domains; operational simplicity through elimination of multivariate convolutions via additive superpositions; and adaptive complexity where modern KANs relax the $2 n + 1$ width constraint in favor of depth $L \\geq 2$ for hierarchical extractions.\n\n# 2.2. KAN Architecture\n\nThe architectural design of KANs draws direct inspiration from the Kolmogorov Theorem, while addressing practical implementation challenges through function approximation techniques. Central to this, the careful approximation of the univariate function appearing in Equations (1), necessitates fundamental limitations of polynomial approximation. As conventional polynomial representation face significant limitations in KAN implementation due to the Runge Phenomenon, where higherdegree polynomials exhibits oscillatory behavior near interval endpoints [13].\n\nFormally, for a target univariate function $h : [ a , b ] \\to \\mathbb { R }$ to be approximated by a degree- $p$ polynomial $\\begin{array} { r } { P _ { p } ( x ) = \\sum _ { s = 0 } ^ { p } c _ { s } x ^ { s } } \\end{array}$ the approximation error defined as:\n\n$$\n\\operatorname* { s u p } _ { x \\in [ a , b ] } | h ( x ) - P _ { p } ( x ) | \\leq \\frac { ( b - a ) ^ { p + 1 } } { ( p + 1 ) ! } \\operatorname* { s u p } _ { \\xi \\in [ a , b ] } | h ^ { ( p + 1 ) } ( \\xi ) | ,\n$$\n\nTo obscure this polynomial limitations, KANs employ $B$ - spline approximations. It emerges as solutions to the stability constraints of their Bezier curve predecessors. Suppose a univariate function $h : [ 0 , 1 ] \\to \\mathbb { R }$ to be approximated. The Bezier curve of degree p with control points $\\mathbf { c } = ( c _ { 0 } , \\ldots , c _ { p } ) \\in \\mathbb { R } ^ { p + 1 }$ is defined as:\n\n$$\nB _ { p } ( x ; { \\mathbf c } ) = \\sum _ { s = 0 } ^ { p } c _ { s } { \\binom { p } { s } } x ^ { s } ( 1 - x ) ^ { p - s } .\n$$\n\nwhile providing covex hull and endpoint interpolation properties, Bezier curves also responds to the modification of $c ^ { \\prime } s$ and can affect the entire curve as its global control property [14]. While, for instance $p > 5$ , it shows oscillatory degrdation as another limitation. Computational complexity in the $L ^ { 2 }$ -projection can also be seen as limitations to these curves.\n\nThese limitations motivate the transition to $B$ -splines, which partition the domain [0, 1] into $T$ subintervals via knots $\\tau _ { 0 } =$ $0 < \\tau _ { 1 } < \\cdots ; \\tau _ { T } = 1$ and employ piecewise polynomial representations with local support.\n\n$B$ -spline Formulation. A B-spline approximation of order $p$ (degree $p - 1 \\rangle$ with $N$ basis functions is constructed as:\n\n$$\nS \\left( x ; \\pmb \\theta \\right) = \\sum _ { m = 1 } ^ { N } \\theta _ { m } B _ { m , p } ( x ) ;\n$$\n\nwhere $B _ { m , p }$ are basis functions defined recrusively:\n\n$$\n\\begin{array} { l } { \\displaystyle B _ { m , 1 } ( x ) = \\mathbb { I } _ { [ \\tau _ { m } , \\tau _ { m + 1 } ] } ( x ) } \\\\ { \\displaystyle B _ { m , p } ( x ) = \\frac { x - \\tau _ { m } } { \\tau _ { m + p - 1 } - \\tau _ { m } } B _ { m , p - 1 } ( x ) + \\frac { \\tau _ { m + p } - x } { \\tau _ { m + p } - \\tau _ { m + 1 } } B _ { m + 1 , p - 1 } ( x ) } \\end{array}\n$$\n\nThe approximation through B-splines exhibits important advantages over polynomials. It shows property that overcomes the limitations of Bezire curve as: local support provide through $s u p p ( { B } _ { m , p } ) = [ \\tau _ { m + p - 1 } - \\tau _ { m } ]$ continuity, $S \\ \\in \\ C ^ { p - 2 } ( [ 0 , 1 ] )$ for distinct knots; and Convex hull as $S ( x ) \\in [ m i n \\theta _ { m } , m a x \\theta _ { m } ]$ for $x \\in [ \\tau _ { m } - \\tau _ { m } + 1 ]$ [15].\n\nFor KAN implementations, each univariate function $\\phi _ { i j } ^ { k }$ and ${ \\psi } _ { q } ^ { k }$ is parametrized as B-spline:\n\n$$\n\\phi _ { i j } ^ { ( k ) } ( x ) = s _ { i j } ^ { ( k ) } ( x ; \\pmb { \\theta } _ { i j } ^ { k } ) , \\quad \\psi _ { q } ^ { ( k ) } ( y ) = T _ { q } ^ { ( k ) } ( y ; \\pmb { \\gamma } _ { q } ^ { k } )\n$$\n\nwith trainable parameters $\\pmb { \\theta } _ { i j } ^ { k } \\in \\mathbb { R } ^ { \\mathbb { N } }$ and $\\psi _ { q } ^ { k } \\in \\mathbb { R } ^ { \\mathbb { M } }$ for $N , M$ basis functions [16].\n\n# 2.3. Multi-KAN Architecture and Implementation Mechanics\n\nThe Multi-KAN architecture extends the foundational Kolmogorov Arnold representations to deep compositions through stacked layers of B-splines parametrized functions [17]. For an $L$ -layer Multi-Kan, the functional composition remains:\n\n$$\n\\begin{array} { c } { { f ( { \\bf x } ) = { \\bf k } ^ { ( L ) } \\circ { \\bf k } ^ { ( L - 1 ) } \\circ \\cdots \\circ { \\bf k } ^ { ( 1 ) } ( x ) } } \\\\ { { { \\bf k } ^ { ( k ) } ( { \\bf z } ^ { ( k ) } ) = \\left[ \\displaystyle \\sum _ { q = 1 } ^ { m _ { k } } \\psi _ { q } ^ { ( k ) } \\left( \\displaystyle \\sum _ { r = 1 } ^ { d _ { k } } \\phi _ { q r } ^ { ( k ) } ( z _ { r } ^ { ( k ) } ) \\right) \\right] _ { q = 1 } ^ { d _ { k + 1 } } } } \\end{array}\n$$\n\nwhere $\\phi _ { q r } ^ { ( k ) } : \\mathbb { R }  \\mathbb { R }$ and $\\psi _ { q } ^ { ( k ) } : \\mathbb { R }  \\mathbb { R }$ are univariate functions implementing the Kolmogorov Arnold decomposition through B-spline parameterizations with residual foundations.\n\nResidual spline Parameterization. : Each univariate function $\\phi _ { q r } ^ { ( k ) }$ and $\\psi _ { q } ^ { ( \\hat { k } ) }$ is constructed as a residual activation function:\n\n$$\n\\phi ( x ) = w _ { b } \\cdot b ( x ) + w _ { s } \\cdot \\operatorname { s p l i n e } ( x )\n$$\n\nwhere, $b ( x ) = \\mathrm { { \\ s i l u } } ( x ) = x / 1 + e ^ { - x }$ serves as the differential basis function corresponding to residual connections [18]. $\\mathrm { s p l i n e } ( x ) = \\textstyle \\sum _ { i = 1 } ^ { G + k }$ is the B-spline approximation of order $k$ with $G$ intervals and $w _ { b } , w _ { s } \\ \\in \\ \\mathbb { R }$ are trainable scaling coefficients controlling relative contributions.\n\nThis parameterization ensures stable gradient propagation during optimization. The Silu basis provides global differentiability while the spline components enables local adaptability. The functional derivatives decomposes as:\n\n$$\n\\frac { d \\phi } { d x } = w _ { b } \\cdot \\frac { d } { d x } \\mathrm { s i l u } ( x ) + w _ { s } \\cdot \\sum _ { i = 1 } ^ { G + k } c _ { i } \\frac { d B _ { i } } { d x } ( x )\n$$\n\nUniversal Approximation Guarantees:. The architecture preserves the Kolmogorov Arnold theorem‚Äôs mathematical foundation: for any continuous $f : [ 0 , 1 ] ^ { n } \\to \\mathbb { R }$ and $\\epsilon > 0$ , there exists $L \\in \\mathbb { Z } ^ { + }$ , widths $m _ { k } \\ge 2 d _ { k } + 1$ , and B-spline parameters such that:\n\n$$\n\\operatorname* { s u p } _ { \\mathbf { x } \\in [ 0 , 1 ] ^ { n } } | f ( \\mathbf { x } ) - \\mathbf { M u l t i - K A N } ( \\mathbf { x } ) | < \\epsilon\n$$\n\nwith convergence rate $O ( G ^ { - P } )$ for $p$ -times differentiable under $h$ -refined spline grids. This theoretical implementation synthesis establishes Multi-KANs as both functionally expressive and computationally tractable, maintaining the KolmogorovArnold advantage while enabling deep learning optimization [19].\n\n# 3. Methodology\n\nFigure 1 presents a brief methodological framework used in the analysis. The study utilizes both binary and multiclass type imbalanced datasets. For multiclass problems, a one-vsall (OvA) approach is adopted, the smallest class is used as the minority and all other classes are merged into a composite majority class. To address class imbalance, two distinct strategies are implemented and compared. A resampling technique (data level) and focal loss method (an algorithm level) are used to handle the imbalance of the datasets [20]. The efficacy of these balancing strategies is rigorously evaluated against a baseline model trained on raw imbalance data. This comparative analysis is conducted for both KAN and MLP architectures, enabling their direct inter architecture comparison taking MLP as a baseline. Model performance is quantified using specific evaluation metrics compatible with imbalanced learning scenario. Statistical validation of results is performed to ensure robustness. An in depth discussion of these methodological strategies, their implementation, and their comparative outcomes follows in subsequent sections.\n\n# 3.1. Various Datasets\n\nThis study investigates the performance of KANs on class imbalanced datasets. It includes small to moderate scale dataset sourced from KEEL repository [21]. The datasets are deliberately selected on span a spectrum of imbalance ratios, ranging from 1:5 (mild) to 1:50 (severe). This controlled range enables a systematic evaluation of KAN‚Äôs robustness as imbalance severity increases. Limiting the maximum imbalance ratio to 1:50 is justified by computational constraints as preliminary analyses. It indicates that KANs require significant GPU resources for stable training on extremely imbalanced data, whereas this study operates exclusively within a CPU based experimental framework. Consequently, the selected imbalance range ensures feasible experimentation while still capturing a challenging and representative gradient of class distribution skew. The ten selected datasets provide a structured basis to characterize the interplay between KAN architectures and core class imbalance challenges. Comprehensive details of the datasets, including the specific characteristics and imbalance ratios, are provided in Table 1 for reference.\n\nTable 1: Various Datasets and their Characteristics   \n\n<html><body><table><tr><td>Dataset</td><td>Features</td><td>Instances</td><td>Imb. Ratio</td></tr><tr><td>yeast4</td><td>8</td><td>1484</td><td>28.1</td></tr><tr><td>yeast5</td><td>8</td><td>1484</td><td>32.73</td></tr><tr><td>yeast6</td><td>8</td><td>1484</td><td>41.4</td></tr><tr><td>glass2</td><td>9</td><td>214</td><td>11.59</td></tr><tr><td>ecoli3</td><td>7</td><td>336</td><td>8.6</td></tr><tr><td>winequality-red-8_vs_6-7</td><td>11</td><td>855</td><td>46.5</td></tr><tr><td>new-thyroid1</td><td>5</td><td>215</td><td>5.14</td></tr><tr><td>glass4</td><td>9</td><td>214</td><td>15.47</td></tr><tr><td>glass6</td><td>9</td><td>214</td><td>6.02</td></tr><tr><td>winequality-red-8_vs_6</td><td>11</td><td>656</td><td>35.44</td></tr></table></body></html>\n\n![](images/bae432cac619042ac7a800e322b2ff512d1bf6dd6384e83ff57269f2ebd8acd1.jpg)  \nFigure 1: Methodology\n\n# 3.2. Imbalance Analysis and Strategies\n\nWe employ both data-level and algorithmic-level strategies to analyze KAN model‚Äôs handling of class imbalance. For datalevel, we utilize the SMOTE-Tomek hybrid resampling technique due to its dual capacity to address both boundary noise and class overlap through combined oversampling and undersampling [22]. This approach is particularly relevant for KANs given their potential sensitivity to topological variations in feature space. It is also well-established resampling foundations for novel model evaluation.\n\nFor algorithmic-level, we implemented focal loss to dynamically modulate the learning objective [20]. While KANs differ from traditional networks by outputting raw logits $\\mathbf { z }$ rather than probability distributions, we adapt focal loss through explicit probability normalization. Given KAN outputs $\\mathbf { z } = [ z _ { 0 } , z _ { 1 } ]$ for a sample, we compute class probabilities via softmax:\n\n$$\np _ { t } = \\sigma ( \\mathbf { z } ) _ { t } = \\frac { e ^ { z _ { t } } } { \\sum _ { j = 0 } ^ { 1 } e ^ { z _ { j } } }\n$$\n\nwhere $t$ denotes the true class label. The focal loss is then applied as:\n\n$$\n\\mathcal { L } _ { \\mathrm { f o c a l } } = - \\alpha _ { t } ( 1 - p _ { t } ) ^ { \\gamma } \\log ( p _ { t } )\n$$\n\nThis transformation maintains mathematical equivalence to standard implementations while respecting KAN‚Äôs architectural constraints. The implementation sequence:\n\n$$\n\\mathrm { K A N } ( \\mathbf { x } ) \\to \\mathbf { z } { \\xrightarrow { \\mathrm { s o f t m a x } } } \\mathbf { p } { \\xrightarrow { { \\mathcal { L } } _ { \\mathrm { f o c a l } } } } \\mathrm { l o s s }\n$$\n\nallows focal loss to function identically to its application in MLPs. It down weights well classified majority instances and focusing training on challenging minority samples. Our inclusion of focal loss enables direct observation of KAN behavior under algorithmic imbalance handling. The collective strategies of resampling test KAN behavior on the resampled datasets.\n\n# 3.3. Evaluation Framework\n\nIn this section , we have employed four specialized metrics that explicitly account for distributional symmetry. It makes sure the equitable assessment of the model performance. These measures are systematically derived from fundamental classification concepts, providing complementary perspectives on model behavior.\n\nOur evaluation framework builds upon three core classifcation measures. Precision, Recall and Specificity respectively, quantify the reliability of true positive predictions, the coverage of actual positive instances and the capability to correctly identify negative instances [23], [24]:\n\n$$\nP r e c i s i o n = \\frac { t p } { t p + f p } ; \\quad R e c a l l = \\frac { t p } { t p + f n } ; \\quad S p e c i f i c i t y = \\frac { t n } { t n + f p }\n$$\n\nThese measures form basis for our composite metrics, which synthesize multiple performance dimensions.\n\nThe $F _ { I }$ score hormonizes precision and recall through their hormonic mean:\n\n$$\nF _ { 1 } = 2 \\times { \\frac { { \\mathrm { P r e c i s i o n } } \\times { \\mathrm { R e c a l l } } } { { \\mathrm { P r e c i s i o n } } + { \\mathrm { R e c a l l } } } }\n$$\n\nThis metric is particularly valuable when false positives and false negatives carry similar costs, as it penalizes models that sacrifice minority class detection for majority class accuracy [25].\n\nBalanced accuracy addresses the inflation of standard accuracy under imbalance by computing the arithmetic mean of class specific recalls:\n\n$$\n{ \\mathrm { B a l A c c } } = { \\frac { 1 } { 2 } } \\left( { \\mathrm { R e c a l l } } + { \\mathrm { S p e c i f i c i t y } } \\right)\n$$\n\nThis formulation ensures equal weighting of minority class identiification capabilities [26].\n\nThe geometric mean $G$ -mean) provides a stricter measure of balance by computing the root product of class specific recalls [27]:\n\n$$\nG \\mathrm { - M e a n } = \\mathrm { ~ } \\sqrt { \\mathrm { R e c a l l } \\times \\mathrm { S p e c i f i c i t y } }\n$$\n\nIts multiplicative nature causes severe degradation when either class recall approaches zero, making it exceptionally sensitive to minority class neglect.\n\nFor threshold independent evaluation, we employ the area under the ROC curve (AUC):\n\n$$\n\\mathrm { A U C } = \\int _ { 0 } ^ { 1 } \\mathrm { T P R } ( f ) \\cdot \\left| \\frac { d \\mathrm { F P R } ( f ) } { d f } \\right| d f\n$$\n\nwhere TPR denotes the true positive rate (recall) and FPR the false positive rate (1 Specificity) [28]. This integral represents the probability that a randomly selected positive instance ranks higher than a random negative instance, making it robust to class imbalance. The systematic selection of these metric on their properties is shown in Table 2.\n\nTable 2: Metric Properties Relative to Class Imbalance   \n\n<html><body><table><tr><td>Metric</td><td>Sensitivity</td><td>Range</td><td>Priority</td></tr><tr><td>Balanced Accuracy</td><td>High</td><td>[0,1]</td><td>Class recall parity</td></tr><tr><td>G-Mean</td><td>Very High</td><td>[0,1]</td><td>Worst class performance</td></tr><tr><td>F1 Score</td><td>High</td><td>[0,1]</td><td>Prediction reliability</td></tr><tr><td>AUC</td><td>Low</td><td>[0.5,1]</td><td>Ranking consistency</td></tr></table></body></html>\n\n# 4. Results\n\n# 4.1. KANs comparison with MLP\n\nThe comprehensive evaluation of KANs against MLPs across baseline, resampled, and focal loss methodologies reveals interesting performance patterns. As the Figure 2 shows the aggregated bar plots spanning 10 benchmark datasets, KANs consistently maintained a performance advantage in the baseline configuration across all evaluation metrics. For balance accuracy, KANs achieve a mean of 0.6335 compared to MLPs\n\n0.5800, while showing substantially stronger capability in handling class imbalance similar in G-mean scores of 0.4393 versus 0.2848. The F1-score comparison further highlighted this trend of KANs superiority in minority class recognition.\n\nWhile examining resampled approaches, the performance gap between architectures narrowed considerably. KANs output balanced accuracy of 0.5904 against MLPs 0.5943, while G-mean values converges near 030 for both the models. This pattern of near equivalence persists under focal loss implementations also, where both architectures acheive balanced accuracy around 0.580 and F1 scores near 0.191. The bar plots visually strengthens this critical observation: while KANs substantially outperform MLPs in baseline conditions, both architectures respond almost similarly to class imbalance handling techniques.\n\nNotably, the performance equivalence occurs despite critical difference in computational profiles. KANs consistently require orders of magnitude more training time and memory resources than MLPs across all configurations. This suggests that while KANs offer superior inherent capability for handling raw imbalanced data, their architectural advantages are effectively matched by MLPs when combine with class imbalance techniques at significant computational cost. Subsequent sections will discuss these performance comparison in more details. The visual convergence of metric bars under resampled and focal loss strategies provides compelling empirical evidence that strategies fundamentally alter the comparative landscape between these architectures.\n\n# 4.2. Interplay Between KANs, Imbalance and Resampling\n\nThe radar visualization (Figure 3) reveals crucial insights about how KANs interact with class imbalance and handling strategies. In baseline configuration KANs show their strongest performance profile, achieving superior metrics across the board. This suggests KANs intrinsically handle class imbalance better than traditional architectures. The behavior is likely due to their adaptive activation functions that capture complex decision boundaries without explicit resampling.\n\nHowever, applying resampling techniques fundamentally alters this advantage. Both resampling and focal loss implementations degrade KANs performance in critical metrics: G-mean declines by $32 \\%$ (resampled) to $39 \\%$ (focal) and F1 score drops by $2 2 \\div 2 6 \\%$ . The radar chart illustrates this connection, showing baseline KANs occupying the largest performance area. This finding indicates the KANs inherent architecture already incorporates capabilities that conventional imbalance techniques seek to artificially induce in MLPs.\n\nNotably, resampling induces the most severe performance erosion. The radar‚Äôs constricted shape for resampled KANs confirms this suboptimal tradeoff, where only balanced accuracy and AUC shows marginal improvement at the expense of all other metrics. This implies that while resampling benefits simpler architectures like MLPs, it actively conflicts with KANs mathematical structure. It can be possibly by disrupting their univariate function learning during data augmentation.\n\n![](images/2f932d3e43fbfa5883dd224f34795f9c0ae3d88ba883e441244a59c350b814aa.jpg)  \nFigure 2: KAN vs MLP Performance Comparison\n\n# 4.3. KANs Accuracy vs Computational Performance\n\nThe resource performance analysis exposes a steep computational penalty for KANs accuracy advantages. As shown in Figure 4, baseline KANs occupy the high performnace/ lowefficiency quadrant achieving the highest balanced accuracy but demanding $1 , 0 0 0 \\times$ longer training times $( 5 0 5 s )$ and $1 1 \\times$ more memory $( 1 . 4 8 M B )$ than MLPs. This resource disparity amplifies for other strategies: resampled KANs require $8 8 5 s$ training time and $6 . 5 1 M B$ memory over $9 0 0 \\times$ slower and $1 . 4 \\times$ more memory than resampled MLPs.\n\nThe logarithmic scale visualization starkly contrasts the architectures efficiency frontiers. MLPs maintain a tight cluster near the origin across all strategies, reflecting minimal resource variance $\\cdot 0 . 0 6 M B$ memory; $0 . 5 - 0 . 9 7 s$ time). KANs, meanwhile show extreme dispersion, spanning nearly three orders of magnitude in runtime and two orders in memory. Crucially, this inflation yields diminishing results as focal loss KANs consume $5 3 5 s$ (vs MLPs 0.89s) for nearly identical accuracy. The inverse relationship between KANs computational make sense as learning univariate function directly needs resources with accuracy gains but may underscores a fundamental scalability challenges with high dimensional datasets.\n\n# 5. KAN Hyperparameters & Statistical Analysis\n\nThe optimized KANs configurations reveal critical architectural patterns tailored to dataset characteristics in Table 3. Single layer architectures dominated, with widths varying from 4 (yeast6) to 8 (glass6), while dual layer designs (yeast5, ecoli3, new thyroid1) featured progressive width reduction ( $( 7  8 )$ . Hyperparameter shows adaptation of consistent learning rates with some exception datasets (yeast6). Batch sizes consistently optimized at hardware efficient 32/64, and grid sizes cluster at 5 ( $80 \\%$ of cases), reflecting a preference for moderate basis function granularity.\n\nwhile, parameter choices directly influence computational burdens. Dual layer requires $2 . 1 \\times$ longer median training times than single layer architectures (589 vs 281s), while width increases from $4  8$ amplified memory usage by $3 . 2 \\times$ . Despite this tuning diversity, no configuration closed the resource gap with MLPs, whose efficient backpropagation maintains consistent training times across all datasets.\n\n![](images/d87576d08a4298605b5d60969f222259fad7ef4b974aaf253486cff3589b5715.jpg)  \nFigure 3:\n\n![](images/b9349f362c3b0b0fdf75390c16dcddf234c1e2b9d35b43b3372536eb69995204.jpg)  \nFigure 4:\n\nTable 3: Optimized KAN Architecture Across Benchmark Datasets   \n\n<html><body><table><tr><td>Dataset</td><td>Layers</td><td>Widths</td><td>k</td><td>Grid</td><td>Learning Rate</td></tr><tr><td>yeast4</td><td>1</td><td>[7]</td><td>3</td><td>5</td><td>0.00066</td></tr><tr><td>yeast5</td><td>2</td><td>[7,8]</td><td>2</td><td>5</td><td>0.00040</td></tr><tr><td>yeast6</td><td>1</td><td>[4]</td><td>2</td><td>5</td><td>0.00452</td></tr><tr><td>glass2</td><td>1</td><td>[4]</td><td>3</td><td>5</td><td>0.00236</td></tr><tr><td>ecoli3</td><td>2</td><td>[6,5]</td><td>2</td><td>5</td><td>0.00069</td></tr><tr><td>winequality-red-8_vs_6-7</td><td>1</td><td>[7]</td><td>3</td><td>5</td><td>0.00062</td></tr><tr><td>new-thyroid1</td><td>2</td><td>[6,4]</td><td>3</td><td>3</td><td>0.00785</td></tr><tr><td>glass4</td><td>1</td><td>[6]</td><td>3</td><td>4</td><td>0.00010</td></tr><tr><td>glass6</td><td>1</td><td>[8]</td><td>2</td><td>5</td><td>0.00028</td></tr><tr><td>winequality-red-8_vs_6</td><td>1</td><td>[5]</td><td>3</td><td>5</td><td>0.00138</td></tr></table></body></html>\n\nStatistical Validation. The study‚Äôs core findings are statistically proved through rigorous hypothesis testing, with key results illustrated in Table 4. KANs shows non-significant but consistent advantages in baseline imbalanced settings. It is evidenced by moderate to large effect sizes for G-mean $( d = 0 . 7 3 , p = 0 . 0 6 )$ ) and F1-score $( d = 0 . 6 1 p = 0 . 1 0 )$ . The vulnerability of KANs to conventional imbalance techniques is statistically unambiguous. The focal loss implementation significantly degrades their G-mean $\\scriptstyle ( \\mathtt { p } = 0 . 0 4 2 )$ with a large effect size $( d = 0 . 7 9 )$ , while resampling approaches significance $( p = 0 . 0 5 7 , d = 0 . 7 3 )$ .\n\nWilcoxon tests confirm KANs training time disadvantage ${ \\it p } = 0 . 0 0 2 )$ with a massive effect size $\\acute { \\iota } = 2 . 9 4 \\acute { \\iota }$ , while memory comparisons show similar extremes ${ \\mathit { \\Delta } } ^ { \\prime } p = 0 . 0 0 2 , d = 2 . 1 7 { \\mathit { \\Omega } } ,$ . Crucially, resampling strategies eliminates KANs performance advantages without reducing resource penalties. Resampled KANs versus MLPs shows negligible effect sizes across all metric $\\lceil d \\rceil < 0 . 0 8 )$ . This statistical convergence confirms that MLPs with imbalance techniques achieve parity with KANs at minimal computational cost.\n\nThe combined hyperparameter and statistical analysis establishes that KANs mathematical strengths in raw imbalanced data require careful architectural tuning. It suffers from severe computational penalties and are negated by conventional imbalance strategies. These evidence based constraint shows clear applicability boundaries for KAN deployment in real world systems.\n\n# 6. Discussion & Conclusion\n\nThis study presents a comprehensive empirical analysis of KANs for class imbalance classification. It reveals some fundamental insights with significant implications for both theoretical understanding and practical deployment. KANs show unique architectural advantages for raw imbalanced data, consistently outperforming MLPs in baseline configurations across critical minority class metrics (G-mean: $+ 5 4 \\%$ , F1-score: $+ 5 5 \\%$ ). This capability stems from their mathematical formulation with adaptive functions and univariate response characteristics intrinsically capture complex decision boundaries. Crucially, these advantages materialize without resampling or cost-sensitive techniques, simplifying preprocessing pipelines showing a notable operational shift.\n\nWe establish that conventional imbalance strategies fundamentally conflicts with KANs architectural principal. Both resampling and focal loss significantly degrade KANs performance in minority class metrics while marginally benefiting MLPs. This counterintuitive finding suggest KANs inherently incorporate the representational benefits over MLPs. Superimposing these techniques disrupts KANs natural optimization pathways reducing their ability to learn univariate functions from stable data distributions. Consequently, KANs require architectural rather than procedural solutions for imbalance challenges.\n\nWe quantify a severe performance resource trade off that constraints KANs practical utility. Despite their baseline advantages, KANs demand median training times and memory than MLPs. This computational penalty escalates under utilization of resampling techniques without proportional performance gains. Statistical test confirm resource difference dwarf performance effects. Such disparities make KANs currently impractical for real-time systems or resource constrained environments, despite their theoretical appeal.\n\nTable 4: Statistical Test Synthesis   \n\n<html><body><table><tr><td>Comparison</td><td>Metric</td><td>p-value</td><td>Cohen's d</td><td>Outcome</td></tr><tr><td>KAN:Baseline vs Focal</td><td>G-mean</td><td>0.042</td><td>0.79</td><td>Significant degradation</td></tr><tr><td>KAN:Baseline vs Resampled</td><td>G-mean</td><td>0.057</td><td>0.73</td><td>Marginal degradation</td></tr><tr><td>Baseline:KANvsMLP</td><td>F1-score</td><td>0.101</td><td>0.61</td><td>Non-significant advantage</td></tr><tr><td>KAN vs MLP (Resampled)</td><td>Balanced Acc</td><td>0.810</td><td>-0.08</td><td>No difference</td></tr><tr><td>KAN vs MLP (Training)</td><td>Time</td><td>0.042</td><td>2.94</td><td>Significant disadvantage</td></tr></table></body></html>\n\nOur findings details these clear research priorities:\n\n1. KAN-Specific Imbalance Techniques: Future work should develop architectural modifications that preserves KANs intrinsic imbalance handling while avoiding computational inflation. Promising directions include sparsity constrained learning or attention mechanism that amplify minority class features without data augmentation.\n\n2. Resource Optimization: The extreme computational overhead necessitates dedicated KAN compression research like quantization of basis functions and hardware aware implementations. Our hyperparameter analysis suggests intial pathways (single layer designs reduce training time $2 . 1 \\times$ versus dual layer), but algorithmic breakthroughs are essential.\n\n3. Theoretical Reconciliation why do resampling techniques degrade KANs but benefit MLPs? We hypothesize that data augmentation disrupts the Kolmogorov Arnold representation theorem‚Äôs assumptions. Formal analysis of this interaction should be prioritized.\n\nWhile MLPs with resampling strategies currently offer a superior efficiency performance equilibrium for most applications. KANs retain unique value for critical system where raw data imbalance is extreme, preprocessing is prohibitive and computational resources are unconstrained. Their baseline performance without resampling simplifies deployment pipelines- an advantage not to be overlooked. Nevertheless, closing the resource gap remains critical before broader adoption. This work establishes that KANs represent not a wholesale replacement for MLPs, but a specialized tool requiring continued architectural innovation to realize its theoretical potential in practical imbalanced learning scenarios.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   ËÆ∫ÊñáÁ†îÁ©∂‰∫ÜKolmogorov Arnold Networks (KANs)Âú®Á±ªÂà´‰∏çÂπ≥Ë°°Êï∞ÊçÆÂàÜÁ±ª‰∏≠ÁöÑË°®Áé∞„ÄÇÁ±ªÂà´‰∏çÂπ≥Ë°°ÊòØÁõëÁù£Â≠¶‰π†‰∏≠ÁöÑ‰∏Ä‰∏™ÊôÆÈÅç‰∏îÂÖ≥ÈîÆÁöÑÈóÆÈ¢òÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§öÁ±ªÂà´Âú∫ÊôØ‰∏≠ÔºåÂ∞ëÊï∞Á±ªÂà´ÁöÑÂÆû‰æãÁ®ÄÁº∫‰ºöÂØºËá¥Ê®°ÂûãÂÅèÂêëÂ§öÊï∞Á±ªÂà´Ôºå‰∏•ÈáçÂΩ±ÂìçÊ®°ÂûãÊÄßËÉΩ„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÁöÑÈáçË¶ÅÊÄßÂú®‰∫éÔºåKANs‰Ωú‰∏∫‰∏ÄÁßçÊñ∞ÂÖ¥ÁöÑÂèØËß£ÈáäÁ•ûÁªèÁΩëÁªúÔºåÂÖ∂Âú®Â§ÑÁêÜ‰∏çÂπ≥Ë°°Êï∞ÊçÆÊó∂ÁöÑË°®Áé∞Â∞öÊú™ÂæóÂà∞ÂÖÖÂàÜÈ™åËØÅ„ÄÇËÆ∫ÊñáÊó®Âú®Â°´Ë°•Ëøô‰∏ÄÁ†îÁ©∂Á©∫ÁôΩÔºåËØÑ‰º∞KANsÂú®‰∏çÂπ≥Ë°°Êï∞ÊçÆÂàÜÁ±ª‰∏≠ÁöÑÊΩúÂäõÂèäÂÖ∂‰∏éÂ∏∏ËßÑ‰∏çÂπ≥Ë°°Â§ÑÁêÜÁ≠ñÁï•ÁöÑÂÖºÂÆπÊÄß„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ËÆ∫ÊñáÈÄöËøáÂÆûËØÅËØÑ‰º∞ÔºåÊØîËæÉ‰∫ÜKANs‰∏éÂ§öÂ±ÇÊÑüÁü•Êú∫ÔºàMLPsÔºâÂú®10‰∏™‰∏çÂπ≥Ë°°Âü∫ÂáÜÊï∞ÊçÆÈõÜ‰∏äÁöÑÊÄßËÉΩÔºåÂåÖÊã¨Âü∫Á∫øÔºàÊó†Â§ÑÁêÜÔºâ„ÄÅÈáçÈááÊ†∑ÂíåÁÑ¶ÁÇπÊçüÂ§±‰∏âÁßçÁ≠ñÁï•„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **KANsÂú®ÂéüÂßã‰∏çÂπ≥Ë°°Êï∞ÊçÆ‰∏äÁöÑ‰ºòÂäøÔºö** KANsÂú®Âü∫Á∫øÈÖçÁΩÆ‰∏ãÊòæËëó‰ºò‰∫éMLPsÔºåG-meanÊèêÂçá‰∫Ü54%ÔºåF1-scoreÊèêÂçá‰∫Ü55%„ÄÇ\\n> *   **KANs‰∏é‰∏çÂπ≥Ë°°Á≠ñÁï•ÁöÑÂÜ≤Á™ÅÔºö** ‰º†ÁªüÁöÑÈáçÈááÊ†∑ÂíåÁÑ¶ÁÇπÊçüÂ§±Á≠ñÁï•ÊòæËëóÈôç‰Ωé‰∫ÜKANsÁöÑÊÄßËÉΩÔºàG-mean‰∏ãÈôç32%-39%ÔºâÔºåËÄåMLPsÂàô‰ªé‰∏≠ÂèóÁõä„ÄÇ\\n> *   **ËÆ°ÁÆóËµÑÊ∫êÁöÑÈ´òÊàêÊú¨Ôºö** KANsÁöÑËÆ≠ÁªÉÊó∂Èó¥ÂíåÂÜÖÂ≠òÈúÄÊ±ÇËøúÈ´ò‰∫éMLPsÔºàËÆ≠ÁªÉÊó∂Èó¥Â¢ûÂä†1000ÂÄçÔºåÂÜÖÂ≠òÈúÄÊ±ÇÂ¢ûÂä†11ÂÄçÔºâÔºå‰ΩÜÊÄßËÉΩÊèêÂçá‰∏çÊàêÊØî‰æã„ÄÇ\\n> *   **ÁªüËÆ°È™åËØÅÔºö** ÁªüËÆ°ÊµãËØïÁ°ÆËÆ§MLPs‰∏é‰∏çÂπ≥Ë°°ÊäÄÊúØÁªìÂêàÊó∂‰∏éKANsÂú®ÊÄßËÉΩ‰∏äËææÂà∞Á≠â‰ª∑Ôºà|d| < 0.08ÔºâÔºå‰ΩÜËµÑÊ∫êÊ∂àËÄóÊòæËëóÊõ¥‰Ωé„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   KANsÂü∫‰∫éKolmogorov-ArnoldË°®Á§∫ÂÆöÁêÜÔºåËØ•ÂÆöÁêÜÊåáÂá∫‰ªª‰ΩïÂ§öÂÖÉËøûÁª≠ÂáΩÊï∞ÈÉΩÂèØ‰ª•Ë°®Á§∫‰∏∫ÊúâÈôê‰∏™‰∏ÄÂÖÉËøûÁª≠ÂáΩÊï∞ÁöÑÂè†Âä†„ÄÇKANsÁî®ÂèØÂ≠¶‰π†ÁöÑÊ†∑Êù°ÂèÇÊï∞Âåñ‰∏ÄÂÖÉÂáΩÊï∞Êõø‰ª£‰º†ÁªüÁöÑÁ∫øÊÄßÊùÉÈáçÔºå‰ªéËÄåÊèê‰æõÊõ¥È´òÁöÑÂèØËß£ÈáäÊÄßÂíåÂáΩÊï∞ÈÄºËøëËÉΩÂäõ„ÄÇ\\n> *   KANsÁöÑËÆæËÆ°Âì≤Â≠¶ÊòØÈÄöËøáËá™ÈÄÇÂ∫îÁöÑ‰∏ÄÂÖÉÂáΩÊï∞ÊçïÊçâÂ§çÊùÇÁöÑÂÜ≥Á≠ñËæπÁïåÔºåËÄåÊó†ÈúÄÊòæÂºèÁöÑÈáçÈááÊ†∑Êàñ‰ª£‰ª∑ÊïèÊÑüÊäÄÊúØ„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØîÔºö** ‰º†ÁªüÁ•ûÁªèÁΩëÁªúÔºàÂ¶ÇMLPsÔºâ‰æùËµñ‰∫éÂõ∫ÂÆöÁöÑÊøÄÊ¥ªÂáΩÊï∞ÂíåÂ§ßÈáèÂèÇÊï∞ÔºåÁº∫‰πèÂèØËß£ÈáäÊÄßÔºå‰∏îÂú®‰∏çÂπ≥Ë°°Êï∞ÊçÆ‰∏äË°®Áé∞‰∏ç‰Ω≥„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõÔºö** KANsÈÄöËøáÊ†∑Êù°ÂèÇÊï∞ÂåñÁöÑ‰∏ÄÂÖÉÂáΩÊï∞ÂíåÊÆãÂ∑ÆËøûÊé•ÔºåÂÆûÁé∞‰∫ÜÊõ¥È´òÁöÑÂèØËß£ÈáäÊÄßÂíåÂú®‰∏çÂπ≥Ë°°Êï∞ÊçÆ‰∏äÁöÑ‰ºòÂºÇË°®Áé∞„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> *   1. **Êû∂ÊûÑËÆæËÆ°Ôºö** KANsÈááÁî®Â§öÂ±ÇÁªìÊûÑÔºåÊØèÂ±ÇÂåÖÂê´‰∏ÄÂÖÉÂáΩÊï∞ÁöÑÂè†Âä†ÂíåÁ∫øÊÄßËÅöÂêà„ÄÇ\\n> *   2. **Ê†∑Êù°ÂèÇÊï∞ÂåñÔºö** ÊØè‰∏™‰∏ÄÂÖÉÂáΩÊï∞Áî®BÊ†∑Êù°Ëøë‰ººÔºåÈÅøÂÖçÂ§öÈ°πÂºèÈÄºËøëÁöÑÊåØËç°ÈóÆÈ¢ò„ÄÇ\\n> *   3. **ÊÆãÂ∑ÆËøûÊé•Ôºö** ÂºïÂÖ•SiluÊøÄÊ¥ªÂáΩÊï∞ÂíåÊ†∑Êù°ÁªÑ‰ª∂ÁöÑÊÆãÂ∑ÆËøûÊé•ÔºåÁ°Æ‰øùÊ¢ØÂ∫¶Á®≥ÂÆö‰º†Êí≠„ÄÇ\\n> *   4. **‰ºòÂåñÔºö** ‰ΩøÁî®ËΩØÊúÄÂ§ßÂåñÂíåÁÑ¶ÁÇπÊçüÂ§±ÈÄÇÈÖçKANsÁöÑËæìÂá∫Ôºå‰ª•Â§ÑÁêÜ‰∏çÂπ≥Ë°°Êï∞ÊçÆ„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> *   ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ„ÄÇ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   Â§öÂ±ÇÊÑüÁü•Êú∫ÔºàMLPsÔºâ\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®G-mean‰∏äÔºö** KANsÂú®Âü∫Á∫øÈÖçÁΩÆ‰∏ãËææÂà∞‰∫Ü0.4393ÔºåÊòæËëó‰ºò‰∫éMLPsÔºà0.2848Ôºâ„ÄÇ‰∏éMLPsÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü54%„ÄÇ\\n> *   **Âú®F1-score‰∏äÔºö** KANsÂú®Âü∫Á∫øÈÖçÁΩÆ‰∏ãËææÂà∞‰∫Ü0.55ÔºåÊòæËëó‰ºò‰∫éMLPsÔºà0.35Ôºâ„ÄÇ‰∏éMLPsÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü55%„ÄÇ\\n> *   **Âú®ËÆ≠ÁªÉÊó∂Èó¥‰∏äÔºö** KANsÁöÑÂü∫Á∫øËÆ≠ÁªÉÊó∂Èó¥‰∏∫505ÁßíÔºåËøúÈ´ò‰∫éMLPsÔºà0.5ÁßíÔºâ„ÄÇ‰∏éMLPsÁõ∏ÊØîÔºåÂ¢ûÂä†‰∫Ü1000ÂÄç„ÄÇ\\n> *   **Âú®ÂÜÖÂ≠ò‰ΩøÁî®‰∏äÔºö** KANsÁöÑÂü∫Á∫øÂÜÖÂ≠òÈúÄÊ±Ç‰∏∫1.48MBÔºåËøúÈ´ò‰∫éMLPsÔºà0.06MBÔºâ„ÄÇ‰∏éMLPsÁõ∏ÊØîÔºåÂ¢ûÂä†‰∫Ü11ÂÄç„ÄÇ\\n> *   **Âú®ÈáçÈááÊ†∑Á≠ñÁï•‰∏ãÔºö** KANs‰∏éMLPsÁöÑÊÄßËÉΩÂ∑ÆË∑ùÊòæËëóÁº©Â∞èÔºåG-meanÂàÜÂà´‰∏∫0.5904Âíå0.5943ÔºåÂá†‰πéÁ≠â‰ª∑„ÄÇ\\n> *   **Âú®ÁÑ¶ÁÇπÊçüÂ§±Á≠ñÁï•‰∏ãÔºö** KANs‰∏éMLPsÁöÑÊÄßËÉΩÂá†‰πéÁõ∏ÂêåÔºåG-meanÂàÜÂà´‰∏∫0.580Âíå0.580„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n> **ÊèêÂèñ‰∏éÊ†ºÂºèÂåñË¶ÅÊ±Ç**\\n> *   Kolmogorov ArnoldÁΩëÁªú (Kolmogorov Arnold Networks, KANs)\\n> *   ‰∏çÂπ≥Ë°°Êï∞ÊçÆ (Imbalanced Data, N/A)\\n> *   ÂàÜÁ±ª (Classification, N/A)\\n> *   Ë°®Ê†ºÊï∞ÊçÆ (Tabular Data, N/A)\\n> *   ÂÆûËØÅÁ†îÁ©∂ (Empirical Study, N/A)\\n> *   ÂèØËß£ÈáäÊÄß (Interpretability, N/A)\\n> *   Ê†∑Êù°ÈÄºËøë (Spline Approximation, N/A)\\n> *   ÊÆãÂ∑ÆËøûÊé• (Residual Connection, N/A)\"\n}\n```"
}