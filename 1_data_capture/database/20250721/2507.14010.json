{
    "source": "ArXiv (Semantic Scholar未收录)",
    "arxiv_id": "2507.14010",
    "link": "https://arxiv.org/abs/2507.14010",
    "pdf_link": "https://arxiv.org/pdf/2507.14010.pdf",
    "title": "Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations",
    "authors": [
        "Yong Feng",
        "Xiaolei Zhang",
        "Shijin Feng",
        "Yong Zhao",
        "Yihan Chen"
    ],
    "categories": [
        "cs.CV"
    ],
    "publication_date": "未找到提交日期",
    "venue": "暂未录入Semantic Scholar",
    "fields_of_study": "暂未录入Semantic Scholar",
    "citation_count": "暂未录入Semantic Scholar",
    "influential_citation_count": "暂未录入Semantic Scholar",
    "institutions": [
        "Urban Mobility Institute, Tongji University",
        "Key Laboratory of Geotechnical and Underground Engineering of Ministry of Education, Department of Geotechnical Engineering, Tongji University"
    ],
    "paper_content": "# Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations\n\nYong Feng1  Xiaolei Zhang2  Shijin Feng1,2  Yong Zhao2  Yihan Chen1   \n（1. Urban Mobility Institute, Tongji University, 4800 Cao an Rd, Shanghai, 201804, China; 2. Key Laboratory of Geotechnical and   \nUnderground Engineering of Ministry of Education, Department of Geotechnical Engineering, Tongji University, 1239 Si ping Rd, Shanghai, 200092, China） E-mail: fengyongtj@tongji.edu.cn\n\nAbstract: Tunnel lining crack is a crucial indicator of tunnels’ safety status. Aiming to classify and segment tunnel cracks with enhanced accuracy and efficiency, this study proposes a two-step deep learning-based method. An automatic tunnel image classification model is developed using the DenseNet-169 in the first step. The proposed crack segmentation model in the second step is based on the DeepLab $\\mathrm { V } 3 +$ , whose internal logic is evaluated via a score-weighted visual explanation technique. Proposed method combines tunnel image classification and segmentation together, so that the selected images containing cracks from the first step are segmented in the second step to improve the detection accuracy and efficiency. The superior performances of the two-step method are validated by experiments. The results show that the accuracy and frames per second (FPS) of the tunnel crack classification model are $9 2 . 2 3 \\%$ and 39.80, respectively, which are higher than other convolutional neural networks (CNN) based and Transformer based models. Also, the intersection over union (IoU) and F1 score of the tunnel crack segmentation model are $5 7 . 0 1 \\%$ and $67 . 4 4 \\%$ , respectively, outperforming other state-of-the-art models. Moreover, the provided visual explanations in this study are conducive to understanding the “black box” of deep learning-based models. The developed two-stage deep learning-based method integrating visual explanations provides a basis for fast and accurate quantitative assessment of tunnel health status.\n\nKeywords: Tunnel crack; Deep learning; Image classification; Semantic segmentation; Visual explanation\n\n# 1 Introduction\n\nDue to the issues of traffic congestion and increased population, numerous transportation infrastructures have been constructed and operated during the past decades (Jiang et al., 2023; Feng et al., 2023). With advantages such as high construction efficiency, high flexibility, low resource consumption, and minor environmental disturbance, tunnels have been widely used and become the core infrastructures (Zhang et al., 2023). However, the tunnels in service, as geotechnical infrastructures, are inevitably subjected to the joint action of adjacent excavation disturbance, untimely maintenance, deterioration of materials, temperature variation, improper construction, and groundwater (Feng et al., 2023). Various defects, such as crack, leakage, and spalling, often occur on the tunnel linings, necessitating timely inspection and management.\n\nNumerous statistical results show that lining crack is one of the most severe defects in tunnel engineering (Huang et al., 2022). The occurrence of cracks on the lining surface will affect the reliability and integrity of the tunnel structure to an extent, thereby causing water leakage, concrete corrosion, and reduced lining-bearing capacity of tunnels. Without limiting their expansion, cracks may lead to serious safety accidents (Zhou et al., 2023). Hence, it is necessary to carry out regular crack inspections to ensure the long-term stability and safety of the tunnels. Traditionally, tunnel crack detection relied on manpower. However, the necessity of active use of intelligent recognition technology for tunnel lining cracks has been emphasized due to the low accuracy, low efficiency, high subjectivity, and high risk of manual visual inspections. Deep learning, especially convolutional neural network (CNN), which has made a paradigm shift recently, provides unprecedented opportunities to facilitate fast, accurate, and automatic recognition of tunnel lining cracks.\n\nIn the past, many researchers have delved into CNN-based approaches for tunnel lining crack identification. Huang et al. (2018) established a two-stream algorithm based on FCN, where one stream was employed to segment the cracks by sliding-window-assembling operation, and the other was used to segment leakages by resizing-interpolation operation. Yang et al. (2018) also adopted FCN to semantically identify and segment pixel-wise cracks at different scales, and then utilized traditional digital image processing techniques to quantitatively measure the morphological features of cracks. Miao et al. (2019) proposed a novel semantic segmentation by integrating the UNet with squeeze-and-excitation and residual learning blocks to segment crack and spalling defects. Following the same design paradigm, Hou et al. (2021) and Dang et al. (2022) also improved the UNet through adding residual learning units to complete the tunnel lining crack segmentation task. In the work of Gao et al. (2019), an optimal adaptive selection model (RetinaNet-AOS) based on RetinaNet was developed for semantic segmentation on tunnel lining crack images. Xu and Yang (2019) implemented crack identification by means of Mask R-CNN, which has been widely used in the field of tunnel lining defect detection. Similar to the tasks completed by the Mask R-CNN, Zhao et al. (2021) presented an improved PANet model to obtain refined crack segmentation results, achieving complete separation of cracks from the lining backgrounds. After that, Zhou et al. (2022) applied YOLOv4 enhanced by EfficientNet and depthwise separable convolution to detect three types of tunnel lining defects, i.e., crack, water leakage, and rebar-exposed. Considering the excellent detection performance of the YOLO series models, Liu et al. (2022) coupled YOLOv5 with a transfer learning technique to localize the cracks in the road tunnel lining images.\n\nBased on the above, it can be summarized that existing studies have achieved outstanding results in tunnel lining defect identification by investigating more perfect algorithms or improving existing ones. However, in real-world applications, it is common that image acquisition devices acquire a large amount of lining image data, including abundant normal (defect-free) images and anomalous (crack) ones. The existing engineering requirement is to efficiently, accurately, and automatically identify cracks from massive amounts of tunnel image data, which has become a considerable and urgent challenge for previous research. Most of the available studies directly treat the crack recognition problem as a single-attribute classification or separately strive to find the locations or areas of the cracks as a localization or segmentation problem. To address these limitations, this study proposes a tunnel lining crack recognition framework in a two-stage manner, which combines image classification and segmentation. The purpose of tunnel lining image classification is to determine whether the image contains cracks, while the aim of tunnel lining crack segmentation is to extract cracks from backgrounds in crack images, which are different in definition. Image classification is much easier than crack segmentation. Specifically, the image classification task classifies the tunnel lining images as defect-free or crack ones, while the segmentation task getting the crack pixels from images is much more complex and time-consuming. Consequently, a tunnel lining image classification model named DenseNet-169 is adopted in the first stage to classify and save the images containing cracks. Another tunnel lining crack segmentation model called DeepLab $\\mathrm { V } 3 +$ is exploited in the second stage to process the crack images and isolate the cracks from backgrounds.\n\nNevertheless, deep learning models also have deficiencies and limitations, such as being considered as ‘black box’ models, which have trouble in explaining physically and rely heavily on hyperparameter settings. To account for mechanisms behind the ‘black box’ of the crack segmentation model and build trust in deep learning models, this study employs an advanced visual interpretation technology, i.e., score-weighted class activation mapping (Score CAM) (Wang et al., 2020). The motivation behind Score CAM is to generate heatmaps highlighting meaningful regions to intuitively explain the CNN-based models' internal mechanism.\n\n# 2 Methodology\n\n# 2.1  Framework for tunnel lining crack identification\n\nThis study designs the tunnel lining crack recognition framework in a two-stage manner, which is shown in Fig. 1. In the beginning, the tunnel lining images collected from practical tunnel engineering are fed into the automatic classification stage, where an image classification model is responsible for selecting and saving crack images from massive tunnel lining image data. Specifically, the DenseNet-169 model takes the lining image as an input and outputs two probabilities to determine whether the image belongs to a crack or defect-free image. Subsequently, the images containing cracks are further input into the second stage to isolate the cracks from the backgrounds. That is, the DeepLab $\\cdot \\mathrm { V } 3 +$ model is dedicated to identifying the pixels belonging to cracks in the images. Through these two stages, cracks' locations and geometric shapes are extracted with high precision and efficiency. The deep learning models employed in each stage are described in detail in the following subsections. Moreover, a CNN explanation step is also incorporated into the overall framework to understand the operational mechanism of the segmentation model and build trust in CNN.\n\nOn-site Inspection of Tunnel Linings Tunnel Lining Images Collection and Preprocessing Classification Step Result: Segmentation Step Explanation Step Crack 1 ↓ Probability Tunnel Lining Cracks ScoreResult:Crack？ Yes Semantic Segmentation CAM No T + Output: Crack Location SalieneyMaps Background Geometric Characteristics Visual Explanations Probability AutomaticSegmentation DeepLearning-based Tunnel Lining Crack Detection Framework\n\n# 2.2  Tunnel lining image classification model\n\nPowered by the successful application of dense convolutional network (DenseNet) (Huang et al., 2018), this study employs DenseNet-169 as the automatic classification model for tunnel lining images in the first step. DenseNet is a type of convolutional neural network that directly connects all layers (with matching feature-map sizes) with each other for maximum information flow between layers in the network. In particular, each layer obtains additional inputs from all preceding layers and passes on its feature maps to all subsequent layers, as observed in Figs. 2 and 3.\n\nBlock Dense Tran- Dense Tran- Dense Tran- Dense block1 sitionl block2 sition2 block3 sition3 block4\n\nDenseNet is a multi-stage architecture where the first stage consists of a standard $7 { \\times } 7$ convolution, batch normalization, ReLU activation function, and maximum pooling, followed by four stages consisting of Denseblocks and three stages composed of Transition blocks, and finally, a global average pooling layer and a fully connected layer responsible for outputting the predictions. DenseNet-169 is one of the DenseNets, containing 6, 12, 32, and 32 Denseblocks in four stages, respectively. It is worth pointing out that the cross entropy function is chosen as the model's loss function.\n\n![](images/b827102f241336227b87ade2dcba22f6b5a04c3b1e69f2e7d979ca8d6eeba067.jpg)  \nFigure 3  Details of modules in DenseNet\n\n# 2.3  Tunnel lining crack segmentation model\n\nAfter selecting images containing cracks from a large number of tunnel lining images, a crucial task is to segment the cracks from the images in an end-to-end and pixel-to-pixel manner. To this end, DeepLab $\\mathrm { V } 3 +$ (Chen et al., 2018) serves as the crack segmentation model in the second step. Like most semantic segmentation models, the DeepLab $\\mathrm { V } 3 +$ model comprises an encoder and a decoder, with the overall architecture shown in Fig. 4.\n\n![](images/a2a7c409e7c72dab742dd52951a07f5b9bbe345da3be915d584d4924c646e59f.jpg)  \nFigure 4  Overall architecture of the DeepLab $\\mathrm { V } 3 +$ model\n\nResNet-101 (He et al., 2016) is used as the backbone structure to extract shallower features containing spatial information and deeper features containing semantic information for tunnel lining cracks. Concretely, the ResNet-101 includes a ‘Block’ for downsampling operation with a factor of four, followed by ‘Layer 1’, ‘Layer $2 ^ { \\circ }$ , ‘Layer $3 ^ { \\circ }$ , and ‘Layer $\\cdot \\cdot \\mathbf { \\partial } _ { 4 } ,$ for feature extraction. The innovation of the DeepLab $\\mathrm { V } 3 +$ model is the atrous spatial pyramid pooling (ASPP) in the encoder. Discontinuity problem is prevalent in crack segmentation tasks. The reason for this phenomenon is the loss of multi-scale information about the cracks. To compensate for this deficiency, the ASPP structure is created with multiple dilated convolutions. For a dilated convolution with a dilation rate of d and kernel size of N, the receptive field reaches the same as that of a standard convolution with a kernel size of $( \\mathbf { N } \\mathrm { ~ - ~ } 1 ) \\times \\mathbf { d } + 1$ . As a result, dilated convolution is capable of increasing the receptive field while maintaining the size of the feature maps, thereby reducing the loss of crack feature information caused by consecutive convolution and pooling downsampling operations. In ASPP, three dilated convolution operations with dilation rates of 6, 12, and 18 conduct multi-scale sampling on the feature maps output by ResNet-101. In addition, the ASPP integrates a $1 \\times 1$ standard convolution operation and an average pooling operation, which together with the dilated convolutions, form feature maps containing multi-scale information about the cracks. A $1 \\times 1$ convolution then processes the features to realize further feature extraction and dimensionality reduction. In the decoder, the shallow features generated by ‘Layer $\\mathbf { \\xi } _ { 1 } \\mathbf { \\cdot } \\mathbf { \\xi } _ { }$ of ResNet-101 are collected and then concatenated with upsampled deep features obtained from ASPP. Skip connection enables the deep learning model to combine local spatial features of cracks with global context information, thus further relieving the discontinuity problem in crack segmentation. Fused features are then subjected to successive $3 { \\times } 3$ convolution and $4 \\times$ bilinearly upsampling operations to restore the size of the original input image and produce the final semantic segmentation result.\n\nLast but not least, the loss function is also crucial in segmenting tunnel lining cracks. Tunnel lining cracks are slender and small targets in the image compared to the background. To put it differently, the crack pixels are much less than the background pixels in tunnel lining images. Therefore, the imbalanced data will make the segmentation model biased towards the background class rather than cracks. To address this issue, dice loss is selected as the loss function of the DeepLab $\\mathrm { V } 3 +$ model.\n\n# 2.4  Visual explanation methods\n\nAs described before, this study aims to understand and explore the mechanisms behind the ‘black box’ of the crack segmentation model. An advanced visual interpretation technology, i.e., Score CAM, is leveraged, and its detailed processing steps are as follows.\n\nGiven the CNN model (i.e., DeepLab $\\mathrm { V } 3 +$ in this study) $Y = f ( X )$ that takes $X$ as an input and outputs a scalar result Y. We select a targeted convolutional layer $l$ in $f$ and the corresponding activations as $A$ . Define the $k$ -th channel of $A _ { l }$ as $\\dot { \\boldsymbol { A } } _ { l } ^ { k }$ . Given a known baseline input $X _ { b }$ , the contribution of $\\boldsymbol { A } _ { l } ^ { k }$ to $Y$ is expressed as follows.\n\n$$\nC ( A _ { l } ^ { k } ) = f ( X \\circ H _ { l } ^ { k } ) - f ( X _ { b } )\n$$\n\n$$\nH _ { l } ^ { k } = s ( U p ( A _ { l } ^ { k } ) )\n$$\n\nwhere $U p ( \\cdot )$ represents the up-sampling operation of $\\boldsymbol { A } _ { l } ^ { k }$ into the input size. $s ( \\cdot )$ denotes the normalization operation converting each element in the input matrix to the interval [0, 1].\n\nFor a class of interest $c$ (i.e., crack in this study), Score-CAM LcScore CAM is defined as follows.\n\n$$\nL _ { S c o r e - C A M } ^ { c } = R e L U ( \\sum C ( A _ { l } ^ { k } ) A _ { l } ^ { k } )\n$$\n\nDefine a weight ckoefficient $\\dot { \\alpha } _ { k } ^ { c } = { \\cal C } ( A _ { l } ^ { k } )$ , then the equation (3) can be abbreviated to equation (4).\n\n$$\nL _ { ( S c o r e - C A M ) } ^ { c } = R e L U ( \\sum \\alpha _ { k } ^ { c } A _ { l } ^ { k } )\n$$\n\nBased on the abovke steps, visual heatmaps can be acquired to understand the crack segmentation model's internal structure and decision-making mechanism.\n\n# 3 Data\n\nAll experiments in this study were carried out based on an open-source dataset named NUAACrack-2000 (Qiu et al., 2022; Zhang et al., 2021), where the tunnel lining images were captured in China. On the basis of the NUAACrack-2000, two datasets were established: one was exploited for training and testing of the DenseNet-169 model, and the other for training and testing of the DeepLab $\\mathrm { V } 3 +$ model.\n\nFig. 5 illustrates the typical crack and defect-free images in the image classification dataset, and Table 1 summarizes the number of different types of images. A total of 1942 images with a pixel size of $5 1 2 \\times 3 7 5$ were divided into training set, validation set, and testing set according to a ratio of 7:2:1.\n\n![](images/72871429c66e5acfe167492ca3d4df09e9a7bd9f4ae366f1922c15e354d177b4.jpg)\n\nFigure 5  Examples of tunnel lining image classification dataset Table 1  Dataset for image classification experiment   \n\n<html><body><table><tr><td></td><td>Training</td><td>Validation</td><td>Testing</td><td>Total</td></tr><tr><td>Background</td><td>333</td><td>94</td><td>47</td><td>474</td></tr><tr><td>Crack</td><td>1029</td><td>293</td><td>146</td><td>1468</td></tr><tr><td>Total</td><td>1362</td><td>387</td><td>193</td><td>1942</td></tr></table></body></html>\n\nAfter that, 1468 crack images and their ground truths made up the semantic segmentation dataset, as shown in Fig. 6. Each crack image corresponds to a label, which was used to supervise the training of the segmentation model. These crack images were also partitioned based on the holdout approach in a ratio of 7:2:1. That is, 1028 images were used for training, 294 images for verification, and the remaining 146 images for testing.\n\n![](images/6d5e66b783715c2cb37ec67bf679a040803bdbef290c5066e7728ebe0370c162.jpg)  \nFigure 6  Examples of crack segmentation dataset\n\n# 4  Experiment\n\nThe experiments in this study mainly involve two parts: image classification and crack segmentation. In the first part, the image classification dataset was utilized to train and test the DenseNet-169 model. In the second part, the crack segmentation dataset was used for the training and testing of the DeepLab $\\mathrm { V } 3 +$ model. Comparative experiments with other dominant models and visual explanations were also included in this section.\n\n# 4.1 Experimental environment\n\nExperiments were conducted under the computer configurations of the Windows 10 operating system, one NVIDIA GeForce RTX 3090 graphics processing unit (GPU), and one Intel Core i9-12900KF central processing unit (CPU). Python 3.8.12, Pytorch 1.12.0, CUDA 11.6, and CUDNN 8.3 constitute the computation software environment.\n\n# 4.2 Tunnel lining image classification experiment\n\n# 4.2.1 Evaluation criteria\n\nThis study adopted two commonly used measures, accuracy and frames per second (FPS), to evaluate the performance of the classification model.\n\n# 4.2.2 Model training and testing\n\nTo accelerate training speed and save computational resources, all images in the classification dataset were resized to $2 2 4 \\times 2 2 4$ before being input into the model. The batch size was set to four; that is, the model processed four images simultaneously in each iteration. To ensure the convergence of the model, the training epoch is determined to be 100. The learning rate is another important hyperparameter. For the purpose of finding the global minimum of the deep learning model, a dynamic learning rate schedule was adopted instead of a constant learning rate. Specifically, the initial learning rate of 0.005 was decreased every ten epochs with an attenuation factor of 0.1. After completing training, the performance evaluation of the model was performed on 193 testing images. Fig. 7 intuitively displays the predicted results of the DenseNet-169 model on several images in the testing set. As seen in Fig. 7, the deep learning model automatically outputs two probability values for each image and then determines the image's category. The first row in Fig. 7 lists four images classified as background, and the second row shows the examples identified as crack images. Through calculation, the accuracy of the DenseNet-169 model reaches $9 2 . 2 3 \\%$ , and the FPS reaches 39.80.\n\n![](images/9356fe0efc340468519726956107e290d9313a0e1665a29a807e66fdb29cbc37.jpg)\n\n![](images/515c531dab664da734cde4f4a644354675cc503b5807f7e8d4053c40f03f7910.jpg)  \nFigure 7  Predictions of DenseNet-169 on several testing images\n\n# 4.2.3 Comparison with other models\n\nFor the sake of further evaluating the classification performance of the DenseNet-169 model for tunnel lining images, six state-of-the-art deep learning models were implemented for comparison. The models used for comparison include CNN-based models and visual Transformer-based models, namely DenseNet-201, EfficientNet-B0, ResNet-50, ResNet-101, Swin Transformer, and Vision Transformer. As reported in Table 2, the DenseNet-169 outperforms the other six deep learning models in terms of accuracy, as reflected in the $2 . 0 7 \\%$ , $1 6 . 5 8 \\%$ , $1 7 . 1 0 \\%$ , $1 8 . 6 5 \\%$ , $1 6 . 5 8 \\%$ , and $1 6 . 5 8 \\%$ improvement over DenseNet-201, EfficientNet-B0, ResNet-50, ResNet-101, Swin Transformer, and Vision Transformer.\n\nTable 2  Results of image classification experiments   \n\n<html><body><table><tr><td>Models</td><td>Accuracy (%)</td><td>FPS (f/s)</td></tr><tr><td>DenseNet-169</td><td>92.23</td><td>39.80</td></tr><tr><td>DenseNet-201</td><td>90.16</td><td>35.60</td></tr><tr><td>EfficientNet-B0</td><td>75.65</td><td>40.77</td></tr><tr><td>ResNet-50</td><td>75.13</td><td>60.22</td></tr><tr><td>ResNet-101</td><td>73.58</td><td>51.68</td></tr><tr><td>Swin Transformer</td><td>75.65</td><td>42.48</td></tr><tr><td>Vision Transformer</td><td>75.65</td><td>69.64</td></tr></table></body></html>\n\nDenseNet-169 incorrectly classified 12 crack images as defect-free images. As for Swin Transformer and Vision Transformer, these two models wrongly classified all defect-free images as crack images, resulting in the lowest accuracy. The reason for this is inferred to be twofold: on the one hand, the small size of the classification dataset used in this paper prevents the self-attention mechanism in the visual Transformer from realizing its potential, and on the other hand, the imbalance in the number of positive and negative samples leads to a bias in the recognition ability of the models towards the crack images.\n\nFrom the perspective of the running speed of the models, the FPS of DenseNet-169 is higher than that of DenseNet-201 but lower than that of other models. Improving the running speed of the classification model deserves further research in the future.\n\n# 4.3 Tunnel lining crack segmentation experiment\n\n# 4.3.1 Evaluation criteria\n\nThis study exploited four metrics widely used in semantic segmentation tasks, i.e., precision, recall, F1 score, and Intersection over Union (IoU), to comprehensively assess the crack segmentation models.\n\nIoU and threshold d were introduced to compare the similarity between two arbitrary cracks. For crack images, the IoU between the segmented crack and ground truth was calculated to judge whether the crack has been detected. Generally, the threshold d was set to 0.5 (Liu and Wang, 2022). In this study, when the IoU is greater than 0.5, it is considered that the model has detected out the crack; otherwise, it is not. IoU also served as the primary evaluation indicator for segmentation models in this paper.\n\n# 4.3.2 Model training and testing\n\nDuring the training process, the original images were first resized to $5 1 2 { \\times } 3 8 4$ pixels, which would not cause a large amount of calculation and is easy to work with. Accordingly, the batch size was set as 8. The learning rate was also dynamically adjusted, with a value of 0.001 for the first 50 epochs and 0.0001 for the last 50 epochs. A total of 100 rounds of training ensured the convergence of the crack segmentation models. After each round of training, the model was evaluated on the validation set, and the training and validation loss curves were recorded in Fig. 8. It can be seen from Fig. 8 that the training loss declines rapidly over the first ten epochs, then slowly decreases in the next 40 epochs, and stabilizes around 0.2 over the last 50 epochs. Validation loss shows fluctuations in the first 50 epochs and stabilizes in the next 50. Based on the values of the loss function on the validation set, the optimal trained model was determined and saved accordingly.\n\n![](images/90f61ae0c1eb76b32df498ba584b08c92a292cfe8f4e5f80c5c3c5b02418ea66.jpg)  \nFigure 8  Visualization diagram of loss curves of DeepLab $\\mathrm { V } 3 +$ model during training and validation process\n\nThe best-trained model was tested using crack images in the semantic segmentation dataset, and the results showed that the DeepLab ${ \\mathrm { V } } 3 + { }$ model achieved $5 7 . 0 1 \\%$ IoU, $67 . 4 4 \\%$ $\\mathrm { F } _ { 1 }$ score, $6 2 . 9 5 \\%$ precision, and $8 2 . 1 2 \\%$ recall. There is a significant gap between the precision and recall of the DeepLab ${ \\mathrm { V } } 3 + { }$ model. It should be emphasized that deep learning models must detect all cracks as much as possible from the perspective of engineering applications. Hence, it is reasonable to improve the recall of tunnel lining crack segmentation during practical inspection tasks, even at the expense of precision (Ren et al., 2020).\n\n# 4.3.3 Comparison with other models\n\nExtensive comparative tests were completed to further verify the applicability and superiority of the DeepLab $\\mathrm { V } 3 +$ model in tunnel lining crack segmentation. Four dominant semantic segmentation models, namely DeepLabV3 with backbone ResNet-101, PSPNet with backbone ResNet-50, UNet backboned by VGG-13, and UNet $^ { + + }$ backboned by VGG-13 were trained with the crack segmentation dataset for more comprehensive comparisons.\n\nAs shown in Table 3, the DeepLab $\\mathrm { V } 3 +$ model obtains the best IoU of $5 7 . 0 1 \\%$ , which is $1 . 7 7 \\%$ , $5 . 7 7 \\%$ , $0 . 1 7 \\%$ , and $0 . 5 9 \\%$ higher than DeepLabV3, PSPNet, UNet, and $\\mathrm { U N e t + + }$ , respectively. Similarly, the precision of DeepLab $\\mathrm { \\Delta V } 3 + \\mathrm { \\Delta }$ is the highest, improved by $2 . 0 2 \\%$ , $3 . 7 2 \\%$ , $0 . 6 0 \\%$ , and $2 . 6 4 \\%$ compared to DeepLabV3, PSPNet, UNet, and $\\mathrm { U N e t + + }$ , respectively. The F1 score for DeepLab $\\mathrm { \\Delta V } 3 + \\mathrm { \\Delta }$ is only slightly lower than UNet but higher than the other three models. The recall of the DeepLab $\\mathrm { V } 3 +$ model is lower than that of UNet and ${ \\mathrm { U N e t } } + +$ but $0 . 8 3 \\%$ and $8 . 5 2 \\%$ higher than that of DeepLabV3 and PSPNet, respectively.\n\nTable 3  Results of semantic segmentation experiments   \n\n<html><body><table><tr><td>Models</td><td>IoU (%)</td><td>F1 score (%)</td><td>Precision (%)</td><td>Recall (%)</td></tr><tr><td>DeepLabV3+</td><td>57.01</td><td>67.44</td><td>62.95</td><td>82.12</td></tr><tr><td>DeepLabV3</td><td>55.24</td><td>66.33</td><td>60.93</td><td>81.29</td></tr><tr><td>PSPNet</td><td>51.24</td><td>62.18</td><td>59.23</td><td>73.60</td></tr><tr><td>UNet</td><td>56.84</td><td>67.68</td><td>62.35</td><td>83.91</td></tr><tr><td>UNet++</td><td>56.42</td><td>67.31</td><td>60.31</td><td>84.91</td></tr></table></body></html>\n\nRepresentative testing results predicted by different deep learning models are depicted in Fig. 9 to complement the quantitative comparison results above, with the orange dashed box and red dashed box denoting the incorrect and missed detections. From top to bottom of Fig. 9, they are the raw images in the crack segmentation dataset, labels, and predicted results made by DeepLab $\\mathrm { V } 3 +$ , DeepLabV3, PSPNet, $\\mathrm { U N e t + + }$ , and UNet. The DeepLab $\\mathrm { V } 3 +$ model performs best on the four testing samples, as seen by the comparison of segmentation results of different models. For case 1, the cracks can be accurately isolated by DeepLab $\\mathrm { ~ V } 3 +$ and PSPNet. However, a small section of crack is omitted by DeepLabV3. There are some false detections in the identification results of UNet. UNet++ also has the situation of false detections. For case 2, all models successfully segment the cracks, except for PSPNet, ignoring large sections of the crack. As seen in the third and fourth columns of Fig. 9, the features of the cracks do not differ significantly from the background features, posing a challenge for crack identification. In case 3, a small portion of cracks are not segmented by PSPNet and UNet, and a small background area is falsely identified as cracks by $\\mathrm { U N e t + + }$ . In case 4, DeepLabV3, PSPNet, $\\mathrm { U N e t + + }$ , and UNet have different degrees of missed detections. Additionally, a small portion of background pixels are mistakenly detected as crack pixels. The width of the crack segmented by the DeepLab $\\mathrm { \\Delta V } 3 + \\mathrm { \\Delta }$ model is larger than the actual crack width. Nevertheless, the predicted result retains more edge information relative to the other four models.\n\n![](images/485eecf9c2784ec1c9e74e0daa581f596be2505beab352cc9bb8e2fb3f07026e.jpg)  \nFigure 9  Comparison of prediction performance of different models on the test set\n\nThese quantitative and qualitative comparative results above indicate that adopting the DeepLab $\\mathrm { , V } 3 +$ model in the second step of our proposed framework for crack segmentation is effective and superior.\n\n# 4.4 Visual explanations\n\nThe heatmaps of some key modules of the DeepLabv3 $^ +$ model are generated based on the Score CAM technique to focus on the encoding and decoding process of the model and understand the decision-making process. For a brief discussion, Fig. 10 presents the heatmaps of the key modules of the DeepLab ${ \\mathrm { V } } 3 + { }$ model when a single tunnel crack image is used as input. As depicted in Fig. 10, the model focuses on the image globally during the encoding process. At the stage of Layer 2, the model pays attention to crack, lining surface, and lighting. Until the stage of Layer 4, the model emphasizes the crack, but the highlighted areas (dark-colored) do not precisely fit the crack. During decoding, the DeepLab $\\mathrm { V } 3 +$ model gradually focuses on the crack itself. Another interesting finding is that the low-level and high-level feature maps used for the concatenation operation shown in Fig. 4 present completely different heatmaps. The heatmaps demonstrate that high-level feature maps focus on the global features of the tunnel lining image due to the inclusion of strong semantic information, while low-level feature maps focus on local regions of the image, confirming that shallower feature maps contain rich spatial information.\n\n![](images/2323e60d906736da3ac984939919fb01f6baeba7c753315926492973ce7f42da.jpg)  \nFigure 10  Visual explanations for several key modules of the DeepLabv3 $+$ model based on Score CAM\n\n# 5 Conclusion\n\nThis study proposes a two-step deep learning-based method for the automatic classification and segmentation of tunnel cracks. DenseNet-169 serves as the tunnel lining image classification model in the first step, through which the crack images can be separated and saved from massive image data. In the second step, the DeepLab $\\mathrm { V } 3 +$ model is employed to separate cracks from backgrounds in the crack images. An advanced visual explanation technique is integrated into the two-step method to understand the ‘black box’ of the crack segmentation model. The superiority and rationality of the two-step method are demonstrated through extensive comparative experiments. DenseNet-169 achieves $9 2 . 2 3 \\%$ accuracy, which is improved by $2 . 0 7 \\%$ , $1 6 . 5 8 \\%$ , $1 7 . 1 0 \\%$ , $1 8 . 6 5 \\%$ , $1 6 . 5 8 \\%$ , and $1 6 . 5 8 \\%$ over DenseNet-201, EfficientNet-B0, ResNet-50, ResNet-101, Swin Transformer, and Vision Transformer. The FPS of DenseNet-169 reaches 39.80, exceeding that of DenseNet-201. For DeepLab ${ \\mathrm { V } } 3 + { \\mathrm { ~ } }$ , an IoU of $5 7 . 0 1 \\%$ is obtained, which is $1 . 7 7 \\%$ , $5 . 7 7 \\%$ , $0 . 1 7 \\%$ , and $0 . 5 9 \\%$ higher than DeepLabV3, PSPNet, UNet, and $\\mathrm { U N e t + + }$ , respectively. Furthermore, the provided visual explanations show that the segmentation model focuses on the image globally during the encoding process and gradually focuses on the crack itself during the decoding process. Another interesting finding is high-level feature maps focus on the global features of the tunnel lining image due to the inclusion of strong semantic information. In contrast, low-level feature maps focus on local regions of the image, confirming that shallower feature maps contain rich spatial information.\n\nThere are still some flaws that need to be addressed in the future. Firstly, it is urgently required to introduce model light-weighting techniques to accelerate the image classification models. Secondly, essential image pre-processing or post-processing approaches need to be incorporated into the crack segmentation process to eliminate the complex environmental interference, thus obtaining refined segmentation results. Thirdly, expanding the tunnel lining image classification and crack segmentation datasets to guarantee higher generalization and robustness of the models. Besides, deep learning-based models should be integrated into intelligent detection devices, such as unmanned aerial vehicles (UAVs), to realize automatic and real-time detection of tunnel lining cracks.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   隧道衬砌裂缝是隧道安全状态的关键指标。传统的人工视觉检测方法存在准确性低、效率低、主观性高和风险高等问题。\\n> *   现有研究大多将裂缝识别问题视为单属性分类或单独定位/分割问题，难以高效、准确、自动地从大量隧道图像数据中识别裂缝。\\n\\n> **方法概述 (Method Overview)**\\n> *   本研究提出了一种基于深度学习的两阶段方法，结合图像分类和分割技术，通过DenseNet-169模型分类裂缝图像，再通过DeepLab V3+模型分割裂缝，并利用Score CAM技术解释模型内部机制。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **两阶段方法：** 结合图像分类和分割，提高了检测准确性和效率。\\n> *   **高性能模型：** DenseNet-169分类模型的准确率和FPS分别为92.23%和39.80，优于其他CNN和Transformer模型；DeepLab V3+分割模型的IoU和F1分数分别为57.01%和67.44%，优于其他最先进模型。\\n> *   **视觉解释：** 通过Score CAM技术理解深度学习模型的“黑箱”机制，生成热图解释模型决策过程。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   通过两阶段方法，先分类后分割，减少计算量并提高效率。\\n> *   利用DenseNet-169的密集连接特性增强特征流动，DeepLab V3+的ASPP结构解决多尺度信息丢失问题。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 现有研究多专注于单一任务（分类或分割），缺乏高效的两阶段方法。\\n> *   **本文的改进：** 提出结合分类和分割的两阶段框架，并引入视觉解释技术增强模型可解释性。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.   **分类阶段：** 使用DenseNet-169模型对隧道衬砌图像进行分类，输出裂缝或非裂缝的概率。\\n> 2.   **分割阶段：** 使用DeepLab V3+模型对分类为裂缝的图像进行像素级分割，提取裂缝位置和几何形状。\\n> 3.   **视觉解释：** 通过Score CAM技术生成热图，解释模型决策机制。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   分类模型：DenseNet-201、EfficientNet-B0、ResNet-50、ResNet-101、Swin Transformer、Vision Transformer。\\n> *   分割模型：DeepLabV3、PSPNet、UNet、UNet++。\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在准确率上：** 本文方法在隧道衬砌图像分类上达到了92.23%，显著优于基线模型DenseNet-201 (90.16%)、EfficientNet-B0 (75.65%)、ResNet-50 (75.13%)、ResNet-101 (73.58%)、Swin Transformer (75.65%)和Vision Transformer (75.65%)。与表现最佳的基线相比，提升了2.07个百分点。\\n> *   **在FPS上：** 本文方法的处理速度为39.80 f/s，高于DenseNet-201 (35.60 f/s)，但低于其他基线模型。\\n> *   **在IoU上：** 本文方法在裂缝分割上达到了57.01%，显著优于基线模型DeepLabV3 (55.24%)、PSPNet (51.24%)、UNet (56.84%)和UNet++ (56.42%)。与表现最佳的基线相比，提升了0.17个百分点。\\n> *   **在F1分数上：** 本文方法的F1分数为67.44%，略低于UNet (67.68%)，但高于其他基线模型。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   隧道裂缝 (Tunnel Crack, N/A)\\n*   深度学习 (Deep Learning, DL)\\n*   图像分类 (Image Classification, N/A)\\n*   语义分割 (Semantic Segmentation, N/A)\\n*   视觉解释 (Visual Explanation, N/A)\\n*   DenseNet (Dense Convolutional Network, DenseNet)\\n*   DeepLab V3+ (DeepLab Version 3 Plus, DeepLab V3+)\\n*   Score CAM (Score-weighted Class Activation Mapping, Score CAM)\"\n}\n```"
}