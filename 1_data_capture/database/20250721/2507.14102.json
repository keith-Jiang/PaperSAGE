{
    "source": "ArXiv (Semantic Scholar未收录)",
    "arxiv_id": "2507.14102",
    "link": "https://arxiv.org/abs/2507.14102",
    "pdf_link": "https://arxiv.org/pdf/2507.14102.pdf",
    "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography",
    "authors": [
        "Shravan Venkatraman",
        "Pavan Kumar S",
        "Rakesh Raj Madavan",
        "Chandrakala S"
    ],
    "categories": [
        "cs.CV",
        "cs.LG"
    ],
    "publication_date": "未找到提交日期",
    "venue": "暂未录入Semantic Scholar",
    "fields_of_study": "暂未录入Semantic Scholar",
    "citation_count": "暂未录入Semantic Scholar",
    "influential_citation_count": "暂未录入Semantic Scholar",
    "institutions": [
        "Vellore Institute of Technology",
        "Shiv Nadar University"
    ],
    "paper_content": "# UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography\n\nShravan Venkatraman1\\* Pavan Kumar $S ^ { 1 * }$ Rakesh Raj Madavan2\\* Chandrakala S2 1Vellore Institute of Technology, Chennai, India 2Shiv Nadar University, Chennai, India\n\n# Abstract\n\nAccurate classification of computed tomography (CT) images is essential for diagnosis and treatment planning, but existing methods often struggle with the subtle and spatially diverse nature of pathological features. Current approaches typically process images uniformly, limiting their ability to detect localized abnormalities that require focused analysis. We introduce UGPL, an uncertainty-guided progressive learning framework that performs a global-tolocal analysis by first identifying regions of diagnostic ambiguity and then conducting detailed examination of these critical areas. Our approach employs evidential deep learning to quantify predictive uncertainty, guiding the extraction of informative patches through a non-maximum suppression mechanism that maintains spatial diversity. This progressive refinement strategy, combined with an adaptive fusion mechanism, enables UGPL to integrate both contextual information and fine-grained details. Experiments across three CT datasets demonstrate that UGPL consistently outperforms state-of-the-art methods, achieving improvements of $3 . 2 9 \\%$ , $2 . 4 6 \\%$ , and $8 . 0 8 \\%$ in accuracy for kidney abnormality, lung cancer, and COVID-19 detection, respectively. Our analysis shows that the uncertaintyguided component provides substantial benefits, with performance dramatically increasing when the full progressive learning pipeline is implemented. Our code is available at https://github.com/shravan-18/UGPL.\n\n# 1. Introduction\n\nMedical image classification plays a central role in automated diagnosis and clinical decision support [11]. Deep learning and convolutional neural networks (CNNs) have shown effectiveness across various imaging modalities, including X-rays [21, 75], magnetic resonance imaging [47, 67], and computed tomography (CT) scans [17, 63]. Par(c) Uncertainty-Guided Progressive Learning (UGPL): Focuses computational resources on uncertain regions for enhanced classification.\n\n![](images/db4f9720fe534b6ee8bd7c5a6735f7ec0b5d8e12ee482efb916e335cbe0ed6f0.jpg)  \n(a) Standard CNN: Single-pass analysis with uniform processing.\n\n![](images/bc3b5d4b378c07ef0472d2e426ba4f0b5b4daa0168d660330eb27a29127405d3.jpg)  \n(b) Bayesian CNN: Produces uncertainty maps but without focused refinement.\n\n![](images/67ec258e2956441170b97a0058caf61f9261e6bb7278e68294dff91936994522.jpg)  \nFigure 1. Comparison of medical image classification methods: unlike standard CNNs (1a) and Bayesian CNNs (1b) that process images uniformly, our proposed UGPL framework (1c) adaptively focuses on high-uncertainty regions for refined local analysis, combining global and local predictions via adaptive fusion.\n\nticularly in CT image analysis, these approaches have achieved promising results for diagnosing pulmonary diseases [35, 49], abdominal abnormalities [5, 48], and COVID-19 infections [7, 61]. While these approaches achieve strong performance on benchmark datasets, they operate uniformly across all spatial regions, overlooking how radiologists selectively attend to diagnostically relevant areas. This limitation affects performance in cases where critical findings are localized and subtle.\n\nConventional CNNs apply identical convolutions across the image, ignoring regional diagnostic value. This is a limiting factor in medical imaging, where abnormalities may occupy a small fraction of the image. For instance, lung nodules or renal cysts often appear in confined regions that can be missed under uniform processing. Increasing model capacity or resolution globally is a possible workaround, but it incurs significant computational costs and delays in inference, both critical concerns in clinical practice. Moreover, such methods do not adapt their assessment based on uncertainty, unlike real practice, where analysis is refined based on perceived ambiguity. This gap in spatial adaptivity limits current models from capturing diagnostically important features in complex cases.\n\n![](images/16c117d3eb5812d22f0f237727e623d52b220b7b8198392cc03a8b79fb0d93c1.jpg)  \nFigure 2. The UGPL architecture pipeline. Our framework processes an input CT image through a global uncertainty estimator to produce classification probabilities and an uncertainty map (left). The progressive patch extractor selects high-uncertainty regions for detailed analysis (center). These patches are processed by a local refinement network and combined with global predictions through an adaptive fusion module (right). Multiple loss functions (CE, UCC, CL, PDL, REG) are jointly optimized to ensure effective training of all components.\n\nSeveral approaches have attempted to address aspects of this problem. Attention mechanisms [26, 66, 69] and Region-based CNNs [39, 40, 50] enable models to focus on specific image regions, but they typically identify regions based on learned patterns rather than diagnostic uncertainty. Bayesian neural networks [6, 16, 58] and Monte Carlo dropout techniques [9, 24, 71] offer uncertainty quantification in medical image analysis, producing pixel-wise uncertainty maps that highlight ambiguous regions. However, these methods primarily use uncertainty for confidence estimation or out-of-distribution detection, but not as analysis feedback. Progressive approaches in computer vision [14, 52, 64] process images in multiple stages of increasing resolution, but follow predetermined schedules rather than adapting based on detected uncertainty. While these methods offer partial solutions, they fail to integrate such uncertainty estimations with subsequent analysis refinement as done in practice.\n\nIn this paper, we introduce Uncertainty-Guided Progressive Learning (UGPL), a novel framework that mimics diagnostic behavior by performing global analysis followed by focused examination of uncertain regions (Figure 1). UGPL addresses limitations of uniform processing by dynamically allocating computational resources where needed. Our framework first employs a global uncertainty estimator to perform initial classification and generate pixel-wise uncertainty maps, then selects high-uncertainty regions for detailed analysis through a local refinement network. These multi-resolution analyses are combined via an adaptive fusion module that weights predictions based on confidence. Unlike existing methods that treat uncertainty merely as an output signal, UGPL explicitly uses it to guide computational focus, maintaining efficiency while improving performance on diagnostically challenging regions.\n\nAs shown in Figure 2, UGPL processes the input CT image to produce both classification probabilities and an uncertainty map that guides the extraction of high-uncertainty patches using non-maximum suppression. Each patch undergoes high-resolution analysis through a local refinement network, producing patch-specific classification scores and confidence estimates. The adaptive fusion module then integrates global and local predictions using learned weights based on their estimated reliability. Multiple specialized loss functions are jointly optimized, guiding components to work in tandem, adapt according to diagnostic difficulty, and improve performance over uniform processing.\n\nTo summarize, our main contributions are:\n\n• a novel uncertainty-guided progressive learning framework that dynamically allocates computational resources to regions of high diagnostic ambiguity, • an evidential deep learning (EDL) approach that provides principled uncertainty quantification through Dirichlet distributions, • an adaptive patch extraction mechanism with non\n\nmaximum suppression that selects diverse, nonoverlapping regions for detailed analysis, and • a multi-component loss formulation that jointly optimizes classification accuracy, uncertainty calibration, and spatial diversity.\n\n# 2. Related Works\n\nEvidential Deep Learning in Medical Imaging. EDL [54] has been applied to various medical imaging tasks to model uncertainty and improve reliability. Early work integrated Dempster-Shafer Theory [15] into encoder-decoder architectures for 3D lymphoma segmentation, using voxel-level belief functions to improve accuracy and calibration over standard UNets [33]. In radiotherapy dose prediction, EDL showed that epistemic uncertainty correlates with prediction error, enabling confidence interval estimation for dose-volume histograms [59]. Extensions include regionbased EDL with Dirichlet modeling for brain tumor delineation [41] and multi-view fusion architectures combining foundation models with uncertainty-aware layers to handle boundary ambiguity [31].\n\nMethods like EVIL [12, 13] introduced efficient semisupervised segmentation via uncertainty-guided consistency training, filtering unreliable pseudo-labels, and achieved strong results on ACDC and MM-WHS datasets. Multimodal and semi-supervised EDL variants further enhanced reliability. Dual-level evidential networks [56] and contextual discounting strategies [34, 73] model modality trust in PET-CT and MRI fusion, improving voxel-level interpretability in tumor segmentation. Tri-branch frameworks like ETC-Net integrate evidential guidance with co-training to stabilize pseudo-labels in low-annotation regimes [72]. Beyond segmentation, EDL has been applied to classification, including three-way decision-making with EviDCNN [70] and out-of-distribution detection using evidential reconcile blocks [23], demonstrating its versatility in uncertainty-aware diagnostics.\n\nUncertainty Quantification in Medical Image Analysis. Uncertainty quantification is widely used in medical image analysis to enhance model reliability amid noisy inputs, ambiguous boundaries, and limited annotations. Multidecoder U-Net architectures capture inter-expert variability and generate uncertainty-calibrated segmentations [68], while probabilistic U-Nets model aleatoric and epistemic uncertainties from annotation variability [30]. For classification, Bayesian deep learning models [6] like UAConvNet and BARF use Monte Carlo dropout [24] to estimate predictive uncertainty, achieving strong COVID-19 detection from chest X-rays [1, 28]. Transfer learning quantifies epistemic uncertainty across modalities, detecting shifts between CT and X-rays [55]. Uncertainty-aware attention in hierarchical fusion networks, as in Hercules, improves performance across OCT, lung CT, and chest X\n\nrays [2].\n\nIn reconstruction, Bayesian deep unrolling jointly models image formation and uncertainty for MRI and CT [20]. Multimodal regression models like MoNIG estimate modality-specific uncertainties via Normal-Inverse Gamma mixtures for adaptive trust calibration [45]. For high-risk tasks such as COVID-19 classification, RCoNet combines mutual information maximization with ensemble dropout for robustness under distributional noise [18]. Distance-based out-of-distribution detection helps identify unreliable lung lesion segmentations [27]. Joint prediction confidence estimation aids data filtering and performance improvements in chest radiograph interpretation and ultrasound view classification [25], underscoring the role of uncertainty quantification in reliable medical AI.\n\n# 3. Method\n\n# 3.1. Background\n\nMedical image classification requires both global contextual understanding and detailed examination of localized abnormalities. In this section, we establish the mathematical foundations for our uncertainty-guided approach.\n\nEvidential Deep Learning. Traditional deep learning classifiers output class probabilities $p ( \\boldsymbol { y } | \\mathbf { x } )$ directly but lack principled uncertainty quantification. Evidential Deep Learning (EDL) [54] addresses this by modeling a distribution over probabilities. For a classification problem with $C$ classes, EDL parameterizes a Dirichlet distribution $\\operatorname { D i r } ( \\mathbf { p } | \\alpha )$ over the probability simplex, where $\\alpha \\ =$ $[ \\alpha _ { 1 } , \\alpha _ { 2 } , . . . , \\alpha _ { C } ]$ are concentration parameters:\n\n$$\n\\mathrm { D i r } ( \\mathbf { p } | \\mathbf { \\alpha } ) = \\frac { 1 } { B ( \\alpha ) } \\prod _ { i = 1 } ^ { C } p _ { i } ^ { \\alpha _ { i } - 1 }\n$$\n\nHere, $B ( \\alpha )$ is the multivariate beta function. The concentration parameters $\\alpha$ can be interpreted as evidence for each class, with $\\alpha _ { i } = e _ { i } + 1$ where $e _ { i } \\geq 0$ represents the evidence for class $i$ . The expected probability for class $i$ is given by $\\begin{array} { r } { \\mathbb { E } [ p _ { i } ] = \\frac { \\alpha _ { i } } { S } } \\end{array}$ , where $\\textstyle S = \\sum _ { i = 1 } ^ { C } \\alpha _ { i }$ is the Dirichlet strength.\n\nEDL enables the quantification of two types of uncertainty: aleatoric uncertainty (data uncertainty) and epistemic uncertainty (model uncertainty). For a Dirichlet distribution, the total predictive uncertainty can be computed as:\n\n$$\n\\mathcal { U } _ { \\mathrm { { t o t a l } } } = \\sum _ { i = 1 } ^ { C } \\frac { \\alpha _ { i } } { S } \\left( 1 - \\frac { \\alpha _ { i } } { S } \\right) \\frac { 1 } { S + 1 }\n$$\n\nThis captures both the entropy of the expected categorical distribution (first term) and the additional uncertainty from the Dirichlet distribution itself (second term).\n\nD Patch Encoder (b × k,256,1,1) Classification Head T D 3 G → → U → G n ： S A Confidence Confidences P Estimation Head G Pn AAP Adaptive Average Sigmoid G I → Pooling\n\n# 3.2. Global Uncertainty Estimation and Evidential Learning\n\nOur global uncertainty estimator produces initial class predictions and generates a spatial uncertainty map to guide patch selection through evidential learning.\n\nGlobal Model Architecture. Given an input CT image $\\mathbf { I } \\in \\mathbb { R } ^ { H \\times W \\times 1 }$ , we employ a ResNet backbone [29] $\\mathcal { F } _ { \\theta }$ to extract feature maps $\\mathbf { F } \\in \\mathbb { R } ^ { h \\times w \\times d }$ . To accommodate grayscale CT images, we modify the first convolutional layer to accept single-channel inputs while preserving pretrained weights by averaging across RGB channels. The feature maps are processed by two parallel heads: a classification head $\\mathcal { C } _ { \\phi }$ and an evidence head $\\mathcal { E } _ { \\psi }$ . The classification head applies global average pooling followed by a fully connected layer to produce class logits: ${ \\bf z } _ { g } = \\mathcal { C } _ { \\phi } ( { \\bf F } )$ .\n\nEvidential Uncertainty Estimation. The evidence head $\\mathcal { E } _ { \\psi }$ generates pixel-wise Dirichlet concentration parameters that quantify uncertainty at each spatial location: $\\textbf { E } =$ $\\mathcal { E } _ { \\psi } ( \\mathbf { F } ) \\ \\in \\ \\mathbb { R } ^ { h \\times w \\times 4 C }$ , where $\\mathbf { E }$ encodes four parameters $( \\alpha , \\beta , \\gamma , \\nu )$ for each class at each location. Following subjective logic principles [37], we parameterize the Dirichlet distribution as:\n\n$$\n\\alpha _ { i , j , c } = \\beta _ { i , j , c } \\cdot \\nu _ { i , j , c } + 1\n$$\n\nwhere $\\beta _ { i , j , c }$ represents the inverse of uncertainty, $\\nu _ { i , j , c }$ represents the mass belief, and we constrain $\\textstyle \\sum _ { c = 1 } ^ { C } \\nu _ { i , j , c } =$ 1. From these parameters, we compute the pixel-wise uncertainty map $\\mathbf { U } \\in \\mathbb { R } ^ { h \\times w }$ by aggregating uncertainty across all classes:\n\n$$\n\\mathbf { U } _ { i , j } = \\frac { 1 } { C } \\sum _ { c = 1 } ^ { C } \\left( \\frac { 1 } { \\alpha _ { i , j , c } } + \\frac { \\beta _ { i , j , c } } { \\alpha _ { i , j , c } ( \\alpha _ { i , j , c } + 1 ) } \\right)\n$$\n\nThe first term accounts for aleatoric uncertainty, and the second for epistemic uncertainty. We normalize the uncertainty map to [0, 1] for easier interpretation and subsequent processing.\n\n# 3.3. Uncertainty-Guided Patch Selection and Local Refinement\n\nProgressive Patch Extraction. Given an input image $\\mathbf { I } \\in \\overline { { \\mathbb { R } } } ^ { H \\times W \\times 1 }$ and its corresponding uncertainty map $\\hat { \\mathbf { U } } \\in$ $\\mathbb { R } ^ { h \\times w }$ , we first upsample the uncertainty map to match the input resolution: $\\mathbf { U } ^ { \\prime } \\dot { = } \\mathcal { U } ( \\hat { \\mathbf { U } } , ( H , W ) )$ . Our objective is to extract $K$ patches of size $P \\times P$ from the input image based on an uncertainty-guided selection process. We formulate this as a greedy algorithm that selects patches from the original image at locations corresponding to maxima in the uncertainty map $\\mathbf { U } ^ { \\prime }$ . The first patch is centered at the global maximum uncertainty:\n\n$$\n( x _ { 1 } , y _ { 1 } ) = \\arg \\operatorname* { m a x } _ { ( x , y ) } { \\bf U } _ { x : x + P , y : y + P } ^ { \\prime }\n$$\n\nFor subsequent patches, we introduce a spatial penalty term that encourages diversity by maintaining minimum distance from previously selected locations:\n\n$$\n\\begin{array} { c } { { \\displaystyle ( x _ { k } , y _ { k } ) = \\arg \\operatorname* { m a x } _ { ( x , y ) } [ \\mathbf { U } _ { x : x + P , y : y + P } ^ { \\prime }  } } \\\\ { { \\displaystyle  -  \\lambda \\cdot \\operatorname* { m i n } _ { i < k } d ( ( x , y ) , ( x _ { i } , y _ { i } ) ) ] } } \\end{array}\n$$\n\nThis sequential optimization ensures that each new patch maximizes uncertainty while preventing redundant selection of nearby regions. We implement this efficiently using a non-maximum suppression approach, applying a Gaussian suppression kernel after selecting each patch. Algorithm 1 details our complete patch extraction procedure, including practical considerations for edge cases.\n\nRequire: Input image $\\textbf { I } \\in \\ \\mathbb { R } ^ { H \\times W \\times 1 }$ , Uncertainty map $\\hat { \\mathbf { U } } \\in \\mathbb { R } ^ { h \\times w }$ , Patch size $P$ , Number of patches $K$   \nEnsure: Set of patches $\\{ \\mathbf { P } _ { 1 } , \\mathbf { P } _ { 2 } , \\dots , \\mathbf { P } _ { K } \\}$ , Patch coordinates $\\{ ( x _ { 1 } , y _ { 1 } ) , ( x _ { 2 } , y _ { 2 } ) , \\ldots , ( x _ { K } , y _ { K } ) \\}$   \n1: $\\mathbf { U } ^ { \\prime }  \\mathcal { U } ( \\hat { \\mathbf { U } } , ( H , W ) )$ {Upsample uncertainty map}   \n2: Initialize patch coordinates list ${ \\mathcal { C } } \\gets \\{ \\}$   \n3: $\\mathbf { M } \\gets \\mathrm { z e r o s } ( H , W )$ Mask for selected regions   \n4: for $k = 1$ to $K$ do   \n5: $\\mathbf { V }  \\mathbf { U ^ { \\prime } } \\odot ( 1 - \\mathbf { M } )$ Apply mask to uncertainty $\\operatorname* { m a p } \\}$   \n6: if $\\operatorname* { m a x } ( \\mathbf { V } ) > 0$ then   \n7: $( y _ { k } , x _ { k } ) \\gets \\arg \\operatorname* { m a x } _ { ( y , x ) } { \\bf V }$ {Find maximum uncertainty location}   \n8: else   \n9: $( y _ { k } , x _ { k } ) \\gets$ random valid location Fallback: random selection}   \n10: end if   \n11: $x _ { k } \\gets \\operatorname* { m a x } ( 0 , \\operatorname* { m i n } ( x _ { k } , W - P ) )$ {Ensure patch fits within image   \n12: $y _ { k } \\gets \\operatorname* { m a x } ( 0 , \\operatorname* { m i n } ( y _ { k } , H - P ) )$   \n13: ${ \\mathcal { C } } \\gets { \\mathcal { C } } \\cup \\{ ( x _ { k } , y _ { k } ) \\}$ {Add coordinates to list}   \n14: $\\mathbf { M } _ { y _ { k } - M : y _ { k } + P + M , x _ { k } - M : x _ { k } + P + M }  \\mathrm { ~ 1 ~ } \\{ \\mathbf { U p c } _ { k } : x _ { k } \\} .$ ate mask with margin $M \\}$   \n15: $\\mathbf { P } _ { k } \\gets \\mathbf { I } _ { y _ { k } : y _ { k } + P , x _ { k } : x _ { k } + P } \\ \\{ \\mathrm { E x t r a c t } \\ \\mathrm { p a t c l }$   \n16: if $\\mathbf { P } _ { k } { \\mathrm { ~ s i z e } } \\neq ( P , P )$ then   \n17: Pk ← Resize $( \\mathbf { P } _ { k }$ , $( P , P ) { \\mathrm { . } }$ ) {Ensure consistent size}   \n18: end if   \n19: end for   \n20: return $\\{ \\mathbf { P } _ { 1 } , \\mathbf { P } _ { 2 } , \\ldots , \\mathbf { P } _ { K } \\} , { \\mathcal { C } }$\n\nLocal Refinement Network.After extracting $K$ patches, we process each independently using a local refinement network with three components (Figure 3): a feature extractor, a classification head, and a confidence estimation head. The feature extractor $\\mathcal { L } _ { f }$ processes each patch to obtain local feature vectors: $\\mathbf { f } _ { k } \\overset { \\cdot } { = } \\mathcal { L } _ { f } ( \\mathbf { P } _ { k } ) \\in \\mathbb { R } ^ { d _ { l } }$ . The classification head maps these features to class logits: $\\mathbf { z } _ { l , k } = \\mathcal { L } _ { c } ( \\mathbf { f } _ { k } ) \\in$ $\\mathbb { R } ^ { C }$ , while the confidence estimation head produces a scalar confidence score: $c _ { k } = \\mathcal { L } _ { \\mathrm { c o n f } } ( \\mathbf { f } _ { k } ) \\in [ 0 , 1 ]$ .\n\nThe confidence score allows the model to express uncertainty about individual patch predictions and weights their contribution in the final classification. The combined local prediction is computed as a confidence-weighted average: PkK=1 cck·z+l,ϵ , where ϵ is a small constant for numerical stability. This naturally reduces the contribution of low-confidence patches, functioning as an implicit attention mechanism that focuses on the most discriminative regions.\n\n# 3.4. Adaptive Fusion and Training Objectives\n\nAdaptive Fusion Module. Given the global logits $\\mathbf { z } _ { g } \\in \\mathbb { R } ^ { C }$ and uncertainty map $\\hat { \\textbf { U } } \\in \\mathbb { R } ^ { h \\times w }$ from the global model, and local logits ${ \\bf z } _ { l } ~ \\in ~ \\mathbb { R } ^ { C }$ with patch confidence scores $\\{ c _ { 1 } , c _ { 2 } , \\dots , c _ { K } \\}$ from the local refinement network, our adaptive fusion module dynamically balances global and local predictions.\n\nWe compute a scalar global uncertainty $\\begin{array} { r l } { u _ { g } } & { { } = } \\end{array}$ $\\begin{array} { r } { \\frac { 1 } { h \\cdot w } \\sum _ { i = 1 } ^ { h } \\sum _ { j = 1 } ^ { w } \\hat { \\mathbf { U } } _ { i , j } } \\end{array}$ to quantify the overall confidence of the global model. The fusion network $\\mathcal { F } _ { \\omega }$ takes as input $\\left[ \\mathbf { z } _ { g } , u _ { g } \\right]$ and outputs a fusion weight $w _ { g } = \\mathcal { F } _ { \\omega } ( [ { \\bf z } _ { g } , u _ { g } ] )$ , implemented as a multi-layer perceptron with sigmoid activation. The fused logits are computed as ${ \\bf z } _ { f } = { \\boldsymbol { w } } _ { g } \\cdot { \\bf z } _ { g } + $ $( 1 - w _ { g } ) \\cdot { \\mathbf { z } } _ { l }$ . This adaptive weighting relies more on global features when the global model is confident, and more on local features when uncertainty is high.\n\nMulti-component Loss Function. Our training uses a comprehensive loss function combining several objectives:\n\n$$\n\\begin{array} { r l } & { { \\mathcal { L } } _ { \\mathrm { t o t a l } } = \\lambda _ { f } { \\mathcal { L } } _ { \\mathrm { f u s e d } } + \\lambda _ { g } { \\mathcal { L } } _ { \\mathrm { g l o b a l } } } \\\\ & { \\phantom { { \\mathcal { L } } } + \\lambda _ { l } { \\mathcal { L } } _ { \\mathrm { l o c a l } } + \\lambda _ { u } { \\mathcal { L } } _ { \\mathrm { u n c e r t a i n t y } } } \\\\ & { \\phantom { { \\mathcal { L } } } + \\lambda _ { c } { \\mathcal { L } } _ { \\mathrm { c o n s i s t e n c y } } + \\lambda _ { \\mathrm { c o n f } } { \\mathcal { L } } _ { \\mathrm { c o n f i d e n c e } } } \\\\ & { \\phantom { { \\mathcal { L } } } + \\lambda _ { d } { \\mathcal { L } } _ { \\mathrm { d i v e r s i t y } } } \\end{array}\n$$\n\nClassification Losses. We apply cross-entropy loss to predictions from each component: $\\mathcal { L } _ { \\mathrm { f u s e d } }$ for the fused predictions, $\\mathcal { L } _ { \\mathrm { g l o b a l } }$ for global predictions, and $\\mathcal { L } _ { \\mathrm { l o c a l } }$ averaged across all patch predictions.\n\nAuxiliary Losses. We also use several auxiliary components to ensure effective training: (1) Luncertainty calibrates the uncertainty map to reflect prediction errors; (2) $\\mathcal { L } _ { \\mathfrak { c } }$ onsistency promotes agreement between global and local predictions using KL divergence weighted by patch confidence; (3) $\\mathcal { L } _ { \\mathrm { c o n f i d e n c e } }$ aligns patch confidence scores with prediction accuracy; and (4) $\\mathcal { L }$ diversity encourages diversity among patch predictions through cosine similarity penalization.\n\n# 4. Experiments\n\n# 4.1. Experimental Setup\n\nDatasets. We conduct experiments on three CT image datasets: the kidney disease diagnosis dataset [36] (multiclass: normal, cyst, tumor, stone), the IQ-OTH/NCCD lung cancer dataset [3, 4, 22] (multiclass: benign, malignant, normal), and the UCSD-AI4H COVID-CT dataset [74] (binary: COVID, non-COVID). All images are resized to $2 5 6 \\times 2 5 6$ resolution during preprocessing and normalized using the respective dataset’s mean and standard deviation.\n\nImplementation Details. All models were trained for 100 epochs using Adam optimizer [38] with learning rate $1 \\times 1 0 ^ { - 4 }$ , weight decay $1 \\times 1 0 ^ { - 4 }$ , batch size 96, and cosine decay scheduling [44]. Standard augmentations included flips, rotations, affine transformations, and contrast adjustments. Dataset-specific ResNet [29] backbones were used with varying patch configurations. The multi-component loss function employed weighted components for fused (1.0), global/local (0.5), uncertainty (0.3), consistency (0.2), and confidence/diversity losses (0.1).\n\nTable 1. Comparison of our UGPL approach with state-of-the-art classification models across three CT datasets. Results on the COVID dataset for CRNet [74] are as reported in the paper. Best results are in red, second-best in blue, and third-best in green.   \n\n<html><body><table><tr><td rowspan=\"2\">Models</td><td colspan=\"2\">Kidney Abnormalities</td><td colspan=\"2\">Lung Cancer Type</td><td colspan=\"2\">COVID Presence</td></tr><tr><td>Accuracy</td><td>F1</td><td>Accuracy</td><td>F1</td><td>Accuracy</td><td>F1</td></tr><tr><td>ShuffleNetV2[46]</td><td>0.96 ±0.0085</td><td>0.95 ± 0.0092</td><td>0.94 ±0.0127</td><td>0.91 ± 0.0143</td><td>0.69±0.0234</td><td>0.67 ± 0.0251</td></tr><tr><td>VGG16 [57]</td><td>0.89 ± 0.0156</td><td>0.88 ± 0.0173</td><td>0.95 ± 0.0098</td><td>0.91 ± 0.0165</td><td>0.48 ± 0.0287</td><td>0.47 ±0.0306</td></tr><tr><td>ConvNeXt [43]</td><td>0.81 ±0.0189</td><td>0.80 ± 0.0195</td><td>0.95 ± 0.0076</td><td>0.95 ± 0.0084</td><td>0.61±0.0267</td><td>0.59 ±0.0278</td></tr><tr><td>DenseNet121[32]</td><td>0.94 ±0.0102</td><td>0.93 ± 0.0118</td><td>0.90 ±0.0171</td><td>0.89 ± 0.0176</td><td>0.78 ± 0.0198</td><td>0.76 ± 0.0213</td></tr><tr><td>DenseNet201 [32]</td><td>0.95 ±0.0093</td><td>0.94 ± 0.0106</td><td>0.84±0.0203</td><td>0.83 ± 0.0218</td><td>0.76 ± 0.0206</td><td>0.74± 0.0229</td></tr><tr><td>EfficientNetB0 [60]</td><td>0.95 ±0.0078</td><td>0.94±0.0089</td><td>0.95 ±0.0081</td><td>0.95 ± 0.0073</td><td>0.73 ±0.0221</td><td>0.71±0.0238</td></tr><tr><td>MobileNetV2 [53]</td><td>0.87 ± 0.0179</td><td>0.85 ± 0.0195</td><td>0.70±0.0267</td><td>0.69 ±0.0283</td><td>0.70 ±0.0241</td><td>0.68 ±0.0256</td></tr><tr><td>ViT[19]</td><td>0.94± 0.0154</td><td>0.92 ± 0.0167</td><td>0.51±0.0389</td><td>0.22 ± 0.0456</td><td>0.56 ± 0.0312</td><td>0.55 ±0.0318</td></tr><tr><td>Swin [42]</td><td>0.68 ±0.0298</td><td>0.40 ± 0.0421</td><td>0.60 ±0.0334</td><td>0.41 ± 0.0398</td><td>0.53 ± 0.0331</td><td>0.53 ±0.0329</td></tr><tr><td>DeiT[62]</td><td>0.92 ± 0.0162</td><td>0.90 ± 0.0178</td><td>0.66 ±0.0312</td><td>0.46 ± 0.0387</td><td>0.44 ±0.0356</td><td>0.35 ±0.0412</td></tr><tr><td>CoaT[65]</td><td>0.98 ± 0.0067</td><td>0.98 ± 0.0072</td><td>0.95 ±0.0089</td><td>0.93 ± 0.0112</td><td>0.68 ± 0.0254</td><td>0.66 ±0.0267</td></tr><tr><td>Cross ViT[10]</td><td>0.97 ± 0.0087</td><td>0.97 ± 0.0094</td><td>0.58±0.0356</td><td>0.39±0.0423</td><td>0.62 ±0.0289</td><td>0.48 ±0.0378</td></tr><tr><td>CRNet [74]</td><td></td><td></td><td></td><td></td><td>0.73 ± 0.0218</td><td>0.76 ± 0.0203</td></tr><tr><td>UGPL (Ours)</td><td>0.99 ± 0.0023</td><td>0.99 ± 0.0031</td><td>0.98 ± 0.0047</td><td>0.97 ± 0.0052</td><td>0.81 ± 0.0134</td><td>0.79 ± 0.0147</td></tr></table></body></html>\n\nTable 2. Analysis of individual component performance in our UGPL framework across the three datasets. The shaded row corresponds to our baseline configuration.   \n\n<html><body><table><tr><td rowspan=\"2\">Model Type</td><td colspan=\"2\">COVID Presence</td><td colspan=\"2\">Lung Cancer Type</td></tr><tr><td>Accuracy F1</td><td>Accuracy F1</td><td>KidneyAbnormalities Accuracy</td><td>F1</td></tr><tr><td>Global Model</td><td>0.7108 0.7078</td><td>0.9617</td><td>0.9611</td><td>0.9811 0.9746</td></tr><tr><td>Local Model</td><td>0.6486 0.6343</td><td>0.5122</td><td>0.2258 0.4057</td><td>0.1443</td></tr><tr><td>Fused Model</td><td>0.8108 0.7903</td><td>0.9817</td><td>0.9764 0.9971</td><td>0.9946</td></tr></table></body></html>\n\n![](images/185c891f32f6c7d5eb624306c7de12f22218601571b6da896aad44001445e7d9.jpg)  \nFigure 4. Performance trends of model components across datasets. Accuracy $\\mathbf { \\bar { x } }$ -axis) and F1 score (y-axis) define trajectories from LM to GM to FM, with contour lines indicating performance density.\n\n# 4.2. Performance Evaluation\n\nTable 1 shows the performance of our method against a range of CNN and transformer-based models. These include lightweight CNNs (MobileNetV2 [53], ShuffleNetV2 [46]), standard convolutional baselines (VGG16 [57], DenseNet121/201 [32], EfficientNetB0 [60], ConvNeXt [43]), and recent transformer-based architectures (ViT [19], Swin [42], DeiT [62], CoaT [65], CrossViT [10]). We compare our UGPL approach across three CT classification tasks. For all models, we report accuracy, macro-averaged F1 score, and include ROC-AUC visualizations for further analysis.\n\nOn the kidney abnormality dataset [36], UGPL achieves the highest accuracy and F1-score at $9 9 \\%$ $( \\pm 0 . 0 0 2 3 \\$ , $\\pm 0 . 0 0 3 1 \\rangle$ . Among CNNs, CoaT [65], CrossViT [10], and EfficientNetB0 [60] follow with F1 between $9 4 \\mathrm { - } 9 8 \\%$ , all with low variance. MobileNetV2 [53] and VGG16 [57] fall below $89 \\%$ . Transformer models like ViT [19] and Swin [42] show lower F1 and higher deviations, with Swin dropping to $40 \\%$ F1 $( \\pm 0 . 0 4 2 1 )$ .\n\nOn the IQ-OTH/NCCD dataset [3, 4, 22], UGPL reports $9 7 \\%$ F1 $( \\pm 0 . 0 0 5 2 )$ , the highest overall. CNNs such as EfficientNetB0 [60] and ConvNeXt [43] reach $9 5 \\%$ F1 with low variance. CoaT [65] and VGG16 [57] follow closely, while transformer models like DeiT [62] and Swin [42] perform poorly, with F1 below $50 \\%$ and higher spread. Variance is generally higher for transformers, with less consistent learning across folds.\n\nFor COVID classification, UGPL leads with $7 9 \\%$ F1 $( \\pm 0 . 0 1 4 7 )$ , followed by DenseNet121 [32] and CRNet [74] at $76 \\%$ . EfficientNetB0 [60] and DenseNet201 [32] also perform in the $71 \\mathrm { - } 7 4 \\%$ range. Most transformer-based models, including ViT [19], Swin [42], and DeiT [62], remain under $60 \\%$ F1 with variances exceeding $\\pm 0 . 0 3$ . These models also show less consistency across folds, with notably higher performance fluctuations.\n\n![](images/870963e7cdfe08b2f4fda071871a45ad1d9a3f3ba239ffb69bb42539ba494984.jpg)  \nFigure 5. ROC curves comparing global and fused model performance across datasets. The FM consistently maintains or improves the already high AUC values of the GM across all classes and datasets.\n\nTable 3. Ablation study of different model component configurations across the three datasets. The shaded row corresponds to our baseline configuration.   \n\n<html><body><table><tr><td rowspan=\"2\">Configuration</td><td colspan=\"2\">COVID Presence</td><td colspan=\"2\">Lung Cancer Type</td><td colspan=\"2\">KidneyAbnormalities</td></tr><tr><td>Accuracy</td><td>F1</td><td>Accuracy</td><td>F1</td><td>Accuracy</td><td>F1</td></tr><tr><td>Global-only</td><td>0.2535</td><td>0.1495</td><td>0.5000</td><td>0.3890</td><td>0.5676</td><td>0.5545</td></tr><tr><td>No UG</td><td>0.2363</td><td>0.1536</td><td>0.4634</td><td>0.3764</td><td>0.5766</td><td>0.5558</td></tr><tr><td>Fixed Patches</td><td>0.2347</td><td>0.1533</td><td>0.4573</td><td>0.3731</td><td>0.5766</td><td>0.5697</td></tr><tr><td>Full Model</td><td>0.8108</td><td>0.7903</td><td>0.9817</td><td>0.9764</td><td>0.9971</td><td>0.9945</td></tr></table></body></html>\n\n# 4.3. Component Analysis\n\nTable 2 shows the contribution of each component in our UGPL framework. The global model (GM), performing whole-image classification, achieves strong performance on the Kidney and Lung datasets $9 8 . 1 1 \\%$ and $9 6 . 1 7 \\%$ accuracy). The local model (LM), using only patch-based classification, shows significantly lower performance when used independently. The fused model (FM), integrating both predictions through our adaptive fusion mechanism, consistently outperforms individual components.\n\nThe performance gap between GM and FM is most evident in COVID-19 detection, with FM reaching $8 1 . 0 8 \\%$ accuracy compared to $7 1 . 0 8 \\%$ for GM. This reflects the benefit of incorporating localized analysis in tasks where global patterns are less prominent. For kidney abnormality detection, FM also improves over GM $9 9 . 7 1 \\%$ vs. $9 8 . 1 1 \\%$ ), showing that local refinement can still enhance outcomes even when global features are already effective.\n\nThe LM performs poorly across all tasks, particularly for kidney abnormalities $( 4 0 . 5 7 \\% )$ and lung cancer classification $( 5 1 . 2 2 \\% )$ , as local patches alone lack sufficient context and focus on irrelevant regions without global guidance. Figure 4 shows performance trends across tasks, with COVID-19 detection showing the most significant gains from LM to FM. ROC curves (Figure 5) show that for COVID-19, GM and FM achieve similar AUC scores (0.901 vs. 0.900). For lung cancer, FM achieves slight improvements across classes, especially for benign cases (0.991 vs. 0.992). For kidney cases, FM improves performance for most classes, including kidney stones (0.984 vs. 0.986).\n\n# 4.4. Ablation Study\n\nWe conduct extensive ablation studies to evaluate the impact of different components in our UGPL framework. We focus on three key aspects: 1) the contribution of each component in the progressive learning pipeline, 2) the influence of patch extraction parameters on model performance, and 3) the effect of varying loss term coefficients in our multi-component optimization objective. We retain the best-performing ResNet variant [29] from our initial evaluations for all experiments.\n\n# 4.4.1. Component Ablation\n\nTo analyze the contribution of each component in our progressive learning framework, we compare four configurations: (1) a global-only setup that uses the global uncertainty estimator without local refinement; (2) a no uncertainty guidance (No UG) variant, where patches are selected randomly instead of using uncertainty maps; (3) a fixed patches configuration that uses predefined patch locations rather than adaptive selection; and (4) the full model, which includes all components of the UGPL framework.\n\nTable 3 shows our full model consistently outperforming all reduced variants by substantial F1 margins. On the COVID dataset, all ablations cause dramatic performance drops, with the global-only variant achieving only $1 4 . 9 5 \\%$\n\nTable 4. Performance comparison of different loss weight configurations across datasets. Loss component weights: Fused $( \\lambda _ { f } )$ , Global $( \\lambda _ { g } )$ , Local $( \\lambda _ { l } )$ , Uncertainty $( \\lambda _ { u } )$ , Consistency $( \\lambda _ { c } )$ , Confidence $( \\lambda _ { \\mathrm { c o } } )$ , Diversity $( \\lambda _ { d } )$ . Configuration C1 represents our baseline model with balanced weights. Best results are in red, second-best in blue, and third-best in green.   \n\n<html><body><table><tr><td rowspan=\"2\">Configuration</td><td colspan=\"6\">Loss Weights</td><td colspan=\"2\">COVID Presence</td><td colspan=\"2\">Lung Cancer Type</td><td colspan=\"2\">Kidney Abnormalities</td></tr><tr><td>入f</td><td>入g</td><td>入</td><td>入u</td><td>入c</td><td>入co</td><td>入d</td><td>Accuracy</td><td>Accuracy</td><td>F1</td><td>Accuracy</td><td>F1</td></tr><tr><td>C1: Baseline</td><td>1.0</td><td>0.5</td><td>0.5</td><td>0.3</td><td>0.2</td><td>0.1 0.1</td><td>0.8108</td><td>0.7903</td><td>0.9817</td><td>0.9764</td><td>0.9971</td><td>0.9945</td></tr><tr><td>C2: Local Emphasis</td><td>1.0</td><td>0.3</td><td>0.7</td><td>0.3</td><td>0.2</td><td>0.1 0.1</td><td></td><td>0.7946 0.7758</td><td>0.9695</td><td>0.9641</td><td>0.9928</td><td>0.9903</td></tr><tr><td>C3: Global-Centric</td><td>1.0</td><td>0.7</td><td>0.3</td><td>0.3</td><td>0.2</td><td>0.1 0.1</td><td>0.7568</td><td>0.7402</td><td>0.9634</td><td>0.9576</td><td>0.9876</td><td>0.9832</td></tr><tr><td>C4: Uncertainty Focus</td><td>1.0</td><td>0.5</td><td>0.5</td><td>0.6</td><td>0.2</td><td>0.1 0.1</td><td>0.8243</td><td>0.8057</td><td>0.9756</td><td>0.9687</td><td>0.9953</td><td>0.9931</td></tr><tr><td>C5: Consistency-Driven</td><td>1.0</td><td>0.5</td><td>0.5</td><td>0.3</td><td>0.5</td><td>0.1 0.1</td><td>0.7892</td><td>0.7689</td><td>0.9786</td><td>0.9723</td><td>0.9913</td><td>0.9889</td></tr><tr><td>C6:Balanced High</td><td>1.0</td><td>0.5</td><td>0.5</td><td>0.4</td><td>0.4</td><td>0.2 0.2</td><td>0.8051</td><td>0.7836</td><td>0.9801</td><td>0.9739</td><td>0.9942</td><td>0.9918</td></tr><tr><td>C7:Diversity-Enhanced</td><td>1.0</td><td>0.5</td><td>0.5</td><td>0.3</td><td>0.2</td><td>0.1 0.4</td><td>0.7784</td><td>0.7569</td><td>0.9667</td><td>0.9602</td><td>0.9895</td><td>0.9856</td></tr><tr><td>C8:Confidence-Calibrated</td><td>1.0</td><td>0.5</td><td>0.5</td><td>0.3</td><td>0.2</td><td>0.4 0.1</td><td>0.7973</td><td>0.7798</td><td>0.9753</td><td>0.9695</td><td>0.9923</td><td>0.9891</td></tr><tr><td>C9: Conservative</td><td>0.5</td><td>0.25</td><td>0.25</td><td>0.15</td><td>0.1</td><td>0.05 0.05</td><td>0.7486</td><td>0.7312</td><td>0.9581</td><td>0.9524</td><td>0.9837</td><td>0.9803</td></tr><tr><td>C10: Aggressive</td><td>2.0</td><td>1.0</td><td>1.0</td><td>0.6</td><td>0.4</td><td>0.2 0.2</td><td>0.8023</td><td>0.7827</td><td>0.9728</td><td>0.9674</td><td>0.9932</td><td>0.9907</td></tr></table></body></html>\n\nTable 5. F1 Scores across patch sizes and number of extracted patches. Bolded values indicate results from C1 configuration.   \n\n<html><body><table><tr><td>Patch Size</td><td>Patches</td><td>Kidney</td><td>Lung</td><td>COVID</td></tr><tr><td rowspan=\"3\">32</td><td>2</td><td>0.9586</td><td>0.8869</td><td>0.7161</td></tr><tr><td>3</td><td>0.9673</td><td>0.9195</td><td>0.7368</td></tr><tr><td>4</td><td>0.9541</td><td>0.8756</td><td>0.7454</td></tr><tr><td rowspan=\"3\">64</td><td>2</td><td>0.9824</td><td>0.9764</td><td>0.7521</td></tr><tr><td>3</td><td>0.9945</td><td>0.8671</td><td>0.7368</td></tr><tr><td>4</td><td>0.9765</td><td>0.9343</td><td>0.7903</td></tr><tr><td rowspan=\"3\">96</td><td>2</td><td>0.9622</td><td>0.8712</td><td>0.7372</td></tr><tr><td>3</td><td>0.9701</td><td>0.9099</td><td>0.7262</td></tr><tr><td>4</td><td>0.9418</td><td>0.8717</td><td>0.6505</td></tr></table></body></html>\n\nF1. For lung cancer detection, the full model obtains $9 7 . 6 4 \\%$ F1, while the global-only setup drops to $3 4 . 1 9 \\%$ . The kidney dataset shows smaller yet significant gaps, with the full model reaching $9 9 . 6 \\%$ F1 versus $5 8 . 7 \\%$ for the best ablated configuration (fixed patches). Interestingly, No UG and fixed patches sometimes perform worse than the globalonly model, showing that naively adding local components without proper guidance can be detrimental and highlighting the importance of uncertainty-guided patch selection.\n\n# 4.4.2. Loss Component Weights\n\nTable 4 compares ten loss weight configurations across datasets. The baseline configuration (C1) with balanced weights performs best overall (fused: 1.0, global/local: 0.5 each, uncertainty: 0.3, consistency: 0.2, confidence/diversity: 0.1 each). Configurations emphasizing either global or local branches underperform, confirming the necessity of combining global context with local detail. Increased uncertainty weighting (C4) improves COVID detection $8 2 . 4 3 \\%$ accuracy, $80 . 5 7 \\%$ F1) but slightly reduces performance on Lung and Kidney datasets where target features are more prominent. C5 (Consistency-Driven) excels on the Lung dataset $( 9 7 . 8 6 \\%$ accuracy) where structural patterns are clearer, while uniform scaling of all components (C9 & C10) shows no improvement, indicating that relative balance matters more than absolute weight values.\n\n# 4.4.3. Patch Extraction Parameters\n\nTable 5 presents F1 scores for different combinations of patch sizes and counts across datasets. Optimal configurations vary by task: kidney abnormality detection performs best with $6 4 \\times 6 4$ patches and 3 patches per image $( \\mathrm { F 1 } = 0 . 9 9 4 5 )$ ), lung cancer classification with $6 4 \\times 6 4$ and 2 patches $( \\mathrm { F 1 } = 0 . 9 7 6 4 )$ ), and COVID-19 detection with $6 4 \\times 6 4$ and 4 patches $( \\mathrm { F } 1 = 0 . 7 9 0 3 \\$ ). A patch size of 64 consistently outperforms both smaller (32) and larger (96) sizes, suggesting it provides an optimal balance between local detail and contextual information. The number of required patches aligns with each task’s complexity - COVID detection needs more regions due to diffuse disease manifestations, while lung cancer classification can focus on fewer, more localized abnormalities.\n\n# 5. Conclusion\n\nThis paper proposed UGPL (Uncertainty-Guided Progressive Learning), a framework for medical image classification that analyzes CT images in two stages: global prediction with uncertainty estimation, followed by local refinement on selected high-uncertainty regions. Our evidential learning-based uncertainty estimation identifies diagnostically challenging areas, while the adaptive fusion mechanism combines global and local predictions based on confidence measures. Extensive experiments across three diverse CT classification tasks (COVID-19 detection, lung cancer classification, and kidney abnormality identification) demonstrate that UGPL significantly outperforms existing methods. Ablations show that the uncertainty-guided patch selection yields upto $5 . 3 \\times$ F1 improvement compared to other configurations. Future work will explore extending UGPL to other modalities like MRI/PET and investigating its potential for uncertainty-guided active learning.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   现有计算机断层扫描（CT）图像分类方法在处理病理特征的细微性和空间多样性时表现不佳，通常采用均匀处理方式，难以检测需要重点分析的局部异常区域。\\n> *   该问题在医学影像诊断中至关重要，因为许多关键病理特征（如肺结节或肾囊肿）仅占据图像的极小部分，均匀处理容易导致漏诊。\\n\\n> **方法概述 (Method Overview)**\\n> *   提出UGPL（不确定性引导的渐进学习框架），通过全局到局部的分析策略，先识别诊断模糊区域，再对这些关键区域进行详细检查。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **不确定性引导的渐进学习框架**：动态分配计算资源至高诊断模糊区域，在三个CT数据集上分别提升准确率3.29%（肾异常）、2.46%（肺癌）和8.08%（COVID-19检测）。\\n> *   **证据深度学习（EDL）方法**：通过Dirichlet分布提供原则性的不确定性量化。\\n> *   **自适应补丁提取机制**：采用非极大值抑制选择多样化、非重叠区域进行详细分析。\\n> *   **多组件损失函数**：联合优化分类准确性、不确定性校准和空间多样性。\\n> *   **性能提升**：在肾脏异常数据集上达到99%（±0.0023）的准确率，显著优于基线模型CoaT（98%）；在肺癌分类任务上达到97%（±0.0052）的F1分数，优于EfficientNetB0（95%）；在COVID-19检测上达到81%（±0.0134）的准确率，优于DenseNet121（76%）。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   UGPL框架模仿放射科医生的诊断行为，通过全局分析后对不确定区域进行聚焦检查。其有效性源于动态分配计算资源至诊断关键区域，避免均匀处理的资源浪费。\\n> *   该方法有效的原因在于：1）不确定性量化能够准确识别需要进一步分析的图像区域；2）渐进式学习策略能够结合全局上下文和局部细节；3）自适应融合机制能够根据置信度动态调整全局和局部预测的权重。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比**：传统CNN采用均匀卷积处理，忽略区域诊断价值；贝叶斯CNN产生不确定性图但无聚焦细化；注意力机制基于学习模式而非诊断不确定性。\\n> *   **本文的改进**：将不确定性估计作为分析反馈，指导计算焦点，保持效率的同时提升诊断挑战区域的性能。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> 1.  全局不确定性估计：使用ResNet骨干网络提取特征，并行处理分类头和证据头，生成初始分类概率和像素级不确定性图。\\n> 2.  渐进补丁提取：基于不确定性图，通过非极大值抑制选择高不确定性区域，提取K个补丁。\\n> 3.  局部细化网络：独立处理每个补丁，生成补丁特定分类分数和置信度估计。\\n> 4.  自适应融合模块：基于估计可靠性，通过学习权重整合全局和局部预测。\\n> 5.  多组件损失优化：联合优化分类损失（交叉熵）、不确定性校准损失、一致性损失、置信度损失和多样性损失。\\n\\n> **关键数学公式**\\n> *   Dirichlet分布参数化：\\n>     $$\\\\alpha_{i,j,c} = \\\\beta_{i,j,c} \\\\cdot \\\\nu_{i,j,c} + 1$$\\n>     其中β表示不确定性倒数，ν表示质量信念。\\n> *   总预测不确定性计算：\\n>     $$\\\\mathcal{U}_{\\\\mathrm{total}} = \\\\sum_{i=1}^C \\\\frac{\\\\alpha_i}{S} \\\\left(1-\\\\frac{\\\\alpha_i}{S}\\\\right) \\\\frac{1}{S+1}$$\\n>     捕获预期分类分布的熵和Dirichlet分布本身的额外不确定性。\\n\\n> **案例解析 (Case Study)**\\n> 论文未明确提供此部分信息\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   轻量级CNN：ShuffleNetV2、MobileNetV2\\n> *   标准卷积基线：VGG16、DenseNet121/201、EfficientNetB0、ConvNeXt\\n> *   基于Transformer的架构：ViT、Swin、DeiT、CoaT、CrossViT\\n> *   专用医学影像模型：CRNet\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在准确率上**：UGPL在肾脏异常数据集上达到99%（±0.0023），显著优于表现最佳的基线CoaT（98%）和CrossViT（97%）。与最佳基线相比，提升了1个百分点。\\n> *   **在F1分数上**：UGPL在肺癌分类任务上达到97%（±0.0052），优于EfficientNetB0（95%）和ConvNeXt（95%），提升2个百分点。\\n> *   **在COVID-19检测上**：UGPL达到81%（±0.0134）的准确率和79%（±0.0147）的F1分数，优于DenseNet121（76%）和CRNet（76%），提升3个百分点。\\n> *   **在模型鲁棒性上**：UGPL在所有数据集上表现出最低的方差（±0.0023-±0.0147），而Transformer模型如Swin和DeiT在某些任务上方差高达±0.0421，表明UGPL具有更稳定的性能。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   计算机断层扫描 (Computed Tomography, CT)\\n*   不确定性引导学习 (Uncertainty-Guided Learning, UGL)\\n*   证据深度学习 (Evidential Deep Learning, EDL)\\n*   渐进学习 (Progressive Learning, N/A)\\n*   医学影像分类 (Medical Image Classification, N/A)\\n*   自适应融合 (Adaptive Fusion, N/A)\\n*   非极大值抑制 (Non-Maximum Suppression, NMS)\\n*   Dirichlet分布 (Dirichlet Distribution, N/A)\"\n}\n```"
}