{
    "source": "ArXiv (Semantic ScholarÊú™Êî∂ÂΩï)",
    "arxiv_id": "2507.14102",
    "link": "https://arxiv.org/abs/2507.14102",
    "pdf_link": "https://arxiv.org/pdf/2507.14102.pdf",
    "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography",
    "authors": [
        "Shravan Venkatraman",
        "Pavan Kumar S",
        "Rakesh Raj Madavan",
        "Chandrakala S"
    ],
    "categories": [
        "cs.CV",
        "cs.LG"
    ],
    "publication_date": "Êú™ÊâæÂà∞Êèê‰∫§Êó•Êúü",
    "venue": "ÊöÇÊú™ÂΩïÂÖ•Semantic Scholar",
    "fields_of_study": "ÊöÇÊú™ÂΩïÂÖ•Semantic Scholar",
    "citation_count": "ÊöÇÊú™ÂΩïÂÖ•Semantic Scholar",
    "influential_citation_count": "ÊöÇÊú™ÂΩïÂÖ•Semantic Scholar",
    "institutions": [
        "Vellore Institute of Technology",
        "Shiv Nadar University"
    ],
    "paper_content": "# UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography\n\nShravan Venkatraman1\\* Pavan Kumar $S ^ { 1 * }$ Rakesh Raj Madavan2\\* Chandrakala S2 1Vellore Institute of Technology, Chennai, India 2Shiv Nadar University, Chennai, India\n\n# Abstract\n\nAccurate classification of computed tomography (CT) images is essential for diagnosis and treatment planning, but existing methods often struggle with the subtle and spatially diverse nature of pathological features. Current approaches typically process images uniformly, limiting their ability to detect localized abnormalities that require focused analysis. We introduce UGPL, an uncertainty-guided progressive learning framework that performs a global-tolocal analysis by first identifying regions of diagnostic ambiguity and then conducting detailed examination of these critical areas. Our approach employs evidential deep learning to quantify predictive uncertainty, guiding the extraction of informative patches through a non-maximum suppression mechanism that maintains spatial diversity. This progressive refinement strategy, combined with an adaptive fusion mechanism, enables UGPL to integrate both contextual information and fine-grained details. Experiments across three CT datasets demonstrate that UGPL consistently outperforms state-of-the-art methods, achieving improvements of $3 . 2 9 \\%$ , $2 . 4 6 \\%$ , and $8 . 0 8 \\%$ in accuracy for kidney abnormality, lung cancer, and COVID-19 detection, respectively. Our analysis shows that the uncertaintyguided component provides substantial benefits, with performance dramatically increasing when the full progressive learning pipeline is implemented. Our code is available at https://github.com/shravan-18/UGPL.\n\n# 1. Introduction\n\nMedical image classification plays a central role in automated diagnosis and clinical decision support [11]. Deep learning and convolutional neural networks (CNNs) have shown effectiveness across various imaging modalities, including X-rays [21, 75], magnetic resonance imaging [47, 67], and computed tomography (CT) scans [17, 63]. Par(c) Uncertainty-Guided Progressive Learning (UGPL): Focuses computational resources on uncertain regions for enhanced classification.\n\n![](images/db4f9720fe534b6ee8bd7c5a6735f7ec0b5d8e12ee482efb916e335cbe0ed6f0.jpg)  \n(a) Standard CNN: Single-pass analysis with uniform processing.\n\n![](images/bc3b5d4b378c07ef0472d2e426ba4f0b5b4daa0168d660330eb27a29127405d3.jpg)  \n(b) Bayesian CNN: Produces uncertainty maps but without focused refinement.\n\n![](images/67ec258e2956441170b97a0058caf61f9261e6bb7278e68294dff91936994522.jpg)  \nFigure 1. Comparison of medical image classification methods: unlike standard CNNs (1a) and Bayesian CNNs (1b) that process images uniformly, our proposed UGPL framework (1c) adaptively focuses on high-uncertainty regions for refined local analysis, combining global and local predictions via adaptive fusion.\n\nticularly in CT image analysis, these approaches have achieved promising results for diagnosing pulmonary diseases [35, 49], abdominal abnormalities [5, 48], and COVID-19 infections [7, 61]. While these approaches achieve strong performance on benchmark datasets, they operate uniformly across all spatial regions, overlooking how radiologists selectively attend to diagnostically relevant areas. This limitation affects performance in cases where critical findings are localized and subtle.\n\nConventional CNNs apply identical convolutions across the image, ignoring regional diagnostic value. This is a limiting factor in medical imaging, where abnormalities may occupy a small fraction of the image. For instance, lung nodules or renal cysts often appear in confined regions that can be missed under uniform processing. Increasing model capacity or resolution globally is a possible workaround, but it incurs significant computational costs and delays in inference, both critical concerns in clinical practice. Moreover, such methods do not adapt their assessment based on uncertainty, unlike real practice, where analysis is refined based on perceived ambiguity. This gap in spatial adaptivity limits current models from capturing diagnostically important features in complex cases.\n\n![](images/16c117d3eb5812d22f0f237727e623d52b220b7b8198392cc03a8b79fb0d93c1.jpg)  \nFigure 2. The UGPL architecture pipeline. Our framework processes an input CT image through a global uncertainty estimator to produce classification probabilities and an uncertainty map (left). The progressive patch extractor selects high-uncertainty regions for detailed analysis (center). These patches are processed by a local refinement network and combined with global predictions through an adaptive fusion module (right). Multiple loss functions (CE, UCC, CL, PDL, REG) are jointly optimized to ensure effective training of all components.\n\nSeveral approaches have attempted to address aspects of this problem. Attention mechanisms [26, 66, 69] and Region-based CNNs [39, 40, 50] enable models to focus on specific image regions, but they typically identify regions based on learned patterns rather than diagnostic uncertainty. Bayesian neural networks [6, 16, 58] and Monte Carlo dropout techniques [9, 24, 71] offer uncertainty quantification in medical image analysis, producing pixel-wise uncertainty maps that highlight ambiguous regions. However, these methods primarily use uncertainty for confidence estimation or out-of-distribution detection, but not as analysis feedback. Progressive approaches in computer vision [14, 52, 64] process images in multiple stages of increasing resolution, but follow predetermined schedules rather than adapting based on detected uncertainty. While these methods offer partial solutions, they fail to integrate such uncertainty estimations with subsequent analysis refinement as done in practice.\n\nIn this paper, we introduce Uncertainty-Guided Progressive Learning (UGPL), a novel framework that mimics diagnostic behavior by performing global analysis followed by focused examination of uncertain regions (Figure 1). UGPL addresses limitations of uniform processing by dynamically allocating computational resources where needed. Our framework first employs a global uncertainty estimator to perform initial classification and generate pixel-wise uncertainty maps, then selects high-uncertainty regions for detailed analysis through a local refinement network. These multi-resolution analyses are combined via an adaptive fusion module that weights predictions based on confidence. Unlike existing methods that treat uncertainty merely as an output signal, UGPL explicitly uses it to guide computational focus, maintaining efficiency while improving performance on diagnostically challenging regions.\n\nAs shown in Figure 2, UGPL processes the input CT image to produce both classification probabilities and an uncertainty map that guides the extraction of high-uncertainty patches using non-maximum suppression. Each patch undergoes high-resolution analysis through a local refinement network, producing patch-specific classification scores and confidence estimates. The adaptive fusion module then integrates global and local predictions using learned weights based on their estimated reliability. Multiple specialized loss functions are jointly optimized, guiding components to work in tandem, adapt according to diagnostic difficulty, and improve performance over uniform processing.\n\nTo summarize, our main contributions are:\n\n‚Ä¢ a novel uncertainty-guided progressive learning framework that dynamically allocates computational resources to regions of high diagnostic ambiguity, ‚Ä¢ an evidential deep learning (EDL) approach that provides principled uncertainty quantification through Dirichlet distributions, ‚Ä¢ an adaptive patch extraction mechanism with non\n\nmaximum suppression that selects diverse, nonoverlapping regions for detailed analysis, and ‚Ä¢ a multi-component loss formulation that jointly optimizes classification accuracy, uncertainty calibration, and spatial diversity.\n\n# 2. Related Works\n\nEvidential Deep Learning in Medical Imaging. EDL [54] has been applied to various medical imaging tasks to model uncertainty and improve reliability. Early work integrated Dempster-Shafer Theory [15] into encoder-decoder architectures for 3D lymphoma segmentation, using voxel-level belief functions to improve accuracy and calibration over standard UNets [33]. In radiotherapy dose prediction, EDL showed that epistemic uncertainty correlates with prediction error, enabling confidence interval estimation for dose-volume histograms [59]. Extensions include regionbased EDL with Dirichlet modeling for brain tumor delineation [41] and multi-view fusion architectures combining foundation models with uncertainty-aware layers to handle boundary ambiguity [31].\n\nMethods like EVIL [12, 13] introduced efficient semisupervised segmentation via uncertainty-guided consistency training, filtering unreliable pseudo-labels, and achieved strong results on ACDC and MM-WHS datasets. Multimodal and semi-supervised EDL variants further enhanced reliability. Dual-level evidential networks [56] and contextual discounting strategies [34, 73] model modality trust in PET-CT and MRI fusion, improving voxel-level interpretability in tumor segmentation. Tri-branch frameworks like ETC-Net integrate evidential guidance with co-training to stabilize pseudo-labels in low-annotation regimes [72]. Beyond segmentation, EDL has been applied to classification, including three-way decision-making with EviDCNN [70] and out-of-distribution detection using evidential reconcile blocks [23], demonstrating its versatility in uncertainty-aware diagnostics.\n\nUncertainty Quantification in Medical Image Analysis. Uncertainty quantification is widely used in medical image analysis to enhance model reliability amid noisy inputs, ambiguous boundaries, and limited annotations. Multidecoder U-Net architectures capture inter-expert variability and generate uncertainty-calibrated segmentations [68], while probabilistic U-Nets model aleatoric and epistemic uncertainties from annotation variability [30]. For classification, Bayesian deep learning models [6] like UAConvNet and BARF use Monte Carlo dropout [24] to estimate predictive uncertainty, achieving strong COVID-19 detection from chest X-rays [1, 28]. Transfer learning quantifies epistemic uncertainty across modalities, detecting shifts between CT and X-rays [55]. Uncertainty-aware attention in hierarchical fusion networks, as in Hercules, improves performance across OCT, lung CT, and chest X\n\nrays [2].\n\nIn reconstruction, Bayesian deep unrolling jointly models image formation and uncertainty for MRI and CT [20]. Multimodal regression models like MoNIG estimate modality-specific uncertainties via Normal-Inverse Gamma mixtures for adaptive trust calibration [45]. For high-risk tasks such as COVID-19 classification, RCoNet combines mutual information maximization with ensemble dropout for robustness under distributional noise [18]. Distance-based out-of-distribution detection helps identify unreliable lung lesion segmentations [27]. Joint prediction confidence estimation aids data filtering and performance improvements in chest radiograph interpretation and ultrasound view classification [25], underscoring the role of uncertainty quantification in reliable medical AI.\n\n# 3. Method\n\n# 3.1. Background\n\nMedical image classification requires both global contextual understanding and detailed examination of localized abnormalities. In this section, we establish the mathematical foundations for our uncertainty-guided approach.\n\nEvidential Deep Learning. Traditional deep learning classifiers output class probabilities $p ( \\boldsymbol { y } | \\mathbf { x } )$ directly but lack principled uncertainty quantification. Evidential Deep Learning (EDL) [54] addresses this by modeling a distribution over probabilities. For a classification problem with $C$ classes, EDL parameterizes a Dirichlet distribution $\\operatorname { D i r } ( \\mathbf { p } | \\alpha )$ over the probability simplex, where $\\alpha \\ =$ $[ \\alpha _ { 1 } , \\alpha _ { 2 } , . . . , \\alpha _ { C } ]$ are concentration parameters:\n\n$$\n\\mathrm { D i r } ( \\mathbf { p } | \\mathbf { \\alpha } ) = \\frac { 1 } { B ( \\alpha ) } \\prod _ { i = 1 } ^ { C } p _ { i } ^ { \\alpha _ { i } - 1 }\n$$\n\nHere, $B ( \\alpha )$ is the multivariate beta function. The concentration parameters $\\alpha$ can be interpreted as evidence for each class, with $\\alpha _ { i } = e _ { i } + 1$ where $e _ { i } \\geq 0$ represents the evidence for class $i$ . The expected probability for class $i$ is given by $\\begin{array} { r } { \\mathbb { E } [ p _ { i } ] = \\frac { \\alpha _ { i } } { S } } \\end{array}$ , where $\\textstyle S = \\sum _ { i = 1 } ^ { C } \\alpha _ { i }$ is the Dirichlet strength.\n\nEDL enables the quantification of two types of uncertainty: aleatoric uncertainty (data uncertainty) and epistemic uncertainty (model uncertainty). For a Dirichlet distribution, the total predictive uncertainty can be computed as:\n\n$$\n\\mathcal { U } _ { \\mathrm { { t o t a l } } } = \\sum _ { i = 1 } ^ { C } \\frac { \\alpha _ { i } } { S } \\left( 1 - \\frac { \\alpha _ { i } } { S } \\right) \\frac { 1 } { S + 1 }\n$$\n\nThis captures both the entropy of the expected categorical distribution (first term) and the additional uncertainty from the Dirichlet distribution itself (second term).\n\nD Patch Encoder (b √ó k,256,1,1) Classification Head T D 3 G ‚Üí ‚Üí U ‚Üí G n Ôºö S A Confidence Confidences P Estimation Head G Pn AAP Adaptive Average Sigmoid G I ‚Üí Pooling\n\n# 3.2. Global Uncertainty Estimation and Evidential Learning\n\nOur global uncertainty estimator produces initial class predictions and generates a spatial uncertainty map to guide patch selection through evidential learning.\n\nGlobal Model Architecture. Given an input CT image $\\mathbf { I } \\in \\mathbb { R } ^ { H \\times W \\times 1 }$ , we employ a ResNet backbone [29] $\\mathcal { F } _ { \\theta }$ to extract feature maps $\\mathbf { F } \\in \\mathbb { R } ^ { h \\times w \\times d }$ . To accommodate grayscale CT images, we modify the first convolutional layer to accept single-channel inputs while preserving pretrained weights by averaging across RGB channels. The feature maps are processed by two parallel heads: a classification head $\\mathcal { C } _ { \\phi }$ and an evidence head $\\mathcal { E } _ { \\psi }$ . The classification head applies global average pooling followed by a fully connected layer to produce class logits: ${ \\bf z } _ { g } = \\mathcal { C } _ { \\phi } ( { \\bf F } )$ .\n\nEvidential Uncertainty Estimation. The evidence head $\\mathcal { E } _ { \\psi }$ generates pixel-wise Dirichlet concentration parameters that quantify uncertainty at each spatial location: $\\textbf { E } =$ $\\mathcal { E } _ { \\psi } ( \\mathbf { F } ) \\ \\in \\ \\mathbb { R } ^ { h \\times w \\times 4 C }$ , where $\\mathbf { E }$ encodes four parameters $( \\alpha , \\beta , \\gamma , \\nu )$ for each class at each location. Following subjective logic principles [37], we parameterize the Dirichlet distribution as:\n\n$$\n\\alpha _ { i , j , c } = \\beta _ { i , j , c } \\cdot \\nu _ { i , j , c } + 1\n$$\n\nwhere $\\beta _ { i , j , c }$ represents the inverse of uncertainty, $\\nu _ { i , j , c }$ represents the mass belief, and we constrain $\\textstyle \\sum _ { c = 1 } ^ { C } \\nu _ { i , j , c } =$ 1. From these parameters, we compute the pixel-wise uncertainty map $\\mathbf { U } \\in \\mathbb { R } ^ { h \\times w }$ by aggregating uncertainty across all classes:\n\n$$\n\\mathbf { U } _ { i , j } = \\frac { 1 } { C } \\sum _ { c = 1 } ^ { C } \\left( \\frac { 1 } { \\alpha _ { i , j , c } } + \\frac { \\beta _ { i , j , c } } { \\alpha _ { i , j , c } ( \\alpha _ { i , j , c } + 1 ) } \\right)\n$$\n\nThe first term accounts for aleatoric uncertainty, and the second for epistemic uncertainty. We normalize the uncertainty map to [0, 1] for easier interpretation and subsequent processing.\n\n# 3.3. Uncertainty-Guided Patch Selection and Local Refinement\n\nProgressive Patch Extraction. Given an input image $\\mathbf { I } \\in \\overline { { \\mathbb { R } } } ^ { H \\times W \\times 1 }$ and its corresponding uncertainty map $\\hat { \\mathbf { U } } \\in$ $\\mathbb { R } ^ { h \\times w }$ , we first upsample the uncertainty map to match the input resolution: $\\mathbf { U } ^ { \\prime } \\dot { = } \\mathcal { U } ( \\hat { \\mathbf { U } } , ( H , W ) )$ . Our objective is to extract $K$ patches of size $P \\times P$ from the input image based on an uncertainty-guided selection process. We formulate this as a greedy algorithm that selects patches from the original image at locations corresponding to maxima in the uncertainty map $\\mathbf { U } ^ { \\prime }$ . The first patch is centered at the global maximum uncertainty:\n\n$$\n( x _ { 1 } , y _ { 1 } ) = \\arg \\operatorname* { m a x } _ { ( x , y ) } { \\bf U } _ { x : x + P , y : y + P } ^ { \\prime }\n$$\n\nFor subsequent patches, we introduce a spatial penalty term that encourages diversity by maintaining minimum distance from previously selected locations:\n\n$$\n\\begin{array} { c } { { \\displaystyle ( x _ { k } , y _ { k } ) = \\arg \\operatorname* { m a x } _ { ( x , y ) } [ \\mathbf { U } _ { x : x + P , y : y + P } ^ { \\prime }  } } \\\\ { { \\displaystyle  -  \\lambda \\cdot \\operatorname* { m i n } _ { i < k } d ( ( x , y ) , ( x _ { i } , y _ { i } ) ) ] } } \\end{array}\n$$\n\nThis sequential optimization ensures that each new patch maximizes uncertainty while preventing redundant selection of nearby regions. We implement this efficiently using a non-maximum suppression approach, applying a Gaussian suppression kernel after selecting each patch. Algorithm 1 details our complete patch extraction procedure, including practical considerations for edge cases.\n\nRequire: Input image $\\textbf { I } \\in \\ \\mathbb { R } ^ { H \\times W \\times 1 }$ , Uncertainty map $\\hat { \\mathbf { U } } \\in \\mathbb { R } ^ { h \\times w }$ , Patch size $P$ , Number of patches $K$   \nEnsure: Set of patches $\\{ \\mathbf { P } _ { 1 } , \\mathbf { P } _ { 2 } , \\dots , \\mathbf { P } _ { K } \\}$ , Patch coordinates $\\{ ( x _ { 1 } , y _ { 1 } ) , ( x _ { 2 } , y _ { 2 } ) , \\ldots , ( x _ { K } , y _ { K } ) \\}$   \n1: $\\mathbf { U } ^ { \\prime }  \\mathcal { U } ( \\hat { \\mathbf { U } } , ( H , W ) )$ {Upsample uncertainty map}   \n2: Initialize patch coordinates list ${ \\mathcal { C } } \\gets \\{ \\}$   \n3: $\\mathbf { M } \\gets \\mathrm { z e r o s } ( H , W )$ Mask for selected regions   \n4: for $k = 1$ to $K$ do   \n5: $\\mathbf { V }  \\mathbf { U ^ { \\prime } } \\odot ( 1 - \\mathbf { M } )$ Apply mask to uncertainty $\\operatorname* { m a p } \\}$   \n6: if $\\operatorname* { m a x } ( \\mathbf { V } ) > 0$ then   \n7: $( y _ { k } , x _ { k } ) \\gets \\arg \\operatorname* { m a x } _ { ( y , x ) } { \\bf V }$ {Find maximum uncertainty location}   \n8: else   \n9: $( y _ { k } , x _ { k } ) \\gets$ random valid location Fallback: random selection}   \n10: end if   \n11: $x _ { k } \\gets \\operatorname* { m a x } ( 0 , \\operatorname* { m i n } ( x _ { k } , W - P ) )$ {Ensure patch fits within image   \n12: $y _ { k } \\gets \\operatorname* { m a x } ( 0 , \\operatorname* { m i n } ( y _ { k } , H - P ) )$   \n13: ${ \\mathcal { C } } \\gets { \\mathcal { C } } \\cup \\{ ( x _ { k } , y _ { k } ) \\}$ {Add coordinates to list}   \n14: $\\mathbf { M } _ { y _ { k } - M : y _ { k } + P + M , x _ { k } - M : x _ { k } + P + M }  \\mathrm { ~ 1 ~ } \\{ \\mathbf { U p c } _ { k } : x _ { k } \\} .$ ate mask with margin $M \\}$   \n15: $\\mathbf { P } _ { k } \\gets \\mathbf { I } _ { y _ { k } : y _ { k } + P , x _ { k } : x _ { k } + P } \\ \\{ \\mathrm { E x t r a c t } \\ \\mathrm { p a t c l }$   \n16: if $\\mathbf { P } _ { k } { \\mathrm { ~ s i z e } } \\neq ( P , P )$ then   \n17: Pk ‚Üê Resize $( \\mathbf { P } _ { k }$ , $( P , P ) { \\mathrm { . } }$ ) {Ensure consistent size}   \n18: end if   \n19: end for   \n20: return $\\{ \\mathbf { P } _ { 1 } , \\mathbf { P } _ { 2 } , \\ldots , \\mathbf { P } _ { K } \\} , { \\mathcal { C } }$\n\nLocal Refinement Network.After extracting $K$ patches, we process each independently using a local refinement network with three components (Figure 3): a feature extractor, a classification head, and a confidence estimation head. The feature extractor $\\mathcal { L } _ { f }$ processes each patch to obtain local feature vectors: $\\mathbf { f } _ { k } \\overset { \\cdot } { = } \\mathcal { L } _ { f } ( \\mathbf { P } _ { k } ) \\in \\mathbb { R } ^ { d _ { l } }$ . The classification head maps these features to class logits: $\\mathbf { z } _ { l , k } = \\mathcal { L } _ { c } ( \\mathbf { f } _ { k } ) \\in$ $\\mathbb { R } ^ { C }$ , while the confidence estimation head produces a scalar confidence score: $c _ { k } = \\mathcal { L } _ { \\mathrm { c o n f } } ( \\mathbf { f } _ { k } ) \\in [ 0 , 1 ]$ .\n\nThe confidence score allows the model to express uncertainty about individual patch predictions and weights their contribution in the final classification. The combined local prediction is computed as a confidence-weighted average: PkK=1 cck¬∑z+l,œµ , where œµ is a small constant for numerical stability. This naturally reduces the contribution of low-confidence patches, functioning as an implicit attention mechanism that focuses on the most discriminative regions.\n\n# 3.4. Adaptive Fusion and Training Objectives\n\nAdaptive Fusion Module. Given the global logits $\\mathbf { z } _ { g } \\in \\mathbb { R } ^ { C }$ and uncertainty map $\\hat { \\textbf { U } } \\in \\mathbb { R } ^ { h \\times w }$ from the global model, and local logits ${ \\bf z } _ { l } ~ \\in ~ \\mathbb { R } ^ { C }$ with patch confidence scores $\\{ c _ { 1 } , c _ { 2 } , \\dots , c _ { K } \\}$ from the local refinement network, our adaptive fusion module dynamically balances global and local predictions.\n\nWe compute a scalar global uncertainty $\\begin{array} { r l } { u _ { g } } & { { } = } \\end{array}$ $\\begin{array} { r } { \\frac { 1 } { h \\cdot w } \\sum _ { i = 1 } ^ { h } \\sum _ { j = 1 } ^ { w } \\hat { \\mathbf { U } } _ { i , j } } \\end{array}$ to quantify the overall confidence of the global model. The fusion network $\\mathcal { F } _ { \\omega }$ takes as input $\\left[ \\mathbf { z } _ { g } , u _ { g } \\right]$ and outputs a fusion weight $w _ { g } = \\mathcal { F } _ { \\omega } ( [ { \\bf z } _ { g } , u _ { g } ] )$ , implemented as a multi-layer perceptron with sigmoid activation. The fused logits are computed as ${ \\bf z } _ { f } = { \\boldsymbol { w } } _ { g } \\cdot { \\bf z } _ { g } + $ $( 1 - w _ { g } ) \\cdot { \\mathbf { z } } _ { l }$ . This adaptive weighting relies more on global features when the global model is confident, and more on local features when uncertainty is high.\n\nMulti-component Loss Function. Our training uses a comprehensive loss function combining several objectives:\n\n$$\n\\begin{array} { r l } & { { \\mathcal { L } } _ { \\mathrm { t o t a l } } = \\lambda _ { f } { \\mathcal { L } } _ { \\mathrm { f u s e d } } + \\lambda _ { g } { \\mathcal { L } } _ { \\mathrm { g l o b a l } } } \\\\ & { \\phantom { { \\mathcal { L } } } + \\lambda _ { l } { \\mathcal { L } } _ { \\mathrm { l o c a l } } + \\lambda _ { u } { \\mathcal { L } } _ { \\mathrm { u n c e r t a i n t y } } } \\\\ & { \\phantom { { \\mathcal { L } } } + \\lambda _ { c } { \\mathcal { L } } _ { \\mathrm { c o n s i s t e n c y } } + \\lambda _ { \\mathrm { c o n f } } { \\mathcal { L } } _ { \\mathrm { c o n f i d e n c e } } } \\\\ & { \\phantom { { \\mathcal { L } } } + \\lambda _ { d } { \\mathcal { L } } _ { \\mathrm { d i v e r s i t y } } } \\end{array}\n$$\n\nClassification Losses. We apply cross-entropy loss to predictions from each component: $\\mathcal { L } _ { \\mathrm { f u s e d } }$ for the fused predictions, $\\mathcal { L } _ { \\mathrm { g l o b a l } }$ for global predictions, and $\\mathcal { L } _ { \\mathrm { l o c a l } }$ averaged across all patch predictions.\n\nAuxiliary Losses. We also use several auxiliary components to ensure effective training: (1) Luncertainty calibrates the uncertainty map to reflect prediction errors; (2) $\\mathcal { L } _ { \\mathfrak { c } }$ onsistency promotes agreement between global and local predictions using KL divergence weighted by patch confidence; (3) $\\mathcal { L } _ { \\mathrm { c o n f i d e n c e } }$ aligns patch confidence scores with prediction accuracy; and (4) $\\mathcal { L }$ diversity encourages diversity among patch predictions through cosine similarity penalization.\n\n# 4. Experiments\n\n# 4.1. Experimental Setup\n\nDatasets. We conduct experiments on three CT image datasets: the kidney disease diagnosis dataset [36] (multiclass: normal, cyst, tumor, stone), the IQ-OTH/NCCD lung cancer dataset [3, 4, 22] (multiclass: benign, malignant, normal), and the UCSD-AI4H COVID-CT dataset [74] (binary: COVID, non-COVID). All images are resized to $2 5 6 \\times 2 5 6$ resolution during preprocessing and normalized using the respective dataset‚Äôs mean and standard deviation.\n\nImplementation Details. All models were trained for 100 epochs using Adam optimizer [38] with learning rate $1 \\times 1 0 ^ { - 4 }$ , weight decay $1 \\times 1 0 ^ { - 4 }$ , batch size 96, and cosine decay scheduling [44]. Standard augmentations included flips, rotations, affine transformations, and contrast adjustments. Dataset-specific ResNet [29] backbones were used with varying patch configurations. The multi-component loss function employed weighted components for fused (1.0), global/local (0.5), uncertainty (0.3), consistency (0.2), and confidence/diversity losses (0.1).\n\nTable 1. Comparison of our UGPL approach with state-of-the-art classification models across three CT datasets. Results on the COVID dataset for CRNet [74] are as reported in the paper. Best results are in red, second-best in blue, and third-best in green.   \n\n<html><body><table><tr><td rowspan=\"2\">Models</td><td colspan=\"2\">Kidney Abnormalities</td><td colspan=\"2\">Lung Cancer Type</td><td colspan=\"2\">COVID Presence</td></tr><tr><td>Accuracy</td><td>F1</td><td>Accuracy</td><td>F1</td><td>Accuracy</td><td>F1</td></tr><tr><td>ShuffleNetV2[46]</td><td>0.96 ¬±0.0085</td><td>0.95 ¬± 0.0092</td><td>0.94 ¬±0.0127</td><td>0.91 ¬± 0.0143</td><td>0.69¬±0.0234</td><td>0.67 ¬± 0.0251</td></tr><tr><td>VGG16 [57]</td><td>0.89 ¬± 0.0156</td><td>0.88 ¬± 0.0173</td><td>0.95 ¬± 0.0098</td><td>0.91 ¬± 0.0165</td><td>0.48 ¬± 0.0287</td><td>0.47 ¬±0.0306</td></tr><tr><td>ConvNeXt [43]</td><td>0.81 ¬±0.0189</td><td>0.80 ¬± 0.0195</td><td>0.95 ¬± 0.0076</td><td>0.95 ¬± 0.0084</td><td>0.61¬±0.0267</td><td>0.59 ¬±0.0278</td></tr><tr><td>DenseNet121[32]</td><td>0.94 ¬±0.0102</td><td>0.93 ¬± 0.0118</td><td>0.90 ¬±0.0171</td><td>0.89 ¬± 0.0176</td><td>0.78 ¬± 0.0198</td><td>0.76 ¬± 0.0213</td></tr><tr><td>DenseNet201 [32]</td><td>0.95 ¬±0.0093</td><td>0.94 ¬± 0.0106</td><td>0.84¬±0.0203</td><td>0.83 ¬± 0.0218</td><td>0.76 ¬± 0.0206</td><td>0.74¬± 0.0229</td></tr><tr><td>EfficientNetB0 [60]</td><td>0.95 ¬±0.0078</td><td>0.94¬±0.0089</td><td>0.95 ¬±0.0081</td><td>0.95 ¬± 0.0073</td><td>0.73 ¬±0.0221</td><td>0.71¬±0.0238</td></tr><tr><td>MobileNetV2 [53]</td><td>0.87 ¬± 0.0179</td><td>0.85 ¬± 0.0195</td><td>0.70¬±0.0267</td><td>0.69 ¬±0.0283</td><td>0.70 ¬±0.0241</td><td>0.68 ¬±0.0256</td></tr><tr><td>ViT[19]</td><td>0.94¬± 0.0154</td><td>0.92 ¬± 0.0167</td><td>0.51¬±0.0389</td><td>0.22 ¬± 0.0456</td><td>0.56 ¬± 0.0312</td><td>0.55 ¬±0.0318</td></tr><tr><td>Swin [42]</td><td>0.68 ¬±0.0298</td><td>0.40 ¬± 0.0421</td><td>0.60 ¬±0.0334</td><td>0.41 ¬± 0.0398</td><td>0.53 ¬± 0.0331</td><td>0.53 ¬±0.0329</td></tr><tr><td>DeiT[62]</td><td>0.92 ¬± 0.0162</td><td>0.90 ¬± 0.0178</td><td>0.66 ¬±0.0312</td><td>0.46 ¬± 0.0387</td><td>0.44 ¬±0.0356</td><td>0.35 ¬±0.0412</td></tr><tr><td>CoaT[65]</td><td>0.98 ¬± 0.0067</td><td>0.98 ¬± 0.0072</td><td>0.95 ¬±0.0089</td><td>0.93 ¬± 0.0112</td><td>0.68 ¬± 0.0254</td><td>0.66 ¬±0.0267</td></tr><tr><td>Cross ViT[10]</td><td>0.97 ¬± 0.0087</td><td>0.97 ¬± 0.0094</td><td>0.58¬±0.0356</td><td>0.39¬±0.0423</td><td>0.62 ¬±0.0289</td><td>0.48 ¬±0.0378</td></tr><tr><td>CRNet [74]</td><td></td><td></td><td></td><td></td><td>0.73 ¬± 0.0218</td><td>0.76 ¬± 0.0203</td></tr><tr><td>UGPL (Ours)</td><td>0.99 ¬± 0.0023</td><td>0.99 ¬± 0.0031</td><td>0.98 ¬± 0.0047</td><td>0.97 ¬± 0.0052</td><td>0.81 ¬± 0.0134</td><td>0.79 ¬± 0.0147</td></tr></table></body></html>\n\nTable 2. Analysis of individual component performance in our UGPL framework across the three datasets. The shaded row corresponds to our baseline configuration.   \n\n<html><body><table><tr><td rowspan=\"2\">Model Type</td><td colspan=\"2\">COVID Presence</td><td colspan=\"2\">Lung Cancer Type</td></tr><tr><td>Accuracy F1</td><td>Accuracy F1</td><td>KidneyAbnormalities Accuracy</td><td>F1</td></tr><tr><td>Global Model</td><td>0.7108 0.7078</td><td>0.9617</td><td>0.9611</td><td>0.9811 0.9746</td></tr><tr><td>Local Model</td><td>0.6486 0.6343</td><td>0.5122</td><td>0.2258 0.4057</td><td>0.1443</td></tr><tr><td>Fused Model</td><td>0.8108 0.7903</td><td>0.9817</td><td>0.9764 0.9971</td><td>0.9946</td></tr></table></body></html>\n\n![](images/185c891f32f6c7d5eb624306c7de12f22218601571b6da896aad44001445e7d9.jpg)  \nFigure 4. Performance trends of model components across datasets. Accuracy $\\mathbf { \\bar { x } }$ -axis) and F1 score (y-axis) define trajectories from LM to GM to FM, with contour lines indicating performance density.\n\n# 4.2. Performance Evaluation\n\nTable 1 shows the performance of our method against a range of CNN and transformer-based models. These include lightweight CNNs (MobileNetV2 [53], ShuffleNetV2 [46]), standard convolutional baselines (VGG16 [57], DenseNet121/201 [32], EfficientNetB0 [60], ConvNeXt [43]), and recent transformer-based architectures (ViT [19], Swin [42], DeiT [62], CoaT [65], CrossViT [10]). We compare our UGPL approach across three CT classification tasks. For all models, we report accuracy, macro-averaged F1 score, and include ROC-AUC visualizations for further analysis.\n\nOn the kidney abnormality dataset [36], UGPL achieves the highest accuracy and F1-score at $9 9 \\%$ $( \\pm 0 . 0 0 2 3 \\$ , $\\pm 0 . 0 0 3 1 \\rangle$ . Among CNNs, CoaT [65], CrossViT [10], and EfficientNetB0 [60] follow with F1 between $9 4 \\mathrm { - } 9 8 \\%$ , all with low variance. MobileNetV2 [53] and VGG16 [57] fall below $89 \\%$ . Transformer models like ViT [19] and Swin [42] show lower F1 and higher deviations, with Swin dropping to $40 \\%$ F1 $( \\pm 0 . 0 4 2 1 )$ .\n\nOn the IQ-OTH/NCCD dataset [3, 4, 22], UGPL reports $9 7 \\%$ F1 $( \\pm 0 . 0 0 5 2 )$ , the highest overall. CNNs such as EfficientNetB0 [60] and ConvNeXt [43] reach $9 5 \\%$ F1 with low variance. CoaT [65] and VGG16 [57] follow closely, while transformer models like DeiT [62] and Swin [42] perform poorly, with F1 below $50 \\%$ and higher spread. Variance is generally higher for transformers, with less consistent learning across folds.\n\nFor COVID classification, UGPL leads with $7 9 \\%$ F1 $( \\pm 0 . 0 1 4 7 )$ , followed by DenseNet121 [32] and CRNet [74] at $76 \\%$ . EfficientNetB0 [60] and DenseNet201 [32] also perform in the $71 \\mathrm { - } 7 4 \\%$ range. Most transformer-based models, including ViT [19], Swin [42], and DeiT [62], remain under $60 \\%$ F1 with variances exceeding $\\pm 0 . 0 3$ . These models also show less consistency across folds, with notably higher performance fluctuations.\n\n![](images/870963e7cdfe08b2f4fda071871a45ad1d9a3f3ba239ffb69bb42539ba494984.jpg)  \nFigure 5. ROC curves comparing global and fused model performance across datasets. The FM consistently maintains or improves the already high AUC values of the GM across all classes and datasets.\n\nTable 3. Ablation study of different model component configurations across the three datasets. The shaded row corresponds to our baseline configuration.   \n\n<html><body><table><tr><td rowspan=\"2\">Configuration</td><td colspan=\"2\">COVID Presence</td><td colspan=\"2\">Lung Cancer Type</td><td colspan=\"2\">KidneyAbnormalities</td></tr><tr><td>Accuracy</td><td>F1</td><td>Accuracy</td><td>F1</td><td>Accuracy</td><td>F1</td></tr><tr><td>Global-only</td><td>0.2535</td><td>0.1495</td><td>0.5000</td><td>0.3890</td><td>0.5676</td><td>0.5545</td></tr><tr><td>No UG</td><td>0.2363</td><td>0.1536</td><td>0.4634</td><td>0.3764</td><td>0.5766</td><td>0.5558</td></tr><tr><td>Fixed Patches</td><td>0.2347</td><td>0.1533</td><td>0.4573</td><td>0.3731</td><td>0.5766</td><td>0.5697</td></tr><tr><td>Full Model</td><td>0.8108</td><td>0.7903</td><td>0.9817</td><td>0.9764</td><td>0.9971</td><td>0.9945</td></tr></table></body></html>\n\n# 4.3. Component Analysis\n\nTable 2 shows the contribution of each component in our UGPL framework. The global model (GM), performing whole-image classification, achieves strong performance on the Kidney and Lung datasets $9 8 . 1 1 \\%$ and $9 6 . 1 7 \\%$ accuracy). The local model (LM), using only patch-based classification, shows significantly lower performance when used independently. The fused model (FM), integrating both predictions through our adaptive fusion mechanism, consistently outperforms individual components.\n\nThe performance gap between GM and FM is most evident in COVID-19 detection, with FM reaching $8 1 . 0 8 \\%$ accuracy compared to $7 1 . 0 8 \\%$ for GM. This reflects the benefit of incorporating localized analysis in tasks where global patterns are less prominent. For kidney abnormality detection, FM also improves over GM $9 9 . 7 1 \\%$ vs. $9 8 . 1 1 \\%$ ), showing that local refinement can still enhance outcomes even when global features are already effective.\n\nThe LM performs poorly across all tasks, particularly for kidney abnormalities $( 4 0 . 5 7 \\% )$ and lung cancer classification $( 5 1 . 2 2 \\% )$ , as local patches alone lack sufficient context and focus on irrelevant regions without global guidance. Figure 4 shows performance trends across tasks, with COVID-19 detection showing the most significant gains from LM to FM. ROC curves (Figure 5) show that for COVID-19, GM and FM achieve similar AUC scores (0.901 vs. 0.900). For lung cancer, FM achieves slight improvements across classes, especially for benign cases (0.991 vs. 0.992). For kidney cases, FM improves performance for most classes, including kidney stones (0.984 vs. 0.986).\n\n# 4.4. Ablation Study\n\nWe conduct extensive ablation studies to evaluate the impact of different components in our UGPL framework. We focus on three key aspects: 1) the contribution of each component in the progressive learning pipeline, 2) the influence of patch extraction parameters on model performance, and 3) the effect of varying loss term coefficients in our multi-component optimization objective. We retain the best-performing ResNet variant [29] from our initial evaluations for all experiments.\n\n# 4.4.1. Component Ablation\n\nTo analyze the contribution of each component in our progressive learning framework, we compare four configurations: (1) a global-only setup that uses the global uncertainty estimator without local refinement; (2) a no uncertainty guidance (No UG) variant, where patches are selected randomly instead of using uncertainty maps; (3) a fixed patches configuration that uses predefined patch locations rather than adaptive selection; and (4) the full model, which includes all components of the UGPL framework.\n\nTable 3 shows our full model consistently outperforming all reduced variants by substantial F1 margins. On the COVID dataset, all ablations cause dramatic performance drops, with the global-only variant achieving only $1 4 . 9 5 \\%$\n\nTable 4. Performance comparison of different loss weight configurations across datasets. Loss component weights: Fused $( \\lambda _ { f } )$ , Global $( \\lambda _ { g } )$ , Local $( \\lambda _ { l } )$ , Uncertainty $( \\lambda _ { u } )$ , Consistency $( \\lambda _ { c } )$ , Confidence $( \\lambda _ { \\mathrm { c o } } )$ , Diversity $( \\lambda _ { d } )$ . Configuration C1 represents our baseline model with balanced weights. Best results are in red, second-best in blue, and third-best in green.   \n\n<html><body><table><tr><td rowspan=\"2\">Configuration</td><td colspan=\"6\">Loss Weights</td><td colspan=\"2\">COVID Presence</td><td colspan=\"2\">Lung Cancer Type</td><td colspan=\"2\">Kidney Abnormalities</td></tr><tr><td>ÂÖ•f</td><td>ÂÖ•g</td><td>ÂÖ•</td><td>ÂÖ•u</td><td>ÂÖ•c</td><td>ÂÖ•co</td><td>ÂÖ•d</td><td>Accuracy</td><td>Accuracy</td><td>F1</td><td>Accuracy</td><td>F1</td></tr><tr><td>C1: Baseline</td><td>1.0</td><td>0.5</td><td>0.5</td><td>0.3</td><td>0.2</td><td>0.1 0.1</td><td>0.8108</td><td>0.7903</td><td>0.9817</td><td>0.9764</td><td>0.9971</td><td>0.9945</td></tr><tr><td>C2: Local Emphasis</td><td>1.0</td><td>0.3</td><td>0.7</td><td>0.3</td><td>0.2</td><td>0.1 0.1</td><td></td><td>0.7946 0.7758</td><td>0.9695</td><td>0.9641</td><td>0.9928</td><td>0.9903</td></tr><tr><td>C3: Global-Centric</td><td>1.0</td><td>0.7</td><td>0.3</td><td>0.3</td><td>0.2</td><td>0.1 0.1</td><td>0.7568</td><td>0.7402</td><td>0.9634</td><td>0.9576</td><td>0.9876</td><td>0.9832</td></tr><tr><td>C4: Uncertainty Focus</td><td>1.0</td><td>0.5</td><td>0.5</td><td>0.6</td><td>0.2</td><td>0.1 0.1</td><td>0.8243</td><td>0.8057</td><td>0.9756</td><td>0.9687</td><td>0.9953</td><td>0.9931</td></tr><tr><td>C5: Consistency-Driven</td><td>1.0</td><td>0.5</td><td>0.5</td><td>0.3</td><td>0.5</td><td>0.1 0.1</td><td>0.7892</td><td>0.7689</td><td>0.9786</td><td>0.9723</td><td>0.9913</td><td>0.9889</td></tr><tr><td>C6:Balanced High</td><td>1.0</td><td>0.5</td><td>0.5</td><td>0.4</td><td>0.4</td><td>0.2 0.2</td><td>0.8051</td><td>0.7836</td><td>0.9801</td><td>0.9739</td><td>0.9942</td><td>0.9918</td></tr><tr><td>C7:Diversity-Enhanced</td><td>1.0</td><td>0.5</td><td>0.5</td><td>0.3</td><td>0.2</td><td>0.1 0.4</td><td>0.7784</td><td>0.7569</td><td>0.9667</td><td>0.9602</td><td>0.9895</td><td>0.9856</td></tr><tr><td>C8:Confidence-Calibrated</td><td>1.0</td><td>0.5</td><td>0.5</td><td>0.3</td><td>0.2</td><td>0.4 0.1</td><td>0.7973</td><td>0.7798</td><td>0.9753</td><td>0.9695</td><td>0.9923</td><td>0.9891</td></tr><tr><td>C9: Conservative</td><td>0.5</td><td>0.25</td><td>0.25</td><td>0.15</td><td>0.1</td><td>0.05 0.05</td><td>0.7486</td><td>0.7312</td><td>0.9581</td><td>0.9524</td><td>0.9837</td><td>0.9803</td></tr><tr><td>C10: Aggressive</td><td>2.0</td><td>1.0</td><td>1.0</td><td>0.6</td><td>0.4</td><td>0.2 0.2</td><td>0.8023</td><td>0.7827</td><td>0.9728</td><td>0.9674</td><td>0.9932</td><td>0.9907</td></tr></table></body></html>\n\nTable 5. F1 Scores across patch sizes and number of extracted patches. Bolded values indicate results from C1 configuration.   \n\n<html><body><table><tr><td>Patch Size</td><td>Patches</td><td>Kidney</td><td>Lung</td><td>COVID</td></tr><tr><td rowspan=\"3\">32</td><td>2</td><td>0.9586</td><td>0.8869</td><td>0.7161</td></tr><tr><td>3</td><td>0.9673</td><td>0.9195</td><td>0.7368</td></tr><tr><td>4</td><td>0.9541</td><td>0.8756</td><td>0.7454</td></tr><tr><td rowspan=\"3\">64</td><td>2</td><td>0.9824</td><td>0.9764</td><td>0.7521</td></tr><tr><td>3</td><td>0.9945</td><td>0.8671</td><td>0.7368</td></tr><tr><td>4</td><td>0.9765</td><td>0.9343</td><td>0.7903</td></tr><tr><td rowspan=\"3\">96</td><td>2</td><td>0.9622</td><td>0.8712</td><td>0.7372</td></tr><tr><td>3</td><td>0.9701</td><td>0.9099</td><td>0.7262</td></tr><tr><td>4</td><td>0.9418</td><td>0.8717</td><td>0.6505</td></tr></table></body></html>\n\nF1. For lung cancer detection, the full model obtains $9 7 . 6 4 \\%$ F1, while the global-only setup drops to $3 4 . 1 9 \\%$ . The kidney dataset shows smaller yet significant gaps, with the full model reaching $9 9 . 6 \\%$ F1 versus $5 8 . 7 \\%$ for the best ablated configuration (fixed patches). Interestingly, No UG and fixed patches sometimes perform worse than the globalonly model, showing that naively adding local components without proper guidance can be detrimental and highlighting the importance of uncertainty-guided patch selection.\n\n# 4.4.2. Loss Component Weights\n\nTable 4 compares ten loss weight configurations across datasets. The baseline configuration (C1) with balanced weights performs best overall (fused: 1.0, global/local: 0.5 each, uncertainty: 0.3, consistency: 0.2, confidence/diversity: 0.1 each). Configurations emphasizing either global or local branches underperform, confirming the necessity of combining global context with local detail. Increased uncertainty weighting (C4) improves COVID detection $8 2 . 4 3 \\%$ accuracy, $80 . 5 7 \\%$ F1) but slightly reduces performance on Lung and Kidney datasets where target features are more prominent. C5 (Consistency-Driven) excels on the Lung dataset $( 9 7 . 8 6 \\%$ accuracy) where structural patterns are clearer, while uniform scaling of all components (C9 & C10) shows no improvement, indicating that relative balance matters more than absolute weight values.\n\n# 4.4.3. Patch Extraction Parameters\n\nTable 5 presents F1 scores for different combinations of patch sizes and counts across datasets. Optimal configurations vary by task: kidney abnormality detection performs best with $6 4 \\times 6 4$ patches and 3 patches per image $( \\mathrm { F 1 } = 0 . 9 9 4 5 )$ ), lung cancer classification with $6 4 \\times 6 4$ and 2 patches $( \\mathrm { F 1 } = 0 . 9 7 6 4 )$ ), and COVID-19 detection with $6 4 \\times 6 4$ and 4 patches $( \\mathrm { F } 1 = 0 . 7 9 0 3 \\$ ). A patch size of 64 consistently outperforms both smaller (32) and larger (96) sizes, suggesting it provides an optimal balance between local detail and contextual information. The number of required patches aligns with each task‚Äôs complexity - COVID detection needs more regions due to diffuse disease manifestations, while lung cancer classification can focus on fewer, more localized abnormalities.\n\n# 5. Conclusion\n\nThis paper proposed UGPL (Uncertainty-Guided Progressive Learning), a framework for medical image classification that analyzes CT images in two stages: global prediction with uncertainty estimation, followed by local refinement on selected high-uncertainty regions. Our evidential learning-based uncertainty estimation identifies diagnostically challenging areas, while the adaptive fusion mechanism combines global and local predictions based on confidence measures. Extensive experiments across three diverse CT classification tasks (COVID-19 detection, lung cancer classification, and kidney abnormality identification) demonstrate that UGPL significantly outperforms existing methods. Ablations show that the uncertainty-guided patch selection yields upto $5 . 3 \\times$ F1 improvement compared to other configurations. Future work will explore extending UGPL to other modalities like MRI/PET and investigating its potential for uncertainty-guided active learning.",
    "summary": "```json\n{\n  \"core_summary\": \"### üéØ Ê†∏ÂøÉÊ¶ÇË¶Å\\n\\n> **ÈóÆÈ¢òÂÆö‰πâ (Problem Definition)**\\n> *   Áé∞ÊúâËÆ°ÁÆóÊú∫Êñ≠Â±ÇÊâ´ÊèèÔºàCTÔºâÂõæÂÉèÂàÜÁ±ªÊñπÊ≥ïÂú®Â§ÑÁêÜÁóÖÁêÜÁâπÂæÅÁöÑÁªÜÂæÆÊÄßÂíåÁ©∫Èó¥Â§öÊ†∑ÊÄßÊó∂Ë°®Áé∞‰∏ç‰Ω≥ÔºåÈÄöÂ∏∏ÈááÁî®ÂùáÂåÄÂ§ÑÁêÜÊñπÂºèÔºåÈöæ‰ª•Ê£ÄÊµãÈúÄË¶ÅÈáçÁÇπÂàÜÊûêÁöÑÂ±ÄÈÉ®ÂºÇÂ∏∏Âå∫Âüü„ÄÇ\\n> *   ËØ•ÈóÆÈ¢òÂú®ÂåªÂ≠¶ÂΩ±ÂÉèËØäÊñ≠‰∏≠Ëá≥ÂÖ≥ÈáçË¶ÅÔºåÂõ†‰∏∫ËÆ∏Â§öÂÖ≥ÈîÆÁóÖÁêÜÁâπÂæÅÔºàÂ¶ÇËÇ∫ÁªìËäÇÊàñËÇæÂõäËÇøÔºâ‰ªÖÂç†ÊçÆÂõæÂÉèÁöÑÊûÅÂ∞èÈÉ®ÂàÜÔºåÂùáÂåÄÂ§ÑÁêÜÂÆπÊòìÂØºËá¥ÊºèËØä„ÄÇ\\n\\n> **ÊñπÊ≥ïÊ¶ÇËø∞ (Method Overview)**\\n> *   ÊèêÂá∫UGPLÔºà‰∏çÁ°ÆÂÆöÊÄßÂºïÂØºÁöÑÊ∏êËøõÂ≠¶‰π†Ê°ÜÊû∂ÔºâÔºåÈÄöËøáÂÖ®Â±ÄÂà∞Â±ÄÈÉ®ÁöÑÂàÜÊûêÁ≠ñÁï•ÔºåÂÖàËØÜÂà´ËØäÊñ≠Ê®°Á≥äÂå∫ÂüüÔºåÂÜçÂØπËøô‰∫õÂÖ≥ÈîÆÂå∫ÂüüËøõË°åËØ¶ÁªÜÊ£ÄÊü•„ÄÇ\\n\\n> **‰∏ªË¶ÅË¥°ÁåÆ‰∏éÊïàÊûú (Contributions & Results)**\\n> *   **‰∏çÁ°ÆÂÆöÊÄßÂºïÂØºÁöÑÊ∏êËøõÂ≠¶‰π†Ê°ÜÊû∂**ÔºöÂä®ÊÄÅÂàÜÈÖçËÆ°ÁÆóËµÑÊ∫êËá≥È´òËØäÊñ≠Ê®°Á≥äÂå∫ÂüüÔºåÂú®‰∏â‰∏™CTÊï∞ÊçÆÈõÜ‰∏äÂàÜÂà´ÊèêÂçáÂáÜÁ°ÆÁéá3.29%ÔºàËÇæÂºÇÂ∏∏Ôºâ„ÄÅ2.46%ÔºàËÇ∫ÁôåÔºâÂíå8.08%ÔºàCOVID-19Ê£ÄÊµãÔºâ„ÄÇ\\n> *   **ËØÅÊçÆÊ∑±Â∫¶Â≠¶‰π†ÔºàEDLÔºâÊñπÊ≥ï**ÔºöÈÄöËøáDirichletÂàÜÂ∏ÉÊèê‰æõÂéüÂàôÊÄßÁöÑ‰∏çÁ°ÆÂÆöÊÄßÈáèÂåñ„ÄÇ\\n> *   **Ëá™ÈÄÇÂ∫îË°•‰∏ÅÊèêÂèñÊú∫Âà∂**ÔºöÈááÁî®ÈùûÊûÅÂ§ßÂÄºÊäëÂà∂ÈÄâÊã©Â§öÊ†∑Âåñ„ÄÅÈùûÈáçÂè†Âå∫ÂüüËøõË°åËØ¶ÁªÜÂàÜÊûê„ÄÇ\\n> *   **Â§öÁªÑ‰ª∂ÊçüÂ§±ÂáΩÊï∞**ÔºöËÅîÂêà‰ºòÂåñÂàÜÁ±ªÂáÜÁ°ÆÊÄß„ÄÅ‰∏çÁ°ÆÂÆöÊÄßÊ†°ÂáÜÂíåÁ©∫Èó¥Â§öÊ†∑ÊÄß„ÄÇ\\n> *   **ÊÄßËÉΩÊèêÂçá**ÔºöÂú®ËÇæËÑèÂºÇÂ∏∏Êï∞ÊçÆÈõÜ‰∏äËææÂà∞99%Ôºà¬±0.0023ÔºâÁöÑÂáÜÁ°ÆÁéáÔºåÊòæËëó‰ºò‰∫éÂü∫Á∫øÊ®°ÂûãCoaTÔºà98%ÔºâÔºõÂú®ËÇ∫ÁôåÂàÜÁ±ª‰ªªÂä°‰∏äËææÂà∞97%Ôºà¬±0.0052ÔºâÁöÑF1ÂàÜÊï∞Ôºå‰ºò‰∫éEfficientNetB0Ôºà95%ÔºâÔºõÂú®COVID-19Ê£ÄÊµã‰∏äËææÂà∞81%Ôºà¬±0.0134ÔºâÁöÑÂáÜÁ°ÆÁéáÔºå‰ºò‰∫éDenseNet121Ôºà76%Ôºâ„ÄÇ\",\n  \"algorithm_details\": \"### ‚öôÔ∏è ÁÆóÊ≥ï/ÊñπÊ°àËØ¶Ëß£\\n\\n> **Ê†∏ÂøÉÊÄùÊÉ≥ (Core Idea)**\\n> *   UGPLÊ°ÜÊû∂Ê®°‰ªøÊîæÂ∞ÑÁßëÂåªÁîüÁöÑËØäÊñ≠Ë°å‰∏∫ÔºåÈÄöËøáÂÖ®Â±ÄÂàÜÊûêÂêéÂØπ‰∏çÁ°ÆÂÆöÂå∫ÂüüËøõË°åËÅöÁÑ¶Ê£ÄÊü•„ÄÇÂÖ∂ÊúâÊïàÊÄßÊ∫ê‰∫éÂä®ÊÄÅÂàÜÈÖçËÆ°ÁÆóËµÑÊ∫êËá≥ËØäÊñ≠ÂÖ≥ÈîÆÂå∫ÂüüÔºåÈÅøÂÖçÂùáÂåÄÂ§ÑÁêÜÁöÑËµÑÊ∫êÊµ™Ë¥π„ÄÇ\\n> *   ËØ•ÊñπÊ≥ïÊúâÊïàÁöÑÂéüÂõ†Âú®‰∫éÔºö1Ôºâ‰∏çÁ°ÆÂÆöÊÄßÈáèÂåñËÉΩÂ§üÂáÜÁ°ÆËØÜÂà´ÈúÄË¶ÅËøõ‰∏ÄÊ≠•ÂàÜÊûêÁöÑÂõæÂÉèÂå∫ÂüüÔºõ2ÔºâÊ∏êËøõÂºèÂ≠¶‰π†Á≠ñÁï•ËÉΩÂ§üÁªìÂêàÂÖ®Â±Ä‰∏ä‰∏ãÊñáÂíåÂ±ÄÈÉ®ÁªÜËäÇÔºõ3ÔºâËá™ÈÄÇÂ∫îËûçÂêàÊú∫Âà∂ËÉΩÂ§üÊ†πÊçÆÁΩÆ‰ø°Â∫¶Âä®ÊÄÅË∞ÉÊï¥ÂÖ®Â±ÄÂíåÂ±ÄÈÉ®È¢ÑÊµãÁöÑÊùÉÈáç„ÄÇ\\n\\n> **ÂàõÊñ∞ÁÇπ (Innovations)**\\n> *   **‰∏éÂÖàÂâçÂ∑•‰ΩúÁöÑÂØπÊØî**Ôºö‰º†ÁªüCNNÈááÁî®ÂùáÂåÄÂç∑ÁßØÂ§ÑÁêÜÔºåÂøΩÁï•Âå∫ÂüüËØäÊñ≠‰ª∑ÂÄºÔºõË¥ùÂè∂ÊñØCNN‰∫ßÁîü‰∏çÁ°ÆÂÆöÊÄßÂõæ‰ΩÜÊó†ËÅöÁÑ¶ÁªÜÂåñÔºõÊ≥®ÊÑèÂäõÊú∫Âà∂Âü∫‰∫éÂ≠¶‰π†Ê®°ÂºèËÄåÈùûËØäÊñ≠‰∏çÁ°ÆÂÆöÊÄß„ÄÇ\\n> *   **Êú¨ÊñáÁöÑÊîπËøõ**ÔºöÂ∞Ü‰∏çÁ°ÆÂÆöÊÄß‰º∞ËÆ°‰Ωú‰∏∫ÂàÜÊûêÂèçÈ¶àÔºåÊåáÂØºËÆ°ÁÆóÁÑ¶ÁÇπÔºå‰øùÊåÅÊïàÁéáÁöÑÂêåÊó∂ÊèêÂçáËØäÊñ≠ÊåëÊàòÂå∫ÂüüÁöÑÊÄßËÉΩ„ÄÇ\\n\\n> **ÂÖ∑‰ΩìÂÆûÁé∞Ê≠•È™§ (Implementation Steps)**\\n> 1.  ÂÖ®Â±Ä‰∏çÁ°ÆÂÆöÊÄß‰º∞ËÆ°Ôºö‰ΩøÁî®ResNetÈ™®Âπ≤ÁΩëÁªúÊèêÂèñÁâπÂæÅÔºåÂπ∂Ë°åÂ§ÑÁêÜÂàÜÁ±ªÂ§¥ÂíåËØÅÊçÆÂ§¥ÔºåÁîüÊàêÂàùÂßãÂàÜÁ±ªÊ¶ÇÁéáÂíåÂÉèÁ¥†Á∫ß‰∏çÁ°ÆÂÆöÊÄßÂõæ„ÄÇ\\n> 2.  Ê∏êËøõË°•‰∏ÅÊèêÂèñÔºöÂü∫‰∫é‰∏çÁ°ÆÂÆöÊÄßÂõæÔºåÈÄöËøáÈùûÊûÅÂ§ßÂÄºÊäëÂà∂ÈÄâÊã©È´ò‰∏çÁ°ÆÂÆöÊÄßÂå∫ÂüüÔºåÊèêÂèñK‰∏™Ë°•‰∏Å„ÄÇ\\n> 3.  Â±ÄÈÉ®ÁªÜÂåñÁΩëÁªúÔºöÁã¨Á´ãÂ§ÑÁêÜÊØè‰∏™Ë°•‰∏ÅÔºåÁîüÊàêË°•‰∏ÅÁâπÂÆöÂàÜÁ±ªÂàÜÊï∞ÂíåÁΩÆ‰ø°Â∫¶‰º∞ËÆ°„ÄÇ\\n> 4.  Ëá™ÈÄÇÂ∫îËûçÂêàÊ®°ÂùóÔºöÂü∫‰∫é‰º∞ËÆ°ÂèØÈù†ÊÄßÔºåÈÄöËøáÂ≠¶‰π†ÊùÉÈáçÊï¥ÂêàÂÖ®Â±ÄÂíåÂ±ÄÈÉ®È¢ÑÊµã„ÄÇ\\n> 5.  Â§öÁªÑ‰ª∂ÊçüÂ§±‰ºòÂåñÔºöËÅîÂêà‰ºòÂåñÂàÜÁ±ªÊçüÂ§±Ôºà‰∫§ÂèâÁÜµÔºâ„ÄÅ‰∏çÁ°ÆÂÆöÊÄßÊ†°ÂáÜÊçüÂ§±„ÄÅ‰∏ÄËá¥ÊÄßÊçüÂ§±„ÄÅÁΩÆ‰ø°Â∫¶ÊçüÂ§±ÂíåÂ§öÊ†∑ÊÄßÊçüÂ§±„ÄÇ\\n\\n> **ÂÖ≥ÈîÆÊï∞Â≠¶ÂÖ¨Âºè**\\n> *   DirichletÂàÜÂ∏ÉÂèÇÊï∞ÂåñÔºö\\n>     $$\\\\alpha_{i,j,c} = \\\\beta_{i,j,c} \\\\cdot \\\\nu_{i,j,c} + 1$$\\n>     ÂÖ∂‰∏≠Œ≤Ë°®Á§∫‰∏çÁ°ÆÂÆöÊÄßÂÄíÊï∞ÔºåŒΩË°®Á§∫Ë¥®Èáè‰ø°Âøµ„ÄÇ\\n> *   ÊÄªÈ¢ÑÊµã‰∏çÁ°ÆÂÆöÊÄßËÆ°ÁÆóÔºö\\n>     $$\\\\mathcal{U}_{\\\\mathrm{total}} = \\\\sum_{i=1}^C \\\\frac{\\\\alpha_i}{S} \\\\left(1-\\\\frac{\\\\alpha_i}{S}\\\\right) \\\\frac{1}{S+1}$$\\n>     ÊçïËé∑È¢ÑÊúüÂàÜÁ±ªÂàÜÂ∏ÉÁöÑÁÜµÂíåDirichletÂàÜÂ∏ÉÊú¨Ë∫´ÁöÑÈ¢ùÂ§ñ‰∏çÁ°ÆÂÆöÊÄß„ÄÇ\\n\\n> **Ê°à‰æãËß£Êûê (Case Study)**\\n> ËÆ∫ÊñáÊú™ÊòéÁ°ÆÊèê‰æõÊ≠§ÈÉ®ÂàÜ‰ø°ÊÅØ\",\n  \"comparative_analysis\": \"### üìä ÂØπÊØîÂÆûÈ™åÂàÜÊûê\\n\\n> **Âü∫Á∫øÊ®°Âûã (Baselines)**\\n> *   ËΩªÈáèÁ∫ßCNNÔºöShuffleNetV2„ÄÅMobileNetV2\\n> *   Ê†áÂáÜÂç∑ÁßØÂü∫Á∫øÔºöVGG16„ÄÅDenseNet121/201„ÄÅEfficientNetB0„ÄÅConvNeXt\\n> *   Âü∫‰∫éTransformerÁöÑÊû∂ÊûÑÔºöViT„ÄÅSwin„ÄÅDeiT„ÄÅCoaT„ÄÅCrossViT\\n> *   ‰∏ìÁî®ÂåªÂ≠¶ÂΩ±ÂÉèÊ®°ÂûãÔºöCRNet\\n\\n> **ÊÄßËÉΩÂØπÊØî (Performance Comparison)**\\n> *   **Âú®ÂáÜÁ°ÆÁéá‰∏ä**ÔºöUGPLÂú®ËÇæËÑèÂºÇÂ∏∏Êï∞ÊçÆÈõÜ‰∏äËææÂà∞99%Ôºà¬±0.0023ÔºâÔºåÊòæËëó‰ºò‰∫éË°®Áé∞ÊúÄ‰Ω≥ÁöÑÂü∫Á∫øCoaTÔºà98%ÔºâÂíåCrossViTÔºà97%Ôºâ„ÄÇ‰∏éÊúÄ‰Ω≥Âü∫Á∫øÁõ∏ÊØîÔºåÊèêÂçá‰∫Ü1‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®F1ÂàÜÊï∞‰∏ä**ÔºöUGPLÂú®ËÇ∫ÁôåÂàÜÁ±ª‰ªªÂä°‰∏äËææÂà∞97%Ôºà¬±0.0052ÔºâÔºå‰ºò‰∫éEfficientNetB0Ôºà95%ÔºâÂíåConvNeXtÔºà95%ÔºâÔºåÊèêÂçá2‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®COVID-19Ê£ÄÊµã‰∏ä**ÔºöUGPLËææÂà∞81%Ôºà¬±0.0134ÔºâÁöÑÂáÜÁ°ÆÁéáÂíå79%Ôºà¬±0.0147ÔºâÁöÑF1ÂàÜÊï∞Ôºå‰ºò‰∫éDenseNet121Ôºà76%ÔºâÂíåCRNetÔºà76%ÔºâÔºåÊèêÂçá3‰∏™ÁôæÂàÜÁÇπ„ÄÇ\\n> *   **Âú®Ê®°ÂûãÈ≤ÅÊ£íÊÄß‰∏ä**ÔºöUGPLÂú®ÊâÄÊúâÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞Âá∫ÊúÄ‰ΩéÁöÑÊñπÂ∑ÆÔºà¬±0.0023-¬±0.0147ÔºâÔºåËÄåTransformerÊ®°ÂûãÂ¶ÇSwinÂíåDeiTÂú®Êüê‰∫õ‰ªªÂä°‰∏äÊñπÂ∑ÆÈ´òËææ¬±0.0421ÔºåË°®ÊòéUGPLÂÖ∑ÊúâÊõ¥Á®≥ÂÆöÁöÑÊÄßËÉΩ„ÄÇ\",\n  \"keywords\": \"### üîë ÂÖ≥ÈîÆËØç\\n\\n*   ËÆ°ÁÆóÊú∫Êñ≠Â±ÇÊâ´Êèè (Computed Tomography, CT)\\n*   ‰∏çÁ°ÆÂÆöÊÄßÂºïÂØºÂ≠¶‰π† (Uncertainty-Guided Learning, UGL)\\n*   ËØÅÊçÆÊ∑±Â∫¶Â≠¶‰π† (Evidential Deep Learning, EDL)\\n*   Ê∏êËøõÂ≠¶‰π† (Progressive Learning, N/A)\\n*   ÂåªÂ≠¶ÂΩ±ÂÉèÂàÜÁ±ª (Medical Image Classification, N/A)\\n*   Ëá™ÈÄÇÂ∫îËûçÂêà (Adaptive Fusion, N/A)\\n*   ÈùûÊûÅÂ§ßÂÄºÊäëÂà∂ (Non-Maximum Suppression, NMS)\\n*   DirichletÂàÜÂ∏É (Dirichlet Distribution, N/A)\"\n}\n```"
}