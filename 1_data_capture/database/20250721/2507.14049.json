{
    "source": "ArXiv (Semantic Scholar未收录)",
    "arxiv_id": "2507.14049",
    "link": "https://arxiv.org/abs/2507.14049",
    "pdf_link": "https://arxiv.org/pdf/2507.14049.pdf",
    "title": "EdgeVLA: Efficient Vision-Language-Action Models",
    "authors": [
        "Paweł Budzianowski",
        "Wesley Maa",
        "Matthew Freed",
        "Jingxiang Mo",
        "Winston Hsiao",
        "Aaron Xie",
        "Tomasz Młoduchowski",
        "Viraj Tipnis",
        "Benjamin Bolte"
    ],
    "categories": [
        "cs.RO",
        "cs.CL"
    ],
    "publication_date": "未找到提交日期",
    "venue": "暂未录入Semantic Scholar",
    "fields_of_study": "暂未录入Semantic Scholar",
    "citation_count": "暂未录入Semantic Scholar",
    "influential_citation_count": "暂未录入Semantic Scholar",
    "institutions": [
        "未找到机构信息"
    ],
    "paper_content": "# EdgeVLA: Efficient Vision-Language-Action Models\n\nPaweł Budzianowski, Wesley Maa, Matthew Freed, Jingxiang Mo, Winston Hsiao Aaron Xie, Tomasz Młoduchowski, Viraj Tipnis, Benjamin Bolte\n\nAbstract— Vision-Language Models (VLMs) have emerged as a promising approach to address the data scarcity challenge in robotics, enabling the development of generalizable visuomotor control policies. While models like OpenVLA showcase the potential of this paradigm, deploying large-scale VLMs on resource-constrained mobile manipulation systems remains a significant hurdle. This paper introduces Edge VLA (EVLA), a novel approach designed to significantly enhance the inference speed of Vision-Language-Action (VLA) models. EVLA maintains the representational power of these models while enabling real-time performance on edge devices. We achieve this through two key innovations: 1) Eliminating the autoregressive requirement for end-effector position prediction, leading to a $\\mathbf { 7 x }$ speedup in inference, and 2) Leveraging the efficiency of Small Language Models (SLMs), demonstrating comparable training performance to larger models with significantly reduced computational demands. Our early results demonstrate that EVLA achieves comparable training characteristics to OpenVLA while offering substantial gains in inference speed and memory efficiency. We release our model checkpoints and training codebase to foster further research.\n\n# I. INTRODUCTION\n\nThe development of robust and generalizable manipulation policies has long been hampered by the limited availability of large-scale, diverse embodied datasets. Recent advancements in Vision-Language Models (VLMs) [11], [8] offer a compelling solution to this challenge. By leveraging the vast amount of readily available image-text data, VLMs can learn rich representations of the world and be adapted for visuomotor control tasks. Open-source models like OpenVLA [9] have demonstrated the effectiveness of this approach, showcasing impressive performance in various robotic manipulation tasks. However, deploying these largescale VLMs, often exceeding billions of parameters, on resource-constrained mobile platforms with edge devices like the Jetson Nano presents significant challenges. Their high computational and memory requirements hinder realtime performance and limit accessibility for researchers and practitioners.\n\nThe progress in the mobile manipulation can be effective only if the systems we design are inexpensive and easily deployable without putting too much strain on compute requirements. That is why, this paper introduces Edge VLA (EVLA), a novel VLA architecture designed to address above-mentioned challenges. EVLA offers significant improvements in inference speed and efficiency without compromising foundation models’ representational power. Our approach centers around two key innovations: First, our work focuses on architectural modifications to achieve significant speedups while maintaining model performance. Specifically by eliminating the autoregressive requirement for endeffector prediction and leveraging the efficiency of SLMs. We challenge the autoregressive approach for predicting endeffector positions, demonstrating that joint control, where the entire position is predicted simultaneously, does not diminish the model’s encoding capabilities. This modification yields a 7-times increase in inference speed, crucial for real-time robotic control on edge devices.\n\nSecondly, we explore the potential of recently developed Small Large Language Models (SMLs), such as Qwen2 [17], Phi [1] or Gemma [13], which achieve comparable performance to their larger counterparts thanks to scaling laws with significantly reduced computational footprints. Our proposed architecture EVLA comprises of a pretrained language model Qwen2-0.5B fused with two visual encoders SigLIP [19] and DINOv2 [12] adding to 1B parameters. EVLA maintains training performance comparable to that of models 7 times larger while significantly reducing hardware requirements.\n\n# II. RELATED WORK\n\nLearning-based approaches to mobile manipulation are beginning to reach or exceed the performance of classical model-based control systems [6]. We can broadly divide these approaches into systems trained from scratch and those fine-tuned on top of the foundation models.\n\nThe former approach has relied on behavioral cloning where visual observations are typically mapped to either end effector position with orientation or joint positions [3], [10]. These models can be enhanced through regularization, planning or multi-task learning pushing the limits of the performance. This approach enables easy deployment with relatively cheap hardware but does not leverage the power of foundation models [18], [6]. These systems typically train models from scratch with model sizes from 10 to 100M parameters which limits their ability to generalize to novel environments [20], [3].\n\nThe line of work that relies on foundation models incorporates all the aforementioned techniques while aiming for more powerful generalization capabilities. The most extensively explored approach relies on vision-language models [11], [8]. The vision component is typically adapted to operate in the same token space as the LLM, allowing for the reuse of different pretrained blocks. Combined with large manipulation datasets such as OpenX [4], these models have demonstrated the promise of this paradigm by generalizing to new environments [2], [9]. Although these works have highlighted the potential of leveraging large language models (LLMs), they come with substantial computational demands. Efforts to improve efficiency include quantization techniques [16] and hardware-specific kernels [14]. Nevertheless, these system achieve speed of only 5 to $1 0 ~ \\mathrm { H z }$ with stationary compute systems preventing their deployment on edge devices even in laboratory settings.\n\n![](images/78e3d196eeebbff8cd084c47fabe87e1a5b993cba132a49b1db9b3390198dd94.jpg)  \nFig. 1: The comparison of generation logic between OpenVLA and EVLA. The pretraining phase is identical for both models. In phase two, the EVLA LLM is being retrained to generate end-effector position in an autoregressive fashion.\n\n# III. METHOD\n\n# A. Phase 1: VLM Pretraining\n\nEVLA is based on a VLM trained using a combination of image-text pairs sourced from diverse captioning datasets and synthetically generated multimodal instruction-tuning examples [11]. The pre-training dataset comprises 1.2M textimage pairs, facilitating the learning of robust visual and language representations following the recipe of the PrismaticVLM family of models [8]. For language processing, we utilize Qwen2 [17] with 0.5B parameters as it demonstrates the effectiveness of SLMs in achieving comparable performance to that of larger models. We adopt a twopart visual encoder, employing pretrained SigLIP [19] and DinoV2 [12] models, following the architecture of OpenVLA [9]. A projection layer that maps the visual representation to the language model’s token space is learned jointly with the finetuned visual and language components.\n\n# B. Phase 2: Joint Control for End-Effector Prediction\n\nThe second phase of training utilizes around 1M of manipulation examples from the OpenX dataset [4]. Traditional VLAs employ an autoregressive approach to predicting endeffector positions, mimicking the causal nature of language generation. However, we hypothesize that for robotic control, this restriction is not inherently necessary. We propose that predicting the entire end-effector position jointly, rather than sequentially, does not compromise the model’s encoding capabilities while significantly improving inference speed.\n\nBy removing the causal mask in the LLM and training the model to output the entire end-effector position at once, we eliminate autoregressive requirements, achieving a six-times speedup in inference - a critical improvement for real-time applications on edge devices.\n\nSee Figure 1 for the overall layout of the model and the comparison to its autoregressive counterpart.\n\n# IV. EARLY RESULTS\n\nIn order to evaluate EVLA’s capabilities of adapting to non-autoregressive loss while utilizing SMLs, we used BridgeData V2 [15] and OpenX datasets [4] as a testbed. We hypothesize that the early training results will shed some light on model characteristics.\n\n# A. BridgeData V2 training characteristics\n\nInitial experiments on the BridgeData V2 dataset conducted on a single node with 8 A100-80GB GPUs, validate that EVLA can achieve similar training performance to its 7.5B parameters counterpart. Figure 2 illustrates the training progress, showcasing the comparable performance of the two models. It is worth pointing out that the training efficiency is distinguishably slower for EVLA due to smaller parametrization capabilities.\n\n# B. OpenX training characteristics\n\nWe further evaluate EVLA on the full OpenX dataset, utilizing 80 A100-40GB GPUs for approximately 5 days. While the training efficiency of EVLA is slower than OpenVLA due to the smaller representational power, the training iteration is around 7 times faster. It also allows for larger batch sizes, effectively mitigating the difference in training efficiency. Figure 3 shows the training progress on the OpenX dataset.\n\nDue to computational constraints, we were not able to reproduce the full training of OpenVLA as in the original implementation [9]. However, the training curves show the signs of stagnation and the behavior is similar to the BridgeV2 case.\n\n![](images/2f08ad3e93e3cd457ab66b31da3c1bd82bc13628b3ac98835aadf1271c0420e2.jpg)  \nFig. 2: The loss (left) and action token accuracy (right) training curves for both OpenVLA and EVLA models during trainin on the BridgeData V2 dataset.\n\n![](images/18d1676d169d0497ccf2f741a4c4e6b46ae9f0741ca0c4fcead0b17b7c096a0f.jpg)\n\n![](images/04da96f8923c2b4882dd8dc55f9ed4def55ad544f76fd2b1590a88fec1eb2e8e.jpg)  \nFig. 3: The loss (left) and the action token accuracy (right) training curves for both OpenVLA and EVLA models during training on the OpenX dataset.\n\n# C. Efficiency Gains\n\n# V. CONCLUSIONS\n\nEVLA’s architectural modifications result in substantial improvements in the inference speed and memory consumption, enabling deployment on resource-constrained edge devices. Table I compares the inference time and memory requirements of EVLA and OpenVLA on an A100-40GB GPU.\n\nTABLE I: Efficiency Comparison of EVLA and OpenVLA.   \n\n<html><body><table><tr><td>Model</td><td>Inference Time (ms)</td><td>MemoryUsage (GB)</td></tr><tr><td>OpenVLA</td><td>20</td><td>16</td></tr><tr><td>EVLA</td><td>5</td><td>4</td></tr></table></body></html>\n\nBy using a smaller VLM and optimizing our architecture, we can achieve significant inference speed and memory improvements. These speedups will only increase with the addition of more degrees of freedom. It is worth noting that OpenVLA uses flash_attention2 [5] kernels, while EVLA is evaluated in the eager mode. Advances in flexible and efficient attention mechanisms, such as FlexAttention [7], are expected to push these numbers even further. These results show the path for the deployment of mobile manipulation systems on CPU architectures.\n\nThis paper presents Edge VLA (EVLA), a novel VLA architecture designed for efficient deployment on mobile manipulators and humanoids. By eliminating the autoregressive requirement for end-effector prediction and leveraging the efficiency of SLMs, EVLA achieves significant improvements in inference time and a reduced memory footprint. While the early results suggest EVLA has the potential to be a good candidate for real-time VLA applications on resourceconstrained platforms, the crucial next step is to evaluate EVLA on a variety of different embodiments. We plan to employ at least two different humanoid platforms to assess its few-shot capabilities.\n\nWe release our model checkpoints and training codebase to facilitate further research. We believe that EVLA’s efficiency and accessibility will empower researchers and practitioners to explore the full potential of VLAs for mobile manipulation. Future work will focus on further optimizing EVLA’s architecture and exploring its deployment on a wider range of edge devices, including CPU-based platforms.",
    "summary": "```json\n{\n  \"core_summary\": \"### 🎯 核心概要\\n\\n> **问题定义 (Problem Definition)**\\n> *   论文解决的核心问题是大型视觉-语言-动作模型（VLA）在资源受限的边缘设备上部署时的高计算和内存需求问题，这一问题限制了实时性能和在移动机器人平台上的应用。\\n> *   该问题的重要性在于，高效的VLA模型可以推动移动机器人和人形机器人的广泛应用，尤其是在需要实时响应的场景中。\\n\\n> **方法概述 (Method Overview)**\\n> *   论文提出Edge VLA（EVLA），一种新型VLA架构，通过消除自回归需求和使用小型语言模型（SLMs）来显著提升推理速度和内存效率。\\n\\n> **主要贡献与效果 (Contributions & Results)**\\n> *   **创新贡献点1：** 消除自回归需求，通过联合控制预测整个末端执行器位置，实现7倍推理速度提升（原文为6倍，修正为7倍）。\\n> *   **创新贡献点2：** 使用SLMs（如Qwen2-0.5B），在保持性能的同时显著降低计算需求。\\n> *   **关键数据：** EVLA在A100-40GB GPU上的推理时间为5ms，内存使用为4GB，而OpenVLA的推理时间为20ms，内存使用为16GB。\",\n  \"algorithm_details\": \"### ⚙️ 算法/方案详解\\n\\n> **核心思想 (Core Idea)**\\n> *   EVLA的核心思想是通过联合预测末端执行器位置（而非自回归预测）和使用高效的SLMs，来减少计算和内存开销，同时保持模型的表示能力。\\n> *   这种方法有效的原因是，机器人控制任务中的位置预测不需要像语言生成那样的因果性，因此可以并行化处理。\\n\\n> **创新点 (Innovations)**\\n> *   **与先前工作的对比：** 传统VLA模型（如OpenVLA）采用自回归方式预测末端执行器位置，导致高延迟和计算成本。\\n> *   **本文的改进：** EVLA通过移除LLM中的因果掩码，训练模型一次性输出整个末端执行器位置，从而显著提升推理速度。\\n\\n> **具体实现步骤 (Implementation Steps)**\\n> *   1. **预训练阶段：** 使用1.2M文本-图像对训练VLM，采用Qwen2-0.5B作为语言模型，SigLIP和DinoV2作为视觉编码器。\\n> *   2. **联合控制阶段：** 使用OpenX数据集的1M操作示例，训练模型一次性预测末端执行器位置，消除自回归需求。\\n\\n> **案例解析 (Case Study)**\\n> *   论文未明确提供此部分信息。\",\n  \"comparative_analysis\": \"### 📊 对比实验分析\\n\\n> **基线模型 (Baselines)**\\n> *   OpenVLA\\n\\n> **性能对比 (Performance Comparison)**\\n> *   **在推理时间上：** EVLA在A100-40GB GPU上的推理时间为5ms，显著优于OpenVLA（20ms），速度提升了4倍。\\n> *   **在内存使用上：** EVLA的内存使用为4GB，远低于OpenVLA（16GB），内存效率提升了4倍。\\n> *   **在训练性能上：** EVLA在BridgeData V2和OpenX数据集上展示了与OpenVLA相当的训练性能，尽管其参数规模更小。\\n> *   **在训练效率上：** EVLA的训练迭代速度比OpenVLA快7倍，但由于较小的表示能力，训练效率较慢。\",\n  \"keywords\": \"### 🔑 关键词\\n\\n*   视觉-语言-动作模型 (Vision-Language-Action Model, VLA)\\n*   边缘计算 (Edge Computing, N/A)\\n*   小型语言模型 (Small Language Model, SLM)\\n*   机器人控制 (Robotic Control, N/A)\\n*   非自回归预测 (Non-autoregressive Prediction, N/A)\\n*   联合控制 (Joint Control, N/A)\\n*   推理效率 (Inference Efficiency, N/A)\\n*   移动机器人 (Mobile Manipulation, N/A)\"\n}\n```"
}