# HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning

Chunlin Tian† University of Macau yc27402@um.edu.mo

Zhan Shi† University of Texas at Austin zshi17@cs.utexas.edu

Zhijiang Guo University of Cambridge zg283@cam.ac.uk

Li Li\* University of Macau llili@um.edu.mo

Chengzhong Xu University of Macau czxu@um.edu.mo

# Abstract

Adapting Large Language Models (LLMs) to new tasks through fine-tuning has been made more efficient by the introduction of Parameter-Efficient Fine-Tuning (PEFT) techniques, such as LoRA. However, these methods often underperform compared to full fine-tuning, particularly in scenarios involving complex datasets. This issue becomes even more pronounced in complex domains, highlighting the need for improved PEFT approaches that can achieve better performance. Through a series of experiments, we have uncovered two critical insights that shed light on the training and parameter inefficiency of LoRA. Building on these insights, we have developed HydraLoRA, a LoRA framework with an asymmetric structure that eliminates the need for domain expertise. Our experiments demonstrate that HydraLoRA outperforms other PEFT approaches, even those that rely on domain knowledge during the training and inference phases. Code is available.

# 1 Introduction

Large Language Models (LLMs; [10, 3, 36, 47, 48, 32, 33]) are notably powerful, yet their training involves substantial expense. Adapting a single LLM for multiple downstream applications via finetuning has emerged as a prevalent method to cater to specific domain needs, balancing performance with practicality. This approach, however, faces a significant challenge due to the extensive memory and computational resources required for full fine-tuning (FFT), i.e., fine-tuning all billions of parameters. A solution to this has been the development of more selective adaptation techniques, involving modifying only a portion of the parameters or integrating external modules designed for new tasks. Key methodologies in this sphere include LoRA [18], Adaptors [37, 17, 31], and many other variants [25, 24, 9, 14, 53], all part of what can be generally termed as Parameter-Efficient Fine-tuning (PEFT). PEFT strategies are characterized by freezing the backbone model parameters while only a minimal number of task-specific parameters are introduced and fine-tuned. This method substantially boosts efficiency in the phases of fine-tuning and subsequent deployment, marking a significant advancement in the practical use of LLMs.

While fine-tuning a small subset of parameters offers a streamlined approach for domain adaptation, it’s well-recognized that model performance is closely tied to the number of parameters involved [22]. This intrinsic characteristic of methods like LoRA often results in them falling short of the FFT baseline, which updates all parameters, thereby creating a trade-off between efficiency and model quality. This issue of compromised quality in a low-parameter setting becomes even more pronounced in target domains characterized by complex sub-domains and diverse tasks. This situation presents a compelling research question:

38th Conference on Neural Information Processing Systems (NeurIPS 2024).

![](images/362f0e488aa11031e305ebdd60d285e1f7e5ce5b5b49b14ecc2f2446f7d64af6.jpg)  
Figure 1: Illustration of LoRA architecture changes in HydraLoRA. Only the tunable parameters are shown in this Figure. (a) LoRA architecture with matrix A to achieve low rank and matrix $\mathbf { B }$ tAou recover. (b) under the same parameter count, a monolithic LoRA is split into multiple smaller A and B matrices to avoid training interference. (c) based on (b), HydraLoRA has an asymmetric structure that has a shared A matrix and multiple B matrices.

What is the optimal architecture that can deliver superior model performance while still capitalizing on the efficiency benefits of a reduced parameter footprint?

In our research, we carry out a series of exploratory experiments, applying LoRA to the LLaMA2 [48] model to adapt it to a new domain encompassing multiple downstream tasks. As shown in Figure 1(a), LoRA adds trainable pairs of rank decomposition matrices A and B in addition to existing weight matrices. Our in-depth analysis of LoRA’s mechanics yields several insightful observations and leads to the formulation of key hypotheses. First, rather than employing a single LoRA for the entire domain, it proves more effective to deploy multiple, smaller LoRA heads, each dedicated to a specific downstream task (see Figure 1(b)). This suggests that domain or task interference might harmfully impact the training process. We further hypothesize that this interference originates from “intrinsic components”—sub-domains or distinct tasks—potentially unknown even to domain experts. Additionally, upon visualizing the parameters of LoRA, we discern a pattern: some parameters predominantly learn the commonalities across all data, while others focus on the unique aspects of each intrinsic component. From these observations, we posit that an optimal LoRA architecture should embody an explicit, asymmetric structure.

Building upon the observations, we propose an improved end-to-end LoRA framework, which we refer to as HydraLoRA. From the architecture perspective, unlike LoRA’s symmetric structure, HydraLoRA has an asymmetric structure that has a shared A matrix and multiple B matrices (see Figure 1(c)). The shared A matrix is used by all samples for parameter efficiency. During the fine-tuning phase, HydraLoRA is designed to auto-identify “intrinsic components” and segregate training samples into distinct B matrices. During the inference phase, HydraLoRA leverages multiple B matrices using Mixture-of-Experts (MoE; [20, 40]) manner. Unlike prior work, HydraLoRA completely eliminates the need for human expertise and assumptions, showing better performance than using domain knowledge to guide the fine-tuning process.

# 2 Background and Motivation

# 2.1 LoRA Basics

LoRA [18] achieves comparable performances to fine-tuning on many benchmarks by freezing the pre-trained model weights $W _ { 0 }$ and inserting trainable rank decomposition matrices into each layer of the pre-trained model. In particular, for each layer, LoRA uses two sequential low-rank matrices $A$ and $B$ to fit the residual weights for adaptation. The forward computation is written as follows:

$$
y \prime = y + \Delta y = W _ { 0 } x + B A x
$$

where $y \in R ^ { \textup { d } }$ is the output and the $x \in R ^ { \textup { k } }$ denotes the input. $B \in { \cal R } ^ { \mathrm { \tiny ~ d \times r } } , A \in { \cal R } ^ { \mathrm { \tiny ~ r \times k } }$ with $r \ll m i n ( d , k )$ . Normally matrix $B$ is initialized with zeroes and matrix $A$ is initialized with Kaiming Uniform [15] to force $\Delta y = 0$ at the beginning.

# 2.2 LoRA’s Practical Dilemma

Parameter count has a clear impact on the performance of neural models [22, 33]. Yet, ParameterEfficient Fine-tuning (PEFT) methods, such as Adapter [17] and prefix-tuning [25], focus on finetuning a limited set of parameters. These approaches present a practical dilemma: while restricting the number of tuned parameters is essential for training efficiency, it hinders the model’s ability to learn from diverse datasets. This trade-off becomes particularly evident when considering corpus heterogeneity [2]. Figure 2 reveals a notable performance disparity between PEFT techniques and full fine-tuning (FFT), with the gap widening in scenarios involving a more diverse or heterogeneous training corpus.

![](images/0927062bf99ec1235e9dba71ef95d7a87a118d766c55ada5a7f2814e13330ff5.jpg)  
Figure 2: Performance impact of corpus heterogeneity on full fine-tuning vs. parameter-efficient fine-tuning. Heterogeneity signifies the diversity within the dataset, often leading to interference due to its varied content and style [2]. Parameter-efficient approaches are particularly sensitive, suffering greater performance losses in heterogeneous cases.

Table 1: Performance on instruction tuning with Dolly-15K [8] and evaluated with MMLU [16] with different ranks. For LoRA (Split) decomposes highrank LoRA modules into smaller, equivalent lowrank components $( r \times n )$ . $n$ is the number of LoRAs, $r$ denotes the rank of each LoRA.   

<html><body><table><tr><td>Schemes</td><td>rxn</td><td>MMLU↑</td><td>%Parameter</td></tr><tr><td>LoRA</td><td>8×1</td><td>43.22</td><td>0.062</td></tr><tr><td>LoRA</td><td>16×1</td><td>45.45</td><td>0.124</td></tr><tr><td>LoRA</td><td>32×1</td><td>46.59</td><td>0.248</td></tr><tr><td>LoRA (Split)</td><td>16×2</td><td>46.82</td><td>0.248</td></tr><tr><td>LoRA (Split)</td><td>8×4</td><td>46.94</td><td>0.248</td></tr><tr><td>LoRA (Split)</td><td>4×8</td><td>46.83</td><td>0.248</td></tr></table></body></html>

# 2.3 Observations

In this work, we aim for a PEFT approach that strikes a better balance between maximizing the learning capability for heterogeneous data and minimizing the number of parameters involved. A key goal is to ensure that our enhanced technique exhibits robust generalization across unseen tasks, independent of any prior task-specific knowledge. To achieve our objectives, we focus on LoRA and conduct a series of experiments as Table 1 to gain a deeper understanding of its mechanisms. Our methodology involves leveraging data from diverse tasks within a domain, and training distinct LoRA heads for each domain, leading to our first observation:

Observation I: With the same parameter count, rather than employing a single LoRA for the entire domain dataset, it proves more effective to deploy multiple, smaller LoRA heads, each dedicated to a specific downstream task.

This suggests that interference among tasks might harmfully impact the training process. Furthermore, we posit that this interference is NOT exclusive to this explicit multi-task training. This interference could happen in any training setting since all datasets inherently consist of multiple implicit intrinsic components, such as sub-domains or tasks within a domain that is even unknown to domain experts. To better understand how multiple LoRA heads mitigate the interference among intrinsic components, in Figure 3, we employ the t-SNE technique [49] to visualize the parameters of matrix A and B across all heads. This analysis yields another critical observation:

Observation II: When multiple LoRA heads are trained individually on different data, the parameters of matrix A from different heads tend to converge, while those of matrix B are distinguishable.

In detail, the parameters of matrix A across all heads exhibit a high degree of similarity, leading to their overlaps in the figure. Conversely, the parameters of matrix B from different heads are distinct and easily distinguishable. We posit that this divergence is an artifact of the initialization schemes, with matrix A inclined toward capturing commonalities across domains, while matrix B adapts to domain-specific diversities. The distinction between matrix A and B offers valuable insights for enhancing both parameter efficiency and effectiveness. From an efficiency standpoint, our hypothesis suggests that the parameters of matrix A could potentially be shared across multiple heads, thereby reducing redundancy. Regarding effectiveness, since the parameters of matrix B of different heads

t-SNE Visualization A-Mat LoRAs - LoRA 100 22 64 60 66 5444 72 A 4QA SUM 100 60 66 22 64 5444 72 9 31424 QA SUM 75 50 89 ★ 29 SUM QA 50 3 IE 50 40 12160 101022 ★ 6 IE 104 18 4 116 0423 8ALL 104 11618584670 102 865 8ALL 25 AL3L7 65 0 52 20 91 31310 92 120 118 0 52 20 16 80 98 32 682 14 92 120 2 118 0 ★ 101 8399 9 76 56 10168023 14 8851712 76 56 106 0 30 96 114 88112 25 9 273 50 50 110 24 6 36 894 7 103 57 42 63 62 38 108 84 28 42 62 38 108 84 28 −50 \* 63 . 81 85 45 1 100 100 75 −100 −50 0 50 100 −100 −50 0 50 100 −50 0 50

Figure 3: Breakdown analysis of LoRA modules. Compare fine-tuned LoRA modules of Dolly-15K [8] with three subtasks of Dolly-15K including “summarization $( S u m ) ^ { \ast }$ , “closed $Q A$ $( { \cal Q A } ) ^ { \prime }$ ” and “information extraction (IE)” using t-SNE. Consider LLaMA2-7B (random seed $\scriptstyle = 4 2$ ), which contains 32 decoder layers, corresponding to 32 adaptive modules. Each module consists of {0: q_proj of A, 1: q_proj of B, 2: v_proj of A, 3: v_proj of B} submodules. This makes a total of $3 2 \times 4$ submodules. Left displays all submodules. Center shows all even submodules, i.e. the A matrix. Right represents all odd submodules, i.e. the B matrix. It can be seen that the differences in the fine-tuned LoRA modules for different tasks arise mainly from the B matrix.

are dispersed, suggesting that using a single head to adapt to multiple domains might be less effective than using individual heads for each domain, which minimizes the interference between domains.

Building upon our observations, we propose an optimized LoRA architecture designed to enhance cost-effectiveness. In this architecture, we share the parameters of A matrix across various subdomains or tasks to improve parameter efficiency, while deploying multiple B matrices, each tailored to handle different intrinsic components. This design allows for a more effective adaptation to the specific characteristics of each component. While these intrinsic components can be manually identified using prior knowledge of the training data, we also introduce end-to-end methods using Mixture-of-Experts (MoEs) [21], which will be detailed in the methodology section. This automatic approach facilitates flexibility and applicability, particularly in scenarios where prior knowledge is limited or unavailable.

# 3 HydraLoRA

In this section, we introduce the proposed HydraLoRA, an asymmetric LoRA architecture for efficient fine-tuning, as illustrated in Figure 1. After that, we show the workflow of HydraLoRA as Figure 4.

# 3.1 Asymmetric LoRA architecture

The LoRA method updates two low-rank matrices $A$ and $B$ , and uses $A B$ as the change of a pretrained and frozen weight $W _ { 0 }$ of a linear layer as shown in Eq. 1. The integral parameters are fine-tuned for the whole corpus in the original LoRA, which causes difficulty in learning the various knowledge aspects. Drawing from a detailed breakdown analysis of LoRA, a potential solution is to segment the entire LoRA into “Hydra” structured LoRA variants, that is, characterized by a central shared matrix $A$ and several distinct matrices $B$ , fostering a blend of shared knowledge and specialized functionalities. As Figure 1, HydraLoRA is to fine-tune LoRAs to achieve robust performance without redundancy, thereby benefiting the entire heterogeneous corpus. The asymmetric LoRA architecture can be formulated as:

$$
\begin{array} { l } { { \displaystyle W = W _ { 0 } + \Delta W } } \\ { ~ = W _ { 0 } + \sum _ { i = 1 } ^ { N } \omega _ { i } \cdot B _ { i } A } \end{array}
$$

The matrics $B _ { i } \in \mathbb { R } ^ { d \times r }$ and shared $A \in \mathbb { R } ^ { r \times k }$ . The hyper-parameter $N$ denotes the number of $B$ matrices. The term $\omega _ { i }$ modulates these contribution weights for head $B _ { i }$ .

# 3.2 Workflow of HydraLoRA

Figure 4 illustrates the workflow of HydraLoRA. Initially, HydraLoRA delves into the adaptive identification and initialization of LoRA modules within a heterogeneous corpus, aligning them with

B. Inference Mixed-task Input   
自自自 1. Initialization B1 B2 BN √ 1 自 Identify intrinsic 南 Attention A   
Heterogeneous components LN ?? ?? ?? Corpus Initialized HydraLoRA ↓ ∆?? ∆?? ∆?? Router FNN ↓ ↓ 2. Tuning B1 B2 BN LN X 3. Inference Pretrained Routing LoRA Merge Segregate training samples to intrinsic A Weights ? Router components by MoE Trained HydraLoRA Output

task relevance through the application of $k$ -means or developer-specified size. Subsequently, we propose a Mixture-of-Experts (MoE) framework that handles $B$ matrices as expert adapters to ensure computational efficiency throughout the fine-tuning (Section 3.2.1) and inference (Section 3.2.2) stages by freezing the rest of the LLM parameters. During inference, it flexibly and dynamically merges multiple $B$ matrices through the MoE router.

# 3.2.1 Fine-tuning

Motivated by Mixture-of-Experts (MoEs; [20, 40]), where experts are selectively activated by a gating mechanism (Router) in response to different inputs. In HydraLoRA, we substitute each expert with a lightweight LoRA adapter. During fine-tuning, while weights of LLMs remain frozen, the experts and router layers are trained from scratch. In order to achieve a unified approach to the distinct forward processes of multiple $B$ matrices, we define a set of experts, denoted as $( E _ { 1 } , \dots , E _ { N } )$ , to learn the updated matrix $\Delta W$ . As HydraLoRA fine-tunes the experts using the heterogeneous corpus, the shared matrix $A$ inherently captures collaborative knowledge to augment intra-gains, and different matrices $B$ foster knowledge modularity to mitigate fine-tuning inter-offsets. Based on this structure, the forward process of HydraLoRA is expressed as:

$$
y = W _ { 0 } x + \sum _ { i = 1 } ^ { N } \omega _ { i } E _ { i } A x \quad ( M o E )
$$

where $N$ denotes the number of experts, i.e., $B$ matrices.To regulate these contributions, we introduce a gate function (router network) commonly consisting of a dense layer with trainable weights (transformation matrix) $W _ { g } \in \mathbb { R } ^ { r \times N }$ followed by a softmax function which takes an intermediate token representation $x$ as input and combines the output of each expert based on the gating scores $( \omega _ { 1 } , \dots , \omega _ { N } )$ :

$$
\omega _ { i } = \mathsf { s o f t m a x } ( W _ { g } ^ { T } x ) \quad ( R o u t e r )
$$

# 3.2.2 Inference

During inference, HydraLoRA merges adapters by enabling routing computation based on the input. Specifically, since matrices B operate as linear functions, we initially compute a weighted average of the experts. Following this, we apply a PEFT transformation using the combined expertise. The HydraLoRA significantly enhances training efficiency through an extremely parameter-efficient MoE formulation. Additionally, the intrinsic structural modularity of HydraLoRA facilitates rapid recovery and merging of the trained parameters during inference, leading to substantial memory savings.

Table 2: Comparative performance of different tuning schemes across multiple benchmarks on a single domain. 8-shot for GSM8K, zero-shot for others. $\# { \bar { B } }$ refers to the average $B$ matrix number.   

<html><body><table><tr><td rowspan="2">Schemes</td><td rowspan="2">MMLU</td><td rowspan="2">Medical</td><td rowspan="2">Law</td><td colspan="2">P@manP@al0</td><td rowspan="2">GSM8K</td><td rowspan="2">%Param</td><td rowspan="2">#A</td><td rowspan="2">#B</td></tr><tr><td></td><td></td></tr><tr><td>LLaMA2-7B [48]</td><td>38.88</td><td>35.98</td><td>33.51</td><td>13.10</td><td>20.34</td><td>10.38</td><td></td><td></td><td></td></tr><tr><td>Full Fine-Tuning</td><td>49.91</td><td>46.78</td><td>46.08</td><td>20.24</td><td>32.93</td><td>25.70</td><td>100</td><td></td><td>1</td></tr><tr><td>Prompt Tuning [24]</td><td>39.91</td><td>37.59</td><td>35.02</td><td>13.66</td><td>21.55</td><td>13.18</td><td>0.001</td><td></td><td></td></tr><tr><td>P-Tuning(256) [29]</td><td>41.11</td><td>39.81</td><td>36.72</td><td>13.60</td><td>21.13</td><td>15.56</td><td>0.193</td><td></td><td></td></tr><tr><td>Prefix Tuning [25]</td><td>41.78</td><td>40.28</td><td>36.54</td><td>13.23</td><td>22.56</td><td>16.89</td><td>0.077</td><td></td><td></td></tr><tr><td>(IA)³ [27]</td><td>40.45</td><td>37.12</td><td>35.25</td><td>13.54</td><td>23.17</td><td>13.98</td><td>0.009</td><td></td><td></td></tr><tr><td>AdaLoRA(r=8) [55]</td><td>44.32</td><td>42.83</td><td>39.36</td><td>14.81</td><td>23.78</td><td>19.51</td><td>0.093</td><td>1</td><td>1</td></tr><tr><td>LoRA(r=8)[18]</td><td>43.22</td><td>41.59</td><td>37.85</td><td>15.67</td><td>22.95</td><td>18.24</td><td>0.062</td><td>1</td><td>1</td></tr><tr><td>LoRA(r=16)</td><td>45.45</td><td>43.10</td><td>39.64</td><td>16.71</td><td>25.60</td><td>20.32</td><td>0.124</td><td>1</td><td>1</td></tr><tr><td>LoRA(r=32)</td><td>46.59</td><td>44.32</td><td>40.81</td><td>17.12</td><td>25.89</td><td>20.67</td><td>0.248</td><td>1</td><td>1</td></tr><tr><td>LoRA-Split(4×8)</td><td>46.94</td><td>45.28</td><td>41.35</td><td>18.20</td><td>26.85</td><td>21.92</td><td>0.248</td><td>4</td><td>4</td></tr><tr><td>HydraLoRA(r=8)</td><td>47.22</td><td>45.71</td><td>42.18</td><td>18.31</td><td>27.43</td><td>22.27</td><td>0.124</td><td>1</td><td>3</td></tr></table></body></html>

# 4 Experiments

In this section, we detail the principal experiments. We begin with an overview of the experimental setup and implementation intricacies. Following this, we share our findings and offer a succinct interpretation.

# 4.1 Experiment Setting

Dataset and Benchmarks To explore the properties and commonalities of the LoRA asymmetric structure, we conduct experiments on both single and multiple domains to evaluate the effectiveness of HydraLoRA for profiling intrinsic components. Single domain. 1) General: we fine-tune with the general instruction tuning databricks-dolly-15k [8] for generic language capability and evaluate with MMLU [16]. 2) Medical: we fine-tune with GenMedGPT and clinic-10k from ChatDoctor [26] for medicine applications and evaluate medical tasks in MMLU. 3) Law: we fine-tune with two legal instruction tuning datasets Lawyer-Instruct [1] and US-Terms [4] then evaluate with law tasks in MMLU. 4) Math: we fine-tune with the training split of GSM8K [7] for mathematical reasoning and evaluate with test set of GSM8K. 5) Code: we fine-tune with CodeAlpaca [5] for code generation and evaluate with HumanEval [6]. Multi-task domain. We select a portion of the Flanv2 [51] datasets covering Natural Language Understanding (NLU) and Natural Language Generation (NLG), which can be grouped into 10 distinct task clusters. Then we evaluate it with the Big-Bench Hard (BBH) [45] benchmark. A detailed description of the benchmarks can be found in Appendix A.1.

Baselines First, we compare HydraLoRA with different PEFT methods on single datasets: 1) Full fine-tuning; 2) Prompt Tuning [24]; 3) $P$ -Tuning [29]; 4) Prefix Tuning [25]; 5) $I A ^ { 3 }$ [27]; 6) AdaLoRA [55]. Second, we extend the experiments exploring HydraLoRA on multiple datasets compared with more weighted average methods: 1) Lorahub [19] employs black-box optimization to learn weights of 20 randomly selected LoRAs for new tasks, using weighted averaging without needing gradient calculations. 2) LoRA MoE [52] combines lightweight experts (LoRA) with MoE architecture for high efficiency, generalizing to new tasks without prior knowledge. A detailed description of the baseline models can be found in Appendix A.2.

# 4.2 Overall Performance

The experimental results of HydraLoRA and the competing baselines are presented in Table 2 with a single domain and Table 3 with the mixed task domain. The evaluation of diverse tasks demonstrates that HydraLoRA consistently outperforms all other schemes. The performances rooted in LoRA outperform those of conventional PEFT methodologies. Compared to the default single LoRA configuration (rank $\scriptstyle = 8$ ), the Hydra architecture, enriched by the integration of several B matrices, effectively addresses the inherent conflicts among intrinsic components of the corpus. Furthermore, with equivalent parameters (rank ${ \boldsymbol { \mathbf { \mathit { \sigma } } } } = 1 6$ ), the model shows superior performance, confirming the effectiveness of the adopted parameters. Based on Table 2 and Table 3, we propose three research questions that confirm the aforementioned observations.

Table 3: Comparative performance of different tuning schemes, including base model (Base), LoRA tuning (LoRA), LoraHub learning, multi-LoRA tuning with MoE inference (LoRA MoE) and our proposed HydraLoRA learning across mix-task domain on the BBH benchmark with LLaMA2-7B, LLaMA2-13B as the base LLM (3-shot).   

<html><body><table><tr><td colspan="2">Metrics</td><td>Base [48]</td><td>LoRA [18]</td><td>Lorahub [19]</td><td>LoRA MoE [52]</td><td>HydraLoRA</td></tr><tr><td rowspan="2">Performance</td><td>7B</td><td>31.6</td><td>36.8</td><td>39.7</td><td>40.3</td><td>41.5</td></tr><tr><td>13B</td><td>38.4</td><td>40.1</td><td>41.9</td><td>43.7</td><td>44.1</td></tr><tr><td colspan="2"># of A/B for training</td><td>0/0</td><td>1/1</td><td>48/48</td><td>48/48</td><td>1/10</td></tr><tr><td colspan="2"># of A/B for inference</td><td>0/0</td><td>1/1</td><td>20/20</td><td>48/48</td><td>1/10</td></tr><tr><td colspan="2">% Params</td><td>-</td><td>0.062</td><td>1.240</td><td>2.976</td><td>0.341</td></tr></table></body></html>

RQ1: Is it more effective to use multiple smaller LoRA heads for specific tasks rather than one single LoRA for the entire domain dataset, given the same parameter count? Comparing the high-dimensional LoRA configuration with $r = 3 2$ against a segmented version using LoRA-Split, a variant introduced by HydraLoRA, which divides the model into four distinct components each with $r = 8$ . That is, multiple vanilla LoRAs are directly utilized to capture the differences between data. We observe a noteworthy trend in the performance across a variety of tasks as detailed in Table 2. It illustrates the superior performance of LoRA-Split in comparison to the traditional LoRA approach, across all the evaluated scenarios. This enhancement in performance is a strong indication of the detrimental impact that task interference can have on the training process. By segregating the tasks into discrete components, LoRA-Split effectively minimizes the conflict and interference between tasks, thereby promoting a more efficient and focused environment.

The concept of LoRA-Split hinges on the construction of different intrinsic component compositions, employing LoRA as a foundational technique to strategically mitigate the interference conflict. This architectural innovation has proven to be a pivotal factor in enhancing model performance. However, it’s important to note that while LoRA-Split marks a significant advancement in model efficiency and task handling, it also introduces a certain level of parameter redundancy. The segmented approach of LoRA-Split inevitably leads to an increase in the overall model parameters, which can be manifold in comparison to the traditional, singular LoRA model. This increase in parameters, while contributing to the model’s robustness and capability to handle multiple tasks simultaneously, also poses new challenges in terms of computational resources and model optimization.

RQ2: Will multiple LoRA heads, individually trained on different data, improve efficiency by distinguishing matrix B parameters? We evaluated the Hydra structure LoRA — HydraLoRA that is characterized by a shared LoRA A matrix, while maintaining distinct B matrices that are trained separately. This configuration was meticulously compared with both the standard LoRA and the LoRA-Split approaches, emphasizing efficiency parameters.

According to the results presented in Table 2, unlike split which straightforwardly adopts multiple vanilla LoRAs, HydraLoRA adopts an asymmetric LoRA structure that not only improves parameter efficiency by separating the uses of A matrix for commonalities and B matrices for diversities with a notably smaller adapter parameter set, but also employs a trainable router to improve the composition of multiple B matrices that outperforms the LoRA-Split approach. This finding is significant as it suggests that HydraLoRA not only enhances performance efficiency but also boosts overall system effectiveness. This may be driven by 1) different B matrices capturing different features of the data-intrinsic knowledge, mitigating mutual interferences, and avoiding performance offsets. 2) Module A maintains the collaborative knowledge by taking the strengths of each and integrating them to improve the model performance.

RQ3: How does HydraLoRA fare against other merge methods in complex, multi-task domains, considering scalability and robustness? While we hypothesize that the asymmetry is mainly rooted in the different initialization methods of A and B matrices, it is possible that this behavior varies on different model architectures and datasets. Recent work confirms similar empirical observations [54, 13]. To the best of our ability, we extended the experiments exploring HydraLoRA on multiple datasets. LoRA MoE and their variants typically aim at tackling multi-tasks by employing multiple independent LoRAs. This makes them suitable for handling various domains. However, for a single dataset like ours, a “default” MoE method might not be optimal. HydraLoRA addresses this by constructing asymmetric structures and utilizing multiple B matrices to capture the specific nuances within the single dataset. The effectiveness of this approach is demonstrated by the experimental results in Table 3.

![](images/aa13acb7958693c3fe8670d23fc4b5d21fe7ec9b7eac6ea4fc75dd3f8e83e661.jpg)  
Figure 5: Energy consumption and latency during fine-tuning with different LoRA approaches (fine-tuning LLaMA2-7B with GSM-8K).

![](images/bd560038486d9c634f7c42a3d21e54604b349a7d479a7af30625162541eda263.jpg)  
Figure 6: Comparative performance of ablation study for HydraLoRA across multiple benchmarks.

# 4.3 Energy and Throughput Analysis

RQ4: How does the “Hydra” structure in HydraLoRA enhance system efficiency, particularly in reducing training energy consumption and latency? We evaluate the system efficiency of HydraLoRA from two perspectives: training energy consumption and latency. The following experiments were executed on a GPU infrastructure consisting of 4 NVIDIA A40 GPUs and a CPU powered by an Intel(R) Xeon(R) Gold 6330 CPU clocked at 2.00GHz. Power consumption measurements were recorded using CodeCarbon [34]. Figure 5 shows the results of various fine-tuning approaches for GSM-8K using the LLaMA2-7B model. we can see that HydraLoRA effectively speeds up the training process $1 . 9 6 \times$ and reduces $4 9 . 6 \%$ energy cost compared to LoRA (rank $\scriptstyle = 3 2$ ). While the energy consumption and latency of LoRA-Split exceeds the LoRA (rank $_ { = 3 2 }$ ). This is for the reason that HydraLoRA jointly considers inherent knowledge modularity and collaboration, which utilizes the “Hydra” structure with a shared A matrix and different B matrix. In this way, it only employs rank $= 1 6$ training overhead but expands to a performance enhancement of more than rank $_ { = 3 2 }$ . Overall, this experiment demonstrates the parameter effectiveness of HydraLoRA.

# 4.4 Ablation Study

RQ6: What impact do the MoE architecture and the gate function have on the fine-tuning process? To delve deeper into understanding the contributions of each component in HydraLoRA. we present the results of our ablation study in Figure 6. The variant w/o MoE (essentially reverts to LoRA) excludes the MoE architecture. Similarly, the w/o gate variant employs uniform expert weights bypassing the gate function. The w/o hydra adopts multiple vanilla LoRAs in a straightforward way. Figure 6 indicates that the full HydraLoRA model outperforms its variants, showing that both the MoE architecture and gate function significantly contribute to its effectiveness across various language understanding domains.

![](images/a06dfe7d8991a8b5566c9892492a3316c5668b057f7f59aa54b8c0e2214e8b21.jpg)  
Figure 7: Number of clusters generated by different approaches including developer-specific (static), $\mathbf { k }$ -means, and DBSCAN.

![](images/fb5e586d4c4ad93f80d09c8b23007d76d21e6a991a7f2b8125d71024d678316c.jpg)  
Figure 8: The results of experiments for hyper-parameters number of clusters.

# 4.5 Hyper-parameter Analysis

RQ7: How do the number of intrinsic component of HydraLoRA influence performance outcomes? As Figure 8 shown, we conduct a comprehensive and meticulous analysis by fine-tuning the Dolly-15K model on the LLaMA2-7B dataset and subsequently evaluating its performance on the MMLU benchmark to rigorously examine the impact of variations in the intrinsic component, symbolized by the variable $N$ , on the model’s overall performance. Empirically we find that the number N of clusters is not a sensitive parameter for HydraLoRA, with a wide range of reasonable number N of clusters (e.g. 2 to 4) performing decently well in all settings in our experiments. Specifically, the performance loss of ${ \Nu } = 3$ vs. the optimal $\Nu = 4$ is only $0 . 4 2 \%$ . Meanwhile, as illustrated in Figure 7, we employ three distinct methods to generate the number of corpus clusters 15-fold, and the results demonstrate that the $\mathbf { k }$ -means [30] yields comparable outcomes with DBSCAN [39]. Therefore, based on this observation, we choose $\mathbf { k }$ -means because it is simple but effective, more sophisticated hyperparameter search approaches (e.g. DBSCAN, parameter sweep and Bayesian optimization) will be unnecessarily costly. It’s noteworthy that HydraLoRA is adeptly designed to orchestrate its components in a way that it can automatically calibrate and navigate toward the optimal performance configuration across various parameters. This intelligent auto-tuning is achieved through the application of the $\mathbf { k }$ -means clustering algorithm. This strategic component orchestration not only enhances performance but also ensures a more efficient and effective utilization of resources, underpinning the model’s capability to adapt and perform efficiently in a dynamic computational environment.

# 5 Related work

Parameter-Efficient Fine-tuning LLMs are becoming increasingly powerful, but fine-tuning them often requires significant computational resources. This has spurred research on parameterefficient fine-tuning (PEFT) techniques that reduce memory and storage costs during adaptation. One prominent PEFT approach is adapters [17, 37]. It introduces new, trainable dense layers within the existing model, keeping the original parameters frozen. This concept has proven successful across various domains [35, 42, 43, 56]. Further improvements on adapter compactness involve constructing parameter matrices using Kronecker products of low-rank matrices [31]. Another PEFT strategy directly manipulates activations with learned vectors. This can be achieved through concatenation [29, 25, 24], multiplication $\mathrm { \Delta [ A ^ { 3 } }$ ; [27]), or addition (BitFit; [53]). Prefix-tuning [25] and prompt-tuning [24] are noteworthy examples that fine-tune continuous prompts instead of designing discrete ones [9]. Interestingly, a study suggests that many PEFT methods can be viewed as a form of adapter, providing a unified perspective [14]. Beyond adding new parameters or altering the computational graph, researchers also explore sparse [12, 44, 46] or low-rank updates (LoRA; [18]).

Multi-LoRA Architecture LoRA has notably garnered increasing interest recently, becoming a standard approach for adapting LLMs such as LLaMA [47, 48] under limited computational resources. Recognizing its potential, researchers have delved deeper, exploring the benefits of employing multiple LoRAs. LoraHub [19] takes this multi-LoRA approach by training several adapters and strategically picking combinations based on the domain during inference. Meanwhile, MultiLoRA [50] focuses on horizontal scaling, aiming to reduce LoRA’s parameter dependence. This involves splitting LoRA modules along the rank dimension and introducing learnable scaling factors for enhanced expressiveness. Addressing scaling challenges from a different angle, the mixture of LoRA concept is further proposed [52]. This mitigates resource consumption when scaling instruction-tuned LLMs. Recognizing the potential for conflict during instruction tuning, LoRAMoE [11] leverages the Mixture-of-Experts (MoEs; [20]) structure to safeguard the pretrained LLM’s knowledge from excessive corruption by instruction data. Similarly, MOELoRA [28] incorporates a MoE framework into LLMs, thereby improving their multitasking capabilities in the medical domain. Shifting the focus to the system perspective, S-LoRA [41] provides a framework for efficiently serving multiple LoRA adapters. Unlike previous methods that relied on choosing LoRA combinations based on their training domains, HydraLoRA breaks free from the dependence on domain knowledge during inference. Additionally, HydraLoRA’s asymmetric structure further enhances parameter efficiency compared to existing symmetric approaches.

# 6 Conclusion

In this work, we start by conducting exploratory experiments applying the LoRA technique to LLaMA2, aiming to adapt it to a new domain across various tasks. This study unveils the limitations of employing a single LoRA for the entire domain, highlighting the detrimental effects of domain interference. In response, we introduce a novel architecture HydraLoRA that features an asymmetric structure with a shared matrix for all samples and distinct matrices for each intrinsic component. This design improves domain adaptation by selectively focusing on distinct components, enhancing both fine-tuning and inference efficiency. Our research highlights the importance of balancing learning capabilities for diverse datasets against the need for a lean model, offering a viable pathway for improving LLMs with minimal parameter growth. More discussion about limitation and broader impacts are available in Appendix D and E.