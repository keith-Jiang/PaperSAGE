# Improved Distribution Matching Distillation for Fast Image Synthesis

Tianwei Yin1 Michaël Gharbi2 Taesung Park2 Richard Zhang2 Eli Shechtman2 Frédo Durand1 William T. Freeman1

1Massachusetts Institute of Technology 2Adobe Research

https://tianweiy.github.io/dmd2/

# Abstract

Recent approaches have shown promises distilling expensive diffusion models into efficient one-step generators. Amongst them, Distribution Matching Distillation (DMD) produces one-step generators that match their teacher in distribution, i.e., the distillation process does not enforce a one-to-one correspondence with the sampling trajectories of their teachers. However, to ensure stable training in practice, DMD requires an additional regression loss computed using a large set of noise–image pairs, generated by the teacher with many steps of a deterministic sampler. This is not only computationally expensive for large-scale text-to-image synthesis, but it also limits the student’s quality, tying it too closely to the teacher’s original sampling paths. We introduce DMD2, a set of techniques that lift this limitation and improve DMD training. First, we eliminate the regression loss and the need for expensive dataset construction. We show that the resulting instability is due to the “fake” critic not estimating the distribution of generated samples with sufficient accuracy and propose a two time-scale update rule as a remedy. Second, we integrate a GAN loss into the distillation procedure, discriminating between generated samples and real images. This lets us train the student model on real data, thus mitigating the imperfect “real” score estimation from the teacher model, and thereby enhancing quality. Third, we introduce a new training procedure that enables multi-step sampling in the student, and addresses the training–inference input mismatch of previous work, by simulating inference-time generator samples during training. Taken together, our improvements set new benchmarks in onestep image generation, with FID scores of 1.28 on ImageNet- $6 4 \times 6 4$ and 8.35 on zero-shot COCO 2014, surpassing the original teacher despite a $5 0 0 \times$ reduction in inference cost. Further, we show our approach can generate megapixel images by distilling SDXL, demonstrating exceptional visual quality among few-step methods, and surpassing the teacher. We release our code and pretrained models.

# 1 Introduction

Diffusion models have achieved unprecedented quality in visual generation tasks [1–8]. But their sampling procedure typically requires dozens of iterative denoising steps, each of which is a forward pass through a neural network. This makes high resolution text-to-image synthesis slow and expensive. To address this issue, numerous distillation methods have been developed to convert a teacher diffusion model into an efficient, few-step student generator [9–20]. However, they often result in degraded quality, as the student model is typically trained with a loss to learn the pairwise noise-to-image mapping of the teacher, but struggles to perfectly mimic its behavior.

38th Conference on Neural Information Processing Systems (NeurIPS 2024).

![](images/b44b82bff94d800361bf690e887e3883f3bdd2827366075e9ce1bf73e7374e32.jpg)  
Figure 1: $1 0 2 4 \times 1 0 2 4$ samples produced by our 4-step generator distilled from SDXL. Please zoom in for details.

Nevertheless, it should be noted that loss functions aimed at matching distributions, such as the GAN [21] or the DMD [22] loss, are not burdened with the complexity of precisely learning the specific paths from noise to image because their goal is to align with the teacher model in terms of distribution—by minimizing either a Jensen-Shannon (JS) or an approximate Kullback-Leibler (KL) divergence between the student and teacher output distributions.

In particular, DMD [22] has demonstrated state-of-the-art results in distilling Stable Diffusion 1.5, yet it remains less investigated than GAN-based methods [23–29]. A likely reason is that DMD still requires an additional regression loss to ensure stable training. In turn, this necessitates creating millions of noise-image pairs by running the full sampling steps of the teacher model, which is particularly costly for text-to-image synthesis. The regression loss also negates the key benefit of DMD’s unpaired distribution matching objective, because it causes the student’s quality to be upper-bounded by the teacher’s.

In this paper, we show how to do away with DMD’s regression loss, without compromising training stability. We then push the limits of distribution matching by integrating the GAN framework into DMD, and enable few-steps sampling with a novel training procedure, which we termed ‘backward simulation’. Taken together, our contributions lead to state-of-the-art fast generative models that outperform their teacher, using as few as 4 sampling steps. Our method, which we call DMD2, achieves state-of-the-art results in one-step image generation, setting a new benchmark with FID scores of 1.28 on ImageNet- $6 4 \times 6 4$ and 8.35 on zero-shot COCO 2014. We demonstrate our approach’s scalability by distilling from SDXL to produce high-quality megapixel images, establishing new standards among few-step methods.

In short, our contributions are as follows:

• We propose a new distribution matching distillation technique that does not require a regression loss for stable training, thereby eliminating the need for costly data collection, and allowing for more flexible and scalable training.   
• We show that training instability in DMD [22] without regression loss stems from an insufficiently trained fake diffusion critic, and implement a two time-scale update rule to address this issue.   
• We integrate a GAN objective into the DMD framework, where the discriminator is trained to distinguish samples from the student generator vs. real images. This additional supervision operates at the distribution level, which better aligns with DMD’s distribution-matching philosophy than the original regression loss. It mitigates approximation errors in the teacher diffusion model and enhances image quality.   
• While the original DMD only supports one-step students, we introduce a technique to support multi-step generators. Unlike previous multi-step distillation methods, we avoid the domain mismatch between training and inference by simulating inference-time generator inputs during training, thus improving overall performance.

# 2 Related Work

Diffusion Distillation. Recent diffusion acceleration techniques have focused on speeding up the generation process through distillation [9, 10, 13–20, 22, 23, 30]. They typically train a generator to approximate the ordinary differential equation (ODE) sampling trajectory of a teacher model, in fewer sampling steps. Notably, Luhman et al. [16] precompute a dataset of noise and images pairs, generated by the teacher using an ODE sampler, and use it to train the student to regress the mapping in a single network evaluation. Follow-up works like Progressive Distillation [10, 13] eliminate the need to precompute this paired dataset offline. They iteratively train a sequence of student models, each halving the number of sampling steps of its predecessor. A complementary technique, Instaflow [11] straightens the ODE trajectories, so they are easier to approximate with a one-step student. Consistency Distillation [9,12,19,26,31,32], and TRACT [33], train student models so their outputs are self-consistent at any timesteps along the ODE trajectory, and thus consistent with the teacher.

GANs. Another line of research employs adversarial training to align the student with the teacher at a broader distribution level. In ADD [23], the generator, initialized with weights from a diffusion model, is trained using a projected GAN objective with an image-space classifier [34]. Building on this, LADD [24] utilizes a pre-trained diffusion model as the discriminator and operates in latent space, thus improving scalability and enabling higher-resolution synthesis. Inspired by DiffusionGAN [28, 29], UFOGen [25] introduces noise injection prior to the real vs. fake classification in the discriminator, to smooth out the distributions, which stabilizes the training dynamics. However, purely GANbased methods often struggle to integrate classifier-free guidance directly. For instance, LADD uses diffusion-generated images with classifier-free guidance as real data in its GAN discriminator. Other approaches combine adversarial objectives with a distillation loss to preserve the original guided sampling trajectory. For instance, SDXL-Lightning [27] integrates a DiffusionGAN loss [25] with a progressive distillation objective [10, 13], while the Consistency Trajectory Model [26] combines a GAN [35] with an improved consistency distillation [9]. In contrast, our approach based on distribution matching [22, 36, 37] inherently integrates classifier-free guidance into the training supervision, significantly simplifying the training process.

Score Distillation was initially introduced in the context of text-to-3D synthesis [37–40], utilizing a pre-trained text-to-image diffusion model as a distribution matching loss. These methods optimize a 3D object by aligning rendered views with a text-conditioned image distribution, using the scores predicted by a pretrained diffusion model. Recent works have extended score distillation [37, 38, 41– 43] to diffusion distillation [22, 30, 36, 44–46]. Notably, DMD [22] minimizes an approximate KL divergence, with its gradient represented as the difference between two score functions: one, fixed and pretrained, for the target distribution and another, trained dynamically, for the output distribution of the generator.

![](images/c1b05ba93da6851dada5c35bf8eda4eb88ebee4044db2a176588f3fda71df3c5.jpg)  
Figure 2: $1 0 2 4 \times 1 0 2 4$ samples produced by our 4-step generator distilled from SDXL. Please zoom in for details.

DMD parameterizes both score functions using diffusion models. This training objective proved more stable than GAN-based methods and has demonstrated superior performance in one-step image synthesis. An important caveat, DMD requires a regression loss for stability, calculated using precomputed noise-image pairs, similar to Luhman et al. [16]. Our work does away with this requirement. We introduce techniques to stabilize the DMD training procedure without the regression regularizer, thus significantly reducing the computational costs incurred by paired data precomputation. Furthermore, we extend DMD to support multi-step generation and integrate the strengths of both GANs and distribution matching approaches [22, 30, 36, 45], leading to state-of-theart results in text-to-image synthesis.

# 3 Background: Diffusion and Distribution Matching Distillation

This section gives a brief overview of diffusion models and distribution matching distillation (DMD).

Diffusion Models generate images through iterative denoising. In the forward diffusion process, noise is progressively added to corrupt a sample $x \sim p _ { \mathrm { r e a l } }$ from the data distribution into pure Gaussian noise over a predetermined number of steps $T$ , so that, at each timestep $t$ , the diffused samples follow the distribution $\begin{array} { r } { p _ { \mathrm { r e a l } , t } ( x _ { t } ) = \int p _ { \mathrm { r e a l } } ( x ) q ( \dot { x } _ { t } | x ) d x } \end{array}$ , with $q _ { t } ( x _ { t } | x ) \sim \bar { \mathcal { N } } ( \alpha _ { t } x , \sigma _ { t } ^ { 2 } \mathbf { I } )$ , where $\alpha _ { t } , \sigma _ { t } > 0$ are scalars determined by the noise schedule [47, 48]. The diffusion model learns to iteratively reverse the corruption process by predicting a denoised estimate $\mu ( x _ { t } , t )$ , conditioned on the current noisy sample $\mathbf { \Psi } _ { x _ { t } }$ and the timestep $t$ , ultimately leading to an image from the data distribution $p _ { \mathrm { r e a l } }$ . After training, the denoised estimate relates to the gradient of the data likelihood function, or score function [48] of the diffused distribution:

$$
s _ { \mathrm { r e a l } } ( x _ { t } , t ) = \nabla _ { x _ { t } } \log p _ { \mathrm { r e a l } , t } ( x _ { t } ) = - \frac { x _ { t } - \alpha _ { t } \mu _ { \mathrm { r e a l } } ( x _ { t } , t ) } { \sigma _ { t } ^ { 2 } } .
$$

Sampling an image typically requires dozens to hundreds of denoising steps [49–52].

Distribution Matching Distillation (DMD) distills a many-step diffusion models into a one-step generator $G$ [22] by minimizing the expectation over $t$ of approximate Kullback-Liebler (KL) divergences between the diffused target distribution $p _ { \mathrm { r e a l } , t }$ and the diffused generator output distribution $p _ { \mathrm { f a k e } , t }$ . Since DMD trains $G$ by gradient descent, it only requires the gradient of this loss, which can be computed as the difference of 2 score functions:

$$
7 . \mathrm { { Z } _ { \mathrm { { D M D } } } } = \mathbb { E } _ { t } \left( \nabla _ { \theta } \mathrm { K L } \left( p _ { \mathrm { f i c } , t } \right| \left| p _ { \mathrm { r e a l } , t } \right) \right) = - \mathbb { E } _ { t } \left( \int \left( s _ { \mathrm { r e a l } } ( F ( G _ { \theta } ( z ) , t ) , t ) - s _ { \mathrm { f i c } } ( F ( G _ { \theta } ( z ) , t ) , t ) \right) \frac { d G _ { \theta } ( z ) } { d \theta } d z \right) ,
$$

where $z \sim \mathcal { N } ( 0 , \mathbf { I } )$ is a random Gaussian noise input, $\theta$ are the generator parameters, $F$ is the forward diffusion process (i.e., noise injection) with noise level corresponding to time step $t$ , and $s _ { \mathrm { r e a l } }$ and $s _ { \mathrm { f a k e } }$ are scores approximated using diffusion models $\mu _ { \mathrm { r e a l } }$ and $\mu _ { \mathrm { f a k e } }$ trained on their respective distributions (Eq. (1)). DMD uses a frozen pre-trained diffusion model as $\mu _ { \mathrm { r e a l } }$ (the teacher), and dynamically updates $\mu _ { \mathrm { f a k e } }$ while training $G$ , using a denoising score-matching loss on samples from the one-step generator, i.e., fake data [22, 47].

Yin et al. [22] found that an additional regression term [16] was needed to regularize the distribution matching gradient (Eq. (2)) and achieve high-quality one-step models. For this, they collect a dataset of noise-image pairs $( z , y )$ where the image $y$ is generated using the teacher diffusion model, and a deterministic sampler [49, 50, 53], starting from the noise map $z$ . Given the same input noise $z$ , the regression loss compares the generator output with the teacher’s prediction:

$$
\begin{array} { r } { \mathcal { L } _ { \mathrm { r e g } } = \mathbb { E } _ { ( z , y ) } d ( G _ { \theta } ( z ) , y ) , } \end{array}
$$

where $d$ is a distance function, such as LPIPS [54] in their implementation. While gathering this data incurs negligible cost for small datasets like CIFAR-10, it becomes a significant bottleneck with large-scale text-to-image synthesis tasks, or models with complex conditioning [55–57]. For instance, generating one noise-image pair for SDXL [58] takes around 5 seconds, amounting to about 700 A100 days to cover the 12 million prompts in the LAION 6.0 dataset [59], as utilized by Yin et al. [22]. This dataset construction cost alone is already more than $4 \times$ our total training compute (as detailed in Appendix I). This regularization objective is also at odds with DMD’s goal of matching the student and teacher in distribution, since it encourages adherence to the teacher’s sampling paths.

# 4 Improved Distribution Matching Distillation

We revisit multiple design choices in the DMD algorithm [22] and identify significant improvements.

![](images/06e244ac90ce2462caad245d56e9dc64ef70322293d86892c28385716c8d2de7.jpg)  
Figure 3: Our method distills a costly diffusion model (gray, right) into a one- or multi-step generator (red, left). Our training alternates between 2 steps: 1. optimizing the generator using the gradient of an implicit distribution matching objective (red arrow) and a GAN loss (green), and 2. training a score function (blue) to model the distribution of “fake” samples produced by the generator, as well as a GAN discriminator (green) to discriminate between fake samples and real images. The student generator can be a one-step or a multi-step model, as shown here, with an intermediate step input.

# 4.1 Removing the regression loss: true distribution matching and easier large-scale training

The regression loss [16] used in DMD [22] ensures mode coverage and training stability, but as we discussed in Section 3, it makes large-scale distillation cumbersome, and is at odds with the distribution matching idea, thus inherently limiting the performance of the distilled generator to that of the teacher model. Our first improvement is to remove this loss.

# 4.2 Stabilizing pure distribution matching with a Two Time-scale Update Rule

Naively omitting the regression objective, shown in Eq. (3), from DMD leads to training instabilities and significantly degrades quality (Tab. 3). For example, we observed that the average brightness, along with other statistics, of generated samples fluctuates significantly, without converging to a stable point (See Appendix F). We attribute this instability to approximation errors in the fake diffusion model $\mu _ { \mathrm { f a k e } }$ , which does not track the fake score accurately, since it is dynamically optimized on the non-stationary output distribution of the generator. This causes approximation errors and biased generator gradients (as also discussed in [30]).

We address this using the two time-scale update rule inspired by Heusel et al. [60]. Specifically, we train $\mu _ { \mathrm { f a k e } }$ and the generator $G$ at different frequencies to ensure that $\mu _ { \mathrm { f a k e } }$ accurately tracks the generator’s output distribution. We find that using 5 fake score updates per generator update, without the regression loss, provides good stability and matches the quality of the original DMD on ImageNet (Tab. 3) while achieving much faster convergence. Further analysis are included in Appendix F.

# 4.3 Surpassing the teacher model using a GAN loss and real data

Our model so far achieves comparable training stability and performance to DMD [22] without the need for costly dataset construction (Tab. 3). However, a performance gap remains between the distilled generator and the teacher diffusion model. We hypothesize this gap could be attributed to approximation errors in the real score function $\mu _ { \mathrm { r e a l } }$ used in DMD, which would propagate to the generator and lead to suboptimal results. Since DMD’s distilled model is never trained with real data, it cannot recover from these errors.

We address this issue by incorporating an additional GAN objective into our pipeline, where the discriminator is trained to distinguish between real images and images produced by our generator.

Trained using real data, the GAN classifier does not suffer from the teacher’s limitation, potentially allowing our student generator to surpass it in sample quality. Our integration of a GAN classifier into DMD follows a minimalist design: we add a classification branch on top of the bottleneck of the fake diffusion denoiser (see Fig. 3). The classification branch and upstream encoder features in the UNet are trained by maximizing the standard non-saturing GAN objective:

$$
\begin{array} { r } { \mathcal { L } _ { \mathrm { G A N } } = \mathbb { E } _ { x \sim p _ { \mathrm { r o l } } , t \sim [ 0 , T ] } [ \log D ( F ( x , t ) ) ] + \mathbb { E } _ { z \sim p _ { \mathrm { m i s c } } , t \sim [ 0 , T ] } [ - \log ( D ( F ( G _ { \theta } ( z ) , t ) ) ) ] , } \end{array}
$$

where $D$ is the discriminator, and $F$ is the forward diffusion process (i.e., noise injection) defined in Section 3, with noise level corresponding to time step $t$ . The generator $G$ minimizes this objective. Our design is inspired by prior works that use diffusion models as discriminators [24, 25, 27]. We note that this GAN objective is more consistent with the distribution matching philosophy since it does not require paired data, and is independent of the teacher’s sampling trajectories.

# 4.4 Multi-step generator

With the proposed improvements, we are able to match the performance of teacher diffusion models on ImageNet and COCO (see Tab. 1 and Tab. 5). However, we found that larger scale models like SDXL [58] remain challenging to distill into a one-step generator because of limited model capacity and a complex optimization landscape to learn the direct mapping from noise to highly diverse and detailed images. This motivated us to extend DMD to support multi-step sampling.

We fix a predetermined schedule with $N$ timestep $\left\{ t _ { 1 } , t _ { 2 } , \dots t _ { N } \right\}$ , identical during training and inference. During inference, at each step, we alternate between denoising and noise injection steps, following the consistency model [9], to improve sample quality. Specifically, starting from Gaussian noise $z _ { 0 } \overset { - } { \sim } \mathcal { N } ( 0 , \mathbf { I } )$ , we alternate between denoising updates $\hat { x } _ { t _ { i } } ^ { \phantom { \dagger } } = \bar { G } _ { \theta } ( x _ { t _ { i } } , \bar { t } _ { i } )$ , and forward diffusion steps $x _ { t _ { i + 1 } } = \alpha _ { t _ { i + 1 } } \hat { x } _ { t _ { i } } + \sigma _ { t _ { i + 1 } } \epsilon$ with $\epsilon \sim \mathcal { N } ( 0 , \mathbf { I } )$ , until we obtain our final image $\hat { x } _ { t _ { N } }$ . Our 4-step model uses the following schedule: 999, 749, 499, 249, for a teacher model trained with 1000 steps.

# 4.5 Multi-step generator simulation to avoid training/inference mismatch

Previous multi-step generators are typically trained to denoise noisy real images [23,24,27]. However, during inference, except for the first step, which starts from pure noise, the generator’s input come from a previous generator sampling step $\hat { x } _ { t _ { i } }$ . This creates a training-inference mismatch that adversely impacts quality (Fig. 4). We address this issue by replacing the noisy real images during training, with noisy synthetic images $\boldsymbol { x } _ { t _ { i } }$ produced by the current student generator running several steps, similar to our inference pipeline $( \ S 4 . 4 )$ . This is tractable because, unlike the teacher diffusion model, our generator only runs for a few steps. Our generator then denoises these simulated images and the outputs are supervised with the proposed loss functions. Using noisy synthetic images avoids the mismatch and improves overall performance (See Sec. 5.3).

#++→ ←- real image forward diffusion:  train/test domain gap “fake” sample backward simulation: train/test alignment

A concurrent work, Imagine Flash [61], proposed a similar technique. Their backward distillation algorithm shares our motivation of reducing the training and testing gap by using the student-generated images as the input to the subsequent sampling steps at training time. However, they do not entirely resolve the mismatch issue, because the teacher model of the regression loss now suffers from the training–test gap: it is never trained with synthetic images. This error is accumulated along the sampling path. In contrast, our distribution matching loss is not dependent on the input to the student model, alleviating this issue.

# 4.6 Putting everything together

In summary, our distillation method lifts DMD [22] stringent requirements for precomputed noise– image pairs. It further integrates the strength of GANs and supports multi-step generators. As shown in Fig. 3, starting from a pretrained diffusion model, we alternate between optimizing the generator $G _ { \theta }$ to minimize the original distribution matching objective as well as a GAN objective, and optimizing the fake score estimator $\mu _ { \mathrm { f a k e } }$ using both a denoising score matching objective on the fake data, and the GAN classification loss. To ensure the fake score estimate is accurate and stable, despite being optimized on-line, we update it with higher frequency than the generator (5 steps vs. 1).

# 5 Experiments

We evaluate our approach, DMD2, using several benchmarks, including class-conditional image generation on ImageNet- $6 4 \times 6 4$ [62], and text-to-image synthesis on COCO 2014 [63] with various teacher models [1, 58]. We use the Fréchet Inception Distance (FID) [60] to measure image quality and diversity, and the CLIP Score [64] to evaluate text-to-image alignment. For SDXL models, we additionally report patch FID [27, 65], which measures FID on 299x center-cropped patches of each image, to assess high-resolution details. Finally, we conduct human evaluations to compare our approach with other state-of-the-art methods. Comprehensive evaluations confirm that distilled models trained using our approach outperform previous work, and even rival the performance of the teacher models. Detailed training and evaluation procedures are provided in the appendix.

# 5.1 Class-conditional Image Generation

Table 1 compares our model with recent baselines on ImageNet- $6 4 \times 6 4$ . With a single forward pass, our method significantly outperforms existing distillation techniques and even outperforms the teacher model using ODE sampler [53]. We attribute this remarkable performance to the removal of DMD’s regression loss (Sec. 4.1 and 4.2), which eliminates the performance upper bound imposed by the ODE sampler, as well as our additional GAN term (Sec. 4.3), which mitigates the adverse impact of the teacher diffusion model’s score approximation error.

Table 1: Image quality comparison on ImageNet- $6 4 \times 6 4$ .   

<html><body><table><tr><td>Method</td><td>#Fwd Pass (↓)</td><td>FID (↓)</td></tr><tr><td>BigGAN-deep [66]</td><td>1</td><td>4.06</td></tr><tr><td>ADM [67]</td><td>250</td><td>2.07</td></tr><tr><td>RIN [68]</td><td>1000</td><td>1.23</td></tr><tr><td>StyleGAN-XL [35]</td><td>1</td><td>1.52</td></tr><tr><td>Progress.Distill. [10]</td><td>1</td><td>15.39</td></tr><tr><td>DFNO [69]</td><td>1</td><td>7.83</td></tr><tr><td>BOOT[20]</td><td>1</td><td>16.30</td></tr><tr><td>TRACT[33]</td><td>1</td><td>7.43</td></tr><tr><td>Meng et al. [13]</td><td>1</td><td>7.54</td></tr><tr><td>Diff-Instruct [36]</td><td>1</td><td>5.57</td></tr><tr><td>Consistency Model [9]</td><td>1</td><td>6.20</td></tr><tr><td>iCT-deep [12]</td><td>1</td><td>3.25</td></tr><tr><td>CTM [26]</td><td>1</td><td>1.92</td></tr><tr><td>DMD [22]</td><td>1</td><td>2.62</td></tr><tr><td>DMD2 (Ours)</td><td>1</td><td>1.51</td></tr><tr><td>+longer training (Ours)</td><td>1</td><td>1.28</td></tr><tr><td>EDM(Teacher,ODE) [53]</td><td>511</td><td>2.22</td></tr><tr><td>EDM(Teacher, SDE) [53]</td><td>511</td><td>1.36</td></tr></table></body></html>

Table 2: Image quality comparison with SDXL backbone on 10K prompts from COCO 2014.   

<html><body><table><tr><td>Method</td><td>#Fwd Pass (↓)</td><td>FID (↓)</td><td>Patch FID (↓)</td><td>CLIP (↑)</td></tr><tr><td>LCM-SDXL [32]</td><td>1 4</td><td>81.62 22.16</td><td>154.40 33.92</td><td>0.275 0.317</td></tr><tr><td>SDXL-Turbo [23]</td><td>1 4</td><td>24.57 23.19</td><td>23.94 23.27</td><td>0.337 0.334</td></tr><tr><td>SDXL</td><td>1</td><td>23.92</td><td>31.65</td><td>0.316</td></tr><tr><td>Lightning [27] DMD2 (Ours)</td><td>4</td><td>24.46</td><td>24.56</td><td>0.323</td></tr><tr><td></td><td>1 4</td><td>19.01 19.32</td><td>26.98 20.86</td><td>0.336 0.332</td></tr><tr><td>SDXL Teacher,cfg=6 [58]</td><td>100</td><td></td><td></td><td></td></tr><tr><td>SDXL Teacher,cfg=8 [58]</td><td>100</td><td>19.36 20.39</td><td>21.38 23.21</td><td>0.332 0.335</td></tr></table></body></html>

# 5.2 Text-to-Image Synthesis

We evaluate DMD2’s text-to-image generation performance on zero-shot COCO 2014 [63]. Our generators are trained by distilling SDXL [58] and SD v1.5 [1], respectively, using a subset of 3 million prompts from LAION-Aesthetics [59]. Additionally, we collect $5 0 0 \mathrm { k }$ images from LAIONAesthetic as training data for the GAN discriminator. Table 2 summarizes distillation results for the SDXL model. Our 4-step generator produces high quality and diverse samples, achieving a FID score of 19.32 and a CLIP score of 0.332, rivaling the teacher diffusion model for both image quality and prompt coherence. To further verify our method’s effectiveness, we conduct an extensive user study comparing our model’s output with those from the teacher model and existing distillation methods. We use a subset of 128 prompts from PartiPrompts [70] following LADD [24]. For each comparison, we ask a random set of five evaluators to choose the image that is more visually appealing, as well as the one that better represents the text prompt. Details about the human evaluation are included in Appendix K. As shown in Figure 5, our model achieves much higher user preferences than baseline approaches. Notably, our model outperforms its teacher in image quality for $24 \%$ of samples and achieves comparable prompt alignment, while requiring $2 5 \times$ fewer forward passes (4 vs 100). Qualitative comparisons are shown in Figure 6. Results for SDv1.5 are provided in Table 5 in Appendix D. Similarly, one-step model trained using DMD2 outperforms all previous diffusion acceleration approaches, achieving a FID score of 8.35, representing a significant 3.14-point improvement over the original DMD method [22]. Our results also surpass the teacher models that uses a 50-step PNDM sampler [50].

![](images/0b1e753eb68d59e1e40e58707d168cffd00a8a85fdf3cb8597a0b4a84df2af3b.jpg)  
Figure 5: User study comparing our distilled model with its teacher and competing distillation baselines [23, 27, 31]. All distilled models use 4 sampling steps, the teacher uses 50. Our model achieves the best performance for both image quality and prompt alignment.

# 5.3 Ablation Studies

Table 3: Ablation studies on ImageNet. TTUR stands for two-timescale update rule.   

<html><body><table><tr><td colspan="4">DMD No Regress. TTUR GAN FID (↓)</td></tr><tr><td>√</td><td></td><td></td><td>2.62</td></tr><tr><td>√</td><td>√</td><td></td><td>3.48</td></tr><tr><td>√</td><td>√ √</td><td></td><td>2.61</td></tr><tr><td>√</td><td>√ √</td><td>√</td><td>1.51</td></tr><tr><td></td><td></td><td>√</td><td>2.56</td></tr><tr><td></td><td>√</td><td>√</td><td>2.52</td></tr></table></body></html>

Table 4: Ablation studies with SDXL backbone on 10K prompts from COCO 2014.   

<html><body><table><tr><td>Method</td><td colspan="3">FID (↓) Patch FID (↓） CLIP(↑)</td></tr><tr><td>w/o GAN</td><td>26.90</td><td>27.66</td><td>0.328</td></tr><tr><td>w/o Distribution Matching</td><td>13.77</td><td>27.96</td><td>0.307</td></tr><tr><td>w/o Backward</td><td>20.66</td><td>24.21</td><td>0.332</td></tr><tr><td>Simulation DMD2 (Ours)</td><td>19.32</td><td>20.86</td><td>0.332</td></tr></table></body></html>

Table 3 ablates different components of our proposed method on ImageNet. Simply removing the ODE regression loss from the original DMD results in a degraded FID of 3.48 due to training instability (see further analysis in Appendix F). However, incorporating our Two Time-scale Update Rule (TTUR, Sec. 4.2) mitigates this performance drop, matching the DMD baseline performance without requiring additional dataset construction. Adding our GAN loss achieves a further 1.1- point improvement in FID. Our integrated approach surpasses the performance of using GAN alone (without distribution matching objective), and adding the two-timescale update rule to GAN alone does not improve it, highlighting the effectiveness of combining distribution matching with GANs in a unified framework.

![](images/f729df879104250421df93b48187db4ed9ebec1bc379431ecc5dd5203d916915.jpg)  
A photo of llama wearing sunglasses standing on the deck of a spaceship with the Earth in the background.   
Figure 6: Visual comparison between our model, the SDXL teacher, and selected competing methods [23, 27, 31]. All distilled models use 4 sampling steps while the teacher model uses 50 sampling steps with classifier-free guidance. All images are generated using identical noise and text prompts. Our model produces images with superior realism and text alignment. (Zoom in for details.) More comparisons are available in Appendix Figure 10.

In Table 4, we ablate the influence of the GAN term (Sec. 4.3), distribution matching objective (Eq. 2), and backward simulation (Sec. 4.4) for distilling the SDXL model into a four-step generator. Qualitative results are shown in Appendix Figure. 7. In the absence of the GAN loss, our baseline model produces oversaturated and oversmoothed images (Appendix Fig. 7 third column). Similarly, eliminating distribution matching objective (Eq. 2) reduces our approach to a pure GAN-based method, which struggles with training stability [71, 72]. Moreover, pure GAN-based methods also lack a natural way to incorporate classifier-free guidance [73], essential for high-quality text-to-image synthesis [1, 2]. Consequently, while GAN-based methods achieve the lowest FID by closely matching the real distribution, they significantly underperform in text alignment and aesthetic quality (Appendix Fig. 7 second column). Likewise, omitting the backward simulation leads to worse image quality, as indicated by the degraded patch FID score.