# Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification

Dani¨elle Schuman Mark V. Seebode Tobias Rohe Maximilian Balthasar Mansky LMU Munich LMU Munich LMU Munich LMU Munich Munich, Germany Munich, Germany Munich, Germany Munich, Germany danielle.schuman@ifi.lmu.de M.Seebode@campus.lmu.de tobias.rohe@ifi.lmu.de maximilian-balthasar.mansky@ifi.lmu.de

Michael Schroedl-Baumann SAP SE Walldorf, Germany michael.schroedl-baumann@sap.com

Jonas Stein   
Aqarios GmbH   
Munich, Germany   
jonas.stein@aqarios.com   
Claudia Linnhoff-Popien   
LMU Munich   
Munich, Germany   
linnhoff@ifi.lmu.de   
Florian Krellner   
SAP SE   
Walldorf, Germany   
florian.krellner@sap.com

Abstract—Exploiting the fact that samples drawn from a quantum annealer inherently follow a Boltzmann-like distribution, annealing-based Quantum Boltzmann Machines (QBMs) have gained increasing popularity in the quantum research community. While they harbor great promises for quantum speed-up, their usage currently stays a costly endeavor, as large amounts of QPU time are required to train them. This limits their applicability in the NISQ era. Following the idea of Noe\` et al. [1], who tried to alleviate this cost by incorporating parallel quantum annealing into their unsupervised training of QBMs, this paper presents an improved version of parallel quantum annealing that we employ to train QBMs in a supervised setting. Saving qubits to encode the inputs, the latter setting allows us to test our approach on medical images from the MedMNIST data set [2], thereby moving closer to real-world applicability of the technology. Our experiments show that QBMs using our approach already achieve reasonable results, comparable to those of similarly-sized Convolutional Neural Networks (CNNs), with markedly smaller numbers of epochs than these classical models. Our parallel annealing technique leads to a speed-up of almost $70 \%$ compared to regular annealing-based BM executions.

Index Terms—Quantum Boltzmann Machines, Medical Image Classification, Parallel Quantum Annealing

# I. INTRODUCTION

Machine learning and more precisely deep learning based methods have proven to be effective for medical image analysis [3]–[5], and can, for example, be used to diagnose pneumonia from chest $\mathbf { \delta X }$ -rays [6].

In the quest for near-term, attainable quantum advantage in areas like this, we propose using Quantum Boltzmann Machines (QBMs) for medical image classification. QBMs were introduced in [7] and are the quantum analogue of classical Boltzmann Machines (BMs), a type of machine learning model introduced by Ackley, Hinton, et al. in the 1980s [8]–[11].

Although BMs are powerful models, they have been superseded by deep neural networks mainly because BMs are notoriously difficult to train with available classical hardware [8], [12]. Therefore, current quantum research concentrates on replacing classical methods for evaluating the model states during training with quantum methods, such as Quantum Annealing (QA). Several recent work [13]–[19] has successfully used QA (or comparable techniques [15], [20]) in BMs trained to classify images, often finding advantages such as faster computation times [14], [16], less fluctuations in training accuracy [17], [20] or a smaller amount of training epochs that is needed [20]. Often, however, these approaches use rather small and simple data sets such as “Bars and Stripes” [17] or (potentially coarse-grained) MNIST images [14], [16], [18].

To our knowledge, QBMs using QA have so far not been used directly to classify medical images: In [20] a combination of a pre-trained Convolutional Neural Network (CNN) and a QBM was used to classify medical images, with the BM being trained by Simulated Annealing (SA) as a proxy for QA, while in [19] a combination of an auto-encoder and Deep Belief Network are used, where only the latter is pre-trained using a QA-based QBM.

However, while QA-based QBMs seem like a promising approach for medical image classification, a lot of the preexisting works on QA-based QBMs (e.g. [7], [17], [20], [21]) do not use actual QA hardware results, as they report that training QA-based QBMs takes up prohibitively large amounts of expensive Quantum Processing Unit (QPU) time [20], [21]. This poses a problem for the near-term applicability of such QA-approaches.

Recently, Noe\` et al. [1] presented a solution to this type of problem in an unsupervised setting, where they used Parallel Quantum Annealing (PQA) [22] – a technique which embeds multiple independent problem instances in a single annealing cycle – to achieve a large decrease in required runtime. Following this idea, our approach to training a QA-based

QBM in a supervised fashion tries to balance accuracy and efficiency by embedding different QBM problem instances isolated from each other onto the topology of the annealer’s QPU to minimize interference. The development of our approach follows the first four stages of a 5-stage structured pipeline for quantum software engineering tailored to NISQ and early post-NISQ era applications [23].

The remainder of this work is structured as follows: First, Sec. II gives some information about the workings of BMs, explains how they can be executed using QA, details the concept of PQA and subsequently introduces our approach of using improved PQA in supervised QBM training. We then briefly introduce CNNs, which will be used as a classical baseline in our experiments on two of the MedMNIST data sets [2]. Sec. III then introduces our employed data sets and performance metrics, and subsequently describes our hyperparameter optimization efforts and experimental results. Finally, our approach and results will be summarized in Sec. IV, which also addresses the limitations of our present paper and how these might be addressed in future work.

# II. MODELS AND BACKGROUND

# A. Boltzmann Machines for supervised learning

A Boltzmann Machine (BM) [8]–[11] is an undirected stochastic neural network composed of $n$ neurons, which can be grouped into $n _ { v }$ visible units $\boldsymbol { v }$ and (optionally) $n _ { h }$ hidden units $h$ . They can take the values 0 or 1 with a certain probability governed by a quadratic energy function [8]. Compare Fig. 1 for a visualization of such a network.

![](images/e6326ab61d64fb8b17424fe606669c9169cacf7e2c3ef7e84087d83d6d8229d2.jpg)  
Fig. 1: Structure of a fully connected Boltzmann Machine. The network consists of visible units (blue and red) and hidden units (gray), with symmetric connections between all pairs of units. No distinction is made between layers; every node is connected to every other node. In supervised learning scenarios, some visible units (here in blue) can be used to represent the input data $d .$ , while others (in red, can also be multiple) can be used to represent the label $l$ .

The visible units are used to embed the input data points into the BM [7]. In our application, these data points consist of an input image $d$ and the corresponding label $l$ . In supervised learning, a part of the visible units (blue in Fig. 1), which we will refer to in the following as input units, will be used to encode the input data $d$ by a string of values $v _ { d }$ of length $n _ { d }$ . We will refer to the (multi-) set of all inputs from the data set encoded this way as $D _ { i n }$ . The remaining $n _ { l }$ visible units (red in Fig. 1), which we will refer to as label units, will be used to represent the label $l$ using an encoding $v _ { l } \in \{ 0 , 1 \} ^ { n _ { l } }$ . The goal of training the BM is that, given only a particular input $d$ (without the corresponding label $l$ ) at inference time, the label units will assume values $v _ { l }$ matching the possible labels $l$ of this input with a probability closely corresponding to the frequency of the co-occurrence of the labels $l$ in question with this particular $d$ in the training data set [7]. This means that by drawing a few samples from the BM, the label $v _ { l }$ most frequently assumed by the label units can be used to classify the input $d$ . (Ideally, if the classification of the input can be performed unambiguously after training, the frequency of this most probable label occurring will be very close to $100 \%$ .)

The probability of sampling a unit configuration $s \in \{ 0 , 1 \} ^ { n }$ from the BM is governed by the Boltzmann distribution

$$
P _ { b , W } ( s ) = \frac { 1 } { Z } \exp \left( - \frac { E _ { b , W } ( s ) } { T } \right)
$$

with the normalization

$$
Z = \sum _ { s \in \{ 0 , 1 \} ^ { n } } \exp \left( - \frac { E _ { b , W } ( s ) } { T } \right)
$$

and energy function

$$
E _ { b , W } ( s ) = \sum _ { i = 1 } ^ { n } b _ { i } s _ { i } - \sum _ { i , j = 1 \atop i < j } ^ { n } W _ { i j } s _ { i } s _ { j } .
$$

where the parameters $W _ { i j }$ and $b _ { i }$ are the BM’s weights and biases and $T$ is the so-called effective temperature, which can be used as a hyperparameter governing the Boltzmann distribution’s shape relative to the energy function’s values [8]: At lower effective temperatures, small differences in the energy values of states lead to more drastic differences in their probability to be sampled than at higher effective temperatures [8].

The parameters of the BM are adjusted during training in a way that maximizes the likelihood of the (encoded) training data $( v _ { d } , v _ { l } ) \in D$ (where $D$ is the encoded data set) to occur when sampling from the BM [7]. More precisely, following Amin et al. [7], we are using a discriminative training procedure that maximizes the likelihood of $v _ { l }$ to be assumed by the label units when the input units are fixed, or clamped, to $\boldsymbol { v } _ { d }$ . This can be achieved by using the (average) negative log-likelihood of the labels $v _ { l }$ as a loss function to be minimized:

$$
b , W \mapsto { \mathcal { L } } ( b , W ) = - \sum _ { ( v _ { d } , v _ { l } ) \in D } \log P _ { b , W } ( v _ { l } | v _ { d } )
$$

Here,

$$
P _ { b , W } ( v _ { l } | v _ { d } ) = \frac { 1 } { Z _ { v _ { d } } } \sum _ { h \in \{ 0 , 1 \} ^ { n _ { h } } } \exp \left( - \frac { E _ { b , W } ( s ) } { T } \right)
$$

with

$$
Z _ { v _ { d } } = \sum _ { ( v _ { l } , h ) \in \{ 0 , 1 \} ^ { ( n _ { l } + n _ { h } ) } } \exp \left( - \frac { E _ { b , W } ( s ) } { T } \right)
$$

is the probability of sampling the label units $v _ { l } \in \{ 0 , 1 \} ^ { n _ { l } }$ when $v _ { d }$ is clamped to a specific input value [7]. The notation $( v _ { l } , h ) \in \{ 0 , 1 \} ^ { ( \overline { { n } } _ { l } + n _ { h } ) }$ denotes the concatenation of vectors $v _ { l }$ and $h$ which has the total length $n _ { l } + n _ { h }$ , while $\boldsymbol { s } = \left( v _ { d } , v _ { l } , h \right)$ represents the concatenation of the vectors of all units as one vector of length $n$ . Since, in this style of learning, $v _ { d }$ is always clamped to an input value $d$ of a specific data point, the energy function $E _ { b , W } ( s )$ can be written as:

$$
E _ { b , W , v _ { d } } ( s \setminus v _ { d } ) = \sum _ { i = n _ { d } + 1 } ^ { n } b _ { i } ^ { d } s _ { i } \quad - \sum _ { \stackrel { i , j = n _ { d } + 1 } { i < j } } ^ { n } W _ { i j } s _ { i } s _ { j }
$$

where

$$
b _ { i } ^ { d } = b _ { i } + \sum _ { k = 1 } ^ { n _ { d } } W _ { i k } v _ { d k }
$$

acts as a bias on the on the remaining units $h$ and $v _ { l }$ [7]. (Here, the notation $s \setminus v _ { d } = ( v _ { l } , h )$ is used to enhance readability.)

The standard technique to minimize $\mathcal { L }$ is via gradient descent methods. The gradient is given by [7]

$$
\partial _ { b , W } \mathcal { L } = \sum _ { v \in D } \langle \partial _ { b , W } E _ { b , W } \rangle _ { v } \ \quad - \ \sum _ { v _ { d } \in D _ { i n } } \langle \partial _ { b , W } E _ { b , W } \rangle _ { v _ { d } }
$$

with the Boltzmann averages

$$
\begin{array} { l } { \displaystyle \langle \partial _ { b , W } E _ { b , W } \rangle _ { \upsilon } } \\ { \displaystyle = \frac { 1 } { Z _ { \upsilon } T } \sum _ { h \in \{ 0 , 1 \} ^ { n _ { \mathrm { h } } } } \partial _ { b , W } E _ { b , W } ( s ) \exp { \left( - \frac { E _ { b , W } ( s ) } { T } \right) } , } \end{array}
$$

with normalization

$$
Z _ { v } = \sum _ { h \in \{ 0 , 1 \} ^ { h } } \exp { \left( \frac { - E _ { b , W } ( s ) } { T } \right) } ,
$$

and

$$
\begin{array} { l } { \displaystyle \langle \partial _ { b , W } E _ { b , W } \rangle _ { v _ { d } } } \\ { = \frac { 1 } { Z _ { v _ { d } } T } \displaystyle \sum _ { ( v _ { l } , h ) \in \{ 0 , 1 \} ^ { ( n _ { l } + n _ { h } ) } } \partial _ { b , W } E _ { b , W } \exp { \left( \frac { - E _ { b , W } ( s ) } { T } \right) } . } \end{array}
$$

Thus, the gradient steps used in training the BM are given by

$$
\begin{array} { r } { \delta b _ { i } = \eta ( \langle s _ { i } \rangle _ { v } - \langle s _ { i } \rangle _ { v _ { d } } ) , \qquad } \\ { \delta W _ { i j } = \eta ( \langle s _ { i } s _ { j } \rangle _ { v } - \langle s _ { i } s _ { j } \rangle _ { v _ { d } } ) , \qquad } \\ { \delta W _ { i k } = \eta ( \langle s _ { i } v _ { d k } \rangle _ { v } - \langle s _ { i } v _ { d k } \rangle _ { v _ { d } } ) , } \end{array}
$$

where $\eta$ is a learning rate and $s _ { i }$ and $s _ { j }$ are the values of hidden or label units [7]. In practice, these values can be determined by sampling them repeatedly from the BM in a state of equilibrium, a certain number of times in the positive (or clamped) and an equal number of times in the negative (or unclamped) phase [7]. The difference between the positive and negative phases is that in the former, in which the sample values to calculate the first terms in the Equations $1 3 - 1 5$ will be determined, all visible units will be clamped to the values $v _ { d }$ and $v _ { l }$ from the encoded training data point, while in the latter, used to calculate the second terms in the respective equations, only the input units will be clamped to $v _ { d }$ [7]. In both cases, the pair-wise products of the sample values will then be formed to calculate the weight’s gradient steps, and subsequently, the values of $\langle . . . \rangle _ { v }$ respectively $\langle . . . \rangle _ { v _ { d } }$ can be calculated by averaging over the samples respectively their products [7], [8]. Doing this repeatedly for all data points in the training data set will eventually cause the probability distribution of the BM (Eq. 1) to mirror the conditional distribution of the labels (conditioned on the inputs $d ,$ ) in the training data set, which enables the BM to correctly classify the inputs $d$ [7].

# B. Using Quantum Annealing for Boltzmann sampling

Reaching an equilibrium state of the BM’s network which can be used to sample from can be a computationally expensive endeavor, however [8], [12]. At least using classical computers, sampling from an arbitrarily connected network requires repeatedly updating each unit according to its stochastic update rule, which involves calculating the probability of its value $s _ { i }$ to become 1 given by

$$
p _ { i } = \frac { 1 } { 1 + \exp { ( - \Delta E _ { b , W } ^ { i } / T ) } } ,
$$

where

$$
\Delta E _ { b , W } ^ { i } = \sum _ { m = 1 } ^ { n _ { m } } W _ { i m } s _ { m } + b _ { i } ,
$$

with $s _ { m }$ being the $n _ { m }$ neighboring units that are directly connected to the units $s _ { i }$ in the network [8]. This has to be done for every unit in BM, until none of the states of the units in the network change anymore [8]. Furthermore, as this process has to be performed multiple times (depending on the number of samples used for averaging) for each data point, this classical training method for arbitrarily connected BMs is often considered intractable [8], [12].

This time consuming process of obtaining samples can, however, be avoided by using Quantum Annealing (QA) to draw samples from the BM [7], [20], [21].

D-Wave quantum annealers implement a time-dependent Hamiltonian that acts on a system of $N$ logical qubits, interpolating between a driver Hamiltonian and a target problem Hamiltonian [24]:

$$
H ( t ) = A ( t ) H _ { D } + B ( t ) H _ { P }
$$

where $A ( t )$ and $B ( t )$ are scheduling functions satisfying

$$
A ( t _ { i } ) \gg B ( t _ { i } ) \mathrm { ~ a n d ~ } A ( t _ { f } ) \ll B ( t _ { f } )
$$

at the initial $( t _ { i } )$ and final $( t _ { f } )$ times of the annealing schedule. If the process of changing the time-dependent Hamiltonian by progressing along this annealing schedule is done adiabatically, i.e. sufficiently slow, and if the initial state at $t _ { i }$ is the ground state of $H _ { D }$ , the resulting state at time $t _ { f }$ should be the ground state of $H _ { P }$ [24], which is to represent the most likely or best solution of a given problem, which one is looking for. Consequently, the ground state of $H _ { D }$ should be easy to prepare [24]. Therefore, D-Wave’s machines employ the transverse field as the driver Hamiltonian [7], [24], [25]:

$$
H _ { D } = - \sum _ { i = 1 } ^ { N } \sigma _ { i } ^ { x }
$$

with

$$
\sigma _ { i } ^ { x } \equiv \underbrace { I \otimes \cdots \otimes I } _ { i - 1 } \otimes \sigma _ { x } \otimes \underbrace { I \otimes \cdots \otimes I } _ { N - i }
$$

representing a state where the Pauli-X operator $\sigma _ { x } = { \binom { 0 } { 1 } } \quad 0 \quad$ acts on the ith logical qubit of the system and $I = { \binom { \dot { 1 } } { 0 } } \ { \stackrel { 0 } { 1 } } \bigg \rangle ^ { \prime } .$ Here, $\otimes$ refers to the tensor product.

Furthermore, the target problem Hamiltonian of interest $H _ { P }$ takes the form of an Ising Hamiltonian in current QAprocessors [26]:

$$
H _ { P } = - \sum _ { i = 1 } ^ { N } h _ { i } \sigma _ { i } ^ { z } - \sum _ { i , j = 1 \atop i < j } ^ { N } J _ { i , j } \sigma _ { i } ^ { z } \sigma _ { j } ^ { z } ,
$$

with

$$
\sigma _ { i } ^ { z } \equiv \underbrace { I \otimes \cdots \otimes I } _ { i - 1 } \otimes \sigma _ { z } \otimes \underbrace { I \otimes \cdots \otimes I } _ { N - i }
$$

representing a state where the Pauli- $Z$ operator

$$
\sigma _ { z } = { \binom { 1 } { 0 } } \quad { \overset { } { - } } 1 \quad
$$

acts on the $i$ -th logical qubit of the system and where $h _ { i }$ and $J _ { i , j }$ denote the logical qubits’ biases and coupling strengths [7], [26]. Ising models – just like their equivalent binary formulations, the Quadratic Unconstrained Binary Optimization (QUBO) models – are a type of energy-based model that makes it relatively easy to encode problems of which optimal, or at least good, solutions are of interest, given that these solutions can be represented by a minimum, or at least low, energy configuration of the model [7], [26]. In our case, by choosing $\mathbf { { h } } _ { i } ~ = ~ \mathbf { { b } } _ { i }$ and $J _ { i , j } ~ = ~ W _ { i , j }$ , the Hamiltonian $H _ { P }$ can be tailored to resemble the energy function of a BM [7], [27]: Here, the logical qubits of the quantum system – which probabilistically assume values 1 or 0 upon measurement at the end of the annealing process – can take up the role of the hidden and label units of the network. Meanwhile, the input units – acting as biases to the rest of the network according to Eq. 8 – do not need to be represented by qubits and can, in theory, possess arbitrary values, including non-binary ones [7]. When samples from a quantum system – i.e. strings of qubit values representing the system’s configuration upon measurement – are acquired using a physical quantum annealer, the hardware interacts with the environment, resulting in a sample distribution that follows an approximate Boltzmann distribution (instead of always returning the ground state of $H _ { P }$ ) [21]. Thus, the QA process can be used in BM training (and inference) to reach an equilibrium from which a sample can be drawn in one step [7], [20], [21]. Unless explicitly stated otherwise, BMs trained and executed in this way is what we will be referring to as Quantum Boltzmann Machines (QBMs) in the remainder of this paper. In related works on image classification tasks, such as one that utilized a Restricted QBM to pre-train a Deep Belief Network for MNIST data classification [16], and another where classification was achieved solely with a Restricted QBM [14], using Quantum Annealing for training offered benefits in computation time.

However, device-specific properties suggest that sampling from a quantum annealer’s distribution will be done with an instance-dependent effective temperature, which differs from the actual hardware temperature [7], [28]. This effective temperature can be estimated and then incorporated by rescaling the weights and biases with the inverse of said temperature [7], [28]. Nonetheless, previous work has shown that quantum BMs trained with raw QA-samples, without any temperature correction, still arrive at probability distributions to be sampled from that sufficiently mimic the training data set’s distribution to be usable for machine learning tasks, even though they might deviate from the classical Boltzmann distribution [27]. Hence, this work will not incorporate a tuning of the effective temperature and instead rely on the benefit of raw QAsampling as has been demonstrated in [27].

# C. Employing Parallel Quantum Annealing

When using QA hardware for BM training and inference, the logical qubits of the Hamiltonian $H _ { P }$ must be mapped to physical qubits on the QA-hardware that have suitable connectivity to represent the connectivity of units in the BM [7], [26]. This process is known as minor embedding and is typically performed using D-Wave’s minorminer API [26], [29].

All experiments in this study are conducted on the D-Wave Advantage System, which contains about 5000 physical qubits and uses the so-called Pegasus topology [30]. This topology supports up to 15 connections per qubit, offering improved flexibility over earlier architectures [30]. However, because a fully connected BM exceeds the connectivity of a single qubit, it is necessary to represent each logical unit by a chain of multiple physical qubits [26]. Depending on the length of the chain, this has proven to add additional noise to the system and should be avoided where ever possible [26].

Minor embedding is generally applied to a single problem instance, where samples are then acquired sequentially. However, Pelofske et al. [22] have shown that multiple disjoint problem instances can be embedded in parallel. This approach, which we call Parallel Quantum Annealing $( P Q A )$ , enables the annealer to generate samples from several instances within a single annealing cycle [22]. Their study reported significant reductions in quantum processing time, but also noted a drop in sample quality [22]. To address this, the authors suggested increasing the spatial distance between embeddings to reduce leakage [22]. Since their method relied on automatic embedding using minorminer, this separation could not be guaranteed [22].

A very recent approach by Noe\` et al. [1] first also used PQA in the context of QBMs. A QBM with 16 visible and 16 hidden units was trained with the D-Wave Advantage4.1 system to reconstruct images from the $4 \mathrm { x } 4$ -pixel “Bars and Stripes” data set in an unsupervised manner, utilizing PQA [1], [22]. For the clamped phase, the researchers embedded 4 instances of the QBM into the QA hardware graph simultaneously, each instance having its visible units clamped to another data point [1]. For the unclamped phase, the QBM was embedded 26 times simultaneously (without any units being clamped) [1]. With this strategy, the quantum annealingbased training method showed an 8.6-fold improvement in wall clock time over a parallelized classical Gibbs sampling method [1]. In addition, their evidence shows that QBMs sampling time scales almost linearly with the size of the model, showing potential for bigger problem instances [1]. However, the actual generative performance of their QBM ended up being worse than the classical approach [1]. Among other reasons, the authors explain these results with unsuitable temperature scaling and current hardware limitations [1].

# D. Our Contribution: Improved Parallel Quantum Annealing for supervised learning using QBMs

From Noe\` et al.’s [1] description of their work, it does not become clear whether they adhered to Pelofske et al.’s [22] above-mentioned suggestion to actively increase the spatial distance between different problem instances – in this case different instances of the BM. Thus, it cannot be ruled out that the problems with sample quality they faced due to hardware limitations where at least partially due to unintended interactions between qubits of different problem instances that had been placed too close together on the hardware graph [22].

The approach presented in this work – which is, to the best of the authors’ knowledge, the first work using PQA in supervised QBM training – avoids that limitation by explicitly controlling the placement of embeddings:

First, to create a QBM instance to embed, we incorporate an input data point into our model. In our experiments, we do this by assigning one input unit to each of the 784 pixel values of an input from the MedMNIST [2] data set that is used. It is important to note that the input units do not necessitate specific hardware resources since they are permanently fixed to the data, effectively serving as biases for the other nodes as described in Eq. 8. As we focus on binary classification, we then embed the class label by adding one label unit to the visible layer – encoding our two classes as 0 and 1. Furthermore, for the number of hidden units, we use a parameter that takes values between 1 and 20. We purposefully chose this relatively low upper limit for the number of hidden units, since this choice avoids exposing the model to unnecessary noise which otherwise might have been caused by excessively long chains of physical qubits that have to be build when embedding the model into hardware. At the end of these steps, we have created a QBM model instance with a maximum of 21 fully connected qubits to embed, as only the hidden units and the label units will need to be embedded onto the QA-hardware. This can be done using a QUBO model of the instance, which essentially takes the same shape as the QBM’s energy function displayed in Eq. 7 and can easily be mapped to a Hamiltonian $H _ { P }$ using the D-Wave API [31].

Given our QBM instance as a QUBO, we then divide the Pegasus topology of the D-Wave Advantage’s hardware graph into ten subgraphs using the Pymetis Python library [32]. This fixed number of subgraphs ensures sufficient possible spacing between the embeddings, given the maximum of 21 fully connected qubits we need to embed. To further isolate each of the ten subgraphs, we introduce a buffer zone between them: Nodes that are connected to other subgraphs are removed to reduce the likelihood of interference between embeddings. The final topology, including both the subgraphs (blue) used for finding embeddings and buffer zones (gray), ensuring a minimum distance between the embeddings, is shown in Fig. 2a. Each subgraph is then used to embed the same QUBO model of the QBM instance using minorminer. Fig. 2b shows an example of this multi-embedding, which includes a clamped QBM instance with 20 hidden units. (Here, the label units do not need to be embedded, given that in the clamped phase, they act as biases in the same way the input units do.) Using this type of multi-embedding, we can subsequently draw ten samples at once using one execution of the annealing cycle. While this strategy ensures a minimum distance between embeddings, thus preserving sample quality, it also results in a significant number of qubits being allocated to buffer zones. This reduces the overall embedding capacity of a given QA hardware graph.

In order to successfully evaluate the performance of our supervised QBMs that employ PQA and see how the approach scales when employing different model sizes, we perform experiments on two of the MedMNIST data sets [2], see Sec. III. Depending on the number of hidden units used, the configurations explored here contain between 1571 and 16695 trainable parameters: The minimum of 1571 parameters includes $7 8 4 \cdot 2 = 1 5 6 8$ weights (acting as biases according to Eq. 7) from the input units to the rest of the network, one weight between the single hidden unit and label unit and one bias for each of those units. Similarly, the maximum of 16695 parameters includes $7 8 4 \cdot 2 1 = 1 6 4 6 4$ input weights, $( 2 0 \cdot 1 9 ) / 2 = 1 9 0$ weights between the hidden units, 20 weights between the hidden units and the single label unit, 20 hidden biases and one label unit bias. In all cases, all parameters are initialized with random values drawn uniformly from the interval $[ - 1 , 1 ]$ . To also compare the performance of these differently sized QBM models not just to each other, but also to that of some classical models of a similar size that are commonly used for the task at hand, we evaluated them against Convolutional Neural Networks (CNNs) using a similar number of parameters [33], [34]. These CNNs are described in the following section.

![](images/9e5f0757849d4aa88a1dd4397559717de232925dd140f188af68bc1972ec3c86.jpg)  
Fig. 2: Visualization of the parallel embedding approach on the Pegasus topology. (a) The topology is partitioned into 10 subgraphs. Blue nodes indicate the regions used for embedding, while gray nodes represent buffer zones that enforce a minimum distance between embeddings. (b) An example embedding of a 20 hidden unit QBM (QA) in the clamped phase, demonstrating how the parallel embedding strategy utilizes the partitioned regions.

# E. Convolutional Neural Networks

Deep Convolutional Neural Networks (CNNs) [33] became the state of the art for image classification with the introduction of AlexNet [34]. CNNs are made of two types of layers. First, convolutional layers that apply filters to input data to extract feature maps, and pooling layers that downsample these feature maps to reduce their dimensionality [35]. Second, additional fully connected layers to make predictions based on the extracted features [34].

There is considerable empirical evidence that CNNs with more parameters tend to be more performant on many tasks. With 60 million parameters, AlexNet [34] was the first deep CNN that showed unprecedented success on large-scale and complex tasks, winning the 2012 ImageNet competition [36]. In [36] VGG networks were introduced. The publication demonstrated that increasing network depth significantly improved performance on the ImageNet challenge [36], providing evidence that deeper networks, and hence networks with more parameters, can learn more powerful representations. The VGG16 model has approximately 138 million parameters [36]. In [37], residual networks were introduced. This work demonstrated that deeper ResNet models perform better than shallower ones [37]. The smaller ResNet-18 model has (about) 11.7 millions parameters [38]; a number not being able to be matched with QBMs, due to the lack of sufficient quantum hardware.

To fairly compare QBMs with CNNs, we propose a CNN architecture with one convolutional layer and two sequential layers. In this setup, we are able to vary the number of parameters in a range similar to that of the QBM. The kernel size of the convolutional layer is 3 or 5 and always of dimension 1. The number of neurons of the first sequential layer is in $\{ 4 , 8 , 1 6 , 2 4 \}$ and for the second layer, it is in $\{ 2 , 4 , 8 , 1 6 \}$ , with the number of neurons of the second sequential layer never being higher than that of the first one.

After the convolutional layer and the first sequential layer we apply the

$$
{ \mathrm { R e L U } } ( x ) = \operatorname* { m a x } ( 0 , x )
$$

activation function. After the last sequential layer, the sigmoid activation

$$
\sigma ( x ) = \frac { 1 } { 1 + \exp \left( - x \right) }
$$

is applied, mapping the output to the interval $[ 0 , 1 ]$ and making it interpretable as a probability.

We use the binary cross entropy loss

$$
\ell ( x , y ) = - \left( y \log ( x ) + ( 1 - y ) \log ( 1 - x ) \right)
$$

for training our CNN. Let

$$
\mathrm { c n n } _ { \theta } : [ 0 , 1 ] ^ { 2 8 , 2 8 }  [ 0 , 1 ]
$$

be the CNN and

$$
( x _ { i } , y _ { i } ) _ { i = 1 , \ldots , n } \in [ 0 , 1 ] ^ { 2 8 , 2 8 } \times \{ 0 , 1 \}
$$

be our encoded data set, then the training of our CNN is the following optimization problem:

$$
\operatorname* { m i n } _ { \theta } \sum _ { i = 1 } ^ { n } \ell ( \mathbf { c n n } _ { \theta } ( x _ { i } ) , y _ { i } ) .
$$

Such optimization problems are solved with gradient-based methods. We used the Adam optimizer for training [39].

# III. EXPERIMENTS

# A. The MedMNIST data sets and their common performance metrics

All experiments are conducted using two datasets from the MedMNIST [2] collection of biomedical images: PneumoniaMNIST and BreastMNIST. Both datasets are preprocessed into grayscale images of size $2 8 \times 2 8$ pixels and provide a standardized train-validation-test split.

The PneumoniaMNIST dataset consists of 5856 pediatric chest $\mathrm { \Delta X }$ -ray images, categorized into two classes: pneumonia and healthy. It is derived from the substantially larger images in [40], from which the images where center-cropped and subsequently resized to match the low-resolution format. The data is divided into training and validation sets in a 9:1 ratio, with the original source validation set used as the test set.

The BreastMNIST dataset includes 780 breast ultrasound images and is based on $5 0 0 \times 5 0 0$ pixels large images from [41]. Due to the significantly lower resolution, the original three-class classification task (normal, benign, malignant) was reformulated as a binary classification problem, “normal” and “benign” being merged into a single positive class, while the “malignant” was treated as negative. The dataset is split into training, validation, and test sets using a 7:1:2 ratio.

Medical image datasets often exhibit substantial class imbalance, which can lead to misleading performance evaluations when using the standard accuracy (ACC) as a primary metric. For instance, approximately $7 3 \%$ of images in both PneumoniaMNIST and BreastMNIST are labeled positive, which means a constant estimator would achieve an accuracy of $7 3 \%$ .

The AUC-Score, which we, as well as [2], use in addition to the ACC, is derived from the ROC curve, which plots the true positive rate against the false positive rate at various thresholds. This metric can thus reflect the models’ ability to distinguish between class labels: A score of 0.5 indicates random guessing, whereas a score of 1.0 indicates perfect separation of class labels.

# B. Hyperparameter optimization

1) QBM: The performance of a BM is influenced by several hyperparameters, which makes an appropriate hyperparameter optimization essential. The selected hyperparameters and their corresponding search spaces are summarized in Table I.

We initially aimed to perform 100 hyperparameter optimization runs per data set, using 10 different random seeds each for, among other things, weights initialization. The results from these runs were to be averaged across seeds, and the optimization was conducted using the Bayesian search algorithm provided by the Weights and Biases framework [42]. In order to have the optimization respect both the accuracy and the AUC-score, we employed a composite score defined as $0 . 5 \cdot \mathrm { A C C } + 0 . 5$ AUC. However, conducting such extensive optimization using QBMs with QA would have exceeded our available access to D-Wave QA-hardware. Prior work [20], [21] has demonstrated that for the purpose of hyperparameter tuning, the Simulated Annealing algorithm (SA) can serve as a practical classical alternative to QA, as it also approximates a Boltzmann distribution [43]. Despite employing this alternative as as a workaround, long algorithm run times and unexpected failures of our classical hardware unfortunately limited the number of optimization runs we were able to complete in time for the preparation of this paper to only 20 runs on the BreastMNIST dataset and only 18 runs on the PneumoniaMNIST dataset regardless, meaning that not much optimization of hyperparameters took place in the end.

Still, the models achieving the best validation performance were then chosen to be retrained with QA, using our PQA strategy and 3 random seeds.

2) CNN: As with the QBM, we also used hyperparameter optimization to find the best architecture and optimization parameters for the CNN. The selected hyperparameters and the search space are summarized in Table II.

For the hyperparameter search for the CNNs we combined grid search [44] with random search [45]: For every pair of choices for kernel size and the number of neurons in the two consecutive sequential layers, making sure that the second layer has no more neurons than the first, we randomly picked 200 values for each of the other hyperparameters. Then, we picked the configuration that produced the highest combined score on the validation set – that is, the sum of the AUC-score and accuracy – during training.

The number of epochs was chosen high enough such that the validation performance stopped improving at the end of the training. Specifically, we ran 50 epochs for PneumoniaMNIST and 350 epochs for BreastMNIST. This difference is due to the datasets’ sizes; using more epochs for BreastMNIST ensured roughly the same number of gradient steps during training, given an equal batch size.

# C. Results

As outlined in Sec. II-D, our experimental objectives were to evaluate how our QBM-based approach compares to CNNs of equivalent size, which are traditionally used in image classification, as well as whether its performance scales with different model sizes. Some preliminary answers to both of theses questions can be found in Fig. 3: For each of the different numbers of hidden neurons explored in our respective hyperparameter optimizations, using the “QBM” with SA (shown as circles) as well as the CNN (shown as crosses), the plot displays the accuracy (ACC) and AUC-Scores of the best models found using these neuron numbers, with respect to the remaining hyperparameters mentioned in Tables I and II as well as the random seed used for initialization, on the test data sets. Notice that in the context of the “QBM” trained with SA, one cannot really speak of the hyperparameters of each of these configurations having been optimized – given that only very little runs per number of hidden neurons have been performed. Still, we suspect that even the performance metrics of the models that did not or only barely undergo hyperparameter optimization allow for some preliminary insights. Furthermore, the plots also show the best overall result obtained by running the QBM on quantum hardware (shown as $+$ ), as well as the results of the millions of parameters large state-of-art CNNs ResNet-18 and ResNet-50 from literature [2], shown in red. For the PneumoniaMNIST data set, displayed in Fig. 3a, we observe no clear trend of parameter numbers much influencing model performance for either of the models – with the exception of the (surely also in other ways optimized) ResNet approaches reaching significantly higher AUC-Scores. While all other approaches show AUC-Scores in a very similar range, we find that the CNN models can generally still outperform their QBM-based counterparts by a bit in terms of accuracy on this data set – some even improving upon ResNet in this regard. The best CNN variants also outperform their similarly sized QBM counterparts regarding the AUC-Score they reach, the overall best models matching each others performance in this regard. While having the CNNs outperform the QBMs is not surprising, given – among other things – their more extensive hyperparameter optimization, an interesting finding in this plot is that while still outperformed by most other models, the QBM trained with actual QA is fairly close in performance to some of the similary sized SA-trained models, and even outperforms some similarly sized CNNs, both in terms of ACC and AUC-Score. On the smaller BreastMNIST data set (Fig. 3b), however, results look vastly different in a lot of aspects: While here, the SA-trained QBMs still do not show a clear trend regarding a possible interconnectedness of performance and parameter number, CNNs do show a fairly clear trend regarding their improvement with size, with ResNet also clearly showing the best performance regarding both metrics this time. Additionally, the QA-based QBM is again noticeably outperformed by the majority of other the examined models, particularly the slightly larger CNNs, when evaluating AUC Score. However, it demonstrates a moderately average ACC, surpassing many of the smaller models in this regard. Quite some of SA-based QBMs do, however, outperform their similarly – or a bit larger – sized CNN counterparts here. Taken together, we do not see any clear conclusions that can be drawn from these two experiments about the general (medical) image classification performance of QBMs in comparison to CNNs just yet. We can, however, say that the number of parameters does not seem to have a huge impact on model performance for QBMs (even though we do not know with certainty that this would not change when employing more hyperparameter optimization).

Table I: Hyperparameter Ranges for the QBM (SA) and the Best Hyperparameter for Each Dataset   

<html><body><table><tr><td></td><td>Hidden Units</td><td>Epochs</td><td>Batch Size</td><td>LearningRate</td><td>Sample Count</td></tr><tr><td>Value ranges</td><td>1-20</td><td>1-20</td><td>1-100</td><td>0.00001-0.6</td><td>10-1000</td></tr><tr><td>PneumoniaMNIST</td><td>10</td><td>20</td><td>73</td><td>0.45295</td><td>100</td></tr><tr><td>BreastMNIST</td><td>8</td><td>13</td><td>12</td><td>0.43496</td><td>400</td></tr></table></body></html>

Table II: Hyperparameter Ranges for the CNN and the Best Hyperparameter for Each Dataset   

<html><body><table><tr><td></td><td>Kernel Size</td><td>Neurons 1st Sequential Layer</td><td>Neurons 2nd Sequential Layer</td><td>Learning Rate</td><td>β1</td><td></td><td>Batch Size</td></tr><tr><td>Value ranges</td><td>3,5</td><td>4,8,16,24</td><td>2,4,8,16</td><td>[0.0005,0.05]</td><td>[0.998,0.9999]</td><td>[0.98,0.999999]</td><td>2²,...,210</td></tr><tr><td>PneumoniaMNIST</td><td></td><td>16</td><td>8</td><td>0.00384</td><td>0.98428</td><td>0.99925</td><td>16</td></tr><tr><td>BreastMNIST</td><td>55</td><td>24</td><td>8</td><td>0.00117</td><td>0.98674</td><td>0.99931</td><td>8</td></tr></table></body></html>

![](images/e7e5e54292476b164f17a12f861464c06c1d392f72739a067a4821ad71da276f.jpg)  
Fig. 3: The figures compare the performance of the QBMs and CNNs on the PneunomiaMNIST and the BreastMNIST data sets. In each panel, the models’ test AUC and ACC-scores are presented, while a color map indicates the number of parameters. For additional context, we included the results for two variants of the ResNet CNN [37] as reported in [2]. They are highlighted in red because their parameter counts exceed the colormap’s representable range: ResNet-18 has 11.69 million parameters [38], and ResNet-50 about 25.56 million [46].

Thus, we want to take a closer look at the average performance of the models with the absolute best hyperparameter configurations we found so far (regarding classification performance on the validation set after the last epoch) – independent of parameter size and initialization with random seeds. The selected hyperparameters used for this are summarized in Table I. As one of our previous works on medical image classification using SA-based QBMs [20] found signs that the usage of QBMs might not only reduce sampling time, but also the number of epochs necessary to reach good classification performance, we investigate the classification performance on the test set after each epoch of training when doing so.

The result, showing the average test accuracies and average test AUC-Scores across different random seeds, as well as their standard deviations, can be seen in Fig. 4. We would like to point out that, as the QBM (QA) was only run with three seeds due to machine-failure-related time constraints, we do not consider its standard deviation representative enough to draw any conclusions from that. To make at least the results for the QBM (SA) and the CNN directly comparable here, only the results of ten random seeds are plotted here for both models. Comparing the standard deviations for both these models, we notice, at least on the Pneumonia data set, that the CNNs standard deviation is a lot larger. This increased variability is due to the fact that in multiple training runs, the CNN consistently predicted the majority class, which led to the gradients vanishing — a common issue when training CNNs [47]. Although these vanishing gradients affected the average performance, they did not impact the highest performance achieved in our training runs. We do not observe this effect on the BreastMNIST data set, however, where the standard deviations are fairly comparable.

Comparing the average performance metrics on the PneumoniaMNIST data set, we notice that both QBM versions seem to clearly outperform the CNN both in terms of ACC as well as in terms of AUC-Score – at least in the first 25 epochs displayed here. Remarkably, they already reach their high classification performances within the first 8 epochs – even the first 5 ones for the QBM (QA). A similar observation can be made for the BreastMNIST data set: While here, the CNN seems to be “catching up” with the QBMs over the course of the training, at the very least with the QBM (SA) in terms of accuracy, the QBMs clearly outperform the CNN in the first 10 epochs of training in terms of both metrics – again with the QA-based version reaching its maximum performance (for the first time) earlier than the SA-based one. Within the first 7 epochs of training, the QBM (QA) outperforms the

QBM (SA) in terms of both metrics, and even in the longer run it still reaches a comparable performance. This suggests that the QBM (QA) suffers very little from hardware noise in this type of application, or perhaps not at all.

Regarding absolute classification performance, the QBMs also provide decent results on the PneumoniaMNIST data set: The average test accuracy at the final epoch in case of the QBM (SA) is $8 5 . 1 0 \%$ and the AUC-Score is 0.8208, the second value of which almost comes close to values reached by large models like ResNet in literature [2], while our QBM (QA) reaches $8 4 . 0 3 \%$ and 0.7996, respectively. In case of the BreastMNIST dataset, the performance of all models deteriorates significantly, however. The QBM (SA) achieves a test accuracy of $7 5 . 0 6 \%$ and an AUC-Score of 0.6677, while the QBM (QA) reaches $7 6 . 2 8 \%$ and an AUC-Score of 0.5946. This decline is likely attributable to the limited size of the training set (546 images [41]), posing challenges for effective generalization.

In addition to evaluating the classification performance of our QBMs, we also conduct a small analysis of our PQA approach in terms of QPU time: For QBM configurations of up to 20 hidden units, we tracked the QPU time expenditure of QBM training with both regular sequential, as well as our parallel QA, in seconds for 3 mini-batches a\` 5 data points each, generating 1000 samples for both the clamped and unclamped training phases with each data point. The results are shown in Fig. 5. Here, PQA unsurprisingly exhibits a much more favorable QPU time usage compared to sequential QA: Averaged over every possible configuration, our PQA approach exhibits a speed up of $6 9 . 6 5 \%$ . Furthermore, it also seems to scale more stably and favorably with increasing network sizes.

# IV. DISCUSSION AND CONCLUSION

In this work, we presented an approach for image classification using quantum annealing (QA)-based Quantum Boltzmann Machines (QBMs) which are efficiently trained in a supervised manner using Parallel Quantum Annealing (PQA): Our approach involves simultaneously embedding several instances of the QBM into a quantum annealer’s hardware graph by embedding them into artificially separated subgraphs. The resulting distance of the instances’ embeddings is thought to preserve sample quality [22], while enabling the usage of QA to draw several Boltzmann samples for the QBM’s training process in parallel. This way, we achieve a speed-up of $6 9 . 6 5 \%$ compared to the usage of regular sequential QA. Although our experiments remain inconclusive regarding the QBM’s ability to outperform classical CNN models of similar size in terms of classification performance, we find that they on average can reach decent classification metrics, similar to those of the CNNs, within much shorter numbers of epochs. Taking together these two aspects, we support future research into the proposed approach, as we deem it not unlikely that a future quantum speed-up could be achieved in this area.

Future work should first address the limitations of our current research:

![](images/48aba77bcf4d724e535433c84980f6c24a6697e34df6ffeb70d086a61c5a0d03.jpg)  
Fig. 4: Test accuracies and AUC-Scores of QBMs trained using SA and QA, as well as a classical CNN with the best identified hyperparameters settings, per epoch. The solid lines represent the average performance over 10 (QBM (SA) & CNN) respectively 3 (QBM (QA)) independent seeds. The transparent area around them is the standard deviation.

![](images/c7848be6c26c851f700affec1baf1ef8f7ff89b4d72fa99a4ae493d97df95a1b.jpg)  
Fig. 5: The plot shows the total QPU time required to perform 3 batches of 5 steps each, with every step generating 1000 samples for both the clamped and unclamped phase of QBM (QA) training. Results are presented for both standard sequential QA and our approach using PQA.

Our experiments should be repeated, this time involving a consistent and extensive approach to hyperparameter optimization, especially for the QBM, to ensure that the best possible performance of each type of model is actually obtained. Ideally, the optimization should already be executed using QA, as SA, albeit resulting in a similar sample distribution, does not outright simulate it and thus does not necessarily return the exact same parameters. The search also should include the optimization of $T$ , which we consciously left out in this work due to the limited availability of QPU time. Furthermore, large numbers of random seeds should be used across all models to enable statistically significant comparisons of e.g. their sensitivity to “bad” weight initializations for a given data set. And moreover, the ranges of hyperparameter search should be extended, e.g. to investigate a QBM’s performance using larger numbers of epochs as well.

Secondly, it would be desirable to execute experiments on future generations of hardware, which offer more qubits and a denser connectivity, to enable the embedding of larger QBMs, in terms of hidden units, while guaranteeing similar or lower levels of noise.

And thirdly, it would be desirable to benchmark our QBM on datasets containing larger images or multiple classes, to see how its performance scales in comparison to that of classical models like (larger) CNNs with increasing problem complexity. Since, as mentioned above, input units only function as bias terms to the remaining units of the QBM within the framework of discriminative learning, the number of units in our model that need to be represented by logical qubits, which are embedded into the quantum annealer’s hardware graph, is determined solely by the sizes of the hidden and label layers. Thus, the additional input units required to represent larger input images, with more pixels, would not impact the size of the embedded QBM instances our model uses. Furthermore, when increasing the number of classes, the number of output units needed scales only linearly with the number of classes our model is supposed to be able to represent, as we need one output unit per class. This means that unless the number of classes in the data set becomes exceedingly high – which might be the case for large generic data sets like ImageNet [48], but is not expected for medical ones – increasing the number of classes does not have much negative impact on the embeddability of the QBM instances either. It remains e investigated whether increasing the number of hidden units, in order to increase the parameter count of the QBM, would be necessary to ensure that a desirable classification formance can be reached on these more complex However, preliminary results on the data se this work that increasing or reducin thus number of hidden units re critical impact on our QBM mod some classical models like even though t also remains to be investigated ho in terms of the rather low training scale with increasing dat plexit we would expect our approach to scale rath ore omp ta sets, as the PQA should still be usable without difficulty. Whether the approach could then outperform larger CNNs on these tasks in terms of classification performance or training speed, e.g. by again needing only a smaller number of epochs, remains to be seen. And finally, it would be interesting to compare the QBM as we use it in our work to more similar classical models, given that the CNNs we use as a classical baseline in this work, while being especially well suited to the task of image classification, process these images in a very different way that makes use of the spacial structure of the input features (compare Sec. II-E), while the QBM works on flattened inputs. Thus, while CNNs might be a good baseline to investigate whether the QBMs perform well, a comparison against them is not particularly well suited to investigate whether any potential advantages we see when using our model stem from using quantum computing. While of course we already present one of the most direct classical comparisons possible in this work in form of the QBM (SA) model, it might be interesting in this context to also compare our QBM to more common comparable classical models such as (Classification) Restricted

Boltzmann Machines [49], [50], classical general Boltzmann Machines trained using typical Boltzmann learning [8] 1, Deep Boltzmann Machines (DBMs) [53], [54] or even convolutional Deep Boltzmann Machines [55]. Another way to limit the issues of comparability between CNNs and our QBM model, and possibly also improve its performance by incorporating techniques that make CNNs so successful at processing data with spatial dependencies, such as image classification, involves modifying the model architecture toward a design resembling classical convolutional DBMs, for which training remains demanding [53]–[55]. This design would consist of multiple layers of hidden units arranged hierarchically and incorporate convolutional connections, both enabling the model to learn increasingly abstract data representations and capture higher-order feature correlations while reducing parameter count [53]–[55]. This might create a quantum model that can be efficiently trained to perform accurate classification.