# NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining

Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh, Georgii Fedorov, Bulat Suleimanov, Vladimir Dokholyan, Aleksandr Gordeev Layer Team, SALUTEDEV https://riko0.github.io/No-Humans-Required/

![](images/17d973d8b33efc0f6c991be2fef374b7abea0d8e57624f014cdcb7f2a2f0a0f7.jpg)  
Figure 1. High-quality samples from our NHR-Edit dataset.

# Abstract

Recent advances in generative modeling enable image editing assistants that follow natural language instructions without additional user input. Their supervised training requires millions of triplets original image, instruction, edited image , yet mining pixel-accurate examples is hard. Each edit must affect only prompt-specified regions, preserve stylistic coherence, respect physical plausibility, and retain visual appeal. The lack of robust automated editquality metrics hinders reliable automation at scale. We present an automated, modular pipeline that mines highfidelity triplets across domains, resolutions, instruction complexities, and styles. Built on public generative models and running without human intervention, our system uses a task-tuned Gemini validator to score instruction adherence and aesthetics directly, removing any need for segmentation or grounding models. Inversion and compositional bootstrapping enlarge the mined set by $\approx 2 . 2 \times$ , enabling large-scale high-fidelity training data. By automating the most repetitive annotation steps, the approach allows a new scale of training without human labeling effort. To democratize research in this resource-intensive area, we release NHR-Edit, an open dataset of $3 5 8 k$ high-quality triplets. In the largest cross-dataset evaluation, it surpasses all public alternatives. We also release Bagel-NHR-Edit, an opensource fine-tuned Bagel model, which achieves state-of-theart metrics in our experiments.

# 1. Introduction

In recent years, we have witnessed an unprecedented acceleration in generative modeling, which has facilitated imageediting assistants that follow purely natural language instructions. Creating such expert editors typically involves a multi-stage process. It begins with foundational pretraining on large, diverse, but often noisy datasets (e.g., [9, 17, 24, 43, 52, 53, 60]). This crucial stage adapts a base text-to-image model for instruction-guided editing, teaching it to process an input image, execute a wide array of user-specified edits, and, most importantly, preserve the unedited regions across diverse visual domains. Following this, a second stage of initial SFT on smaller (from thousands to hundreds of thousands examples), meticulously curated datasets can elevate performance on specific tasks. ObjectDrop [10] and OmniPaint [36] have shown that as few as 2500-3300 high-quality examples can teach a model to master a task like object removal with remarkable precision.

The third stage is continual supervised fine-tuning (SFT) and preference optimization [33, 42], intended to make the model able to handle more complex edits and to iteratively improve its overall quality. This stage, however, presents a critical data bottleneck, as it requires a scalable method for generating a continuous stream of high-fidelity training examples.

Scaling this stage remains constrained by reliance on human annotators who must review millions of routine, pixel-level candidate edits to verify instruction fidelity and context preservation. Such reviews are not the best uses of expert attention; human expertise matters most on subtle, ambiguous, or creative cases. Existing large-scale data collection methods, however, have fundamental drawbacks:

• Complex synthetic pipelines and data poisoning: Methods using a cascade of external tools (e.g., for grounding [27], segmentation [25], and inpainting [37]) not only create visual artifacts but can lead to a more tricky problem. The model could learn a spurious correlation when an imperfect "remove" edit with inpainting artifacts is inverted to an "add" operation. Instead of understanding the instruction’s true semantic meaning, it learns to use the artifacts as a spatial cue, correctly guessing where to place the object. This effectively poisons the training data.   
• Manual and semi-manual methods: Approaches like 3D rendering [14] lack realism and scalability, while video frame extraction [28] depends on complex and error-prone auxiliary models.   
• The validation bottleneck: the lack of a reliable validation metric sensitive enough to detect subtle defects. While the image editing field has recently shifted towards using Multimodal Large Language Models (MLLMs) as evaluators [43, 45, 52], we found that even top-ranked

proprietary models like Gemini $2 . 5 ~ \mathrm { P r o }$ [19] are insufficient for this task. To overcome this, we developed a specialized validator by fine-tuning Gemini-2.0-flash [20] on human scoring data (3.2.

We posit that the latent potential of a model that has undergone initial SFT remains under-exploited. By utilizing the model’s new abilities and sensitivity to stochastic initialisation, the editor itself can be transformed into a virtually unlimited source of high-quality synthetic data. To realize this potential, we introduce a novel, end-to-end tripletmining pipeline. Our pipeline begins with a source image, which can be a real-world photo or one we generate synthetically. The framework performs multiple editing iterations on the image for each instruction, producing a diverse set of candidate outputs.. These candidates first go through a coarse pre-filtering step to eliminate obvious failures. The ones that pass are then carefully judged by our fine-tuned Gemini-2.0-flash validator [20]. For each instruction, the validator picks only the single best edit, and only if it meets our strict quality standards (detailed in Algorithm 3). This two-stage filtering process is crucial for ensuring the final dataset’s quality.

This self-contained framework unlocks several unique capabilities for continual learning:

• Direct complexity measurement for curricula: The difficulty of any instruction for the current model can be quantified by counting the attempts required for a successful edit. This provides a direct, empirical signal to implement an easy-to-hard learning curriculum. • Targeted weakness correction: Rare successes can be mined. Even if a model succeeds at a complex task only once in a while, it can be run repeatedly to harvest a targeted dataset that fixes that specific weakness. • Compositional edit synthesis: Complex training data can be created by combining multiple instructions of any type: simple or compositional, forward or inverted. For example, it is possible to form a single, multi-part instruction that executes two additions, one deletion, and a global style change in a single pass. • Flexible input sourcing: The framework can operate on both real-world user images and synthetically generated inputs, each offering distinct advantages. Real images provide authentic user scenarios with natural lighting conditions and artifacts that the base editing model must learn to handle while preserving unedited regions. Synthetic images enable exploration of the long-tail distribution, including scenarios that are impossible or impractical to photograph (e.g., a corgi in a spacesuit standing on a rocket flying through space), ensuring comprehensive coverage of potential editing requests. • Unparalleled simplicity and flexibility: The framework is model-agnostic and requires no external specialist models for segmentation, depth estimation, or grounding.

To demonstrate effectiveness, NOHUMANSREQUIRED DATASET (NHR-Edit) is released as a publicly available dataset of $3 5 8 \mathrm { k \Omega }$ rigorously validated triplets. Building on this data, we release BAGEL-NHR-EDIT, a LoRA-tuned BAGEL[15] variant trained on NHR-Edit that surpasses the base model on two benchmarks, indicating that targeted fine-tuning with NHR-Edit improves instruction-guided image editing capability. The primary contribution is an endto-end pipeline that serves as a powerful engine for advancing research in self-improving generative models [12, 59].

# 2. Related Work

Our research builds upon two main pillars of generative modeling: methodologies for creating instruction-based editing data and the paradigm of model self-improvement through preference optimization.

# 2.1. Methodologies for Editing Data Generation

Creating high-quality editing triplets is a foundational challenge. Existing large-scale approaches can be categorized by their data generation strategy, each presenting a unique set of trade-offs in complexity, fidelity, and scope.

Pipelines on Real-World Data. The most common strategy involves a cascade of models to edit real images. Methods like AnyEdit [53] and ImgEdit [52] typically use a pipeline for open-vocabulary object detection (e.g., GroundingDINO [27], YOLO-World [13]), segmentation (SAM [18, 25]), and inpainting (e.g., [5, 37]). While grounded in realistic photos, each stage in this cascade can introduce errors that propagate downstream, and global edits using methods like ControlNet [56] can struggle to preserve fine details. Sourcing pairs from video, as in Step1XEdit [28], introduces further complexity: their pipeline uses BiRefNet [61] and RAFT [39] filter out dynamic backgrounds and estimate foreground motion, which is then described by an MLLM to generate the edit instruction. Furthermore, this approach can suffer from dataset bias; for instance, ImgEdit builds on LAION Aesthetic [35], which may not reflect typical user photos.

Fully Synthetic Generation. To gain more control, other methods generate data synthetically. AURORA [14] uses 3D rendering, which offers high control but is laborintensive, struggles with photorealism, and is ill-suited for global style edits. UltraEdit [60] uses diffusion inversion to derive a noise latent from a real photo, but the inversion process itself can introduce artifacts. HQ-Edit [24] generates diptychs with DALL-E 3 [3], which tends to alter fine-grained details. Seed-Data-Edit [17] also employs a synthetic pipeline using techniques like Plug-and-Play [41], which, while flexible, generates data that may not align with real-world distributions.

Specialist Models. A distinct approach, taken by OmniEdit [43], is to train separate, specialized diffusion models for each editing task and integrate them into the pipelines similar to described above. For example, their inpainting specialist uses mask from GroundingDINO $[ 2 7 ] + \mathrm { S A M }$ [25] detection and segmentation steps, while their attribute modification pipeline consists of GroundingDINO $\mathsf { \Omega } + \mathsf { S A M }$ followed by Prompt-to-Prompt [21]. While this approach can ensure higher quality for simple, predefined operations, it inherits the same cascade complexity and error propagation issues as other pipeline-based methods. Furthermore, it cannot generate complex, compositional instructions that combine multiple editing operations.

In summary, existing methods navigate a complex landscape of trade-offs. Our work offers a different approach: by using the editor model itself as the data source, our simple, general-purpose framework bypasses the need for external specialist models for segmentation, inpainting, 3D assets, or complex video analysis.

# 2.2. The Metric Gap in Image Editing

A key challenge in our field is evaluation. Traditional metrics are doubly flawed for this task: they have been shown to correlate poorly with human preferences for editing, and furthermore, their reliance on ground-truth references makes them unsuitable for our generative framework. This includes image-based metrics that require a target image, such as LPIPS [57], DINO (as proposed in [34], using [11]), and CLIP-I (as proposed in [16], using [32]), as well as textbased metrics like CLIPScore [22], which requires a target caption. In Image Quality Assessment (IQA), models like VisualQuality-R1 [46] and Q-Align [44] teach MLLMs to assess image quality. In Text-to-Image (T2I) generation, a diverse toolkit has emerged, including MLLMs like VisionReward [50], BLIP-based model like ImageReward [49], and CLIP-based models like HPSv2 [47] and PickScore [26]. Even the Text-to-Video (T2V) domain has specialized metrics like VQ-Insight [58]. The use of MLLMs as reference-free evaluators for image editing was pioneered by VIEScore [45], which showed that GPT-4o judgments correlate with human preferences better than CLIP-I, DINO and LPIPS. Subsequent work built on this approach: OmniEdit [43] distilled these judgments into a smaller model, and ImgEdit [52] fine-tuned Qwen-2.5-VL [31] for the same purpose. However, the precision required to curate data for high-quality SFT demands a higher standard. Our findings indicate that even top-performing foundation models are insufficient out-of-the-box. For instance, Gemini 2.5 Pro [19], despite leading public vision leaderboards [29], fails to reliably detect subtle yet critical editing flaws. We therefore developed a specialized validator by fine-tuning Gemini-2.0- flash [20] on human preference data to achieve the required sensitivity.

# 2.3. Self-improvement and Iterative Learning

The concept of a model generating its own training data to refine its capabilities has proven highly effective. In NLP, this was demonstrated through methods like rejection sampling in Llama 2 [40] and preference optimization algorithms like IPO [33]. More recently, this paradigm has been extended to generative computer vision, with self-play finetuning for text-to-image models [2, 54] and preference optimization for text-to-video [51]. Our framework serves as a direct, automated engine for applying these state-of-the-art preference alignment techniques to the more complex domain of image editing. Modern algorithms like DPO [42] and KTO [4] depend on a scalable source of preferencelabeled data, which our pipeline provides automatically:

• For KTO, which requires labeled "good" and "bad" examples, our framework’s output can be used directly. The pointwise scores from our validator provide a clear signal, where edits above a certain quality threshold are labeled as "successes" and those below are labeled as "failures". • For DPO, which requires preference pairs, our framework’s "one-to-many" generation process is a natural fit. For any given source image and instruction, we can sample two edited candidates from the multiple attempts. The one with the higher validator score is designated the "winner", and the one with the lower score becomes the "loser", creating a correctly formatted preference pair $\langle y _ { w } , y _ { l } \rangle$ .

By solving the data generation and labeling bottleneck, our work makes it practical to explore these powerful selfimprovement techniques for the nuanced task of instructionbased image editing.

# 3. Methodology

This section details the autonomous triplet-mining pipeline. The framework is composed of four primary modules: (i) a prompt engineering module that generates mutually consistent text-to-image (T2I) and image-to-image (I2I) instructions; (ii) a high-fidelity T2I generator; (iii) an instructionguided image editor; and (iv) a multi-stage validation stack to ensure triplet quality.

# 3.1. Automated Mining Pipeline

Figure 6 and Algorithm 3 provide a schematic overview of the pipeline. The process begins with a set of initial constraints or descriptions (Algorithm 1), such as a topic, domain, style, edit category, or any other conditions. These conditions are supplied manually in this work, but an adaptive agent designed to guide an iterative self-improvement loop can also be used. The prompt engineering module uses these constraints to produce a T2I prompt $( p _ { \mathrm { t 2 i } } )$ and a corresponding set of coherent edit instructions $( \{ p _ { e } \} _ { k } )$ , as demonstrated in the example in Listing 1.

\\ T2I prompt   
"prompt": "A living room with a large window: a small cactus on the windowsill, a half-eaten bowl of cereal on the coffee table, a remote control, a crocheted blanket, and a dog toy on the rug.",   
\\ I2I prompts for editing   
"edits": [ "Get rid of that cactus.", "Remove the cereal bowl.", "No remote control, thanks.", "Lose the crocheted blanket.", "Eliminate the dog toy.", "Remove the cactus, cereal, remote, blanket, and toy"   
]

For each T2I prompt, the pipeline generates $N$ candidate source images $( I _ { 0 } )$ using different random seeds (Algorithm 2). Each source image undergoes $M$ edit attempts for every instruction $p _ { e }$ . This process yields a large pool of candidate triplets $\langle I _ { 0 } , p _ { e } , I _ { e } \rangle$ . These candidates are subjected to a coarse pre-filtering step before final validation (see 3.2). In the final stage, for each unique source image and edit prompt pair $\left. I _ { 0 } , p _ { e } \right.$ , the highest-quality edited image $I _ { e } ^ { \star }$ is selected based on a scoring prompt (see Algorithm 3). This image is added to the final dataset $\mathcal { D }$ only if its scores exceed predefined quality thresholds.

<html><body><table><tr><td>Algorithm1SAMPLESPROMPTSDESIGN</td></tr><tr><td>Input: Task description PA.1</td></tr><tr><td>Output: SetP={(𝑝t2i,{𝑝e}k)},</td></tr><tr><td>1: P ← OpenAI o3(PA.1)</td></tr><tr><td>2:return P</td></tr></table></body></html>

# 3.2. Validation Framework

A key challenge in automated triplet mining is the robust validation of edited images. A proficient validator must therefore combine strong visual reasoning with specialization for the editing task.

To meet this requirement, a two-stage validation process was designed. First, a pre-filter using Qwen-VL $\mathbf { 7 2 B }$ discards obvious failures. This reduces the number of calls to the more powerful and computationally expensive final validator, but it is acknowledged that an open-source level model cannot filter out all the noise in such a broad

Input: T2I prompt $p _ { \mathrm { t 2 i } }$ , edits $\{ p _ { e } \} _ { k }$ , parameters $N , M$   
global GPU-hour budget Budget   
Output: Candidate pool $\mathcal { C }$   
1: $\mathcal { C }  \emptyset , J o b s  \emptyset$   
2: for $i \gets 1$ to $N$ do   
3: $\mathbf { s e e d } _ { i } \gets \mathbf { R a n d o m } ( i )$   
4: I0 FLUX.1-schnell(pt2i, seedi)   
5: if not $\mathrm { Q w e n } _ { \mathrm { 7 B } } \left( I _ { 0 } , \mathcal { P } _ { A . 5 } \right)$ then   
6: continue   
7: for all $p _ { e } \in \{ p _ { e } \} _ { k }$ do   
8: for $j  1$ to $M$ do   
9: $J o b s \gets J o b s \cup \{ ( I _ { 0 } , p _ { e } , \mathrm { R a n d o m } ( j ) ) \}$   
10: while $J o b s \neq \emptyset$ and GPU_hours $<$ Budget do   
11: sample $( I _ { 0 } , p _ { e } , s ) \sim \mathrm { U n i f o r m } ( J o b s )$   
12: $J o b s \gets J o b s \setminus \{ ( I _ { 0 } , p _ { e } , s ) \}$   
13: $I _ { e }  \Pi 2 \mathrm { I }$ Dit (internal) $( I _ { 0 } , p _ { e } , s )$   
14: $\begin{array} { r } { \left( s _ { \mathrm { a e s } } , s _ { \mathrm { a d h } } \right) \gets \mathrm { Q w e n } _ { 7 2 \mathrm { B } } \left( I _ { 0 } , p _ { e } , I _ { e } , \mathcal { P } _ { A . 2 } \right) } \end{array}$   
15: if $s _ { \mathrm { a e s } } \geq T _ { \mathrm { a e s } }$ and $s _ { \mathrm { a d h } } \geq T _ { \mathrm { a d h } }$ then   
16: $c h e c k _ { p } \gets \mathrm { Q w e n } _ { 7 \mathrm { 2 B } } ( I _ { 0 } , p _ { e } , I _ { e } , \mathcal { P } _ { A . 3 } , \mathcal { P } _ { A . 4 } )$   
17: checkl LowLevelCheck(I0, Ie)   
18: if checkp and checkl then   
19: $\mathcal { C }  \mathcal { C } \cup \{ \langle I _ { 0 } , p _ { e } , I _ { e } \rangle \}$   
return

Input: Task description in PA.1, parameters N, M, Taes, Tadh, Tinv,aes, Tinv,adh ▷ 3.5   
Output: Final dataset $\mathcal { D }$ 1: $\mathcal { D }  \emptyset$ , $P o o l \gets \emptyset$   
2: $\mathcal { P }  S$ AMPLESPROMPTSDESIGN $\left( \mathcal { P } \right)$   
3: for all $( p _ { \mathrm { t 2 i } } , \{ p _ { e } \} _ { k } ) \in \mathcal { P }$ do   
4: $P o o l  P o o l \cup \mathrm { T R I P L E T M I N I N G } ( p _ { \mathrm { t 2 i } } , \{ p _ { e } \} _ { k } , N , M )$   
5: for all distinct $\langle I _ { 0 } , p _ { e } \rangle$ in Pool do   
6: $S \gets \{ I _ { e } \ | \ \langle I _ { 0 } , p _ { e } , I _ { e } \rangle \in P o o l \}$   
7: $s _ { \mathrm { a e s } } ( I _ { e } )$ , $s _ { \mathrm { a d h } } ( I _ { e } ) \gets \mathrm { G e m i n i } \left( I _ { 0 } , p _ { e } , I _ { e } , \mathcal { P } _ { A . 2 } \right)$ for every $I _ { e } \in \mathcal { S }$   
8: $\mathcal { S }  \{ I _ { e } \in \mathcal { S } \mid s _ { \mathrm { a e s } } \geq T _ { \mathrm { a e s } } \land s _ { \mathrm { a d h } } \geq T _ { \mathrm { a d h } } \}$   
9: if $ { \boldsymbol { S } } \neq \boldsymbol { \emptyset }$ then   
10: $\begin{array} { r l } & { I _ { e } ^ { \star } \gets \arg \operatorname* { m a x } _ { I _ { e } \in { \cal S } } \sqrt { s _ { \mathrm { a e s } } ( I _ { e } ) s _ { \mathrm { a d h } } ( I _ { e } ) } } \\ & { \mathcal { D } \gets \mathcal { D } \cup \{ \langle I _ { 0 } , p _ { e } , I _ { e } ^ { \star } \rangle \} } \end{array}$   
11:   
12: $\mathcal { D }  \mathcal { D } \cup$ APPLYINVERSIONS $( \mathcal { D } )$ ▷ 3.6   
13: $\mathcal { D }  \mathcal { D } \vert$ APPLYBOOTSTRAPS $( \mathcal { D } )$ ▷ 3.6   
14: return $\mathcal { D }$

and generic task. The second stage employs a specialized Gemini 2.0 Flash model, fine-tuned on a curated corpus of instruction-editing examples, to assign final scores for aesthetic quality and instruction adherence.

Low-level check. The absolute-difference image $D \ = \ | I _ { e } \ - \ I _ { 0 } |$ is thresholded $( > 4 0 )$ and analysed with

ConnectedComponents (OpenCV [8], Spaghetti backend [7]) using 4-connectivity and 32-bit labels; a triplet is discarded if the largest connected component covers $< 0 . 5 \%$ of all pixels flagged as changed. This purely heuristic, optional filter empirically outperforms a raw image-difference threshold, removing silent failures where the output appears plausible yet ignores the instruction, though about $3 \%$ of such cases still bypass the soft pre-filter validation (Table 3).

# 3.3. Gemini Validator

Modern editing pipelines often evaluate generated edits with large proprietary vision–language models such as GPT-4o [24, 43, 45]. While convenient, these generalpurpose assessors are not optimised for fine-grained pixellevel changes (see Fig. 5). To obtain reliable quality estimates, we fine-tune a Gemini-2.0-flash [38] model is fine-tuned on a dedicated human-annotated corpus. This corpus was meticulously constructed to cover a wide spectrum of edit qualities. The triplets were generated using a combination of in-house trained DiT editor, and leading proprietary models, including Grok [48] and Gemini [38], operating on both real-world photographs and synthetically generated images. This diverse sourcing strategy ensured that the assessor was trained on a broad distribution of potential successes and failures, preventing overfitting to the idiosyncrasies of a single generator.

Following HQ-Edit [24], OmniEdit [43] and AnyEdit [53], each edited image is rated on two five-point scales: (i) Instruction score, measuring semantic compliance with the textual edit; (ii) Aesthetics score, reflecting overall visual appeal. The collected set contains 2 998 training and 827 validation examples; every example is judged by two to four independent raters. Inter-rater reliability, estimated as the mean pair-wise Spearman correlation, equals $\rho { = } 0 . 4 1 { \pm } 0 . 0 9$ for Aesthetics and $\rho { = } 0 . 6 4 { \pm } 0 . 0 5$ for Instruction, corresponding to moderate and substantial agreement in the Landis–Koch nomenclature. The higher consistency on the instruction axis is expected — semantic correctness is less subjective than perceived beauty — while the aesthetic agreement is on par with other public datasets, where visual preference typically shows larger variance. To aggregate scores, each rating is first normalized by subtracting the annotator’s mean deviation from the global average, which mitigates individual rater bias. The final score for each triplet was then computed as the mean of these normalized ratings.

This de-biasing process consists of two steps. First, the bias $b _ { j }$ for each rater $j$ is calculated as the difference between their personal average score and the average of the mean scores for the exact set of triplets they rated:

Table 1. Quality metrics of the Assessor model on validation data   

<html><body><table><tr><td>Model</td><td>Instr. MAE↓</td><td>Instr. ρ↑</td><td>Aesth.MAE↓</td><td>Aesth.ρ↑</td></tr><tr><td>Qwen 2.5 72B</td><td>0.961</td><td>0.551</td><td>0.839</td><td>0.361</td></tr><tr><td>Gemini-2.5-pro</td><td>0.869</td><td>0.609</td><td>0.915</td><td>0.523</td></tr><tr><td>Gemini-2.0-flash</td><td>1.241</td><td>0.359</td><td>1.063</td><td>0.245</td></tr><tr><td>Gemini-2.0-flash (finetune)</td><td>0.503</td><td>0.815</td><td>0.568</td><td>0.631</td></tr></table></body></html>

$$
b _ { j } = \underbrace { \left( { \frac { 1 } { | N _ { j } | } } \sum _ { i \in N _ { j } } s _ { i , j } \right) } _ { { \mathrm { R a t e r ~ j } } \mathrm { s ~ m e a n ~ s c o r e } } - \underbrace { \left( { \frac { 1 } { | N _ { j } | } } \sum _ { i \in N _ { j } } \left( { \frac { 1 } { | R _ { i } | } } \sum _ { k \in R _ { i } } s _ { i , k } \right) \right) } _ { \mathrm { M e a n ~ s c o r e ~ o f ~ t r i p l e t s ~ r a t e d ~ b y ~ j ~ } }
$$

where $N _ { j }$ is the set of triplets rated by rater $j$ , and $R _ { i }$ is the set of all raters for triplet $i$ .

The final score $S _ { i }$ for a triplet is then the mean of the bias-corrected scores from each rater:

$$
S _ { i } = \frac { 1 } { | R _ { i } | } \sum _ { j \in R _ { i } } ( s _ { i , j } - b _ { j } )
$$

Using this annotated validation set, we benchmarked our task-specific, fine-tuned Gemini 2.0-flash model against several baselines: its original version (Gemini 2.0-flash), the larger Gemini $2 . 5 \mathrm { - } \mathtt { p r o }$ [38], and Qwen 2.5 72B (our pipeline’s pre-validator, see Section 3.2). Table 1 compares the mean absolute error (MAE) and Spearman $\rho$ on the validation set. Vanilla checkpoints suffer from noticeable calibration error, whereas finetuning halves the MAE and boosts rank correlation on the instruction axis from 0.36 to 0.82 — outperforming even the larger 2.5-pro model. Notably, the fine-tuned model provides these high-quality scores directly, without a costly chain-of-thought reasoning step, confirming that a specialized assessor is a more efficient paradigm for large-scale automated filtering.

To further validate our assessor’s robustness, we benchmarked it against the publicly available ImgEdit validator [52] on a per-category basis. As shown in Table 2 the Spearman correlation between each assessor’s scores and humanannotated ground-truth ratings across various edit types. Our fine-tuned Gemini validator demonstrates significantly higher correlation across all categories, with particularly large gains in complex operations like Replace $( + 0 . 5 8 \rho )$ , Action $( + 0 . 2 5 \rho )$ , and Compose $( + 0 . 3 6 \rho )$ . This confirms the superior generalization capability of our specialized assessor, highlighting its reliability for a diverse range of editing tasks.

# 3.4. Image Editing Backbone

For our data mining framework it is important to have an instruction guided image-to-image (I2I) model that accepts a source image $I _ { 0 }$ and a natural-language edit prompt $p _ { e }$ , and returns an edited image $\hat { I } _ { e }$ . Throughout the paper, we instantiate this role with an internal diffusion-based editor that we previously trained ourselves. The exact architecture and weights are proprietary, but in terms of the current method, no component of the pipeline depends on the model’s internals. The editing model is invoked as a black-box function, meaning our framework interacts with it solely through its defined inputs and outputs. This design choice ensures that no other component of our pipeline is dependent on the model’s internal workings, allowing for easy swapping with alternative I2I models, whether they are open-source, commercially available, or different in-house solutions, as long as they adhere to the same input/output specifications. All quality control and validation are performed by an external validation stack, as described previously, further reinforcing this modular approach.

# 3.5. Implementation Details

Component specification. Our pipeline is fully modular; each block can be replaced by any compatible alternative. Unless otherwise noted, we use the following defaults:

• Prompt engineer. We query the reasoning-centric OpenAI o3 model [30] with the template in A.1 to jointly emit a text-to-image (T2I) prompt and a set of $k$ logically consistent edit instructions.   
• T2I generator. Source images are synthesised with FLUX.1-schnell [6] at a random resolution (long side $\in$ [860, 2200] px; aspect ratio bounded by $1 { : 6 } \leq \mathrm { A R } \leq 6 { : } 1 )$ . using 4 DDIM steps.   
• Plausibility gate. We retain only sample seeds whose captions pass a plausibility check by Qwen2.5-VL-7B [31] using A.5.   
• Instruction-guided editor. By default we employ our internal I2I DiT model with 18-28 diffusion steps.   
• Soft pre-validation filter. Candidate edits first pass a coarse screen with Qwen2.5-VL-72B [31] using A.2, A.3, A.4.   
• Hard validation filter. The fine-tuned Gemini validator (Sec. 3.2) runs at temperature 0.0 with A.2.

All Qwen-VL calls use the HuggingFace transformers default configuration with temperature 10−6.

Configuration. Optimal counts for the T2I seeds $( N )$ and the edit retries $( M )$ are determined by prompt difficulty, instruction complexity, and the specific models in the loop. A smaller $N$ is preferable because a new prompt injects fresh diversity, whereas additional random seeds for the same prompt yield diminishing returns. Harder samples benefit from a larger $M$ , which trades additional compute for a higher chance of success. Here, $N { = } 1 0$ and $M { = } 5$ are selected as a reasonable compromise to generate the dataset.

Table 2. Per-category Spearman correlation $( \rho )$ comparing our Gemini validator to the ImgEdit assessor [52] against a unified human ground-truth score. For our model, this ground truth is the geometric mean of the human-annotated Instruction and Aesthetics scores. Score aggregation for the ImgEdit-Judge assessor follows the method described in [52].   

<html><body><table><tr><td>Model</td><td>Overall</td><td>Remove</td><td>Replace</td><td>Style</td><td>Adjust</td><td>Background</td><td>Add</td><td>Extract</td><td>Action</td><td>Compose</td></tr><tr><td colspan="9">Gemini-2.0-flash</td><td></td><td></td></tr><tr><td>(finetune)</td><td>0.79</td><td>0.75</td><td>0.89</td><td>0.55</td><td>0.79</td><td>0.70</td><td>0.72</td><td>0.59</td><td>0.83</td><td>0.43</td></tr><tr><td>ImgEdit-Judge</td><td>0.41</td><td>0.46</td><td>0.31</td><td>0.30</td><td>0.39</td><td>0.53</td><td>0.38</td><td>-0.16</td><td>0.58</td><td>0.07</td></tr></table></body></html>

No dedicated ablation is presented, since the expected utility of increasing either parameter is monotonic and saturates quickly, making the choice mainly a budget question. Validation thresholds remain fixed at $T _ { \mathrm { a e s } } = T _ { \mathrm { a d h } } = 4 . 7$ .

Budget-aware random scheduler. Up to $N \times k \times$ $M$ seed–instruction pairs are enumerated, filtered by the Qwen-7B plausibility test, placed in a queue, and drawn uniformly without replacement until the global GPU-hour budget is exhausted. In our case, the queue almost never runs out; realized compute, dataset yield, and quality are dictated by the budget, not by the nominal $( N , M )$ values, and larger values simply leave more jobs unsampled. In future iterations, $M$ may be set per instruction category, or retries may continue until the first pre-filter success, removing $M$ as a fixed hyperparameter. The same queue can support adaptive sampling methods, such as giving higher priority to instruction categories where the model performs worse.

# 3.6. Data Augmentation

The dataset is further refined and expanded through several post-processing and augmentation strategies.

Semantic Inversion. Obviously, any edit operation can be inverted. The only requirement is to rewrite the original edit instruction into its logical inverse. This is done with Gemini 2.5 Flash and Prompt A.6.

Crucially, because the model has access to the original T2I prompt, the process preserves positional and descriptive details to create a high-quality learning signal. For the example in Listing 1, the inverse of the composite deletion instruction is not simply “Add a cactus, a bowl, a remote control, a crocheted blanket, and a dog toy” but rather the fully specified prompt: “Add a small cactus on the windowsill, a half-eaten bowl of cereal on the coffee table, a remote control, a crocheted blanket, and a dog toy on the rug.”

Bootstrap Composition. Since each source image $I _ { 0 }$ can be successfully edited into multiple distinct images ${ \left. { { I _ { e 1 } } } \right. }$ , $I _ { e 2 }$ , etc.), new triplets can be constructed from these existing results. Given two successful edits, a new instruction $p _ { e 2 } ^ { \prime }$ can be formulated to transform $\boldsymbol { I _ { e 1 } }$ into ${ { I } _ { e 2 } }$ , yielding a novel compositional triplet $\langle I _ { e 1 } , p _ { e 2 } ^ { \prime } , I _ { e 2 } \rangle$ (demonstrated in Figure 2).

Backward consistency filter. The semantic inversion process enables an optional backward consistency filter, which is particularly useful for complex T2I prompts where the generation model is prone to error. For instance, the T2I model may occasionally fail to generate requested content from a complex prompt, producing a semantically invalid scene (e.g., the prompt specifies “a cat on the sofa”, but the image contains no cat), and the subsequent T2I validator may also fail to detect this mistake. In given example, a forward edit, such as “remove the cat” could then receive a high score because the hard filter model interprets the instruction as correctly performed and, by design, is not trained to judge the validity of the instruction itself. To filter out such cases, every inverse instruction is re-scored after augmentation. If the backward edit (e.g., “add the cat on the sofa”) receives a low score, both the inversion and its forward counterpart are removed. This filter is not crucial for the framework’s performance. Its necessity depends heavily on the T2I model, the validator, and the complexity of the prompts, and it requires re-running the data through the hard filter. Nevertheless, in scenarios where it is considered necessary, it serves as an additional layer of quality assurance.

![](images/da6174f18156fe5429febb4e0ea25ecd06bca8a57990fb6b915d6dd7d5a8ecea.jpg)  
Figure 2. Solid arrows represent forward instructions, and dashed arrows represent their semantic inversions. Instructions for compositional triplets are aggregated from both forward instructions and inversions.

# 3.7. NoHumansRequired Dataset

The final pipeline yields a dataset of 358 463 high-quality triplets. A detailed breakdown of the data volume changes at each stage is provided in Table 3. The initial generation and editing phases have survival rates of $56 \%$ and $57 \%$ , respectively, with subsequent filtering and quality control steps further refining the set. Augmentation through inversion and compositional generation increases the dataset size by $9 9 . 4 \%$ and $1 7 . 6 7 \%$ , respectively.

sketch (3406) illustration (3603) poster (3301) painting (3700) oillopwai-natnignlge  (29013) widatee-arcnogloer((4236763)) realistic shot (2628) mm (4709) panorama (2303) tempera (5227) futuristic (2250) snapshot (6071) overhead shot (2161) minimalist (6506) photoreal (2124) crochet (6715) anime (2123) macro (7116) portrait (10541) standard (73524) drone (10601) close-up (10748) neon (13869) ink (21998) dslr (38962) realistic (30666) and 65 more (37490) photo (31678)

Additon (125393) Removal (125533) Composite (53823) Miscellaneous (53714) (a) General category group distribution. Change background (9767)   
Change time of day (8923) General color correction (438) Change human emotion (516) Reshape object (978) Add background (1107)   
Change object (8865) Remove background (1107) Change haircut (1134) Restore (1853) Change color (6957) ChDaengrgaedse a(s1o8n5 (42)288) and 13 more (2685) Change material or texture (5242) (b) Miscellaneous operations distribution. Add object & Remove object (43692) Change background (834) Remove background (536) Change object & Remove object (510) Add object & Change object (495) Change time of day & Change time of day (458) Change color & Change object (362) Change object & Change color (358) Change color & Remove object (290) Add object & Change color (277)   
Change material or texture & Change material or texture (256) Change object & Change object (253) Change background & Remove object (236) Add object & Change background (207) and 242 more (5059) 2 510002 5 10k 2 (c) Composite operations distribution, logarithmic scale.

As shown in Figure 3, the primary focus is on object removal instructions (Figure 3a), as these relatively simple operations, upon successful inversion, provide high-quality and challenging object addition examples, which are crucial for improving modern instruction-based image editors. The dataset also includes a variety of other miscellaneous and compositional operations (Figures 3b and 3c), obtained during the stage described in Section 3.6. Furthermore, the data spans a wide range of different styles and perspectives, from standard photographic images to cartoon renderings, ensuring real-world diversity (Figure 4). This diversity extends to image aspect ratios, which are detailed in Table 8 of the Appendix.

The framework enables the scalable generation of highquality triplets. It also supports weakness-targeted mining: if an editor exhibits high performance on one class of simpler edits but performs poorly on another class of more complex edits, the pipeline can focus on simpler operations with subsequent reversals. Therefore, the pipeline can function as a self-correcting loop, continuously targeting and addressing model weaknesses without any human intervention.

# 3.8. Cross-dataset comparison.

To situate the quality of the dataset within the broader landscape, a fine-tuned assessor is leveraged to perform a largescale comparison, scoring 5 000 randomly sampled edits from each public benchmark. Table 4 reports the mean Instruction and Aesthetics scores for each dataset. Following OmniEdit [43], the geometric mean of these two scores is additionally computed for every edit and its average is reported as an aggregate measure of overall quality.

The results presented in Table 4 unequivocally demonstrate the superior quality of the automatically mined dataset. With a geometric mean score of 4.53, NHR-Edit not only establishes a new state-of-the-art for instructionbased editing datasets but also significantly outperforms the next-best-in-class. This result validates that the fully automated methodology can produce a corpus whose quality is not just competitive with, but superior to, existing benchmarks, including those with significant manual curation.

# 4. Experiments

In this section, we conduct experiment to address the following research question: Can NoHumansRequired Dataset improve the performance of an existing edit method?

Table 3. Each stage statistics for 63 292 prompts. Taking $\approx 2 3 1 7 7 6 4$ generation attempts, the survival rate can be estimated as $2 0 . 3 \%$ excluding the squeezing step. All rates depend strongly on instruction complexity.   

<html><body><table><tr><td>Processing Stage</td><td>Method/Model</td><td>Delta (%)</td><td>Remaining Data Volume</td></tr><tr><td>Initial Generation</td><td>FLUX.1-schnell</td><td></td><td>1171773</td></tr><tr><td>Generation Filtering</td><td>Qwen-7B</td><td>-56.00</td><td>515 584</td></tr><tr><td>Editing Generation</td><td>In-house DiT</td><td>+495.90</td><td>3 072385</td></tr><tr><td>Editing Filtering</td><td>Qwen-72B (Pre-Filter)</td><td>-57.00</td><td>1321126</td></tr><tr><td>LowLevel Check</td><td>Connected Component Analysis</td><td>-3.00</td><td>1281492</td></tr><tr><td>Quality Scoring</td><td>Gemini Validator (Hard Filter)</td><td>-63.21</td><td>471523</td></tr><tr><td>Final Selection</td><td>ArgMax Selection</td><td>-64.04</td><td>169 538</td></tr><tr><td>Inversion</td><td>Gemini 2.5 Flash</td><td>+99.40</td><td>338 065</td></tr><tr><td>Composition</td><td>Bootstrap& Concatenation</td><td>+17.67</td><td>397804</td></tr><tr><td>Backward Consistency Filtering</td><td>Gemini Validator (Hard Filter)</td><td>-9.89</td><td>358 463</td></tr></table></body></html>

![](images/41f0b7aed80932d283716276f995d0784d211a93e4a4e1dcc17d8a41e56a4df0.jpg)

(b) Remove the sandwich and the headphones.   
Figure 5. Illustration of poor performance by vanilla MLLMs. Scores for instruction adherence and image aesthetics are provided below: a) gpt-4o-2024-08-06: 5.0, 4.8; Gemini 2.5 Pro: 5.0, 5.0. b) gpt-4o-2024-08-06: 5.0, 4.9; Gemini 2.5 Pro: 5.0, 4.5.

# 4.1. Experimental Setup

BAGEL[15] is a 14B-parameter open-source multimodal foundation model with Mixture-of-Transformer-Experts architecture: a multimodal-understanding expert and an image-generation expert exchange information through shared self-attention in a single transformer stack. In this work, we performed parameter-efficient adaptation only to the generation expert’s attention and feed-forward projection layers using LoRA[23] ${ \mathrm { f r a n k } } = 1 6$ , alpha $= 1 6$ , dropout $= 0 . 0 5$ , bias $\mathbf { \tau } = \mathbf { \tau }$ "none"). We refer to this fine-tuned variant as BAGEL-NHR-EDIT. All other BAGEL components remain frozen to preserve the model’s pretrained multimodal reasoning and VQA capabilities. We choose LoRA because it offers greater training stability than full-parameter updates and substantially lower computational and memory cost.

Table 4. Quality metrics across editing datasets, sorted in ascending order by geometric mean. The ’Type’ column indicates the generation method: A for Automatic and M for Manual. The asterisk $( ^ { * } )$ denotes a highly curated automatic dataset.   

<html><body><table><tr><td>Dataset</td><td>Type</td><td>Instr. 个</td><td>Aesth. 个</td><td>Geom. 个</td></tr><tr><td>UltraEdit[60]</td><td>A</td><td>2.67</td><td>3.30</td><td>2.92</td></tr><tr><td>Seed Part2[17]</td><td>M</td><td>3.20</td><td>3.03</td><td>3.09</td></tr><tr><td>Seed Unsplash[17]</td><td>A</td><td>3.01</td><td>3.84</td><td>3.28</td></tr><tr><td>InstructPix2Pix[9]</td><td>A</td><td>3.17</td><td>3.58</td><td>3.30</td></tr><tr><td>MagicBrush[55]</td><td>A</td><td>3.62</td><td>3.27</td><td>3.38</td></tr><tr><td>AnyEdit[53]</td><td>A</td><td>3.39</td><td>3.64</td><td>3.44</td></tr><tr><td>HQ-Edit[24]</td><td>A</td><td>2.90</td><td>4.21</td><td>3.45</td></tr><tr><td>ImgEdit[52]</td><td>A</td><td>3.26</td><td>3.91</td><td>3.49</td></tr><tr><td>Seed OpenImages[17]</td><td>A</td><td>3.42</td><td>3.86</td><td>3.50</td></tr><tr><td>Seed Part 3[17]</td><td>M</td><td>4.06</td><td>4.37</td><td>4.13</td></tr><tr><td>OmniEdit[43]</td><td>A*</td><td>4.21</td><td>4.35</td><td>4.23</td></tr><tr><td>NHR-Edit</td><td>A</td><td>4.56</td><td>4.52</td><td>4.53</td></tr></table></body></html>

# 4.2. Benchmarks and Evaluation Metrics.

We evaluate BAGEL-NHR-EDIT and the baseline BAGEL model on two public image-editing benchmarks: GEditBench[28], which targets instruction-driven guided edits across varied edit types, and ImgEdit-Bench[52], which spans single- and multi-turn region-targeted edits with diverse difficulty and preservation requirements.

![](images/725300a08002b289a6ddca1fba682f06734277ddb9fb892d21d945f6132160a4.jpg)  
Figure 6. Proposed NoHumansRequired framework scheme.

GEdit-Bench metrics. We follow the authors’ VIEScore[45] automated protocol: Semantic Consistency $( S Q , \ 0 – 1 0 )$ for instruction adherence; Perceptual Quality $( P Q , 0 – 1 0 )$ for realism and artifact absence; and an Overall $( O )$ composite derived from SQ and PQ, with scoring produced by GPT-4o[1].

ImgEdit-Bench metrics. We follow the ImgEdit-Bench authors’ evaluation: GPT-4o[1] scores each edited result on three 1–5 dimensions — Instruction Adherence (prompt comprehension and conceptual alignment), Edit Quality (precision of changes in the target region), and Detail Preservation (fidelity of regions meant to remain unchanged).

# 4.3. Results

Table 5 reports performance on ImgEdit-Bench and Table 6 reports performance on GEdit-Bench. Across both, BAGEL-NHR-EDIT model fine-tuned on NoHumansRequired Dataset consistently outperforms the baseline, indicating that additional training on our dataset yields measurable improvements in instruction adherence, edit fidelity, and perceptual quality for image editing.

# 5. Conclusion and Discussion

We introduced an end-to-end, fully automated, iterative pipeline for mining high-quality triplets for instructionguided image editing. Unlike traditional dataset construction methods tied to narrow domains, our approach uses a pretrained editor to generate new training data. By sampling multiple edit attempts and filtering only successful ones, we capture both simple and hard cases where the model succeeds stochastically.

Table 5. Comparison results on ImgEdit-Bench [52]. “Overall” is calculated by averaging all scores across tasks.   

<html><body><table><tr><td>Model</td><td>Add</td><td>Adjust</td><td>Extract</td><td>Replace</td><td>Remove</td><td>Background</td><td>Style</td><td>Compose</td><td>Action</td><td>Overall 个</td></tr><tr><td>BAGEL [15]</td><td>3.98</td><td>3.55</td><td>1.53</td><td>3.5</td><td>3.04</td><td>3.3</td><td>4.22</td><td>3.0</td><td>4.07</td><td>3.3</td></tr><tr><td>BAGEL-NHR-EDIT</td><td> 4.19</td><td>3.55</td><td>1.62</td><td>3.77</td><td>3.18</td><td>3.42</td><td>4.3</td><td>2.94</td><td>3.95</td><td>3.39</td></tr></table></body></html>

Table 6. Quantitative comparison on GEdit-Bench-EN [28] with categories. SC (Semantic Consistency) evaluates instruction following, and PQ (Perceptual Quality) assesses image naturalness and artifacts. Higher scores are better for all metrics.   

<html><body><table><tr><td rowspan="2">Alteration Category</td><td colspan="3">Bagel-7B-MoT[15]</td><td colspan="3">BAGEL-NHR-EDIT</td></tr><tr><td>SC</td><td>PQ</td><td>0</td><td>SC</td><td>PQ</td><td>0</td></tr><tr><td>background_change</td><td>8.625</td><td>6.750</td><td>7.356</td><td>8.825</td><td>6.825</td><td>7.511</td></tr><tr><td>color_alter</td><td>8.525</td><td>6.350</td><td>6.985</td><td>9.000</td><td>6.800</td><td>7.480</td></tr><tr><td>material_alter</td><td>7.625</td><td>5.975</td><td>6.202</td><td>7.800</td><td>6.125</td><td>6.441</td></tr><tr><td>motion_change</td><td>8.175</td><td>7.000</td><td>7.298</td><td>7.175</td><td>7.800</td><td>6.835</td></tr><tr><td>ps_human</td><td>6.171</td><td>6.457</td><td>5.885</td><td>6.614</td><td>6.714</td><td>6.253</td></tr><tr><td>style_change</td><td>8.133</td><td>4.817</td><td>6.020</td><td>7.933</td><td>4.667</td><td>5.847</td></tr><tr><td>subject-add</td><td>9.183</td><td>7.633</td><td>8.178</td><td>9.333</td><td>7.750</td><td>8.354</td></tr><tr><td>subject-remove</td><td>8.561</td><td>6.912</td><td>7.513</td><td>8.649</td><td>7.211</td><td>7.622</td></tr><tr><td>subject-replace</td><td>8.683</td><td>6.400</td><td>7.252</td><td>8.650</td><td>7.017</td><td>7.631</td></tr><tr><td>text_change</td><td>7.253</td><td>8.020</td><td>7.218</td><td>7.354</td><td>8.333</td><td>7.503</td></tr><tr><td>tone_transfer Average</td><td>6.875 7.983</td><td>5.950 6.570</td><td>6.224 6.921</td><td>7.400 8.067</td><td>6.450</td><td>6.785 7.115</td></tr></table></body></html>

Strong quality requirements enforced during filtering, along with instruction inversion and compositional editing, allow us to build semantically rich, diverse triplets suitable for generative models tuning. The use of a T2I model further enables domain and stylistic diversity beyond real-world data, helping mitigate the overfitting risks associated with narrow training domain. Moreover, the ability to jointly use both real and synthetic domains provides fine-grained control over diversity, quality, and bias in the dataset, ensuring robust generalization without degradation on real-world data.

Crucially, the pipeline may be cyclic and self-improving: as the editor improves, it generates higher-quality triplets for more edit types, creating a feedback loop where the model not only learns from data but also curates it. We release BAGEL-NHR-EDIT, a LoRA-tuned BAGEL variant that achieves higher scores than the base model on ImgEditBench and GEdit-Bench, demonstrating that fine-tuning on NHR-Edit improves the underlying editing model.

# 5.1. Limitations

The framework is bounded by the base editor: operations it cannot perform yield no valid triplets, and inversion/composition help only where edits are logically reversible. Multiseed sampling mitigates but does not remove this ceiling. Additionally, the capabilities and limitations of the text-toimage generator and the LLM used for producing generation prompts and editing instructions directly affect the resulting dataset. Certain visual concepts may lie beyond the generative capacity of the T2I model, while others may not be expressible or interpretable by the instruction-generating LLM.

Generating large candidate pools and running multistage vision–language validation is compute- and costintensive, particularly when proprietary APIs are involved.

Prompt specification can introduce bias from both human templates and model priors. Without careful design, reusing narrow templates limits variation, and the data may repeat similar concepts. T2I models can further skew the distribution by generating some styles or subject types more frequently than others.

Finally, LLM-generated edit prompts may be differ from real user queries. We mitigate this with paraphrasing and diverse prompting strategies.

# 5.2. Future Work

Future directions include extending our fully synthetic pipeline, which currently contains no real user images or instructions, to settings that incorporate real data. In such deployments, user-provided images and edit requests substitute for the synthetic T2I stage, simplifying the pipeline while allowing the existing editing, validation, and mining loop to apply with appropriate consent and privacy safeguards.

The same architecture could support agentic selfsupervised learning by iteratively retraining on mined hard cases.

# 5.3. Ethical and Legal Considerations

The released NHR-Edit dataset is produced with FLUX.1- schnell(text-to-image generation) and ChatGPT o3 (prompt generation). Despite automated moderation, residual adult or otherwise objectionable content may persist. We disclaim liability for any such material.

Furthermore, we disclaim responsibility for models trained on our data that may be used for misinformation, identity spoofing, or content manipulation. Responsible use and transparency remain with the community.

# 5.4. Broader Impact

By releasing NHR-Edit, we aim to support research on textbased editing and provide a strong benchmark for future training and evaluation pipelines.