# Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog

Lautaro Estienne1,2, 4, 6 Gabriel Ben Zenou1,4 Nona Naderi2, 4, 6 Jackie Cheung3, 7 Pablo Piantanida1,3, 4, 5, 6

1International Laboratory on Learning Systems (ILLS) 2Laboratoire Interdisciplinaire des Sciences du Numérique (LISN) 3Mila - Quebec AI Institute 4CNRS 5CentraleSupélec 6Université Paris-Saclay 7McGill University, Canada CIFAR AI Chair lautaro.estienne@universite-paris-saclay.fr

# Abstract

As AI systems take on collaborative roles, they must reason about shared goals and beliefs—not just generate fluent language. The Rational Speech Act (RSA) framework offers a principled approach to pragmatic reasoning, but existing extensions face challenges in scaling to multi-turn, collaborative scenarios. In this paper, we introduce Collaborative Rational Speech Act (CRSA), an information-theoretic (IT) extension of RSA that models multi-turn dialog by optimizing a gain function adapted from rate-distortion theory. This gain is an extension of the gain model that is maximized in the original RSA model but takes into account the scenario in which both agents in a conversation have private information and produce utterances conditioned on the dialog. We demonstrate the effectiveness of CRSA on referential games and template-based doctor–patient dialogs in the medical domain. Empirical results show that CRSA yields more consistent, interpretable, and collaborative behavior than existing baselines—paving the way for more pragmatic and socially aware language agents.

# 1 Introduction

Modeling conversations is central to the development of grounded and useful agentic AI systems, which are increasingly characterized by collaborative interactions between humans and machines. Several applications benefit from dialog systems capable of natural and pragmatic interactions with users. For instance, in the medical domain, conversational agents could support diagnostic interviews (Tu et al., 2025) or serve as tools for physician training in controlled environments (Karunanayake, 2025). In enterprise settings, dialog agents could autonomously handle routine tasks—such as scheduling, data entry, or report generation—freeing human effort for higher-level decision-making (Tupe and Thube, 2025; Satav, 2025). In education, they offer the potential to personalize content delivery, adapting to learners’ styles and paces (Nabhani et al., 2025; Vorobyeva et al., 2025). While such applications are still emerging, a key enabler is the development of models that can manage collaborative, goal-oriented interactions in a robust and interpretable manner.

To succeed in real-world settings, dialog generative-based models must do more than generate fluent language—they must track shared tasks to communicate meaningfully and contextually (Lin et al., 2024). For example, a physician in a diagnostic exchange refines hypotheses as the conversation evolves, requiring interpretable and scalable frameworks for reliable interaction.

Yet, many existing models prioritize taskspecific response generation (He et al., 2017; Jiang et al., 2019; Meta Fundamental AI Research Diplomacy Team (FAIR) et al., 2022), or optimize for superficial conversation properties using narrowly defined objectives (Khani et al., 2018; Dafoe et al., 2020; Lin et al., 2024; Jeon et al., 2020). While these methods often yield strong performance, they typically lack principled foundations and depend on ad hoc design choices.

The Rational Speech Act (RSA) framework (Frank and Goodman, 2012) offers a principled foundation for modeling pragmatic reasoning as recursive social inference between speakers and listeners. Viewed through an information-theoretic (IT) lens, RSA approximates a Rate-Distortion solution (Cover and Thomas, 2001), where the listener reconstructs intended meaning from observed utterances (Zaslavsky et al., 2021). RSA has successfully captured phenomena such as reference (Degen et al., 2020), implicature (Bergen et al., 2016), and vagueness (Herbstritt and Franke, 2019), and powered applications from grounded captioning (Cohn-Gordon et al., 2018) to controlled generation (Wang and Demberg, 2024). Yet, despite this promise, existing RSA extensions remain limited in multi-turn, task-oriented dialog: they struggle to model evolving beliefs or integrate dialog history (Carenini et al., 2024; Degen, 2023). We argue this shortfall stems from the absence of a unified, theoretically grounded mechanism for belief and task tracking in collaborative interaction.

# Contributions

Our main contributions are as follows:

• We introduce Collaborative RSA (CRSA), a novel, information-theoretically grounded extension of the RSA framework tailored for multiturn, goal-driven dialog. • A generalized multi-turn gain function: We extend the rate-distortion to model multi-turn collaborative settings of RSA, capturing both task progression and evolving partner beliefs. CRSA jointly models the agent’s belief about (i) the shared task target and (ii) the interlocutor’s private knowledge—enabling socially aware and context-sensitive communication. • Empirical validation: We evaluate CRSA on referential games and semi-automatically generated doctor-patient dialogs, showing that it improves consistency, interpretability, and collaborative alignment compared to existing baselines.

# 2 Related work

RSA model and pragmatics. The Rational Speech Act (RSA) framework (Frank and Goodman, 2012) serves as a model for pragmatic communication designed to emulate human behavior in linguistic tasks (Degen et al., 2020; Bergen et al., 2016; Herbstritt and Franke, 2019). This framework is both conceptually intuitive and computationally versatile, making it readily adaptable for integration with neural language models to tackle more intricate challenges, including machine translation (Cohn-Gordon and Goodman, 2019), image captioning (Cohn-Gordon et al., 2018), controllable text generation (Shen et al., 2019; Wang and Demberg, 2024). Extensions to the original RSA framework have been proposed to accommodate more complex scenarios. For instance, adaptations have addressed cases where agents lack shared vocabularies (Bergen et al., 2016) or where common ground evolves dynamically during interaction (Degen et al., 2015). A comprehensive overview of RSA’s development and its numerous variants is provided by Degen (2023).

Information-theoretic results for interactive rate-distortion. Information theory offers a robust framework for analyzing communication as the exchange of information between agents. Within this domain, the rate-distortion problem (Shannon, 1993) offers a principled way to balance compression efficiency with the fidelity of reconstruction. This problem has been pivotal in exploring the trade-offs between fidelity and compression in message transmission. Kaspi (1985) investigated scenarios involving two agents engaging in iterative interactions to collaboratively infer each other’s observations. Building on this foundation, Rey Vega et al. (2017) extended the analysis to multi-agent contexts, accommodating communication frameworks with three or more participants and significantly advancing the understanding of collective information exchange. Focusing on two-agent systems, Vera et al. (2019) explored a variation wherein each agent is tasked not merely with understanding one another but with predicting a target random variable representing a (possible stochastic) function of each other’s observations. This approach highlights the promise of IT methods in supporting more efficient and collaborative communication among agents in complex environments, as shown by Zaslavsky et al. (2021), who reformulate the standard RSA framework as a ratedistortion optimization problem.

Collaborative dialog modeling. Multiple works frame a collaborative or task-oriented dialog as a Partially Observable Markov Decision Process (POMDP) (Williams and Young, 2007), which provide a suitable framework to model end-to-end networks on specific tasks (Wen et al., 2017; Jiang et al., 2019). Reinforcement Learning has been widely used in this context, in order to provide interpretable and trackable training procedures that incorporate the structure of the dialog in their policy training or decoding strategy (Lin et al., 2024; Li et al., 2016; Xu et al., 2025). Related to this, game-theoretic perspective has also been used in dialog modeling (Jeon et al., 2020; Lin et al., 2022). In this context, multiple tasks and datasets have been developed to evaluate dialog modeling (He et al., 2017; Khani et al., 2018; Macherla et al., 2023), usually by assessing the task performance and the similarity with human conversations. The RSA model has also found applications in dialog systems, often complementing neural models to enhance agent self-awareness (Kim et al., 2020) or to improve the interpretation of emotional subtext (Kim et al., 2021).

![](images/d86ed0b085e8ab78e2c1ed8fbd4c3b0e288f32eb0d327cef938c3c5ca1474649.jpg)  
Figure 1: RSA variants proposed in this work(1b, 1c) compared to the original one (1a).

# 3 Review of the RSA Model from the Lens of Information Theory

Fig. 1a presents a schematic view of the classic RSA model from an information-theoretic perspective. Here, a meaning $m \in { \mathcal { M } }$ is received by the speaker $S : \mathcal { M } \times \mathcal { U }  [ 0 , 1 ]$ who uses it to produce a posterior probability $S ( u | m )$ for all possible utterances $u \in \mathcal { U }$ . The utterance $u$ is then transmitted to the listener $L : \mathcal { U } \times \mathcal { M }  [ 0 , 1 ]$ who produces a posterior $L ( m | u )$ for all possible reconstructions $m \in { \mathcal { M } }$ of the meaning $m$ that the speaker is trying to convey. Additionally, there is a distribution $P : \mathcal { M }  [ 0 , 1 ]$ that is known by the two agents and represents the prior of the meanings. Finally, the function $C : \mathcal { U }  \mathbb { R }$ assigns a prior cost value to each utterance produced by the speaker.

In the classic RSA model, agents update their values based on the other’s perspective. For simplicity, and without loss of generality, we adopt the listener’s viewpoint—assuming the speaker updates first1:

$$
\begin{array} { l } { { S ^ { k + 1 } ( u | m ) \propto \exp \big [ \alpha \big ( \log L ^ { k } ( m | u ) - C ( u ) \big ] , } } \\ { { L ^ { k + 1 } ( m | u ) \propto S ^ { k + 1 } ( u | m ) P ( m ) . } } \end{array}
$$

In this case, the listener is initialized with a predefined lexicon function $\mathcal { L } : \mathcal { U } \times \mathcal { M } \to \{ 0 , 1 \}$ , which specifies the possible meanings associated with each utterance:

$$
L ^ { 0 } ( m | u ) \propto P ( m ) \mathcal { L } ( u , m ) .
$$

Zaslavsky et al. (2021) show that this iteration process is equivalent to maximize the next objective:

$$
\mathcal { G } _ { R S A } ^ { \alpha } ( L , S ) = H _ { S } ( U | M ) + \alpha \mathbb { E } _ { S } [ V _ { L } ( U , M ) ] ,
$$

where $H _ { S } ( U | M )$ is the conditional entropy between the estimated meanings and the utterances, $V _ { L } ( u , m ) \triangleq \log L ( m | u ) - C ( u )$ is called the “listener value”, and $\mathbb { E } _ { S } [ V _ { L } ]$ is computed with respect to the distribution of the speaker. That is,

$$
\begin{array} { c } { { \displaystyle H _ { S } ( U | M ) = - \sum _ { \forall ( u , m ) } P _ { S } ( u , m ) \mathrm { l o g } S ( u | m ) , } } \\ { { \displaystyle \mathbb { E } _ { S } [ V _ { L } ] = \sum _ { \forall ( u , m ) } P _ { S } ( u , m ) V _ { L } ( u , m ) , } } \end{array}
$$

where $P _ { S } ( u , m ) \triangleq S ( u | m ) P ( m )$ represents the joint probability of the speaker.

# 4 Main Theoretical Results

# 4.1 Modeling private meanings (YRSA)

To extend the RSA model to bidirectional dialogue with explicit task modeling, we first distinguish between private meanings and shared task outcomes. In real conversations, each participant holds their own prior knowledge and worldview, which may differ from that of their interlocutor. In our example of a dialogue between a patient and a physician: the patient must describe their symptoms, which are not directly observable by the physician, while the physician brings medical expertise the patient lacks. Both types of knowledge are essential to determine the appropriate diagnosis or treatment plan. Notably, neither the patient’s symptoms nor the physician’s prior knowledge fully align with the shared goal of the conversation, i.e. the identification of a suitable medical outcome.

In this context, we identify the need of representing a private set of meanings $\mathcal { M } _ { A }$ and $\mathcal { M } _ { B }$ for each agent, which may or may not match. In addition, the result $y$ of the shared task is going to be represented with a separate space $y$ that contains all the possible outcomes of it. For simplicity, we will assume that all these are discrete spaces. Fig. 1b represents a schematic of this model. We will refer to this extension as the YRSA model.

The YRSA model redefines the notion of prior from the classic RSA framework by conditioning the dialogue on the joint realization of the agents’ private meanings $( m _ { A } , m _ { B } )$ and the shared task target $y$ , which together define the context in which the interaction unfolds. Importantly, we assume for the development of our model that both the realizations and the joint distribution of these three variables do not change over time during the conversation. This implies that the prior is completely defined by the joint distribution $P : \mathcal { M } _ { A } \times \mathcal { M } _ { B } \times \mathcal { Y }  [ 0 , 1 ]$ given to both agents.

We now turn to defining the updated agent posteriors. The new speaker $S : \mathcal { M } _ { A } \times \mathcal { U }  [ 0 , 1 ]$ produces a posterior $S ( u | m _ { A } )$ that only depends on its the private meaning $m _ { A }$ . Similarly, the listener $L : \mathcal { M } _ { B } \times \mathcal { U } \times \mathcal { Y }  [ 0 , 1 ]$ is represented by the posterior $L ( y | m _ { B } , u )$ , which is conditional independent of the private meanings $m _ { A }$ . In this formulation, the representation of task performance is delegated to the listener, who updates their belief upon receiving the utterance.

We can now propose the corresponding gain function to be maximized by this model:

and Demberg, 2024; Kim et al., 2020; Lin et al., 2022). In many cases, this lexicon is given by the output of a neural language model and can be very robust to the evolving dialog. However, that variant of the RSA does not correspond to maximizing the gain of Eq. (1), but a modified version of it in which $U _ { t }$ is replaced by $( U _ { t } , W _ { t } )$ :

$$
H _ { S } ( U _ { t } , W _ { t } | M ) + \alpha \mathbb { E } _ { S } [ V _ { L } ( U _ { t } , W _ { t } , M , Y ) ] .
$$

This is equivalent to applying an RSA model at each turn by initializing it with a lexicon $\mathcal { L } ( u _ { t } , m , w _ { t } )$ depending on $w _ { t }$ , the past utterances.

$$
\begin{array} { r l } {  { \mathcal { G } _ { Y R S A } ^ { \alpha } ( L , S ) = H _ { S } ( U | M _ { A } ) } } \\ & { + \alpha \mathbb { E } _ { S } [ V _ { L } ( U , M _ { B } , Y ) ] } \end{array}
$$

with $V _ { L } ( u , m _ { B } , y ) = \log L ( y | u , m _ { B } ) - C ( u )$ and $H _ { S } ( U | M _ { A } )$ defined as in the classic RSA. A detailed derivation of the equations used to maximize this function is provided in Appendix A.

The issue with Eq. (3) is that the speaker’s utterance $U _ { t }$ at turn $t$ is modeled jointly with the dialogue history $W _ { t }$ , rather than being explicitly conditioned on it. To express the gain in terms of the conditional entropy of the current utterance alone, we condition it on both the dialogue history $W _ { t }$ and the speaker’s intended meaning $M$ , rather than on $M$ alone. In Section 4.2.2, we formally introduce the corresponding expressions of the CRSA model, which incorporates this notion of multi-turn conditioned to the past utterances, as well as private meanings and target task.

Effective collaboration requires not only modeling agents’ private meanings and the shared task, but also supporting multi-turn dialogue. In a medical consultation, for instance, the patient shares symptoms and background, while the physician asks questions, proposes diagnoses, and recommends treatments. To capture such interactions, we denote the speaker’s utterance at turn $t$ as $U _ { t }$ , and the dialogue history up to that point as $W _ { t } = ( U _ { 1 } , \dots , U _ { t - 1 } )$ , representing the sequence of prior exchanges.

# 4.2.1 Modeling multi-turn dialog with explicit history (baseline)

# 4.2 The CRSA Model

The attempt of previous approaches to incorporate the history of the conversation to the RSA model relies on defining the lexicon (or directly the literal listener/speaker) as a function of each turn (Wang

# 4.2.2 Equations of the CRSA model

Figure 1c illustrates our extension of the YRSA model to the collaborative setting. As in the original setup, agents alternate roles—one acting as the speaker, the other as the listener—to achieve a shared task. Each agent has access to a private meaning space, $\mathcal { M } _ { A }$ or $\mathcal { M } _ { B }$ , which remains hidden from their counterpart. Then, at turn $t$ , the private meanings of the speaker will correspond to the meanings of the agent playing the role of the speaker and vice-versa. We refer as $\mathcal { M } _ { S _ { t } }$ and $\mathcal { M } _ { L _ { t } }$ to the private meanings of the speaker and the listener at turn $t$ , respectively. Both agents also have access to the conversation history, denoted as $w _ { t } = ( u _ { 1 } , \ldots , u _ { t - 1 } ) \in \mathcal { W } _ { t } \triangleq \mathcal { U } _ { 1 } \times \cdot \cdot \cdot \times \mathcal { U } _ { t - 1 }$ , where each $\mathcal { U } _ { i }$ represents the space of possible utterances at turn $i$ . The shared objective is to jointly predict a target class $y$ from a finite discrete set $y$ .

As discussed earlier, the joint distribution $P ( m _ { A } , m _ { B } , y )$ serves as a fixed prior throughout the conversation. To maintain consistency as agents alternate roles, we define the prior at turn $t$ over the active speaker and listener meanings, i.e., $P ( m _ { S _ { t } } , m _ { L _ { t } } , y )$ , as follows:

$$
P _ { t } ( m _ { S _ { t } } , m _ { L _ { t } } , y ) = { \left\{ \begin{array} { l l } { P ( m _ { S _ { t } } , m _ { L _ { t } } , y ) } & { { \mathrm { i f } } \ S _ { t } = A } \\ { P ^ { \top } ( m _ { L _ { t } } , m _ { S _ { t } } , y ) } & { { \mathrm { i f } } \ S _ { t } = B } \end{array} \right. }
$$

where $P ^ { \top } : { \mathcal { M } } _ { B } \times { \mathcal { M } } _ { A } \times { \mathcal { Y } }  [ 0 , 1 ]$ is such as $P ^ { \top } ( b , a , y ) = P ( a , b , y )$ . This definition simply represents swapping the arguments corresponding to agent A and $\mathbf { B }$ to reflect the role change.

Formally, we define the distribution of each agent at turn $t$ . The speaker $S _ { t } : \mathcal { M } _ { S _ { t } } \times \mathcal { U } _ { t } \times \mathcal { W } _ { t } \to$ $[ 0 , 1 ]$ produces a posterior $S _ { t } ( u _ { t } | m _ { S _ { t } } , w _ { t } )$ that depends on its private meaning $m _ { S _ { t } }$ and the past utterances $w _ { t }$ . On the other hand, the listener $L _ { t } : \mathcal { M } _ { L _ { t } } \times \mathcal { U } \times \mathcal { W } _ { t } \times \mathcal { Y }  [ 0 , 1 ]$ is represented by the posterior $L _ { t } ( y | m _ { L _ { t } } , u _ { t } , w _ { t } )$ which is independent of the private meanings of the speaker.

Building on the gain function in Eq. (1), we extend the joint speaker distribution and listener utility to incorporate private meanings and multiturn dialogue:

$$
\begin{array} { r l } & { P _ { S } ( u _ { t } , w _ { t } , m _ { S _ { t } } , m _ { L _ { t } } , y ) \triangleq S _ { t } ( u _ { t } | m _ { S _ { t } } , w _ { t } ) \times } \\ & { P _ { S } ( w _ { t } | m _ { S _ { t } } , m _ { L _ { t } } ) P _ { t } ( m _ { S _ { t } } , m _ { L _ { t } } , y ) , } \\ & { V _ { L } ( u _ { t } , w _ { t } , m _ { L _ { t } } , y ) \triangleq \log L _ { t } ( y | u _ { t } , m _ { L _ { t } } , w _ { t } ) - C ( u _ { t } ) . } \end{array}
$$

Then, we define one gain function at each turn to be maximized:

$$
\begin{array} { r l } & { \mathcal { G } _ { C R S A } ^ { \alpha } ( L _ { t } , S _ { t } ) = H _ { S _ { t } } ( U _ { t } | M _ { S _ { t } } , W _ { t } ) } \\ & { ~ + \alpha \mathbb { E } _ { S _ { t } } [ V _ { L } ( U _ { t } , W _ { t } , M _ { S _ { t } } , M _ { L _ { t } } , Y ) ] , } \end{array}
$$

where the expectation of both terms is over $P _ { S }$ . In all cases, we will model $P _ { S } ( w _ { t } | m _ { S _ { t } } , m _ { L _ { t } } )$ with the past speakers’ utterances:

$$
\underbrace { \prod _ { i < t \atop S _ { t } = S _ { t } } S _ { i } ( w _ { i } \vert m _ { S _ { t } } , m _ { L _ { t } } ) } _ { B _ { L , t } ( m _ { S _ { t } } ) } \underbrace { \prod _ { i < t } S _ { i } ( u _ { i } \vert w _ { i } , m _ { L _ { t } } ) } _ { S _ { S , t } ( m _ { L _ { t } } ) } .
$$

This formulation naturally leads to interpreting $B _ { L , t } ( m _ { S _ { t } } )$ and $B _ { S , t } ( m _ { L _ { t } } )$ as each agent’s belief about their interlocutor’s private meaning. In Section 5, we illustrate why this interpretation is reasonable with a concrete example.

Once modeled the gain, the equations that correspond to its maximization are the following:

$$
\begin{array} { l } { S _ { t } ^ { k + 1 } ( u _ { t } | w _ { t } , m _ { S _ { t } } ) \propto } \\ { \displaystyle \exp \big [ \alpha \sum _ { k \atop \forall ( m _ { L _ { t } } , y ) } B _ { t } ^ { \prime } ( m _ { S _ { t } } , m _ { L _ { t } } , y ) V _ { L } ( u _ { t } , w _ { t } , m _ { L _ { t } } , y ) \big ] , } \\ { \displaystyle } \\ { L _ { t } ^ { k + 1 } ( y | u _ { t } , w _ { t } , m _ { L _ { t } } ) \propto } \\ { \displaystyle \sum _ { \forall m _ { S _ { t } } } B _ { L , t } ( m _ { S _ { t } } ) P _ { t } ( m _ { S _ { t } } , m _ { L _ { t } } , y ) S _ { t } ^ { k + 1 } ( u _ { t } | w _ { t } , m _ { S _ { t } } ) , } \end{array}
$$

where we replace $B _ { t } ^ { \prime } ( m _ { S } , m _ { L } , y ) =$

$$
{ \frac { B _ { S , t } ( m _ { L } ) P ( m _ { L } | m _ { S } ) } { \sum _ { \forall m _ { L } ^ { \prime } } B _ { S , t } ( m _ { L } ^ { \prime } ) P ( m _ { L } ^ { \prime } | m _ { S } ) } } P ( y | m _ { L } , m _ { S } ) .
$$

A complete derivation of these equations is provided in Appendix B. Finally, there is no single prescribed method for initializing the iteration at each turn. In Section 5, we adopt the listener’s perspective and explore two variants of the initial lexicon $\mathcal { L }$ , initializing the literal listener as:

$$
\begin{array} { l } { { { \cal L } ^ { 0 } ( y | u _ { t } , w _ { t } , m _ { L _ { t } } ) \propto } } \\ { { \displaystyle \sum _ { \forall m _ { S } } P ( m _ { S } , m _ { L _ { t } } , y ) { \cal L } _ { u _ { t } , w _ { t } } ( m _ { S _ { t } } ) } } \end{array}
$$

with $\mathcal { L } _ { u _ { t } , w _ { t } } ( m _ { S _ { t } } )$ depending on the variant of the RSA. In contrast, in Section 6 we initialize the literal speaker directly with a LLM:

$$
S _ { t } ^ { 0 } ( u _ { t } | m _ { S _ { t } } , w _ { t } ) \propto P _ { L M } ( u _ { t } | w _ { t } , \mathrm { p r o m p t } ( m _ { S _ { t } } ) ) ,
$$

where $\mathrm { p r o m p t } ( m _ { S _ { t } } )$ is the text used to prompt the speaker at that turn. As shown, CRSA retains the flexibility of the original RSA framework in modeling both the listener’s and speaker’s perspectives.

Regarding algorithmic complexity, we find that at turn $t$ , these new set of equations scale as $\mathcal { O } \left( K \cdot \vert \mathcal { M } _ { A } \vert \cdot \vert \mathcal { M } _ { B } \vert \cdot \vert \mathcal { V } \vert \cdot \vert \mathcal { U } _ { t } \vert \right)$ where $K$ is the number of iterations to produce the pragmatic agents. In contrast, the classic RSA equations scale as $\mathcal { O } \left( K \cdot \left| \mathcal { M } \right| \cdot \left| \mathcal { U } \right| \right)$ .

# 5 CRSA for Reference Games

To evaluate CRSA, we adapt the reference game of Khani et al. (2018). In this setting, two agents are shown the same sequence of $N$ cards, each labeled with one letter (A or B) and one number (1 or 2). Agent A sees only the letter on each card, while Agent B sees only the number. Their goal is to collaboratively identify the position of the card labeled A1. At each turn, an agent may utter a number from 1 to $N$ , indicating a card position. For simplicity, we assume that each round contains at most one A1 card and that Agent A always initiates the exchange.

# 5.1 Experimental set-up

For this simulation we consider that the set ${ { \mathcal { U } } _ { t } }$ of possible utterances at turn $t$ is always $( \forall t )$ the set ${ { \mathcal U } _ { t } } \ = \ \{ 1 , \dots , N \}$ representing the messages of the form $^ { * } A I$ card may be at position $n ^ { \prime \prime }$ with $n \in \mathcal { U } _ { t }$ . For the set $y$ of possible classes, the results can be as well $^ { * } A I$ card may be at position $n ^ { \prime \prime }$ , with the addition that there is also the possibility of “There is no A1 card”. That is, $\mathcal { Y } = \{ 0 , 1 , \ldots , N \}$ with 0 representing the mentioned possibility. Regarding the meaning spaces, they correspond to the possible sequences of length $N$ that can be obtained combining without replacement the letters $\mathbf { A }$ and B (for agent A) and the numbers 1 and 2 (for agent B). That is, for instance if $N = 3$ , $\mathcal { M } _ { A } = \{ \mathrm { A A A } , \mathrm { A A B } , \dotsc , \mathrm { B B B } \}$ and $\mathcal { M } _ { B } = \{ 1 1 1 , 1 1 2 , . . . , 2 2 2 \}$ . Finally, the prior distribution $P ( m _ { A } , m _ { B } , y )$ can be defined as follows:

$$
P ( m _ { A } , m _ { B } , y ) \propto \left\{ \begin{array} { l l } { { 1 } } & { { \mathrm { i f } m _ { A } \mathrm { a n d } m _ { B } \mathrm { f o r m } } } \\ { { } } & { { \mathrm { A l } \mathrm { a t } \mathrm { p o s i t i o n } y } } \\ { { 0 } } & { { \mathrm { o t h e r w i s e } } } \end{array} \right.
$$

Since this is a reference game, we adopt the listener’s perspective. In all cases, the literal listener is initialized using Eq. (7), and different model variants are defined based on the update equations and the specification of the lexicon $\mathcal { L } _ { u _ { t } , w _ { t } } ( m _ { S _ { t } } )$ .

• CRSA: We apply the CRSA update equations and define a lexicon $\mathcal { L } _ { u _ { t } , w _ { t } } ( m _ { S _ { t } } ) = \mathcal { L } ( u _ { t } , m _ { S _ { t } } )$ that do not depend on $w _ { t }$ :

$$
\mathcal { L } ( u _ { t } , m _ { S _ { t } } ) = \left\{ \begin{array} { l l } { 1 } & { \mathrm { i f } m _ { S _ { t } } \mathrm { c o n t a i n s ~ A / 1 ~ a t } } \\ & { \mathrm { p o s i t i o n } n \mathrm { a n d } u _ { t } = n } \\ { 1 } & { \mathrm { i f ~ t h e r e ~ i s ~ n o ~ A / 1 ~ i n } m _ { S _ { t } } } \\ { 0 } & { \mathrm { o t h e r w i s e } } \end{array} \right.
$$

• CRSA- $\mathbf { \nabla } _ { \cdot } W _ { t }$ : We apply the CRSA update equations, but with the lexicon $\begin{array} { r l } { \mathcal { L } _ { u _ { t } , w _ { t } } ( m _ { S _ { t } } ) } & { { } = } \end{array}$ $\mathcal { L } ( u _ { t } , m _ { S _ { t } } , w _ { t } )$ depending on the the past $w _ { t }$ . To define $\mathcal { L } ( u _ { t } , m _ { S _ { t } } , w _ { t } )$ , we follow the simple rule:

$$
\mathcal { L } ( u _ { t } , m _ { S _ { t } } , w _ { t } ) = \left\{ \begin{array} { l l } { 0 } & { \mathrm { i f ~ } u _ { t } \in w _ { t - 1 } } \\ & { \land u _ { t } \neq u _ { t - 1 } } \\ { \mathcal { L } ( u _ { t } , m _ { S _ { t } } ) } & { \mathrm { o t h e r w i s e } } \end{array} \right.
$$

![](images/f934e6a59022cefe97566ed0770b8cf3b7eb7057609864666642bb9d3dbe0685.jpg)  
Figure 2: Average of correct predictions with the listener value (top) and information gain (bottom) for 500 rounds of the reference game.

We expect efficient conversational behavior in this game to involve repeating an utterance only to confirm the correct A1 card position. If the correct position is identified, agents should repeat the utterance until the round ends; otherwise, repeating it would be inefficient. The rule in Eq. (10) explicitly encodes this behavior.

• YRSA: We initialize the listener using the YRSA iterative equations and the lexicon from Eq. (9), effectively applying the RSA iteration in the setting where each agent holds a private meaning—that is, the standard YRSA setup.   
• YRSA- $W _ { t }$ : The same as the one above but using Eq. (10) as lexicon instead of Eq. (9).   
• Literal: In this case, there is no iteration and we simply use Eq. (7) to predict the target. We use lexicon of Eq (9).   
• Literal- $W _ { t }$ : This is the same as above but using Eq. (10) as lexicon.   
• Prior: In this case, we compute $P ( \boldsymbol { y } | m _ { L _ { t } } )$ from $P ( m _ { S _ { t } } , m _ { L _ { t } } , y )$ for all turns instead of $L _ { t } ( y | u _ { t } , m _ { L _ { t } } , w _ { t } )$ . This case does not account for the dialog or the current utterance.

AAAAAA AAAAAA AAAAAA AAAAAB 111112 AAAAAB 111112 AAAAAB AAAABA 111121 AAAABA 112 AAAABA 22 AAAABB 11122 AAAABB AAAABB AAABAA 1112 AAABAA 12 AAABAA 111212 AAABAB 111212 AAABAB 11212 AAABAB 111221 AAABBA AAABBA 111221 AAABBA AAABBB 222 AAABBB 111222 AAABBB AABAAA 112111 AABAAA 112111 AABAAA 112112 AABAAB 112112 AABAAB 112112 AABAAB 112121 AABABA 112121 AABABA 112121 AABABA 112122 AABABB 112122 AABABB 112122 AABABB 112211 AABBAA 112211 AABBAA 112211 AABBAA 112212 AABBAB 112212 AABBAB 112212 AABBAB 112221 AABBBA 112221 AABBBA 112221 AABBBA 112222 AABBBB 112222 AABBBB 112222 AABBBB ABAAAA 121111 ABAAAA ABAAAA 1211 ABAAAB 121112 ABAAAB ABAAAB 121121 ABAABA 121121 ABAABA ABAABA 121 ABAABB 121122 ABAABB ABAABB 12121 ABABAA 121211 ABABAA ABABAA 12121 ABABAB 121212 ABABAB ABABAB 12122 ABABBA 121221 ABABBA ABABBA 12122 ABABBB 121222 ABABBB ABABBB 12211 12211 ABBAAA ABBAAB 122112 122111 ABBAAB ABBAAA 122111 122112 ABBAAA ABBAAB 12212 ABBABA 122121 ABBABA 12212 ABBABA 122122 ABBABB 122122 ABBABB 122122 ABBABB 12221 ABBBABA 1222112 ABBBABA 1222112 ABBBABA 12222 ABBBBA 122221 ABBBBA 12 ABBBBA ABBBBB 122222 ABBBBB 2222 ABBBBB 211 BAAAAA 211111 BAAAAA BAAAAA 2 BAAAAB 211112 BAAAAB BAAAAB 211 BAAABA 211121 BAAABA BAAABA 11 BAAABB 211122 BAAABB 111 BAAABB 2112 BAABAA 211211 BAABAA 12 BAABAA 21 BAABAB 211212 BAABAB 1121 BAABAB 211 BAABBA 211221 BAABBA 1122 BAABBA BAABBB 211222 BAABBB 21122 BAABBB 2121 BABAAA 212111 BABAAA 121 BABAAA 12 BABAAB 212112 BABAAB 21211 BABAAB 1212 BABABA 212121 BABABA 212121 BABABA 212122 BABABB 2 12122 BABABB 212122 BABABB 122 BABBAA 122 BABBAA 12 BABBAA 122 BABBAB BABBAB 212212 BABBAB 2 BABBBA 1222 BABBBA 21222 BABBBA BABBBB 222 BABBBB 212222 BABBBB BBAAAA BBAAAA 211 BBAAAA BBAAAB BBAAAB 221112 BBAAAB BBAABA BBAABA 221121 BBAABA BBAABB BBAABB 221122 BBAABB BBABAA 12 BBABAA 212 BBABAA BBABAB 12 BBABAB 221212 BBABAB BBABBA 122 BBABBA 22122 BBABBA BBABBB BBABBB 221222 BBABBB BBBAAA BBBAAA 221 BBBAAA BBBAAB BBBAAB 22112 BBBAAB BBBABA BBBABA 222121 BBBABA BBBABB 212 BBBABB 222122 BBBABB BBBBAA 2221 BBBBAA 2221 BBBBAA BBBBAB 22212 BBBBAB 222212 BBBBAB BBBBBA 222 BBBBBA 222221 BBBBBA BBBBBB BBBBBB 222222 BBBBBB Turn 1 Turn 2 Turn 3 Turn 4 Turn 5 Turn 6 SA: Position 1 SB: Position 5 SA: Position 5 SB: Position 5 SA: Position 2 SB: Position 5

0.20

until the gain converged using a tolerance of $1 e - 3$ , so the number of iterations may vary between each turn. We tried various values of $\alpha > 1$ and all values showed best performance of the CRSA model. For values $\alpha \leq 1$ , all iterative algorithms always produced uniform distributions.

As shown in the plots, the CRSA model outperforms all baselines across both metrics. Moreover, incorporating a lexicon that depends on the past $\boldsymbol { w } _ { t }$ neither improves nor diminishes performance, suggesting that the information encoded in Eq. (10) is already effectively captured by the CRSA model. In contrast, the information in Eq. 10 is not captured by the YRSA- $\mathbf { \nabla } _ { W _ { t } }$ model, which appears to improve as the conversation progresses. As expected, models that do not incorporate dialog history maintain consistent performance across turns, with variations driven only by role changes. We also observed that the CRSA model’s variance decreases over time, although this is not shown in the plots for clarity.

# 5.2 Numerical results and discussion

Figure 2 presents the performance of the CRSA model compared to baseline models for $\alpha = 2 . 5$ Each curve corresponds to a different model evaluated over 500 rounds of the game. The top plot displays task accuracy, measured as the proportion of correct guesses obtained by taking the argmax of the listener’s posterior probability. As accuracy may not be fully representative of the confidence on the decision made by the listener, we also show the Information Gain (in the bottom plot) for each turn $t$ , computed as the difference $\mathrm { I G } ( L _ { t } ) \ = \ H _ { P } ( Y | M _ { L _ { t } } ) \ : - \ : H _ { L } ( Y | U _ { t } , M _ { L _ { t } } , W _ { t } )$ That is, given a set of $N$ rounds (all with the same number of turns), the listener’s conditional $H _ { L } ( Y | U _ { t } , M _ { L _ { t } } , W _ { t } ) =$ $- 1 / N \textstyle \sum _ { i = 1 } ^ { N } \log L ( y ^ { ( i ) } | u _ { t } ^ { ( i ) } , w _ { t } ^ { ( i ) } , m _ { L _ { t } } ^ { ( i ) } )$ conditional entropy of the prior is defined as $\begin{array} { r c l } { H _ { P } \big ( Y | M _ { L _ { t } } \big ) } & { = } & { - 1 / N \sum _ { i = 1 } ^ { N } \log P ( y ^ { ( i ) } | m _ { L _ { t } } ^ { ( i ) } ) } \end{array}$ where the super-index $( i )$ denotes the value at round $i$ . As $P ( \boldsymbol { y } | m _ { L _ { t } } )$ takes no account for the interchanged utterances, this metric could be interpreted as the amount of information gained by using the utterances of the dialog up to turn $t$ . For all models where there is iteration, we run the model

Figure 3 presents an example of a dialogue between the agents, along with their internal belief states at each turn. This dialog was generated by sampling the pragmatic speaker distribution at each turn. Each column displays the value of $B _ { S , t } ( m _ { L _ { t } } )$ for each possible meaning $m _ { L _ { t } }$ of the listener at turn $t$ . Notably, as the conversation progresses, the meanings associated with previously uttered messages tend to gain higher belief values, reflecting a refinement in the speaker’s inference about the listener’s state. We note that speaker at turn 5 produce the "Position $2 "$ utterance, which is a little uninintuitive. However, since utterances are sampled from the full pragmatic speaker distribution rather than chosen greedily and "Position $2 "$ has non-zero probability, this utterance was drawn randomly out of other possible utterances. Such decoding strategy can be interpreted as an exploratory strategy. Importantly, Agent B’s return to "Position $5 "$ in the following turn is consistent with its high posterior belief. We also note that the value that maximizes $B _ { S , t } ( m _ { L _ { t } } )$ at turn 6 does not correspond exactly to the correct meaning, but it is a close approximation since the utterance “Position $6 ^ { \circ }$ never occurred during the round. This supports interpreting $B _ { S , t } ( m _ { L _ { t } } )$ as the speaker’s belief about the listener’s meaning $m _ { L _ { t } }$ at turn $t$ .

# 6 Modeling Conversations Using Pragmatic LLMs

In this Section, we provide preliminary evidence that the CRSA model can produce reasonably good estimations of both the likelihood of each utterance $u _ { t }$ and the target $y$ of the task in doctor–patient conversations, which is the disease corresponding to the symptoms described by the patient. To this end, we used the MDDial dataset (Macherla et al., 2023), which consists of template-based conversations between a doctor and a patient. In each dialog, the patient is assigned a subset of predefined symptoms, and the doctor must determine the correct disease from a set of possible pathologies.

As anticipated in Section 4.2.2, in order to apply the pragmatic models, we compute the literal speaker with equation 8 using a pre-trained LLaMA3.2–1B-Instruct language model. In this equation, $\mathrm { p r o m p t } ( m _ { S _ { t } } )$ is the text used to prompt the model with the relevant medical scenario. When $S _ { t }$ is the doctor, the prompt includes specific instructions to ask questions and produce a diagnosis, followed by two example doctor–patient conversations. When $S _ { t }$ is the patient, the prompt instructs the model to play the role of the patient. It uses the same conversation examples as in the doctor prompt but additionally includes the patient’s current symptoms at that turn. The full prompts used can be found in Appendix C. In order to save computation, we pre-computed the value of the literal speaker for each possible combination of utterance $u _ { t }$ and symptoms $m _ { S _ { t } }$ for each turn $t$ and then applied the update equations to that.

Finally, since each dialog ends with the doctor explicitly stating the disease, we compute the final listener distribution $L _ { T ^ { ( i ) } } ( y | u _ { T ^ { ( i ) } } , m _ { L _ { T ^ { ( i ) } } } , w _ { T ^ { ( i ) } } )$ for the last turn $\boldsymbol { T } ^ { ( i ) }$ of round $i$ by replacing the disease name in the utterance with each candidate diagnosis $y \in \mathcal { V }$ . This probability is used at each round as the listener probability of the literal model.

# 6.1 Numerical results and discussion

To evaluate performance, we compute the speaker entropy as $\begin{array} { r } { \begin{array} { r } { H _ { S } \ = \ - \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \sum _ { t = 1 } ^ { T _ { i } } \log S _ { t } \big ( u _ { t } ^ { ( i ) } \ \mid } \end{array} } \end{array}$ $w _ { t } ^ { ( i ) } , m _ { S _ { t } } ^ { ( i ) } )$ , where $N$ is the number of rounds and $T _ { i }$ is the number of turns in round $i$ . For the Listener, we compute $\begin{array} { r } { H _ { L } = - \frac { 1 } { N } \sum _ { i = 1 } ^ { N } L _ { T _ { i } } ( y \mid } \end{array}$ $u _ { T _ { i } } , m _ { L _ { T _ { i } } } , w _ { T _ { i } } )$ , which is analogous to the method described in Section 5, but considers only the distribution at the final turn of each round.

Table 1: Entropies for $\alpha = 2 . 5$ of the listener and the speaker for each model, computed for 65 samples of the MDDial dataset.   

<html><body><table><tr><td></td><td>Hs</td><td>HL</td></tr><tr><td>CRSA</td><td>8.18</td><td>2.19</td></tr><tr><td>RSA</td><td>14.27</td><td>2.86</td></tr><tr><td>Literal</td><td>18.00</td><td>3.29</td></tr></table></body></html>

The results are presented in Table 1 for 65 samples of the dataset and for a value of $\alpha = 2 . 5$ , which is the same as used for Section 5. We observed the same trend mentioned in that Section when varying the value of $\alpha$ . The CRSA model achieves substantially lower entropy for both the speaker and the listener, outperforming both the RSA and Literal baselines. While the utterances in the MDDial dataset are not naturally produced—since they follow pre-defined templates—these results offer initial evidence that CRSA captures key aspects of pragmatic reasoning and task-oriented belief updates across turns.

# 7 Possible Future Directions of this work

There are many ways in which the CRSA model can be improved. One of the major limitations of the model is that there is no systematic way of directly modeling the meaning spaces $\mathcal { M } _ { A }$ and $\mathcal { M } _ { B }$ , which are always application-dependent. One possible way of moving towards a scalable application of the CRSA model in the large language model architecture is by modeling these spaces as continuous. That is, producing an embedding representation of each private meaning, $e _ { A } , e _ { B } \in \mathbb { R } ^ { d }$ , and incorporating that embedding into the computation of $S _ { t } ( u _ { t } | w _ { t } , m _ { S _ { t } } )$ . Although the equations of the model remain essentially the same in this scenario, this approach opens many challenging points. For instance, the computation of the sums in the model’s equations, which now become integrals; the way of combining the language model with the embedding $e _ { S _ { t } }$ in order to compute $S _ { t } ( u _ { t } | w _ { t } , m _ { S _ { t } } )$ , which is definitely non-unique; or the modeling of $p ( m _ { A } , m _ { B } , y )$ , which now becomes a mixed probability function.

In addition to this, there is the problem of modeling the space of utterances, which is inherited from classic RSA. However, since the past utterances are part of the design of the CRSA, the natural way to scale this model to more realistic applications in which generation is done token by token is by directly replacing utterances with tokens. We expect that this shift may influence the model’s pragmatic capabilities, since the reasoning is performed at the token level, not at the utterance level. We intend to investigate these trade-offs carefully in future work.

Finally, there are many ways in which the original gain function from which the equations of the model are derived could be modified depending on the application scenario. For instance, situations in which the meanings are not fixed in time, or where more than two agents participate in the dialogue, can also fit within a similar procedure to that used in this work. This allows for the introduction of pragmatic reasoning in more realistic scenarios in the same mathematically grounded way as was done in the current work.

# 8 Summary and Concluding Remarks

In this work, we introduced the Collaborative Rational Speech Act (CRSA) framework, an information-theoretic extension of RSA tailored for principled pragmatic reasoning in multi-turn, task-oriented dialogues. By integrating a novel multi-turn gain function grounded in interactive rate-distortion theory, CRSA effectively models the evolving belief dynamics of both interlocutors, overcoming key limitations of traditional RSA in collaborative contexts. Our preliminary results demonstrate that CRSA successfully captures the progression of shared understanding, partner beliefs, and utterance generation, providing the way for more natural and efficient communication in complex conversational settings.

CRSA lays the foundation for developing conversational agents driven by mathematically grounded principles of pragmatic reasoning. This principled formulation enhances both the interpretability and controllability of agent behavior, enabling the construction of language models that move beyond surface-level fluency to demonstrate structured, socially coherent, and contextually appropriate dialogue. In this way, CRSA represents a significant step toward building pragmatic agents whose interactions are not only effective but also firmly rooted in the formal theory of communication.

# Limitations

This work focuses on simulated referential games and template-based doctor–patient dialogues, which, while controlled and insightful, do not capture the full variability and complexity of real-world conversations. Additionally, the CRSA framework relies on a fixed, predefined set of possible utterances at each turn, limiting its applicability to open-ended or generative dialogue scenarios involving variable-length token sequences. These factors currently restrict the scalability of our approach to more naturalistic domains. Future work will aim to overcome these limitations by extending CRSA to handle dynamically generated utterance spaces and by evaluating its effectiveness in less structured, real-world conversational settings.

# Ethical considerations

This work presents a theoretically grounded framework for pragmatic reasoning in multi-turn dialogs. It is primarily methodological and does not involve direct deployment or interaction with real users. The datasets employed—simulated referential games and template-based medical dialogues—are synthetic and contain no personal or sensitive data.

However, since CRSA aims to inform the development of more interpretable, goal-driven conversational agents, potential applications in sensitive domains like automatic medical diagnosis raise important ethical considerations. In such contexts, errors in belief tracking or task inference could result in incorrect recommendations, especially if users overestimate the system’s understanding or authority. While our medical domain experiments are purely illustrative and not intended for clinical use, they underscore the critical need for caution when adapting theoretical models to real-world diagnostic settings. Future deployments must involve rigorous domain-specific validation, proper oversight, and human supervision to ensure safety and reliability.