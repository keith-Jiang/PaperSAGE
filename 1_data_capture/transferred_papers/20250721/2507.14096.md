# Graphical Abstract

# Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track

Brian Ondov, William Xia, Kush Attal, Ishita Unde, Jerry He, Hoa Dang, Ian Soboroff, Dina Demner-Fushman

STask 1: Rewrite Task 2: Handle Jargon   
Remeoalofsetser Adat Task 2A Task 1B Task 1C Generate Nephrologists reviewed medical records for Kidney specialists looked at medical   
  
ofjargon 三 We identifed a 1:100 time-matched cohort For comparisonwealsofoundgoup Thevisueydedis Generalizatkon Seitigio epPar -term- SUBSTITUTES T 21,595 dlniPy 2 The visual acuty abityto se saldealsina sandadvision tesy had replacements for term 10,314 Me40Plus X 100d = 400 1,600 Vi terms

# Highlights

# Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track

Brian Ondov, William Xia, Kush Attal, Ishita Unde, Jerry He, Hoa Dang, Ian Soboroff, Dina Demner-Fushman

• We introduce the new shared task of plain language adaptation of biomedical abstracts.   
Teams from twelve countries submitted runs with systems based on BERT, BART, T5, GPT, Llama, Gemini, and Mistral.   
• In manual judgments, top-performing models neared human levels of factual accuracy, but not simplicity or brevity.   
• Automatic, reference-based metrics generally did not correlate well with manual judgments.

# Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track

Brian Ondov $^ { \mathrm { a } , \ast }$ , William Xia $^ \mathrm { b } _ { , }$ , Kush Attal $\mathrm { c }$ , Ishita Unde $\mathrm { d }$ , Jerry He $^ \mathrm { e }$ , Hoa Dang $\mathrm { f }$ , Ian Soborofff, Dina Demner-Fushman $^ \mathrm { g }$

$a$ Yale School of Medicine, 333 Cedar Street, New Haven, 06510, CT, USA $b$ Tufts University, 419 Boston Avenue, Medford, 02155, MA, USA $c$ NYU Grossman School of Medicine, 550 First Avenue, New York, 10016, NY, USA $d$ Johns Hopkins University, 3400 N. Charles Street, Baltimore, 21218, MD, USA $e$ Georgia Institute of Technology, North Avenue, Atlanta, 30332, GA, USA $f$ National Institute of Standards and Technology, 100 Bureau Drive, Gaithersburg, 20899, MD, USA $g$ National Library of Medicine, 8600 Rockville Pike, Bethesda, 20894, MD, USA

# Abstract

Objective: Recent advances in language models have shown potential to adapt professional-facing biomedical literature to plain language, making it accessible to patients and caregivers. However, their unpredictability, combined with the high potential for harm in this domain, means rigorous evaluation is necessary. Our goals with this track were to stimulate research and to provide high-quality evaluation of the most promising systems.

Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts (PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included complete, sentence-level, rewriting of abstracts (Task 1) as well as identifying and replacing difficult terms (Task 2). For automatic evaluation of Task 1, we developed a four-fold set of professionally-written references. Submissions for both Tasks 1 and 2 were provided extensive manual evaluation from biomedical experts.

Results: Twelve teams spanning twelve countries participated in the track, with models from multilayer perceptrons to large pretrained transformers. In manual judgments of Task 1, top-performing models rivaled human levels of factual accuracy and completeness, but not simplicity or brevity. Automatic, reference-based metrics generally did not correlate well with manual judgments. In Task 2, systems struggled with identifying difficult terms and classifying how to replace them. When generating replacements, however, LLM-based systems did well in manually judged accuracy, completeness, and simplicity, though not in brevity.

Conclusion: The PLABA track showed promise for using Large Language Models to adapt biomedical literature for the general public, while also highlighting their deficiencies and the need for improved automatic benchmarking tools.

# Keywords:

Evaluation, Large Language Models, Plain Language, Text Simplification

# 1. Introduction

The inability of patients to fully understand available information about their health has a significant impact on outcomes (Stableford and Mettger, 2007; Berkman et al., 2011; King, 2010; Kindig et al., 2004). While more information than ever is available on the web, the “language barrier” of professional-facing content, such as literature, can make it counterproductive (Rosenberg et al., 2017; White and Horvitz, 2009). Though many consumer-facing knowledge bases exist, these are cumbersome and laborintensive to update and thus typically do not include the latest medical knowledge from the literature. This is especially important for consumers seeking out the latest information about emerging health issues (SeyyedHosseini et al., 2022).

The many recent, successful applications of Deep Learning to the field of Natural Language Processing suggest the possibility of automatic adaptation of text intended for a professional audience to text that can be read and understood by a consumer audience. Though Text Simplification has been widely studied in the open domain (Al-Thanyyan and Azmi, 2021), the unique lexicon of biomedical literature makes it a distinct sublanguage (Friedman et al., 2002). In this respect, the task overlaps with Machine Translation (Stahlberg, 2020). Recent examples of applying Deep Learning to Text Simplification specifically in the biomedical domain include Van den Bercken et al. (2019), Devaraj et al. (2021), and Flores et al. (2023). However, the existing body of research evaluates systems with disparate datasets that are mined from imperfect sources, making progress difficult to measure.

With the aims of providing common benchmarks, encouraging progress, and rigorous expert evaluation, we hosted the Plain Language Adaptation of Biomedical Abstracts (PLABA) shared task at the Text REtrieval Conference (TREC), spanning 2023 and 2024. This track included tasks for rewriting abstracts from the biomedical literature for the general public, as well as identifying expert terms and providing replacements or additional text for clarification. By encouraging creation of systems that can coherently rewrite entire abstracts, but at the sentence level, we hope to allow the public to engage with the latest research, while providing fine-grained provenance of each statement to ensure transparency and reliability.

Across both years, twelve teams spanning twelve countries participated, submitting a total of 38 runs. Systems were diverse and powered by a variety of models, from Multilayer Perceptrons, to encoder-only transformer models such as BioBERT (Lee et al., 2020) and RoBERTa (Liu et al., 2019), to encoder-decoder models like BART (Lewis et al., 2020a; Yuan et al., 2022) and T5 (Raffel et al., 2020), to modern, instruction-tuned decoderonly Large Language Models, including variants of Llama (Touvron et al., 2023), GPT (Brown et al., 2020; Ouyang et al., 2022), Gemini (Team et al., 2023), and Mistral (Karamcheti et al., 2021). We find that the best performing systems can be highly factually accurate while making abstracts much more comprehensible to the general public. Still, occasional falsehoods and hallucinations mean that care must be taken in deploying such systems, especially given the risk of harm in the biomedical domain. Additionally, the task revealed there is work to be done in automatically evaluating both the simplicity and factuality of outputs. We hope the lessons from the PLABA shared task will inform future tasks and research directions.

# 2. Related Work

Shared tasks on text simplification go back at least to Task 1 of the Sixth International Workshop on Semantic Evaluation (SemEval 2012) (Specia et al., 2012). This task challenged teams to rank replacement words by simplicity, given context for disambiguation. Lists of potential lexical replacements for this task came from SemEval 2007 Task 10, for which the goal was to provide substitutions, regardless of simplicity (McCarthy and Navigli, 2007). Both these tasks, however, were open domain, and did not focus on scientific text, which adds the complexity of technical language or jargon.

Beginning with a pilot in 2021, the SimpleText track (Ermakova et al., 2021, 2022c, 2023c, 2024b) has been hosted yearly at the Conference and Labs of the Evaluation Forum (CLEF), a long-running European initiative with similar goals to TREC. As with PLABA, the SimpleText track aims to evaluate systems for simplifying scientific text. However, SimpleText has largely focused on Computer Science literature, with two relevant exceptions. In 2022, SimpleText data included a narrowly focused biomedical sub-corpus, comprising articles resulting from searching PubMed and Google Scholar for muscle health and hypertrophy. In 2024 a similar sub-corpus was included but expanding the search to articles on “health and medicine.”

Each offering of SimpleText has had variations of three core tasks. Task 1 involves identifying passages of a scientific article that would be helpful for understanding by the general public (SanJuan et al., 2022, 2023, 2024). Task 2 involves identifying difficult terms and providing definitions or explanations (Ermakova et al., 2022a, 2023a; Di Nunzio et al., 2024). This task is similar to PLABA Task 2, though PLABA’s Task 2 additionally allows each term to be substituted, generalized, exemplified, or omitted, and further challenges teams to identify which of these are appropriate given the term and context. SimpleText Task 3 involves rewriting sentences or abstracts for a general audience, similar to PLABA’s Task 1 (Ermakova et al., 2022b, 2023b, 2024a). The 2024 offering of SimpleText further included Task 4, which involves extraction of reported state-of-the-art results, though this only focused on Computer Science literature on the topic of Artificial Intelligence (D’Souza et al., 2024).

In addition to being focused solely on the biomedical domain, PLABA differentiates from SimpleText in the extent of manual evaluation. Evaluation of SimpleText Task 3 largely focuses on automatic, reference-based metrics, such as BLEU (Papineni et al., 2002) and SARI (Xu et al., 2016), and count-based readability measures, such as FKGL (Kincaid et al., 1975). The 2024 offering additionally included analysis of spurious content (i.e. hallucinations) via alignment against sources. Extensive manual evaluation of a subset of outputs was performed for SimpleText 2022’s Task 3, but only for the Computer Science corpus. Though manual evaluation was performed for SimpleText 2024’s Task 3, reported results were limited to overall observations based on a small sample of outputs. PLABA’s Task 1 adds to this work by providing in-depth manual evaluation for a purely biomedical test corpus, judging outputs for every sentence of 40 abstracts in 2023 and 400 abstracts in 2024.

Finally, the BioLaySumm track (Goldsack et al., 2023, 2024) has been hosted at the 22nd and 23rd Workshops on Biomedical Natural Language Processing (BioNLP 2023 and 2024), co-located with the 61st and 62nd Annual Meetings of the Association for Computational Linguistics (ACL 2023 and 2024), respectively. Like PLABA, BioLaySumm aims to evaluate and improve systems for conveying health information to consumers. However, the core BioLaySumm task is to summarize a complete scientific article into a short plain language paragraph, rather than to adapt each sentence of a scientific abstract. This assumes a large amount of information loss, giving readers only key takeaways. In contrast, by challenging teams to adapt each sentence of an abstract, PLABA involves preserving as much information as possible, in a manner that is more like ‘translating’ from expert to plain language. This approach would offer readers direct provenance of each line of the plain language version, allowing them to check for consistency and learn more about the expert terminology.

# 3. Tasks & Data

The PLABA track focused on public biomedical abstracts, sourced from PubMed. Abstracts are useful because they are freely available and summarize the relevant findings from the latest literature, even if the complete articles are not open access. They are also small enough to be tractable and easily evaluated as whole units, but still large and varied enough to contain much of the types of text encountered in literature, such as biomedical terms, clinical jargon, and statistical details. The inaugural offering of the PLABA track had a single task, which was complete, sentence-by-sentence rewriting of biomedical abstracts, which we refer to here as Task 1. For the second year, an additional task of identifying and replacing complex terms was added, which we refer to as Task 2.

# 3.1. Task 1: Rewriting abstracts

Task 1 challenged teams to rewrite biomedical abstracts at the sentence level and followed the format of the PLABA dataset (Attal et al., 2023).

# 3.1.1. Task Definition

For a set of 40 consumer health questions, teams are presented with ten abstracts relevant to each question, each one split into sentences. The task is to create a sentence-aligned plain language adaptation of each abstract

Remohvealadofersection Buascekogfrocuentdraflotrefirrmst Purpose: To fSinodurpcre (diPctMiIvDe 3m5a5rk11er7s8f1o)r the visual This study Aaidmaeptdatio fninbdyoHutuimf avins Eoxnpceartn be Source (PMID 34362836) Adaptation by human expert Background: Little population-based data Nephrotic Syndrome (NS) is a potential in optical coherence tomography (OCT) predicted by diagnostic imaging one month after exist about adults with primary nephrotic combination of symptoms relating to the one month after surgical repair of macula- surgery to repair common retinal detachment (the syndrome. kidneys. There is not much information involving rhegmatogenous retinal detachment damage to light-sensitive layer at the back of the about how NS exists in the population. (miRD) with and without internal limiting eye (retina) when it is pulled away from its normal Nephrologists reviewed medical records for Kidney specialists looked at medical membrane (ILM) peeling. position). In some surgeries, the tissues next to clinical presentation, laboratory findings, records to confirm NS and determined retina were pulled to repair the detachment. and biopsy results to confirm primary causes. aCbobrnesviisatteinotn Methods: This retrospective single-center, single- This study looked at past cases at a single health Replacement nephrotic syndrome and assigned etiology. surgeon cohort study included 74 patients who care center, and that had surgery done by a single of jargon noWfepaihdrueoltnistcifwiseiytdhnodaru1otmd1i0ea,0boteirtmepser,-otdmeiaiantgcunhrioeasdeacdsohort Fopfroratdceiuonltmusrpiawa ir(titshoono,mwduiecahba lepstroeosft,oeiuNnSid,noatrhger oup up2rn0id2me0arrwyiethmn ifRopllDaorsbwe-ptulwapneeaxnvaiJtmraiencuattoairomynys2(0fPo1rP3Vat)nlfedoarAstug6ust surgeon. Generalization controls to compare rates of ESKD, urine) through 2014. This group was to months. cardiova ar outcomes, and death compare rates of kidney failure, heart Patients developing recurrent detachments, media Patients who had other retina detachments in the trhergoreusgshio2n0.14, using multivariable Cox dmisetehaosde,s.and death using statistical oexpcalcuitdiesd, rormw theaannaxliyasl lse. ngth over 27 mm were pnaotstinocrlumdedi ianotphaecisttiuedsy(.such as cataracts) were Generalization Splsitetintgenocfelsong explaPnartieonthoent fcirasl  use Omission Substitution Exemplification (a) (b)   
19[Sentence 3] expert_term tm Simplification Type Count Proportion   
20The visual acuity had decreased in many patients. -term- Substitutions 13,966 0.6467   
2 Explanations 4,161 0.1927 -term-term- Omissions 1,963 0.0909 EXPLAINS   
23The visual acuity (abilitytosee smalldetails inastandard vision test) had decreased in many patients. Generalizations 1,368 0.0633 ←term SUBSTITUTES Exemplifications 137 0.0063 Visual sharpness had decreased in many patients. (c) (d)

(Fig. 1). Being sentence-aligned means that output must be provided for each source sentence, but the entire rewritten abstract is also expected to read fluently as one document (e.g. not repeatedly explain expert terms). As in the PLABA dataset, sentences may be split, meaning output for a single source sentence can have multiple sentences, and this is in fact encouraged when source sentences are long and complex. Sentences may also be dropped if they are deemed not relevant to a consumer’s understanding of the abstract. However, sentences may not be merged (i.e. providing a single output sentence that spans the semantic content of multiple source sentences), to ensure there is a direct and consistent mapping from source to output for evaluation purposes.

# 3.1.2. Training Data

The training data for the task comprises the 750 abstracts initially published by Attal et al. (2023) chosen to answer 75 consumer health questions. Each abstract is manually adapted (rewritten) by at least one biomedical expert, for a total of 921 gold references. However, teams were welcome to use any other data at their disposal.

# 3.1.3. Test Data

As the test split from Attal et al. (2023) was already public and thus not suitable for shared task evaluation, in each year of Task 1 (2023 and 2024) we followed the same workflow as Attal et al. (2023) to choose an additional 40 questions and 10 abstracts for each question, totaling 400 abstracts. In the first offering (at TREC 2023), we created gold-standard manual reference adaptations of the 400-abstract test set. For robustness of automatic metrics, each abstract was adapted by four different annotators for the gold reference set. Three authors each adapted all 400 abstracts. The fourth adaptation for each abstract was performed by one of four contracted science writers. For the second offering (at TREC 2024), we did not create references for the test set and instead focused on manual evaluation (see §4.2).

# 3.2. Task 2: Identifying and Replacing Complex Terms

The initial offering of Task 1 (end-to-end rewriting of abstracts) at TREC 2023 revealed that language models on the order of billions of parameters were necessary for competitive performance. This could have limited participation in the first offering, as expensive, state-of-the-art GPUs and servers are required to fine-tune such models. Zero-shot configurations, on the other hand, required recent commercial models, which were also associated with significant costs. For the second year of PLABA at TREC, we thus designed a new sub-task around identifying and replacing terms, rather than complete rewriting. One goal of this task was partly to lower the barrier to entry by including shorter generations and classification tasks. Another was to generate finer-grained feedback for the lexical aspect of plain language adaptation.

# 3.2.1. Task Definition

An important step towards making biomedical text legible to consumers is replacing or explaining jargon or otherwise difficult terms. However, in existing text adapted for consumers, terms may be replaced or explained in various ways, depending on factors such as available synonyms, conceptual difficulty and importance of the term to the sentence or abstract. To code a set of replacement types, a group of three biomedical informatics experts examined the types of replacements in the PLABA dataset Attal et al. (2023). In multiple rounds of discussion, the experts finalized proposed types and suggested new types until a consensus was reached. This resulted in five types:

• Substitute: If the term is jargon for a more commonly understood concept, it can simply be replaced. For example, myocardial infarction is the technical term for heart attack. This may also apply to open-domain but more arcane words, for example indicate can often be replaced with the synonym show.   
• Explain: Other terms may not have any commonly understood equivalent, such as duodenum. If such a term is important to the thought being conveyed, it can be left in the text but explained. Syntactically, this could take many forms, such as nonrestrictive clauses, parenthesis, or additional sentences. This type of replacement risks lengthening the original text, interrupt the flow of a sentence, or potentially introducing more complex jargon within the provided explanation. Generalize: Many difficult terms have more broadly understandable hypernyms or superordinate concepts. For example, nucleic acid amplification test may be generalized as lab test if the specific type is not crucial to conveying the idea of the sentence.   
• Exemplify: A difficult concept may have widely known examples that can be provided alongside the term to elucidate it. For example, “such as Parkinson’s” may be inserted after neurodegenerative diseases. In a sense, this operation is the inverse of generalization. Care must be taken, however, that the examples provided do not break assumptions in the surrounding context.   
• Omit: If a difficult term is not necessary to understand the main thought that is being conveyed, the sentence may be rewritten in a way that avoids it entirely.

Note that Explain and Exemplify assume preservation of the original term in the new text, while Substitute, Generalize, and Omit assume its removal.

Task 2 was broken into three sub-tasks. Teams could participate in any number of these, but each successive sub-task required participation in all previous sub-tasks.

• Task 2A - Identification: The first sub-task was to identify difficult terms. Given the text of an entire abstract, systems should return a list of unique substrings representing words or phrases that a consumer would not understand.   
• Task 2B - Classification: Given identified expert terms, the second sub-task was to classify how the terms should be replaced. As there may be multiple valid ways a human writer could handle replacing a given term, this is framed as a multi-label problem (that is, a binary classification problem for each of the five labels).   
• Task 2C - Generation: The third sub-task was the generate simplifications (replacement terms for substitute and generalize or additional text for explain and exemplify) given an abstract and the expert terms identified and classified within it. Note that if a term was classified only as omit in Task 2B, it would not have any generation for Task 2C.

# 3.2.2. Data

The Task 2 dataset is derived from the 400 abstracts and associated adaptations created for the 2023 Task 1 test data. Abstracts were aligned at the sentence level with their corresponding adaptations, then annotated by two authors using the brat rapid annotation tool $^ { 1 }$ (Stenetorp et al., 2012), which involved selecting expert terms and linking them with their respective simplifications. Figure 1c shows an example of the brat interface during annotation. In total, the Task 2 dataset contains 10,314 expert terms (25.79 terms per abstract) and 21,595 simplifications. Table 1d displays counts of each simplification type. As many terms appear throughout an abstract in different forms (alternative wordings or abbreviations), annotators were also tasked with linking these synonyms. Annotations of replacements then only had to be performed for one representative term across the abstract. These annotations were then propagated to all synonyms during annotation postprocessing (see Appendix B).

The annotations exhibit a moderate inter-annotator agreement for both the identification task (0.5203 F1) and the classification task (0.4577 F1). Investigating disagreements for identification revealed that many related to minor differences in boundaries of annotated spans. We account for this during evaluation of Task 2A by requiring only 75% overlap with a reference span to be considered correct. Upon investigating lower than ideal agreement for classification, we attribute it to multiple replacement types being valid for a given term in a given context, even though annotators had to choose one. We account for this in evaluation of Task 2B by framing it as a multilabel, rather than multiclass, classification problem. For further details on creation of the dataset for Task 2, see Xia et al. (2025).

# 4. Evaluation

Here we describe how submissions were evaluated across all sub-tasks, including manual and automatic evaluation.

# 4.1. Task 1 at TREC 2023

For the initial offering of Task 1, we included both automatic and manual evaluation. As all evaluation was performed at the sentence level, we also developed a pipeline to automatically align submissions, allowing teams to make document-level submissions if they preferred.

# 4.1.1. Automatic Evaluation

As the primary metric for automatic, reference-based evaluation, we adopt SARI, a metric specifically designed to assess simplification by including the source and balancing n-grams kept, inserted, and deleted in the references (Xu et al., 2016). Aside from the original implementation, several others exist, including in the EASSE package (Alva-Manchego et al., 2019) and in the Huggingface evaluate package (Wolf et al., 2019). Notably, the Huggingface implementation has several differences that purport to fix issues with the original implementation, leading to significantly different scores for some passages.2 However, to our knowledge, only the original implementation has been shown to correlate with human judgments, and we thus use this implementation for the official results.3 For analysis of correlation of metrics with human judgments, we also compute SAMSA (Sulem et al., 2018), a referencefree metric based on semantic structure, using the EASSE package (AlvaManchego et al., 2019), and BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and BERTscore (Zhang et al., 2019) using the Huggingface evaluate package (Wolf et al., 2019).

Table 1: Axes for manual evaluation of Task 1 at TREC 2023.   

<html><body><table><tr><td colspan="3">Simplicity</td></tr><tr><td></td><td>SEN Sentence simplicity TRM Term simplicity</td><td>Are long, complex sentences appropriately split? Are expert terms in the source replaced with al- ternatives,or explained,either in this sentence or</td></tr><tr><td></td><td>TAC Term accuracy</td><td>a previous sentence? Are substitutions and explanations of expert</td></tr><tr><td>FLU Fluency</td><td></td><td>terms accurate? Does the output follow grammatical rules and</td></tr><tr><td></td><td>SIM Final simplicity</td><td>read smoothly? Average(SEN,TRM,TAC,FLU)</td></tr><tr><td colspan="3">Accuracy</td></tr><tr><td>COM Completeness</td><td></td><td>How much of the source information does the out- put provide?</td></tr><tr><td></td><td>FTH Faithfulness</td><td>Do points made in the output match those of the</td></tr><tr><td></td><td>ACC Final accuracy</td><td>source? Average(COM,FTH)</td></tr><tr><td>FIN Final score</td><td></td><td>Average(SIM,ACC)</td></tr></table></body></html>

# 4.1.2. Manual Evaluation

For each of the 40 consumer questions, we randomly selected one of the ten abstracts retrieved for that question, creating a manual evaluation set of 40 abstracts representing all questions. Evaluation was divided into two main axes: simplicity and accuracy (Table 1). Simplicity was further broken into four sub-axes: sentence simplicity, term simplicity, term accuracy, and fluency. Accuracy was broken into two sub-axes: completeness and faithfulness. Note that we instructed annotators to judge faithfulness based on accurately carrying over statements made in the abstract, rather than comparing to general medical consensus. This was both because we aim to preserve the nuanced findings of various research studies and because medical consensus

can be subjective.

Simplicity judgments were made for system output for each sentence of the set of 40 manual evaluation abstracts. We anticipated accuracy would be more labor-intensive due to the possibility of nuance and the need to research highly specialized topics discussed in the abstracts. We thus created a further restricted set of 3 sentences within each of these abstracts for accuracy judgments. To choose these sentences, each of three authors choose up to three lines of each abstract that they thought best answered or were most relevant to the consumer question for which the abstract was retrieved. Within each abstract, all lines were then ranked by the number of annotators choosing it, and the top three were chosen, with ties broken randomly.

We contracted four science writing experts to perform the annotations. Annotators were provided with rubrics for 3-point likert scale (-1, 0, 1) judgments for each axis and trained via a live video session. For final results, averages of likert scale values were linearly interpolated from (-1, 1) to (0, 100) for easier interpretation. As a pilot, all four annotated the first abstract, after which significant differences were discussed and questions clarified during a followup video session. During full evaluation of systems, a subset was double-annotated to compute inter-annotator agreement (see Appendix G).

# 4.1.3. Automatic Sentence Alignment

Though PLABA is a sentence-level task, we anticipated that the importance of context would make document-level systems attractive. However, such systems typically do not assure sentence-alignment, which we require for evaluation. To give teams this option without the added burden of aligning output sentences, we provided an automatic alignment pipeline for document-level submissions. This pipeline was based on Vecalign (Thompson and Koehn, 2019), which was developed for bilingual alignment for Machine Translation tasks but worked well in this setting. Vecalign implements efficient dynamic programming using embeddings to score similarity. Additionally, embeddings of overlapping blocks of sentences are used to allow many-to-many alignment. As in Thompson and Koehn (2019), we use LASER (Artetxe and Schwenk, 2019) for embeddings. Initial sentencesegmentation of document-level system outputs was performed using Stanza (Q et al., 2020). Finally, since Vecalign allows insertions in one-to-many mode, but the PLABA task does not, we also developed an assignment algorithm to handle these cases. Additional details can be found in Appendix A.

# 4.2. Task 1 at TREC 2024

Though the first offering of Task 1 (at TREC 2023) included manually written-references to perform automated evaluation (§4.1.1), analysis revealed poor correlation with manual evaluation, which we consider to be the gold standard (see §5.1.5). For the second offering of Task 1 (at TREC 2024), we thus forewent expensive development of a reference set for the new test data, and instead diverted resources to further manual evaluation.

Additionally, based on results and feedback from contracted annotators from the first offering, we streamlined manual evaluation axes, resulting in four, rather than six, axes:

• SIM (Simplicity): Is the output easy to understand for a non-expert? • ACC (Accuracy): Does the output accurately reflect the source? COM (Completeness): Does the output minimize information loss? BRV (Brevity): Is the output as concise as possible? • FIN (Final score): Average of SIM, ACC, COM, and BRV.

This included merging “sentence simplicity” (SEN) and “term simplicity” (TRM) into an overall “simplicity” axis (SIM) and merging “term accuracy” (TAC) and “faithfulness” (FTH) into a simpler “accuracy” axis (ACC).

Further, for the second offering we deemed the “fluency” (FLU) axis unnecessary. Though fluency has traditionally been a basic aspect of manual evaluation of biomedical text simplification (Ondov et al., 2022), at the first offering of Task 1 at TREC 2023, 98% of manually evaluated outputs were judged to have perfect fluency. In the intervening time between TREC 2023 and TREC 2024, we expected fluency of state-of-the-art models would only continue to improve thanks to scaling laws (Kaplan et al., 2020).

Instead of fluency, we introduce brevity (BRV), which captures whether systems produce output that conveys information as concisely as possible (taking into account how much information should be conveyed). This aspect of outputs had been conflated with SEN (sentence simplicity) in the initial offering, which caused confusion among annotators and contributed to poor inter-annotator agreement for this metric (see Appendix G). We thus deemed it better to provide it as a separate axis for the second offering.

Both the focus on manual evaluation (over manually created references for automatic evaluation) and streamlining of annotation axes and guidelines meant that annotators were able to evaluate all outputs of all systems, so it was not necessary to select a subset of abstracts for manual evaluation as at TREC 2023.

# 4.3. Task 2 at TREC 2024

We used automatic metrics for evaluating Tasks 2A (identification) and 2B (classification), and manual evaluation of generations from Task 2C.

# 4.3.1. Automatic Evaluation

Identification models were evaluated using F1 score against the union of terms identified by the two Task 2 data annotators. Classification models were evaluated according to union F1 score as well. Classification model scores were macro-averaged across the five simplification methods to account for the class imbalance in our data.

# 4.3.2. Manual Evaluation of Task 2C

Evaluations were performed manually along the same 4 axes as Task 1 at TREC 2024. Each axis was rated by contractors on a 3-point symmetric likert scale, which was interpolated to a 0-100 score for reporting.

# 5. Results

Both TREC 2023 and TREC 2024 were held as hybrid events at NIST’s National Cybersecurity Center of Excellence facility in Rockville, MD, USA, in November of each year. We will present the results in three parts: the two instances of Task 1 (at TREC 2023 and 2024), and the single instance of Task 2, at TREC 2024.

# 5.1. Task 1 at TREC 2023

Four teams participated in the track. Initial submissions were due August $\mathrm { 3 0 ^ { t h } }$ , 2023, and results were announced October 18 $\operatorname { t h }$ , 2023. Following the workshop, remaining resources allowed for an additional round of manual evaluation, for which teams were invited to submit an additional run if desired. For teams that did not submit an additional run, the run they had designated as second priority from the original submissions, if they had submitted more than one, was manually evaluated.

# 5.1.1. Teams and Systems

Team submissions are detailed in Appendix C. Each team was initially allowed three submissions, which they could rank in order of priority for manual evaluation. We also asked each team to provide short description of the strategy of each submission.

# 5.1.2. Baseline Systems

For the inaugural offering of PLABA, we did not know how many submissions to expect, and thus developed a handful of our own baseline systems. These explored various prompting strategies as well as both zero-shot proprietary models and fine-tuned open-source models.

PLABA 1: Sentence-level adaptation via zero-shot auto-regressive prompting of text-davinci-003, a variant of OpenAI’s proprietary GPT 3.5 base model with instruction tuning via Reinforcement Learning from Human Feedback (Ouyang et al., 2022). This involved progressively building prompts for each sentence of the abstract, using prior model outputs as in-context examples. This inherently assigned outputs to individual source sentences, but also encouraged use of context when adapting each sentence. Note that, for a given source sentence, the model may return multiple sentences, essentially performing a split operation. However, these can still be attributed to a single source sentence, so alignment is not required. Prompt details are provided in Appendix D.

• PLABA 2: Document-level adaptation via text-davinci-003 with instruction to rewrite each sentence (prompt: “Rewrite each sentence for a lay audience:”). The goal of this prompting strategy was to ensure better sentence alignment, but as a document-level system.

• PLABA 3: Document-level adaptation via zero-shot prompting of the open-source falcon-7b-instruct model (Almazrouei et al., 2023). This baseline was included to assess how a (relatively) small, opensource language model would perform, as this type of model may be desirable for economy, privacy, and transparency. In initial tests, this model did not respond well to complex prompts such as those used for PLABA 1 and PLABA 2; thus, the more straightforward prompt “Simplify:” was used, followed by the entire text of an abstract.

• PLABA 4: Llama-2-7B-chat fine-tuned for the autoregressive prompting strategy of PLABA 1. Training is run for 70,000 steps using a batch size of 1, Low-Rank Adaptation (LoRA) (Hu et al., 2022) with $\mathrm { r } = 1 6$ and $\alpha = 3 2$ , and a maximum sequence length of 4,096. Source-Copying Exposure Regularization (SCER) (Ondov and Demner-Fushman, 2024) is employed to encourage simplification over source-copying.

• PLABA 5: The same model as PLABA 4, but fine-tuned without SCER.

# 5.1.3. Automatic Evaluation Results

Results for automatic, reference-based evaluation using various metrics are shown in Table 2 (note we omit SAMSA due to lack of poor correlation with manual judgments, see §5.1.5). Not surprisingly, fined-tuned systems did better in such metrics than zero-shot systems. However, this is not necessarily reflective of system quality, as there are many valid ways to adapt passages to plain language that may not be captured by the references, even with four-fold reference annotation.

<html><body><table><tr><td>Submission</td><td>SARI</td><td>SARI-hf</td><td>BLEU</td><td>RL</td><td>R1</td><td>R2</td></tr><tr><td>PLABA_1</td><td>35.90</td><td>39.00</td><td>28.95</td><td>46.75</td><td>53.25</td><td>28.63</td></tr><tr><td>PLABA_2*</td><td>36.91</td><td>39.96</td><td>36.30</td><td>51.88</td><td>57.62</td><td>34.44</td></tr><tr><td>PLABA_3*</td><td>23.31</td><td>23.31</td><td>5.30</td><td>9.21</td><td>11.07</td><td>4.81</td></tr><tr><td>PLABA_4</td><td>44.12</td><td>49.11</td><td>59.51</td><td>69.59</td><td>71.87</td><td>55.92</td></tr><tr><td>PLABA_5</td><td>42.65</td><td>49.58</td><td>61.93</td><td>71.23</td><td>73.39</td><td>58.20</td></tr><tr><td>BeeManc_1</td><td>40.58</td><td>42.97</td><td>42.05</td><td>56.45</td><td>60.20</td><td>39.75</td></tr><tr><td>BeeManc_2</td><td>41.98</td><td>49.15</td><td>66.89</td><td>74.16</td><td>75.74</td><td>62.03</td></tr><tr><td>BeeManc_3</td><td>42.12</td><td>49.92</td><td>68.19</td><td>75.33</td><td>76.91</td><td>63.15</td></tr><tr><td>BeeManc_4</td><td>36.88</td><td>39.80</td><td>36.38</td><td>53.08</td><td>59.53</td><td>35.11</td></tr><tr><td>BoschAI_1</td><td>44.69</td><td>49.03</td><td>58.50</td><td>68.98</td><td>71.81</td><td>54.98</td></tr><tr><td>BoschAI_2</td><td>43.80</td><td>49.58</td><td>61.36</td><td>70.22</td><td>72.59</td><td>56.88</td></tr><tr><td>BoschAI_3</td><td>32.84</td><td>35.94</td><td>19.97</td><td>36.44</td><td>44.79</td><td>20.51</td></tr><tr><td>MasonNLP_1</td><td>39.47</td><td>42.92</td><td>42.44</td><td>56.86</td><td>60.88</td><td>38.98</td></tr><tr><td>MasonNLP_2</td><td>39.97</td><td>43.47</td><td>43.49</td><td>57.16</td><td>60.95</td><td>41.90</td></tr><tr><td>MasonNLP_3</td><td>38.38</td><td>50.89</td><td>68.34</td><td>75.00</td><td>76.78</td><td>63.12</td></tr><tr><td>PT3M_1*</td><td>34.47</td><td>38.15</td><td>30.30</td><td>42.44</td><td>46.24</td><td>32.57</td></tr></table></body></html>

\*Automatically aligned document-level submission (see §4.1.3).   
Table 2: Automatic evaluation results. SARI-hf is the HuggingFace implementation of SARI. RL, R1, and R2 refer to RougeL, Rouge1, and Rouge2, respectively.

Table 3: Results of the first and second rounds of manual evaluation of Task 1 at TREC 2023.   

<html><body><table><tr><td></td><td colspan="5">Simplicity</td><td colspan="3">Accuracy</td><td rowspan="2">FIN</td></tr><tr><td>Submission</td><td>SEN</td><td>TRM</td><td>TAC</td><td>FLU</td><td>SIM</td><td>COM</td><td>FTH</td><td>ACC</td></tr><tr><td colspan="9">Round 1</td></tr><tr><td>PLABA_1</td><td>91.45</td><td>86.84</td><td>91.22</td><td>93.53</td><td>90.76</td><td>95.73</td><td>94.02</td><td>94.87</td><td>92.82</td></tr><tr><td>PLABA_2*</td><td>94.33</td><td>81.94</td><td>87.50</td><td>95.25</td><td>89.76</td><td>90.17</td><td>88.46</td><td>89.32</td><td>89.54</td></tr><tr><td>PLABA_3*</td><td>84.67</td><td>63.67</td><td>42.67</td><td>87.00</td><td>69.50</td><td>20.94</td><td>18.80</td><td>19.87</td><td>44.69</td></tr><tr><td>BeeManc_1</td><td>92.84</td><td>82.33</td><td>64.20</td><td>91.57</td><td>82.74</td><td>79.49</td><td>70.51</td><td>75.00</td><td>78.87</td></tr><tr><td>BoschAI_1</td><td>91.11</td><td>77.25</td><td>94.11</td><td>92.96</td><td>88.86</td><td>95.30</td><td>94.44</td><td>94.87</td><td>91.87</td></tr><tr><td>MasonNLP_1</td><td>91.63</td><td>91.74</td><td>88.26</td><td>93.49</td><td>91.28</td><td>94.44</td><td>90.60</td><td>92.52</td><td>91.90</td></tr><tr><td>PT3M_1*</td><td>38.19</td><td>34.38</td><td>21.99</td><td>18.17</td><td>28.18</td><td>16.24</td><td>8.97</td><td>12.61</td><td>20.40</td></tr><tr><td colspan="10">Round 2</td></tr><tr><td>PLABA_1†</td><td>83.14</td><td>87.30</td><td>96.88</td><td>98.73</td><td>91.51</td><td>98.72</td><td>96.58</td><td>97.65</td><td>94.58</td></tr><tr><td>PLABA_4</td><td>82.71</td><td>84.35</td><td>93.93</td><td>95.68</td><td>89.17</td><td>95.73</td><td>96.15</td><td>95.94</td><td>92.55</td></tr><tr><td>PLABA_5</td><td>78.67</td><td>76.92</td><td>90.33</td><td>97.09</td><td>85.75</td><td>97.01</td><td>96.58</td><td>96.80</td><td>91.27</td></tr><tr><td>BeeManc_4</td><td>79.91</td><td>74.02</td><td>92.49</td><td>97.58</td><td>86.00</td><td>96.58</td><td>95.30</td><td>95.94</td><td></td></tr><tr><td>BoschAI_2</td><td>79.49</td><td>76.81</td><td>90.33</td><td>98.83</td><td>86.36</td><td>95.30</td><td>97.01</td><td>96.15</td><td>90.97</td></tr><tr><td>MasonNLP_2</td><td>79.78</td><td>85.17</td><td></td><td>92.34</td><td>86.72</td><td>93.16</td><td></td><td></td><td>91.26</td></tr><tr><td>Manual_1</td><td></td><td></td><td>89.59</td><td></td><td>97.25</td><td></td><td>86.75</td><td>89.96</td><td>88.34</td></tr><tr><td></td><td>96.67</td><td>98.02</td><td>96.67 98.71</td><td>97.65 97.53</td><td>96.68</td><td>92.74</td><td>93.16</td><td>92.95</td><td>95.10</td></tr><tr><td>Manual_2</td><td>94.35 84.78</td><td>96.12</td><td></td><td>97.54</td><td>90.69</td><td>96.58</td><td>97.86</td><td>97.22</td><td>96.95</td></tr><tr><td>Manual_3</td><td></td><td>87.59</td><td>92.86</td><td>95.71</td><td>89.62</td><td>96.15</td><td>94.87</td><td>95.51</td><td>93.10</td></tr><tr><td>Manual_4</td><td>81.79</td><td>86.08</td><td>94.90</td><td></td><td></td><td>99.15</td><td>99.15</td><td>99.15</td><td>94.39</td></tr><tr><td>Manual_avg</td><td>89.40</td><td>91.95</td><td>95.79</td><td>97.11</td><td>93.56</td><td>96.16</td><td>96.26</td><td>96.21</td><td>94.88</td></tr></table></body></html>

\*Automatically aligned document-level submission (see §4.1.3). †Reevaluated by second round evaluators for comparison with round o

As further resources were available following the first round of manual evaluation, we invited teams to submit an additional run for a second round of manual evaluation. As only one team (BeeManc) did so, we manually evaluated the second priority runs for the other teams, except PT3M, who had only submitted one run. Resources also allowed us to include two additional baselines, both fine-tuned (PLABA 4 and PLABA 5), and to include the manually written reference adaptations for manual evaluation, creating a human performance baseline. Finally, since different annotators were used versus the first round, we included the top-performing system (PLABA 1) for re-evaluation as a point of reference. Note, however, that these results are not directly comparable to those in round 1 because of the change in annotators and timing (after the conference rather than before). Results of both rounds of manual evaluation are shown in Table 3 and visualized in Figure 2a. During the second round of evaluation, resources allowed for giving annotators overlapping assignments for three abstracts in order to estimate inter-annotator agreement (see Appendix G).

![](images/d47dbb345393351b439026111184e6c5f515cdc96ee8abfedc06c8cdd32fac46.jpg)  
Figure 2: Radar charts of manual evaluation results for Task 1 at (a) TREC 2023 and (b) TREC 2024.

# 5.1.5. Analysis of Metrics

To assess the utility of automated metrics, we compute their correlation with manual judgments. Correlations of all computed metrics are shown in Figure 3a. Scatterplots with regression lines for each metric are shown in Appendix H.

# 5.2. Task 1 at TREC 2024

Submissions to Task 1 at TREC 2024 were due September $2 0 ^ { \mathrm { t h } }$ , 2024 and preliminary results were announced November $1 0 ^ { \mathrm { t h } }$ , 2024. With the increase in interest from the previous year, we expected more submissions. We thus did not develop any baselines for the second offering of this task and instead focused on developing baselines for the newly introduced Task 2 (see §5.3). Eight teams participated in the second offering of Task 1, with 19 total submissions. Results of manual evaluation of all submitted systems, based on sentence-level outputs for all 400 test abstracts, are shown in Table 4, and visualized in Figure 2b.

Additionally, we investigate potential tradeoffs in system strengths by computing correlations among each pair of manual judgment axes (Fig. 3b).

![](images/2a068be62e04283149ec70e339cf337ff709968382405ca3e88e310943b27f63.jpg)  
Figure 3: In (a), correlation of automatic metrics with manually evaluated Simplicity (SIM) and Accuracy (ACC). For evaluating relationships between manual evaluation axes, pairwise scatterplots of the four manual evaluation axes are shown for (b) Task 1 and (c) Task 2, both at TREC 2024. Each point represents output for one abstract by one system, with values averages across sentences (Task 1) or terms (Task 2). Each scatterplot is labeled with its Pearson correlation value (r). Histograms on the diagonal show the distributions of scores for each of the axes.

<html><body><table><tr><td>Submission</td><td>ACC</td><td>COM</td><td>SIM</td><td>BRV</td><td>FIN</td></tr><tr><td>BU_1</td><td>88.31</td><td>84.47</td><td>77.92</td><td>52.40</td><td>75.78</td></tr><tr><td>CLAC_1</td><td>87.88</td><td>85.67</td><td>71.88</td><td>41.22</td><td>71.66</td></tr><tr><td>CLAC_2</td><td>88.18</td><td>86.03</td><td>71.18</td><td>41.55</td><td>71.74</td></tr><tr><td>CLAC_3</td><td>95.37</td><td>89.96</td><td>78.50</td><td>37.82</td><td>75.41</td></tr><tr><td>ntu_nlp_1</td><td>89.48</td><td>84.63</td><td>81.97</td><td>62.72</td><td>79.70</td></tr><tr><td>ntu_nlp_2</td><td>93.07</td><td>85.37</td><td>86.42</td><td>64.68</td><td>82.38</td></tr><tr><td>ntu_nlp_3</td><td>93.95</td><td>86.81</td><td>88.60</td><td>65.86</td><td>83.81</td></tr><tr><td>SIB_1</td><td>81.70</td><td>73.24</td><td>87.56</td><td>73.92</td><td>79.10</td></tr><tr><td>SIB_2</td><td>93.74</td><td>86.14</td><td>83.63</td><td>65.94</td><td>82.36</td></tr><tr><td>SIB_3</td><td>94.33</td><td>88.91</td><td>69.65</td><td>62.10</td><td>78.75</td></tr><tr><td>UAmsterdam_1</td><td>95.46</td><td>92.38</td><td>56.53</td><td>67.81</td><td>78.04</td></tr><tr><td>UAmsterdam_2</td><td>97.72</td><td>94.66</td><td>48.81</td><td>69.06</td><td>77.56</td></tr><tr><td>UM_1</td><td>74.40</td><td>77.25</td><td>70.44</td><td>59.03</td><td>70.28</td></tr><tr><td>UM_2</td><td>93.07</td><td>81.18</td><td>92.32</td><td>87.55</td><td>88.53</td></tr><tr><td>um_fhs_1</td><td>89.85</td><td>84.27</td><td>84.21</td><td>76.18</td><td>83.63</td></tr><tr><td>um_fhs_2</td><td>94.47</td><td>85.88</td><td>89.09</td><td>79.30</td><td>87.18</td></tr><tr><td>um_fhs_3</td><td>95.04</td><td>86.81</td><td>79.49</td><td>67.28</td><td>82.15</td></tr><tr><td>Yseop_1</td><td>91.17</td><td>87.00</td><td>76.54</td><td>65.33</td><td>80.01</td></tr><tr><td>Yseop_2</td><td>84.24</td><td>71.32</td><td>45.00</td><td>41.77</td><td>60.58</td></tr></table></body></html>

Table 4: Results of manual evaluation of Task 1 at TREC 2024.

Though individual judgments were made at the sentence level, this would be too fine-grained for correlation analysis since judgments are discrete (-1, 0, or 1). System-level aggregation, however, would leave too few points to see evidence of patterns. We thus aggregate scores at the abstract level for this analysis. As can be seen from the scatterplots and correlation values, accuracy is strongly related to completeness, but other pairs are not highly correlated. Interestingly, the lowest r value is for completeness versus brevity, which one might expect to be in tension. However, even this value is still slightly positive.

# 5.3. Task 2 at TREC 2024

Six teams participated in Task 2 at TREC 2024, with nine total submissions. Submissions were due September 20 $\mathrm { t h }$ , and preliminary results were announced November $1 0 ^ { \mathrm { t h } }$ , 2024. Team submissions are detailed in Appendix F. Each team was allowed three submissions, which they could rank in order of priority for manual evaluation. Tasks 2B and 2C were optional, though participation in 2C required participation in 2B. We also asked each team to provide a short description of each submission, as well as any base LLMs and training data used.

# 5.3.1. Baseline Systems

We developed baseline systems for the identification (Task 2A) and classification (Task 2B) sub-tasks within Task 2. This included three baselines for Task 2A:

• PLABA 2A 1: Uses MetaMapLite (Demner-Fushman et al., 2017) and the Unified Medical Language System (UMLS) (Lindberg et al., 1993) to identify expert terms. Two term frequency datasets from Kaggle— one derived from the Google Web Trillion Word Corpus Tatman (2020) and the other derived from BookCorpus and a 2019 dump of Wikipedia Cook (2020)—were used to filter out false positives.   
• PLABA 2A 2: DeBERTa Large (He et al., 2021) (435M parameters) was fine-tuned for the identification task by treating it as a named entity recognition problem.   
• PLABA 2A 3: Llama3 Instruct (8B parameters) (Dubey et al., 2024) was prompted for this task by providing the following instruction prompt immediately followed by the sentence to operate on: “Identify all nonconsumer biomedical terms in the user’s sentence using a comma-separated list. Generate no other text besides the list.”

We developed two baselines for Task 2B, which also required performing Task 2A as an intermediate step:

• PLABA 2B 1: BERT Large (340M parameters) (Devlin et al., 2018) was fine-tuned for a named entity recognition problem that combined the identification and classification sub-tasks into a single problem. • PLABA 2B 2: DeBERTa Large was fine-tuned for the same named entity recognition problem as CLS-BERT.

# 5.3.2. Evaluation Results

All results for Task 2 at TREC 2024, including automatic metrics for Tasks 2A and 2B and manual judgments for Task 2C, are shown in Table

<html><body><table><tr><td></td><td>Task 2A</td><td>Task 2B</td><td colspan="5">Task 2C (manual eval., 0-100)</td></tr><tr><td>Submission</td><td>(F1)</td><td>(F1)</td><td>ACC</td><td>COM</td><td>SIM</td><td>BRV</td><td>FIN</td></tr><tr><td>PLABA_2A_1</td><td>0.2487</td><td>-</td><td></td><td>1</td><td>-</td><td>1</td><td>1</td></tr><tr><td>PLABA_2A_2</td><td>0.5255</td><td>-</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>PLABA_2A_3</td><td>0.4085</td><td>1</td><td>1</td><td>-</td><td>1</td><td>1</td><td></td></tr><tr><td>PLABA_2B_1</td><td>0.3399</td><td>0.3413</td><td></td><td></td><td>1</td><td>-</td><td>1</td></tr><tr><td>PLABA_2B_2</td><td>0.4009</td><td>0.3363</td><td>1</td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>BU_1</td><td>0.0459</td><td>0.2868</td><td>80.56</td><td>82.79</td><td>95.40</td><td>87.43</td><td>86.55</td></tr><tr><td>CLAC_1</td><td>0.4410</td><td>0.3317</td><td>93.39</td><td>93.68</td><td>97.75</td><td>64.26</td><td>87.27</td></tr><tr><td>CLAC_2</td><td>0.3767</td><td>0.1865</td><td>95.57</td><td>95.17</td><td>98.54</td><td>58.98</td><td>87.06</td></tr><tr><td>IITH_1</td><td>0.1956</td><td>0.2759</td><td>-</td><td>1</td><td></td><td></td><td></td></tr><tr><td>UM_1</td><td>0.4787</td><td>0.3180</td><td></td><td></td><td>-</td><td>-</td><td></td></tr><tr><td>ntu_nlp_1</td><td>0.4885</td><td>0.3931</td><td>86.59</td><td>86.77</td><td>95.13</td><td>57.29</td><td>81.44</td></tr><tr><td>ntu_nlp_2</td><td>0.4431</td><td>0.3715</td><td>86.48</td><td>87.84</td><td>94.22</td><td>53.25</td><td>80.45</td></tr><tr><td>ntu_nlp_3</td><td>0.4518</td><td>0.3287</td><td></td><td>1</td><td>-</td><td>1</td><td></td></tr><tr><td>Yseop_1</td><td>0.5036</td><td>0.1854</td><td></td><td>-</td><td></td><td>-</td><td></td></tr></table></body></html>

Table 5: All results for Task 2 at TREC 2024, including automatic metrics for Tasks 2A and 2B and manual judgments for Task 2C. Missing values mean the submission did not participate in that sub-task. Task 2B F1 scores are macro-averaged over the five classes.

5. As for Task 1, we compute correlations among each pair of manual evaluation axes for Task 2C (Fig. 3c). Again, accuracy is strongly related to completeness; however, in this case, brevity and completeness exhibit a negative correlation, illustrating the tradeoff between outputs being complete and brief. Pairwise scatterplots and correlations broken down by replacement type are shown in Appendix J.

# 6. Discussion

Here we discuss key lessons that we hope will inform future iterations of this task and potentially other tasks in the Large Language Model era.

# 6.1. Bridging the Gap from Sentences to Documents

Though prior shared tasks have involved either sentence-level or document level plain language generation, the PLABA track is, to our knowledge, the first shared task that evaluated biomedical text simplifications at the sentence level while accounting for the context of the entire abstract. This posed unique challenges for test data generation, system submission, and evaluation. Document-level systems generally do not ensure sentencealigned output for rewriting tasks, which can complicate evaluation of finegrained details. In this shared task, automatically aligning system output to sentences was a reasonable compromise, with state-of-the-art alignment tools built for Machine Translation proving valuable (though in practice only one of the ten submissions, and two of the three baselines, utilized this option). Further, our autoregressive PLABA 1 baseline showcased how Large Language Models can be strategically prompted to include context while adapting individual sentences.

Still, challenges remain for both evaluation and generation. Due to its reliance on dynamic programming, our automatic alignment pipeline cannot account for transposition of sentences, which may be useful when rewriting a document for a lay audience. By choice, we also did not allow source sentences to be merged, as it is not yet clear how to standardize sentence-level evaluation with this possibility. Yet, merging sentences is also a potentially useful operation. Future work can explore evaluation methods that capture how well fine-grained semantic content is preserved across document-level adaptations.

Finally, though dropping sentences was allowed for this task, we found manual writers used it very rarely, and sentence-level submissions from teams did not use it at all. For document-level submissions that contain dropped sentences after being automatically aligned, we did not have an explicit evaluation metric for this operation. Future iterations of the task could either eliminate this possibility (requiring output for every sentence) or treat sentence dropping as a binary classification sub-task to capture performance.

# 6.2. Insufficiency of Automated Metrics

The main PLABA task is similar to the longstanding tasks of Machine Translation and Text Simplification. Machine Translation, however is fairly restricted semantically, whereas Plain Language Adaptation has much more freedom to rephrase, add content, and remove content. PLABA also deviates from more traditional forms of Text Simplification, which until recently have largely revolved around (1) lexical substitutions and (2) atomic operations on syntax, both of which preserve much of the dependency structure of a given sentence (Ondov et al., 2022). Whether automatic, reference-based metrics that have worked for the latter tasks would extend to PLABA was thus an open question. We find that n-gram based metrics generally correlated poorly with manual judgments of both simplicity and accuracy of content, with metrics specifically designed for the task of Text Simplification (SARI and SAMSA) notably having the worst correlations. This is despite including four unique, manually written reference adaptations for each source sentence. BERTScore had much higher correlation with both simplicity and accuracy of content, concurring with findings of Alva-Manchego et al. (2021).

Still, the discrepancies between system rankings by automatic metrics versus manual judgments highlight that there are many possible ways to rewrite sentences for this task, and these may not captured even after considerable effort to write several high-quality versions. Further, small changes in similarity to a reference could have outsize influence on the message a healthcare consumer takes from an adaptation, and further work is needed to assess how well any automated metric captures possible harms. Future work can also focus on developing new ways to automatically asses how easy to read output is and how well it captures the factual content of the original, without relying on word or n-gram similarity.

# 6.3. Factuality and Hallucinations

A consequence of using pretrained language models for transfer learning and zero-shot applications is the mismatch between their explicit training objective (maximizing the likelihood of the next word given a corpus) and more specialized downstream tasks, which often implicitly require knowledge or reasoning. This leads to the well-known problem of “hallucination,” or the output of cogent but unfounded text. In biomedical text simplification, the shift from the rule-based era to the neural era represented a marked shift in error profiles, from chiefly errors of grammatically (despite factual accuracy) to chiefly errors of factual accuracy (despite often perfect fluency) (Ondov et al., 2022). This phenomenon has been exacerbated by LLMs, which can now fabricate entire abstracts, complete with internally coherent study details and imagined citations.

In this shared task, however, we find that, generally, the most fluent systems are also rated to be highly factual. In fact, when blindly manually evaluated alongside reference adaptations (which were manually written by biomedical experts) for Task 1 at TREC 2023, the top-performing PLABA 1 system even exceed the average manual score for completeness (COM), faithfulness (FTH), and their combination, accuracy (ACC) (see §5.1). Top systems from Task 1 at TREC 2024 pushed accuracy measures even higher (though we caution against direct comparisons because of the change in annotators across years). This suggests that state-of-the-art language models, guided by the context of original abstracts, can produce very accurate information on detailed biomedical topics.

Still, this shared task also revealed that, even when generation is narrowly focused on one source line, large, convincing hallucinations can still occur (as described in Appendix I). These may be all the more insidious if users learn to trust systems that are largely accurate. Health information provided to consumers can be highly actionable, and there is thus a high potential for harm even in rare edge cases. We must thus be vigilant in evaluating and deploying systems that provide such information. Future work should investigate more rigorous ways to detect and mitigate hallucinations and automatically assess factuality to ensure these kinds of errors do not go undetected.

# 7. Conclusion

Two years of the Plain Language Adaptation of Biomedical Abstracts (PLABA) track challenged teams to rewrite biomedical abstracts for the general public, and to identify and replace expert terms in appropriate ways. The track drew a diverse group of teams from around the world, with some attending the conference in person and some taking advantage of the virtual option. Submissions showcased a wide variety of systems, with language models of many types and sizes, different prompting strategies, and custom training pipelines. The track also featured baseline systems running the gamut from ruled-based systems for expert term identification to state-of-theart instruction-following pretrained transformers for end-to-end rewriting of abstracts. Though systems had a wide range of results, especially in manual evaluation, the best-performing systems neared or surpassed human levels of factual accuracy while performing near-human levels of simplification. These systems in their current form may already be able to help consumers interpret the latest biomedical research for better healthcare engagement and literacy. Accurate consumer-oriented biomedical texts with each line attributable to a trustworthy, peer-reviewed abstract could also provide a valuable intermediate resource for abstractive consumer question answering or Retrieval Augmented Generation (Lewis et al., 2020b). Still, we urge caution when deploying such systems, as even the best systems from this track still made errors, and even minor errors may have the potential for harm in the biomedical setting. Erring on the side of less simple, but more accurate systems, using our evaluations as a guide, will allow researchers to follow a “progressive caution” approach (Goodman and Miller, 2021). We hope the lessons from this task and the resulting systems will inform and inspire future work in the domain of consumer-focused biomedical text generation.