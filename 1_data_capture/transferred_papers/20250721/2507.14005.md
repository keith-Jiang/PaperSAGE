# On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes

Mathieu Godbout   
Department of Computer Science Université Laval, Canada   
mathieu.godbout.3@ulaval.ca Audrey Durand∗   
Department of Computer Science Université Laval, Canada   
audrey.durand@ift.ulaval.ca

# Abstract

Recent work has shown that dynamic programming (DP) methods for finding static CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when based on the dual formulation, yet the root cause for the failure has remained unclear. We expand on these findings by shifting focus from policy optimization to the seemingly simpler task of policy evaluation. We show that evaluating the static CVaR of a given policy can be framed as two distinct minimization problems. For their solutions to match, a set of “risk-assignment consistency constraints” must be satisfied, and we demonstrate that the intersection of the constraints being empty is the source of previously observed evaluation errors. Quantifying the evaluation error as the CVaR evaluation gap, we then demonstrate that the issues observed when optimizing over the dual-based CVaR DP are explained by the returned policy having a non-zero CVaR evaluation gap. We then leverage our proposed risk-assignment perspective to prove that the search for a single, uniformly optimal policy via on the dual CVaR decomposition is fundamentally limited, identifying an MDP where no single policy can be optimal across all initial risk levels.

# 1 Introduction

The goal of reinforcement learning (RL) [Sutton and Barto, 2018] is to learn (sequential) decisionmaking policies such as to maximize some outcome (return) in a given environment, typically modeled as a Markov decision process (MDP). This is usually approached from the objective of maximizing the expected return, which has led to impressive successes in videogames [Vinyals et al., 2019], board games [Silver et al., 2018], and content recommendation [Li et al., 2010]. However, in safety-critical domains like healthcare, autonomous driving, or financial planning, some erroneous actions may lead to disastrous consequences. For instance, in the task of identifying the shortest path to an organ for surgery, paths at high risk of endangering the patient (e.g., as they are too close to an artery, a nerve, or a critical region of the brain), should be avoided [Baek et al., 2018]. Automation in this context and other safety-critical domains can only be achieved with safe decision-making policies [Gottesman et al., 2019]. This can be achieved by optimizing a risk-averse objective instead of simply maximizing the expected return [Artzner et al., 1999]. In particular, conditional value-at-risk (CVaR), which is considered a gold standard risk measure in banking regulations [Basel Committee on Banking Supervision, 2019], has received a lot of focus in risk-averse RL [Prashanth et al., 2022].

More specifically, the static CVaR evaluation consists of computing the CVaR of a policy’s cumulative random return. Unfortunately, optimizing a policy w.r.t. the static CVaR objective in MDPs has proven to be quite challenging since CVaR suffers from the time inconsistency issue [Pflug and Pichler, 2016, Gagne and Dayan, 2021] and optimal policies may be history-dependent [Shapiro et al., 2014]. To tackle both these problems, prior work has considered dynamic programs (DPs) applied on augmented state spaces in both the primal and dual representations of risk measures. Working under the primal representation, states are augmented by keeping track of the running cumulative return [Bäuerle and Ott, 2011, Boda et al., 2004, Chow and Ghavamzadeh, 2014, Chow et al., 2018]. However, these methods are considered practically inefficient since they require computing the value function on an unbounded continuous state space [Chapman et al., 2021, Chow et al., 2015, Li et al., 2022]. The dual representation has therefore been identified as a promising direction, where sequential decomposition of risk measures can also be leveraged [Chow et al., 2015, Pflug and Pichler, 2016]. States here are augmented by keeping track of the current risk-level (between 0 and 1). Although the resulting augmented state space is still continuous, discretization can be applied efficiently since risk-levels are bounded [Chow et al., 2015, Li et al., 2022]. Hence, Chow et al. [2015] proposed a Value Iteration (VI) procedure for CVaR in the dual representation, which served as the basis for many later developments in the field [Chapman et al., 2019, 2021, Chow et al., 2015, Ding and Feinberg, 2022a,b, Stanko and Macek, 2019, Rigter et al., 2021], until Hau et al. [2023] later showed counterexample MDPs where this procedure fails to recover the optimal value function and policy. However, from these few empirical results alone, it is not possible to understand what makes CVaR VI fail, and therefore how, or even $i f ,$ it can be fixed.

Contributions The main goal of this work is to diagnose the root causes of recently discovered failures in the static dual CVaR DP decomposition. To this end, we first establish a formal analysis framework that recasts the static CVaR evaluation and its DP decomposition as two distinct optimization problems over perturbations. Leveraging this perspective, we identify a set of risk-assignment consistency constraints that must be satisfied for the DP’s evaluation of a policy to be accurate. We show that the previously observed cases where the DP decomposition returned a suboptimal policy are explained by these constraints being inconsistent for the returned policy. We then leverage this constraint-based view to present an MDP where the action constraints required for optimality at different initial risk levels are irreconcilable. This proves that no single risk-dependent policy can be uniformly optimal, revealing a fundamental limitation of the pursuit of a single, universally optimal policy for all risk levels via the dual decomposition, independent of the DP algorithm used. To increase readability, we postpone most of the proofs to the appendix.

# 2 Static Risk-Averse Reinforcement Learning

Following Puterman [2014], let us define a finite Markov decision process (MDP) by a tuple $( S , \mathcal { A } , P , \mathcal { R } , s _ { 0 } , \gamma )$ where $s$ is a finite state space, $\mathcal { A }$ is a finite action space, $P : \mathcal { S } \times \mathcal { A } \mapsto \Delta ( \mathcal { S } )$ is the transition function between states ( $\Delta$ denoting the probability simplex), $\mathcal { R } : \mathcal { S } \times \mathcal { A } \times \mathcal { S } \mapsto [ 0 , R _ { \operatorname* { m a x } } ]$ is the reward function, $s _ { 0 } \in \mathcal { S }$ is the initial state from which the process begins, and $\gamma \in [ 0 , 1 )$ is a discount factor. At each time step $t \in \mathbb { N } _ { 0 }$ , an agent performs an action $A _ { t } \in { \mathcal { A } }$ in the current state $S _ { t } \in \mathcal { S }$ according to some decision policy $\pi$ . This leads to a transition into the state $S _ { t + 1 }$ sampled from $P ( S _ { t } , \bar { A _ { t } } )$ , following which the agent receives the reward $R _ { t + 1 } = \mathscr { R } ( S _ { t } , A _ { t } , S _ { t + 1 } )$ . Throughout this paper, we will adopt the convention of using uppercase letters to distinguish objects subject to random realization, such as referring to the state $S _ { t } = s \in \mathcal { S }$ .

Remark 1 Imposing a deterministic reward function and initial state is done with minimal loss of generality. Any MDP with a stochastic reward function or initial state can be converted to an equivalent MDP with deterministic counterparts, provided the distributions are discrete. This transformation involves augmenting the state space, with the initial action having no effect on the state transition [Sutton and Barto, 2018].

Policies We consider policies as deterministic action-selection mechanisms. The most general form of policies is history-dependent policies $\pi _ { h } : { \mathcal { H } } \to A$ , where $\mathcal { H }$ is the set of histories defined as all previous states and actions seen prior to the action selection being made. Let $H _ { t } : = ( S _ { 0 } , A _ { 0 } , S _ { 1 } , A _ { 1 } , S _ { 2 } , . . . , S _ { t } )$ and $\mathcal { H } _ { t }$ respectively denote the current history and the set of possible histories at time $t$ . At time $t = 0$ , the set of histories is limited to the initial state, that is $\mathcal { \bar { H } } _ { 0 } : = \{ ( s _ { 0 } ) \}$ . For subsequent steps it is defined recursively as the combination of possible previous histories with the previous action and current state concatenated, that is $\mathcal { H } _ { t + 1 } : = \mathcal { H } _ { t } \times \mathcal { A } \times \mathcal { S }$ Allowing a slight abuse of notation, we extend transition and action selection dynamics to histories and define the probability of observing a given history $H _ { t }$ given policy $\pi _ { h }$ as

$$
P ^ { \pi _ { h } } ( H _ { t } ) : = \prod _ { \tau = 0 } ^ { t - 1 } P ( S _ { \tau + 1 } | S _ { \tau } , A _ { \tau } ) \mathbb { 1 } [ A _ { \tau } = \pi _ { h } ( H _ { \tau } ) ] ,
$$

where 1 denotes the indicator function.

Because of the degree of complexity brought upon by policies operating on the possibly immense set of histories, we are often interested in Markovian policies $\pi : S  A$ , a special case of historydependent policies where actions are selected only based on the current state. Hereafter, $\pi$ will be used to denote Markovian policies while $\pi _ { h }$ will consider history-dependent policies.

Standard objective The return associated with a history $H$ in the MDP is defined as the discounted sum of rewards

$$
\mathcal { R } _ { 0 : T } ^ { H } : = \sum _ { t = 0 } ^ { T - 1 } \gamma ^ { t } R _ { t + 1 } ,
$$

where $\gamma \in [ 0 , 1 )$ is the MDP’s discount factor and $R _ { t + 1 } = \mathscr { R } ( S _ { t } , A _ { t } , S _ { t + 1 } )$ . Given that trajectories in an MDP are generated using a random process, due to next state transitions being stochastic, we denote the random return of a trajectory generated by policy $\pi _ { h }$ as a random variable $Z ^ { \pi _ { h } }$ , taking value $\mathcal { R } _ { 0 : T } ^ { H }$ (Eq. 1) with $H \sim P ^ { \pi _ { h } }$ . The standard RL objective [Sutton and Barto, 2018] is to identify the optimal policy $\pi _ { h } ^ { \star }$ that maximizes the expected return over histories

$$
\pi _ { h } ^ { \star } \in \arg \operatorname* { m a x } _ { \pi _ { h } } \mathbb { E } [ Z ^ { \pi _ { h } } ] .
$$

It is known that Equation 2 can always be solved by a Markovian policy $\pi$ [Szepesvári, 2022]. Unfortunately, because the expectation only weighs random outcomes according to their likelihood without taking their value into account, the optimal policy according to this objective may lead to catastrophic outcomes over some trajectories [Mannor et al., 2007]. In critical applications where such trajectories should be avoided, one may optimize a risk-averse objective instead, where large negative outcomes are assigned higher importance.

# 2.1 Static CVaR for risk aversion

Let $Z \in \mathbb { R }$ denote a bounded variable on a probability space $( \Omega , { \mathcal { F } } , \mathbb { P } )$ , with cumulative distribution function $F _ { Z } ( z ) = \mathbb { P } [ Z \leq z ]$ for some threshold $z \in \mathbb { R }$ . Denote the Value-at-Risk (VaR) at risk-level $\alpha \in [ 0 , 1 ]$ as $\operatorname { V a R } _ { \alpha } [ { \dot { Z } } ] : = \operatorname* { i n } \ln \left\{ z \mid F _ { Z } ( z ) \geq \alpha \right\}$ . Assuming that $Z$ represents a payoff that should be maximized, the conditional-value-at-risk (CVaR) [Rockafellar and Uryasev, 2000, Föllmer and Schied, 2016] at risk level $\alpha$ is given by

$$
\operatorname { C V a R } _ { \alpha } \left[ Z \right] : = \frac { 1 } { \alpha } \int _ { 0 } ^ { \alpha } \operatorname { V a R } _ { \beta } ( Z ) d \beta = \operatorname* { i n f } _ { \xi \in \Xi _ { \alpha } ( \mathbb { P } ) } \mathbb { E } _ { \xi } \left[ Z \right] ,
$$

where $\begin{array} { r } { \Xi _ { \alpha } \left( \mathbb { P } \right) : = \left\{ \xi : \omega \mapsto \left[ 0 , \frac { 1 } { \alpha } \right] \ \Big \vert \ \int _ { \omega \in \Omega } \xi ( \omega ) \mathbb { P } ( \omega ) d \omega = 1 \right\} } \end{array}$ defines the $\mathrm { C V a R } _ { \alpha }$ risk envelope around distribution $\mathbb { P }$ and $\mathbb { E } _ { \xi } [ Z ]$ is the $\xi$ -reweighed expectation of $Z$ . If $Z$ has a continuous distribution, it is well known that we have $\mathrm { C V a R } _ { \alpha } [ Z ] = \mathbb { E } \left[ Z | Z \leq \mathrm { V a R } _ { \alpha } [ Z ] \right] ,$ , which can be interpreted as the expected value of the worst $\alpha$ outcomes of $Z$ . Note that CVaR is monotonically increasing in $\alpha$ with edge cases representing $\mathrm { C V a R } _ { 0 } \left[ Z \right] = \mathrm { e s s i n f } \left[ Z \right]$ and $\mathrm { C V a R } _ { 1 } \left[ Z \right] = \mathbb { E } [ Z ]$ .

The dual formulation in Equation 3 shows that the CVaR can be expressed as an optimization problem, where the objective is to find perturbations $\xi$ applied to the stochastic generative process of variable $Z$ such as to minimize its expectation. From the definition of the risk envelope $\Xi _ { \alpha } \left( \mathbb { P } \right)$ , we can observe that the perturbations enjoy two interesting properties. First, because $\xi$ represents multiplicative interventions on an event’s likelihood, it only affects events with nonzero probability. Also, because the largest magnitude of the perturbation on an event is $\textstyle { \frac { 1 } { \alpha } }$ , one can view $\textstyle { \frac { \hat { \operatorname { I } } } { \alpha } }$ as a perturbation budget which is naturally minimal at $\alpha = 1$ and increases as $\alpha$ decreases to 0, simultaneously recovering the monotonically increasing property of CVaR and its edge cases.

CVaR-RL objective Recalling that the random return of policy $Z ^ { \pi _ { h } }$ is a random variable, one can therefore define the static CVaR of a policy as

$$
\mathrm { C V a R } _ { \alpha } \left[ Z ^ { \pi _ { h } } \right] : = \operatorname* { m i n } _ { \xi \in \Xi _ { \alpha } ( P ^ { \pi _ { h } } ) } \sum _ { H \in \mathcal { H } _ { T } } P ^ { \pi _ { h } } ( H ) \xi ( H ) \mathcal { R } _ { 0 : T } ^ { H } ,
$$

where we shall hereafter refer to $\xi$ as history perturbations. By emphasizing negative outcomes, Equation 4 naturally yields the CVaR-RL risk-averse objective [Tamar et al., 2015]

$$
\pi _ { h } ^ { \star } \in \mathop { \mathrm { a r g ~ m a x } } _ { \pi _ { h } } \mathrm { C V a R } _ { \alpha } \left[ Z ^ { \pi _ { h } } \right] ,
$$

where one aims to instead find a policy maximizing the $\mathrm { C V a R } _ { \alpha }$ of its random return. Because $\mathrm { C V a R } _ { \alpha } [ Z ^ { \pi _ { h } } ]$ can be intuitively interpreted as the expectation of the worst $\alpha$ trajectories when following $\pi _ { h }$ , optimizing the CVaR-RL objective should yield policies less prone to catastrophic outcomes than the standard RL objective (Eq. 2), with a lower $\alpha$ leading to increased cautiousness.

# 2.2 CVaR-RL dynamic decomposition

Trajectory-level computation of static CVaR (Eq. 4) is impractical because it requires computing $P ^ { \pi _ { h } }$ for all trajectories, which can be prohibitive for large state and action spaces. Fortunately, the CVaR decomposition Theorem [Pflug and Pichler, 2016, Chow et al., 2015] grants a recipe for expressing the evaluation at state-level.

Theorem 2 (CVaR decomposition, Thm. 2 from Chow et al. [2015]) For any time step $t \geq 0$ , denote by $Z _ { t : T } ^ { \pi _ { h } }$ the return from time $t + 1$ onward under history-dependent policy $\pi _ { h }$ . Given current history $H _ { t }$ , the $C V a R _ { \alpha }$ of $Z _ { t : T } ^ { \pi _ { h } }$ obeys the following decomposition:

$$
\mathrm { C V a R } _ { \alpha } [ Z _ { t : T } ^ { \pi _ { h } } | H _ { t } ] = \operatorname* { m i n } _ { \tilde { \xi } \in \Xi _ { \alpha } ( P ( \cdot | S _ { t } , A _ { t } ) ) } \sum _ { s ^ { \prime } \in S } P ( s ^ { \prime } | S _ { t } , A _ { t } ) \tilde { \xi } ( s ^ { \prime } ) \mathrm { C V a R } _ { \alpha \cdot \tilde { \xi } ( s ^ { \prime } ) } [ Z _ { t : T } ^ { \pi _ { h } } | H ^ { \prime }  ] ,
$$

where $\tilde { \xi }$ is a perturbation over next state transitions, action $A _ { t }$ is given by policy $\pi _ { h } ( H _ { t } )$ , and $H ^ { \prime } = H _ { t } \cup ( A _ { t } , s ^ { \prime } )$ is a possible history realization at time $t + 1$ .

Remark 3 We distinguish perturbations over next states $( \tilde { \xi } )$ from perturbations over histories $( \xi )$ . While both perturbations impact the sampling of events, $\tilde { \xi }$ also dictates the risk-level evolution. This distinction is crucial and will be a core component of our analysis.

Risk-dependent policies Theorem 2 shows that the $\mathrm { C V a R } _ { \alpha }$ at any given time $t$ can be expressed as combination of $\mathrm { C V a R } _ { \alpha ^ { \prime } }$ values of possible next states $s ^ { \prime }$ for updated risk-levels $\alpha ^ { \prime } = \alpha \cdot \xi ( \bar { s } ^ { \prime } )$ at time $t + 1$ . The running risk-level therefore contains all the information necessary to compute the CVaR of a history-dependent policy $\pi _ { h }$ . This motivated Chow et al. [2015] to introduce the risk-augmented state space $\tilde { S } : S \times [ 0 , 1 ]$ , defined for any state $s \in \mathcal S$ and risk-level $y \in [ 0 , 1 ]$ , and the corresponding risk-dependent Markovian policies on the augmented state space $\tilde { \pi } : \dot { S } \times [ 0 , 1 ]  A$ . Chow et al. [2015] suggested that operating over $\tilde { s }$ would suffice to retrieve the optimal history-dependent policy and evaluate its corresponding static CVaR through a value function mimicking the mechanism of Theorem 2.

Definition 4 (Risk-dependent-policy value function) The value function $v ^ { \tilde { \pi } } ( s , y )$ of any riskdependent policy $\tilde { \pi }$ is the solution to

$$
v _ { t + 1 } ^ { \tilde { \pi } } ( s , y ) = \operatorname* { m i n } _ { \tilde { \xi } \in \Xi _ { y } ( P ( \cdot | s , a ) ) } \sum _ { s ^ { \prime } \in \mathcal { S } } P ( s ^ { \prime } | s , a ) \tilde { \xi } ( s ^ { \prime } ) \left[ \mathcal { R } ( s , a , s ^ { \prime } ) + \gamma v _ { t } ^ { \tilde { \pi } } ( s ^ { \prime } , y ^ { \prime } ) \right]
$$

where $a = \tilde { \pi } ( s , y )$ is the action selected by the policy, $y ^ { \prime } = y \cdot \tilde { \xi } ( s ^ { \prime } )$ is the subsequent risk-level, and $\pmb { v } _ { 0 } ^ { \tilde { \pi } } ( s , y ) = 0$ for all states $s \in \mathcal S$ and risk-levels $y \in [ 0 , 1 ]$ . We let $\tilde { \pmb { v } } ^ { \tilde { \pi } } : = \pmb { v } _ { T } ^ { \tilde { \pi } }$ .

Crucially, any risk-dependent policy $\tilde { \pi }$ induces a corresponding history-dependent policy $\tilde { \pi } _ { h } ^ { \alpha }$ for a given initial risk-level $Y _ { 0 } = \alpha$ . At time $t$ , given the risk-augmented state $( S _ { t } , Y _ { t } )$ and the selected action $A _ { t } = \tilde { \pi } ( S _ { t } , Y _ { t } )$ , the subsequent risk-level is updated to $Y _ { t + 1 } = Y _ { t } { \cdot } \tilde { \xi } ^ { \star } ( S _ { t + 1 } | S _ { t } , Y _ { t } , A _ { t } )$ , where the optimal perturbation $\tilde { \xi } ^ { \star }$ denotes the solution to the value function $v ^ { \tilde { \pi } } ( S _ { t } , Y _ { t } )$ . Repeating this process $t$ times allows one to compute the action $\tilde { \pi } _ { h } ^ { \alpha } ( H _ { t } )$ for any history $H _ { t }$ . In the remainder of this paper, we will slightly abuse terminology and directly refer to a risk-dependent policy’s static CVaR to represent the static CVaR of its history-dependent counterpart.

In light of this correspondence between risk-dependent and history-dependent policies, Chow et al. [2015] proposed a Value Iteration algorithm, which we will refer to as CVaR VI, to find the riskdependent policy with the optimal value function $\tilde { \pi } ^ { \star } \in \arg \operatorname* { m a x } _ { \tilde { \pi } } { \pmb v } ^ { \tilde { \pi } } ( s _ { 0 } , \alpha )$ . Tentative proofs [Chow et al., 2015, Li et al., 2022] claimed $\tilde { \pi } ^ { \star }$ represented a risk-dependent version of the CVaR-optimal history-dependent policy $\pi _ { h } ^ { \star }$ , hence presenting a dynamic program decomposition of the CVaR-RL objective (Eq. 5). The optimality of the policy returned by CVaR VI was however refuted by Hau et al. [2023], who presented a counterexample MDP where the algorithm returns a suboptimal policy.

CVaR evaluation gap In this work, we aim to explain why CVaR VI fails at a more fundamental level. The root cause we identify is the discrepancy between the value function of a risk-dependent policy and its corresponding static $\mathrm { C V a R }$ , which we formally define as the CVaR evaluation gap

$$
\pmb { v } ^ { \tilde { \pi } } ( s _ { 0 } , \alpha ) - \mathrm { C V a R } _ { \alpha } \left[ Z ^ { \tilde { \pi } _ { h } ^ { \alpha } } \right] .
$$

A positive gap indicates that the value function of the risk-dependent policy overestimates its historydependent counterpart’s true CVaR.

# 3 An Explicit Mapping from the Value Function to the Static CVaR

To diagnose the source of the CVaR evaluation gap (Eq. 7), we first formalize the relationship between the value function of a risk-dependent policy (Eq. 6) at the initial state $( s _ { 0 } , \alpha )$ and its corresponding static CVaR. We find that both problems can be cast as distinct, but closely related, perturbation optimization problems. Although they operate over different optimization spaces, respectively statelevel perturbations $\tilde { \xi }$ and history-level perturbations $\xi$ , we can derive a formal mapping from one problem to the other. Exploiting the mapping’s properties, we then establish that the value function of a risk-dependent policy constitutes an upper bound on the static CVaR of the policy.

While the static CVaR evaluation (Eq. 4) is inherently defined as a minimization problem, evaluating the value function of a risk-dependent policy (Eq. 6) requires $T$ recursive calls, hindering its interpretation as a well-defined minimization problem. In order to make explicit the optimization problem behind the latter, we first define the value function of a risk-dependent policy under a fixed set of state-level perturbations. For a fixed risk-dependent policy $\tilde { \pi }$ , let $\tilde { \xi }$ denote a complete specification of state-level perturbations such that $\tilde { \pmb { \xi } } ( \cdot | s , \bar { y } , a ) \in \bar { \Xi } _ { y } \left( \bar { P } ( \cdot | s , a ) \right)$ for all states $s \in { \mathcal { S } }$ , risk-levels $y \in [ 0 , 1 ]$ , and actions $a = \tilde { \pi } ( s , y )$ . We further define $\tilde { \Xi } ( \tilde { \pi } )$ as the set of all such valid state-level perturbations, highlighting the dependence on $\tilde { \pi }$ .

Definition 5 (Policy-perturbation value function) The value function of a risk-dependent policy $\tilde { \pi }$ under state-level perturbations $\tilde { \xi } \in \tilde { \Xi } ( \tilde { \pi } )$ is the solution to:

$$
\boldsymbol { v } _ { t + 1 } ^ { \tilde { \pi } , \tilde { \xi } } ( s , y ) = \sum _ { s ^ { \prime } \in \mathcal { S } } P ( s ^ { \prime } | s , a ) \tilde { \xi } ( s ^ { \prime } | s , y , a ) \left[ R ( s , a , s ^ { \prime } ) + \gamma \boldsymbol { v } _ { t } ^ { \tilde { \pi } , \tilde { \xi } } ( s ^ { \prime } , y ^ { \prime } ) \right] ,
$$

where we used action $a = \tilde { \pi } ( s , y )$ , updated risk-level $y ^ { \prime } = y \cdot \tilde { \xi } ( s ^ { \prime } | s , y , a )$ , and ${ \pmb v } _ { 0 } ^ { \tilde { \pi } , \tilde { \pmb { \xi } } } ( s , y ) = 0$ for all states $s \in \mathcal S$ and risk-levels $y \in [ 0 , 1 ]$ . We let $v ^ { \tilde { \pi } , \tilde { \xi } } ( s , y ) : = v _ { T } ^ { \tilde { \pi } , \tilde { \xi } } ( s , y )$ .

This definition differs from the one in Equation 6 because it concerns fixed state-level perturbations $\tilde { \xi }$ instead of computing them recursively at every iteration. The value function of a risk-dependent policy $\tilde { \pi }$ can now be seen as finding the best possible perturbation $\tilde { \xi } \in \tilde { \Xi } ( \tilde { \pi } )$ to minimize this value.

Lemma 6 (Value function evaluation) The value function evaluation of a risk-dependent policy $\tilde { \pi }$ (Eq. 6) is equivalent to solving

$$
{ \pmb v } ^ { \tilde { \pi } } ( s , y ) = \operatorname* { m i n } _ { { \tilde { \xi } } \in { \tilde { \Xi } } ( \tilde { \pi } ) } { \pmb v } ^ { \tilde { \pi } , \tilde { \xi } } ( s , y ) ,
$$

where the above holds for all state-risk-level pairs $( s , y )$ simultaneously, meaning a single state-level perturbation policy $\tilde { \xi } ^ { \star }$ is optimal for all $( s , y )$ .

We now have two distinct optimization problems: the static evaluation over history perturbations $\xi$ (Eq. 4) and the value function evaluation over state-level perturbation policies $\tilde { \xi }$ (Eq. 9). The two problems are in fact intimately connected. That is, any state-level perturbation $\tilde { \xi }$ can be mapped to a corresponding history-level perturbation $\xi$ by taking the product of state-level perturbations along each history. For an initial risk-level $\alpha$ and history $H \in \mathcal { H } _ { T }$ , we define this mapping as

$$
\zeta _ { \alpha } ^ { \tilde { \pmb \xi } } ( H ) : = \prod _ { t = 0 } ^ { T - 1 } \tilde { \xi } ( S _ { t + 1 } | S _ { t } , Y _ { t } , A _ { t } ) ,
$$

where the risk levels $Y _ { t }$ are incremented following $\tilde { \xi }$ and starting from $Y _ { 0 } = \alpha$ . We now show that the mapping $\zeta _ { \alpha } ^ { \tilde { \pmb { \xi } } }$ is a valid history perturbation, and that it also recovers the value function for $\tilde { \pi }$ at the history-level.

Proposition 7 (State-level perturbation evaluation correspondence) For any risk-dependent policy $\tilde { \pi }$ , initial risk-level $\alpha$ , and state-level perturbation $\tilde { \xi } \in \tilde { \Xi } ( \tilde { \pi } )$ , the mapped perturbation $\zeta _ { \alpha } ^ { \tilde { \pmb { \xi } } }$ is $a$ valid history perturbation, that is $\zeta _ { \alpha } ^ { \tilde { \pmb { \xi } } } \in \Xi _ { \alpha } \left( P ^ { \tilde { \pi } _ { h } ^ { \alpha } } \right) )$ , for which we have

$$
\sum _ { H \in \mathcal { H } _ { T } } P ^ { \tilde { \pi } _ { h } ^ { \alpha } } ( H ) \zeta _ { \alpha } ^ { \tilde { \xi } } ( H ) \mathcal { R } _ { 0 : T } ^ { H } = v ^ { \tilde { \pi } , \tilde { \xi } } ( s _ { 0 } , \alpha ) .
$$

Because Proposition 7 applies to any state-level perturbation $\tilde { \xi } \in \tilde { \Xi } ( \tilde { \pi } )$ , in particular it applies to the optimal $\begin{array} { r } { \tilde { \pmb { \xi } } ^ { \star } \in \arg \operatorname* { m i n } _ { \tilde { \pmb { \xi } } \in \tilde { \Xi } ( \tilde { \pi } ) } { \pmb v } ^ { \tilde { \pi } , \tilde { \pmb { \xi } } } ( s _ { 0 } , \alpha ) } \end{array}$ . It directly follows that the value function evaluation (Lemma 9) is always an upper-bound to the static CVaR formulation over histories (Eq. 4).

Corollary 8 (Static CVaR upper-bound) For any risk-dependent policy $\tilde { \pi }$ and initial risk-level $\alpha$ , we have

$$
\mathrm { C V a R } _ { \alpha } \left[ Z ^ { \tilde { \pi } _ { h } ^ { \alpha } } \right] \leq v ^ { \tilde { \pi } } ( s _ { 0 } , \alpha ) .
$$

As a result of Corollary 8, the CVaR evaluation gap (Eq. 7) is non-zero if and only if the optimal history perturbation $\xi ^ { \star }$ is not in the image of the mapping $\zeta _ { \alpha }$ . That is, if the best global (history-level) perturbation cannot be decomposed into a sequence of valid local (state-level) perturbations, the value function evaluation will return an erroneous estimation of the policy’s static CVaR.

# 4 Characterizing the CVaR Evaluation Gap

We previously established that the value function of a risk-dependent policy provides an upper bound on its true static CVaR. In this section, we investigate the exact conditions under which the upper bound fails to be tight, leading to a CVaR evaluation gap. We show that the gap emerges when the optimal history-level perturbations are not realizable at the state level, a property that we formalize through a set of consistency constraints.

Definition 9 (Realizable trajectory perturbation) For a given risk-dependent policy $\tilde { \pi }$ and initial risk-level $\alpha \in [ 0 , 1 ]$ , a trajectory perturbation $\xi \in \vec { \Xi } _ { \alpha } \left( P ^ { \tilde { \pi } _ { h } ^ { \alpha } } \right)$ is realizable if there exists a state-level perturbation $\tilde { \xi } \in \tilde { \Xi } ( \tilde { \pi } )$ such that $\zeta _ { \alpha } ^ { \tilde { \pmb { \xi } } } = \xi$ .

The existence of such state-level perturbations $\tilde { \xi }$ hinges on our ability to define a sequence of intermediate risk levels $Y _ { t }$ that are mutually consistent for all histories. Recall that $\textstyle { \mathcal { H } } : = { \bar { \bigcup } } _ { t = 1 } ^ { T } { \mathcal { H } } _ { t }$ is the set off all possible histories of length $| H | \leq T$ and define $H _ { 0 : k } : = ( S _ { 0 } , A _ { 0 } , \ldots , S _ { k } ) \in \bar { \mathcal { H } } _ { k } ^ { - }$ as the $k$ -length subsequence of a given history $H$ . We formalize the mutual consistency notion by defining a risk-level assignment $\mathcal { V } : \mathcal { H }  [ 0 , 1 ]$ that maps any history $H$ to a risk-level $Y _ { | H | }$ . A risk-level assignment $y$ is consistent with respect to a trajectory perturbation $\xi$ if it enforces the correct total perturbation on all histories, while also respecting all stepwise constraints on the CVaR risk envelope and maintaining the correct sampled action from the risk-dependent policy. We will refer to these sets of constraints as the risk-assignment consistency constraints.

Definition 10 (Risk assignment consistency constraints) For a given risk-dependent policy $\tilde { \pi }$ , initial risk-level $\alpha \in [ 0 , 1 ]$ , and history perturbations $\xi \in \Xi _ { \alpha } \left( P ^ { \tilde { \pi } _ { h } ^ { \alpha } } \right)$ , a risk-level assignment $y$ is consistent if it satisfies the following for all histories $H \in \mathcal { H } _ { T }$ with $\ P ^ { \tilde { \pi } _ { h } ^ { \alpha } } ( H ) > 0$ :

1. Risk propagation: The assignment must propagate risk according to $\xi$ , that is $\mathcal { V } ( H _ { 0 : 0 } ) = \alpha$ and $\mathcal { V } ( H ) = \alpha \cdot \xi ( H )$ .

2. State-level risk envelope: For $t \in \{ 0 , \ldots , T - 1 \}$ , the risk envelope constraint over states must be respected for all possible states:

$$
\sum _ { s ^ { \prime } \in S } { P ( s ^ { \prime } | S _ { t } , A _ { t } ) \frac { \mathcal { V } ( H _ { 0 : t } \cup ( A _ { t } , s ^ { \prime } ) ) } { \mathcal { V } ( H _ { 0 : t } ) } } = 1 .
$$

3. Action-selection consistency: For $t \in \{ 0 , \ldots , T - 1 \}$ , the actions taken in the history must match the risk-dependent policy’s output for the assigned risk level:

$$
\tilde { \pi } ( S _ { t } , \mathcal { V } ( H _ { 0 : t } ) ) = A _ { t } .
$$

We are now prepared to formally connect the risk-assignment consistency constraints with the realizability property introduced earlier.

Lemma 11 (Consistency is necessary and sufficient for realizability) For any risk-dependent policy $\tilde { \pi }$ and initial risk-level $\alpha \in [ 0 , 1 ]$ , history perturbations $\xi \in \Xi _ { \alpha } \left( P ^ { \tilde { \pi } _ { h } ^ { \alpha } } \right)$ are realizable $i f$ and only if there exists a consistent risk-level assignment $y$ such that all risk-assignment consistency constraints (Def. 10) hold simultaneously.

The difficulty in satisfying the risk-assignment consistency constraints (Def. 10) lies in finding an assignment $y$ that satisfies all three constraint sets simultaneously. While it is clear that each constraint set can be satisfied in isolation, their intersection may be empty. This tension is the fundamental source of the CVaR evaluation gap (Eq. 7), which we formalize in the following theorem.

Theorem 12 (Conditions for CVaR evaluation gap) For any risk-dependent policy $\tilde { \pi }$ and initial risk-level $\alpha \in [ 0 , 1 ]$ , we have $\mathrm { C V a R } _ { \alpha } \left[ Z ^ { \tilde { \pi } _ { h } ^ { \alpha } } \right] = \upsilon ^ { \tilde { \pi } } ( s _ { 0 } , \alpha )$ if and only if there exists at least one optimal history perturbations $\xi ^ { \star }$ solution to the static CVaR evaluation (Eq. 4) such that the riskassignment constraints (Def. 10) have non-empty intersection.

Theorem 12 presents a formal characterization of necessary and sufficient conditions for when a CVaR evaluation gap occurs. It provides the valuable insight that, hidden under the mismatch between a risk-dependent policy’s value function and its static CVaR lies an unsolvable constraint satisfaction problem on the risk-level evolutions. More specifically, risk-dependent policies can induce action-selection consistency constraints that cannot be satisfied simultaneously with the other risk propagation and state-level risk envelope requirements, hampering the evaluation of a policy’s true CVaR.

The proposed constraint satisfaction perspective also clarifies why the evaluation is always accurate for risk-independent policies (Thm. 3.1 in Hau et al. [2023]). For such policies, the action-selection consistency constraint is non-binding, as the policy does not depend on the risk-level. A consistent risk-assignment can therefore always be constructed by recursively applying the CVaR decomposition Theorem (Thm. 2), guaranteeing the absence of a CVaR evaluation gap.

Corollary 13 (Existence of corresponding risk-dependent policy) For any history-dependent policy $\pi _ { h } : \mathcal { H }  \mathcal { A } ,$ , there exists a risk-dependent policy $\tilde { \pi }$ such that $\mathrm { \dot { C V a R } } _ { \alpha } \left[ \dot { Z ^ { \pi _ { h } } } \right] = \dot { \pmb { v } } ^ { \tilde { \pi } } ( \acute { s } _ { 0 } , \alpha )$ for all $\alpha \in [ 0 , 1 ]$ .

The biggest issue that comes from Theorem 12 and Corollary 13 is that one cannot in general guarantee that all risk-dependent policies will have optimal history perturbations $\xi ^ { \star }$ that have consistent riskassignment constraints (Def. 10). As a result, the set of all risk-dependent policies contains policies with a positive CVaR evaluation gap who will have an inaccurately high $\pmb { v } ^ { \tilde { \pi } } ( s _ { 0 } , \alpha )$ , impeding on the optimality of algorithms searching for $\tilde { \pi } ^ { \star } \in \arg \operatorname* { m a x } _ { \tilde { \pi } } { \pmb v } ^ { \tilde { \pi } } ( s _ { 0 } , \alpha )$ like CVaR VI [Chow et al., 2015]. The following section presents a worked example where the optimal risk-dependent policy has a positive CVaR evaluation gap and is therefore suboptimal.

![](images/7ec3acc28a785f44102f254ccd6a865df4db8035e8bb0b9fce3c097abffd7392.jpg)  
Figure 1: Sample MDP from Hau et al. [2023]. Next state transition probabilities are in blue while rewards are in green.

# A deeper dive into Hau et al. [2023]’s counterexample

We now apply our newly introduced constraint satisfaction lens to a counterexample presented in Hau et al. [2023]. We use the MDP shown in Figure 1, with horizon $T = 2$ and initial risk $\alpha = 0 . 5$ . Note that our MDP differs superficially from the one in Hau et al. [2023], namely because we use a deterministic initial state $s _ { 0 }$ with a single action available $a _ { 1 }$ that does not impact the transition to the first state $S _ { 1 }$ , a procedure equivalent to the stochastic initial state presented in the original MDP.

For our example, we consider the risk-dependent policy $\tilde { \pi }$ produced by CVaR VI [Chow et al., 2015] and its corresponding optimal state-level perturbation $\tilde { \xi } ^ { \star }$ solution to the risk-dependent-policy value function (Eq. 6):

$$
\tilde { \pi } ( s _ { 1 } , y ) = \left\{ \begin{array} { l l } { a _ { 1 } } & { \mathrm { i f ~ } y > 0 . 5 } \\ { a _ { 2 } } & { \mathrm { i f ~ } y \leq 0 . 5 } \end{array} \right. \qquad \tilde { \xi } ^ { \star } ( s ^ { \prime } | s , y , a ) = 1 , \quad \mathrm { f o r ~ a l l ~ r e a c h a b l e ~ } ( s , y , a ) .
$$

The optimal state-level perturbation $\tilde { \xi } ^ { \star }$ therefore applies no changes to next states sampled, so the risk level remains $Y _ { 1 } = 0 . 5$ after the transition from $s _ { 0 }$ . As a result, the corresponding history-dependent takes action $a _ { 2 }$ when reaching $s _ { 1 }$ , that is $\tilde { \pi } _ { h } ^ { 0 . 5 } \left( \left( s _ { 0 } , a _ { 1 } , s _ { 1 } \right) \right) = a _ { 2 }$ . Solving the static CVaR evaluation (Eq. 4), we find the corresponding history probabilities and optimal history perturbation $\xi ^ { \star }$ :

$$
{ P ^ { \bar { \pi } _ { h } ^ { 0 , 5 } } } ( H ) = \{ { 0 . 5 , } \quad H = ( { s _ { 0 } , a _ { 1 } , s _ { 1 } , a _ { 2 } , s _ { 5 } } ) \qquad { \xi ^ { \star } } ( H ) = \{ { 2 , } \quad H = ( { s _ { 0 } , a _ { 1 } , s _ { 1 } , a _ { 2 } , s _ { 5 } } ) \in ( { 0 . 5 , 0 , 1 0 } ) \} ,
$$

For the optimal history perturbation $\xi ^ { \star }$ to be realizable, there must exist a consistent risk-level assignment $y$ such that the risk-assignment consistency constraints (Def. 10) have non-empty intersection. Observing that histories of any length are fully defined by their final state since states never repeat in this MDP, the constraints can be expressed as:

1. Risk propagation: Directly applying trajectory perturbation $\xi ^ { \star }$ , we get

$$
\mathcal { V } ( s _ { 0 } ) = \alpha = 0 . 5 , \qquad \quad \mathcal { V } ( s _ { 5 } ) = 1 , \qquad \mathrm { ~ a n d ~ } \qquad \mathcal { V } ( s _ { 8 } ) = 0 .
$$

2. State-level risk envelope: For $t = 0$ , the constraint is equivalent to $\mathcal { V } ( s _ { 1 } ) + \mathcal { V } ( s _ { 2 } ) = 1$ . The constraints for $t = 1$ are $\mathcal { V } ( s _ { 5 } ) = \mathcal { V } ( s _ { 1 } )$ and $\mathcal { Y } ( s _ { 8 } ) = \mathcal { Y } ( s _ { 2 } )$ . Combining with the risk propagation constraint, we find that the risk assignment at $t = 1$ must be

$$
\mathcal { Y } ( s _ { 1 } ) = \mathcal { 1 } \qquad \mathrm { a n d } \qquad \mathcal { Y } ( s _ { 2 } ) = 0 .
$$

3. Action-selection consistency: The history that ends in $s _ { 5 }$ requires that at state $s _ { 1 }$ , the action $a _ { 2 }$ was selected. According to the policy $\tilde { \pi } ( s _ { 1 } , y )$ , this is only possible if the risk level satisfies

$$
\begin{array} { r } { \mathcal { V } ( s _ { 1 } ) \leq 0 . 5 . } \end{array}
$$

Figure 2 displays a visual representation of the risk-assignment constraints on the considered MDP (Figure 1) given the risk-dependent policy $\tilde { \pi }$ obtained using CVaR VI and an initial risk $\alpha = 0 . 5$ .

$$
\begin{array} { r l } & { \Big \{ \mathcal { Y } ( s _ { 1 } ) = 0 . 5 \Big \} \cap \Big \{ \mathcal { Y } ( s _ { 1 } ) = 1 \Big \} } \\ & { \Big \{ \mathcal { Y } ( s _ { 0 } ) = 0 . 5 \Big \} \quad s _ { 0 } , a _ { 1 } \underbrace { \longrightarrow ^ { s _ { 1 } } \xrightarrow { s _ { 1 } } \cdots \Big \{ \mathcal { Y } ( s _ { 1 } ) = 1 \Big \} } _ { s _ { 2 } } } \\ & { \Big \{ \mathcal { Y } ( s _ { 2 } ) = 0 . \Big \} \quad \xrightarrow { s _ { 2 } } \cdots \quad s _ { 2 } , a _ { 1 } \xrightarrow { s _ { 2 } } \cdots \quad \Big \{ \mathcal { V } ( s _ { 8 } ) = 0 \Big \} } \end{array}
$$

Figure 2: Visual representation of the risk-assignment constraints on the MDP from Figure 1, with the policy $\tilde { \pi }$ obtained using CVaR VI [Chow et al., 2015] and setting $\alpha = 0 . 5$ . Risk propagation constraints are in brown, state-level risk envelope constraints are in purple, and action selection constraints are in teal.

We observe that the state-level risk envelope (Eq. 10) and the action-selection consistency (Eq. 11) constraints are impossible to satisfy simultaneously. Thus, no consistent risk-level assignment $y$ exists for $\xi ^ { \star }$ . By Theorem 12, this confirms a positive CVaR evaluation gap at $\alpha = 0 . 5$ , explaining the empirical results obtained by Hau et al. [2023].

# 5 From Impossible Evaluation to Impossible Uniform Optimality

Leveraging the risk-assignment constraints perspective developed in the previous section, we now show that there exists an MDP where it is impossible for a single risk-dependent policy $\tilde { \pi }$ to be uniformly optimal, that is, being optimal for all initial risk levels $\alpha \in [ 0 , \bar { 1 } ]$ simultaneously. This suggests that the limitations of the dual CVaR decomposition are fundamental to the current problem formulation and cannot be solved by simple algorithmic improvements. We begin by formalizing the notion of a uniformly optimal policy.

Definition 14 (Uniformly optimal policy) $A$ risk-dependent policy $\tilde { \pi }$ is uniformly optimal if its corresponding history-dependent policy $\tilde { \pi } _ { h } ^ { \alpha }$ is optimal for all initial risk levels $\alpha \in [ 0 , 1 ]$ . That is, for all $\alpha \in [ 0 , 1 ]$ , we have

$$
\mathrm { C V a R } _ { \alpha } \left[ Z ^ { \tilde { \pi } _ { h } ^ { \alpha } } \right] = \operatorname* { m a x } _ { \pi _ { h } } \mathrm { C V a R } _ { \alpha } \left[ Z ^ { \pi _ { h } } \right] .
$$

To simplify our argument, we will assume without loss of generality that there is always a single optimal history-dependent policy for every $\alpha$ . That is $\begin{array} { r } { \operatorname * { a r g m i n } _ { \pi _ { h } } \operatorname { C V a } \dot { \operatorname { R } } _ { \alpha } \left[ Z ^ { \pi _ { h } } \right] } \end{array}$ is always a singleton, where we break ties consistently for policies with the same CVaR values.

For a policy $\tilde { \pi }$ to achieve uniform optimality, selected actions must align with those of the true optimal history-dependent policy $\pi _ { h , \alpha } ^ { \star }$ for every value of $\alpha$ and all possible histories. By Corollary 13, each optimal policy $\pi _ { h , \alpha } ^ { \star }$ induces a unique risk-dependent $\tilde { \pi } _ { \alpha }$ and a corresponding risk-level assignment $\mathcal { V } _ { \alpha }$ . To be uniformly optimal, a policy $\tilde { \pi }$ therefore has to ensure it simultaneously follows every $\tilde { \pi } _ { \alpha }$ alongside its risk-level assignment ${ \mathcal { V } } _ { \alpha }$ . These requirements can be grouped in a set of constraints, which we refer to as the optimal-action-selection constraints.

Proposition 15 (Uniform optimality constraints) A risk-dependent policy $\tilde { \pi }$ is uniformly optimal if and only if it simultaneously satisfies all the optimal-action-selection constraints

$$
\tilde { \pi } \big ( S _ { t } , \mathcal { V } _ { \alpha } ( H _ { 0 : t } ) \big ) = \pi _ { h , \alpha } ^ { \star } ( H _ { 0 : t } ) ,
$$

defined for all initial risk-levels $\alpha ~ \in ~ [ 0 , 1 ]$ , optimal policies $\pi _ { h , \alpha } ^ { \star }$ , histories $H ~ \in ~ \mathcal { H } _ { T }$ with $P ^ { \pi _ { h , \alpha } ^ { \star } } ( H ) > 0$ , and time steps $t = 0 , \ldots , T - 1$ .

Proposition 15 provides a clear test for uniform optimality: one must check if the set of optimalaction-selection constraints is feasible on an MDP to know whether or not there exists a uniformly optimal policy. If, for two different initial risk levels, the respective optimal policies generate the same risk-augmented state $( S , Y )$ but require different actions, then no single deterministic policy $\tilde { \pi }$ can satisfy all constraints simultaneously. This conflict is the basis for the following impossibility result.

![](images/5d0c739a451a2279e14acfd53aea337e397f3c8f07c961a03147c4eb8affec7c.jpg)  
Figure 3: Evolution of the static CVaR evaluation of all possible policies on the MDP presented in Figure 1 at different initial risk levels $\alpha$ . Shaded regions represent the optimal policy $\pi _ { h } ^ { \star }$ at a given initial risk-level $\alpha$ .

Theorem 16 There exists an MDP for which no single risk-dependent policy $\tilde { \pi } : \mathcal { S } \times [ 0 , 1 ] \to \mathcal { A }$ is uniformly optimal.

Proof We prove the result by providing an example MDP for which uniform optimality is impossible. We once again use the MDP from Hau et al. [2023], displayed in Figure 1, with horizon $T = 2$ . This MDP contains only three different history-dependent policies, which are all fully characterized by the selected action in state $s _ { 1 }$ . We can therefore easily compute the static CVaR of history-dependent policies to see which one is optimal at different initial risk-level $\alpha$ . To simplify notation, let us denote the three history-dependent policies π(hi to indicate which action $a _ { i }$ they select in $s _ { 1 }$ . Figure 3 shows the $\mathrm { C V a R } _ { \alpha }$ of each history-dependent policy based on the initial risk-level $\alpha$ . We can therefore deduce the optimal policy:

$$
\pi _ { h , \alpha } ^ { \star } : = \left\{ \begin{array} { l l } { \pi _ { h } ^ { ( 2 ) } } & { \mathrm { i f } \quad \alpha \in [ 0 , 0 . 3 7 5 ) } \\ { \pi _ { h } ^ { ( 3 ) } } & { \mathrm { i f } \quad \alpha \in [ 0 . 3 7 5 , 0 . 6 8 7 5 ) } \\ { \pi _ { h } ^ { ( 1 ) } } & { \mathrm { i f } \quad \alpha \in [ 0 . 6 8 7 5 , 1 ] . } \end{array} \right.
$$

Corollary 13 applies to every policy $\pi _ { h } ^ { ( i ) }$ because they are risk-independent, hence computing their respective value functions (Eq. 6) grants us state-level perturbations $\tilde { \xi }$ from which we can extract a consistent risk assignment. Combining these risk assignments with the $\alpha$ values where each $\pi _ { h } ^ { ( i ) }$ (i is optimal, we can extract the optimal-action-selection constraints. To streamline our argument, let us consider the state-level risk envelope constraints imposed at state $s _ { 1 }$ for cases $\alpha = 0 . 2 5$ , $\alpha = 0 . 5$ , and $\alpha = 0 . 7 5$ in particular. For these, the optimal-action-selection-constraints in $s _ { 1 }$ yield:

$$
\begin{array} { r l r l r l r l r l r l } & { \pi _ { h , 0 . 2 5 } ^ { \star } = \pi _ { 2 } } & & { \Longrightarrow } & & { \mathcal { Y } _ { 0 . 2 5 } ( s _ { 1 } ) = 0 . 5 } & & { \Longrightarrow } & & { \tilde { \pi } ^ { \star } ( s _ { 1 } , 0 . 5 ) = a _ { 2 } } & \\ & { \pi _ { h , 0 . 5 } ^ { \star } = \pi _ { 3 } } & & { \Longrightarrow } & & { \mathcal { Y } _ { 0 . 5 } ( s _ { 1 } ) = 0 . 5 } & & { \Longrightarrow } & & { \tilde { \pi } ^ { \star } ( s _ { 1 } , 0 . 5 ) = a _ { 3 } } & \\ & { \pi _ { h , 0 . 7 5 } ^ { \star } = \pi _ { 1 } } & & { \Longrightarrow } & & { \mathcal { Y } _ { 0 . 7 5 } ( s _ { 1 } ) = 0 . 5 } & & { \Longrightarrow } & & { \widehat { \pi } ^ { \star } ( s _ { 1 } , 0 . 5 ) = a _ { 1 } } & \end{array}
$$

That is, in order to be simultaneously optimal for initial risk levels $\alpha = 0 . 2 5$ , $\alpha = 0 . 5$ , and $\alpha = 0 . 7 5$ , a risk-dependent policy $\tilde { \pi }$ is required to take all actions $a _ { 1 } , a _ { 2 }$ , and $a _ { 3 }$ in state $S _ { t } = s _ { 1 }$ when the risk-level is $Y _ { t } = 0 . 5$ , proving the impossibility of uniform optimality.

Note that the range of values preventing the existence of a uniformly optimal policy in the MDP presented in Figure 1 extends beyond the $\alpha$ values presented in the proof of Theorem 16. In Figure 4, we visualize the optimal-action-selection constraints for all $\alpha \in [ 0 , 1 ]$ . The figure highlights the presence of a wide range of $\alpha$ where the optimal-action-selection constraints overlap, precluding the presence of a single optimal risk-dependent policy $\pi$ simultaneously optimal for these initial risk-levels $\alpha$ .

![](images/d406750dfeb10b6dbed7b2b4feeb9a811025df70e48534de62cc34ee36796146.jpg)  
Figure 4: Relation between initial risk-level $\alpha$ and the corresponding risk-level $\mathcal { \scriptsize { y } } ( s _ { 1 } )$ for all three possible policies on the MDP presented in Figure 1. Shaded regions represent the optimal policies, with the resulting constraint on $\tilde { \pi }$ displayed explicitly.

# 6 Conclusion

In this work, we diagnosed the root cause of failures in the static dual CVaR dynamic program decomposition. We started by framing the problem of evaluating a policy’s static CVaR as two distinct but related optimization tasks: one over history-level perturbations for the true static CVaR and another over state-level perturbations for the dynamic program. This perspective revealed that a CVaR evaluation gap arises precisely when a set of risk-assignment consistency constraints have an empty intersection. Building on this risk-assignment constraint perspective, we proved that the dual decomposition itself is fundamentally limited by identifying an MDP where no single risk-dependent policy can be uniformly optimal for all initial risk levels, as the action requirements for optimality at different risk levels become contradictory.

Our findings show that seeking a single, uniformly optimal policy with the current dual decomposition approach is flawed. Future work should therefore pivot towards developing algorithms that find the optimal policy for a specific initial risk level, similar to methods used for primal-based decompositions [Bäuerle and Ott, 2011]. Another alternative research direction is to identify the conditions on the MDP or the policy class under which the risk-assignment consistency constraints are always satisfiable, thereby guaranteeing the absence of an evaluation gap. Ultimately, our analysis provides a foundation for further characterizing, and potentially overcoming, the challenges of risk-averse reinforcement learning.