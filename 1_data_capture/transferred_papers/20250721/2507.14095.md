# C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected δ-Overlap Graphs

Yung-Hong Sun∗, Ting-Hung Lin, Jiangang Chen, Hongrui Jiang, Yu Hen Hu Department of Electrical and Computer Engineering University of Wisconsin - Madison, WI 53705, USA

Abstract—Multi-view multi-object association is a fundamental step in 3D reconstruction pipelines, enabling consistent grouping of object instances across multiple camera views. Existing methods often rely on appearance features or geometric constraints such as epipolar consistency. However, these approaches can fail when objects are visually indistinguishable or observations are corrupted by noise. We propose C-DOG, a training-free framework that serves as an intermediate module bridging object detection (or pose estimation) and 3D reconstruction, without relying on visual features. It combines connected $\delta$ -overlap graph modeling with epipolar geometry to robustly associate detections across views. Each 2D observation is represented as a graph node, with edges weighted by epipolar consistency. A $\delta$ -neighbor-overlap clustering step identifies strongly consistent groups while tolerating noise and partial connectivity. To further improve robustness, we incorporate Interquartile Range (IQR)- based filtering and a 3D back-projection error criterion to eliminate inconsistent observations. Extensive experiments on synthetic benchmarks demonstrate that C-DOG outperforms geometry-based baselines and remains robust under challenging conditions, including high object density, without visual features, and limited camera overlap, making it well-suited for scalable 3D reconstruction in real-world scenarios.

Index Terms—Multi-View Association, Training-Free Visual Perception, Featureless Matching, $\delta$ -Overlap Graphs, GraphBased Object Matching, Epipolar Geometry, 3D Reconstruction

# I. INTRODUCTION

Multi-view multi-object association plays a critical role in 3D reconstruction, aiming to consistently identify and group object instances across multiple camera views. This task is fundamental to a wide range of applications, including autonomous driving [1] and 3D human pose estimation [2]– [4]. However, it remains highly challenging due to factors such as occlusions, noisy observations, and imperfect camera calibration, all of which can degrade matching accuracy. Errors in associating 2D observations across views often propagate and significantly impair the quality of the resulting 3D reconstruction. Classical approaches typically rely on distinctive appearance features and geometric constraints such as epipolar consistency, but these methods can struggle in cluttered scenes with numerous objects and limited visual distinguishability.

Recent trends have shifted toward learning-based methods that employ neural networks to associate objects across views

Code available at GitHub: https://github.com/Yunghong/C-DOG [1]–[3], [5]–[10]. These approaches often integrate appearance cues—such as texture, color, or shape—with spatial and temporal information to resolve correspondence. While effective in visually rich and structured environments, they tend to fail in more difficult cases where appearance cues are ambiguous, degraded, or altogether absent. In particular, scenes containing multiple visually identical instances pose a fundamental challenge: in such settings, even temporal dynamics are insufficient to disambiguate object identities across views.

To overcome this, some approaches dispense with appearance features and instead rely solely on multi-view geometric constraints [4], [11]–[18]. Specifically, they leverage known camera poses—which encode the relative position and orientation of each camera—to infer associations between 2D observations across views. These geometric relationships are often encoded as pairwise affinity scores indicating the likelihood of correspondence between detections from different views. A common formulation treats each object detection as a graph node, with weighted edges indicating the strength of geometric consistency between views.

However, these methods are highly sensitive to observation noise and scaled instance counts. As illustrated in Fig. 2, in the simple case of two views and two nearby objects, a noisefree observation yields a distinct association pattern—correct correspondences have zero or near-zero scores—while under noisy conditions, the correct and incorrect associations become nearly indistinguishable. This ambiguity worsens as the number of objects increases. Although graph-based approaches incorporating epipolar constraints or back-projection errors have been proposed, their performance degrades significantly in the presence of noise and as the number of objects increases, limiting their reliability in real-world scenarios.

To address these challenges, we propose a novel trainingfree multi-view multi-object associator, which integrates connected $\delta$ -neighbor-overlap graph (C-DOG) structures with epipolar geometric constraints. This method uses a given threshold $\delta$ to check the likelihood of the correctness of any association pair utilizing the mutual neighbor connections. Designed to be lightweight and easy to integrate, C-DOG acts as a plug-and-play module for bridging object detection or pose estimation with 3D reconstruction, as illustrated in Fig. 1.

Inspired by existing geometry-based multi-view association algorithms, we model each 2D observation from a camera view as an independent node in a graph, and initialize crossview edges based on epipolar consistency. We then apply a $\delta$ -neighbour-overlap clustering [19] and filtering procedure to construct a graph that captures object correspondences as connected subgraphs, while tolerating missing or noisy edges. This stage filters out erroneous associations based on structural consistency rather than raw epipolar scores, yielding clusters that represent geometrically consistent observations of the same object across views.

![](images/aecd358a64b171c910958e3d42426d8f98ff5cf326cc25fe7a36e6e4650f76dd.jpg)  
Fig. 1. Demonstration of the C-DOG applied to $1 3 0 ~ 3 \mathrm { D }$ points in a multi-camera system from our benchmark. (a) Multiple visually indistinguishable 3D points are observed within a multi-camera setup. (b) Each camera captures 2D projections of the 3D points. These 2D observations can be obtained using object detectors or keypoint estimators. (c) C-DOG is used to establish correspondences across views by associating multi-view multi-object observations. (d) The resulting correspondences are then used to perform 3D reconstruction.

To further refine these clusters, we apply a filtering step based on the Interquartile Range (IQR) [20] of edge consistency scores and the 3D back-projection error, which quantifies the triangulation error of matched points. Our approach is robust to appearance ambiguity and limited view overlap, and significantly increases the feasible number of objects that can be accurately associated under challenging multi-camera conditions.

Our contributions are summarized as follows:

We propose C-DOG, a novel multi-view multi-object association framework that integrates connected $\delta$ -overlap

![](images/f1eb243b75157dc3b16bbc269d33b6bb79d9679f75ca9a6f8a51a4eee1bc53a1.jpg)  
Fig. 2. Example of multi-view multi-object association with two views and two nearby object instances under (a) noise-free and (b) noisy observations. Each node represents a 2D observation in one view. (a) In the ideal case, the association scores between corresponding points (same object across views) are zero, while those between different objects are significantly higher. (b) Under noisy observations, the score distribution becomes ambiguous, making correct correspondence more difficult to establish.

graph modeling with epipolar geometric constraints. • We incorporate an IQR-based outlier filtering mechanism to improve robustness against noisy observations and ambiguous pairwise associations. • Extensive experiments on synthetic benchmarks demonstrate that our method outperforms existing baselines in terms of both precision and completeness.

# II. RELATED WORKS

# A. Deep Learning Multi-view Object Association Methods

Current mainstream approaches to multi-view multi-object association predominantly rely on visual cues, typically employing neural networks to extract implicit features from images [1], [5], [6]. Among them, Transformer-based architectures [21], [22] have demonstrated strong capabilities in aggregating multi-view features by leveraging self-attention mechanisms to capture inter-dependencies within each view’s feature space [1]–[3], [7]. To further enhance correspondence matching, spatial camera information—such as epipolar geometry and homography—is often integrated into these pipelines [5], [8], [9]. Temporal cues have also been incorporated to improve consistency across frames. Yu et al. [10] and Zhang et al. [6] exploit features extracted from earlier frames to jointly refine multi-view associations over time. In parallel, Graph Neural Networks (GNNs) [23], [24] have been increasingly adopted for this task, where detections across views are modeled as graph nodes and correspondence estimation is cast as the correspondence group prediction problem [25]–[27]. These methods jointly model appearance descriptors and geometric relationships. However, they typically require large-scale, task-specific training data and exhibit reduced robustness in scenarios lacking distinctive visual features—making them less suitable for texture-poor or privacy-sensitive environments. Moreover, deep learningbased methods often incur significant computational overhead. Although some achieve real-time inference in isolation, their integration into larger systems—especially those involving multiple networks or concurrent tasks—can exceed available GPU resources. As such, lightweight, feature-independent alternatives offer practical advantages in both performance and deployability. In addition, reliance on feature-based learning limits adaptability, as these approaches generally require objects with distinguishable visual appearances and demand extensive training, thereby constraining their flexibility in dynamic or unseen environments.

# B. Training-Free Multi-view Object Association Methods

While a handful of deep learning-based methods can handle multi-view multi-object association, a training-free approach would significantly enhance adaptability. Training-free approaches typically treat each observed object as a node and utilize graph methods to establish correspondences. Greedy matchers select the best-scoring edge at each step without backtracking. Classic variants date back to Shafique et al. [28] and Lu et al. [29], while iterative improvements remain popular in real-time pose estimation [30], [31]. These methods typically settle for local optimal solutions and may easily encounter structured errors when the local optimum is incorrect. While these methods aim for efficiency through local optima, other approaches seek global optima. Dong’s method [4] reconstructs 3D points from each candidate pair via triangulation, followed by a voting mechanism to eliminate outliers. Some works formulate the association as a binary pairwise energy problem and resolve it using an ST graph-cut, guaranteeing the global minimum. This idea dates back to Kolmogorov et al.’s multi-camera reconstruction [32] and the volumetric stereo method of Vogiatzis et al. [33], where edge weights are derived from epipolar consistency or 3D back-projection error. Recent systems still incorporate an ST-cut step between upstream detection and downstream optimization, such as in messy tabletop object reconstruction [11] and multi-view multi-person pose reconstruction pipelines [12]. These methods typically require a designated reference view to initiate the matching process. However, they tend to perform poorly in complex scenarios involving numerous objects and are highly sensitive to geometric inaccuracies, such as calibration errors or noisy observations. Similar to ST-cut methods that ensure global consistency, permutation synchronization [13], [34] treats pairwise correspondences as partial permutation matrices and synchronizes them across views. CLEAR [14] adopted this approach in its association pipeline, and Li et al. [15] scaled permutation synchronization to improve computational efficiency. Factorized graph matching (FGM) [16] factorizes the pairwise affinity tensor to avoid costly memory and computational overhead. Compositionbased Affinity Optimization (CAO) style extensions [17], [35], [36] further improve both time and space complexity, enabling larger-scale graph matching. Zach proposed a robust bundle adjustment method [37] by lifting the cost function’s dimensionality, allowing soft matching to better handle outliers. RKHS-BA [38] further advanced this by enabling correspondence-free point group association, enhancing pose estimation quality under noisy conditions. Spectral clustering segments an affinity graph using Laplacian eigenvectors [18], [39], but remains sensitive to kernel bandwidth selection and the preset number $k$ of clusters, especially when objects are visually identical. Methods employing connected component clustering [40] construct K-NN graphs and remove edges exceeding a threshold, an approach later adapted for 3D human pose estimation [25], [41].

Although these training-free methods serve as easily integrable modules for multi-view multi-object association, some still rely on visual features represented by feature descriptors such as SIFT [42], resulting in significant degradation in both accuracy and computational efficiency when handling large numbers of points without visual cues. In this work, we propose an efficient yet robust training-free method that utilizes pure spatial geometry (featureless) to establish multi-view multi-object correspondences, thus extending the applicability beyond the limitations of existing approaches.

# III. PRELIMINARY

# A. Epipolar Geometry

Let $m \in \{ 1 , 2 , \ldots , M \}$ be the indices of the camera views, and let $i _ { m } \in \{ 1 , 2 , \dots , I ( m ) \}$ be the indices of the 2D points detected in the $m ^ { \mathrm { t h } }$ camera view, where $I ( m )$ is the indices of 2D points detected in the $m ^ { t h }$ view.

Let $V _ { m , i _ { m } }$ and $V _ { m ^ { \prime } , i _ { m } ^ { \prime } }$ be two 2D points on two distinct views $m$ and $m ^ { \prime }$ , $m \neq m ^ { \prime }$ respectively. If these two points are the images associated with the same 3D point on these two views, their relationship may be described using a fundamental matrix via the epipolar geometry [42]. Specifically, the fundamental matrix between views $m$ and $m ^ { \prime }$ is defined as

$$
\mathbf { F } _ { m , m ^ { \prime } } = \mathbf { K } _ { m } ^ { - T } [ \mathbf { T } _ { m , m ^ { \prime } } ] _ { \times } \mathbf { R } _ { m , m ^ { \prime } } ^ { - 1 } \mathbf { K } _ { m ^ { \prime } } ^ { - 1 } .
$$

where the intrinsic parameter matrices are denoted by ${ \bf K } _ { m }$ and ${ \bf K } _ { m ^ { \prime } }$ , and the extrinsic parameter matrices are denoted by $\left[ \mathbf { R } _ { m } \mid \mathbf { T } _ { m } \right]$ and $\left[ \mathbf { R } _ { m ^ { \prime } } \mid \mathbf { T } _ { m ^ { \prime } } \right]$ respectively. $[ \mathbf { T } _ { m , m ^ { \prime } } ] _ { \times } \in \mathbb { R } ^ { 3 \times 3 }$ is a skew-symmetric matrix of the cross-product of translation vector $\mathbf { T } _ { m , m ^ { \prime } }$ . The $3 \times 3$ fundamental matrix has a rank equals two and is skew-symmetric, namely,

$$
{ \bf F } _ { m , m ^ { \prime } } = - { \bf F } _ { m , m ^ { \prime } } ^ { T } = - { \bf F } _ { m ^ { \prime } , m }
$$

Given the fundamental matrix, an epipolar line on the image plane of the $m ^ { \prime t h }$ view can be computed:

$$
\mathbf { l } _ { m  m ^ { \prime } , i _ { m } } = \mathbf { F } _ { m , m ^ { \prime } } V _ { m , i _ { m } } ,
$$

This epipolar line is the image on the $m ^ { \prime t h }$ view of a ray originated from the camera center of the $m ^ { t h }$ view to the 3D point corresponding to $V _ { m , i _ { m } }$ . Since $V _ { m ^ { \prime } , i _ { m ^ { \prime } } }$ is the image of the same 3D point in the $m ^ { \prime t h }$ view, it must lies on the epipolar line $\mathbf { l } _ { m  m ^ { \prime } , i _ { m } }$ . In homogeneous coordinate representation, this implies

$$
V _ { m ^ { \prime } , i _ { m ^ { \prime } } } ^ { T } \mathbf { l } _ { m  m ^ { \prime } , i _ { m } } = V _ { m , i _ { m } } ^ { T } \mathbf { l } _ { m ^ { \prime }  m , i _ { m ^ { \prime } } } = 0
$$

The second expression states that $V _ { m , i _ { m } }$ must lie on the epipolar line corresponding to a ray from the camera center

![](images/aa84232027b12176698c2581f533002de46aac70ed473f195e55723a38637c50.jpg)  
Fig. 3. Illustration of the epipolar line concept. Views $m$ and $m ^ { \prime }$ observe the same 3D point. All 3D points lying along the viewing ray of $m ^ { \prime }$ project to the same image location in view $m ^ { \prime }$ , forming a 3D line. The projection of this line onto view $\mid m \mid$ yields the epipolar line. In the noise-free case, the corresponding observation in view $m$ should lie exactly on this epipolar line, as both views observe the same underlying 3D point.

of the $m ^ { \prime t h }$ view to $V _ { m ^ { \prime } , i _ { m ^ { \prime } } }$ as well as the common 3D point. Substituting eq. (2) into above, one has

$$
V _ { m ^ { \prime } , i _ { m ^ { \prime } } } ^ { T } \mathbf { F } _ { m ^ { \prime } , m } V _ { m , i _ { m } } = V _ { m , i _ { m } } ^ { T } \mathbf { F } _ { m , m ^ { \prime } } V _ { m ^ { \prime } , i _ { m ^ { \prime } } } = 0
$$

These epipolar geometry constraints are illustrated in Fig. 3.

Eq. (3) can be interpreted as the Euclidean distance from $V _ { m ^ { \prime } , i _ { m ^ { \prime } } }$ to the epipolar line $ { \mathbf { l } } _ { m  m ^ { \prime } , i _ { m } }$ equals 0 and from $V _ { m , i _ { m } }$ to the epipolar line $\mathbf { l } _ { m ^ { \prime }  m , i _ { m ^ { \prime } } }$ equals 0. In practice, due to numerical error as well as estimation errors of 2D feature points, the Euclidean distance between a 2D point and corresponding epipolar line may be non-zero. Let us define the epipolar distance $d _ { m , m ^ { \prime } , i _ { m } , i _ { m ^ { \prime } } }$ to be the Euclidean distance from a 2D point $V _ { m , i _ { m } } = [ x _ { o } y _ { o } 1 ] ^ { T }$ (homogeneous coordinate representation) to the epipolar line $\begin{array} { r } { \mathbf { l } _ { m ^ { \prime }  m , i _ { m ^ { \prime } } } = [ a b c ] ^ { T } } \end{array}$ . Then

$$
d _ { m , i _ { m } , m ^ { \prime } , i _ { m ^ { \prime } } } = \frac { a x _ { o } + b y _ { o } + c } { \sqrt { a ^ { 2 } + b ^ { 2 } } }
$$

# B. Triangulation and Back-Projection Error

Given camera intrinsic parameters $\mathbf { K } _ { m }$ , and extrinsic parameters ${ \bf { R } } _ { m }$ , ${ \bf { T } } _ { m }$ , $m = 1 , 2 , \ldots$ and a 3D coordinate $\mathbf { r }$ , its projection onto the $m ^ { t h }$ camera plane can be determined by the pin-hole camera equation [42]:

$$
V _ { m } = \mathbf { K } _ { m } \left[ \mathbf { R } _ { m } \mid \mathbf { T } _ { m } \right] \left[ \mathbf { \Lambda } _ { 1 } ^ { \mathbf { r } } \right] = \mathbf { K } _ { m } \mathbf { R } _ { m } \mathbf { r } + \mathbf { K } _ { m } \mathbf { T } _ { m }
$$

Let $\{ V _ { m } ; m \in M ( \mathbf { r } ) \}$ be a set of 2D points where $M ( \mathbf { r } )$ is a subset of indices of camera views such that each 2D point in it is the projection of a 3D point $\mathbf { r }$ . Substituting into eq. (6), it can be expressed as

$$
\left[ \begin{array} { c } { V _ { 1 } } \\ { \vdots } \\ { V _ { M ( \mathbf { r } ) } } \end{array} \right] = \left[ \begin{array} { c } { \mathbf { K } _ { 1 } \mathbf { R } _ { 1 } } \\ { \vdots } \\ { \mathbf { K } _ { M ( \mathbf { r } ) } \mathbf { R } _ { M ( \mathbf { r } ) } } \end{array} \right] \mathbf { r } + \left[ \begin{array} { c } { \mathbf { K } _ { 1 } \mathbf { T } _ { 1 } } \\ { \vdots } \\ { \mathbf { K } _ { M ( \mathbf { r } ) } \mathbf { T } _ { M ( \mathbf { r } ) } } \end{array} \right]
$$

If there are two or more sets of 2D observations, then the 3D coordinate $\mathbf { r }$ may be estimated from eq. (7) by minimizing the least square estimation error.

Denote the estimated 3D coordinate as $\hat { \mathbf { r } }$ . One may then back-project this coordinate to the image planes using the pinhole camera equation eq. (6) to yield a set of back-projected 2D coordinates $\{ \hat { V } _ { m } ; m \stackrel { \cdot } { \in } M ( { \bf r } ) \}$ .

Ideally, one would expect $V _ { m } = \hat { V } _ { m } ; m \in M ( \mathbf { r } )$ . However, in practice, their difference

$$
e _ { m } = V _ { m } - \hat { V } _ { m }
$$

is often non-zero. This is known as back projection error (BPE). BPE has a number of potential causes: estimation error of 2D feature points, calibration error of camera poses (intrinsic and extrinsic parameters), and data association error. In this work, we will focus on the data association error due to erroneous association of some 2D observations to a 3D point.

# IV. METHODOLOGY

# A. Problem Formulation

In this work, we assume $M$ calibrated cameras (views) are used to capture images of unknown number of 3D points. A 3D point needs not appear in the field of views of every cameras. A feature detection (and localization) algorithm will is used to detect and estimate the 2D coordinate of the image of 3D points. The goal is to correctly estimate the number and 3D coordinates of 3D points. The averaged square back projection error is used as an indirect performance metric of the accuracy of the estimated 3D coordinate.

To estimate the number and 3D coordinates of the 3D points, one must associate the 2D feature points detected at multiple views to corresponding 3D point before the triangulation step described in eq. (7) is applied. Unlike existing feature association methods [1]–[3], [5]–[10], here no feature descriptor is available. Hence, the feature association must be solely based on epipolar geometry. This leads to the following 2D Point feature matching problem:

Point Feature Matching Problem Give a calibrated multicamera system so that the intrinsic and extrinsic camera poses are known. Let $\{ V _ { m , i _ { m } } ; 1 \leq m \leq M , 1 \leq i _ { m } \leq I ( m ) \}$ be a set of 2D feature points where $m$ is the index of camera views, $i _ { m }$ is the point index within the $m ^ { t h }$ view and $I ( m )$ is the number of 2D points within the $m ^ { t h }$ view. We assume these 2D feature points are images of $K$ 3D points. The objective of the point feature matching problem is to group these 2D feature points into $L$ association groups $\{ G _ { \ell } ; 1 \le \ell \le L \}$ , $L \leq K$ so that the 2D feature points within each association group are associated with the same 3D point. This grouping must satisfy the following constraints:

1) $2 \leq | G _ { \ell } | \leq M$ where $\vert G _ { \ell } \vert$ is the number of members (size) in the set $G _ { \ell }$ . 2) If $V _ { m , i _ { m } } \in G _ { \ell }$ and $V _ { m ^ { \prime } , i _ { m ^ { \prime } } } \in G _ { \ell }$ , then $m \neq m ^ { \prime }$ .

The quality of the grouping shall be evaluated using the averaged square back projection error:

$$
E _ { B P } = \sum _ { \ell = 1 } ^ { L } \frac { 1 } { | G _ { \ell } | } \sum _ { ( m , i _ { m } ) \in G _ { \ell } } \Vert V _ { m , i _ { m } } - \hat { V } _ { m , i _ { m } } \Vert ^ { 2 }
$$

where $V _ { m , i _ { m } }$ is the observed 2D feature point, and $\hat { V } _ { m , i _ { m } }$ is the back-projected 2D feature point.

In this work, we will solve this point feature matching problem without assuming any feature descriptor is available.

![](images/8b5e1c42518a987f33aa2817ddfffdae07f3429753f8ef24e866e9f94cc09de6.jpg)  
Fig. 4. Overview of C-DOG with an illustrative example with three 3D points and 6 cameras: (a) A multi-camera system observes object points from multiple viewpoints. (b) Required inputs include the camera pose for each view and 2D observed points in pixel coordinates. (c) Initial graph generation: each node represents a 2D point in a specific view, and edges between nodes are weighted by epipolar distances. Only view-wise minimal connections that satisfy the epipolar constraint are retained. (d) Weak connection removal: sparse inter-group ections are identified and eliminated based on node neighborhood overlap density [19], as they typically represent incorrect associations. (e) Group-level outlier removal: outliers are detected using 3D back-projection error within each group. (f) Final output: associated groups of 2D points across views, where each group corresponds to a distinct 3D object instance.

If one evaluate the epipolar distance between a 2D point Vm,im in the $m ^ { t h }$ view and every 2D feature points $V _ { m ^ { \prime } , i _ { m ^ { \prime } } }$ in the $m ^ { \prime t h }$ camera view, one may expect

$$
d _ { m , i _ { m } , m ^ { \prime } , i _ { m ^ { \prime } } } \geq 0
$$

If for a specific $i _ { m ^ { \prime } } ^ { * }$ such that

$$
d _ { m , i _ { m } , m ^ { \prime } , i _ { m ^ { \prime } } ^ { * } } = 0
$$

then one may conclude that $V _ { m , i _ { m } }$ and $V _ { m ^ { \prime } , i _ { m ^ { \prime } } ^ { * } }$ are a pair of feature points associated with the same 3D point. Hence both of them should be assigned to the same association group. By repeating this process for every 2D points in every camera view, a desired grouping for the Point Feature Matching Problem may be obtained.

We remark that in rare cases, there may be more than one points on the $m ^ { \prime t h }$ view satisfying eq. (11). This happens when the 3D points corresponding to these $V _ { m ^ { \prime } , i _ { m ^ { \prime } } ^ { * } }$ 2D points fall on the same ray originated from the camera center of the $m ^ { t h }$ camera view. The same $V _ { m , i _ { m } }$ then may be assigned to the corresponding association groups. We also consider the presence of outliers which are 2D feature points that are not included in any association groups. Each outlier corresponds to a 3D point which appears in only one camera view and hence stereo vision triangulation cannot be applied to estimate its 3D coordinate.

The association groups may be represented graphically: Denote each 2D feature point as a vertex in a graph, and assign an edge between a pair of vertices if both are associated to the same 3D point. As such, all the vertices in an association group form a clique because there will be an edge between any pair of vertices of this group.

Ideally procedures discussed so far should be able to solve this Point Feature Matching Problem. However, this is under the assumption that the 2D feature points observations are made without any modeling (camera calibration model) error nor any numerical estimation error during triangulation and back projection. In other words, in the ideal situation, $E _ { B P } = 0$ . In practice, noisy observations and camera models are expected. hence, a robust feature association algorithm that is less sensitive to the observation and modeling noise is desired. Below, we propose a robust algorithm that leverages a Connected $\delta$ -Overlap Graph (C-DOG) representation and epipolar geometry to solve the point feature matching problem.

# B. C-DOG Overview

C-DOG is a training-free, geometry-aware association framework designed to serve as an intermediate module between 2D object detection (or pose estimation) and 3D reconstruction. In C-DOG, each 2D detection in a camera view is represented as a node in a graph. An edge will be established between a pair of nodes if these nodes are likely associated with the same 3D point. As illustrated in Fig. 4, the C-DOG consists of four main stages:

1) Association graph initialization: construct candidate connections using epipolar geometry.

2) Weak edge pruning: detect and eliminate unreliable links based on $\delta$ -overlap constraints.   
3) Group-level outlier removal: refine groups using Interquartile Range (IQR) filtering and 3D back-projection error.

# C. Initial Association and Connection Graph Generation

To construct the initial association graph, we leverage epipolar geometry to assess geometric consistency across camera views. By calculating the epipolar distance for each point pair as shown in Section III(a), we initialize a connectivity graph where each 2D point in each view is represented as a node that belongs to an instance, and edges between nodes are determined by their corresponding epipolar distances. As illustrated in Fig. 5, each point is evaluated against all points from other views, and edges with large epipolar distances are subsequently pruned. Algorithm 1 outlines the procedure for constructing the initial graph. Each point $X _ { m , i }$ is represented as node $V _ { m , i }$ in the graph system.

# Algorithm 1 Association Graph Initialization

Require: Camera intrinsics $\{ K _ { m } \} _ { m = 1 } ^ { M }$ , rotations $\{ R _ { m } \} _ { m = 1 } ^ { M }$ and translations $\{ T _ { m } \} _ { m = 1 } ^ { M }$ ; 2D point (nodes) $\begin{array} { r l } { \nu } & { { } = } \end{array}$ $\{ V _ { m , i _ { m } } \ | \ m = 1 , \ldots , M ; \ i _ { m } = 1 , \ldots , I ( m ) \}$ in pixel coordinates; Epipolar distance threshold $\tau$   
Ensure: Initial graph $\mathcal { G } = ( \nu , \mathcal { E } )$   
1: for each view $m \in \{ 1 , \ldots , M \}$ do   
2: for each view $m ^ { \prime } \in \{ 1 , \ldots , M \}$ where $m ^ { \prime } \neq m$ do   
3: for each node $V _ { m , i _ { m } }$ in view $m$ do   
4: Compute the epipolar line $ { \mathbf { l } } _ { m  m ^ { \prime } , i _ { m } }$   
5: for each node $V _ { m ^ { \prime } , i _ { m ^ { \prime } } }$ in view $m ^ { \prime }$ do   
6: Compute epipolar distance dm,m′,im,i   
7: end for   
8: Find $\begin{array} { r } { i _ { m ^ { \prime } } ^ { * } = \arg \operatorname* { m i n } _ { i _ { m ^ { \prime } } } d _ { m , m ^ { \prime } , i _ { m } , i _ { m ^ { \prime } } } } \end{array}$   
9: if $d _ { m , m ^ { \prime } , i _ { m } , i _ { m ^ { \prime } } ^ { * } } < \tau$ then   
10: Add directional edge Em,im m′,i∗ $\mathcal { E } _ { m , i _ { m }  m ^ { \prime } , i _ { m ^ { \prime } } ^ { * } } = V _ { m , i _ { m } } $ $V _ { m ^ { \prime } , i _ { m ^ { \prime } } ^ { * } }$ to $\mathcal { E }$ with weights $d _ { m , m ^ { \prime } , i _ { m } , i _ { m ^ { \prime } } ^ { * } }$   
11: end if   
12: end for   
13: end for   
14: end for   
15: Summarize connected component $\mathcal G _ { k } \subseteq \mathcal G$   
16: return $\mathcal { G }$

For each point $X _ { m , i _ { m } }$ in view $m$ , we compute its epipolar distance to all points $X _ { m ^ { \prime } , i _ { m ^ { \prime } } }$ in every other view $m ^ { \prime } \neq m$ . For each such view $m ^ { \prime }$ , we retain the point with the minimum distance that satisfies a geometric consistency threshold $\tau$ :

$$
i _ { m ^ { \prime } } ^ { * } = \arg \operatorname* { m i n } _ { i _ { m ^ { \prime } } } d _ { m , m ^ { \prime } , i _ { m } , i _ { m ^ { \prime } } } \mathrm { s . t . } d _ { m , m ^ { \prime } , i _ { m } , i _ { m ^ { \prime } } ^ { * } } < \tau .
$$

If no point satisfies this condition, no correspondence is added for that view.

Thresholding provides an effective mechanism for discarding clearly incorrect point correspondences. The threshold $\tau$ can be estimated based on the level of observation noise,

![](images/46b37c72491156df62b5c10392e8197f03e9edbec65d36eeee54128f427825dd.jpg)  
Fig. 5. An example of association graph initialization using object instances 2 and 3 from Fig. 4. (a) Epipolar geometry is applied to each pair of points to compute edge weights. Node pairs with lower edge weights (epipolar scores) are more likely to be correct associations. (b) Edges with higher weights are removed, forming several groups. This completes the initialization of the association graph.

assuming the 2D points are perturbed by zero-mean Gaussian noise. That is, each observed point $\hat { X } _ { m , i _ { m } } ^ { \ ' }$ is modeled as a noisy version of the true point:

$$
\hat { X } _ { m , i _ { m } } = X _ { m , i _ { m } } + \epsilon , \quad \epsilon \sim \mathcal { N } ( 0 , \sigma ^ { 2 } ) ,
$$

where $\epsilon$ denotes zero-mean Gaussian noise with standard deviation $\sigma$ .

Since the epipolar distance involves comparisons between two independently noisy observations, the combined noise follows a Gaussian distribution with standard deviation ${ \sqrt { 2 } } \sigma$ . In practical situations, the noise standard deviation $\sigma$ can be reliably estimated from 2D observations using statistical methods [20], assuming the noise is zero-mean and follows a Gaussian distribution. By applying the empirical rule, we set the threshold using a scale factor $\alpha$ , where typical choices like $\alpha = 2$ capture approximately $9 5 \%$ of the noise variation in one dimension.

Thus, the threshold is defined as:

$$
\tau = \alpha \sqrt { 2 } \sigma ,
$$

where $\alpha$ controls the tolerance to noise.

After this step, all point pairs have been traversed, resulting in an initial bidirectional connection graph composed of multiple connected groups. Each connected component represents a potential correspondence, indicating that the 2D points from different views within the group likely correspond to the same underlying 3D object instance.

# D. Weak Edge Pruning

Once the association graph is initialized, the next step is to identify and eliminate erroneously connected point pairs. Specifically, we evaluate the structural consistency of each edge based on the neighborhood overlap between the connected nodes. Rather than relying on edge weights (e.g., epipolar distances) and edge directions, we utilize graph connectivity information to distinguish between strong and weak connections. Although epipolar geometry serves as a principled foundation for initializing associations, its effectiveness diminishes in the presence of noise and viewpoint variation, limiting its reliability for downstream decision-making. Strong connections are indicative of likely correct associations, whereas weak connections suggest potential mismatches. By removing weak edges, we aim to prune erroneous links and improve the purity of the graph’s internal grouping.

In a correctly associated group, the connections are typically dense: each node is expected to be directly connected to most or all other nodes within the group. In contrast, inter-group connections—introduced by noise or ambiguity—are sparse. This structural distinction motivates a neighborhood-overlapbased pruning strategy: when a wrong connection bridges two distinct object groups, it typically results in low mutual connectivity. Removing such weak connections helps isolate distinct object instances.

To quantify the connection strength between two nodes $V _ { m , i }$ and $V _ { m ^ { \prime } , i ^ { \prime } }$ , we introduce a neighborhood-overlap score inspired by the Szymkiewicz–Simpson coefficient [43]:

$$
\theta ( V _ { m , i } , V _ { m ^ { \prime } , i _ { m ^ { \prime } } } ) = \frac { \left| N [ V _ { m , i } ] \cap N [ V _ { m ^ { \prime } , i _ { m ^ { \prime } } } ] \right| } { \operatorname* { m a x } \left( \left| N [ V _ { m , i } ] \right| , \left| N [ V _ { m ^ { \prime } , i _ { m ^ { \prime } } } ] \right| \right) } ,
$$

where $N [ V _ { m , i } ]$ denotes the closed neighborhood of $V _ { m , i }$ —i.e., the node and its directly connected neighbors. A high score indicates that both nodes share a large number of common neighbors, suggesting consistency in their local graph structures. Including the node itself in the neighborhood computation helps mitigate score underestimation in cases where the number of nodes is limited. For example, if only two nodes are connected and present in a group, the open neighborhood intersection yields a score of zero, while the closed neighborhood yields a score of one, appropriately reflecting the fact that the two nodes form a densely connected group of size two. This adjustment similarly benefits small groups with only a few nodes, where sparse connectivity can otherwise lead to misleadingly low overlap scores.

Instead of the minimum used in the Szymkiewicz–Simpson coefficient, the maximum is incorporated in the denominator to ensure a conservative estimate: mismatches with imbalanced neighborhood sizes receive lower scores, introducing a higher penalty and improving robustness to incorrect associations. Such that, $0 ~ < ~ \theta ( . ) ~ \leq ~ 1$ . In the ideal case, where both nodes are correctly associated within the same group, the score approaches 1. Given a threshold $\delta$ , a connection is classified as strong if $\theta ( V _ { m , i } , V _ { m ^ { \prime } , i ^ { \prime } } ) > \delta$ , and as weak if $\theta ( V _ { m , i } , V _ { m ^ { \prime } , i ^ { \prime } } ) \leq \delta$ . Through grid-search on our benchmark, we found that setting $\delta \in [ 0 . 5 , 0 . 5 5 ]$ yields the best overall performance. This threshold is intuitive, as it requires that at least half of each node’s neighborhood overlaps with the other, ensuring that a majority consensus supports the connection before it is classified as strong.

Through the Algorithm 2, by retaining only strong connections, we preserve coherent groups in which nodes are densely connected and structurally consistent. This step significantly enhances the quality of the association graph by separating likely object instances and eliminating ambiguous or spurious links.

# Algorithm 2 Weak Edge Pruning

<html><body><table><tr><td>Require: Initial graph g = (V,ε) Ensure: Filtered graph 9pruned = (V,εstrong) 1: for each connected component Sk ≌  do 2: for each connected pair (Vk,m,i, Vk,m',i'） ∈ 9k do</td></tr><tr><td>3: Compute overlap score 0(Vk,m,i, Vk,m',i') 4: if 0(Vk,m,i, Vk,m',i'）< δ then</td></tr><tr><td>5: Remove weak edge (Vk,m,i, Vk,m',i'） from ε end if</td></tr><tr><td>6:</td></tr><tr><td>7:</td></tr><tr><td>end for</td></tr><tr><td>8:end for 9:Summarize the connected componenst as groups</td></tr></table></body></html>

# E. Group-level Outlier Removal

While pruning weak connections effectively separates loosely connected or mismatched components, certain outliers may still remain within groups. These outliers may satisfy the epipolar constraint and retain structurally strong connections, making them difficult to detect using connectivity-based measures alone. To further refine group consistency, we introduce a statistically driven outlier removal strategy that leverages 3D reconstruction and back-projection consistency across views as shown in Section III(b).

Specifically, for each group containing three or more nodes, we consider all pairs of 2D observations to reconstruct candidate 3D points. Each reconstructed point is then back-projected onto the remaining views in the same group. The discrepancy between a back-projected point and its corresponding observed point in each view is measured as the back-projection error (BPE). Ideally, if all observations are correctly associated, the BPE should remain low. By aggregating these errors, we compute a per-node average BPE, which serves as a quantitative score for identifying outliers.

For each group, we select all valid node pairs, reconstruct their corresponding 3D points, and compute the BPE in each remaining view. We then average the BPE values for each node across all reconstructions and back-projections, resulting in a single mean BPE score per node.

To identify outliers, we apply an IQR filter to the distribution of average BPE scores. Let $Q _ { 1 }$ and $Q _ { 3 }$ denote the $2 5 \mathrm { t h }$ and 75th percentiles, respectively. The IQR is defined as:

$$
\mathrm { I Q R } = Q _ { 3 } - Q _ { 1 } .
$$

The acceptable BPE range is then bounded by:

$$
\begin{array} { r } { \mathrm { l b } = Q _ { 1 } - \alpha \cdot \mathrm { I Q R } } \\ { \mathrm { u b } = Q _ { 3 } + \alpha \cdot \mathrm { I Q R } , } \end{array}
$$

where $\alpha$ is a hyperparameter that controls the strictness of outlier rejection (set to $\alpha = 2$ in our experiments). Given that the minimum possible BPE is zero, which indicates a perfect projection, we incorporate this prior by padding the list of average BPE values with a zero before computing percentiles and explicitly enforcing the lower bound as zero.

# Algorithm 3 Group-level Outlier Removal

Require: Refined graph $\mathcal { G } _ { p r u n e d } = ( \mathcal { V } , \mathcal { E } _ { s t r o n g } )$   
Ensure: Graph $\mathcal { G } _ { r m } = ( \nu , \mathcal { E } _ { r m } )$ with outlier nodes’ edges removed   
1: for each group $G _ { k } \subseteq { \mathcal { G } } _ { p }$ do   
2: repeat   
3: for each pair of nodes $V _ { m , i }$ and $V _ { m ^ { \prime } , i _ { m ^ { \prime } } }$ in $\nu _ { k }$ do   
4: Triangulate 3D point $X _ { l } ^ { 3 D }$ from $X _ { m , i }$ and $X _ { m ^ { \prime } , i _ { m ^ { \prime } } }$   
5: for each remaining node $V _ { m ^ { \prime \prime } , i _ { m ^ { \prime \prime } } } \subseteq \mathcal { V } _ { k }$ , where $m ^ { \prime \prime } \neq m , m ^ { \prime }$ do   
6: Back-project $X _ { l } ^ { 3 D }$ to view $m ^ { \prime \prime }$ as $\hat { X } _ { m ^ { \prime \prime } , i _ { m ^ { \prime \prime } } }$   
7: Compute back-projection error $\mathrm { B P E } _ { m , m ^ { \prime } , m ^ { \prime \prime } }$   
8: end for   
9: end for   
10: Compute average BPE for each node $V \subseteq \mathcal { V } _ { k }$   
11: Apply IQR-based filtering to identify outliers   
12: Remove outlier nodes from $\nu _ { k }$ and corresponding edges from $\mathcal { E } _ { s t r o n g }$   
13: until no outliers are detected in $G _ { k }$   
14: end for   
15: return Graph $\mathcal { G } _ { r m }$ with cleaned groups

This adjustment increases sensitivity in scenarios where the BPE values are generally high, allowing the method to better distinguish relatively large errors. A node is considered an outlier and removed from the group if its average BPE falls outside the interval [lb, ub]. This step helps minimize BPE, thereby improving point association consistency and enhancing the accuracy of 3D reconstruction.

Algorithm 3 outlines the detailed procedure, where each group is iteratively examined for outliers until none are detected. This refinement step yields cleaner groups with fewer outlier-associated nodes, thereby reducing reconstruction errors and enhancing the overall consistency of the multi-view association.

# F. Synthetic Benchmark Generation

To evaluate the proposed method in a controlled and featureless setting, we design a synthetic benchmark that eliminates confounding factors such as texture, keypoint structures, and semantic cues. This benchmark isolates the core challenges of multi-view multi-object association by minimizing all variables except for camera poses and 2D observation coordinates. Our goal is to assess the method’s performance under idealized conditions, with a focus solely on geometric consistency.

Unlike popular datasets [44], [45], which include rich semantic and structural cues, our benchmark omits any explicit feature information or high-level spatial priors, such as object articulated poses. Instead, it consists solely of 3D points and their 2D projections across multiple views. We employ ten calibrated camera views obtained from the EasyVis system [46], [47], where camera parameters are estimated using SfM [48]. Each camera is represented by its intrinsic matrix $K$ , rotation matrix $R$ , and translation vector $T$ . These matrices describe the orientation and position of a camera in 3D space, and the rule of projection of 3D points to the 2D camera view in pixel coordinates.

The dataset consists of 3D point instances, ranging from 1 to 130, with increments of 1 from 1 to 20 and 5 from 20 to 130. Each number of point instance is generated in 5 batches, and each batch is unique. Each instance is generated from a uniform 3D distribution. The corresponding 2D projections are computed for all ten calibrated views, after which Gaussian noise is added to the 2D points independently in the $x$ and $y$ axes. The noise standard deviation varies from 0 to 5 pixels, in increments of 0.25. In total, the benchmark contains 9,575 unique 3D point groups and 46,192 associated 2D projections for each noise level, spanning a comprehensive range of noise levels and scene complexities. The dataset does not simulate missing detections or false positives and each 3D point has a corresponding 2D observation in all views.

This fully synthetic setup allows precise control over the number of 3D points, the level of observation noise, and the spatial distribution of points. Moreover, it avoids the need for time-consuming manual annotations, which become increasingly impractical as the number of points and views increases. Automatic access to ground-truth associations is particularly critical in our setting for calculating evaluation scores.

# V. EXPERIMENTS

# A. Experimental Setup

We constructed a synthetic benchmark by generating random 3D points uniformly distributed in space and projecting them onto 10 camera views using known camera poses. Gaussian noise was added to the 2D projections with standard deviations ranging from 0 to 5 pixels, in increments of 0.25. The number of 3D points varies from 1 to 130: from 1 to 20 in steps of 1, and from 20 to 130 in steps of 5. The camera parameters were estimated from a real multi-camera system [46] using SfM [48]. To evaluate robustness under varying view availability, we randomly drop a subset of views, retaining between 2 and 9 views in increments of 1. For each subset, excluding the full 10-view case, the selected views are randomly sampled from the original set of 10 calibrated views.

All experiments were conducted on a desktop equipped with an NVIDIA RTX 3080 Ti GPU, an Intel i9-12900K CPU, and $3 2 \mathrm { G B }$ of RAM. The implementation was developed in Python.

The evaluation metrics are based on true positives (TP), false positives (FP), and false negatives (FN), whose definitions vary depending on whether the evaluation is performed at the group level or the point level.

We adopt the following standard evaluation metrics: precision $( P = \mathrm { T P } / ( \mathrm { T P } + \mathrm { F P } ) )$ ), recall $( R = \mathrm { T P } / ( \mathrm { T P } + \mathrm { F N } ) )$ , $F _ { 1 }$ score $( F _ { 1 } = 2 P R / ( P + R ) )$ , and intersection over union $\mathrm { ( I o U = T P / ( T P + F P + F N ) } ]$ .

Based on these formulations, we report three kinds of scores with different definitions of TP, FP, and FN:

Group-level Scores (G-): All predicted groups are evaluated, with isolated nodes treated as separate groups. TP is the number of predicted groups that match ground-truth groups. FP is the number of predicted groups that do not match any ground-truth group. FN is the number of ground-truth groups that are not matched by any prediction. This score reflects the system’s overall ability to associate groups correctly, allowing for reasonable matching slack.

TABLE I ABLATION STUDY OF PERFORMANCE UNDER VARYING NUMBERS OF CAMERA VIEWS WITH GAUSSIAN NOISE $\sigma = 3 . 0 0$ .   

<html><body><table><tr><td># views</td><td>G-F1</td><td>G-IoU</td><td>mP-P</td><td>mP-R</td><td>mP-F1</td><td>mP-IoU</td><td>PG-P</td><td>PG-R</td><td>PG-F1</td><td>3DErr</td><td>bpe</td><td>time(ms)</td></tr><tr><td>2 views</td><td>0.836</td><td>0.789</td><td>0.782</td><td>0.782</td><td>0.782</td><td>0.782</td><td>0.819</td><td>0.726</td><td>0.766</td><td>0.050</td><td>9.651</td><td>1.704</td></tr><tr><td>3 views</td><td>0.861</td><td>0.792</td><td>0.770</td><td>0.746</td><td>0.754</td><td>0.738</td><td>0.794</td><td>0.676</td><td>0.726</td><td>0.153</td><td>20.157</td><td>14.835</td></tr><tr><td>4 views</td><td>0.743</td><td>0.639</td><td>0.612</td><td>0.598</td><td>0.601</td><td>0.581</td><td>0.679</td><td>0.501</td><td>0.562</td><td>0.343</td><td>36.743</td><td>26.760</td></tr><tr><td>5 views</td><td>0.775</td><td>0.674</td><td>0.637</td><td>0.612</td><td>0.618</td><td>0.591</td><td>0.718</td><td>0.540</td><td>0.601</td><td>0.368</td><td>38.126</td><td>75.601</td></tr><tr><td>6 views</td><td>0.855</td><td>0.767</td><td>0.721</td><td>0.664</td><td>0.684</td><td>0.648</td><td>0.753</td><td>0.612</td><td>0.668</td><td>0.480</td><td>44.305</td><td>195.533</td></tr><tr><td>7 views</td><td>0.875</td><td>0.790</td><td>0.732</td><td>0.662</td><td>0.686</td><td>0.646</td><td>0.765</td><td>0.628</td><td>0.685</td><td>0.550</td><td>29.571</td><td>324.526</td></tr><tr><td>8 views</td><td>0.890</td><td>0.811</td><td>0.753</td><td>0.680</td><td>0.706</td><td>0.666</td><td>0.797</td><td>0.662</td><td>0.719</td><td>0.455</td><td>25.259</td><td>471.898</td></tr><tr><td>9 views</td><td>0.880</td><td>0.793</td><td>0.746</td><td>0.710</td><td>0.721</td><td>0.699</td><td>0.838</td><td>0.681</td><td>0.747</td><td>0.828</td><td>21.033</td><td>498.642</td></tr><tr><td>10 views</td><td>0.881</td><td>0.796</td><td>0.751</td><td>0.715</td><td>0.727</td><td>0.706</td><td>0.854</td><td>0.694</td><td>0.761</td><td>0.702</td><td>15.585</td><td>523.373</td></tr></table></body></html>

Mean Point Scores (mP-): For each predicted group with valid observations (i.e., from at least two views), we compute point-level precision. A point is counted as a TP if it is correctly associated with the dominant ground-truth group label, a FP if wrongly associated, and a FN if missed. The group class is determined by the majority of its associated ground-truth points. We compute precision per group and report the average across all such groups. This score reflects the method’s fine-grained association performance—i.e., how accurately each group aggregates its points.

Perfect Group Scores (PG-): A predicted group is considered a TP if it is perfectly associated—that is, it contains only the points from one ground-truth instance. Imperfect or duplicated groups are counted as FP, and missed ground-truth groups are counted as FN. This score reflects the method’s ability to produce entirely correct groupings. In practical applications, having a larger number of perfectly associated groups is crucial for achieving high-quality and interpretable 3D reconstructions.

We denote final scores by combining the prefix and metric symbol.We report: G- ${ \bf \nabla } \cdot { \bf F } _ { 1 }$ , G-IoU, mP- $P$ , mP- $R$ , mP- ${ \bf \nabla } \cdot { \cal F } _ { 1 }$ , mPIoU, PG- $P$ , PG- $R$ , and $\mathrm { P G } { - } F _ { 1 }$ . A higher score indicates better performance.

In addition, to evaluate the quality of 3D reconstruction, we compute the 3D reconstruction error (3DErr) and backprojection error (BPE). 3DErr is defined as the average Euclidean distance between the reconstructed 3D points and their corresponding ground-truth positions, where reconstruction is performed using points within each associated group. BPE is calculated by projecting each reconstructed 3D point onto all valid ground-truth 2D views, and measuring the Euclidean distance between the back-projected points and the corresponding ground-truth 2D observations in pixel space. Lower values of 3DErr and BPE indicate higher association accuracy and reconstruction quality.

Finally, we assess runtime performance by measuring the average time required to complete one full association process.

This provides insight into the computational cost of each evaluated method.

# B. Ablation Study

In this section, we conduct an ablation study to investigate the relationship between performance and the number of available camera views. We repeat the experiment using the $\sigma = 3 . 0 0$ noise benchmark, varying the number of views from 2 to 10 in increments of 1.

Interestingly, the performance initially drops as the number of camera views increases, but then begins to improve after reaching a certain point. Specifically, when increasing from 2 to 5 views, performance declines due to the inclusion of more noisy observations, which makes it harder for the method to correctly distinguish between groups. However, once the number of views exceeds 5, performance begins to recover and gradually improves up to 10 views.

This behavior can be explained by the trade-off between increased observation ambiguity and mutual consistency. As more cameras are added, the number of pairwise comparisons grows, which introduces more potential for error under noisy conditions. However, a higher number of views also brings more mutual affinity constraints, effectively acting as a voting mechanism that improves robustness to outliers.

At the 10-camera setting, the method achieves the best performance in terms of group association, with a $5 . 4 \%$ improvement in G- ${ \bf \nabla } \cdot { \cal F } _ { 1 }$ compared to 2-view setups. In terms of runtime, the computational cost increases polynomially with the number of views, since more point pairs must be evaluated for mutual affinity.

Moreover, reducing the number of views not only limits cross-view observations but also decreases the number of points visible in the system. While the 2-camera setup may show slightly higher accuracy for the small set of observable points, it suffers from limited perception coverage. In contrast, the 10-camera setup offers a wider field of view and higher coverage, enabling better overall association despite the increased complexity.

# C. Comparison With Existing Works

In this experiment, we compared our method against feature-free spatial geometry baselines, which rely solely on 2D point observations and camera poses as input. While some of the comparison methods are relatively dated, this is due to the underexplored nature of the problem addressed in this paper. These classical approaches are rarely updated, but they continue to serve as foundational modules for solving more complex tasks.

TABLE II QUANTITATIVE COMPARISON OF ALL METHODS UNDER GAUSSIAN NOISE WITH STANDARD DEVIATION $\sigma = 0 . 0 0$   

<html><body><table><tr><td>Method</td><td>G-F1</td><td>G-IoU</td><td>mP-P</td><td>mP-R</td><td>mP-F1</td><td>mP-IoU</td><td>PG-P</td><td>PG-R</td><td>PG-F1</td><td>3DErr</td><td>bpe</td><td>time(ms)</td></tr><tr><td>Greedy</td><td>0.218</td><td>0.161</td><td>0.138</td><td>0.162</td><td>0.145</td><td>0.138</td><td>0.326</td><td>0.111</td><td>0.137</td><td>0.464</td><td>87.516</td><td>102.733</td></tr><tr><td>CAO</td><td>0.606</td><td>0.496</td><td>0.383</td><td>0.164</td><td>0.229</td><td>0.163</td><td>0.566</td><td>0.367</td><td>0.383</td><td>0.056</td><td>6.227</td><td>198.950</td></tr><tr><td>ST-Cut 3D bpj</td><td>0.924</td><td>0.864</td><td>0.815</td><td>0.663</td><td>0.708</td><td>0.642</td><td>0.626</td><td>0.707</td><td>0.662</td><td>1.799</td><td>35.796</td><td>527.993</td></tr><tr><td>ST-Cut Epipolar</td><td>0.844</td><td>0.772</td><td>0.757</td><td>0.759</td><td>0.757</td><td>0.753</td><td>0.940</td><td>0.740</td><td>0.803</td><td>0.013</td><td>1.218</td><td>148.860</td></tr><tr><td>Bdl.Adj.w/Sft.Mat.</td><td>0.854</td><td>0.782</td><td>0.769</td><td>0.769</td><td>0.768</td><td>0.766</td><td>0.963</td><td>0.760</td><td>0.827</td><td>0.011</td><td>0.870</td><td>986.089</td></tr><tr><td>CCA</td><td>0.574</td><td>0.542</td><td>0.547</td><td>0.521</td><td>0.531</td><td>0.521</td><td>0.810</td><td>0.547</td><td>0.572</td><td>0.000</td><td>0.000</td><td>609.857</td></tr><tr><td>Spectral Clustering</td><td>0.411</td><td>0.275</td><td>0.265</td><td>0.272</td><td>0.266</td><td>0.264</td><td>0.870</td><td>0.257</td><td>0.383</td><td>0.007</td><td>0.548</td><td>1812.623</td></tr><tr><td>Permutation Sync.</td><td>0.841</td><td>0.733</td><td>0.392</td><td>0.424</td><td>0.383</td><td>0.317</td><td>0.053</td><td>0.051</td><td>0.052</td><td>1.475</td><td>209.246</td><td>69.874</td></tr><tr><td>Fact. Graph Mat.</td><td>0.635</td><td>0.473</td><td>0.999</td><td>0.431</td><td>0.563</td><td>0.431</td><td>0.473</td><td>1.000</td><td>0.635</td><td>0.000</td><td>0.000</td><td>1122.561</td></tr><tr><td>Ours w/o IQR</td><td>0.845</td><td>0.745</td><td>0.879</td><td>0.579</td><td>0.609</td><td>0.567</td><td>0.733</td><td>0.571</td><td>0.636</td><td>0.787</td><td>30.816</td><td>539.157</td></tr><tr><td>Ours</td><td>0.950</td><td>0.908</td><td>0.903</td><td>0.905</td><td>0.904</td><td>0.902</td><td>0.986</td><td>0.896</td><td>0.937</td><td>0.030</td><td>2.511</td><td>371.608</td></tr></table></body></html>

TABLE III QUANTITATIVE COMPARISON OF ALL METHODS UNDER GAUSSIAN NOISE WITH STANDARD DEVIATION $\sigma = 1 . 0 0$ .   

<html><body><table><tr><td>Method</td><td>G-F1</td><td>G-IoU</td><td>mP-P</td><td>mP-R</td><td>mP-F1</td><td>mP-IoU</td><td>PG-P</td><td>PG-R</td><td>PG-F1</td><td>3DErr</td><td>bpe</td><td>time(ms)</td></tr><tr><td>Greedy</td><td>0.212</td><td>0.155</td><td>0.131</td><td>0.155</td><td>0.138</td><td>0.131</td><td>0.324</td><td>0.104</td><td>0.130</td><td>0.468</td><td>88.245</td><td>102.739</td></tr><tr><td>CAO</td><td>0.603</td><td>0.493</td><td>0.382</td><td>0.169</td><td>0.234</td><td>0.169</td><td>0.559</td><td>0.370</td><td>0.382</td><td>0.152</td><td>15.164</td><td>196.065</td></tr><tr><td>ST-Cut 3D bpj</td><td>0.918</td><td>0.853</td><td>0.812</td><td>0.651</td><td>0.698</td><td>0.630</td><td>0.606</td><td>0.691</td><td>0.644</td><td>1.715</td><td>38.573</td><td>575.972</td></tr><tr><td>ST-Cut Epipolar</td><td>0.788</td><td>0.715</td><td>0.689</td><td>0.692</td><td>0.689</td><td>0.683</td><td>0.853</td><td>0.649</td><td>0.703</td><td>0.024</td><td>3.131</td><td>223.230</td></tr><tr><td>Bdl.Adj.w/Sft.Mat.</td><td>0.808</td><td>0.735</td><td>0.719</td><td>0.718</td><td>0.718</td><td>0.715</td><td>0.951</td><td>0.708</td><td>0.775</td><td>0.024</td><td>2.591</td><td>1121.568</td></tr><tr><td>CCA</td><td>0.360</td><td>0.288</td><td>0.289</td><td>0.268</td><td>0.276</td><td>0.268</td><td>0.778</td><td>0.286</td><td>0.354</td><td>0.018</td><td>2.140</td><td>635.351</td></tr><tr><td>Spectral Clustering</td><td>0.403</td><td>0.269</td><td>0.257</td><td>0.264</td><td>0.259</td><td>0.256</td><td>0.865</td><td>0.248</td><td>0.372</td><td>0.010</td><td>2.006</td><td>1826.594</td></tr><tr><td>Permutation Sync.</td><td>0.841</td><td>0.733</td><td>0.393</td><td>0.423</td><td>0.382</td><td>0.317</td><td>0.053</td><td>0.050</td><td>0.051</td><td>1.591</td><td>217.600</td><td>65.002</td></tr><tr><td>Fact. Graph Mat.</td><td>0.687</td><td>0.531</td><td>0.922</td><td>0.374</td><td>0.500</td><td>0.369</td><td>0.456</td><td>0.843</td><td>0.583</td><td>0.323</td><td>14.521</td><td>1144.011</td></tr><tr><td>Ours w/o IQR</td><td>0.919</td><td>0.856</td><td>0.815</td><td>0.836</td><td>0.821</td><td>0.809</td><td>0.889</td><td>0.769</td><td>0.822</td><td>0.161</td><td>12.012</td><td>309.959</td></tr><tr><td>Ours</td><td>0.919</td><td>0.856</td><td>0.841</td><td>0.840</td><td>0.839</td><td>0.835</td><td>0.955</td><td>0.821</td><td>0.880</td><td>0.087</td><td>14.810</td><td>445.098</td></tr></table></body></html>

TABLE IV QUANTITATIVE COMPARISON OF ALL METHODS UNDER GAUSSIAN NOISE WITH STANDARD DEVIATION $\sigma = 3 . 0 0$ .   

<html><body><table><tr><td>Method</td><td>G-F1</td><td>G-IoU</td><td>mP-P</td><td>mP-R</td><td>mP-F1</td><td>mP-IoU</td><td>PG-P</td><td>PG-R</td><td>PG-F1</td><td>3DErr</td><td>bpe</td><td>time(ms)</td></tr><tr><td>Greedy</td><td>0.211</td><td>0.154</td><td>0.131</td><td>0.154</td><td>0.138</td><td>0.131</td><td>0.325</td><td>0.103</td><td>0.129</td><td>0.467</td><td>89.223</td><td>103.430</td></tr><tr><td>CAO</td><td>0.606</td><td>0.494</td><td>0.373</td><td>0.173</td><td>0.238</td><td>0.172</td><td>0.535</td><td>0.363</td><td>0.373</td><td>0.426</td><td>32.985</td><td>197.913</td></tr><tr><td>ST-Cut 3D bpj</td><td>0.905</td><td>0.833</td><td>0.801</td><td>0.626</td><td>0.676</td><td>0.604</td><td>0.565</td><td>0.654</td><td>0.604</td><td>1.495</td><td>43.839</td><td>551.621</td></tr><tr><td>ST-Cut Epipolar</td><td>0.678</td><td>0.618</td><td>0.596</td><td>0.583</td><td>0.586</td><td>0.576</td><td>0.751</td><td>0.553</td><td>0.585</td><td>0.060</td><td>6.809</td><td>328.626</td></tr><tr><td>Bdl.Adj.w/Sft.Mat.</td><td>0.729</td><td>0.660</td><td>0.643</td><td>0.639</td><td>0.639</td><td>0.635</td><td>0.890</td><td>0.623</td><td>0.675</td><td>0.057</td><td>6.413</td><td>1311.430</td></tr><tr><td>CCA</td><td>0.239</td><td>0.182</td><td>0.180</td><td>0.160</td><td>0.167</td><td>0.160</td><td>0.576</td><td>0.176</td><td>0.230</td><td>0.085</td><td>5.096</td><td>641.257</td></tr><tr><td>Spectral Clustering</td><td>0.385</td><td>0.254</td><td>0.244</td><td>0.248</td><td>0.244</td><td>0.240</td><td>0.854</td><td>0.233</td><td>0.352</td><td>0.034</td><td>4.592</td><td>1824.498</td></tr><tr><td>Permutation Sync.</td><td>0.841</td><td>0.733</td><td>0.391</td><td>0.423</td><td>0.381</td><td>0.316</td><td>0.054</td><td>0.051</td><td>0.052</td><td>1.551</td><td>223.036</td><td>65.643</td></tr><tr><td>Fact. Graph Mat.</td><td>0.771</td><td>0.644</td><td>0.781</td><td>0.285</td><td>0.396</td><td>0.277</td><td>0.419</td><td>0.594</td><td>0.480</td><td>1.117</td><td>43.025</td><td>1123.199</td></tr><tr><td>Ours w/o IQR</td><td>0.881</td><td>0.796</td><td>0.727</td><td>0.713</td><td>0.711</td><td>0.684</td><td>0.785</td><td>0.643</td><td>0.703</td><td>0.513</td><td>24.178</td><td>487.891</td></tr><tr><td>Ours</td><td>0.881</td><td>0.796</td><td>0.751</td><td>0.715</td><td>0.727</td><td>0.706</td><td>0.854</td><td>0.694</td><td>0.761</td><td>0.702</td><td>15.585</td><td>523.373</td></tr></table></body></html>

TABLE V QUANTITATIVE COMPARISON OF ALL METHODS UNDER GAUSSIAN NOISE WITH STANDARD DEVIATION $\sigma = 5 . 0 0$ .   

<html><body><table><tr><td>Method</td><td>G-F1</td><td>G-IoU</td><td>mP-P</td><td>mP-R</td><td>mP-F1</td><td>mP-IoU</td><td>PG-P</td><td>PG-R</td><td>PG-F1</td><td>3DErr</td><td>bpe</td><td>time(ms)</td></tr><tr><td>Greedy</td><td>0.200</td><td>0.146</td><td>0.123</td><td>0.146</td><td>0.130</td><td>0.123</td><td>0.298</td><td>0.097</td><td>0.119</td><td>0.488</td><td>92.961</td><td>104.400</td></tr><tr><td>CAO</td><td>0.606</td><td>0.496</td><td>0.353</td><td>0.175</td><td>0.240</td><td>0.173</td><td>0.490</td><td>0.355</td><td>0.353</td><td>0.806</td><td>50.091</td><td>188.021</td></tr><tr><td>ST-Cut 3D bpj</td><td>0.897</td><td>0.820</td><td>0.797</td><td>0.615</td><td>0.667</td><td>0.592</td><td>0.533</td><td>0.618</td><td>0.570</td><td>1.314</td><td>49.919</td><td>601.483</td></tr><tr><td>ST-Cut Epipolar</td><td>0.597</td><td>0.534</td><td>0.525</td><td>0.441</td><td>0.467</td><td>0.434</td><td>0.645</td><td>0.477</td><td>0.483</td><td>1.717</td><td>17.239</td><td>379.713</td></tr><tr><td>Bdl.Adj.w/Sft.Mat.</td><td>0.657</td><td>0.582</td><td>0.567</td><td>0.550</td><td>0.553</td><td>0.543</td><td>0.735</td><td>0.523</td><td>0.543</td><td>0.463</td><td>16.317</td><td>1470.134</td></tr><tr><td>CCA</td><td>0.215</td><td>0.167</td><td>0.161</td><td>0.132</td><td>0.143</td><td>0.132</td><td>0.470</td><td>0.157</td><td>0.202</td><td>0.071</td><td>8.209</td><td>636.594</td></tr><tr><td>Spectral Clustering</td><td>0.338</td><td>0.219</td><td>0.209</td><td>0.205</td><td>0.205</td><td>0.199</td><td>0.792</td><td>0.191</td><td>0.292</td><td>0.109</td><td>10.592</td><td>1817.438</td></tr><tr><td>Permutation Sync.</td><td>0.841</td><td>0.733</td><td>0.391</td><td>0.423</td><td>0.380</td><td>0.315</td><td>0.050</td><td>0.048</td><td>0.049</td><td>1.470</td><td>218.785</td><td>65.964</td></tr><tr><td>Fact. Graph Mat.</td><td>0.800</td><td>0.689</td><td>0.725</td><td>0.264</td><td>0.366</td><td>0.255</td><td>0.392</td><td>0.487</td><td>0.421</td><td>1.373</td><td>78.721</td><td>1120.728</td></tr><tr><td>Ours w/o IQR</td><td>0.845</td><td>0.745</td><td>0.640</td><td>0.576</td><td>0.590</td><td>0.543</td><td>0.651</td><td>0.512</td><td>0.568</td><td>0.510</td><td>36.212</td><td>551.228</td></tr><tr><td>Ours</td><td>0.845</td><td>0.745</td><td>0.672</td><td>0.579</td><td>0.609</td><td>0.567</td><td>0.733</td><td>0.571</td><td>0.636</td><td>0.787</td><td>30.816</td><td>539.157</td></tr></table></body></html>

We selected methods that operate purely on spatial geometry, as discussed in Section II-B. These methods take the camera poses and 2D point coordinates in the image plane as input and produce association results in the form of grouped 2D points. The compared baselines include: the Greedy method [28], Composition-based Affinity Optimization (CAO) [36], ST-cut using 3D back-projection constraints (ST-Cut 3D bpj) [33], ST-cut using epipolar constraints (STCut Epipolar) [32], bundle adjustment with soft matching (Bdl.Adj. w/ Sft. Mat.) [37], Connected Component Analysis (CCA) [40], Spectral Clustering [18], Permutation Synchronization (Permutation Sync.) [15], and Factorized Graph Matching (Fact. Graph Mat.) [16]. We also reported results for our proposed method under three different configurations: the original full model, and a variant without IQR-based outlier removal.

We conducted experiments under four different levels of Gaussian noise: $\sigma = 0 . 0 0$ , $\sigma = 1 . 0 0$ , $\sigma = 3 . 0 0$ , and $\sigma =$ 5.00. The results are summarized in Tables II, III, IV, and $\mathrm { \Delta V }$ . For each batch (i.e., scene), evaluation metrics were computed individually and then averaged across all batches.

Under noise-free conditions $\begin{array} { r l r } { ( \sigma } & { { } = } & { 0 . 0 0 ) } \end{array}$ , our method achieves a $1 3 . 3 \%$ improvement over the second-best method in perfect group association $( P G - F _ { 1 } )$ , a $2 . 8 \%$ improvement in imperfect group association $( G { - } F _ { 1 } )$ , and a $1 7 . 7 \%$ gain in point-wise association performance $( m P  – F _ { 1 } )$ . In terms of IoU, our method outperforms the second-best by $5 . 1 \%$ for imperfect groups $G$ -IoU) and $1 7 . 5 \%$ for point-wise association quality $m p$ -IoU). Under $\sigma = 1 . 0 0$ conditions, our method achieves a $1 0 . 5 \%$ improvement over the second-best method in $P G { - } F _ { 1 }$ , a $1 6 . 6 \%$ gain in $m P  – F _ { 1 }$ , a $0 . 1 \%$ gain in $m P  – F _ { 1 }$ , an $0 . 3 \%$ gain in $G$ -IoU), and an $1 6 . 8 \%$ gain in $\overset { \cdot } { m } P$ -IoU). Under $\sigma = 3 . 0 0$ conditions, our method achieves a $1 2 . 7 \%$ improvement over the second-best method in $P G \mathrm { - } F _ { 1 }$ , a $7 . 5 \%$ gain in $m P  – F _ { 1 }$ , and an $1 1 . 1 \%$ gain in $_ m P$ -IoU).

While our method ranks mid-range in 3D reconstruction error and BPE, this is largely due to the conservative nature of several baselines. These methods tend to produce fewer or no associations under point-rich conditions, leading to a higher likelihood of skipping difficult cases. As a result, they often report lower mean 3D error and BPE, since missing group associations do not incur penalties in the error calculation.

We provided qualitative comparisons using 3D visualizations to compare the top four performing baseline methods with our proposed approach under varying numbers of points (# points) and different noise levels $( \sigma )$ . Ideally, a wellperforming method should produce 3D score maps with a flat, elevated surface at $z ~ = ~ 1$ , indicating consistent and accurate associations. These visual results are summarized in Fig. 6. Our method consistently outperformed the baselines in both point-wise and group associations. The $m P$ score maps generated by our method were notably flatter and higher than those of the baselines, especially under low noise conditions. Moreover, our method exhibited more gradual performance degradation as the number of points and noise levels increased. In terms of perfect group association $( \mathrm { P G } - E _ { 1 } )$ , our method maintained relatively high and stable performance, with a slower decline across increasing noise and point density. The imperfect group association scores $( \mathrm { G } \ – F _ { 1 } )$ also demonstrated a flat plateau near the top surface, further highlighting the robustness of our method. These results collectively indicated that our approach offered superior robustness and accuracy compared to baseline methods under varying noise and scene complexity.

We used the association results from various baseline methods and our proposed approach to perform 3D reconstruction, and we summarized the visualization results in Fig. 7. A scene containing 130 instances was randomly selected from the dataset for this experiment. This visualization provides an intuitive understanding of how many points are correctly reconstructed, how many are incorrectly reconstructed, and how many are missed entirely. An ideal method would accurately reconstruct all points without introducing errors or omissions. As shown in the figure, our method performs well under noise-free conditions, missing only a few points. Compared to the baselines, our approach yields more accurate association groups with fewer incorrect associations. Notably, the number of missed associations is significantly higher than that of incorrect ones, which is desirable in challenging, high-noise conditions where avoiding false positives is more critical than recovering every point. This characteristic is beneficial for downstream tasks, as it reduces the introduction of noisy data into subsequent processing stages.

Finally, we compare the runtime performance of our method against the baselines, as shown in Fig. 8. The time cost for each method is computed by averaging the runtime over all batches. As the number of points increases, the computational cost of most baseline methods rises rapidly, often exhibiting polynomial growth. In particular, methods that rely on pairwise affinity scores tend to show approximately quadratic scaling in both time and memory complexity. Although some localoptimum-based methods are more computationally efficient, their performance—as discussed in previous sections—is significantly worse in terms of accuracy. In contrast, our method demonstrates favorable scalability, maintaining a competitive runtime while achieving superior association performance.

# VI. DISCUSSION

Our method demonstrates consistent improvements over geometry-based baselines across various noise levels and numbers of point instances. In addition to its accuracy, our method shows favorable runtime performance, achieving a strong balance between efficiency and accuracy. It performs particularly well in noisy and instance-rich environments by effectively leveraging mutual affinities, leading to cleaner association results and robust correspondence under challenging conditions. In noise-free or low-noise scenarios, the method remains reliable and achieves high scores in the dense-point setups.

![](images/d3d31d173d2c9e3db06054037fe682d1126c3950714b38619bafc5a918cda2f7.jpg)  
Fig. 6. Accuracy comparison of various baselines across varying noise levels and object instance counts.

We observed that the method initially suffers when the number of camera views increases under high-noise conditions, due to greater ambiguity introduced by additional noisy observations. However, as the number of views increases beyond a certain threshold, performance improves again, benefiting from more consistent affinity information. This behavior highlights the importance of mutual voting in improving robustness against noise.

A key strength of our method is its robustness without relying on visual features, making it particularly well-suited for texture-poor or featureless scenarios. In addition, its trainingfree nature makes it ideal as an intermediate module that bridges upstream and downstream tasks—for example, serving as a correspondence associator between multi-view object detection and 3D reconstruction.

This method was originally developed for a real-time 3D visualization system in laparoscopic surgery training [46], where a dense 10-camera setup is used to observe object interactions. In that context, our method functions as a multi-view, multiobject associator between detection outputs and downstream

![](images/64bedeed995d3cb45c554bef823df247d546d2e8bb0ea2c74d7cf11184dce241.jpg)  
Fig. 7. Comparison of 3D reconstruction results for 130 instances across various baselines under different noise levels. (a) 10 observision views with visualized 2D points ${ \mathit { \Omega } } ^ { \prime } \sigma = 0 . 0 0 { \mathit { \Omega } } ,$ ). (b) 3D reconstruction results.

3D reconstruction, even in the absence of distinctive object features. The camera poses in this system are calibrated using SfM, which motivated our design choice to leverage known poses and operate solely on 2D point observations.

While the method demonstrates high accuracy and runtime efficiency in this configuration, its time cost increases polynomially with the number of object instances. Future work will focus on enhancing scalability for high-instance-count scenes and improving runtime performance for real-time deployment.

# VII. CONCLUSION

We proposed an efficient, training-free method, C-DOG, for multi-view, multi-object association using only 2D point observations and known camera poses. Designed to operate effectively in featureless and noisy environments, our approach eliminates the need for visual features while preserving both scalability and accuracy. Extensive evaluations across varying noise levels, object densities, and view configurations demonstrate that our method consistently outperforms geometrybased baselines, achieving a strong balance between precision and runtime. This provides a general and robust solution that can serve as a bridging module between multi-view detection and 3D reconstruction. Future work will focus on improving computational efficiency and extending the approach to realtime applications in instance-rich environments.

![](images/53dbc747c0968de11eff678881cd787804e23cd3ef7307c4205685e1d38c6e8c.jpg)  
Fig. 8. Comparison of time cost across different methods under varying object instance counts.