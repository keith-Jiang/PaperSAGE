# Training-free Token Reduction for Vision Mamba

Qiankun Ma1,2, Ziyao Zhang1,2, Chi $\mathbf { S } \mathbf { u } ^ { 3 }$ , Jie Chen1,3, Zhen Song1, Hairong Zheng1,2,4, Wen Gao1,3

1Peng Cheng Laboratory, 2Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences 3Peking University, 4University of the Chinese Academy of Sciences maqiankun $2 0 1 8 @$ gmail.com, zhangzy $@$ pcl.ac.cn

# Abstract

Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs) due to its ability to efficiently capture long-range dependencies with linear computational complexity. While token reduction, an effective compression technique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision Mamba’s efficiency is essential for enabling broader applications. However, we find that directly applying existing token reduction techniques for ViTs to Vision Mamba leads to significant performance degradation. This is primarily because Mamba is a sequence model without attention mechanisms, whereas most token reduction techniques for ViTs rely on attention mechanisms for importance measurement and overlook the order of compressed tokens. In this paper, we investigate a Mamba structure-aware importance score to evaluate token importance in a simple and effective manner. Building on this score, we further propose MTR, a training-free Mamba Token Reduction framework. Without the need for training or additional tuning parameters, our method can be seamlessly integrated as a plug-and-play component across various Mamba models. Extensive experiments demonstrate that our approach significantly reduces computational workload while minimizing performance impact across various tasks and multiple backbones. Notably, MTR reduces FLOPs by approximately $40 \%$ on the Vim-B backbone, with only a $1 . 6 \%$ drop in ImageNet performance without retraining.

# Introduction

In recent years, Transformers have made remarkable progress in the field of computer vision, with Vision Transformers (ViTs) (Dosovitskiy 2020) being a prime example. However, ViTs encounter challenges due to the quadratic growth of self-attention complexity as the input size increases. Mamba (Gu and Dao 2023), as a sequence model, has demonstrated substantial potential in addressing these issues, thanks to its linear computational complexity. The emergence of Vision Mamba (Zhu et al. 2024; Liu et al. 2024) has garnered extensive attention and is regarded as a strong competitor to ViTs (Liu, Zhang, and Zhang 2024).

Token reduction (Rao et al. 2021; Pan et al. 2021; Yuan et al. 2021; Renggli et al. 2022; Chen et al. 2023; Meng et al. 2022; Cao, Paranjape, and Hajishirzi 2023; Liang et al. 2022) have been shown to be effective in enhancing the efficiency of ViTs, as the token length or number of to

Pumer (DeiT-S) Pumer (Vim-S) Pumer-Reorder (Vim-S)   
50 0% 10% 20% 30% 40% Reduction Ratio (a) Mamba is sensitive to the order of compressed tokens   
85   
1 EViT (DeiT-S) EViT (Vim-S)   
60 0% 10% 20% 30% 40% Reduction Ratio (b) Attention-based method underperforms in Mamba

kens is independent of the model architecture. Consistent with existing research efforts to improve the efficiency of ViTs, exploring the efficiency of Vision Mamba is crucial for enabling real-time applications. Recently, UTR (Zhan et al. 2024b) first introduced the training-free token reduction technique to Mamba-based models for natural language processing (NLP) tasks; however, training-free token reduction remains largely unexplored in Vision Mamba.

Given that Vision Mamba processes input tokens by dividing them into patches, similar to ViTs, applying existing ViTs’ token reduction techniques to Vision Mamba might seem like a straightforward approach to enhance efficiency. However, as illustrated in Fig. 1, directly applying existing token reduction methods for ViTs to Vision Mamba results in significant performance degradation. We attribute this to two main factors. First, Mamba is a sequential model, and the order of compressed tokens significantly impacts performance. As shown in Fig. 1(a), using the classical token reduction method Pumer (Cao, Paranjape, and Hajishirzi

2023) on Vision Mamba (Vim-S) (Zhu et al. 2024) and ViT (DeiT-S) (Touvron et al. 2021) backbones on the ImageNet dataset reveals a dramatic performance drop when Pumer is applied directly to Vim-S. This occurs because the compression disrupts the original token order, which can be mitigated by reordering tokens post-compression. Second, existing training-free token reduction methods often rely on attention mechanisms and can be categorized into two types: attention-based and CLS token-based. Attentionbased methods, such as Zero-TPrune (Wang, Dedhia, and Jha 2024) and VoMix (Peng et al. 2024), heavily rely on the attention mechanism, frequently utilizing intermediate results from the attention computation, such as the Q, K matrices or the attention maps. Since the Mamba model lacks these intermediate results, such methods cannot be directly migrated to the Mamba model. Another type, CLS tokenbased methods, such as EViT (Liang et al. 2022), uses the [CLS] token’s attention score to measure token importance, a highly effective approach in ViT token compression (Wang et al. 2024; Haurum et al. 2023; Zhang et al. 2024), and can be directly applied to Mamba. However, since Mamba lacks an attention mechanism, we substitute attention scores with token similarity. Consequently, when applying EViT to Vision Mamba, we use the similarity between the [CLS] token and other tokens to assess importance. As depicted in Fig. 1(b), Vision Mamba’s performance is notably inferior to ViT’s, especially at high reduction rates (e.g., a $40 \%$ compression rate results in a $4 . 9 \%$ performance gap), highlighting a substantial discrepancy.

This observation prompted us to consider whether Mamba possesses an “attention score”, a indicator that assesses token importance without incurring additional computational overhead. Through extensive analysis and experimentation, we found that the timescale parameter $\Delta$ in Mamba effectively serves this purpose. Building on this insight, we developed a training-free Mamba token reduction framework, named MTR. Specifically, MTR first evaluates each token’s importance using timescale parameter $\Delta$ and groups them according to importance level. We then merge the least important tokens with those in a specific grouping based on similarity to accomplish the compression process. Our approach is generalizable across tasks and applicable to any Mamba-based model. To the best of our knowledge, we are the first to explore Mamba structure-aware token evaluation scores and to propose a training-free Mamba token reduction framework. Empirically, our method can significantly reduce computational demands while maintaining competitive accuracy without any retraining. We summarize our contributions as follows:

• We identified that directly applying existing token reduction techniques from ViTs to Vision Mamba leads to significant performance degradation. Analysis revealed two primary reasons. First, Mamba is a sequential model, and token order significantly impacts performance, which can be mitigated by reordering tokens. Second, existing token reduction methods rely on attention mechanisms, which Mamba lacks. To address this, we explored Mamba’s internal ”attention score” and found that the timescale parameter $\Delta$ can effectively assess token im

portance.

• Based on our exploration, we developed a trainingfree Mamba token reduction framework MTR. MTR first evaluates token importance using Mamba structureaware scores, followed by asymmetric grouping based on the computed importance. Finally, it merges the least important tokens with those in a specific grouping to achieve token reduction.   
• Extensive experiments show that MTR significantly reduces computational workload while maintaining competitive accuracy across various tasks and multiple backbones. For instance, on the $\mathrm { V i m \mathrm { - } B }$ backbone, it reduces FLOPs by $40 \%$ with only a $1 . 6 \%$ drop in ImageNet performance, without retraining.

# Related Work

# Vision Mamba

Mamba (Gu and Dao 2023), an extension of state space model (SSM) (Gu, Goel, and R´e 2021; Smith, Warrington, and Linderman 2022; Mehta et al. 2022; Fu et al. 2022; Wang et al. 2023), has achieved excellent performance in NLP tasks. Its ability to capture long-range dependencies with linear computational complexity has led many researchers to adapt it for visual tasks (Chen et al. 2024; Guo et al. 2024; Hatamizadeh and Kautz 2024; Li et al. 2024; Patro and Agneeswaran 2024; Pei, Huang, and $\mathtt { X u 2 0 2 4 }$ ; Qiao et al. 2024; Ruan, Li, and Xiang 2024; Shi, Dong, and $\mathtt { X u 2 0 2 4 }$ ; Yang, Xing, and Zhu 2024; Zhan et al. 2024a; Behrouz, Santacatterina, and Zabih 2024). For example, ViM (Zhu et al. 2024) incorporates a bidirectional SSM module and constructs an isotropic architecture similar to ViT (Dosovitskiy 2020). VMamba (Liu et al. 2024) introduces a cross-scan module, creating a hierarchical SSMbased architecture. PlainMamba (Yang et al. 2024) enhances spatial continuity through continuous 2D scanning, ensuring token adjacency in the scanning sequence. LocalMamba (Huang et al. 2024) uses a local scanning strategy to capture local dependencies. However, most of these studies focus on Mamba’s structure and scanning mechanisms, with limited exploration of model inference efficiency. Our proposal effectively accelerates Vision Mamba’s inference through token reduction, offering a simple, training-free, and plug-and-play solution for various Mamba-based models.

# Token Reduction

Token reduction is a highly effective strategy to enhance computational efficiency by reducing the number of processed tokens or patches. It has shown significant potential in accelerating Transformers in both natural language processing (Goyal et al. 2020; Kim and Cho 2020; Kim et al. 2022) and computer vision (Fayyaz et al. 2022; Meng et al. 2022; Rao et al. 2021; Song et al. 2022; Yin et al. 2022; Bolya et al. 2022; Kong et al. 2022; Dou et al. 2023; Marin et al. 2021; Ryoo et al. 2021; Xu et al. 2022; Shang et al. 2024; Shen et al. 2025; Xu et al. 2025). For example, EViT (Liang et al. 2022) identifies informative tokens based on the [CLS] token, thereby simplifying the training process. PuMer (Cao, Paranjape, and Hajishirzi 2023) introduced a token reduction framework for large-scale vision-language models (VLMs) that employs text-informed pruning and modality-aware merging strategies to progressively reduce the number of input image and text tokens. ToMe (Bolya et al. 2022) determines token redundancy by measuring the dot product similarity between token keys and merges tokens accordingly.

However, token reduction techniques remain largely unexplored in Mamba. As a sequence model lacking the attention mechanism found in transformers, Mamba is not directly compatible with existing transformer-based token reduction methods. To our knowledge, HSA (Zhan et al. 2024a) was the first to investigate token compression in Vision Mamba, achieving this through importance-based token cropping and retraining. Nonetheless, the exploration of an “attention score” in Mamba and the development of a training-free approach remain uncharted territories. Our method not only clarifies why prior token reduction techniques are unsuitable for Mamba but also thoroughly examines the “attention score” for assessing token importance within the Mamba framework. Furthermore, we introduce a simple, effective, and training-free solution that both accelerates and restores the performance of compressed Mamba models.

# Methodology

# Preliminary

The classical state space model (SSM) is a continuous system that employs an implicit hidden state $h ( t ) \in \mathbb { R } ^ { N \times 1 }$ to transform a 1-D sequence input $x ( t ) \in \mathbb { R }$ into an output $y ( t ) \in \mathbb { R }$ , which can be written as follows:

$$
\begin{array} { r } { h ^ { \prime } ( t ) = \mathbf { A } h ( t ) + \mathbf { B } x ( t ) , } \\ { y ( t ) = \mathbf { C } h ( t ) + \mathbf { D } x ( t ) , \quad } \end{array}
$$

where ${ \bf A } \in { \bf R } ^ { N \times N }$ denotes the evolution matrix, while $\mathbf { B } \in$ ${ \mathbf { R } } ^ { N \times 1 }$ and ${ \bf C } \in { \bf R } ^ { 1 \times N }$ serve as the projection parameters, and the skip connection $\mathbf { D } \in \mathbb { R }$ .

SSM faces great challenges when integrated into deep learning algorithms due to its continuous-time nature. To be effectively applied to deep neural networks, SSM must first be transformed into their discrete counterparts through zero-order hold (ZOH) discretization. Specifically, the continuous parameters $\mathbf { A } , \mathbf { B }$ are converted into their discretized versions $\overline { { \mathbf { A } } } , \overline { { \mathbf { B } } }$ using a timescale parameter $\Delta \in \mathbb { R }$ :

$$
\begin{array} { r l } & { \overline { { \mathbf { A } } } = \exp ( \Delta \mathbf { A } ) , } \\ & { \overline { { \mathbf { B } } } = ( \Delta \mathbf { A } ) ^ { - 1 } ( \exp ( \Delta \mathbf { A } ) - \mathbf { I } ) \cdot \Delta \mathbf { B } . } \end{array}
$$

After obtaining the discretized $\overline { { \mathbf { A } } }$ and $\overline { { \mathbf { B } } }$ , the discrete SSM rewrite Eq. 1 as follows:

$$
\begin{array} { l } { { h _ { t } = \overline { { { \bf A } } } h _ { t - 1 } + \overline { { { \bf B } } } x _ { t } , } } \\ { { y _ { t } = { \bf C } h _ { t } + { \bf D } x _ { t } . } } \end{array}
$$

Mamba (Gu and Dao 2023) enhances the SSM by introducing selection, thereby proposing the selective state space model. In this model, the parameters ${ \bf \delta B } , { \bf C } , \Delta$ are directly derived from the input data $\boldsymbol { x } _ { t }$ , making them inputdependent parameters $\mathbf { B } _ { t } , \mathbf { C } _ { t } , \Delta _ { t }$ . Consequently, the discretized parameters $\overline { { \mathbf { A } } } _ { t } = \exp ( \Delta _ { t } \mathbf { A } )$ , $\overline { { \mathbf { B } } } _ { t } = \Delta _ { t } \mathbf { B } _ { t }$ are also input-dependent. The selective state space model is formulated as:

$$
\begin{array} { r l } & { { h } _ { t } = \overline { { { \bf A } } } _ { t } h _ { t - 1 } + \overline { { { \bf B } } } _ { t } x _ { t } , } \\ & { { y } _ { t } = { \bf C } _ { t } h _ { t } + { \bf D } x _ { t } . } \end{array}
$$

Mamba practically sets $\mathbf { A } , \overline { { \mathbf { A } } } _ { t }$ as diagonal matrices. Therefore, $\overline { { \mathbf { A } } } _ { t } \dot { h } _ { t - 1 } = \widetilde { \mathbf { A } } _ { t } \odot h _ { t - 1 }$ , where $\odot$ denotes the Hadamard product, and $\widetilde { \mathbf { A } } _ { t } = \mathrm { d i a g } ( \overline { { \mathbf { A } } } _ { t } ) \in \mathbb { R } ^ { N \times 1 }$ represents the matrix composedeof the diagonal elements of $\overline { { \mathbf { A } } } _ { t }$ . Additionally, given $\overline { { \mathbf { B } } } _ { t } = \Delta _ { t } \mathbf { B } _ { t }$ with $\Delta _ { t } \in \mathbb { R }$ , we have:

$$
\begin{array} { r } { \overline { { \mathbf { B } } } _ { t } x _ { t } = \Delta _ { t } \mathbf { B } _ { t } x _ { t } = \mathbf { B } _ { t } \big ( \Delta _ { t } \odot x _ { t } \big ) . } \end{array}
$$

Similarly, $ { \mathbf Ḋ x Ḍ } _ { t } =  { \mathbf Ḋ \odot Ḍ } x _ { t }$ . Consequently, we can rewrite Eq. 4 as:

$$
\begin{array} { r l } & { h _ { t } = \widetilde { \mathbf { A } } _ { t } \odot h _ { t - 1 } + \mathbf { B } _ { t } ( \Delta _ { t } \odot x _ { t } ) , } \\ & { y _ { t } = \mathbf { C } _ { t } h _ { t } + \mathbf { D } \odot x _ { t } , } \end{array}
$$

where $\mathbf { B } _ { t } , \mathbf { C } _ { t } , \Delta _ { t }$ are all derived from the input. Specifically, Mamba uses the following formulas to generate these parameters: $\mathbf { B } _ { t } ~ = ~ ( x W _ { B } ) ^ { \top }$ , $\mathbf { C } _ { t } ~ = ~ x W _ { C }$ , $\Delta _ { t } \ =$ Softplus $( x W _ { 1 } W _ { 2 } )$ , where $W _ { B }$ , $W _ { C }$ , $W _ { 1 }$ and $W _ { 2 }$ serve as projection matrices.

# Assessing Token Importance in Vision Mamba

As previously stated, we aim to explore the “attention score” in Mamba, which measures token importance without requiring additional computation. Given that $\begin{array} { r l } { \mathbf { B } _ { t } } & { { } = } \end{array}$ $\begin{array} { r c l } { ( x W _ { B } ) ^ { \top } , \mathbf { C } _ { t } ^ { \top } } & { = } & { x W _ { C } } \end{array}$ , $\begin{array} { r l r } { \Delta _ { t } } & { { } = } & { \mathrm { S o f t p l u s } ( x W _ { 1 } W _ { 2 } ) } \end{array}$ , $\mathbf { B } _ { t } , \mathbf { C } _ { t } , \Delta _ { t }$ , all these parameters are derived from the input $x$ and can serve as token importance assessment scores without incurring extra computational costs. Further analysis reveals that $\Delta _ { t }$ can be viewed as an input gate that modulates the weight of the current input token $\mathbf { \Psi } _ { x _ { t } }$ (Han et al. 2024). Specifically, a larger $\Delta _ { t }$ indicates greater focus on the current input, whereas a smaller $\Delta _ { t }$ suggests more reliance on historical memory. The properties of $\Delta _ { t }$ align well with the desired characteristics of an importance score, and thus, we select $\Delta _ { t }$ to evaluate token importance in this study. Additionally, we demonstrate the superiority of $\Delta _ { t }$ over other indicators $( \mathbf { B } _ { t } , \mathbf { C } _ { t }$ , etc.) through ablation experiments in Section .

For the ${ l ^ { t h } }$ layer of the Vision Mamba model, the input token sequence $\bar { x ^ { l } } \in \mathbb { R } ^ { B \times L \times D }$ is transformed into the output $y ^ { l } \in \mathbb { R } ^ { \mathbf { \mathring { B } } \times L \times D }$ us∈ing the following formulation:

$$
y ^ { l } = L i n e a r ^ { T } ( \sum _ { s \in \mathbf { S } } S S M _ { s } ( x ^ { l } ) ) ,
$$

where $S$ denotes the set of scanning heads. For simplicity, residual connections are omitted here. Each scanning head represents a distinct SSM module with a specific scanning pattern; for instance, ViM (Zhu et al. 2024) employs two scanning heads: forward and backward. To quantify the importance of each token, we first aggregate $\Delta _ { t }$ across scanning heads, resulting in $\begin{array} { r } { \Delta ^ { l } = \sum _ { s \in \mathbf { S } } \bar { \Delta } _ { t s } } \end{array}$ . Given that SSM leverages its extensive channel capacity to enable a more nuanced attention distribution, thereby enhancing the model’s ability to discern subtle features and interactions among tokens (Zhan et al. 2024a), we compute the average of $\Delta ^ { l }$ across the last dimension (i.e., the feature dimension $D$ ) to evaluate token importance:

![](images/78b3cf2144058ab5bf492556cd35f2ab8157549d72934e9b0ec6e33b7284ea8e.jpg)  
Figure 2: Overview of our proposed framework MTR. Image tokens are processed by the Mamba block and subsequently sorted in descending order according to their importance scores. The tokens are divided into three categories: ’Keep’, ’Target’, and ’Source’. ’Source’ tokens are merged with the most similar ’Target’ tokens based on feature similarity. Finally, the remaining tokens are sorted by their original index order and passed to the next Mamba block for further processing.

$$
s ^ { l } = \frac { \sum _ { d = 1 } ^ { D } \Delta _ { d } ^ { l } } { D } .
$$

We employ $s ^ { l } \in \mathbb { R } ^ { B \times L \times 1 }$ as the token importance indicator corresponding to $B \times L$ tokens to guide the reduction process.

# Importance-Based Token Grouping and Compression

Based on the importance scores calculated in Eq. 8, we evaluate the significance of different tokens to enable effective token reduction. Accordingly, we have designed a training-free framework, MTR, as illustrated in Fig. 2. We progressively compress the number of tokens by integrating the MTR module after the Mamba block. Specifically, each MTR module first uses the importance scores from Eq. 8 to assess each token and then classifies them into three groups: ’keep,’ ’target,’ and ’source,’ ordered by importance from highest to lowest. Tokens in the ’source’ group are merged with those in the ’target’ group, while the critical tokens in the ’keep’ group remain unchanged. Through this asymmetric grouping, we both protect the core knowledge from alteration and reduce computational costs in similarity calculations. Fig. 3 vividly demonstrates the superiority of our grouping method over other methods. For simplicity, we set the token ratios for the ’keep’ and ’source’ groups to $k \%$ , and the ’target’ group to $1 - 2 k \%$ . Here, $k$ is a reduction parameter determined by the desired compression ratio. Subsequently, MTR merges the ’source’ tokens with the most similar tokens in the ’target’ group. Finally, the remaining tokens are reordered according to their original sequential positions. The token reduction algorithm is described in Algorithm 1.

Notably, according to the above design, token reduction can be performed at any layer, and the grouping ratio $k$ is fixed across all layers. $k$ is not a hyperparameter that requires manual adjustment but is determined by the desired compression ratio.

# Experiments

# Implementation Details

We conducted comprehensive experiments on the ImageNet-1K (Deng et al. 2009) classification task, reporting top-1 accuracy $( \% )$ . ViM (Zhu et al. 2024) and VideoMamba (Li et al. 2024) are used as baseline Mamba models. Given that our method is training-free, we adopted the inference techniques from prior work (Zhan et al. 2024b) and applied varying FLOPS reduction ratios to the models to validate our approach’s effectiveness. All experiments are performed on four NVIDIA V100 GPUs.

Algorithm 1: Token Reduction Process

Input: token sequence $y ^ { l - 1 }$ , importance scores $s ^ { l - 1 }$ , token reduction ratio $k$ .

Output: token sequence $x ^ { l }$

1: Sort the token sequence $y ^ { l - 1 }$ in descending order based on importance scores $s ^ { l - 1 }$ ;   
2: Calculate the number of tokens in the ’target’ group $k ^ { \prime }$ , $k ^ { \prime } = ( 1 - 2 k ) | y ^ { l - 1 } |$ ;   
3: Divide the tokens into three groups: ’keep’ $\mathbf { K }$ , ’target’ $\mathbf { T }$ and ’source’ S.   
4: Merge ’source’ tokens $\mathbf { S }$ into ’target’ tokens $\mathbf { T }$ using bipartite soft matching: $\mathbf { T } =$ bipartite merge $( \mathbf { S } , \mathbf { T } )$ ;   
5: Aggregate and reorder the remaining tokens: $\begin{array} { r l } { x ^ { l } } & { { } = } \end{array}$ $r e o r d \bar { e } r ( c o n c a t ( { \bf K } , { \bf T } ) )$   
6: procedure BIPARTITE MERGE( S, T)   
7: For each token $\mathbf { S } _ { a }$ in S, compute its top-1 similar token $\mathbf { T } _ { b }$ in $\mathbf { T }$ , save the indices $a$ and $b$ into a token edge (an edge between $\mathbf { S } _ { a }$ and $\mathbf { T } _ { b }$ ), store all token edges in a set $\mathbf { P }$   
8: For each token edge $( a , b )$ in $\mathbf { P }$ , collect tokens from S and $\mathbf { T }$ connected by the edge, merge these tokens by computing the mean of their token vectors   
9: output: merged tokens $\mathbf { T }$   
10: end procedure

# Comparison Methods

Following previous studies (Zhan et al. 2024b,a), we compare our method with PuMer (Cao, Paranjape, and Hajishirzi 2023) and EViT (Liang et al. 2022), two representative transformer token reduction methods. To date, no training-free Vision Mamba token reduction methods have been developed. Consequently, we compare with two existing Mamba token reduction methods: UTR (Zhan et al. 2024b) and HSA (Zhan et al. 2024a). UTR is designed for NLP tasks, while HSA is a Mamba token pruning method that involves retraining. To ensure fair comparisons, we evaluate these methods in a training-free setting. Notably, we also compare our method with state-of-the-art token reduction methods in ViT and include comparisons on other tasks in the Appendix.

# Main Results

Evaluation on ViM. As shown in Table 1, we compare the performance of MTR with baseline methods on the ViM backbone. To ensure a fair comparison, all methods perform token reduction followed by token reordering. It is evident that MTR consistently outperforms all baselines under the same FLOPS reduction ratios. Notably, with a $40 \%$ FLOPS reduction on the ViM-S backbone, MTR outperforms UTR and HSA by $3 . 9 \%$ and $4 . 2 \%$ , respectively. For the more robust ViM-B backbone, the performance drop due to token reduction is relatively smaller. Even so, our approach still has a significant advantage over other methods. For instance, at a $40 \%$ FLOPS reduction ratio, our method only decreases by $1 . 6 \%$ , while UTR and HSA decrease by $3 . 9 \%$ and $4 . 2 \%$ , respectively.

Evaluation on VideoMamba. In Table 2, we present the performance of our method compared to baseline methods on the VideoMamba backbone. Consistent with previous findings, MTR outperforms all baselines, further demonstrating the effectiveness of our approach. Notably, the EViT method underperforms compared to other methods in most cases, primarily because it relies on the attention mechanism for importance measurement, which, as discussed earlier, leads to significant performance degradation when applied to Mamba.

Table 1: Main results of the training-free performance on ViM-S and ViM-B. We compared our method with baseline token reduction methods and evaluated them on the ImageNet-1K dataset under $20 \%$ , $30 \%$ , and $40 \%$ FLOPS reduction.   

<html><body><table><tr><td>Method</td><td>FLOPS Reduction</td><td>Params (M)</td><td>Top-1 Acc.(%)</td><td>△</td></tr><tr><td>ViM-S</td><td>0%</td><td>26</td><td>80.5</td><td>0.0</td></tr><tr><td>+EViT</td><td></td><td>26</td><td>75.8</td><td>4.7↓</td></tr><tr><td>+ PuMer</td><td rowspan="4">20%</td><td>26</td><td>76.9</td><td>3.6↓</td></tr><tr><td>+UTR</td><td>26</td><td>77.3</td><td>3.2↓</td></tr><tr><td>+ HSA</td><td>26</td><td>76.7</td><td>3.8↓</td></tr><tr><td>+ MTR</td><td>26</td><td>78.8</td><td>1.7↓</td></tr><tr><td></td><td rowspan="4"></td><td></td><td></td><td>8.7↓</td></tr><tr><td>+EViT + PuMer</td><td>26 26</td><td>71.8 74.6</td><td>5.9↓</td></tr><tr><td>+ UTR</td><td>26</td><td>75.0</td><td>5.5↓</td></tr><tr><td>+ HSA</td><td>26</td><td>74.8</td><td>5.7↓</td></tr><tr><td>+ MTR</td><td></td><td>26</td><td>77.7</td><td>2.8↓</td></tr><tr><td>+EViT</td><td rowspan="4">40%</td><td>26</td><td>64.8</td><td>15.7↓</td></tr><tr><td>+ PuMer</td><td>26</td><td>69.1</td><td>11.4↓</td></tr><tr><td>+UTR</td><td>26</td><td>71.5</td><td>9.0↓</td></tr><tr><td>+ HSA</td><td>26</td><td>71.2</td><td>9.3↓</td></tr><tr><td>+ MTR</td><td></td><td>26</td><td>75.4</td><td>5.1↓</td></tr><tr><td>ViM-B</td><td>0%</td><td>98</td><td>81.9</td><td>0.0</td></tr><tr><td>+EViT</td><td></td><td>98</td><td>80.4</td><td>1.5↓</td></tr><tr><td>+ PuMer</td><td rowspan="4">20%</td><td>98</td><td>79.9</td><td>2.0↓</td></tr><tr><td>+UTR</td><td>98</td><td>80.4</td><td>1.5↓</td></tr><tr><td>+HSA</td><td>98</td><td>80.1</td><td>1.8↓</td></tr><tr><td>+ MTR</td><td>98</td><td>81.2</td><td>0.7↓</td></tr><tr><td>+EViT</td><td></td><td></td><td></td><td></td></tr><tr><td>+PuMer</td><td rowspan="4">30%</td><td>98</td><td>78.9</td><td>3.0↓</td></tr><tr><td></td><td>98</td><td>78.9</td><td>3.0↓</td></tr><tr><td>+UTR</td><td>98</td><td>79.2</td><td>2.7↓</td></tr><tr><td>+HSA</td><td>98</td><td>79.1</td><td>2.8↓</td></tr><tr><td>+ MTR</td><td></td><td>98</td><td>81.0</td><td>0.9↓</td></tr><tr><td>+ EViT + PuMer</td><td rowspan="4">40%</td><td>98</td><td>75.9</td><td>6.0↓</td></tr><tr><td></td><td>98</td><td>76.8</td><td>5.1↓</td></tr><tr><td>+ UTR</td><td>98</td><td>78.0</td><td>3.9↓</td></tr><tr><td>+ HSA</td><td>98</td><td>77.7</td><td>4.2↓</td></tr><tr><td> + MTR</td><td></td><td>98</td><td>80.3</td><td>1.6↓</td></tr></table></body></html>

# Ablation Studies.

Analysis on importance indicator. To comprehensively evaluate the most effective token importance measures in Mamba, we explored various importance indicators, as shown in Table 3. Clearly, $\Delta _ { t }$ outperforms other indicators when used as an importance indicator, which aligns with our previous analysis. Additionally, using $\mathbf { B } _ { t }$ as an importance indicator intuitively yields good performance; as in Eq. 6, both $\mathbf { B } _ { t }$ and $\Delta _ { t }$ directly influence the sequence input $\scriptstyle { \boldsymbol { x } } _ { t }$ , providing a better measure of token importance. We believe that jointly considering $\mathbf { B } _ { t }$ and $\Delta _ { t }$ could offer an even better measure of token importance in Mamba, and we leave this exploration for future work. Furthermore, the [CLS] token, an effective indicator of token importance in transformers (Wang et al. 2024; Haurum et al. 2023; Zhang et al. 2024), underperforms in the Mamba model. We speculate this is because Mamba is a sequential model, and token positions affect token similarity. For instance, tokens neighboring the [CLS] token naturally exhibit higher similarity, which is unlike in transformers.

<html><body><table><tr><td>Method</td><td>FLOPS Reduction</td><td>Params (M)</td><td>Top-1 Acc. (%)</td><td>△</td></tr><tr><td>VideoM-S</td><td>0%</td><td>26</td><td>81.2</td><td>0.0</td></tr><tr><td>+EViT</td><td rowspan="4">20%</td><td>26</td><td>78.2</td><td>2.8↓</td></tr><tr><td>+ PuMer</td><td>26</td><td>78.4</td><td>3.0↓</td></tr><tr><td>+UTR</td><td>26</td><td>78.9</td><td>2.3↓</td></tr><tr><td>+HSA</td><td>26</td><td>79.0</td><td>2.2↓</td></tr><tr><td>+ MTR</td><td></td><td>26</td><td>80.2</td><td>1.0↓</td></tr><tr><td>+EViT</td><td rowspan="4">30%</td><td>26</td><td>75.5</td><td>5.7↓</td></tr><tr><td>+PuMer</td><td>26</td><td>76.2</td><td>5.0↓</td></tr><tr><td>+UTR</td><td>26</td><td>77.1</td><td>4.1↓</td></tr><tr><td>+ HSA</td><td>26</td><td>77.2</td><td>4.0↓</td></tr><tr><td>+ MTR</td><td></td><td>26</td><td>79.0</td><td>2.2↓</td></tr><tr><td>+EViT</td><td rowspan="4">40%</td><td>26</td><td>70.2</td><td>11.0↓</td></tr><tr><td>+PuMer</td><td>26</td><td>71.1</td><td>10.1↓</td></tr><tr><td>+UTR</td><td>26</td><td>74.1</td><td>7.1↓</td></tr><tr><td>+HSA</td><td>26</td><td>74.0</td><td>7.2↓</td></tr><tr><td>+ MTR</td><td></td><td>26</td><td>76.6</td><td>4.6↓</td></tr><tr><td>VideoM-B</td><td>0%</td><td>98</td><td>82.7</td><td>0.0</td></tr><tr><td>+EViT</td><td></td><td>98</td><td>80.4</td><td>2.3↓</td></tr><tr><td>+PuMer</td><td rowspan="4">20%</td><td>98</td><td>81.8</td><td>0.9↓</td></tr><tr><td>+ UTR</td><td>98</td><td>82.0</td><td>0.7↓</td></tr><tr><td>+ HSA</td><td>98</td><td>82.0</td><td>0.7↓</td></tr><tr><td>+ MTR</td><td>98</td><td>82.4</td><td>0.3↓</td></tr><tr><td>+EViT</td><td></td><td>98</td><td>77.7</td><td>5.0↓</td></tr><tr><td>+PuMer</td><td rowspan="4">30%</td><td>98</td><td>80.5</td><td>2.2↓</td></tr><tr><td>+ UTR</td><td></td><td></td><td></td></tr><tr><td></td><td>98</td><td>81.0</td><td>1.7↓</td></tr><tr><td>+ HSA + MTR</td><td>98 98</td><td>81.2 81.7</td><td>1.5↓ 1.0↓</td></tr><tr><td>+EViT</td><td></td><td>98</td><td></td><td></td></tr><tr><td>+ PuMer</td><td rowspan="4">40%</td><td></td><td>73.7</td><td>9.0↓</td></tr><tr><td></td><td>98</td><td>78.4</td><td>4.3↓</td></tr><tr><td>+UTR</td><td>98</td><td>79.4</td><td>3.3↓</td></tr><tr><td>+ HSA</td><td>98</td><td>79.6</td><td>3.1↓</td></tr><tr><td>+ MTR</td><td></td><td>98</td><td>80.5</td><td>2.2↓</td></tr></table></body></html>

Analysis on reduction operation. Unlike previous approaches that treat the hidden state and residual in Mamba separately (Zhan et al. 2024b,a), our approach applies the same reduction strategy to both the hidden state and residual, ensuring simplicity and information consistency. Table 4 presents experiments on different reduction strategies. The results indicate that the merging strategy outperformed other reduction methods, as it minimizes information loss. Additionally, our results indicate that with stronger models or smaller reduction ratios, even the pruning strategy does not significantly impact performance, confirming that the filtered ’Source’ tokens are indeed unimportant. More experiments on reduction strategies are provided in the Appendix. Visualization. To further investigate the interpretability of MTR, we visualize the retained visual tokens in various scenarios in Fig. 3. We present the original images and the retained visual tokens of different methods. It can be observed that the red tokens in MTR essentially correspond to the most responsive regions in the CAM. This indicates that the tokens within our ‘Keep’ group align with the image’s core content. Moreover, foreground objects are primarily encompassed within red or blue tokens, while black tokens predominantly represent task-irrelevant regions. This suggests that MTR effectively retains category-specific tokens and excludes irrelevant background tokens. It is worth noting that UTR and HSA, which are also importance-based token reduction methods, still exclude some tokens related to the foreground.

Table 3: Ablation study on the impact of different indicator choices on top-1 accuracy $( \% )$ . $\mathbf { X } _ { t }$ means using hidden state features as importance indicator. [CLS] indicates that we use the similarity between the [CLS] token and other tokens as an importance assessment. The timescale parameter $\Delta _ { t }$ offers a better measure of token importance than other indicators.   

<html><body><table><tr><td>Model</td><td>Indicator</td><td>20% Reduction</td><td>30% Reduction</td><td>40% Reduction</td></tr><tr><td rowspan="2">ViM-S</td><td>[CLS]</td><td>77.0</td><td>74.4</td><td>68.8</td></tr><tr><td>Xt</td><td>77.9</td><td>76.1</td><td>72.6</td></tr><tr><td rowspan="2"></td><td>Ct Bt</td><td>78.1 78.1</td><td>76.5 77.1</td><td>73.3</td></tr><tr><td>△t</td><td>78.8</td><td>77.7</td><td>74.5 75.4</td></tr><tr><td></td><td>[CLS]</td><td>80.9</td><td>79.8</td><td>77.8</td></tr><tr><td rowspan="2">ViM-B</td><td>Xt</td><td>80.5</td><td>80.3</td><td>79.5</td></tr><tr><td>Ct</td><td>80.7</td><td>80.0</td><td>78.8</td></tr><tr><td rowspan="2"></td><td>Bt</td><td>80.7</td><td>80.1</td><td>79.2</td></tr><tr><td>△t</td><td>81.2</td><td>81.0</td><td>80.3</td></tr></table></body></html>

Table 2: Main results of the training-free performance on VideoMamba-S and VideoMamba-B. We compared our method with baseline token reduction methods and evaluated them on the ImageNet-1K dataset under $20 \%$ , $30 \%$ , and $40 \%$ FLOPS reduction.   
Table 4: Ablation study of different reduction choices on top-1 accuracy $( \% )$ . Pruning involves directly removing tokens from the ’Source’ group, while Merging refers to our method of combining ’Source’ tokens with those in the ’Target’ group. Hybrid combines both Pruning and Merging methods; for simplicity, we allocate $50 \%$ of the tokens to each strategy.   

<html><body><table><tr><td>Model</td><td>Strategy</td><td>20% Reduction</td><td>30% Reduction</td><td>40% Reduction</td></tr><tr><td rowspan="3">ViM-S</td><td>Pruning</td><td>78.5</td><td>77.1</td><td>74.0</td></tr><tr><td>Hybrid</td><td>78.6</td><td>77.5</td><td>74.6</td></tr><tr><td>Merging</td><td>78.8</td><td>77.7</td><td>75.4</td></tr><tr><td rowspan="3">ViM-B</td><td>Pruning</td><td>81.1</td><td>80.8</td><td>80.1</td></tr><tr><td>Hybrid</td><td>81.2</td><td>80.9</td><td>80.3</td></tr><tr><td>Merging</td><td>81.2</td><td>81.0</td><td>80.3</td></tr></table></body></html>

![](images/d3707f1b6b8b4ffbceb06d88244bed6250b03b69dc7f466d918e6c61bddc4e77.jpg)  
Figure 3: Visualization of reduction tokens on ViM-S under $20 \%$ overall reduction of FLOPS. We present visualizations of the original image and the corresponding image after token reduction for each method. The masked regions represent the reduction tokens. For our MTR, the red tokens indicate those in the ’Keep’ group, while the blue tokens indicate the ’Target’ group, and the masked tokens represent the ’Source’ group. We also display Class Activation Maps (CAM) in the rightmost column. Notably, the yellow and blue areas in the CAM diagram indicate highly responsive regions, while the red areas indicate low responsive regions.

![](images/a28437c40f11147c286c6ef5b8adb89a4c57eb3320b91b2f48a87d2f68301e2f.jpg)  
Figure 4: Comparison of generation throughput across different FLOPS reduction ratios for ViM-S and ViM-B.

Inference throughput. As our approach compresses the input token number, we can accelerate inference and achieve higher model throughput, as illustrated in Fig. 4. The throughput increases with the reduction ratio. By adjusting the reduction ratio, we can choose to prioritize model performance, inference speed, or a balance of both.

# Conclusion

In conclusion, this paper introduces a training-free Mamba token reduction framework, MTR, addressing the incompatibility of existing Vision Transformer (ViT) token reduction methods with Mamba, which lacks attention mechanisms and relies on token order. To solve these challenges, MTR leverages Mamba’s internal timescale parameter $\Delta$ to assess token importance, groups tokens by importance into ’Keep,’ ’Target,’ and ’Source’ categories, and merges similar tokens while preserving order. The proposed MTR framework can be easily adapted to Mamba models without introducing additional parameters or requiring a training process. Extensive experiments demonstrate that MTR achieves state-ofthe-art performance across various benchmarks and significant inference acceleration, underscoring its superiority.