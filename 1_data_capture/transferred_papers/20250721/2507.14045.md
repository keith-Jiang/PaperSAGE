# Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks

Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Xiangji Huang York University Toronto, Ontario, Canada

# Abstract

This paper presents a comprehensive evaluation of cost-efficient Large Language Models (LLMs) for diverse biomedical tasks spanning both text and image modalities. We evaluated a range of closed-source and open-source LLMs on tasks such as biomedical text classification and generation, question answering, and multimodal image processing. Our experimental findings indicate that there is no single LLM that can consistently outperform others across all tasks. Instead, different LLMs excel in different tasks. While some closed-source LLMs demonstrate strong performance on specific tasks, their open-source counterparts achieve comparable results (sometimes even better), with additional benefits like faster inference and enhanced privacy. Our experimental results offer valuable insights for selecting models that are optimally suited for specific biomedical applications.

Keywords: Large Language Models, LLM, Multimodal, Biomedical, LLM Evaluation.

# 1. Introduction

Large Language Models (LLMs) have demonstrated impressive capabilities across various domains [1], including biomedicine [2]. Recently, the capability of LLMs from only understanding textual data has been further extended, allowing them to understand multimodal data [3]. These improved capabilities of LLMs have made it possible to utilize them in various real-world biomedical applications. In biomedicine, LLMs have the potential to perform critical tasks like drug discovery, disease diagnosis, radiology report generation, etc [2, 3].

However, despite the potential of AI to revolutionize biomedicine, the utilization of LLMs in real-world biomedical settings is very limited due to the high cost (e.g., computing resources, API cost, data annotation) associated with LLM development and deployment [3]. Moreover, sharing sensitive data externally for model development and inference raises privacy concerns, necessitating secure pipelines, which further increases costs.

To this end, this paper aims to study how to make LLMs more efficient and cost-effective while retaining their accuracy in performing diverse biomedical tasks in practical scenarios. This would require an extensive evaluation of the smaller LLMs that are currently available to study their capabilities and limitations in benchmark biomedical datasets and tasks. Our hypothesis is that while larger LLMs may generally exhibit superior performance, strategically chosen smaller LLMs can offer a compelling balance of performance and efficiency.

By benchmarking the performance of the cost-efficient open-source and closed-source models, this paper makes the following key contributions:

For open-source models, this would give insights on which models to select for further fine-tuning to make them more specialized in certain biomedical tasks. For closed-source models, in addition to identifying which one of them can be used in practical applications via their respective APIs, our findings will also be useful to select the right closed-source model for the development of specialized open-source models for biomedicine (i.e., using closed-source models for generating synthetic data for continual pre-training or instruction tuning of the open-source models). The findings from this research will provide valuable insights for researchers and practitioners seeking to deploy these models in the biomedical domain.

Biomedical TextProcessing Tasks Multimodal (Biomedical Image Processing) Tasks 1.HoC:Classification of biomedical text across1 1.ChEBl-20-MM: Molecule Image Captioning (3297 test samples) Example: of 10 HoC classes. [4947 test samples] 2. NCBl: Disease type NER on biomedical text. [940 test samples]. Caption:ThemoleculeisadipeptidecomposedofL-asparticacidand 3.PubMedQA:Question Answering on PubMed L-argininejoinedbyapeptidelinkage. articles. [500 test samples] 2.PathvQA: Pathology ImageQuestion Answering [3659 test samples] 4.MediQA-QS:Summarization of Questions in Example: health forums.[100 test samples] 5.MediQA-Ans: Summarization of Answers to Question:Isremotekidneyinfarctreplacedbyalargefibroticscar. health questions.[552 test samples] Answer:Yes

# 2. Related Work

In recent years, the utilization of pre-trained transformer-based models by fine-tuning on task-specific biomedical datasets have demonstrated state-of-the-art performance in a wide range of biomedical text processing tasks [2]. However, one major limitation of using such pre-trained biomedical models is that they do not have instruction-following capability and also require task-specific large annotated datasets for fine-tuning, which is significantly less available in the biomedical domain [2]. In this regard, having a strong zero-shot model with instruction-following capability could potentially alleviate the need for large annotated datasets, enabling the model to perform well in tasks that it was not explicitly trained on.

While prior research has demonstrated that LLMs can outperform the state-of-the-art fine-tuned biomedical models even in zero-shot scenarios on biomedical datasets that have smaller training sets [2], the evaluation was predominantly focused on earlier generation LLMs (e.g., GPT-3.5 [6], Claude-2 [7], LLaMA-2-13B [8], and PaLM-2 [9]). Moreover, there is a lack of comprehensive evaluation of LLMs in multimodal biomedical tasks. While numerous newly proposed LLMs have demonstrated multimodal capabilities, a comprehensive benchmarking of these new LLMs in biomedicine across multimodal tasks is still missing [3]. In addition, prior research has also ignored the importance of computational efficiency which is a crucial factor for practical deployment of LLMs [10].

To address the above issues, in this paper, we provide a comprehensive evaluation of efficient LLMs that require lower computational resources. Our extensive experiments of these cost-effective LLMs across diverse biomedical tasks (both text data and images) would provide critical insights on their applicability in real-world clinical settings.

# 3. Methodology

# 3.1. Datasets and Tasks

For evaluation, we use biomedical tasks from diverse modalities (also see Figure 1):

(i) Tasks for Biomedical Text Data: This category consists of the tasks that require the analysis of biomedical text data. We use the Hallmarks of Cancer [11] dataset for biomedical text classification across 1 of the 10 HoC classes, the NCBI-disease [12] named entity recognition (NER) dataset for biomedical entity extraction (disease entity), the PubMedQA [13] dataset for biomedical question answering, the MediQA-QS [14] dataset for medical question summarization and the MediQA-ANS [15] dataset for medical answer summarization. (ii) Tasks for Biomedical Image Data (Multimodal): We use the ChEBI-20-MM [4] dataset for caption generation from molecular images and the PathVQA [5] dataset for question answering from pathology images. For PathVQA, we use its binary Yes/No type question answering subset since the other subset that requires open-ended answer generation is quite similar to the caption generation task.

# 3.2. Prompt Construction

Prompts are essential for interacting with LLMs. For biomedical text processing, we use prompts from Jahan et al. [2]. In biomedical image processing, tasks often require minimal prompt engineering. For example, a simple prompt—“Generate a descriptive caption of the molecular structure image”—works well for molecular captioning, which we also use. In PathVQA, we use dataset-provided questions as the prompt.

# 3.3. Models

We primarily use the cost-efficient LLMs currently available, considering their real-world applicability. Therefore, for closed-source LLMs, we use: (i) GPT-4o-Mini [6], (ii) Gemini1.5-Flash [16], (iii) Claude-3-Haiku [7]. All these closed-source models have multimodal capabilities. For the open-source LLMs, we select models having fewer than 13B parameters. For text-based tasks, we select the instruction-tuned version of respective open-source models such that they can properly follow the instructions in the prompt: (iv) LLaMA3.1-8B-Instruct [17], (v) Qwen-2.5-7B-Instruct [18], (vi) Mistral-7B-v0.3-Instruct [19], and (vii) Phi-3.5-Mini-3.8B-Instruct [20]. With the recent success of reasoning-based LLMs like DeepSeek-R1 [21], we also use its distilled versions based on Qwen and LLaMA, (vii) DeepSeek-R1-Distill-Qwen-7B and (viii) DeepSeek-R1-Distill-LLaMA-8B, respectively. For image-based tasks using open-source models, we select: Phi-3.5-Vision [20], Qwen-2-VL [22], LLaVA-Next 1 based on Mistral-7B [19], Janus-Pro [23], and LLaMA-3.2-11B-Vision2.

The inference of each model was conducted by leveraging zero-shot prompts (as described in Section 3.2) on a machine with 1 NVIDIA A100 GPU. The temperature value was set to 1.0, with other decoding parameters being set to the default values in the respective API providers for the closed-source models and in HuggingFace $^ 3$ for the open-source models.

# 3.4. Evaluation

For classification and information extraction tasks, a parsing script is required to first extract answers from the LLM-generated responses to compare against gold labels [24]. Afterwards, their performance is measured using dataset-specific metrics like Accuracy, Precision, Recall, and $F 1$ , which are commonly used in the literature [2].

For generative tasks (e.g., summarization or caption generation), parsing scripts are not required [24] and the full response generated by LLMs are compared against the gold reference. Similar to prior research [2], we use ROUGE [25] and BERTScore [26] metrics.

# 4. Results and Discussion

# 4.1. Performance in Biomedical Text Processing Tasks

We show the results of different models in HoC, PubMedQA and NCBI-Disease datasets in Table 1 and in MediQA-QS and MediQA-ANS datasets in Table 2. Based on the results, we find that there is not a single LLM that achieves the best result across all datasets.

For instance, GPT-4o-Mini achieves the best in HoC, whereas Gemini-1.5-Flash and Claude-3-Haiku achieve the best result in NCBI-Disease and PubMedQA datasets, respectively. In summarization, we find that Gemini-1.5-Flash has the best result in MediQA-QS while Claude-3-Haiku outperforming GPT-4o-Mini and Gemini-1.5-Flash in MediQA-ANS.

In terms of open-source LLMs, we find that they perform on par (and in some cases even better) than closed-source LLMs. For instance, Qwen-2.5-7B-Instruct even outperforms

Table 1. Results on HoC, PubMedQA, and NCBI-Disease datasets.   

<html><body><table><tr><td rowspan="2">Model</td><td>HoC</td><td>PubMedQA</td><td colspan="3">NCBI-Disease</td></tr><tr><td>Accuracy</td><td>Accuracy</td><td>Precision</td><td>Recall</td><td>F1</td></tr><tr><td>GPT-4o-Mini</td><td>63.04</td><td>55.6</td><td>20.71</td><td>21.88</td><td>21.28</td></tr><tr><td>Gemini-1.5-flash</td><td>55.86</td><td>54.0</td><td>52.94</td><td>49.69</td><td>51.26</td></tr><tr><td>Claude-3-Haiku</td><td>52.48</td><td>61.6</td><td>18.54</td><td>27.29</td><td>22.08</td></tr><tr><td>Phi-3.5-Mini-3.8B-Instruct</td><td>49.45</td><td>58.4</td><td>6.81</td><td>28.23</td><td>10.98</td></tr><tr><td>Mistral-7B-v0.3-Instruct</td><td>49.47</td><td>57.2</td><td>4.41</td><td>21.98</td><td>7.35</td></tr><tr><td>Qwen-2.5-7B-Instruct</td><td>62.41</td><td>23.2</td><td>19.29</td><td>25.00</td><td>21.78</td></tr><tr><td>LLaMA-3.1-8B-Instruct</td><td>14.83</td><td>55.0</td><td>8.13</td><td>13.75</td><td>10.22</td></tr><tr><td>DeepSeek-R1-Distill-Qwen-7B</td><td>49.02</td><td>54.0</td><td>19.71</td><td>27.08</td><td>22.82</td></tr><tr><td>DeepSeek-R1-Distill-LLaMA-8B</td><td>52.68</td><td>59.6</td><td>10.02</td><td>23.54</td><td>14.06</td></tr></table></body></html>

<html><body><table><tr><td rowspan="2">Model</td><td colspan="4">MediQA-QS</td><td colspan="4">MediQA-ANS</td></tr><tr><td>R-1</td><td>R-2</td><td>R-L</td><td>B-S</td><td>R-1</td><td>R-2</td><td>R-L</td><td>B-S</td></tr><tr><td>GPT-4o-Mini</td><td>28.79</td><td>10.95</td><td>22.36</td><td>89.15</td><td>30.14</td><td>9.26</td><td>19.15</td><td>87.09</td></tr><tr><td>Gemini-1.5-Flash</td><td>33.25</td><td>12.50</td><td>27.65</td><td>89.85</td><td>28.44</td><td>8.75</td><td>19.50</td><td>86.87</td></tr><tr><td>Claude-3-Haiku</td><td>28.21</td><td>11.12</td><td>23.77</td><td>88.83</td><td>31.01</td><td>11.45</td><td>19.88</td><td>86.49</td></tr><tr><td>Phi-3.5-Mini-3.8B</td><td>28.49</td><td>10.29</td><td>22.89</td><td>89.07</td><td>25.63</td><td>7.12</td><td>15.39</td><td>85.65</td></tr><tr><td>Qwen-2.5-7B</td><td>25.84</td><td>8.79</td><td>19.87</td><td>88.11</td><td>27.58</td><td>8.71</td><td>18.04</td><td>86.25</td></tr><tr><td>Mistral-7B-v0.3</td><td>24.47</td><td>8.56</td><td>20.00</td><td>88.14</td><td>29.20</td><td>10.21</td><td>18.20</td><td>86.29</td></tr><tr><td>LLaMA-3.1-8B</td><td>24.15</td><td>7.76</td><td>18.58</td><td>87.37</td><td>32.55</td><td>13.28</td><td>22.11</td><td>86.29</td></tr><tr><td>DeepSeek-R1-Distill-Qwen-7B</td><td>23.16</td><td>8.94</td><td>18.47</td><td>87.64</td><td>26.29</td><td>6.69</td><td>16.26</td><td>85.94</td></tr><tr><td>DeepSeek-R1-Distill-LLaMA-8B</td><td>14.40</td><td>4.09</td><td>11.27</td><td>85.52</td><td>26.38</td><td>7.01</td><td>16.49</td><td>86.05</td></tr></table></body></html>

Table 2. Text Summarization Results. Here, ‘ROUGE-’ is ‘R-’ and ‘BertScore’ is ‘B-S’.

Gemini and Claude in HoC, while Phi-3.5 outperforms GPT-4o and Gemini in PubMedQA.   
Interestingly, LLaMA-3.1-8B achieves the best result across all models in MediQA-ANS.

None of the DeepSeek models could achieve the best result in any datasets, although they still achieve decent results. Among the DeepSeek models, we find that the DeepSeekDistilled model based on Qwen-7B performs better than LLaMA-8B in NCBI-Disease and MediQA-QS, the opposite happens in HoC, PubMedQA, and MediQA-ANS datasets.

With the performance of LLMs varying across datasets, LLMs can be chosen for finetuning or zero-shot inference based on their task-specific performance in different datasets.

# 4.2. Performance in Biomedical Image Processing (Multimodal) Tasks

We show the results for Molecular Image Captioning and Pathology Image Question Answering (QA) in Table 3. While performance in Molecular Image Captioning is quite similar for most LLMs (except LLaMA-3.2-11B-Vision), many LLMs perform quite poorly in PathVQA, with only Gemini-1.5-Flash and Janus-Pro-7B achieving more than 40% accuracy. While LLaMA-3.2-11B-Vision performs quite poorly in image captioning, it performs quite better in PathVQA (third best among 8 multimodal models). Among all models, Janus-Pro7B and Gemini-1.5-Flash achieve the most consistent results in both datasets, establishing themselves as a good choice for multimodal biomedical tasks.

Table 3. Results for Molecular Image Captioning and Pathology Image QA.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="3">Molecular Image Captioning</td><td rowspan="2">PathVQA</td></tr><tr><td>ROUGE-1</td><td>ROUGE-2</td><td>ROUGE-L</td></tr><tr><td>GPT-4o-Mini</td><td>19.24</td><td>2.06</td><td>12.69</td><td>8.27</td></tr><tr><td>Gemini-1.5-Flash</td><td>21.61</td><td>2.64</td><td>13.11</td><td>40.28</td></tr><tr><td>Claude-3-Haiku</td><td>22.03</td><td>2.67</td><td>15.01</td><td>8.49</td></tr><tr><td>Phi-3.5-Vision-4.2B</td><td>19.67</td><td>1.91</td><td>13.81</td><td>15.93</td></tr><tr><td>Qwen2-VL-7B</td><td>20.24</td><td>3.05</td><td>14.18</td><td>21.84</td></tr><tr><td>LLaVA-Next-7B</td><td>19.43</td><td>3.26</td><td>13.85</td><td>5.25</td></tr><tr><td>Janus-Pro-7B</td><td>21.23</td><td>3.51</td><td>14.29</td><td>41.19</td></tr><tr><td>LlaMA-3.2-11B-Vision</td><td>12.69</td><td>1.88</td><td>9.52</td><td>32.25</td></tr></table></body></html>

![](images/52c8f6d7d00f0732b5ae418a77d66662100e170e80338ccb0afa50da25bba7e2.jpg)  
Figure 2. Model Scaling Results on Multimodal QA (PathVQA), alongside Text-based Classification (HoC), NER (NCBI-Disease), and Summarization (MedQA-Ans).

# 4.3. Model Scaling Experiments

In this section, we conduct some model scaling experiments to investigate (i) can scaling up the model size for closed-source LLMs improves the performance (this will give us insights on whether larger closed-source LLMs can be utilized as a better synthetic data generator to train smaller open-source LLMs), and (ii) can scaling down the model size for open-source models retains their performance (this will provide insights on whether more cost-efficient models are reliable in real-world scenarios). For the closed-source LLMs, we select the worstperforming model in the respective dataset (see Figure 2) to investigate their performance with their larger counterpart: Gemini-1.5 (Flash vs Pro), GPT-4 (o-mini vs o), and Claude-3 (Haiku vs Opus). For the open-source LLMs, we compared the Qwen models of various sizes. From Figure 2, we find that scaling up the model size is always helpful for the closed-source models, while scaling down leads to a performance drop for open-source models.

# 5. Conclusion and Future Work

This study evaluates cost-efficient LLMs across diverse biomedical tasks, covering text and image modalities. With no single model consistently outperforming others, we observe the task-specific nature of existing LLMs in biomedicine. Notably, some open-source models match or surpass closed-source ones while offering efficiency and greater privacy. Our findings guide future research in selecting the right models for further training on complex tasks [27]. Expanding evaluations to broader biomedical datasets will also enhance our understanding of cost-efficient LLMs in practical healthcare applications [28].