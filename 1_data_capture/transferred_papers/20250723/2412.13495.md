# Federated t-SNE and UMAP for Distributed Data Visualization

Dong Qiao\*, Xinxian Ma\*, Jicong Fan†

School of Data Science, The Chinese University of Hong Kong, Shenzhen, China dongqiao@link.cuhk.edu.cn, xinxianma@link.cuhk.edu.cn, fanjicong@cuhk.edu.cn

# Abstract

High-dimensional data visualization is crucial in the big data era and these techniques such as t-SNE and UMAP have been widely used in science and engineering. Big data, however, is often distributed across multiple data centers and subject to security and privacy concerns, which leads to difficulties for the standard algorithms of t-SNE and UMAP. To tackle the challenge, this work proposes Fed-tSNE and Fed-UMAP, which provide high-dimensional data visualization under the framework of federated learning, without exchanging data across clients or sending data to the central server. The main idea of Fed-tSNE and Fed-UMAP is implicitly learning the distribution information of data in a manner of federated learning and then estimating the global distance matrix for t-SNE and UMAP. To further enhance the protection of data privacy, we propose Fed- $\mathrm { t S N E } +$ and Fed-UMAP+. We also extend our idea to federated spectral clustering, yielding algorithms of clustering distributed data. In addition to these new algorithms, we offer theoretical guarantees of optimization convergence, distance and similarity estimation, and differential privacy. Experiments on multiple datasets demonstrate that, compared to the original algorithms, the accuracy drops of our federated algorithms are tiny.

UMAP map the data points to a two- or three-dimensional space, exhibiting the intrinsic data distribution or pattern of the original high-dimensional data. Due to their superiority over other methods such as PCA (Jolliffe and Cadima 2016), Isomap (Tenenbaum, De Silva, and Langford 2000), and autoencoder (Hinton and Salakhutdinov 2006), they have been used for visualizing images, tabular data (Hao et al. 2021), text (Grootendorst 2022), and graphs (Wu, Zhang, and Fan 2023) in diverse fields and provide huge convenience for scientific research and engineering practice (Becht et al. 2019). Besides visualization, t-SNE and UMAP are also useful in clustering (Linderman and Steinerberger 2019) and outlier detection (Fu, Zhang, and Fan 2024). There are also a few variants of t-SNE (Yang et al. 2009; Carreira-Perpina´n 2010; Xie et al. 2011; Van Der Maaten 2014; Gisbrecht, Schulz, and Hammer 2015; Pezzotti et al. 2016; Linderman et al. 2019; Chatzimparmpas, Martins, and Kerren 2020; Sun, Han, and Fan 2023) and UMAP (Sainburg, McInnes, and Gentner 2021; Nolet et al. 2021). For instance, Van Der Maaten (2014) used tree-based algorithms to accelerate the implementation of t-SNE. Sainburg, McInnes, and Gentner (2021) proposed a parametric UMAP that can visualize new data without re-training the model.

# 1 Introduction

High-dimensional data are prevalent in science and engineering and their structures are often very complicated, which makes dimensionality reduction and data visualization appealing in knowledge discovery and decision-making (Jolliffe and Cadima 2016; Hinton and Salakhutdinov 2006; Van Der Maaten et al. 2009). In the past decades, many algorithms have been proposed for dimensionality and visualization (Pearson 1901; Fisher 1936; Sammon 1969; Baker 1977; Kohonen 1982; Scho¨lkopf, Smola, and Mu¨ller 1998; Roweis and Saul 2000; Tenenbaum, De Silva, and Langford 2000; Van der Maaten and Hinton 2008; Fan et al. 2018; McInnes et al. 2018). Perhaps, the most popular algorithms in recent years are the t-distributed stochastic neighbor embedding (t-SNE) developed by (Van der Maaten and Hinton 2008) and the Uniform Manifold Approximation and Projection (UMAP) proposed by (McInnes et al. 2018). T-SNE and

In many real cases such as mobile devices, IoT networks, medical records, and social media platforms, the high-dimensional data are distributed across multiple data centers and subject to security and privacy concerns (Dwork, Roth et al. 2014; McMahan et al. 2017; Kairouz et al. 2021; Qiao, Ding, and Fan 2024), which leads to difficulties for the standard algorithms of t-SNE and UMAP. Specifically, in tSNE and UMAP, we need to compute the pair-wise distance or similarity between all data points, meaning that different data centers or clients should share their data mutually or send their data to a common central server, which will leak data privacy and lose information security. To address this challenge, we propose federated t-SNE and federated UMAP in this work. Our main idea is implicitly learning the distribution information of data in a manner of federated learning and then estimating the global distance matrix for t-SNE and UMAP. The contribution of this work is summarized as follows:

• We propose Fed-tSNE and Fed-UMAP that are able to visualize distributed data of high-dimension. • We further provide Fed-tSNE $^ +$ and Fed-UMAP $+$ to en

hance privacy protection.

• We extend our idea to federated spectral clustering for distributed data with privacy protection. • We provide theoretical guarantees such as reconstruction error bounds and differential privacy analysis.

# 2 Related Work

t-SNE t-SNE (Van der Maaten and Hinton 2008) aims to preserve the pair-wise similarities from high-dimension space $\mathcal { P }$ to low-dimension space $\mathcal { Q }$ . The pair-wise similarities are measured as the probability that two data points are neighbors mutually. Specifically, given high-dimensional data points $\mathbf { x } _ { 1 } , \mathbf { x } _ { 2 } , \ldots , \mathbf { x } _ { N }$ in $\mathbb { R } ^ { \bar { D } }$ , $\mathbf { \sigma } _ { \mathrm { t - S N E } }$ computes the joint probability matrix $\mathbf { P } \in \mathbb { R } ^ { N \times N }$ , in which $p _ { i j } = 0$ if $i = j$ , and $\begin{array} { r } { p _ { i j } = \frac { p _ { i \mid j } + p _ { j \mid i } } { 2 N } } \end{array}$ , if $i \neq j$ , where

$$
\begin{array} { r } { p _ { j | i } = \frac { \exp \left( - \| \mathbf { x } _ { i } - \mathbf { x } _ { j } \| _ { 2 } ^ { 2 } / ( 2 \tau _ { i } ^ { 2 } ) \right) } { \sum _ { \ell \in [ N ] \setminus \{ i \} } \exp \left( - \| \mathbf { x } _ { i } - \mathbf { x } _ { \ell } \| _ { 2 } ^ { 2 } / ( 2 \tau _ { i } ^ { 2 } ) \right) } . } \end{array}
$$

In (1), $\tau _ { i }$ is the bandwidth of the Gaussian kernel. Suppose y1, y2, . . . , yN are the low-dimensional embeddings in Rd, where $d \ll D$ , t-SNE constructs a probability matrix $\mathbf { Q }$ by

$$
\begin{array} { r } { q _ { i j } = \frac { \left( 1 + \| \mathbf { y } _ { i } - \mathbf { y } _ { j } \| _ { 2 } ^ { 2 } \right) ^ { - 1 } } { \sum _ { \ell , s \in [ N ] , \ell \neq s } \left( 1 + \| \mathbf { y } _ { \ell } - \mathbf { y } _ { s } \| _ { 2 } ^ { 2 } \right) ^ { - 1 } } } \end{array}
$$

where $i \neq j$ . Then t-SNE obtains $\mathbf { y } _ { 1 } , \mathbf { y } _ { 2 } , \ldots , \mathbf { y } _ { N }$ by minimizing the Kullback-Leibler (KL) divergence

$$
\underset { { \bf y } _ { 1 } , . . . , { \bf y } _ { N } } { \mathrm { m i n i m i z e } } ~ \sum _ { i \ne j } p _ { i j } \log \frac { p _ { i j } } { q _ { i j } }
$$

UMAP UMAP (McInnes et al. 2018) is a little similar to tSNE. It starts by constructing a weighted k-NN graph in the high-dimensional space. The edge weights between points $\mathbf { x } _ { i }$ and $\mathbf { x } _ { j }$ are defined based on a fuzzy set membership, representing the probability that $\mathbf { x } _ { j }$ is in the neighborhood of $\mathbf { x } _ { i }$ . Specifically, the membership strength is computed using

$$
\begin{array} { r } { \mu _ { i | j } = \exp \big ( - \| \mathbf { x } _ { i } - \mathbf { x } _ { j } \| ^ { 2 } / \sigma _ { i } \big ) , } \end{array}
$$

where $\sigma _ { i }$ is a local scaling factor determined by the $\mathbf { k }$ -NNs of $\mathbf { x } _ { i }$ . The final membership strength is symmetrized as

$$
\mu _ { i j } = \mu _ { i | j } + \mu _ { j | i } - \mu _ { i | j } \cdot \mu _ { j | i }
$$

In the low-dimensional space, the probability of two points being neighbors is modeled using a smooth, differentiable approximation to a fuzzy set membership function. The edge weights between points $\mathbf { y } _ { i }$ and $\mathbf { y } _ { j }$ are given by

$$
\begin{array} { r } { \mu _ { i j } ^ { \prime } = \frac { 1 } { 1 + a \| \mathbf { y } _ { i } - \mathbf { y } _ { j } \| ^ { 2 b } } } \end{array}
$$

where $a$ and $b$ are hyperparameters typically set based on empirical data to control the spread of points in the lowdimensional space. UMAP minimizes the cross-entropy between the high-dimensional fuzzy simplicial set and the lowdimensional fuzzy simplicial set, i.e.,

$$
\underset { \mathbf { y } _ { 1 } , . . . , \mathbf { y } _ { N } } { \mathrm { m i n i m i z e } } \sum _ { i \neq j } \mu _ { i j } \log \Big ( \frac { \mu _ { i j } } { \mu _ { i j } ^ { \prime } } \Big ) + \big ( 1 - \mu _ { i j } \big ) \log \Big ( \frac { 1 - \mu _ { i j } } { 1 - \mu _ { i j } ^ { \prime } } \Big )
$$

Discussion Studies about federated dimensionality reduction or data visualization are scarce in the literature. Grammenos et al. (2020) proposed a federated, asynchronous, and $( \epsilon , \delta )$ -differentially private algorithm for PCA in the memory-limited setting. Briguglio et al. (2023) developed a federated supervised PCA for supervised learning. NovoaParadela, Fontenla-Romero, and Guijarro-Berdin˜as (2023) proposed a privacy-preserving training algorithm for deep autoencoders. Different from PCA and autoencoders, in tSNE and UMAP, we need to compute the pair-wise distance or similarity between data points, which leads to significantly greater difficulty in developing federated learning algorithms. Saha et al. (2022) proposed a decentralized data stochastic neighbor embedding, dSNE. However, dSNE assumes that there is a shared subset of data among different clients, which may not hold in real applications.

# 3 Federated Distribution Learning

# 3.1 Framework

Suppose data $\mathbb { R } ^ { m \times n _ { x } } \ni X = \{ X _ { p } \} _ { p = 1 } ^ { P }$ are distributed at $P$ clients, where $\pmb { X } _ { p } \in \mathbb { R } ^ { m \times n _ { p } }$ belongs to client $p$ and $\begin{array} { r } { \sum _ { p = 1 } ^ { P } n _ { p } = n _ { x } } \end{array}$ . To implement t-SNE and UMAP, we need to compute a matrix $D _ { X , X } \in \mathbb { R } ^ { n _ { x } \times n _ { x } }$ of distances between all data pairs in $X$ , which requires data sharing between the clients and central server, leading to data or privacy leaks. We propose to find an estimate of the distance or similarity matrix without data sharing. To do this, we let the central server construct a set of intermediate data points denoted by $\pmb { Y } = [ \pmb { y } _ { 1 } , \dots , \pmb { y } _ { n _ { y } } ] \in \mathbb { R } ^ { m \times n _ { y } }$ and then compute distance matrices $D _ { Y , Y }$ and $\{ D _ { X _ { p } , Y } \} _ { p = 1 } ^ { P }$ . These distance matrices can be used to construct an estimate $\widehat { D } _ { X , X }$ of $D _ { X , X }$ by applying the Nytro¨m method (Williambs and Seeger 2001) (to be detailed later). However, the choice of $\boldsymbol { Y }$ affects the accuracy of $\widehat { D } _ { X , X }$ , further influencing the performance of t-SNE and UbMAP.

Since Nytro¨m method (Williams and Seeger 2001) aims to estimate an entire matrix using its small sub-matrices, the sub-matrices should preserve the key information of the entire matrix, which means a good $Y = [ \pmb { y } _ { 1 } , \dots , \pmb { y } _ { n _ { y } } ] \in$ $\mathbb { R } ^ { m \times n _ { y } }$ should capture the distribution information of $X$ . Therefore, we propose to learn such a $\boldsymbol { Y }$ adaptively from the $P$ clients via solving the following federated distribution learning (FedDL) framework:

$$
\underset { \pmb { Y } } { \mathrm { m i n i m i z e } } \ F ( \pmb { Y } ) \triangleq \sum _ { p = 1 } ^ { P } \omega _ { p } f _ { p } ( \pmb { Y } )
$$

where $f _ { p }$ is the local objective function for each client, and $\omega _ { 1 } , \ldots , \omega _ { P }$ are nonnegative weights for the clients. Without loss of generality, we set $\omega _ { 1 } = \cdot \cdot \cdot = \omega _ { P } = 1 / P$ for convenience in the remaining context. In this work, we set $f _ { p }$ to be the Maximum Mean Discrepancy (MMD) (Gretton et al.

2012) metric:

$$
\begin{array} { r l } & { \quad f _ { p } ( \boldsymbol { Y } ) = \boldsymbol { \mathrm { M M D } } ( \boldsymbol { X } _ { p } , \boldsymbol { Y } ) } \\ & { = \displaystyle \frac { 1 } { n _ { p } ( n _ { p } - 1 ) } \sum _ { i = 1 } ^ { n _ { p } } \sum _ { j \neq i } ^ { n _ { p } } k \left( ( \boldsymbol { X } _ { p } ) _ { : , i } , ( \boldsymbol { X } _ { p } ) _ { : , j } \right) } \\ & { \quad - \displaystyle \frac { 2 } { n _ { p } n _ { y } } \sum _ { i = 1 } ^ { n _ { p } } \sum _ { j = 1 } ^ { n _ { y } } k \left( ( \boldsymbol { X } _ { p } ) _ { : , i } , ( \boldsymbol { Y } ) _ { : , j } \right) } \\ & { \quad + \displaystyle \frac { 1 } { n _ { y } ( n _ { y } - 1 ) } \sum _ { i = 1 } ^ { n _ { y } } \sum _ { j \neq i } ^ { n _ { y } } k \left( ( \boldsymbol { Y } ) _ { : , i } , ( \boldsymbol { Y } ) _ { : , j } \right) } \end{array}
$$

or in the following compact form

$$
\begin{array} { r l } & { ~ f _ { p } ( { \pmb Y } ) = { \mathrm { M M D } } ( X _ { p } , { \pmb Y } ) } \\ & { = \frac { 1 } { n _ { p } ( n _ { p } - 1 ) } \left[ { \pmb 1 } _ { n _ { p } } ^ { T } K _ { X _ { p } , { \pmb X } _ { p } } { \bf 1 } _ { n _ { p } } - n _ { p } \right] - \frac { 2 } { n _ { p } n _ { y } } { \bf 1 } _ { n _ { p } } ^ { T } K _ { { \pmb X } _ { p } , { \pmb Y } } { \bf 1 } _ { n _ { y } } } \\ & { ~ + \frac { 1 } { n _ { y } ( n _ { y } - 1 ) } \left[ { \bf 1 } _ { n _ { y } } ^ { T } K _ { { \pmb Y } , { \pmb Y } } { \bf 1 } _ { n _ { y } } - n _ { y } \right] } \end{array}
$$

where $k ( \cdot , \cdot )$ is a kernel function and $\kappa .$ denotes the kernel matrix computed from two matrices. MMD is a distance metric between two distributions and (10) is actually an estimation of MMD with finite samples from two distributions. If we use the Gaussian kernel $\bar { k } ( { \pmb x } _ { i } , { \pmb y } _ { j } ) = \exp ( - \gamma \| { \pmb x } _ { i } -$ $y _ { j } \| ^ { 2 } )$ , MMD compares all-order statistics between two distributions. For any $\pmb { X } \in \mathbb { R } ^ { m \times n _ { x } }$ and $\pmb { Y } \in \mathbb { R } ^ { m \times n _ { y } }$ , we calculate the Gaussian kernel matrix as $K _ { X , Y } = \exp ( - \gamma D ^ { 2 } )$ , where $D ^ { 2 }$ is the squared pairwise distance matrix between $X$ and $\boldsymbol { Y }$ , i.e., $\begin{array} { r } { \dot { D } ^ { 2 } \ = \ \operatorname { D i a g } ( X ^ { T } X ) \mathbf { 1 } _ { n _ { y } } ^ { T } \ - \ 2 X ^ { T } Y \ + } \end{array}$ $\mathbf { 1 } _ { n _ { x } } { \mathrm { D i a g } } ( { \pmb Y } ^ { T } { \pmb Y } ) ^ { T }$ .

Combining (8) and (10), we have the following optimization problem of federated distribution learning

$$
\underset { \mathbf { Y } } { \mathrm { m i n i m i z e } } \ \sum _ { p = 1 } ^ { P } \omega _ { p } \times \mathbf { M M D } ( X _ { p } , \mathbf { Y } )
$$

By solving this problem, the central server or $\boldsymbol { Y }$ equivalently can learn the distribution information of the data distributed on the $P$ clients. Based on such an $\boldsymbol { Y }$ , we can estimate the distance or similarity matrix between all data points in $X$ , which will be detailed later.

# 3.2 Optimization

For a client $p$ , we consider the corresponding local optimization problem

$$
{ \underset { \pmb { Y } } { \mathrm { m i n i m i z e } } } \ f _ { p } ( \pmb { Y } )
$$

where $f _ { p } ( Y ) =  { \mathrm { M M D } } ( X _ { p } , Y )$ . Due to the presence of kernel function, we have to use some numerical methods like gradient descent to update the decision variable $\boldsymbol { Y }$ . The gradient of $f _ { p }$ at $\boldsymbol { Y }$ is

$$
\begin{array} { c } { { \displaystyle \nabla f _ { p } ( { \pmb Y } ) = \frac { - 4 \gamma } { n _ { p } n _ { y } } \left[ { \pmb X } _ { p } { \pmb K } _ { { \pmb X } _ { p } , { \pmb Y } } - { \pmb Y } \mathrm { D i a g } ( \mathbf { 1 } _ { n _ { p } } ^ { T } { \pmb K } _ { { \pmb X } _ { p } , { \pmb Y } } ) \right] } } \\ { { \displaystyle + \frac { 4 \gamma } { n _ { y } ( n _ { y } - 1 ) } \left[ { \pmb Y } { \pmb K } _ { { \pmb Y } , { \pmb Y } } - { \pmb Y } \mathrm { D i a g } ( \mathbf { 1 } _ { n _ { y } } ^ { T } { \pmb K } _ { { \pmb Y } , { \pmb Y } } ) \right] } } \end{array}
$$

Algorithm 1: Federated Distribution Learning

1: Server broadcast an initial $\mathbf { Y } ^ { 0 }$ to all clients.   
2: for round $s = 1$ to $S$ do   
3: Client side:   
4: for client $p = 1$ to $P$ in parallel do   
5: Set $\bar { Y _ { p } ^ { s , 0 } } = Y ^ { s - 1 }$   
6: Update local variable $Y _ { p } ^ { s }$ :   
7: for $t = 1$ to $Q$ do   
8: 1 $Y _ { p } ^ { s , t } = Y _ { p } ^ { s , t - 1 } - \eta _ { s } \nabla f _ { p } ( Y _ { p } ^ { s , t - 1 } )$   
9: end for   
10: Denote Y  = Y s,Q   
11: Upload $Y _ { p } ^ { s }$ (resp., $\nabla f _ { p } ( Y _ { p } ^ { s , t } ) )$ to the server.   
12: end for   
13: Server side: compute $\begin{array} { r } { Y ^ { s } = \frac { 1 } { P } \sum _ { p = 1 } ^ { P } Y _ { p } ^ { s } } \end{array}$ .   
14: $\begin{array} { r } { \Big ( r e s p . , Y ^ { s } \gets Y ^ { s - 1 } - \eta _ { s } ^ { \prime } \times \frac { 1 } { P } \sum _ { p = 1 } ^ { P } \nabla f _ { p } ( Y _ { p } ^ { s } ) \Big ) } \end{array}$   
15: Broadcast $\mathbf { \nabla } \mathbf { Y } ^ { s }$ to all clients.   
16: end for   
Ensure: $\boldsymbol { Y }$

To make it more explicit, we outline the key steps of FedDL to demonstrate how the central server coordinates local models for learning global distribution in a federated way.

• Step 1: The central server initializes a global $Y _ { g }$ before the learning cycle begins and broadcasts it to all participating local models.   
• Step 2: The local clients copy the global $Y _ { g }$ as their uniform initial guess $Y _ { p }$ and compute the gradient $\nabla f _ { p } ( { \pmb Y } _ { p } )$ .   
• Step 3: Each client $p$ sends its gradient $\nabla f _ { p } ( { \pmb Y } _ { p } )$ or the updated $\boldsymbol { Y }$ , i.e.,

$$
Y _ { p } \gets Y _ { p } - \eta \nabla f _ { p } ( Y _ { p } )
$$

to the central server, where $\eta$ is the step size and can be set as the reverse of the Lipschitz constant of gradient if possible.

• Step 4: The central server updates the global $\boldsymbol { Y }$ by averaging all posted $Y _ { p }$ , i.e.,

$$
Y = \frac { 1 } { P } \sum _ { p = 1 } ^ { P } Y _ { p } ,
$$

or performing gradient descent with the average of all $\nabla \bar { f } _ { p } ( { \pmb Y } _ { p } )$ , i.e.,

$$
\boldsymbol { Y }  \boldsymbol { Y } - \boldsymbol { \eta ^ { \prime } } \times \frac { 1 } { P } \sum _ { p = 1 } ^ { P } \nabla f _ { p } ( Y _ { p } ) ,
$$

where $\eta ^ { \prime }$ is a step size.

• Step 5: The central server broadcasts the newly aggregated communication variables so as to trigger the next local updates.

The optimization details are summarized in Algorithm 1. In the algorithm, for each client $p$ , the time complexity per iteration is $\mathcal { O } ( m n _ { p } ^ { 2 } + m n _ { p } n _ { y } )$ and the space complexity is $\mathcal { O } ( m n _ { p } + m n _ { y } + n _ { p } n _ { y } )$ .

In Algorithm 1, it is necessary to share some variables like the global distribution information $\boldsymbol { Y }$ or the gradient $\nabla f _ { p } ( { \pmb Y } )$ for proceeding the process of training. This may result in data privacy leakage. Data or gradient perturbation by some special types of noise is a common way to enhance the security of federated algorithms. In Section 5, we present the theoretical guarantees of distance estimation and similarity estimation and analyze the properties of differential privacy in such two ways, respectively.

# 3.3 Convergence Analysis

Since we adopt MMD as our local objective function, they are all bounded below. Here, we give the convergence guarantee of Algorithm 1.

Theorem 1. Assume the gradient of all local objective functions $\{ f _ { p } \} _ { p = 1 } ^ { P }$ are $L _ { p }$ -Lipschitz continuous, $L \ =$ $\scriptstyle \sum _ { p = 1 } ^ { P } \omega _ { p } L _ { p }$ with $\begin{array} { r } { \omega _ { p } = \frac { n _ { p } } { n _ { x } } } \end{array}$ , = PpP=L1 2ωpL2p , and ∥∇fp − $\nabla f _ { p ^ { \prime } } \| _ { F } \leq \zeta$ for all $p , p ^ { \prime }$ , the sequence $\{ Y ^ { s , t } \}$ generated by Algorithm $^ { l }$ with step size $1 / L$ satisfies

$$
\begin{array} { r l } & { \displaystyle \frac { 1 } { S Q } \sum _ { s = 1 } ^ { S } \sum _ { t = 1 } ^ { Q } \| { \boldsymbol { Y } } ^ { s , t } - { \boldsymbol { Y } } ^ { s , t - 1 } \| _ { F } ^ { 2 } \leq \frac { 4 } { S Q L } [ F ( { \boldsymbol { Y } } ^ { 0 } ) - F ( { \boldsymbol { Y } } ^ { S } ) ] } \\ & { \displaystyle + \frac { 1 2 \rho _ { L } \zeta ^ { 2 } ( Q + 1 ) ( 2 Q + 1 ) } { L ^ { 2 } [ 1 - 3 ( Q - 1 ) ^ { 2 } ( \rho _ { L } + \frac { \operatorname* { m a x } _ { p } L _ { p } ^ { 2 } } { L ^ { 2 } } ) ] } } \end{array}
$$

The proof can be found in Appendix F. It can be seen that when $S Q$ goes large enough, our algorithm converges to a finite value that is small provided that $\zeta$ is small. Figure 2 in Section 6.1 will show the convergence of the optimization numerically.

# 4 Applications of FedDL

# 4.1 Federated tSNE and UMAP

Nystrom approximation is a technique that can approximate a positive semi-definite (PSD) matrix merely through a subset of its rows and columns (Williams and Seeger 2001). Consider a PSD matrix $S _ { + } ^ { n } \ni H \succeq 0$ that has a representation of block matrix

$$
\mathbf { } S _ { + } ^ { n } \ni \mathbf { \vec { H } } = \left[ \begin{array} { c c } { W } & { B ^ { T } } \\ { B } & { Z } \end{array} \right]
$$

where ${ \pmb W } \in \mathcal { S } _ { + } ^ { c } , { \pmb B } \in \mathbb { R } ^ { ( n - c ) \times c }$ , and $Z \in S _ { + } ^ { n - c }$ for which $c \ll n$ . Specifically, suppose $z$ is unknown, we can approximate it using $W , B$ , and $B ^ { T }$ as

$$
Z \approx B W _ { k } ^ { \dagger } B ^ { T } \triangleq \widehat { Z }
$$

This means we can approximate the incomplete $H$ by $\widehat { H } =$ $[ W , B ^ { T } ; B , \widehat { Z } ]$ . By Nystro¨m method, we can approximcate a distance or  ibmilarity matrix on large-scale dataset in a relatively low computational complexity. Some literature gives some useful upper bounds on Nystro¨m approximation in terms of Frobenius norm and spectral norm for different sampling techniques (Kumar, Mohri, and Talwalkar $2 0 0 9 { \mathrm { b } }$ ; Drineas and Mahoney 2005; Zhang, Tsang, and Kwok 2008;

Kumar, Mohri, and Talwalkar $2 0 0 9 \mathrm { a }$ ; Li, Kwok, and Lu 2010). Here, we present the upper bounds of Nystro¨m approximation in (Drineas and Mahoney 2005) for our subsequent derivation.

Theorem 2 (Error bounds of Nystr¨om approximation). Given $\pmb { X } = [ \pmb { x } _ { 1 } , \dots , \pmb { x } _ { n } ] \in \mathbb { R } ^ { m \times n }$ , let $\widehat { H }$ be the rank- $k$ Nystrom approximation of $\pmb { H }$ only throughc columns sampled uniformly at random without replacement from $H$ , and $\pmb { H } _ { k }$ be the best rank- $k$ approximation of $H$ . Then, the following inequalities hold for any sample of size $c$ :

$$
\begin{array} { r l } & { \| \pmb { H } - \widehat { \pmb { H } } \| _ { 2 } \leq \| \pmb { H } - \pmb { H } _ { k } \| _ { 2 } + \frac { 2 n \rho } { \sqrt { c } } } \\ & { \| \pmb { H } - \widehat { \pmb { H } } \| _ { F } \leq \| \pmb { H } - \pmb { H } _ { k } \| _ { F } + \rho \left( \frac { 6 4 k } { c } \right) ^ { 1 / 4 } } \end{array}
$$

where $\rho = \operatorname* { m a x } _ { i } H _ { i i }$ .

Without the retrieval of raw data from clients, we present federated tSNE (Fed-tSNE) and federated UMAP (FedUMAP) to visualize the high-dimensional data distributed across multiple regional centers. The main idea is to perform Algorithm 1 to learn a $\boldsymbol { Y }$ and then each client $p$ posts the distance matrix $D _ { X _ { p } , Y } \in \mathbb { R } ^ { n _ { p } \times n _ { y } }$ between $X _ { p }$ and $\boldsymbol { Y }$ to the central server. Consequently, the central server assembles all $D _ { X _ { p } , Y }$ to form

$$
\pmb { B } = [ \pmb { D } _ { { \pmb X } _ { 1 } , { \pmb Y } } ^ { \top } \pmb { D } _ { { \pmb X } _ { 2 } , { \pmb Y } } ^ { \top } \cdot \cdot \cdot \pmb { D } _ { { \pmb X } _ { P } , { \pmb Y } } ^ { \top } ] ^ { \top }
$$

and estimate $D _ { X , X }$ as

$$
\widehat { D } _ { X , X } = B W _ { k } ^ { \dagger } B ^ { \top }
$$

where $W = D _ { Y , Y }$ ,bi.e., the distance matrix of $\boldsymbol { Y }$ . Note that in the case that $\dot { \boldsymbol W }$ is singular, we can add an identity matrix to it, i.e., $W + \lambda I$ , where $\lambda > 0$ is a small constant. Finally, the central server implements either t-SNE or UMAP based on $D _ { X , X }$ . The steps are summarized into Algorithm 2.

# Algorithm 2: Fed-tSNE and Fed-UMAP

Require: Distributed data $\{ X _ { 1 } , X _ { 2 } , \ldots , X _ { P } \}$ at $P$ clients.   
1: Perform Algorithm 1 to compute $\boldsymbol { Y }$ .   
2: Each client $p$ computes the distance matrix $D _ { X _ { p } , Y }$ and posts it to the central server.   
3: The central server constructs $B$ using (21) and computes $\widehat { D } _ { X , X }$ using (22).   
4: The  ebntral server runs either t-SNE or UMAP on $\widehat { D } _ { X , X }$ to obtain the low-dimensional embeddings $z$ .

# Ensubre: $z$

Note that sampling data points from clients like in classical Nystro¨m approximation is prohibitive in the federated settings. Thus, it motivates us to use FedDL to learn a useful set of fake points (i.e., landmarks) close enough to the data across the clients in terms of MMD.

# 4.2 Federated Spectral Clustering

Note that after running Algorithm 1, if each client post the kernel matrix $K _ { X _ { p } , Y }$ rather than the distance matrix $D _ { X _ { p } , Y }$ to the central server, the central server can construct a kernel or similarity matrix $\widehat { K } _ { X , X }$ that is useful for spectral clustering. Thus we obta ncfederated spectral clustering, of which the steps are summarized into Algorithm 3.

![](images/bf744d14775785e1c49265737c69d7307503c13e1f9dc24129a285e2d638e415.jpg)  
Figure 1: Visualization of MNIST data using t-SNE, UMAP, and the proposed federated variants.

# Algorithm 3: Fed-SpeClust

Require: Distributed data $\{ X _ { 1 } , X _ { 2 } , \ldots , X _ { P } \}$ at $P$ clients.   
1: Perform Algorithm 1 to compute $\mathbf { \Delta } _ { Y }$ .   
2: Each client $p$ computes the kernel matrix $K _ { X _ { p } , Y }$ and posts it to the central server.   
3: The central server constructs $\begin{array} { r l r l } { C } & { { } } & { } & { { } = } \end{array}$ $\begin{array} { r l } { [ K _ { X _ { 1 } , Y } ^ { \top } } & { { } K _ { X _ { 2 } , Y } ^ { \top } \quad \cdot \cdot \quad K _ { X _ { P } , Y } ^ { \top } ] ^ { \top } } \end{array}$ and computes $\widehat { K } _ { X , X } = C W ^ { - 1 } C ^ { \top }$ with $W = K _ { Y , Y }$ .   
4: Tche central server runs spectral clustering on $\widehat { K } _ { X , X }$ to obtain the clusters $\mathcal { C } = \{ \mathcal { C } _ { 1 } , \mathcal { C } _ { 2 } , \ldots , \mathcal { C } _ { c } \}$ .

Ensure: $\mathcal { C }$

# 5 FedDL with Differential Privacy 5.1 FedDL by Data Perturbation

We inject noise into the raw data in each client and then run FedDL to learn the global distribution information. Note that data perturbation is a one-shot operation before performing Algorithm 1. Specifically, the data $X$ is perturbed by a noise matrix $\pmb { { \cal E } } \in \mathbb { R } ^ { m \times n _ { x } }$ to form the noisy data matrix $\tilde { \pmb X } =$ $X + E$ , where $e _ { i , j } \sim \mathcal { N } ( 0 , \sigma ^ { 2 } )$ . Define $\tilde { \pmb { X } } = \{ \tilde { \pmb { X } } _ { p } \} _ { p = 1 } ^ { P }$ and we then perform Algorithm 1 on $\tilde { X }$ to obtain $\boldsymbol { Y }$ which gives the Nystro¨m approximation

$$
\widehat { H } _ { \tilde { X } , \tilde { X } | Y } \simeq B W _ { k } ^ { \dagger } B ^ { T }
$$

where $B = { K } _ { \tilde { X } , Y }$ (or $D _ { \tilde { { X } } , Y } )$ , $W = K _ { Y , Y }$ (or $D _ { Y , Y } )$ .

Following the logistics of existing literature, we give the upper bounds on the approximation error of Nystro¨m approximation involved with FedDL, where we focus only on the kernel matrix because it is more complex than the distance matrix.

Theorem 3 (Error bound of Nystro¨m approximation with FedDL having data perturbation). Given $\pmb { X } ~ = ~ \{ \pmb { X } _ { p } \} _ { p = 1 } ^ { P }$ with $\begin{array} { r c l } { \pmb { { X } } _ { p } } & { \in } & { \mathbb { R } ^ { m \times n _ { p } } } \end{array}$ having $\begin{array} { r c l } { \sum _ { p = 1 } ^ { P } n _ { p } } & { = } & { n _ { x } } \end{array}$ , $\begin{array} { r l } { \pmb { Y } } & { { } = } \end{array}$ $[ { \pmb y } _ { 1 } , \dots , { \pmb y } _ { n _ { y } } ] \ \in \ \mathbb { R } ^ { m \times n _ { y } }$ , let $\tilde { \cal X } _ { a } ^ { x } \ = \ [ { \cal Y } , \tilde { \cal X } ]$ be the augmented matrix, $C = K _ { \tilde { X } _ { a } ^ { x } , Y }$ , $W = K _ { Y , Y }$ with $\boldsymbol { W } _ { \boldsymbol { k } } ^ { \dagger }$ being the Moore-Penrose inverse of the best rank- $k$ approximation of $W$ , $\xi _ { m } = \sqrt { m + \sqrt { 2 m t } } + 2 t$ , and $C o n d ( \cdot )$ denote condition number of matrix. Denoting $\widehat { \pmb { H } } _ { \tilde { \pmb { X } } , \tilde { \pmb { X } } | \pmb { Y } } = \pmb { C } \pmb { W } _ { k } ^ { \dagger } \pmb { C } ^ { T }$ , it holds with probability at least $1 - n ( n - 1 ) e ^ { - t }$ that

$$
\begin{array} { r l } & { \quad \| \widehat { H } _ { { \tilde { X } } , { \tilde { X } } | Y } - K _ { X , X } \| _ { 2 } } \\ & { \leq \mathrm { C o n d } ( K _ { { \tilde { X } } _ { a } ^ { x } , { \tilde { X } } _ { a } ^ { x } } ) \left( \frac { | \mathrm { M M D } ( \tilde { X } , { Y } ) | } { n _ { x } + n _ { y } } + 1 \right) + 2 n _ { x } } \\ & { \quad + \sqrt { 2 } n _ { x } \gamma \left[ \sigma ^ { 2 } \xi _ { m } ^ { 2 } + \sqrt { 2 } \| D _ { X , X } \| _ { \infty } \sigma \xi _ { m } \right] } \end{array}
$$

alternatively, it holds that

$$
\begin{array} { r l } & { \quad \big \| \widehat { H } _ { { \tilde { X } } , { \tilde { X } } \mid { Y } } - K _ { X , X } \big \| _ { F } } \\ & { \leq \sqrt { n _ { x } + n _ { y } - k } \mathrm { C o n d } ( K _ { { \tilde { X } } _ { a } ^ { x } , { \tilde { X } } _ { a } ^ { x } } ) \left( \frac { | \mathbf { M } \mathbf { M } \mathbf { D } ( { \tilde { X } } , \mathbf { Y } ) | } { n _ { x } + n _ { y } } + 1 \right) } \\ & { + 2 k ^ { \frac { 1 } { 4 } } n _ { x } \sqrt { 1 + \frac { n _ { y } } { n _ { x } } } + \sqrt { 2 } n _ { x } \gamma \left[ \sigma ^ { 2 } \xi _ { m } ^ { 2 } + \sqrt { 2 } \big \| D _ { X , X } \big \| _ { \infty } \sigma \xi _ { m } \right] } \end{array}
$$

Theorem 4 (Differential privacy of FedDL with data perturbation). Assume $\mathrm { m a x } _ { p , j } \| ( \boldsymbol { X } _ { p } ) _ { : , j } \| _ { 2 } = \tau _ { X }$ , FedDL with perturbed data given by Section 5.1 is $( \varepsilon , \delta ) - { \dot { a } }$ ifferentially private if $\dot { \delta } \geq 2 c \tau _ { X } / \varepsilon$ , where $c ^ { 2 } > 2 \ln ( \mathrm { 1 . 2 5 } / \delta )$ .

# 5.2 FedDL by Variable and Gradient Perturbation

We can also perturb the optimization variable $\mathbf { \Delta } _ { Y }$ or the gradient $\nabla f _ { p } ( { \pmb Y } _ { p } )$ by Gaussian noise in the training progression to improve the security of Algorithm 1. No matter which method we follow, the $\boldsymbol { Y }$ obtained by the central server is noisy, i.e., $\tilde { \cal Y } = { \cal Y } + { \cal E }$ , where $\scriptstyle { E }$ is drawn elementwise from ${ \mathcal { N } } ( 0 , \sigma ^ { 2 } )$ . Then, we do Nystrom approximation by

$$
\widehat { \pmb { H } } _ { X , X | \tilde { Y } } \simeq B W _ { k } ^ { \dagger } B ^ { T }
$$

where $B = K _ { X , \tilde { Y } }$ (or $D _ { X , { \tilde { Y } } } )$ ), $W = K _ { \tilde { Y } , \tilde { Y } }$ (or $D _ { \tilde { Y } , \tilde { Y } } )$ .

Theorem 5 (Error bound of Nystro¨m approximation with FedDL having gradient perturbation). With the same notations in Theorem $3$ , let $\tilde { \cal X } _ { a } ^ { y } \ = \ [ \tilde { \cal Y } , { \cal X } ]$ be the augmented matrix. Then it holds that

$$
\begin{array} { r l } & { \quad \left\| \widehat { H } _ { X , X | \tilde { Y } } - K _ { X , X } \right\| _ { 2 } } \\ & { \leq \mathrm { C o n d } ( K _ { \tilde { X } _ { a } ^ { y } , \tilde { X } _ { a } ^ { y } } ) \left( \frac { \left| \mathrm { M M D } ( X , \tilde { Y } ) \right| } { n _ { x } + n _ { y } } + 1 \right) + 2 n _ { x } } \end{array}
$$

alternatively, it holds that

$$
\begin{array} { r l } & { \quad \left\| \widehat { H } _ { X , X | \tilde { Y } } - K _ { X , X } \right\| _ { F } } \\ & { \leq \sqrt { n _ { x } + n _ { y } - k } \mathrm { C o n d } \left( K _ { \tilde { X } _ { a } ^ { y } , \tilde { X } _ { a } ^ { y } } \right) \left( \frac { | \mathrm { M M D } ( X , \tilde { Y } ) | } { n _ { x } + n _ { y } } + 1 \right) } \\ & { \quad + 2 k ^ { 1 / 4 } n _ { x } \sqrt { 1 + \frac { n _ { y } } { n _ { x } } } } \end{array}
$$

Note that $\mathrm { M M D } ( X , \tilde { Y } ) \leq \mathrm { M M D } ( X , Y ) + \mathrm { M M D } ( Y , \tilde { Y } )$ is related to $\sigma$ . A smaller $\sigma$ leads to a lower estimation error (higher estimation accuracy) but weaker privacy protection. We can obtain a precise trade-off between accuracy and privacy by combining Theorem 5 with Theorem 6.

Theorem 6 (Differential privacy of FedDL with gradient perturbation). Suppose $\begin{array} { r l r } { \operatorname* { m a x } _ { p , j } \| ( \boldsymbol { X } _ { p } ) _ { : , j } \| _ { 2 } } & { { } = } & { \tau _ { X } , } \end{array}$ , $\begin{array} { r } { \operatorname* { m a x } _ { p , i , j } \| ( \mathbf { Y } _ { p } ) _ { : , i } - ( \mathbf { \boldsymbol { X } } _ { p } ) _ { : , j } \| = \mathrm { ~ \widetilde { ~ } \gamma ~ } } \end{array}$ , $\| { \bf Y } _ { p } ^ { s } \| _ { s p } \leq \tau _ { Y } \forall s ,$ , let $\{ \nabla f _ { p } ( Y _ { p } ^ { s } ) \} _ { p = 1 } ^ { P }$ for $\begin{array} { r l r } { s } & { { } \in } & { [ S ] } \end{array}$ be the sequence that is perturbed by noise drawn from $\mathcal { N } ( 0 , \sigma ^ { 2 } )$ with variance $8 S \Delta ^ { 2 } \log ( e \mathrm { ~  ~ \xi ~ } + \mathrm { ~  ~ \xi ~ } ( \varepsilon / \delta ) ) / \varepsilon ^ { 2 }$ where 8√n ynγτX {1 + 2γ(τX + τY ) (τX + Υ)}. Then, the Gaussian Mechanism that injects noise to $\{ \nabla f _ { p } ( Y _ { p } ^ { s } ) \} _ { s = 1 } ^ { S }$ for $p \in [ P ]$ is $( \varepsilon , \delta )$ −differentially private.

Note that it is intuitively appropriate to choose a decreasing sequence of noise variance $\lbrace \dot { \sigma } _ { s } ^ { 2 } \rbrace _ { s = 1 } ^ { S }$ adapted to the gradient norm, which may make the algorithm converge well. In practice, we do not have to do this and can instead inject homoscedastic noise while incorporating a carefully chosen scaling factor into the step size of the gradient descent. By doing so, the differential privacy of our FedDL with gradient perturbation can be guaranteed by Theorem 6.

# 5.3 Fed-tSNE $^ +$ and Fed-UMAP+

Based on the above discussion, we propose the securityenhanced versions of Fed-tSNE and Fed-UMAP, denoted by Fed-tSNE+ and Fed-UMAP+, for which Algorithm 2 has noise injection in line 1 (Algorithm 1).

# 6 Experiments

# 6.1 Data Visualization

We applied the proposed Fed-tSNE and Fed-UMAP methods to the MNIST and Fashion-MNIST datasets, with $m _ { X } = 4 0 , 0 0 0$ , and set $n _ { Y } ~ = ~ 5 0 0$ . We designed the experiment with 10 clients, where IID (independent and identically distributed) refers to each client’s data being randomly sampled from the MNIST dataset, thus including all classes. In contrast, non-IID means that each client’s data contains only a single class. After reducing the data dimension to two, we visualized them. Figure 1 presents the results on MNIST, showing the data distribution under both

IID and non-IID conditions. Additionally, we included results using $\mathrm { F e d - t S N E + }$ and Fed-UMAP+, where noise $\pmb { { \cal E } }$ is introduced to the gradient $\nabla f _ { p } ( Y _ { p } )$ . Each element of $\scriptstyle { E }$ is drawn from $\mathcal { N } ( 0 , \mathrm { s d } ^ { 2 } ( \nabla f _ { p } ( Y _ { p } ) ) )$ , where $\operatorname { s d } ( \nabla f _ { p } ( Y _ { p } ) )$ represents the standard deviation of $\nabla f _ { p } ( { \pmb Y _ { p } } )$ . Due to space limitations, the results on Fashion-MNIST are provided in Appendix (Figure 4). Based on the visualization results, our proposed methods perform very well in all settings, with only minor differences compared to the non-distributed results. They preserved nearly all the essential information and structure of the data. Tables 1 and 2 provide quantitative evaluations using the following metrics (detailed in Appendix A): CA (Classification Accuracy) with $\mathbf { k }$ -NN, NPA (Neighbor Preservation Accuracy) with k-NN, NMI (Normalized Mutual Information) of $\mathbf { k }$ -means, and SC (Silhouette Coefficient) of $\mathbf { k }$ -means. It can be observed that the performance of our proposed method shows a slight decline in various metrics compared to the nondistributed results, which is unavoidable. However, the overall differences remain within an acceptable range. Notably, the method performs slightly better on distributed data when the distribution is IID compared to non-IID. Moreover, the performance of Fed-tSNE+ and Fed-UMAP+ with added noise to protect privacy is somewhat inferior to the performance without noise, which is expected, as the non-IID scenario and the introduction of noise both impact the accuracy of $\boldsymbol { Y }$ ’s learning on whole $X$ , thereby affecting the final results.

Convergence Analysis We also conducted experiments to test the convergence of our methods. In Figure 2, the relevant metrics reached convergence after approximately 50 epochs. Figure 3 provides a more intuitive demonstration that, with the increase in epochs, the learning of $\boldsymbol { Y }$ significantly improves the final results of Fed-tSNE and Fed-UMAP, further confirming the feasibility of our method. (The full process visualization is included in Figure 5 of Appendix A.)

0.5 0.95 0.34 0.90 0.85 0.75 Fed-tSNE 0.1 0.0 0.70 Fed-UMAP 0 1 10 50 100 200 500 0 1 10 50 100 200 500 epoch epoch 0.35 0.80 0.30 0.25 0.705   
0.20 Fed-tSNE M   
0.15 Fed-UMAP 0.65 0.60 Fed-tSNE 0.10 0.05 0.55 Fed-UMAP 0 1 10 50 100 200 500 0 1 10 50 100 200 500 epoch epoch

In addition, we also studied the impact of $n _ { y }$ and noise level $\beta$ on NMI (Figures 6 and 7 in Appendix A). The noise level $\beta$ controls the scale of noise, with each element of noise $\scriptstyle { E }$ being drawn from $\mathcal { N } ( 0 , \beta ^ { 2 } \mathrm { s d } ^ { 2 } ( \nabla f _ { p } ( \mathbf { Y } _ { p } ) ) )$ . We see, regardless of the method or conditions, that the larger the $\boldsymbol { Y }$ volume or the smaller the noise level $\beta$ (indicating a lower privacy protection requirement), the better the NMI results.

Table 1: Performance (mean std) of dimensionality reduction on MNIST   

<html><body><table><tr><td></td><td></td><td colspan="2">ID</td><td colspan="2">non-IID</td></tr><tr><td>Metric</td><td>tSNE</td><td>Fed-tSNE</td><td>Fed-tSNE+</td><td>Fed-tSNE</td><td>Fed-tSNE+</td></tr><tr><td>CA1-NN CA 10-NN CA 50-NN</td><td>0.9618±0.0015 0.9656±0.0017</td><td>0.9400±0.0017 0.9477±0.0017</td><td>0.9364±0.0020 0.9443±0.0012</td><td>0.9412±0.0021 0.9483±0.0019</td><td>0.9189±0.0030 0.9307±0.0026</td></tr><tr><td rowspan="5">NPA 1-NN NPA 10-NN NPA 50-NN</td><td>0.9609±0.0015</td><td>0.9401±0.0022</td><td>0.9354±0.0022</td><td>0.9406±0.0020</td><td>0.9209±0.0035</td></tr><tr><td>0.4176±0.0016</td><td>0.2728±0.0022</td><td>0.2543±0.0016</td><td>0.2729±0.0022</td><td>0.1928±0.0019</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td>0.3905±0.0005</td><td>0.3373±0.0007</td><td>0.3263±0.0005</td><td>0.3375±0.0013</td><td>0.2827±0.0010</td></tr><tr><td>0.3441±0.0007 0.7747±0.0243</td><td>0.3301±0.0007</td><td>0.3258±0.0007</td><td>0.3305±0.0006</td><td>0.3030±0.0012</td></tr><tr><td>NMI SC</td><td>0.4226±0.0082</td><td>0.7534±0.0202 0.4407±0.0103</td><td>0.7471±0.0073 0.4478±0.0066</td><td>0.7399±0.0109 0.4321±0.0058</td><td>0.7025±0.0149 0.4441±0.0045</td></tr><tr><td>Metric</td><td>UMAP</td><td>Fed-UMAP</td><td>Fed-UMAP+</td><td>Fed-UMAP</td><td>Fed-UMAP+</td></tr><tr><td>CA 1-NN</td><td>0.9322±0.0053</td><td>0.9066±0.0031</td><td>0.9007±0.0034</td><td>0.9064±0.0026</td><td>0.8730±0.0041</td></tr><tr><td>CA 10-NN</td><td>0.9613±0.0048</td><td>0.9445±0.0018</td><td>0.9416±0.0023</td><td>0.9449±0.0022</td><td>0.9224±0.0036</td></tr><tr><td>CA 50-NN</td><td>0.9602±0.0049</td><td>0.9432±0.0020</td><td>0.9400±0.0025</td><td>0.9441±0.0022</td><td>0.9219±0.0037</td></tr><tr><td>NPA 1-NN</td><td>0.0308±0.0009</td><td>0.0293±0.0007</td><td>0.0277±0.0008</td><td>0.0298±0.0011</td><td>0.0218±0.0009</td></tr><tr><td>NPA 10-NN</td><td>0.1227±0.0010</td><td>0.1133±0.0008</td><td>0.1088±0.0009</td><td>0.1131±0.0012</td><td>0.0914±0.0006</td></tr><tr><td>NPA 50-NN</td><td>0.2226±0.0015</td><td>0.2099±0.0011</td><td>0.2053±0.0011</td><td>0.2095±0.0013</td><td>0.1860±0.0013</td></tr><tr><td>NMI</td><td>0.8285±0.0150</td><td>0.7844±0.0208</td><td>0.7812±0.0153</td><td>0.7919±0.0217</td><td>0.7368±0.0194</td></tr><tr><td>SC</td><td>0.6118±0.0207</td><td>0.5812±0.0261</td><td>0.5746±0.0229</td><td>0.5889±0.0248</td><td>0.5422±0.0173</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

Table 2: Performance (mean std) of dimensionality reduction on Fashion-MNIST   

<html><body><table><tr><td></td><td></td><td colspan="2">IID</td><td colspan="2">non-IID</td></tr><tr><td>Metric</td><td>tSNE</td><td>Fed-tSNE</td><td>Fed-tSNE+</td><td>Fed-tSNE</td><td>Fed-tSNE+</td></tr><tr><td>CA 1-NN CA 10-NN</td><td>0.8112±0.0049</td><td>0.7473±0.0029</td><td>0.7198±0.0041</td><td>0.7453±0.0044</td><td>0.6669±0.0044</td></tr><tr><td></td><td>0.8260±0.0039</td><td>0.7892±0.0030</td><td>0.7706±0.0034 0.7631±0.0037</td><td>0.7898±0.0039 0.7760±0.0045</td><td>0.7280±0.0048 0.7280±0.0043</td></tr><tr><td>CA 50-NN NPA 1-NN</td><td>0.8064±0.0041</td><td>0.7754±0.0033</td><td>0.0718±0.0013</td><td></td><td></td></tr><tr><td></td><td>0.3518±0.0018</td><td>0.1251±0.0021</td><td></td><td>0.1275±0.0017</td><td>0.0274±0.0006</td></tr><tr><td>NPA 10-NN</td><td>0.3635±0.0007</td><td>0.2551±0.0010</td><td>0.1954±0.0011</td><td>0.2571±0.0011</td><td>0.1090±0.0010</td></tr><tr><td>NPA 50-NN</td><td>0.3710±0.0003</td><td>0.3363±0.0006</td><td>0.3004±0.0006</td><td>0.3369±0.0008</td><td>0.2204±0.0017</td></tr><tr><td>NMI</td><td>0.5787±0.0212</td><td>0.5780±0.0154</td><td>0.5733±0.0149</td><td>0.5778±0.0044</td><td>0.5162±0.0129</td></tr><tr><td>SC Metric</td><td>0.4049±0.0101</td><td>0.4382±0.0070</td><td>0.4638±0.0147</td><td>0.4389±0.0085</td><td>0.4564±0.0111</td></tr><tr><td>CA 1-NN</td><td>UMAP</td><td>Fed-UMAP</td><td>Fed-UMAP+</td><td>Fed-UMAP</td><td>Fed-UMAP+</td></tr><tr><td>CA 10-NN</td><td>0.7146±0.0029</td><td>0.6756±0.0036</td><td>0.6587±0.0055</td><td>0.6766±0.0043</td><td>0.6110±0.0037</td></tr><tr><td></td><td>0.7734±0.0039</td><td>0.7413±0.0045</td><td>0.7287±0.0041</td><td>0.7437±0.0030</td><td>0.6875±0.0041</td></tr><tr><td>CA 50-NN</td><td>0.7781±0.0039</td><td>0.7491±0.0052</td><td>0.7383±0.0039</td><td>0.7501±0.0040</td><td>0.7006±0.0033</td></tr><tr><td>NPA 1-NN NPA 10-NN</td><td>0.0356±0.0012</td><td>0.0218±0.0011</td><td>0.0156±0.0009</td><td>0.0223±0.0011</td><td>0.0071±0.0004</td></tr><tr><td>NPA 50-NN</td><td>0.1401±0.0013</td><td>0.1002±0.0015</td><td>0.0799±0.0012</td><td>0.1020±0.0010</td><td>0.0423±0.0007</td></tr><tr><td>NMI</td><td>0.2518±0.0018 0.6187±0.0127</td><td>0.2152±0.0028 0.5915±0.0112</td><td>0.1907±0.0018 0.5755±0.0090</td><td>0.2167±0.0022</td><td>0.1226±0.0015</td></tr><tr><td></td><td></td><td></td><td></td><td>0.5877±0.0181</td><td>0.5191±0.0132</td></tr><tr><td>SC</td><td>0.5304±0.0286</td><td>0.5448±0.0264</td><td>0.5476±0.0176</td><td>0.5338±0.0252</td><td>0.5322±0.0191</td></tr></table></body></html>

<html><body><table><tr><td></td><td>Metric</td><td>k=5</td><td>k=10</td></tr><tr><td>Fed-tSNE</td><td>NMI NPA10-NN</td><td>0.741±0.011 0.336±0.001</td><td>0.740±0.011 0.338±0.001</td></tr><tr><td>Fed-tSNE+</td><td>NMI NPA 10-NN</td><td>0.709±0.026 0.286±0.001</td><td>0.702±0.015 0.283±0.001</td></tr></table></body></html>

Table 3: Performance (mean±std) of Fed-tSNE and Fed-tSNE $+$ on non-IID data for different values of the number of clients $k$

Besides, in the current non-IID case, each client has one class of data, which is the hardest setting (the distribution of clients is highly heterogeneous), while the IID case is the easiest setting. Other settings interpolate between these two extreme cases. To further investigate the impact of the number of clients $k$ , we conducted additional experiments on

able 4: Performance (mean std) of Fed-tSNE and $\mathrm { F e d - t S N E + }$ on IID data for different values of the number of clients $k$   

<html><body><table><tr><td></td><td>Metric</td><td>k=5</td><td>k=10</td><td>k=20</td><td>k=50</td><td>k=100</td></tr><tr><td rowspan="2">Fed-tSNE</td><td rowspan="2">NMI NPA 10-NN</td><td rowspan="2">0.747±0.016</td><td rowspan="2">0.753±0.020 0.337±0.001</td><td rowspan="2">0.745±0.009 0.337±0.001</td><td rowspan="2">0.747±0.014 0.338±0.001</td><td rowspan="2">0.749±0.015 0.338±0.001</td></tr><tr><td>0.337±0.001</td></tr><tr><td>Fed-tSNE+</td><td>NMI</td><td>0.740±0.013</td><td>0.747±0.007</td><td>0.742±0.019</td><td>0.741±0.018</td><td>0.741±0.012</td></tr><tr><td></td><td>NPA 10-NN</td><td>0.323±0.001</td><td>0.326±0.001</td><td>0.329±0.001</td><td>0.332±0.001</td><td>0.333±0.001</td></tr></table></body></html>

Table 5: Performance (mean $\pm$ std) of spectral clustering   

<html><body><table><tr><td></td><td></td><td></td><td>IID</td><td></td><td>non-IID</td><td></td></tr><tr><td></td><td>Metric</td><td>SpeClust</td><td>Fed-SpeClust</td><td>Fed-SpeClust+</td><td>Fed-SpeClust</td><td>Fed-SpeClust+</td></tr><tr><td rowspan="2">MNIST</td><td rowspan="2">NMI</td><td>0.5415±0.0009</td><td>0.5240±0.0038</td><td>0.5220±0.0052</td><td>0.5235±0.0051</td><td>0.5025±0.0068</td></tr><tr><td>0.3837±0.0008</td><td>0.3815±0.0076</td><td>0.3807±0.0088</td><td>0.3806±0.1123</td><td>0.3829±0.0102</td></tr><tr><td rowspan="2">COIL-20</td><td>ARI</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>NMI</td><td>0.6085±0.0012</td><td>0.8425±0.0218</td><td>0.4333±0.0173</td><td>0.8339±0.0216</td><td>0.4215±0.0163</td></tr><tr><td rowspan="2">Mice-Protein</td><td>NMI</td><td>0.3241±0.0063</td><td>0.3233±0.0121</td><td>0.3220±0.0143</td><td>0.3222±0.0190</td><td>0.3198±0.0100</td></tr><tr><td>ARI</td><td>0.1837±0.0037</td><td>0.1827±0.0033</td><td>0.1802±0.0154</td><td>0.1809±0.0024</td><td>0.1783±0.0016</td></tr></table></body></html>

![](images/8ffca057768a86c73d3834c86d419f3ff7f352eb35f1a8a35cc42911739e98a8.jpg)  
Figure 3: Visualization of Fed-tSNE and Fed-UMAP Convergence from epoch 1 to 10 (MNIST)

MNIST to explore the impact of client number. In the IID setting, the dataset is randomly divided among $k$ clients. In the non-IID setting, we focus on the extreme case of completely different distributions, adding $k = 5$ , with each client containing data from two distinct classes. The results, shown in Table 3 and 4, demonstrate that our proposed methods show stable performance across different values of $k$ .

# 6.2 Clustering Performance

We utilized three datasets MNIST, COIL-20, and MiceProtein (detailed in Appendix) to evaluate the effectiveness of our Fed-SpeClust. The hyperparameters were adjusted accordingly and the corresponding results are presented in Table 5. In addition to the NMI metric used previously, we also employed the ARI (Adjusted Rand Index) metric, detailed in Appendix. We see that both NMI and ARI indicate that FedSpeClust achieves results comparable to the original spectral clustering, despite a slight decrease in performance, demonstrating the feasibility of our method.

# 7 Conclusion

This work proposed FedDL and applied it to t-SNE and UMAP to visualize distributed data. The idea was also extended for spectral clustering to cluster distributed data. We provided theoretical guarantees such as differential privacy for the proposed federated learning algorithms. Experimental results demonstrated that the accuracies of our federated algorithms are close to the original algorithms.