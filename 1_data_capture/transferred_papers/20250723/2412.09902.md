# One Node One Model: Featuring the Missing-Half for Graph Clustering

Xuanting $\mathbf { X _ { i } ^ { \bullet } } ^ { 1 }$ , Bingheng $\mathbf { L i } ^ { 2 }$ , Erlin $\mathbf { P a n } ^ { 3 }$ , Zhaochen Guo1, Zhao Kang1\*, Wenyu Chen1

1University of Electronic Science and Technology of China, Chengdu, Sichuan, China 2Michigan State University, East Lansing, US 3Alibaba Group x624361380@outlook.com, libinghe $@$ msu.edu, panerlin.pel $@$ alibaba-inc.com, zkang, cwy $@$ uestc.edu.cn

# Abstract

Most existing graph clustering methods primarily focus on exploiting topological structure, often neglecting the “missinghalf” node feature information, especially how these features can enhance clustering performance. This issue is further compounded by the challenges associated with high-dimensional features. Feature selection in graph clustering is particularly difficult because it requires simultaneously discovering clusters and identifying the relevant features for these clusters. To address this gap, we introduce a novel paradigm called “one node one model”, which builds an exclusive model for each node and defines the node label as a combination of predictions for node groups. Specifically, the proposed “Feature Personalized Graph Clustering (FPGC)” method identifies cluster-relevant features for each node using a squeeze-andexcitation block, integrating these features into each model to form the final representations. Additionally, the concept of feature cross is developed as a data augmentation technique to learn low-order feature interactions. Extensive experimental results demonstrate that FPGC outperforms state-of-the-art clustering methods. Moreover, the plug-and-play nature of our method provides a versatile solution to enhance GNN-based models from a feature perspective.

# Introduction

As a fundamental task in graph data mining, attributed graph clustering aims to partition nodes into different clusters without labeled data. It is receiving increasing research attention due to the success of Graph Neural Networks (GNNs) (Welling and Kipf 2017; Li, Pan, and Kang 2024; Qian, Li, and Kang 2024). Typically, GNNs use many graph convolutional layers to learn node representations by aggregating neighbor node features into the node. In recent years, many graph clustering techniques have achieved promising performance (Pan and Kang 2023; Liu et al. 2023; Kang et al. 2024; Shen, Wang, and Kang 2024).

Most of these methods apply a self-supervised learning framework to fully explore the graph structure (Zhu et al. 2024; Yang et al. 2024), often neglecting the “missing-half” of node feature information, i.e., how node features can enhance clustering quality. Consequently, their performance heavily depends on the quality of the input graph. If the original graph is of poor quality, such as having many nodes from different classes connected together and missing important links, the representations learned through multilayer aggregation become nondiscrimination. This issue is known as representation collapse (Chen, Wang, and Li 2024).

For the attributed graph, the node feature and the topological structure should play an equal role in the unsupervised graph embedding and downstream clustering. We argue that a node’s personalized features should be crucial for identifying its cluster label. For example, the Cora dataset contains 2,708 papers, each described by a 1,433-dimensional binary feature indicating the presence of keywords. We visualize the feature distributions for each cluster in Fig. 1(a). It can be seen that each cluster exhibits its own dominant features, while other features may be irrelevant or less important. The cluster-relevant features are the collection of all personalized features. To distinguish between clusters, we consider features that are 20 times greater than the mean value in each cluster and apply the dynamic time warping (DTW) technique (Mu¨ller 2007) to each cluster. DTW is a similarity measure for sequences of different lengths, and we set the maximum distance to 5,000 for better visualization. From Fig. 1(b), we can see that there are significant differences in the cluster-relevant features. Thus, these personalized features are representative of different clusters.

Furthermore, most GNN-based methods capture only higher-order feature interactions and tend to overlook loworder interactions (Kim, Choi, and Kim 2024). The graph filtering process transforms input features into higher-level hidden representations. Recent studies on GNNs focus mainly on designing node aggregation mechanisms that emphasize various connection properties, such as local similarity (Welling and Kipf 2017), structural similarity (Donnat et al. 2018), and multi-hop connectivity (Li et al. 2022). However, the cross feature information is always ignored, despite its importance for model expressiveness (Feng et al. 2021). For example, by crossing a paper’s keywords in the Cora dataset, such as $\{ t i t l e = a u t o n o m o u s$ & approach ${ } = Q$ − learning & $e x p e r i m e n t = s i m u l a t i o n \}$ , the model can achieve a better paper representation and produce more accurate clustering results (e.g., this paper is categorized under Reinforcement Learning). Until recently, most existing clustering models have struggled to capture such low-order feature interactions.

![](images/e3566c678ca49ddbc5e9c660751b190ab3254b9bd595253cdd1f23afa93f849e.jpg)  
Figure 1: Visualization of results on Cora. (a) is the feature distribution of different clusters, which shows that clusters are characterized by different features. (b) is the DTW distance matrix based on cluster-relevant features, which verify their distinctiveness. We can draw the conclusion that the cluster-relevant features contain valuable information about clusters.

![](images/14f67f10a194567df6aa9775ef8a3622c5354d75d5333234dd826c6f511855c6.jpg)  
(b) DTW distance matrix with only cluster-relevant features on Cora.

To address these shortcomings, we introduce “Feature Personalized Graph Clustering (FPGC)”. We propose a novel paradigm “one node one model” to learn a personalized model for each node, with a squeeze-and-excitation block selecting cluster-relevant features. A new data augmentation technique based on feature cross is developed to effectively capture low-order feature interactions. In summary, our contributions are as follows.

• Orthogonal to existing works, we tackle graph clustering from a feature perspective. By employing a squeezeand-excitation block, we effectively select cluster-relevant features and propose the “one node one model” paradigm. • We develop a novel data augmentation technique based on feature cross to capture low-order feature interactions. • Extensive experiments on benchmark datasets demonstrate the superiority of our method. Notably, our approach can function as a plug-and-play tool for existing GNNbased models, rather than serving solely as a stand-alone method.

# Related Work

Graph clustering can be roughly divided into three categories (Xie et al. 2024; Li et al. 2024). (1) Shallow methods. FGC (Kang et al. 2022) incorporates higher-order information into graph structure learning for clustering. MCGC (Pan and Kang 2021) uses simplified graph filtering rather than GNNs to obtain a smooth representation. (2) Contrastive methods. SCGC (Liu et al. 2023) simplifies network architecture and data augmentation. CCGC (Yang et al. 2023a) leverages highconfidence clustering information to improve sample quality. CONVERT (Yang et al. 2023b) uses a perturb-recover network and semantic loss for reliable data augmentation. (3) Autoencoder methods. SDCN (Bo et al. 2020) transfers learned representations from auto-encoders to GNN layers, employing data structure information. AGE (Cui et al. 2020) uses Laplacian smoothing filters and adaptive learning to improve node embeddings. DGCN (Pan and Kang 2023) uses an adaptive filter to capture important frequency information and reconstructs heterophilic and homophilic graphs to handle real-world graphs with different levels of homophily. DMGNC (Yang et al. 2024) uses a masked autoencoder for node feature reconstruction. However, their training is highly dependent on the input graph. Consequently, if the original graph is poor quality, the learned representation through multilayer aggregation becomes indiscriminate.

To improve the discriminability of representation, some recent methods learn node-specific information by considering the distinctness of each node (Zhu et al. 2024; Dai et al. 2021). NDLS (Zhang et al. 2021) assigns different filtering orders to each node based on its influence score. DyFSS (Zhu et al. 2024) learns multiple self-supervised learning task weights derived from a gating network considering the difference in the node neighbors. In an orthogonal way, in this paper, we improve the representation learning from a feature perspective. We perform an in-depth analysis of how the “one node one model” paradigm benefits clustering.

# Methodology

# Notation

Define the graph data as $\mathcal { G } = \{ \mathcal { V } , E , X \}$ , where $\nu$ represents a set of $N$ nodes and $e _ { i j } \in E$ denotes the edge between node $i$ and node $j$ . $X = \{ \bar { X } _ { 1 } , . . . , X _ { N } \} ^ { \top } \in \mathbb { R } ^ { N \times d }$ is the feature matrix with $d$ dimensions, $X _ { i }$ and $X _ { \cdot j }$ indicate the $i$ -th row and the $j$ -th column and of $X$ , respectively. Adjacency matrix A ∈ RN×N represents the graph structure. D represents the de gree matrix. The normalized adjacency matrix is $A =$ D− 21 (A + I)D− 12 , and the corresponding graph Laplacian is $L = I - A$ .

![](images/c489640ee9f5e10603d1a94944eb2dcd4189103e53ed9037846fa8958d528a48.jpg)  
Figure 2: The pipeline of our FPGC. We preprocess the node features by stacked graph filters. Besides, we also input original features into the squeeze-and-excitation block to select the top $n$ significant features, based on which we learn a model for each node. Then, the contrastive framework encodes the smoothed node features and augmented features to achieve discriminative node representations.

# The Pipeline of FPGC

The FPGC framework is shown in Fig. 2. Our design comprises two critical components. The first component concretizes “one node one model” with GNNs, with a squeezeand-excitation block selecting cluster-relevant features. The second is the contrastive learning framework with a new data augmentation technique based on feature cross.

# Squeeze-and-Excitation Block

Cluster-relevant features characterize the clusters. We design a squeeze-and-excitation block to select them.

Squeeze: The squeeze operation compresses the node features from $X \in \mathbb { R } ^ { \hat { N } \times d }$ to $\overset { \cdot } { q } \in \mathbb { R } ^ { 1 \times d }$ . Specifically, it computes the summation of the features as follows:

$$
\boldsymbol { q } = \boldsymbol { F _ { s q } } \left( \boldsymbol { X } \right) = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \boldsymbol { X _ { i } } ,
$$

where $q$ serves as a channel-wise statistic that captures global feature information.

Excitation: The excitation operation follows the squeeze step and aims to capture dependencies among all channels. It is a simple gating mechanism that uses the sigmoid function $\sigma ( \cdot )$ to activate the squeezed feature map, enabling the learning of nonlinear connections between channels. Firstly, the dimension is reduced using a multi-layer perceptron (MLP) $W _ { 1 }$ . Next, a ReLU function $\delta ( \cdot )$ and another MLP $W _ { 2 }$ are used to increase the dimensionality, returning it to the original dimension $d$ . Finally, the sigmoid function is applied. The process is summarized as follows:

$$
\tilde { q } = F _ { e x } \left( q \right) = \sigma \left( W _ { 2 } \delta \left( W _ { 1 } q \right) \right) .
$$

Selection: The outcome of the excitation operation is considered to be the significance of each feature. Then, we define

the function $F _ { t o p }$ to select top $n$ important features according to $\tilde { q }$ . This operation is defined as follows:

$$
X ^ { n } = F _ { t o p } \left( \tilde { q } X \right) ,
$$

where $X ^ { n } \in \mathbb { R } ^ { N \times n }$ is the node feature matrix containing the top $n$ significant features.

# One Node One Model

Ideally, an exclusive model should be constructed for each individual node, which is the core of our method:

$$
Y _ { i } = f ^ { ( i ) } ( X _ { i } ) ,
$$

where $Y$ and $f$ are the predicted label and the model. In this paradigm, personalization is fully maintained in the models. Unfortunately, the enormous number of nodes makes this scheme unfeasible. One possible solution is establishing several individual models for each type of cluster (the nodes in a group share personalized features). Each node can be considered as a combination of different clusters. For example, in a social network, a person is a member of both a mathematics group and several sports interest groups. Due to the diversity of these different communities, he may exhibit different characteristics when interacting with members from various communities. Specifically, the information about the mathematics group may be related to his professional research, while the information about sports clubs may be associated with his hobbies. As a result, we can decompose the output for a specific node as a combination of predictions for clusters:

$$
Y _ { i } = \sum _ { j = 1 } ^ { M } w _ { j } f ^ { ( j ) } ( X _ { i } ) ,
$$

where $j$ denotes the model index and there are $M$ models. For an unsupervised task, learning $w _ { j }$ directly is difficult. Instead, we generate $w _ { j }$ from personalized features: $w _ { j } =$ $\left[ g ( X ^ { n } ) \right] _ { j }$ , where $g ( X ^ { n } ) = \sigma \left( W _ { 3 } X ^ { n } \right) .$ . In other words, we use personalized features to identify clusters. A higher value of a personalized feature corresponds to a higher weight. $W _ { 3 }$ represents an MLP.

For simplicity, we only consider SGC (Wu et al. 2019) as our basic model, i.e. $f ^ { ( j ) } ( X _ { i } ) = A _ { i } ^ { k } X W ^ { j }$ , where $A ^ { k }$ denotes the stacked $k$ -layer graph filter. Other GNNs can also be the base models. For instance, DAGNN (Liu, Gao, and Ji 2020) has $f ^ { ( j ) } ( X _ { i } ) \ = \ \sum _ { t = 0 } ^ { k } s _ { t } A _ { i } ^ { t } X W ^ { j } .$ , where $s _ { t }$ is the learnable weight; APPNP (Gasteiger, Bojchevski, and G¨unnemann 2018) has $\begin{array} { r l } { f ^ { ( j ) } ( X _ { i } ) } & { { } = } \end{array}$ softmax $\left( \eta ( \pmb { I } - ( 1 - \eta ) A ) _ { i } ^ { - 1 } X \right) W ^ { j }$ , where $\eta$ is the a hyper-parameter. For convenience, we define ${ \bar { X } } = A ^ { k } X =$ $\{ \bar { X } _ { 1 } , . . . , \bar { X } _ { N } \} ^ { \top }$ . Note that $\bar { X } _ { j } \in \mathbb { R } ^ { d }$ and $W ^ { j } \in \mathbb { R } ^ { d \times d _ { o u t } }$ , where $d _ { o u t }$ is the output dimension. We have:

$$
Y _ { i } = \sum _ { j = 1 } ^ { M } \left[ g \left( X ^ { n } \right) \right] _ { j } { \bar { X } } _ { i } W ^ { j } .
$$

The $u$ -th entry of $Y _ { i }$ is given by:

$$
Y _ { i u } = \sum _ { j = 1 } ^ { M } \sum _ { v = 1 } ^ { d } \left[ g \left( X ^ { n } \right) \right] _ { j } W _ { u v } ^ { j } \bar { X } _ { i v } ,
$$

which introduces a complexity of $M$ times. Thus far, we still need to design $M$ individual models to identify clusters, which is computationally demanding. Fortunately, we have enough free parameters to simplify the process. Here we present a simple yet effective way. We set $M = d _ { o u t }$ , which gives $Y _ { i } = \sum _ { j = 1 } ^ { d _ { o u t } } \sum _ { v = 1 } ^ { d _ { i n } } \left( \left[ g \left( X ^ { n } \right) \right] _ { j } W _ { u v } ^ { j } \bar { X } _ { i v } \right)$ . Then we $W _ { u v } ^ { j } = \left\{ \begin{array} { l l } { W _ { u v } , j = u } \\ { 0 , j \ne u } \end{array} \right.$ , which yields:

$$
\begin{array} { c } { { \displaystyle Y _ { i u } = [ g \left( { \cal X } ^ { n } \right) ] _ { u } \sum _ { v = 1 } ^ { d } W _ { u v } \bar { \cal X } _ { i v } } } \\ { { = [ g \left( { \cal X } ^ { n } \right) ] _ { u } \bar { \cal X } _ { i } W _ { . u } , } } \end{array}
$$

or equivalently,

$$
Y = g \left( X ^ { n } \right) \odot { \bar { X } } W ,
$$

where $\odot$ denotes the Hadamard product. Consequently, learning a model for each node is achieved through element-wise multiplication, which is computationally efficient.

Flexibility: Note that the above approach is a generic method to customize existing techniques rather than a standalone way. It can be seamlessly incorporated with many SOTA GNN models to enhance performance.

# Theoretical Analysis

Most theoretical analysis in the GNN area focuses on graph structures (Mao et al. 2024). In this section, we establish a theoretical analysis from the feature perspective. Without loss of generality, we consider the case with two clusters, $c _ { 1 }$ and $c _ { 2 }$ . Assume the original node features follow the

Gaussian distribution: $X _ { i } \sim { \cal N } \left( \mu _ { 1 } , \mathbf { I } \right)$ for $i \in c _ { 1 }$ and $X _ { i } \sim$ ${ \cal N } \left( { \pmb \mu } _ { 2 } , { \bf I } \right)$ for $i \in c _ { 2 }$ $( \pmb { \mu } _ { 1 } \pmb { \mu } _ { 2 } \geq 0 )$ . We define the filtered feature as $\bar { X } = D ^ { - 1 } \widetilde { A } X$ . We define $f _ { i }$ and $f _ { j }$ as models for $X _ { i }$ and $X _ { j }$ , respectiv ely, focusing on the personalized features in sets $T _ { i }$ and $T _ { j }$ , while other irrelevant features are ignored. We then present the following theorem:

Theorem 1. Assuming the distribution of filtered features $\bar { X }$ shares the same variance $\sigma \mathbf { I }$ and the cluster has a balance distribution $\mathbf { P } \left( Y = c _ { 1 } \right) = \mathbf { P } \left( Y = c _ { 2 } \right)$ . The upper bound of $\left| \mathbf { P } \left( Y _ { i } = c _ { 1 } \mid f _ { i } ( { \bar { X } } _ { i } ) \right) - \mathbf { P } \left( Y _ { j } = c _ { 1 } \mid f _ { j } ( { \bar { X } } _ { j } ) \right) \right|$ is decreasing with respect to $\sum _ { u \in T _ { i } \cap T _ { j } } \bar { X } _ { i u } \bar { X } _ { j u }$ .

Note that the assumptions are not strictly necessary but are used to simplify the proof in the Appendix. Theorem 1 indicates that if two nodes share more cluster-relevant features, they are more likely to be classified into one cluster, and vice versa.

# Contrastive Clustering

Feature Cross Augmentation Although graph filtering can smooth the features between node neighborhoods, it only captures high-order feature interactions and suffers from overlooking low-order feature interaction (Feng et al. 2021). Feature cross can provide valuable information for downstream tasks. For example, features like {title & approach & experiment can provide additional information about the paper’s category. Features that collectively represent title can be viewed as a field, like $t i t l e = \{ 0 , 1 , 0 \}$ . To this end, we randomly divide the filtered features into $m$ fields:

$$
\bar { X } = \{ \bar { X } _ { f i e l d _ { 1 } } , \bar { X } _ { f i e l d _ { 2 } } , . . . , \bar { X } _ { f i e l d _ { m } } \} .
$$

Then, every node has $m$ fields, i.e., $\begin{array} { r l } { \bar { X } _ { i } } & { { } = } \end{array}$ $\{ \bar { X } _ { i f i e l d _ { 1 } } , \bar { X } _ { i f i e l d _ { 2 } } ^ { \bullet } , . . . , \bar { X } _ { i f i e l d _ { m } } \}$ We use $\bar { X }$ instead of $X$ because $\bar { X }$ has more information. The feature cross is defined as:

$$
R _ { i } = \left\{ \bar { X } _ { i f i e l d _ { z } } \bar { X } _ { i f i e l d _ { j } } ^ { \top } ~ | ~ 1 \leq z < j \leq m \right\} ,
$$

We input the original features $X$ as a view for contrastive learning. It’s well known that matrix factorization techniques can capture the low-order interaction (Guo et al. 2017). Thus, we propose to perform data augmentation as follows:

$$
X ^ { a u g } = X + R W ,
$$

where $W$ is an MLP to adjust the dimension. Unlike previous methods, Eq.(12) has two advantages. First, instead of using typical graph enhancements such as feature masking or edge perturbation, our method generates augmented views from filtered features. This maintains the semantics of the augmented view. Second, other learnable augmentations design complex losses to remain close to the initial features. Eq.(12) is based on the feature cross, which can provide rich semantic information. The complexity of the feature cross is $O ( N m ^ { 2 } )$ , which is linear to $N$ . The results can be stored and accessed after a one-time computation.

Similarly to Eq.(9), the second view is finally formulated as:

$$
Y ^ { \prime } = g \left( X ^ { n } \right) \odot X ^ { a u g } W .
$$

Note that the same node in two different views shares the same model.

Table 1: Statistics information of datasets.   

<html><body><table><tr><td>Graph datasets</td><td>Nodes</td><td>Dims.</td><td>Edges</td><td>Clusters</td><td>ACD</td></tr><tr><td>Cora</td><td>2708</td><td>1433</td><td>5429</td><td>7</td><td>0.52</td></tr><tr><td>Citeseer</td><td>3327</td><td>3703</td><td>4732</td><td>6</td><td>0.36</td></tr><tr><td>Pubmed</td><td>19717</td><td>500</td><td>44327</td><td>3</td><td>0.36</td></tr><tr><td>UAT</td><td>1190</td><td>239</td><td>13599</td><td>4</td><td>0.35</td></tr><tr><td>AMAP</td><td>7650</td><td>745</td><td>119081</td><td>8</td><td>0.28</td></tr><tr><td>EAT</td><td>399</td><td>203</td><td>5994</td><td>4</td><td>0.25</td></tr><tr><td>BAT</td><td>1190</td><td>239</td><td>13599</td><td>4</td><td>0.46</td></tr><tr><td>Flickr</td><td>89250</td><td>500</td><td>899756</td><td>7</td><td>0.02</td></tr><tr><td>Twitch-Gamers</td><td>168114</td><td>7</td><td>67997557</td><td>2</td><td>0.09</td></tr></table></body></html>

Loss Function For two views $Y$ and $Y ^ { \prime }$ , we treat the same node in different views as positive samples and all other nodes as negative samples. The pairwise loss is defined as follows:

$$
\begin{array} { l } { { \ell \left( Y _ { i } , Y _ { i } ^ { \prime } \right) = - \log \frac { e ^ { \sin \left( Y _ { i } , Y _ { i } ^ { \prime } \right) } } { \sum _ { j = 1 } ^ { N } e ^ { \sin \left( Y _ { i } , Y _ { j } ^ { \prime } \right) } + \sum _ { j = 1 } ^ { N } e ^ { \sin \left( Y _ { i } , Y _ { j } \right) } } , } } \\ { { \mathcal { L } _ { c o n } = \displaystyle \frac { 1 } { 2 N } \sum _ { i = 1 } ^ { N } \left[ \ell \left( Y _ { i } , Y _ { i } ^ { \prime } \right) + \ell \left( Y _ { i } ^ { \prime } , Y _ { i } \right) \right] , } } \end{array}
$$

where sim is the cosine similarity. Besides, the reconstruction loss can be calculated as follows:

$$
\mathcal { L } _ { r e } = \frac { 1 } { N ^ { 2 } } \left. Y ^ { \prime } Y ^ { \top } - A \right. _ { F } .
$$

Finally, the total loss is formulated as:

$$
\begin{array} { r } { \mathcal { L } = \mathcal { L } _ { r e } + \lambda \mathcal { L } _ { c o n } , } \end{array}
$$

where $\lambda > 0$ is a trade-off parameter. The clustering results are obtained by performing K-means on ${ \textstyle \frac { 1 } { 2 } } ( Y + { \check { Y } } ^ { \prime } )$ . The detailed learning process of FPGC is illustrated in the Appendix.

# Experiments

# Datasets

We select seven graph clustering benchmark datasets, which are: Cora (Yang, Cohen, and Salakhudinov 2016), CiteSeer (Yang, Cohen, and Salakhudinov 2016), Pubmed (Yang, Cohen, and Salakhudinov 2016), Amazon Photo (AMAP) (Liu et al. 2022), USA Air-Traffic (UAT) (Mrabah et al. 2022), Europe Air-Traffic (EAT) (Mrabah et al. 2022), and Brazil Air Traffic (BAT) (Mrabah et al. 2022). We also add two largescale graph datasets, the image relationship network Flickr (Zeng et al. 2020) and the social network Twitch-Gamers (Rozemberczki and Sarkar 2021). To see the relevance between graph structure and downstream task, we also report the Aggregation Class Distance (ACD) (Shen, He, and Kang 2024).The statistics information is summarized in Table 1. We can see that these datasets are inherently low-quality.

# Comparison Methods

To demonstrate the superiority of FPGC, we compare it to several recent baselines. These methods can be roughly divided into three kinds: 1) traditional GNN-based methods: SSGC (Zhu and Koniusz 2021). 2) shallow methods: MCGC (Pan and Kang 2021), FGC (Kang et al. 2022), and CGC (Xie et al. 2023). 3) contrastive learning-based methods: MVGRL (Hassani and Khasahmadi 2020), SDCN (Bo et al. 2020), DFCN (Tu et al. 2021), SCGC (Liu et al. 2023), CCGC (Yang et al. 2023a), and CONVERT (Yang et al. 2023b). 4) Advanced autoencoder-based methods: AGE (Cui et al. 2020), DGCN (Pan and Kang 2023), DMGNC (Yang et al. 2024), and DyFSS (Zhu et al. 2024).

# Experimental Setting

To ensure fairness, all experimental settings follow the DGCN (Pan and Kang 2023), which performs a grid search to find the best results. Our network is trained with the Adam optimizer for 400 epochs until convergence. Three MLPs consist of a single embedding layer, with 100 dimensions on Flickr and Twitch-Gamers, and 500 dimensions on other datasets. The learning rate is set to 1e-2 on BAT/EAT/UAT/Twitch-Gamers, 1e-3 on Cora/Citeseer/Pumbed/Flickr, and 1e-4 on AMAP. The graph aggregating layers $k$ is searched in $\{ 2 , 3 , 4 , 5 \}$ . The number of fields $m$ is set according to the density of features, i.e., denser features should have smaller values to cross fewer times, the number of important features $n$ is set according to the number of features, i.e., more features should have larger values, the trade-off parameter $\lambda$ is tuned in $\{ 0 . 0 0 1 \$ , $0 . 1 \bar { , } 1 , 1 0 0 \}$ . Thus, $\{ k , m , \bar { n } , \lambda \}$ are set to $\{ 3 , 6 0 , 1 0 0 , 1 \}$ on Cora, $\{ 4 , 5 0 , 5 0 , 0 \dot { . } 1 \}$ on Citeseer, $\{ 5 , 1 0 , 1 0 , 0 . 0 0 1 \}$ on Pubmed, $\{ 5 , 2 0 , 1 0 , 1 0 0 \}$ on UAT, $\{ 2 , 1 0 , 1 0 , 0 . 0 0 1 \}$ on AMAP, $\{ 4 , 1 0 , 1 0 , 0 . 0 0 1 \}$ on EAT, $\{ 5 , 2 0 , 2 0 , 1 \}$ on BAT, $\{ 4 , 1 0 , 1 0 0 , 1 \}$ on Flickr and $\{ 2 , 2 , 4 , 1 \}$ on Twitch-Gamers. We evaluate clustering performance with two widely used metrics: ACC and NMI. All experiments are conducted on the same machine with the Intel(R) Core(TM) i9-12900k CPU, two GeForce GTX 3090 GPUs, and 128GB RAM 1.

# Results Analysis

The results are illustrated in Table 2. We find that FPGC achieves dominant performance in all cases. For example, on the Cora dataset, FPGC surpasses the runner-up by $4 . 0 4 \%$ and $2 . 6 5 \%$ in terms of ACC and NMI. Traditional GNN-based methods have poor performance compared to other methods, which dig for more structure information or use contrastive learning to implicitly capture the supervision information. Note that SCGC, CCGC, and CONVERT are the most recent contrastive methods that design new augmentations. Our method constantly exceeds advanced GNN-based methods with adaptive filters, which indicates the significance of fully exploring feature information.

DMGNC and DyFSS are the latest methods that explicitly consider feature information. DMGNC’s performance does not show clear advantages compared to other baselines on Cora and Citeseer. Though applying a node-wise feature fusion strategy, DyFSS also performs poorly. For example, our method’s ACC and NMI are $7 . 0 0 \%$ and $4 . 0 6 \%$ higher on Cora, and $1 5 . 5 4 \%$ and $1 3 . 0 1 \%$ higher on EAT, respectively. This is because they perform general embedding without exploiting the relationship between features and clusters.

<html><body><table><tr><td rowspan="2">Methods</td><td colspan="2">Cora</td><td colspan="2">Citeseer</td><td colspan="2">Pubmed</td><td colspan="2">UAT</td><td colspan="2">AMAP</td><td colspan="2">EAT</td><td colspan="2">BAT</td></tr><tr><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td></tr><tr><td>DFCN</td><td>36.33</td><td>19.36</td><td>69.50</td><td>43.90</td><td></td><td></td><td>33.61</td><td>26.49</td><td>76.88</td><td>69.21</td><td>32.56</td><td>8.27</td><td>35.56</td><td>8.25</td></tr><tr><td>SSGC</td><td>69.60</td><td>54.71</td><td>69.11</td><td>42.87</td><td></td><td></td><td>36.74</td><td>8.04</td><td>60.23</td><td>60.37</td><td>32.41</td><td>4.65</td><td>36.74</td><td>8.04</td></tr><tr><td>MVGRL</td><td>70.47</td><td>55.57</td><td>68.66</td><td>43.66</td><td>_-_</td><td></td><td>44.16</td><td>21.53</td><td>45.19</td><td>36.89</td><td>32.88</td><td>11.72</td><td>37.56</td><td>29.33</td></tr><tr><td>SDCN</td><td>60.24</td><td>50.04</td><td>65.96</td><td>38.71</td><td>65.78</td><td>29.47</td><td>52.25</td><td>21.61</td><td>53.44</td><td>44.85</td><td>39.07</td><td>8.83</td><td>53.05</td><td>25.74</td></tr><tr><td>AGE</td><td>73.50</td><td>57.58</td><td>70.39</td><td>44.92</td><td></td><td>，</td><td>52.37</td><td>23.64</td><td>75.98</td><td></td><td>47.26</td><td>23.74</td><td>56.68</td><td>36.04</td></tr><tr><td>MCGC</td><td>42.85</td><td>24.11</td><td>64.76</td><td>39.11</td><td>66.95</td><td>32.45</td><td>41.93</td><td>16.64</td><td>71.64</td><td>61.54</td><td>32.58</td><td>7.04</td><td>38.93</td><td>23.11</td></tr><tr><td>FGC</td><td>72.90</td><td>56.12</td><td>69.01</td><td>44.02</td><td>70.01</td><td>31.56</td><td>53.03</td><td>27.06</td><td>71.04</td><td>-</td><td>36.84</td><td>10.07</td><td>47.33</td><td>18.90</td></tr><tr><td>CONVERT</td><td>74.07</td><td>55.57</td><td>68.43</td><td>41.62</td><td>68.78</td><td>29.72</td><td>57.36</td><td>28.75</td><td>77.19</td><td>62.70</td><td>58.35</td><td>33.36</td><td>78.02</td><td>53.54</td></tr><tr><td>SCGC</td><td>73.88</td><td>56.10</td><td>71.02</td><td>45.25</td><td>67.73</td><td>28.65</td><td>56.58</td><td>28.07</td><td>77.48</td><td>67.67</td><td>57.94</td><td>33.91</td><td>77.97</td><td>52.91</td></tr><tr><td>CGC</td><td>75.15</td><td>56.90</td><td>69.31</td><td>43.61</td><td>67.43</td><td>33.07</td><td>49.58</td><td>17.49</td><td>73.02</td><td>63.26</td><td>44.32</td><td>30.25</td><td>53.44</td><td>26.97</td></tr><tr><td>CCGC</td><td>73.88</td><td>56.45</td><td>69.84</td><td>44.33</td><td>68.06</td><td>30.92</td><td>56.34</td><td>28.15</td><td>77.25</td><td>67.44</td><td>57.19</td><td>33.85</td><td>75.04</td><td>50.23</td></tr><tr><td>DGCN</td><td>72.19</td><td>56.04</td><td>71.27</td><td>44.13</td><td></td><td></td><td>52.27</td><td>23.54</td><td>76.07</td><td>66.13</td><td>51.27</td><td>31.98</td><td>70.15</td><td>49.52</td></tr><tr><td>DMGNC</td><td>73.12</td><td>54.80</td><td>71.27</td><td>44.40</td><td>70.46</td><td>34.21</td><td></td><td></td><td></td><td>-</td><td></td><td>-</td><td></td><td></td></tr><tr><td>DyFSS</td><td>72.19</td><td>55.49</td><td>70.18</td><td>44.80</td><td>68.05</td><td>26.87</td><td>51.43</td><td>25.52</td><td>76.86</td><td>67.78</td><td>43.36</td><td>21.23</td><td>77.25</td><td>51.33</td></tr><tr><td>FPGC</td><td>79.19</td><td>59.55</td><td>72.59</td><td>46.36</td><td>71.03</td><td>34.57</td><td>58.07</td><td>30.64</td><td>78.44</td><td>69.40</td><td>58.90</td><td>34.24</td><td>79.38</td><td>55.57</td></tr></table></body></html>

Table 2: Clustering Results. The best performance is marked in red. “-” indicates the original paper does not have this result and the provided code can’t produce the result.

![](images/62b4d2b77673673014f883ac76acb6056c2ea6c0ca5ed0fbb0caaf26afd823fd.jpg)  
Figure 3: Parameter analysis of $m$ and $n$ on Cora and EAT.

# Parameter Analysis

To assess the impact of parameters, we evaluate the clustering accuracy of FPGC across them on Cora and EAT. First, we test the performance with different $m$ and $n$ . As shown in Fig. 3, Cora is less sensitive to these two parameters than EAT. In addition, Cora prefers a large $m$ while EAT opts for a small $m$ , this is consistent with the density (proportion of non-zero values) of $\bar { X }$ : $3 8 . 6 9 \%$ on Cora and $5 0 . 2 5 \%$ on EAT. Too large $m$ will degrade the performance since excessive augmentation could deteriorate the original information. Cora prefers a large $n$ while EAT opts for a small one, which is reasonable since the attribute dimension of Cora is much larger.

Secondly, the impact of $k$ and $\lambda$ is shown in Fig. 4. FPGC can perform in effect for a wide range of $\lambda$ . A small $k$ is enough to achieve a decent result.

# Results on Large-scale Data

To evaluate the scalability of FPGC, we conduct the experiments on large graph datasets Flickr and Twitch-Gamers, which have 89250 and 168114 nodes, respectively. Note that many methods fail to run on these datasets, such as DGCN. We set the batch size to 1000 for all methods. We select three recent baselines to see their performance and the total training time. Fig. 5 shows the results. FPGC continues to outperform all comparison approaches. The required training time of FPGC is notably lower than that of the CGC and

![](images/0b8ee54d370444b40a7e41a85f0ae1ce36364524b51ef7eb9c07fc15deb20781.jpg)  
Figure 4: Parameter analysis of $k$ and $\lambda$ on Cora and EAT.   
Figure 5: Clustering results and running time on Flickr and Twitch-Gamers.

Clustering Results on Large-scale Datasets TrainingTimeonLarge-scaleDatasets 60 1000 Flickr Flickr Twitch-Gamers 800 55   
GR 600   
C50   
A 400 45 200 40 0 CGC CCGCDyFSS FPGC Methods Methods

DyFSS and exhibits a marginal advantage over CCGC. This is because the feature-personalized step involves only matrix multiplication, while graph filtering and feature crossing can be pre-computed and stored. The result not only highlights the effectiveness of FPGC but also underscores its scalability and efficiency in handling large-scale datasets.

# Robustness Analysis

To demonstrate the significance of the feature, we examine the situation in which the graph structure is of poor quality. Specifically, we add edges to the graphs and remove the same number of original edges from them at random ( $\mathrm { F u }$ , Zhao, and Bian 2022). We define = #random edges as the random edge rate. With $r = \{ 0 . 2 , 0 . 4 , 0 . 6 , 0 . 8 \}$ , we report the ACC of CGC, CCGC, DyFSS, and FPGC in Cora and Citeseer. From Fig. 6, it can be seen that the PFGC achieves the best performance in all cases. Especially when the perturbation rate is extremely high, our method shows a more stable tendency than other clustering methods. The performance of other methods changes dramatically, indicating that they rely highly on the graph structure. Thus, our method is robust to the graph structure noise.

Table 3: Results of ablation study. The best performance is marked in red and the runner-up is marked in bold.   

<html><body><table><tr><td rowspan="2">Methods</td><td colspan="2">FPGC w/o aug</td><td colspan="2">FPGC w aug</td><td colspan="2">FPGC w/o g()</td><td colspan="2">FPGCωDAGNN</td><td colspan="2">FPGC</td></tr><tr><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td><td>ACC</td><td>NMI</td></tr><tr><td>Cora</td><td>75.95</td><td>58.98</td><td>76.42</td><td>57.52</td><td>78.52</td><td>58.72</td><td>78.27</td><td>59.02</td><td>79.19</td><td>59.55</td></tr><tr><td>Citeseer</td><td>71.04</td><td>44.90</td><td>71.33</td><td>44.65</td><td>70.23</td><td>44.34</td><td>72.10</td><td>46.02</td><td>72.59</td><td>46.36</td></tr><tr><td>Pubmed</td><td>66.98</td><td>28.47</td><td>67.76</td><td>29.42</td><td>71.20</td><td>34.96</td><td>72.05</td><td>35.26</td><td>71.03</td><td>34.57</td></tr><tr><td>UAT</td><td>57.25</td><td>29.06</td><td>57.61</td><td>29.69</td><td>57.13</td><td>29.86</td><td>58.24</td><td>30.72</td><td>58.07</td><td>30.64</td></tr><tr><td>AMAP</td><td>77.45</td><td>68.32</td><td>78.01</td><td>68.96</td><td>76.88</td><td>66.74</td><td>79.52</td><td>70.22</td><td>78.44</td><td>69.40</td></tr><tr><td>EAT</td><td>57.82</td><td>33.32</td><td>58.05</td><td>33.96</td><td>57.25</td><td>33.19</td><td>59.28</td><td>34.71</td><td>58.90</td><td>34.24</td></tr><tr><td>BAT</td><td>78.12</td><td>53.81</td><td>78.23</td><td>53.96</td><td>77.23</td><td>52.19</td><td>79.03</td><td>55.02</td><td>79.38</td><td>55.57</td></tr></table></body></html>

![](images/119f62c2a2f7fbe5a30e3a155336e6330745fd4f8256404a380197058d7ae7d2.jpg)  
Figure 6: Results of robustness test on Cora (left) and Citeseer (right).

![](images/8d08c110bc6e6c988841f0df8823a1e105109df032e55207ceeb1a69827196d0.jpg)  
Figure 7: The results on different important features.

# Ablation Study

To assess the impact of feature cross, we test the clustering performance after removing it, marking it as “FPGC $w / o$ aug”. The results are shown in Table 3. It is clear that feature cross does improve the clustering performance. In addition, we replace feature cross with classical graph augmentation strategies, including drop edges (Liu et al. 2024), add edges (Xia et al. 2022), graph diffusion (Hassani and Khasahmadi 2020), and mask feature (Yu et al. 2022). We set the change rate to $20 \%$ according to the suggestions of the original paper. The best performance among these four methods is marked as “FPGC w aug”. Our method can still beat them, thus feature cross can provide more rich information.

To see the influence of feature selection, we test the performance without $g ( X ^ { n } )$ and mark it as “FPGC $w / o g ( \bar { ) } ^ { \dag }$ . We can see the performance degradation in most cases in Table 3. Thus learning a model for each node can successfully keep each node’s personality. FPGC does not achieve better performance on Pubmed. This may be because Pubmed has a large node size but only has 3 clusters, which means that it is difficult to select the cluster-relevant features in this case. We also test the performance with DAGNN (Liu, Gao, and Ji 2020) as the base model and mark this method as “FPGC $w$ DAGNN”. It can be seen that it achieves the best performance in 4 out of 7 cases in Table 3. Thus, our proposed “one node one model” paradigm is promising with other SOTA GNNs.

To verify that the proposed squeeze-and-excitation block can select essential features, we categorize them into three intervals: features with the top $3 3 . 3 \%$ (highest) $\tilde { q }$ , features from $3 3 . 3 \%$ to $6 6 . 7 \%$ , and the remaining $6 6 . 7 \%$ to $100 \%$ . We use these indexes to select features for $X ^ { n }$ and test their performance, respectively. From Fig. 7, we can see that when selecting the most important features (top $3 3 . 3 \%$ ), we achieve the best results in all cases. This indicates that features with high value in $\tilde { q }$ are vital. Therefore, our proposed squeezeand-excitation block can successfully assign high weights to essential features, which is beneficial for downstream task.

# Conclusion

In this paper, we introduce the “one node one model” paradigm, which addresses the limitations of existing graph clustering methods by emphasizing missing-half feature information. By incorporating a squeeze-and-excitation block to select cluster-relevant features, our method effectively enhances the discriminative power of the learned representations. The proposed feature cross data augmentation further enriches the information available for contrastive learning. Our theoretical analysis and experimental results demonstrate that our method significantly improves clustering performance.