# Textualize Visual Prompt for Image Editing via Diffusion Bridge

Pengcheng ${ \bf X } { \bf u } ^ { 1 , 2 }$ , Qingnan $\mathbf { F a n } ^ { 2 }$ , Fei $\mathbf { K o u } ^ { 2 }$ , Shuai $\mathbf { Q } \mathbf { i n } ^ { 2 }$ , Hong $\mathbf { G } \mathbf { u } ^ { 2 }$ , Ruoyu Zhao2,3, Charles Ling1, Boyu Wang1 \* 1 Western University 2 VIVO 3 Xidian University   
{pxu67, charles.ling} $@$ uwo.ca, fqnchina $@$ gmail.com, koufei $@$ hotmail.com, royzhao $@$ stu.xidian.edu.cn, {guhong, shuai.qin} $@$ vivo.com, bwang $@$ csd.uwo.ca,

# Abstract

Visual prompt, a pair of before-and-after edited images, can convey indescribable imagery transformations and prosper in image editing. However, current visual prompt methods rely on a pretrained text-guided image-to-image generative model that requires a triplet of text, before, and after images for retraining over a text-to-image model. Such crafting triplets and retraining processes limit the scalability and generalization of editing. In this paper, we present a framework based on any single text-to-image model without reliance on the explicit image-to-image model thus enhancing the generalizability and scalability. Specifically, by leveraging the probability-flow ordinary equation, we construct a diffusion bridge to transfer the distribution between before-and-after images under the text guidance. By optimizing the text via the bridge, the framework adaptively textualizes the editing transformation conveyed by visual prompts into text embeddings without other models. Meanwhile, we introduce differential attention control during text optimization, which disentangles the text embedding from the invariance of the before-and-after images and makes it solely capture the delicate transformation and generalize to edit various images. Experiments on real images validate competitive results on the generalization, contextual coherence, and high fidelity for delicate editing with just one image pair as the visual prompt.

Project page — pengchengpcx.github.io/TextVDB/

# 1 Introduction

Prompting (Touvron et al. 2023; Floridi and Chiriatti 2020; Liu et al. 2023), providing specific instructions or context for the model, is an effective and emergent tool to guide the largescale text-to-image (T2I) models to generate (Rombach et al. 2022; Podell et al. 2024; Saharia et al. 2022; Ramesh et al. 2022) or edit (Hertz et al. 2023; Kawar et al. 2023; Parmar et al. 2023; Mokady et al. 2023; Gal et al. 2023) remarkable images. However, in the visual task, not all aspects can be accurately and comprehensively described through language alone. For instance, how to explain the tone transformation between photographs or the personal repaint of a painting? These imagery transformations are abstract, complex, and difficult to convey via a text prompt. In such cases, a pair of before-and-after images, serving as the visual prompt, are more precise and expressive to represent the imagery transformation and guide the image generation and editing. This motivates us to explore the visual prompt for image editing: given a pair of before-and-after images as a visual prompt, how to distill the transformation from the visual prompt into text embedding for the text-guided image editing.

Early on, visual prompt for image editing is employed with in-context learning, primarily effective on classical tasks (e.g., segmentation, detection) (Bar et al. 2022; Wang et al. 2024). Recently, it is explored within diffusion models for arbitrary visual content editing (Nguyen et al. 2024; Motamed, Paudel, and Van Gool 2025; Cheng et al. 2023; Yang et al. 2024; Gu et al. 2024), and there are mainly three types of approaches.

The first category relies on the text-guided image-to-image (TI2I) model (e.g., InstructPix2Pix) (Brooks, Holynski, and Efros 2023). Such a TI2I model has an explicit text-guided image-to-image transformation process. By directly optimizing the text condition that guides the model to manipulate and change the before-image to the after-image, the model converts the transformation between the image pair into text (Gal et al. 2023; Nguyen et al. 2024). However, training such a TI2I model needs to construct a data triplet of text, and before-and-after images. More importantly, such training data is constructed based on the T2I model. For more delicate editing, crafting such triplets requires meticulous effort, limiting the scalability of the training data in the TI2I model (Zhang et al. 2024). Similarly, the second type in-context learning methods (Gu et al. 2024; Yang et al. 2024) leverages the diffusion inpainting model which also requires extra complex data (mask and cropped images) for more training to get good generalization ability. This contrasts with the general T2I model, which utilizes a simpler tuple of images and text for training, making it easier to collect and scale up. The third category adopts textual inversion (Gal et al. 2023; Motamed, Paudel, and Van Gool 2025; Šubrtová et al. 2023) to invert the before-and-after images into text embeddings sequentially and learn the transformation or concept by comparison of two embeddings. However, textual inversion is inferior for image reconstruction and cannot capture all image details, making it challenging to learn delicate editing transformations and resulting in a coarse concept. Thus, these designs limit the model to learning precise transformations and generalizing to edit high-fidelity images.

![](images/fc760e6f5f510eea3cf260b9102b2484122d1e5d28e3718dd3705802bb13810a.jpg)  
Figure 1: Image editing via visual prompt. The visual prompt defines the visual transformation, which is difficult to describe accurately by language, by a before-and-after image pair. Our method learns such delicate transformation into pseudo text $( < \mathsf { A } >$ and ${ \mathrm { < C > } }$ ), supports hybrid editing with natural text, and can control the intensity of editing with rigorous consistency.

To address the aforementioned issues, in this paper, we aim to answer the following questions: why and how to use a general T2I model to learn delicate imagery transformation from visual prompts. For the why, considering the training data of the TI2I model is mainly derived from the T2I model and manually crafting large-scale triplet (or more complex) data is cumbersome, visual prompts learned directly based on the general T2I model can be more scalable and generalized. For the how, there are two challenges: 1) Construct and textualize the image-to-image (I2I) process but with the single T2I model. The T2I model only has textto-image mapping but lacks explicit I2I mapping. We have to construct the I2I translation process for the image pair and distill the process into text embedding used for later editing. 2) Capture imagery details for delicate transformation. The constructed I2I process should encode all details of visual prompts otherwise the learned transformation is ambiguous.

To solve these challenges, we propose to textualize the visual prompt based on the diffusion bridge. Firstly, we use a single T2I model to build a diffusion bridge (Zhou et al. 2024; Su et al. 2023), that transforms the distribution of the before-image to the after-image, to represent the I2I process. As shown in Fig. 2, the bridge is built on a single pretrained T2I model by leveraging its unconditional (null text) and conditional (text) generating abilities. Based on the probability-flow ordinary equation, the before-image is first transformed into a deterministic latent with the unconditional model and then regenerated to the after-image with the text condition. Such a design breaks the dependency on the explicit I2I process and thus requires no retraining of any TI2I model. Besides, leveraging the more general T2I model, the method adaptively inverts different visual prompts into different text embeddings, supporting generalized, hybrid, and high-fidelity editing.

Secondly, to make the text embedding focus on capturing transformation details, and generalize to edit various images, we propose differential attention control to preserve detailed contents of the before-image during training. This module injects attention weights of the before-image while supporting backpropagation for optimizing text embedding. By learning with attention from the before image, the inverted text embedding is disentangled from the invariance of the before-image, which is irrelevant to the transformation, focuses on capturing fine-grained transformations, and generalizes to edit various images. We summarize our contributions and findings:

We design a framework to textualize visual prompts via the T2I diffusion bridge for image editing. The method does not require both image and text conditions as the TI2I model that needs triplets of training data and retraining, which improves the scalability and generalization. • We introduce differential attention control for optimizing the text embedding, learning delicate imagery transformations, and generalizing to edit various types of images. Experiments with various evaluation metrics validate our method can learn delicate imagery transformation and generalize better to edit faithful and high-fidelity images, compared to existing methods.

# 2 Related Work

Text guided image-to-image models. To make the text-toimage (T2I) models control the spatial content of the synthesized images, the text-guided image-to-image model (TI2I) adds an extra image condition based on the T2I model. ControlNet (Zhang, Rao, and Agrawala 2023; Zhao et al. 2024) and Adapter (Mou et al. 2024; Ye et al. 2023) methods create an additional branch to the UNet to introduce the conditions for generation. Another type of approach combines the embedding of the image with the latent in the diffusion and retrains the model to have both image and text conditions such as InstructPix2Pix (Brooks, Holynski, and Efros 2023), InstructDiffusion (Brooks, Holynski, and Efros 2023), and MagicBrush (Zhang et al. 2024). While these TI2I models support human-intuitive text for image editing, collecting image pairs strictly aligned with editing texts is not as easy as simply collecting tuples of the image and its text description, and retraining is inefficient, which limits the capacity of these models. In contrast, our method is directly built on the T2I model for its large text-to-image prior that can be easily scaled up with the increasing scale of pairs of text and image. Image editing via the visual prompt. Visual prompting was initially proposed in NLP (Brown et al. 2020) based on in-context learning and recently introduced to computer vision (Bar et al. 2022; Wang et al. 2023a,b). ImageBrush (Yang et al. 2024) and Analogist (Gu et al. 2024) formulate the editing as inpainting in analogy to the image pair but it requires a retrained inpainting model and is limited in image resolutions and encoding intricate details. Image analogy (Šubrtová et al. 2023; Hertzmann et al. 2023) adapts the analogous relation of image pair to new images but the learned editing relation is coarse and ambiguous and cannot accurately edit or preserve structures. Recent methods extract the image transformation from image pairs into text embeddings for text-guided editing. VII (Nguyen et al. 2024) leverages the pretrained TI2I model, InstructPix2Pix, to distill the image-to-image transformation between the image pair into text embeddings. Lego (Motamed, Paudel, and Van Gool 2025) and DIA (Hertzmann et al. 2023) uses textual inversion to learn the disentangled concept by comparing the image pair. However, textual inversion is inferior in image reconstruction and cannot encode all imagery details, which causes the learned concept to be coarse and inaccurate. In contrast, our method accurately recovers the image and textualizes the visual prompts without dependence on the pretrained TI2I model, and produces generalized and delicate editing results.

# 3 Methodology

We aim to learn the delicate imagery transformation from the visual prompt and use it for generalized and high-fidelity image editing. In the following, we first briefly present the score-based generative model (Song et al. 2021) and the diffusion bridge (Song, Meng, and Ermon 2021; Su et al. 2023) in Sec 3.1. Then, we discuss converting the visual prompt into text embedding via diffusion bridge in Sec 3.2. In Sec 3.3, we introduce a differential attention control strategy during text optimization, which helps learn disentangled and generalized text guidance. Last, the full algorithm is in Sec 3.4.

# 3.1 Preliminaries

Scored-based generative model. Song et al. (2021) proposed the unified diffusion framework with Stochastic Differential Equations (SDE) and further showed that any diffusion process can be represented by a deterministic probability flow (PF) ordinary differential equation (ODE) that transfers the data encodings to deterministic and unqiue latent encodings. By solving the PF ODE in Eq. 2 forward and backward, the data $\mathbf { x } _ { \mathrm { 0 } }$ and latent $\mathbf { x } _ { T }$ are transferred between each other.

$$
\mathrm { d } \mathbf { x } _ { t } = \mu \left( \mathbf { x } _ { t } , t \right) \mathrm { d } t + \sigma ( t ) \mathrm { d } \mathbf { w } _ { t }
$$

$$
\mathrm { d } \mathbf { x } _ { t } = \left[ \pmb { \mu } ( \mathbf { x } _ { t } , t ) - \frac { 1 } { 2 } \sigma ( t ) ^ { 2 } \nabla _ { \mathbf { x } } \log p _ { t } ( \mathbf { x } _ { t } ) \right] \mathrm { d } t
$$

Here, $\mathbf { w } _ { t }$ is the standard Wiener process, $\pmb { \mu } ( t )$ and $\sigma ( t )$ are drift and diffusion coefficients respectively. The score function $\nabla _ { \mathbf { x } } \log p _ { t } ( \mathbf { x } _ { t } )$ is approximated with neural network $\mathbf { s } _ { \theta } ( \mathbf { x } _ { t } , t )$ . For the T2I model, the text condition c is added to replace the marginal score function $\nabla _ { \mathbf { x } } \log p _ { t } ( \mathbf { x } _ { t } )$ with the conditional score function $\nabla _ { \mathbf { x } } \log p _ { t } ( \mathbf { x } _ { t } | \mathbf { c } )$ via classifier-free guidance ( $\mathrm { \Delta H o }$ and Salimans 2022) in Eq. 3, where $w$ is the guidance scale, and $\varnothing$ represents the null text.

$$
\begin{array} { r l } & { \nabla _ { \mathbf { x } } \log p _ { t } ( \mathbf { x } _ { t } | \mathbf { c } ) \approx \tilde { \mathbf { s } } _ { \theta } ( \mathbf { x } _ { t } , t , \mathbf { c } ) = \mathbf { s } _ { \theta } ( \mathbf { x } _ { t } , t , \mathcal { O } ) } \\ & { \qquad + \textit { w } \cdot ( \mathbf { s } _ { \theta } ( \mathbf { x } _ { t } , t , \mathbf { c } ) - \mathbf { s } _ { \theta } ( \mathbf { x } _ { t } , t , \mathcal { O } ) ) } \end{array}
$$

Dual diffusion implicit bridges (DDIB). Following the above PF ODE in Eq. 2, DDIB (Su et al. 2023) uses two diffusion models $\mathbf { s } _ { \theta } ^ { ( s ) }$ and $\mathbf { s } _ { \theta } ^ { ( t ) }$ trained on separated (source and target) domains to perform I2I transformation. Concretely, as shown in Eq. 5, it first uses an ODE solver in Eq. 6 to forward the source image $\mathbf { x } ^ { ( s ) }$ to the latent $\mathbf { x } ^ { ( l ) }$ , and then reverse the latent to the target image $\mathbf { x } ^ { ( t ) }$ . Such a translation process has been theoretically proven to be the most efficient optimal transport between two distributions (Su et al. 2023).

$$
\begin{array} { r l } & { \mathbf { x } ^ { ( l ) } = \mathrm { O D E S o l v e } \left( \mathbf { x } ^ { ( s ) } ; \mathbf { s } _ { \theta } ^ { ( s ) } , 0 , 1 \right) } \\ & { \mathbf { x } ^ { ( t ) } = \mathrm { O D E S o l v e } \left( \mathbf { x } ^ { ( l ) } ; \mathbf { s } _ { \theta } ^ { ( t ) } , 1 , 0 \right) } \end{array}
$$

$$
\mathrm { O D E S o l v e } \left( \mathbf { x } _ { t } ; \mathbf { s } _ { \theta } , t _ { 0 } , t _ { 1 } \right) = \mathbf { x } _ { t _ { 0 } } + \int _ { t _ { 0 } } ^ { t _ { 1 } } \mathbf { s } _ { \theta } \left( \mathbf { x } _ { t } , t \right) \mathrm { d } t
$$

Our method is driven and designed by such theoretical advantage over transport efficiency. However, instead of using two different models trained on two different domains, we only use a single T2I model and leverage the text condition to transfer to versatile domains.

# 3.2 Visual Prompt Learning via Diffusion Bridge

T2I diffusion bridge. To learn the transformation represented by the visual prompt on the more general and scalable T2I model, we need to construct the I2I process (before-toafter image transformation) based on a single T2I model. Inspired by the DDIB, we build a diffusion bridge to represent the I2I transformation. Specifically, we use unconditional (null text $\varnothing$ ) and conditional (text c) models to replace the two different diffusions in DDIB, and adopt DDIM as the ODE solver. This can be implemented with a single pretrained T2I model and require no retraining of any model.

Concretely, let $\mathbf { \dot { x } } _ { 0 } ^ { b } \sim p ( \mathbf { x } ^ { b } )$ and $\mathbf { x } _ { 0 } ^ { a } \sim p ( \mathbf { x } ^ { a } )$ represent the before-and-after images, respectively. The intermediate latent is $\mathbf { x } _ { T } \sim \mathcal { N } ( 0 , I )$ . Our diffusion bridge is defined in Eq. 7 and 8 in which the before-image $\mathbf { x } _ { 0 } ^ { b }$ is first transformed into $\mathbf { x } _ { T }$ with null text $\varnothing$ in $T$ steps and then regenerated to $\mathbf { x } _ { 0 } ^ { a }$ under the guidance of text embedding c. Such a process model the distribution transition of $p ( \mathbf { x } ^ { b } ) \overset { } {  } p ( \mathbf { x } ^ { a } )$ . We aim to learn c that can guide any samples drawn from $p ( \mathbf { x } ^ { b } )$ to $p ( \mathbf { x } ^ { a } )$ in a few-shot manner.

$$
\mathbf { x } _ { 0 } ^ { b } \longrightarrow \mathbf { x } _ { T } : \mathbf { x } _ { T } = \mathrm { D D I M } \left( \mathbf { x } _ { 0 } ^ { b } ; \tilde { \mathbf { s } } _ { \theta } ( \mathbf { x } _ { t } , t , \boldsymbol { \mathcal { O } } ) , 0 , T \right)
$$

$$
{ \bf x } _ { T } \longrightarrow { \bf x } _ { 0 } ^ { a } : { \bf x } _ { 0 } ^ { a } = \mathrm { D D I M } ( { \bf x } _ { T } ; \tilde { \bf s } _ { \theta } ( { \bf x } _ { t } , t , { \bf c } ) , T , 0 )
$$

Note that the DDIM in Eq. 7 and 8 is deterministic which means that once the prior model $ { { \widetilde { \mathbf { s } } } } _ { \theta }$ is determined, for any $\mathbf { x } _ { 0 } ^ { b }  \mathbf { x } _ { T }$ in Eq. 7, $\mathbf { x } _ { T }$ is deterministic and unique. The

“ø” Tokenizer Diffusion Bridge Tokenizer “<c>” Attention of before-image AD PAD PAD PAD " <s> PAD PAD PAD <E> PAD Before Image !"! <S> <E> P P P P !\$ <S> c c c <E> P After Image !#! "!# msAN Attention of after-image Masked ⊙ \* T2I Model T2I Model < S> c c c <E> PAD <E> PAD "!" Masked ⊙(# − %) + "! "# " s--E4--\*---- C C C c c c <E> PAD "" Attention Maps ∗ " "! ! ⊙ ! ⊙ (( − \*) Differential Attention Data Flow of Differential Attention Control ! Control

same applies to ${ \bf x } _ { T } \to { \bf x } _ { 0 } ^ { a }$ . This forms a unique one-to-one mapping for every pair of before-and-after images, which enables high-fidelity and stable image editing.

However, such a property makes the widely used textual inversion (Gal et al. 2023) invalid. This is because when optimizing the text embedding c, textual inversion adds random noise to the image at every step, and the final latent $\mathbf { x } _ { T }$ will not be deterministic anymore, which violates our constraint for $\mathbf { x } _ { T }$ . Essentially, the learning process of textual inversion tries to map a single image $\mathbf { x } _ { 0 } ^ { a }$ to the whole latent Gaussian distribution rather than the fixed noise vector $\mathbf { x } _ { T }$ . Apart from this, this stochastic procedure is inferior in image reconstruction and results in the learned text embeddings being unable to capture and recover all imagery details (Mokady et al. 2023). Mostly, textual inversion is effective in learning objects but not the whole image and details, and always introduces significant changes. As a result, it is not suitable for image editing (Nguyen et al. 2024). This motivates us to design the optimization procedure of the text embedding c with fixed start $\mathbf { x } _ { T }$ and end $\mathbf { x } _ { 0 } ^ { a }$ states.

Optimization conditioned on both start and end states of diffusion. The text embedding c is optimized to satisfy the diffusion process ${ \bf x } _ { T } \to { \bf x } _ { 0 } ^ { a }$ given the start and end states $\{ \mathbf { x } _ { T } , \mathbf { x } _ { 0 } ^ { a } \}$ . Such textualization based on the implicit bridge is conditioned on both the start and end states. Thus, we optimize the text embedding c to maximize the conditional probability $p _ { \mathbf { c } , \theta } \big ( \mathbf { x } _ { 0 } ^ { a } \big | \mathbf { x } _ { T } \big )$ which is parameterized by the T2I model. This is different from the general diffusion process in textual inversion (Gal et al. 2023) whose end state $\mathbf { x } _ { T }$ is an unconstrained Gaussian noise. We discuss the optimization design of our textualization as follows. With the deterministic DDIM, the process in Eq. 8 for each timestep $t$ is written as:

$$
\mathbf { x } _ { t - 1 } = \sqrt { \alpha _ { t - 1 } } \mathbf { f } _ { \theta } ( \mathbf { x } _ { t } , t , \mathbf { c } ) + \sqrt { 1 - \alpha _ { t - 1 } } \tilde { \mathbf { s } } _ { \theta } ( \mathbf { x } _ { t } , t , \mathbf { c } )
$$

$$
\mathbf { f } _ { \theta } ( \mathbf { x } _ { t } , t , \mathbf { c } ) = \frac { \mathbf { x } _ { t } - \sqrt { 1 - \alpha _ { t } } \tilde { \mathbf { s } } _ { \theta } ( \mathbf { x } _ { t } , t , \mathbf { c } ) } { \sqrt { \alpha _ { t } } }
$$

Considering that the start and end states $\{ \mathbf { x } _ { T } , \mathbf { x } _ { 0 } ^ { a } \}$ are given and fixed and the DDIM is deterministic, one intuitive way to optimize $\mathbf { c }$ is to first do forward pass ${ \bf x } _ { T } \to { \bf x } _ { 0 } ^ { a }$ to get the final output image $\hat { \mathbf { x } } _ { 0 }$ with Eq. 9 and calculate the loss with $\| \hat { \mathbf { x } } _ { 0 } - \mathbf { x } _ { 0 } ^ { a } \| _ { 2 }$ . However, this recurrent backpropagation needs to cache the intermediate result of $\scriptstyle \mathbf { s } _ { \theta }$ at each timestep $t$ , which is not feasible for multiple steps in diffusion.

To cope with this, we propose to optimize the predicted $\mathbf { x } _ { \mathrm { 0 } }$ , $\mathbf { f } _ { \theta } ( \mathbf { x } _ { t } , t )$ , with the ground-truth after-image $\mathbf { x } _ { 0 } ^ { a }$ . The predicted $\mathbf { x } _ { \mathrm { 0 } }$ , $\mathbf { f } _ { \boldsymbol { \theta } } ( \mathbf { \dot { x } } _ { t } , t )$ is an estimator of the final output at the current timestep, which indicates how far away the current result is from the desired output (Song, Meng, and Ermon 2021). Besides, we find that the losses in the initial steps are much larger than those in the final steps. Optimizing the loss at each timestep equally may cause $\hat { \mathbf { x } } _ { 0 }$ to deviate from $\mathbf { x } _ { 0 } ^ { a }$ at the final step. To make the last step $\hat { \mathbf { x } } _ { 0 } \approx \mathbf { x } _ { 0 } ^ { a }$ , we scale down the loss with time-aware scaling function $\bar { \beta } ( t )$ and the optimization at each timestep is:

$$
\mathcal { L } ( \mathbf { c } , t ) = \beta ( t ) \| \mathbf { f } _ { \theta } ( \mathbf { x } _ { t } , t , \mathbf { c } ) - \mathbf { x } _ { 0 } ^ { a } \| _ { 2 }
$$

# 3.3 Learning with Differential Attention Control

Attention injection can preserve the invariance of before-andafter editing, which enables saving the before-image content in image editing. It is generally used in inference but not in training the T2I model. Besides, current attention-based image editing methods (Hertz et al. 2023; Cao et al. 2023; Tumanyan et al. 2023) are not straightforward differential and do not support backpropagation.

Learning motivation. We introduce attention injection during training the text embedding c. Intuitively, the injected attention introduces the information from the before-image and thus disentangles the learned text embedding c from the before-image information. This makes the text embedding c solely learn the transformation and generalize to edit various images. Otherwise, the information irrelevant to the transformation from the before-image leaks to c such that it only fits on the before-image but cannot generalize on other images. In summary, our motivation for training with attention injection has two aspects: 1) In training, leveraging the injected attention capturing the invariance between the before-and-after images, the text embedding learns a visual transformation disentangled from a specific before-image and can be more

# Algorithm 1: Textualize Visual Prompt

1: Input: An image pair $\{ \mathbf { x } _ { 0 } ^ { b } , \mathbf { x } _ { 0 } ^ { a } \}$   
2: T2I diffusion model $\scriptstyle \mathbf { s } _ { \theta }$   
3: Number of training epochs $N$ ; Number of diffusion steps $T$   
4: Learning rate $\gamma$ ; Attention injection timestamp $\tau$   
5: Initialize c   
6: for $i = 1 , \cdots , N$ do   
7: ${ \bf x } _ { T } = \mathrm { D D I M } \left( { \bf x } _ { 0 } ^ { b } ; \tilde { \bf s } _ { \theta } ( { \bf x } _ { t } , t , \mathcal { O } ) , 0 , T \right)$   
8: for $t = T , \cdots$ , 1 do   
9: if $t < \tau$ then   
10: Build column-transformation matrix $\pmb { \Lambda }$ and mask $\mathbf { F }$   
11: Get $M _ { t }$ with attention injection with Eq. 12   
12: end if   
13: Calculate predicted ${ \bf x } _ { 0 }$ , $\mathbf { f } _ { \theta } ( \mathbf { x } _ { t } , t , M _ { t } )$ with Eq. 10   
14: Calculate next state $\mathbf { x } _ { t - 1 }$ with Eq. 9   
15: Calculate $\mathcal { L } ( \mathbf { c } , t ) = \beta ( t ) \| \mathbf { f } _ { \theta } ( \mathbf { x } _ { t } , t , \mathbf { c } , M _ { t } ) - \mathbf { x } _ { 0 } ^ { a } \| _ { 2 }$ with   
Eq. 13   
16: Update $\mathbf { c } = \mathbf { c } - \gamma \nabla \mathcal { L } ( \mathbf { c } , t )$   
17: end for   
18: end for   
19: Output: c

generalized. 2) In inference, the injected attention preserves the invariance of the before-image and achieves high fidelity. Module design. To make the attention injection differential and support backpropagation, we implement the attention injection with only multiplication and addition. The detailed process is depicted in Fig. 2. For simplicity, let $\boldsymbol { M } _ { t } ^ { b }$ , $M _ { t } ^ { a } \in \mathbb { R } ^ { j \times k }$ denote the attention weights from beforeand-after images at timestep $t$ , respectively, where $j$ is the feature dimension and $k$ is the length of total tokens. The text prompt c of the after-image includes $y$ tokens for learning the transformation. So, we keep the attentions of $y$ tokens in $\mathbf { \nabla } M _ { t } ^ { a }$ and replace the rest $k - y$ with attentions from $\boldsymbol { M } _ { t } ^ { b }$ . Besides, we make the $( y + 1 ) ^ { t h }$ attention in $\mathbf { \nabla } M _ { t } ^ { a }$ to be the <end> token attention of $\boldsymbol { M } _ { t } ^ { b }$ so that the new attention $M _ { t }$ also follows the linguistic format determined by the text encoder. We use a column transformation matrix $\pmb { \Lambda }$ and mask $\mathbf { F } = [ 1 , . . . , 0 , 1 , 1 ] \in \{ 0 , 1 \} ^ { k }$ to achieve this. Specifically, the column-transformation matrix is a modified identity matrix that switches the columns of 1 and $y + 1$ . It shifts the cross-attention of the <end> token in $\boldsymbol { M } _ { t } ^ { b }$ to the $( y + 1 ) ^ { t h }$ column, and the mask $\mathbf { F }$ injects all cross-attention weights of $\boldsymbol { M } _ { t } ^ { b }$ that are not in the position of tokens of $\mathbf { c }$ , into $M _ { t }$ . For self-attention, we do not include $\pmb { \Lambda }$ since self-attention is not related to text. Consequently, the training objective in Eq. 11 is added with the new attention and becomes Eq. 13. $\odot$ is column-multiplication that multiplies each column in the attention matrix with a scalar value of 0 or 1.

$$
\begin{array} { c } { M _ { t } = M _ { t } ^ { b } \mathbf { A } \odot \mathbf { F } + M _ { t } ^ { a } \odot ( 1 - \mathbf { F } ) } \\ { \mathcal { L } ( \mathbf { c } , t ) = \beta ( t ) \lVert \mathbf { f } _ { \theta } ( \mathbf { x } _ { t } , t , \mathbf { c } , M _ { t } ) - \mathbf { x } _ { 0 } ^ { a } \rVert _ { 2 } } \end{array}
$$

# 3.4 Visual Prompt for Image Editing

Finally, by constructing a diffusion bridge with the T2I model, we distill the I2I transformation defined by an image pair $\mathbf { x } _ { 0 } ^ { b }$ and $\mathbf { x } _ { 0 } ^ { a }$ into a text embedding c. Our framework is in Algorithm 1. In inference, the new image goes through the diffusion bridge as the before-image under the guidance of the learned text embedding. The output is the edited image with the desired transformation. Our method also adapts to multiple image pairs with the same transformation. For each epoch, we randomly select an image pair for training.

# 4 Evaluation on Real Images

# 4.1 Experiment setup

Baselines and implementations. The closest work to ours is visual instruction inversion (VII) (Nguyen et al. 2024) which also learns image transformation by an image pair. VII is based on the pretrained TI2I model, InstructPix2Pix (InsP2P) (Brooks, Holynski, and Efros 2023). We also add InsP2P with ground-truth text instructions, which are actually unavailable. Additionally, we add diffusion image analogy DIA (Šubrtová et al. 2023) that transfers the analogy relation between the image pair to new images, and visual in-context learning Analogist (Gu et al. 2024) that inpainting the edited image with in-context learning, and style transfer methods DiffuseIT (Kwon and Ye 2023) that transfer the style of afterimage to the new image. We implement our method based on SD v1.5, consistent with VII and InsP2P. All baselines are evaluated with their official codebases.

Datasets. We evaluate our framework on real images based on the two latest benchmarks Dreambooth (Ruiz et al. 2023) and PIE (Ju et al. 2024). Following the procedure of creating image pairs in InsP2P and VII, we also use P2P (Hertz et al. 2023) to generate different image pairs with different text instructions as visual prompts. We use this dataset to evaluate the generalization and fidelity of our method since the data is not included in the training set of SD v1.5 or InsP2P. The details of implementation and dataset are in the Appendix D.

# 4.2 Comparion with previous methods

Real image editing and fidelity. We evaluate our method in real image editing. As shown in Fig. 3, our method produces higher fidelity and preserves better invariant details of the original image under different editing types. This indicates that our method can learn more accurate and generalized editing effects from the image pair. In contrast, style transfer (DiffuseIT) and image analogy (DIA) methods cannot disentangle the delicate transformation from irrelevant image contents, leading to ambiguous and coarse results. VII and Analogist cannot preserve the structural and invariant details of the original image. This is unacceptable for image tone transformation such as HDR, that has strict fidelity requirements. With the ground-truth editing instructions, the results of InsP2P are low-fidelity and do not exactly follow the desired pattern in visual prompts. This shows the necessity of visual prompts for indescribable visual transformation.

Leakage of irrelevant content and disentanglement. Our method aims to learn a disentangled editing effect by training with attention injection. The learned text embedding should not encode irrelevant information from visual prompts. The results show that our method only learns the editing effect but does not introduce the color, objects, or textures from the visual prompts. Concretely, Fig. 4 and 3 show that VII introduces the bear and its texture (first row), dogs (second row), and color (third row) to the edited images. DiffuseIT learns the transformation only from the single after-image and cannot learn the contrasted and accurate editing from the image pair. DIA cannot accurately learn the disentangled effect from the image pair, yielding inaccurate results.

![](images/36bc832d42fec8a537818d31c659245002b6c3b538fa902992613901a9598a26.jpg)  
Figure 3: Qualitative comparisons on real images. Visual prompts with different editing types and different levels of geometric changes. Our method generalizes to different editing types and scenes while preserving different levels of geometric structures.

![](images/cee99f80ddce9a3ac08d6fc34b6c19b8f08168fa46a67dd3df1de1a5504f9277.jpg)  
Figure 4: Generalization in heterogeneous scenes and categories. Results of tone and style editing show our method does not introduce leaked content (bear texture, dog face, color) from visual prompts when the category and scene of test images differ greatly from the visual prompts.

Generalization. Fig. 4 also validates the generalization in heterogeneous scenes and categories. Our method can edit images of various scenes and categories with high fidelity whereas VII (based on the TI2I model) degrades significantly when test images’ scenes or categories differ greatly from visual prompts. This also validates our motivation to use T2I model with larger prior and differential attention control.

Necessity of visual prompts and ambiguity of text prompt. We demonstrate the value of visual prompts from two sides. First, some editing effects cannot be accurately identified or explained by observation. Second, even if the effect can be identified as text such as ‘watercolor’, the text prompt can be ambiguous, and the ‘watercolor’ defined by the text prompt can be different from the ‘watercolor’ defined in the visual prompt. We demonstrate this in Fig. 13 in Appendix. There are various sub-styles of painting. The ‘psychedelic painting is difficult to recognize and the ‘watercolor’ effect in the visual prompt differs from that in InsP2P using text.

Quantitative comparison and user study. We extend evaluating metrics in Nguyen et al. (2024); Ju et al. (2024) with DINOv2 and VIE (Ku et al. 2024) based on GPT-4o. The VisualCLIP (V-CLIP) measures the cosine similarity of the editing direction between the before/after example pair and test pair based on their CLIP embeddings, indicating the agreement of the editing direction with the visual prompt. The ImageCLIP (I-CLIP) measures the cosine similarity between the edited and original images, indicating the fidelity to the original test image. The same applies to DINOv2. Similarly, A user study evaluates the preservation of test image invariance (Fidelity) and the analogy of the edit direction (Edit Analogy). VIE scores, evaluated by GPT-4o, assess these properties similarly to human criteria (Ku et al. 2024). Our method achieves competitive results across all metrics, consistent with qualitative results. See Appendix D.

Table 1: Quantitative comparison and user study. We evaluate editing direction and fidelity in classical metrics, CLIP, DINO, GPT-4o, and human preference. We achieve better editing consistency aligned with the visual prompts and fidelity on all metrics.   

<html><body><table><tr><td>Method</td><td>PSNR↑ SSIM↑ LPIPS↓|V-CLIP↑ I-CLIP↑V-DINO↑ I-DINO↑|V-VIE↑ I-VIE↑|Edit Analogy↑ Fidelity↑ Overall↑</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>VII</td><td>12.76</td><td>0.4460</td><td>0.5238</td><td>0.1819</td><td>0.7012</td><td>0.1564</td><td>0.7144</td><td>1.92</td><td>1.69</td><td>3.21</td><td>1.33</td><td>2.27</td></tr><tr><td>InsP2P</td><td>15.75</td><td>0.5977</td><td>0.3168</td><td>0.2674</td><td>0.8114</td><td>0.2247</td><td>0.8763</td><td>3.01</td><td>2.93</td><td>4.21</td><td>2.93</td><td>3.57</td></tr><tr><td>DiffuseIT|</td><td>12.06</td><td>0.3610</td><td>0.5261</td><td>0.1103</td><td>0.7180</td><td>0.1167</td><td>0.7538</td><td>2.11</td><td>3.32</td><td>2.98</td><td>3.28</td><td>3.13</td></tr><tr><td>DIA</td><td>17.78</td><td>0.5344</td><td>0.4279</td><td>0.1201</td><td>0.7453</td><td>0.1010</td><td>0.7429</td><td>1.18</td><td>4.01</td><td>2.14</td><td>3.87</td><td>3.01</td></tr><tr><td>Analogist|</td><td>15.26</td><td>0.5102</td><td>0.4061</td><td>0.2123</td><td>0.7037</td><td>0.1914</td><td>0.7419</td><td>2.51</td><td>3.24</td><td>3.98</td><td>3.02</td><td>3.50</td></tr><tr><td>Ours</td><td>24.57</td><td>0.8091</td><td>0.1197</td><td>0.2750</td><td>0.8178</td><td>0.3234</td><td>0.9073</td><td>4.48</td><td>4.29</td><td>4.81</td><td>4.62</td><td>4.72</td></tr></table></body></html>

![](images/194c02fde28da79305e14afe96a13229bc2e677a56b7521bb222cc69d41cc22b.jpg)  
Figure 5: Train and test results with/without attention. 1st column: the visual prompt; 2nd column: training results without/with injected attention. 3rd column: test images; 4&5 columns: test results without and with attention control.

# 4.3 Ablation and Analysis

Differential attention control. The differential attention control injects the attention of the before-image into that of the after-image. We validate its two benefits with experiments in Fig. 5. First, during training, the injected attention preserves the invariance making the text embedding fit the after-image better. As shown in Fig. 5 2nd column, the model produces a detailed dog portrait with injected attention, unlike the coarse version without it. This helps the text embedding focus on the transformation, improving generalization. Second, during testing, the module can preserve the invariance from the before-image, achieving high fidelity. In 4&5 columns, the attention module allows the text embedding to edit different images accurately. Without it, outputs are distorted, indicating overfitting to the training image pair.

Time-aware scaling loss. We analyzed the predicted $\mathbf { x } _ { 0 }$ , $\mathbf { f } _ { \theta } ( \mathbf { x } _ { t } , t )$ along all timesteps. The observation in Fig. 6 shows that equally optimizing the loss at each timestep may reduce the loss of predicted $\mathbf { x } _ { \mathrm { 0 } }$ in the early steps too much and make the final output deviate from the ground truth $\mathbf { x } _ { 0 } ^ { a }$ . Meanwhile, the output that is closest to $\mathbf { x } _ { 0 } ^ { a }$ appears earlier and the image quality degrades undesirably. After scaling, the final output becomes closest to $\mathbf { x } _ { 0 } ^ { a }$ , and the image quality also improves. Limitation. One limit is the prior of the T2I model. Despite using the more general T2I model with a larger prior, editing real images remains challenging since many practical transformations are out of the model’s prior. Learning visual prompts is essentially a process of extracting and composing the prior of the text-to-image. Such prior restricts our method to learning visual prompts and editing for real images. Another is choosing the timestamp of the attention injection for different editing types since the invariance of them is different. To fit the delicate transformation, we need to choose the appropriate timestep for each editing type in training.

![](images/05604f5f34501323274952b399def6c5e309e05f69a4e34bbf7e252498633fdc.jpg)  
Figure 6: The predicted $\mathbf { x } _ { \mathrm { 0 } }$ at different timesteps. The results of early steps without scaling are closer to $\mathbf { x } _ { 0 } ^ { b }$ but the final output deviates. With scaling, the results of early steps are farther but the final output approaches $\mathbf { x } _ { 0 } ^ { b }$ .

# 5 Conclusion

This paper introduces a method using a single, general textto-image model as a diffusion bridge for learning complex image transformations from visual prompts. This approach removes the dependence on a text-guided image-to-image model and extensive triplet data of text and before-and-after images, which are harder to create than simple text-image pairs and are not scalable on a large scale. We also demonstrate that training with differential attention allows the visual transformation to be embedded into the text space, disentangling the text embedding from the irrelevant before-image information and enhancing the generalizability of text embeddings for various image edits. Results show that leveraging the scalable T2I model achieves high-fidelity image editing with visual prompts, suggesting a new paradigm for leveraging scalable generative models for detailed visual prompts.