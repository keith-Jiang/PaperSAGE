# Neural Networks Perform Sufficient Dimension Reduction

Shuntuo Xu, Zhou $\mathbf { Y } \mathbf { u } ^ { * }$

Key Laboratory of Advanced Theory and Application in Statistics and Data Science – MOE School of Statistics, East China Normal University, Shanghai, China oaksword@163.com, zyu $@$ stat.ecnu.edu.cn

# Abstract

This paper investigates the connection between neural networks and sufficient dimension reduction (SDR), demonstrating that neural networks inherently perform SDR in regression tasks under appropriate rank regularizations. Specifically, the weights in the first layer span the central mean subspace. We establish the statistical consistency of the neural network-based estimator for the central mean subspace, underscoring the suitability of neural networks in addressing SDR-related challenges. Numerical experiments further validate our theoretical findings, and highlight the underlying capability of neural networks to facilitate SDR compared to the existing methods. Additionally, we discuss an extension to unravel the central subspace, broadening the scope of our investigation.

# Introduction

Neural networks have achieved significant success in a tremendous variety of applications (Lee and Abu-El-Haija 2017; Silver et al. 2018; Jumper et al. 2021; Brandes et al. 2022; Thirunavukarasu et al. 2023). As the most essential foundation, a feedforward neural network is typically constructed by a series of linear transformations and nonlinear activations. To be specific, a function $f$ implemented by a feedforward neural network with $L$ layers can be represented as

$$
f ( x ) = \phi _ { L } \circ \sigma _ { L - 1 } \circ \phi _ { L - 1 } \circ \cdot \cdot \cdot \circ \sigma _ { 1 } \circ \phi _ { 1 } ( x ) .
$$

Here, $\scriptscriptstyle \mathrm { ~ o ~ }$ is the functional composition operator, $\phi _ { i } ( z ) ~ =$ $W _ { i } ^ { \top } z + b _ { i }$ denotes the linear transformation and $\sigma _ { i }$ represents the elementwise nonlinear activation function. Despite a clear architecture, the formula (1) provides limited insight into how the information within the input data is processed by the neural networks. To comprehensively understand how neural networks retrieve task-relevant information, there have been extensive efforts towards unraveling their interpretability (Doshi-Velez and $\mathrm { K i m } \ 2 0 1 7$ ; Guidotti et al. 2018; Zhang et al. 2021). For instance, post-hoc interpretability (Lipton 2018) focused on the predictions generated by neural networks, while disregarding the detailed mechanism and feature importance. Ghorbani, Abid, and

Zou (2019) highlighted the fragility of this type of interpretation, as indistinguishable perturbations could result in completely different interpretations.

For the sake of striking a balance between the complexity of representation and the power of prediction, Tishby and Zaslavsky (2015) and Saxe et al. (2019) pioneered the interpretability of deep neural networks via the information bottleneck theory. Ghosh (2022) further linked the information bottleneck to sufficient dimension reduction (SDR) (Li 1991; Cook 1996), which is a rapidly developing research field in regression diagnostics, data visualization, pattern recognition and etc.

In this paper, we provide a theoretical understanding of neural networks for representation learning from the SDR perspective. Let $x \in \mathbb { R } ^ { p }$ represent the covariates and $y \in$ $\mathbb { R }$ represent the response. Consider the following regression model

$$
\begin{array} { r } { y = f _ { 0 } ( B _ { 0 } ^ { \top } x ) + \epsilon , } \end{array}
$$

where $B _ { 0 } ~ \in ~ \mathbb { R } ^ { p \times d }$ is a nonrandom matrix with $d \ \leq \ p$ , $f _ { 0 } : \mathbb { R } ^ { d }  \mathbb { R }$ is an unknown function, and $\epsilon$ is the noise such that $\mathbb { E } ( \epsilon | x ) = 0$ and $\mathrm { V a r } ( \epsilon | x ) = \nu ^ { 2 }$ for some positive constant $\nu$ . Intuitively, the semiparametric and potentially multi-index model (2) asserts that the core information for regression is encoded in the low-dimensional linear representation $B _ { 0 } ^ { \top } x$ . In the literature of SDR, model (2) has been extensively studied. Based on model (2) , Cook and Li (2002) proposed the objective of sufficient mean dimension reduction as

$$
y \perp \perp \mathbb { E } ( y | x ) | B ^ { \top } x
$$

for some matrix $B$ , where means statistical independence. Denote the column space spanned by $B$ as $\Pi _ { B }$ , which is commonly referred to as the mean dimension-reduction subspace. It is evident that the matrix $B$ satisfying (3) is far away from uniqueness. Hence, we focus on the intersection of all possible mean dimension-reduction subspace, which is itself a mean dimension-reduction subspace under mild conditions (e.g., when the domain of $x$ is open and convex; see Cook and Li (2002)), and term it the central mean subspace. Agreeing to condition (3), $B _ { 0 }$ defined in model (2) yields the central mean subspace, denoted as $\Pi _ { B _ { 0 } }$ , under certain assumption. Statistical estimation and inference about the central mean subspace is the primary goal of SDR. Popular statistical methods for recovering central mean subspace include ordinary least squares (Li and Duan 1989), principal Hessian directions (Li 1992), minimum average variance estimation (Xia et al. 2002), semiparametric approach (Ma and Zhu 2013), generalized kernel-based dimension reduction (Fukumizu and Leng 2014), and many others. Although numerous studies have demonstrated the ability of neural networks to approximate complex functions (Hornik, Stinchcombe, and White 1989; Barron 1993; Yarotsky 2017; Shen, Yang, and Zhang 2021) and to adapt low-dimensional structures (Bach 2017; Bauer and Kohler 2019; SchmidtHieber 2020; Abbe, Adsera, and Misiakiewicz 2022; Jiao et al. 2023; Troiani et al. 2024), it is of subsequent interest to investigate whether neural networks are capable of correctly identifying the intrinsic structure encapsulated in $B _ { 0 }$ , thereby deepening the interpretation of neural networks.

Our study was inspired by an observation that the weight matrix in the first layer, i.e., $W _ { 1 }$ , could accurately detect the presence of $B _ { 0 }$ in a toy data set, with the rank regularization. Specifically, consider the toy model $y = ( B _ { 0 } ^ { \mathcal { \tilde { T } } } x ) _ { - } ^ { 3 } + \epsilon$ where $\bar { x } \sim \mathrm { U n i f o r m } ( [ 0 , 1 ] ^ { 1 0 } )$ , $B _ { 0 } = ( 1 , - 2 , 0 , . . . , 0 ) ^ { \top }$ and $\epsilon \sim \mathrm { N o r m a l } ( 0 , 0 . 1 ^ { 2 } )$ . We trained neural networks using the least squares loss with $W _ { 1 } = W _ { 1 1 } W _ { 1 2 }$ where $W _ { 1 1 } \in \mathbb { R } ^ { 1 0 \times q }$ and $\bar { W _ { 1 2 } } \in \mathbb { R } ^ { q \times 6 4 }$ for $q = 1 , \ldots , 1 0$ . Evidently, the rank of $W _ { 1 }$ did not exceed $q$ . It was then observed that for each $q$ , (i) $B _ { 0 }$ was closely contained within the column space of $W _ { 1 1 }$ , as the absolute cosine similarity between $B _ { 0 }$ and its projection on $\Pi _ { W _ { 1 1 } }$ was close to 1, and (ii) the leading eigenvector of $W _ { 1 } W _ { 1 } ^ { \top }$ closely aligned with $B _ { 0 }$ (see Figure 1). This observation indicates that the first layer of the neural network may potentially discover the underlying low-dimensional intrinsic structure.

It is important to note that the application of neural networks for estimating the central mean subspace based on model (2) was previously explored by Kapla, Fertl, and Bura (2022) through a two-stage method. The first stage focused on obtaining a preliminary estimation of $\Pi _ { B _ { 0 } }$ , which subsequently served as an initial point for the joint estimation of $\Pi _ { B _ { 0 } }$ and $f _ { 0 }$ in the second stage. Given our toy example, however, it is prudent to critically evaluate the necessity of the first stage. Furthermore, their work lacks a comprehensive theoretical guarantee. Additionally, another related work conducted by Liang, Sun, and Liang (2022) concentrated on seeking nonlinear sufficient representations. In contrast, we focus more on revealing the fundamental nature of neural networks.

![](images/8dd333af202e3c45ab1061356ea08673d24b4dce256ba2ccb528b8be98cd5d24.jpg)  
Figure 1: Absolute cosine similarity between (i) $B _ { 0 }$ and its projection on $\Pi _ { W _ { 1 1 } }$ (the dot line with triangle marks), (ii) $B _ { 0 }$ and the leading eigenvector of $W _ { 1 } W _ { 1 } ^ { \top }$ (the solid line with square marks).

We in this paper show that, with suitable rank regularization, the first layer of a feedforward neural network conducts SDR in a regression task, wherein $d ( W _ { 1 1 } , B _ { 0 } ) \to 0$ in probability for certain distance metric $d ( \cdot , \cdot )$ . Furthermore, numerical experiments provide empirical evidences supporting this result, while demonstrating the efficiency of neural networks in addressing the issue of SDR.

Throughout this paper, we use $\| v \| _ { 2 }$ to represent the Euclidean norm of a vector $v$ . For a matrix $A$ , $\| A \| _ { F }$ is the Frobenius norm of $A$ , $\pi _ { A } = A ( A ^ { \top } A ) ^ { - } A ^ { \top }$ denotes the projection matrix of $A$ where $A ^ { - }$ is the generalized inverse of $A$ , and $\Pi _ { A }$ stands for the linear space spanned by the columns of $A$ . For a measurable function $f : \mathcal { X } \ \to \ \mathbb { R } .$ , $\| f \| _ { L ^ { 2 } ( \mu ) }$ represents the $L ^ { 2 }$ norm of $f$ with respect of a given probability measure $\mu$ , and $\| f \| _ { L ^ { \infty } ( \mathcal { X } ) } \ = \ \operatorname* { s u p } _ { x \in \mathcal { X } } | f ( x ) |$ represents the supreme norm of $f$ over a set $\mathcal { X }$ . $B ( \mathcal { X } )$ is the unit ball induced by $\chi$ , such that $B ( { \mathcal { X } } ) \subset { \mathcal { X } }$ and $\lVert \boldsymbol { v } \rVert _ { 2 } \leq 1$ for any $v \in \mathcal X$ .

# Theoretical Justificatitons Population-Level Unbiasedness

Suppose the true intrinsic dimension $d$ defined in model (2) is known and the covarites $x \ \in \ B ( [ 0 , 1 ] ^ { p } )$ . As $B _ { 0 }$ is not identifiable in (2) and (3) , it is assumed without loss of generality that $B _ { 0 } ^ { \top } B _ { 0 } = { \dot { I } } _ { d }$ where $I _ { d }$ is the identity matrix with $d$ rows. By defining $\bar { \Psi _ { d } } = \{ \bar { B } ^ { \prime } \in \mathbb { R } ^ { p \times d } : \bar { B } ^ { \top } B = I _ { d } \}$ we have $B _ { 0 } \in \Psi _ { d }$ . In this paper, we consider the following neural network function class

$$
\begin{array} { r l } & { \mathcal { F } _ { \mathcal { L } , \mathcal { M } , \mathcal { S } , \mathcal { R } } = \Big \{ } \\ & { f ( x ) = \phi _ { L } \circ \sigma _ { L - 1 } \circ \phi _ { L - 1 } \circ \cdot \cdot \cdot \circ \sigma _ { 1 } \circ \phi _ { 1 } ( B ^ { \top } x ) : } \\ & { B \in \Psi _ { d } , \phi _ { i } ( z ) = W _ { i } ^ { \top } z + b _ { i } , W _ { i } \in \mathbb { R } ^ { d _ { i } \times d _ { i + 1 } } , i = 1 , \cdot \cdot \cdot , L , } \end{array}
$$

$$
\| f \| _ { L ^ { \infty } ( \mathcal { B } ( [ 0 , 1 ] ^ { p } ) ) } \leq \mathcal { R } \Big \}
$$

The activation functions $\sigma _ { i } ( i = 1 , \ldots , L - 1 )$ utilized are the rectified linear units, i.e., $\sigma _ { i } ( x ) = \operatorname* { m a x } ( x , 0 ) .$ . We emphasize that $\mathcal { F } _ { \mathcal { L } , \mathcal { M } , S , \mathcal { R } }$ incorporates rank regularization of $d$ in the first layer. For arbitrary $f \in \mathcal { F } _ { \mathcal { L } , \mathcal { M } , \mathcal { S } , \mathcal { R } }$ , we use $\boldsymbol { \mathcal { T } } ( \boldsymbol { f } )$ to represent the component $B \in \Psi _ { d }$ in the first layer of $f$ .

For a regression task, the theoretical study is highly related to the smoothness of underlying conditional mean function (Yang and Barron 1999). Here, we introduce the following assumptions of model (2).

Assumption 1 (Smoothness). $f _ { 0 }$ is a H¨older continuous function of order $\alpha \in ( 0 , 1 ]$ with constant $\lambda _ { i }$ , i.e., $| f _ { 0 } ( x ) -$ $f _ { 0 } ( z ) | \leq \lambda \| x - z \| _ { 2 } ^ { \alpha }$ for any $x , z \in [ 0 , 1 ] ^ { d }$ . Additionally, $\| f _ { 0 } \| _ { L ^ { \infty } ( [ 0 , 1 ] ^ { d } ) } \leq \mathcal { R } _ { 0 }$ for some constant $\mathcal { R } _ { 0 } \geq 1$ .

Assumption 2 (Sharpness). For any scalar $\delta > 0$ and $B \in$ $\Psi _ { d } ,$ , $\Vert \pi _ { B } - \pi _ { B _ { 0 } } \Vert _ { F } > \delta$ implies $\mathbb { E } \{ \mathrm { V a r } [ f _ { 0 } ( B _ { 0 } ^ { \top } x ) | B ^ { \top } x ] \} >$ $M ( \delta )$ for some $M ( \delta ) > 0$ .

Assumption 3. y is sub-exponentially distributed such that there exists $\tau > 0$ satisfying $\mathbb { E } [ \exp ( \tau \vert y \vert ) ] < \infty$ .

Assumption 1 is a technical condition to study the approximation capability of neural networks (Shen, Yang, and Zhang 2020). Alternatively, other functional spaces, such as the Sobolev space, can also be employed for this purpose (Abdeljawad and Grohs 2022; Shen et al. 2022). Furthermore, Assumption 2 establishes a restriction on the sharpness of $f _ { 0 }$ . Consider the case that $f _ { 0 }$ is solely a constant function as zero. Then, a trivial neural network by setting all the parameters except $B$ to zero perfectly fits $f _ { 0 }$ , regardless of the value of $B$ . With Assumption 2, it becomes difficult to accurately capture the overall behavior of $f _ { 0 } ( B _ { 0 } ^ { \top } x )$ using a biased $B$ . In other words, $B _ { 0 } ^ { \top } x$ is sufficient and necessary for recovering $\mathbb { E } ( y | x )$ , i.e., $\Pi _ { B _ { 0 } }$ is the central mean subspace. Similar condition was also adopted in Theorem 4.2 of Li and Dong (2009) to distinguish sufficient directions $B _ { 0 }$ from other $B$ . Assumption 3 is a commonly used condition for applying empirical process tools and concentration inequalities (Van der Vaart 2000; Zhu, Jiao, and Jordan 2022).

Theorem 1. Suppose that Assumptions $\boldsymbol { { \mathit { 1 } } }$ and 2 hold. Let $\begin{array} { r } { f ^ { * } = \mathrm { a r g m i n } _ { f \in \mathcal { F } _ { \mathcal { L } , \mathcal { M } , \mathcal { S } , \mathcal { R } } } \mathbb { E } [ y - f ( x ) ] ^ { 2 } } \end{array}$ , then

$$
\Pi _ { \mathcal { T } ( f ^ { * } ) } = \Pi _ { B _ { 0 } } ,
$$

provided that $\mathcal { R }$ is sufficiently large, and $\mathcal { L }$ and $\mathcal { M }$ tend to infinity.

Theorem 1 builds a bridge connecting neural networks and SDR. It demonstrates that neural networks indeed achieve representation learning as the first layer of the optimal neural network perfectly reaches the target of SDR at population level. Theorem 1 also inspires us to perform SDR based on neural networks with a minor adjustment for the first layer. The detailed proof of Theorem 1 can be found in Section Proofs.

# Sample Estimation Consistency

We now investigate the theoretical property of the neural network-based sample-level estimator for SDR. Given sample observations $\bar { \mathcal { D } _ { n } } \ = \ \{ ( X _ { 1 } , Y _ { 1 } ) , \ldots , ( X _ { n } , Y _ { n } ) \}$ , where $( X _ { i } , Y _ { i } )$ is an independent copy of $( x , y )$ for $i = 1 , \ldots , n ,$ , the commonly used least squares loss is adopted, i.e.,

$$
L _ { n } ( f ) = { \frac { 1 } { n } } \sum _ { i = 1 } ^ { n } [ Y _ { i } - f ( X _ { i } ) ] ^ { 2 } .
$$

Denote the optimal neural network estimator at the sample level as

$$
\hat { f } _ { n } = \underset { f \in \mathcal { F } _ { \mathcal { L } , \mathcal { M } , \mathcal { S } , \mathcal { R } } } { \mathrm { a r g m i n } } L _ { n } ( f ) .
$$

$\mathcal { T } ( \hat { f } _ { n } )$ is then the sample estimator approximately spanning the central mean subspace.

To examine the closeness between $\Pi _ { \mathcal { T } ( \hat { f } _ { n } ) }$ and $\Pi _ { B _ { 0 } }$ , we define the distance metric $d ( \cdot , \cdot )$ as

$$
d ( B , B _ { 0 } ) = \operatorname* { m i n } _ { Q \in \mathcal { Q } } \| B _ { 0 } - B Q \| _ { 2 } ,
$$

where $B \in \Psi _ { d }$ and $\mathcal { Q }$ is the collection of all orthogonal matrices in $\mathbb { R } ^ { d \times d }$ . We see that $d ( B , B _ { 0 } ) = 0$ if and only if $\Pi _ { B } = \Pi _ { B _ { 0 } }$ . And we make the following assumption essentially another view of Assumption 2.

Assumption 4. For any positive scalar $\delta$ , $d ( B , B _ { 0 } ) > \delta$ implies $\mathbb { E } \{ \mathrm { V a r } [ f _ { 0 } ( B _ { 0 } ^ { \top } x ) | \hat { B } ^ { \top } x ] \} > M _ { 1 } ( \delta )$ for some $M _ { 1 } ( \delta ) >$ 0.

Theorem 2. Suppose the Assumptions $\jmath$ , 3 and 4 hold. Then we have

$$
d ( \mathcal { T } ( \hat { f } _ { n } ) , B _ { 0 } ) \xrightarrow { P } 0 ,
$$

when the depth $\mathcal { L } = O ( n ^ { d / ( 2 d + 8 \alpha ) } )$ , the width $\mathcal { M } = O ( 1 )$ and $\mathcal { R } \geq \mathcal { R } _ { 0 }$ .

Theorem 2 confirms that $\Pi _ { \mathcal { T } ( \hat { f } _ { n } ) }$ converges to $\Pi _ { B _ { 0 } }$ in probability. As a result, under the least squares loss, the neural networks, with appropriate rank regularization, provide a consistent estimator of the central mean subspace. Therefore, it is promising to adopt the neural network function class $\mathcal { F } _ { \mathcal { L } , \mathcal { M } , S , \mathcal { R } }$ to address sufficient mean dimension reduction problems. More importantly, this approach offers several advantages over existing methods. Unlike SIR (Li 1991), SAVE (Cook and Weisberg 1991) and PHD (Li 1992), our method does not impose stringent probabilistic distributional conditions on $x$ , such as linearity, constant variance and normality assumptions. Compared to MAVE (Xia et al. 2002), our method adopts the powerful neural networks to estimate the link function, thereby working similar or better than classical nonparametric tools based on firstorder Taylor expansion. We present the proof of Theorem 2 in Section Proofs.

The results illustrated in Theorem 1 and 2 are contingent on the availability of true intrinsic dimension $d$ . In the case where $d$ is unknown, a natural modification for $\mathcal { F } _ { \mathcal { L } , \mathcal { M } , S , \mathcal { R } }$ is to set $\boldsymbol { B } \in \mathbb { R } ^ { p \times p }$ without rank regularization. Under Assumptions 1 and 2, in this scenario, the optimal $f ^ { * }$ at the population level still ensures that $\Pi _ { \mathcal { T } ( f ^ { * } ) }$ encompasses $\Pi _ { B _ { 0 } }$ , where the sharpness of $f _ { 0 }$ plays a crucial role; see the Supplementary Material for more details.

# Simulation Study

From the perspective of numerical experiments, we utilized simulated data sets to demonstrate that (i) the column space of $\mathcal { T } ( \hat { f } _ { n } )$ approached the central mean subspace $\Pi _ { B _ { 0 } }$ as sample size $n$ increased, (ii) the performance of neural network-based method in conducting SDR was comparable to classical methods. In some cases, the neural network-based method outperformed classical methods, particularly when the latent intrinsic dimension $d$ was large. Five commonly used methods, sliced inverse regression (SIR), sliced average variance estimation (SAVE), principal Hessian directions (PHD), minimum average variance estimation (MAVE) and generalized kernel dimension reduction (GKDR), were included for comparisons.

The neural network-based method was implemented in Python using PyTorch. Specifically, we utilized a linear layer without bias term to represent the matrix $B$ (recall that we suppose $d$ is known), which was further appended by a fullyconnected feedforward neural network with number of neurons being $h { - } h / 2 { - } 1$ . Here, we set $h = 6 4$ when the sample size was less than 1000, and set $h = 1 2 8$ when sample size was between 1000 and 2000. Overall, the neural network architecture was $p - d - h - h / 2 - 1$ . Code is available at https://github.com/oaksword/DRNN.

<html><body><table><tr><td colspan="2"></td><td>NN</td><td>MAVE</td><td></td><td>GKDR</td><td>SIR</td><td>SAVE</td><td>PHD</td></tr><tr><td rowspan="6">Setting 1</td><td>(n,p)= (100,10)</td><td>mean</td><td>0.135</td><td>0.160</td><td>0.388</td><td>0.897</td><td>0.315</td><td>0.623</td></tr><tr><td></td><td>std</td><td>0.064</td><td>0.056</td><td>0.126</td><td>0.186</td><td>0.082</td><td>0.150</td></tr><tr><td>(n,p)= (200,30)</td><td>mean</td><td>0.276</td><td>0.214</td><td>1.012</td><td>0.946</td><td>0.298</td><td>0.807</td></tr><tr><td>(n,p)= (300,30)</td><td>std</td><td>0.274</td><td>0.051</td><td>0.299</td><td>0.113</td><td>0.049</td><td>0.107</td></tr><tr><td></td><td>mean</td><td>0.120</td><td>0.130</td><td>0.542</td><td>0.900</td><td>0.337</td><td>0.709</td></tr><tr><td></td><td>std</td><td>0.038</td><td>0.029</td><td>0.166</td><td>0.136</td><td>0.063</td><td>0.108</td></tr><tr><td rowspan="6">Setting 2</td><td>σ=0.1</td><td>mean</td><td>0.296</td><td>0.730</td><td>1.130</td><td>0.665</td><td>0.319</td><td>1.550</td></tr><tr><td>σ=0.2</td><td>std</td><td>0.126</td><td>0.312</td><td>0.271</td><td>0.166</td><td>0.076</td><td>0.126</td></tr><tr><td></td><td>mean</td><td>0.628</td><td>0.899</td><td>1.155</td><td>0.705</td><td>0.338</td><td>1.526</td></tr><tr><td>σ=0.5</td><td>std</td><td>0.269</td><td>0.329</td><td>0.237</td><td>0.149</td><td>0.074</td><td>0.138</td></tr><tr><td></td><td>mean std</td><td>1.197 0.213</td><td>1.187 0.204</td><td>1.278 0.200</td><td>0.830</td><td>0.367</td><td>1.567</td></tr><tr><td>n=200</td><td></td><td></td><td></td><td></td><td>0.169</td><td>0.072</td><td>0.131</td></tr><tr><td rowspan="6">Setting 3</td><td></td><td>mean</td><td>0.639</td><td>1.246</td><td>1.759</td><td>1.669</td><td>1.728</td><td>1.740</td></tr><tr><td>n= 500</td><td>std</td><td>0.418</td><td>0.289</td><td>0.149</td><td>0.235</td><td>0.136</td><td>0.221</td></tr><tr><td></td><td>mean</td><td>0.248</td><td>1.076</td><td>1.752</td><td>1.650</td><td>1.683</td><td>1.721</td></tr><tr><td>n= 1000</td><td>std</td><td>0.242</td><td>0.331</td><td>0.125</td><td>0.271</td><td>0.201</td><td>0.228</td></tr><tr><td></td><td>mean</td><td>0.075 0.081</td><td>0.924</td><td>0.554</td><td>1.652</td><td>1.678</td><td>1.737</td></tr><tr><td>n=1000,d= 4</td><td>std</td><td></td><td>0.382</td><td>0.371</td><td>0.259</td><td>0.245</td><td>0.191</td></tr><tr><td rowspan="5">Setting 4</td><td></td><td>mean</td><td>0.127</td><td>0.293</td><td>0.368</td><td>1.363</td><td>0.429</td><td>0.960</td></tr><tr><td>n=1500,d=6</td><td>std</td><td>0.161</td><td>0.050</td><td>0.079</td><td>0.167</td><td>0.079</td><td>0.178</td></tr><tr><td></td><td>mean</td><td>0.144</td><td>0.415</td><td>0.386</td><td>1.530</td><td>0.467</td><td>0.902</td></tr><tr><td>n= 2000,d=8</td><td>std</td><td>0.067</td><td>0.076</td><td>0.077</td><td>0.153</td><td>0.082</td><td>0.206</td></tr><tr><td></td><td>mean std</td><td>0.140 0.055</td><td>0.360 0.084</td><td>0.344 0.010</td><td>1.410 0.163</td><td>0.364 0.065</td><td>0.735 0.175</td></tr></table></body></html>

Table 1: The results of average and standard deviation of $\| \pi _ { \hat { B } } - \pi _ { B _ { 0 } } \| _ { F }$ on 100 replicates across different methods. NN represents the neural network-based method.

We considered the following four scenarios, with two of them attaining small $d = 1 , 2$ and the rest of $d \geq 3$ .

Setting 1: $\bar { y } = x _ { 1 } ^ { 4 } + \epsilon$ where $x \in \mathrm { N o r m a l } ( 0 , I _ { p } )$ and $\epsilon \in { \mathfrak { t } } _ { 5 }$

Setting 2: $\begin{array} { r c l } { y } & { \bar { = } } & { \log ( x _ { 1 } + x _ { 1 } x _ { 2 } ) + \epsilon } \end{array}$ where $\mathrm { ~ { ~ \bf ~ { ~ x ~ } ~ } ~ } \in \mathrm { \Gamma }$ Uniform $( [ 0 , 1 ] ^ { 1 0 } )$ and $\epsilon \in \mathrm { N o r m a l } ( 0 , \sigma ^ { 2 } )$ .

Setting 3: $\begin{array} { r } { \dot { y = ( 1 + \beta _ { 1 } ^ { \top } x ) ^ { 2 } \exp ( \beta _ { 2 } ^ { \top } x ) + 5 { \cdot } 1 ( \beta _ { 3 } ^ { \top } x > 0 ) + \epsilon } } \end{array}$ where $x \in \mathrm { U n i f o r m } ( [ - 1 , 1 ] ^ { 6 } ) , \epsilon \sim \chi _ { 2 } ^ { 2 } - 2$ and

$$
( \beta _ { 1 } , \beta _ { 2 } , \beta _ { 3 } ) = \left( { \begin{array} { c c c c c c } { - 2 } & { - 1 } & { 0 } & { 1 } & { 2 } & { 3 } \\ { 1 } & { 1 } & { 1 } & { 1 } & { 1 } & { 1 } \\ { 1 } & { - 1 } & { 1 } & { - 1 } & { 1 } & { - 1 } \end{array} } \right) ^ { \top } .
$$

Setting 4:

$$
y = \sum _ { k = 1 } ^ { d } \frac { e ^ { x _ { i } } } { 2 + \sin ( \pi x _ { i } ) } + \epsilon ,
$$

where $\epsilon \sim \mathrm { N o r m a l } ( 0 , 0 . 1 ^ { 2 } )$ ,

$$
x \sim \mathrm { N o r m a l } \left( 1 _ { 1 0 } , I _ { 1 0 } - 1 _ { 1 0 } 1 _ { 1 0 } ^ { \top } / 2 0 \right) .
$$

In setting 1, we tested the combinations of $\begin{array} { l l } { ( n , p ) } & { = } \end{array}$ (100, 10), (200, 30) and (300, 30), where $n$ was the sample size. In setting 2, we fixed $n = 2 0 0 , p = 1 0$ and set $\sigma = 0 . 1 , 0 . 2 , 0 . 5$ . In setting 3, we fixed $p = 6$ and tested $n = 2 0 0 , 5 0 0 , 1 0 0 0$ . In setting 4, we fixed $p = 1 0$ and tested $( n , d ) \ : = \ : ( 1 0 0 0 , 4 )$ , (1500, 6) and $( 2 0 0 0 , 8 )$ . Settings 1, 2, 4 were equipped with continuous regression functions, and setting 3 involved discontinuity. We set the number of iterations of our method to 1000 for settings 1 and 2, and increased the iterations to 2000 and 4000 for settings 3 and 4, due to their high complexity.

To evaluate the performance of each method, we calculated the distance metric $\big \| \pi _ { \hat { B } } - \pi _ { B _ { 0 } } \big \| _ { F }$ where $\hat { B }$ represented the estimate of $B _ { 0 }$ . Particularly, for our neural networkbased method, $\hat { B } \ = \ \mathcal { T } ( \hat { f } _ { n } )$ . Smaller value of this metric indicated better performance. We ran 100 replicates for each setting. The results are displayed in Table 1 and Figure 2.

In simple cases (settings 1 and 2), the neural networkbased method showed similar performance to classical methods, with sliced average variance estimation being the most effective method in setting 2. However, complex settings demonstrated the significant superiority of neural network-based method compared to other methods. Specifically, in setting 3, as the sample size $n$ increased, the metric $\big \| \pi _ { \hat { B } } - \pi _ { B _ { 0 } } \big \| _ { F }$ decreased rapidly, indicating the convergence of $\Pi _ { \hat { B } }$ to $\Pi _ { B _ { 0 } }$ . According to the results in setting 4, the neural network-based method was capable of handling higherdimensional scenarios. In summary, numerical studies advocated neural networks as a powerful tool for SDR.

# Real Data Analysis

We further applied the neural network-based method involving rank regularization to a real regression task. In practice,

三 丰   
1.0 1.0 1.0   
0.5 重中 0.5 丰 1 1 0.5 王 丰 中 王 重 + 中中 + 宝 Setting 1, n = 100, p = 20 0.0 Setting 1, n = 200, p = 30 0.0 Setting 1, n = 300, $p = 3 0$ 手 南 王   
1.5 中 1.5 + 雨 1.5 南雨車 电   
1.0 + 南 1.0 牛 1.0 王 王 王 +   
0.5 中 生 T 0.5 王 T重 0.5   
0.0 Setting 2, $\overline { { \sigma = 0 . 1 } }$ 0.0 Setting 2, $\sigma = 0 . 2$ 0.0 Setting 2, $\overline { { \sigma = 0 . 5 } }$   
2.0 中雨更电 2.0 重雨中电 2.0 工 王   
1.5 1.5 王 1.5 +   
1.0 1.0 + 1.0   
0.5 0.5 + 0.5 主   
0.0 T Setting 3, n = 200 0.0 中 Setting 3, $n = 5 0 0$ 0.0 美 Setting 3, $\overline { { \mathsf { n } = 1 0 0 0 } }$ + 幸 王   
1.5 王 1.5 H 三 1.5 中 车 中 中 山 1.0   
1.0 1.0 丰 丰 丰   
0.5 王 重 0.5 工圭 1 0.5 丰王 圭圭 全 中 地 电电 壹   
0.0 0.0 0.0 NN MAVE GKDR SIR SAVE PHD NN MAVE GKDR SIR SAVE PHD NN MAVE GKDR SIR SAVE PHD Setting 4, $\mathsf { n } = 1 0 0 0$ , ${ \mathsf { d } } = 4$ Setting 4, $\mathsf { n } = 1 5 0 0$ , ${ \mathsf { d } } = 6$ Setting 4, $n = 2 0 0 0$ , ${ \mathsf { d } } = 8$

the precise intrinsic dimension $d$ is unknown, and it is uncertain whether a low-dimensional structure exists. To address this issue, we used cross validation to determine an appropriate $d$ from the range of values $\{ 1 , \ldots , p \}$ . More specifically, the raw data set was divided into $80 \%$ training data and $20 \%$ testing data. The optimal value of $d$ was determined through cross validation on the training data. Subsequently, the final model was fit using the selected $d$ on the training data, and the mean squared error on the testing data was evaluated.

In order to reduce randomness, the aforementioned process was repeated 20 times and the resulting testing errors were recorded. Additionally, we conducted a comparative analysis between the neural network-based approach and alternative methods including vanilla neural network without rank regularization, SIR-based regression, SAVE-based regression, and MAVE-based regression. For the latter three techniques, we initially executed SDR to acquire the embedded data, followed by the utilization of a fully-connected neural network for predictive purpose. The optimal value for $d$ was also determined through cross validation.

We utilized a data set of weather records from Seoul, South Korea during the summer months from 2013 to 2017 (Cho et al. 2020), available at the UCI data set repository (bias correction of numerical prediction model temperature forecast). This data set contained 7750 observations with 23 predictors and 2 responses, specifically the next-day maximum air temperature and next-day minimum air temperature. After excluding the variables for station and date, the data set was reduced to 21 predictors, which were further standardized using StandardScaler in scikit-learn package.

Table 2: The results of testing errors on 20 replicates across different methods. We report the average and standard deviation of testing errors, along with averaged optimal $d$ determined through cross validation. NN-RR, NN-VN, MAVE, SIR, SAVE represent the neural network-based method with rank regularization, vanilla neural network regression, MAVE-based regression, SIR-based regression, and SAVEbased regression, respectively.   

<html><body><table><tr><td></td><td>NN-RR</td><td>NN-VN</td><td>MAVE</td><td>SIR</td><td>SAVE</td></tr><tr><td>mean</td><td>0.602</td><td>0.774</td><td>0.743</td><td>1.324</td><td>0.772</td></tr><tr><td>std</td><td>0.043</td><td>0.116</td><td>0.159</td><td>0.190</td><td>0.161</td></tr><tr><td>d</td><td>19.6</td><td></td><td>19.25</td><td>19.6</td><td>20.6</td></tr></table></body></html>

It is evident from Table 2 that the neural network-based method with rank regularization outperformed other methods, demonstrating the effectiveness of the modification compared to the vanilla neural network, and the sound performance in reducing dimensions compared to other SDR methods. The presence of latent structures was partially supported by the averaged optimal $d$ . It was possible that 19 or 20 combinations of raw predictors might be sufficient, as opposed to the original 21 predictors.

# Proofs

# Proof of Theorem 1

Define the vanilla neural network function class without rank regularization as

$$
\begin{array} { r l } & { \mathcal { F } _ { \mathcal { L } , \mathcal { M } , \mathcal { S } , \mathcal { R } } ^ { * } = \Big \{ } \\ & { f ( x ) = \phi _ { L } \circ \sigma _ { L - 1 } \circ \phi _ { L - 1 } \circ \cdot \cdot \cdot \circ \sigma _ { 1 } \circ \phi _ { 1 } ( x ) : } \\ & { \phi _ { i } ( z ) = W _ { i } ^ { \top } z + b _ { i } , W _ { i } \in \mathbb { R } ^ { d _ { i } \times d _ { i + 1 } } , i = 1 , \cdot \cdot . . , L , } \end{array}
$$

$$
\begin{array} { r l } & { \operatorname { w i t h } L = \mathcal { L } , \displaystyle \operatorname* { m a x } _ { i = 1 , \ldots , L + 1 } d _ { i } = \mathcal { M } , \displaystyle \sum _ { i = 1 } ^ { L } ( d _ { i } + 1 ) d _ { i + 1 } = \mathcal { S } , } \\ & { \| f \| _ { L ^ { \infty } ( [ 0 , 1 ] ^ { d } ) } \leq \mathcal { R } \Big \} . } \end{array}
$$

Lemma 1. Suppose that $h$ is H¨older continuous with $\alpha \in$ $( 0 , 1 ]$ and $\lambda > 0$ . Then, for any $\zeta > 0$ , there exists a function $g$ in neural network function class $\mathcal { F } _ { \mathcal { L } , \mathcal { M } , S , \mathcal { R } } ^ { * }$ , with rectified linear unit activation and $\mathcal { L } , \mathcal { M } , \mathcal { S } , \mathcal { R }$ large enough, such that

$$
\| h - g \| _ { L ^ { \infty } ( [ 0 , 1 ] ^ { d } ) } < \zeta .
$$

We note that Lemma 1 is a simplified version of Theorem 1.1 in (Shen, Yang, and Zhang 2020).

Proof of Theorem $\jmath$ . For $f \in \mathcal { F } _ { \mathcal { L } , \mathcal { M } , \mathcal { S } , \mathcal { R } }$ such that $\pi _ { T ( f ) } \neq$ $\pi _ { B _ { 0 } }$ , under Assumption 2, there exists a scalar $t > 0$ satisfying

$$
\mathbb { E } \left\{ \mathrm { V a r } \left[ f _ { 0 } ( B _ { 0 } ^ { \top } x ) | { \mathcal T } ( f ) ^ { \top } x \right] \right\} > t .
$$

Then, we have

$$
\begin{array} { r l } { \mathbb { E } [ y - f ( x ) ] ^ { 2 } = \mathbb { E } [ \epsilon + f _ { 0 } ( B _ { 0 } ^ { \top } x ) - f ( x ) ] ^ { 2 } } \\ & { = \mathbb { E } ( \epsilon ^ { 2 } ) + \mathbb { E } [ f _ { 0 } ( B _ { 0 } ^ { \top } x ) - f ( x ) ] ^ { 2 } } \\ & { = \nu ^ { 2 } + \mathrm { V a r } [ f _ { 0 } ( B _ { 0 } ^ { \top } x ) - f ( x ) ] } \\ & { \phantom { = } + \mathbb { E } ^ { 2 } [ f _ { 0 } ( B _ { 0 } ^ { \top } x ) - f ( x ) ] } \\ & { \geq \nu ^ { 2 } + \mathrm { V a r } [ f _ { 0 } ( B _ { 0 } ^ { \top } x ) - f ( x ) ] } \\ & { \geq \nu ^ { 2 } + \mathbb { E } \{ \mathrm { V a r } [ f _ { 0 } ( B _ { 0 } ^ { \top } x ) | \mathcal { T } ( f ) ^ { \top } x ] \} } \\ & { > \nu ^ { 2 } + t . } \end{array}
$$

On the other hand, for  ∈ F , , , such that π ˜ = $\pi _ { B _ { 0 } }$ , there exists an orthogonal matrix $Q \in \mathbb { R } ^ { d \times d }$ such that $B _ { 0 } \stackrel { \cdot } { = } \tau ( \tilde { f } ) Q$ . Hence, $\tilde { f } ( x ) = \tilde { f } \circ \mathcal T ( \tilde { f } ) \circ Q ( B _ { 0 } ^ { \top } x )$ and $\tilde { f } \circ T ( \tilde { f } ) \circ Q$ is still a neural network function. Assumption 1 and Lemma 1 imply that there is a neural network function $g$ satisfying

$$
\| f _ { 0 } ( B _ { 0 } ^ { \top } x ) - g ( B _ { 0 } ^ { \top } x ) \| _ { L ^ { \infty } ( \mathcal { B } ( [ 0 , 1 ] ^ { p } ) ) } < t ^ { 1 / 2 } / 2 ,
$$

for sufficiently large $\mathcal { L } , \mathcal { M } , \mathcal { S } , \mathcal { R }$ . As a result, select $\tilde { f }$ such that $\tilde { f } \circ \mathcal { T } ( \tilde { f } ) \circ Q = g$ , and it follows that

$$
\begin{array} { r } { \mathbb { E } [ y - \tilde { f } ( x ) ] ^ { 2 } = \nu ^ { 2 } + \mathbb { E } [ f _ { 0 } ( B _ { 0 } ^ { \top } x ) - g ( B _ { 0 } ^ { \top } x ) ] ^ { 2 } < \nu ^ { 2 } + t / 4 . } \end{array}
$$

Therefore, for any $f \in \mathcal { F } _ { \mathcal { L } , \mathcal { M } , \mathcal { S } , \mathcal { R } }$ such that $\pi _ { \mathcal { T } ( f ) } \neq \pi _ { B _ { 0 } }$ , there exists a neural network function $\tilde { f }$ , with the same rank regularization, satisfying

$$
\begin{array} { r } { \mathbb { E } [ y - \tilde { f } ( x ) ] ^ { 2 } < \mathbb { E } [ y - f ( x ) ] ^ { 2 } . } \end{array}
$$

To conclude, $\pi _ { \mathcal { T } ( f ^ { * } ) } = \pi _ { B _ { 0 } }$ , which entails that $\Pi _ { \mathcal { T } ( f ^ { * } ) } =$ ΠB0.

# Proof of Theorem 2

Let $L ( f ) = \mathbb { E } [ y - f ( x ) ] ^ { 2 }$ . Without loss of generality, suppose $\mathcal { R } = \mathcal { R } _ { 0 }$ . We first present some useful lemmas.

Lemma 2. For sufficiently large $n$ , it follows that

$$
\operatorname* { s u p } _ { f \in { \mathcal { F } } _ { \angle , M , S , { \mathcal { R } } } } | L ( f ) - L _ { n } ( f ) | = O _ { p } \left( { \sqrt { \frac { S { \mathcal { L } } \log ( S ) ( \log n ) ^ { 5 } } { n } } } \right) .
$$

The proof of Lemma 2 is presented in the Supplementary Material. We note this convergence rate may not be optimal, but is sufficient for deducing consistency.

Lemma 3. Under Assumptions 1 and 3, for sufficiently large $n$ , we have

$$
\begin{array} { r } { \mathbb { E } [ y - \hat { f } _ { n } ( x ) ] ^ { 2 } \leq \nu ^ { 2 } + C n ^ { - 2 \alpha / ( d + 4 \alpha ) } ( \log n ) ^ { 3 } , } \end{array}
$$

where $C$ is a constant depending on $\mathcal { R } _ { 0 }$ and $d$ .

Proof of Lemma 3. We begin with the decomposition that

$$
\begin{array} { r l } & { | L ( j )  \leqslant \mathbb { E } \Bigg [ | a _ { \mathrm { p } }  \Bigg | \frac { \partial } { \partial a _ { \mathrm { p } } } \Bigg | \int _ { \mathcal { C } } \frac { \mathrm { d } \hat { \mathcal { G } } } { \partial a _ { \mathrm { p } , \mathcal { A } } ( j ) } \Bigg | \frac { \partial \hat { \mathcal { G } } } { \partial a _ { \mathrm { p } } } \Bigg | \mathrm { d } \hat { \mathcal { G } } \Bigg \rangle - L _ { \mathrm { a p } } ( \hat { \mathcal { G } } ) \Bigg | \hat { \mathcal { G } } \Bigg \rangle } \\ & { \qquad \le \mathbb { E } \Bigg [ \Bigg | \underset { \mathcal { C } } { \underbrace { \mathrm { e f f . e f . } \mathrm { i n f } } } \frac { \mathrm { d } } { \partial a _ { \mathrm { p } } } \Bigg | \int _ { \mathcal { C } } \hat { \mathcal { T } } ( j ) \cdot \mathcal { D } _ { \mathrm { a p } } ^ { \dagger } \Bigg | } \\ & { \qquad + \underset { \mathcal { C } } { \underbrace { \mathrm { s u p . } \mathrm { s u p . } } } \Bigg | | \hat { \mathcal { G } } ( j ) - \mathcal { D } _ { \mathrm { a l } } ( \hat { \mathcal { G } } )  \Bigg | \Bigg ] } \\ & { \qquad \le \mathbb { E } \Bigg [ \Bigg | \underset { \mathcal { C } } { \underbrace { \mathrm { e f f . e f . } \mathrm { i n f } } } \frac { \mathrm { d } } { \partial a _ { \mathrm { p } } } \Bigg | | ( j ) - \mathcal { D } _ { \mathrm { a l } } ( \hat { \mathcal { G } } )  \Bigg | \Bigg ] } \\ & { \qquad + \underset { \int _ { \mathcal { C } } \le \mathbb { E } \times \mathcal { B } , \mathrm { a d d } } { \underbrace { 1 } \int _ { \mathcal { C } } \mathrm { s u p . } } | | | \hat { \mathcal { G } } ( j ) - \mathcal { D } _ { \mathrm { a l } } ( \hat { \mathcal { G } } )  | } \\ & { \qquad \le \int _ { \mathcal { C } } \frac { \mathrm { d } \hat { \mathcal { G } } } { \varepsilon _ { \mathrm { A d } } } \frac { \partial } { \partial a _ { \mathrm { p } } } | | \hat { \mathcal { G } } ( j ) - \mathcal { D } _ { \mathrm { a d } } ( \hat { \mathcal { G } } )  | } \\ &  \qquad + \sum _ { \mathcal { C } } \frac { \mathrm { d } \hat { \mathcal { G } } } { \varepsilon _ { \mathrm { A d } } } \frac { \mathrm { d } } { \varepsilon _ { \mathrm { A d } } } \frac { \mathrm { d } } { \varepsilon _ { \mathrm { A d } } } \frac { \mathrm { d } } { \varepsilon _ { \mathrm { B } } } \Bigg | \mathrm { d } \hat { \mathcal { G } } ( j ) - \mathcal { D } _  \mathrm  a \end{array}
$$

where $C _ { 1 }$ is a constant, for sufficiently large $n$ . Hence,

$$
\begin{array} { r l } & { \quad L ( \hat { f _ { n } } ) - \nu ^ { 2 } } \\ & { = L ( \hat { f _ { n } } ) - L ( \hat { f _ { 0 } } \circ B _ { 0 } ^ { \top } ) } \\ & { = L ( \hat { f _ { n } } ) - \frac { \mathrm { i n f } } { \rho } } \\ & { \quad + L ( \hat { f _ { n } } ) - \frac { \mathrm { i n f } } { \rho } } \\ & { \quad \quad + \frac { \mathrm { i n f } } { f _ { 0 } \kappa \rho _ { \mathrm { A } , \lambda \sigma , \kappa } ^ { - 1 } } L ( f \circ \mathcal { T } ( f ) \circ B _ { 0 } ^ { \top } ) } \\ & { \quad \quad + \frac { \mathrm { i n f } } { f _ { 0 } \kappa \rho _ { \mathrm { A } , \lambda \sigma , \kappa } ^ { - 1 } } L ( f \circ \mathcal { T } ( f ) \circ B _ { 0 } ^ { \top } ) - L ( f _ { 0 } \circ B _ { 0 } ^ { \top } ) } \\ & { \quad \le C _ { 1 } \sqrt { \kappa \xi \log ( \delta ) ( \log n ) ^ { 5 } n ^ { - 1 } } } \\ & { \quad \quad + \frac { \mathrm { i n f } } { f _ { 0 } \kappa \rho _ { \mathrm { A } , \lambda \sigma , \kappa } ^ { - 1 } } \mathbb { R } [ f _ { 0 } ( B _ { 0 } ^ { \top } x ) - f \circ \mathcal { T } ( f ) ( B _ { 0 } ^ { \top } x ) ] ^ { 2 } } \\ & { \quad \le C _ { 1 } \sqrt { \kappa \xi \log ( \delta ) ( \log n ) ^ { 5 } n ^ { - 1 } } } \\ &  \quad \quad + \frac { \mathrm { i n f } } { \rho \sigma \kappa \rho _ { \mathrm { A } , \lambda \sigma , \kappa , \eta } ^ { - 1 } } \| f _ { 0 } - g \| _ { L ^ { \infty } ( \{ 0 , 1 \} ^ { 1 \} ) } ^ { 2 } . } \end{array}
$$

By Theorem 1.1 in Shen, Yang, and Zhang (2020), there exists $g ^ { \ast }$ with $\mathcal { M } \ = \ 3 ^ { d + 3 } \operatorname* { m a x } ( d \lfloor K ^ { 1 / d } \rfloor , K + 1 ) , \mathcal { L } \ =$ $1 2 D + 1 4 + 2 d$ for some constant $K , D > 0$ such that

$$
\| f _ { 0 } - g ^ { * } \| _ { L ^ { \infty } ( [ 0 , 1 ] ^ { d } ) } \leq 1 9 \sqrt { d } \lambda ( K D ) ^ { - 2 \alpha / d } .
$$

Let

$$
D = O \left( n ^ { d / ( 2 d + 8 \alpha ) } \right) , \quad K = O ( 1 ) ,
$$

and observe that $S = O ( \mathcal { M } ^ { 2 } \mathcal { L } )$ . Then, it follows that

$$
\begin{array} { r l } & { L ( \hat { f } _ { n } ) = \mathbb { E } [ y - \hat { f } _ { n } ( x ) ] ^ { 2 } } \\ & { \qquad \leq \nu ^ { 2 } + C n ^ { - 2 \alpha / ( d + 4 \alpha ) } ( \log { n } ) ^ { 3 } , } \end{array}
$$

where $C$ is a constant depending on $\mathcal { R } _ { 0 }$ and $d$ .

With Lemma 3, the proof of Theorem 2 is a direct application of Theorem 5.9 in Van der Vaart (2000).

Proof of Theorem 2. Recall that $m _ { 0 } = f _ { 0 } \circ B _ { 0 } ^ { \top }$ . Let $R ( f ) =$ $L ( \boldsymbol { f } ) - L ( \boldsymbol { m } _ { 0 } ) = \mathbb { E } [ f _ { 0 } ( B _ { 0 } ^ { \top } \boldsymbol { x } ) - f ( \boldsymbol { x } ) ] ^ { 2 } \geq 0$ and $R _ { n } ( f ) =$ $L _ { n } ( f ) - L _ { n } ( m _ { 0 } )$ . Then, Lemma 2 indicates that

$$
\operatorname* { s u p } _ { f \in { \mathcal { F } } _ { \mathcal { L } , \mathcal { M } , \mathcal { S } , \mathcal { R } } } | R _ { n } ( f ) - R ( f ) |  0 , \quad \mathrm { i n ~ p r o b a b i l i t y } ,
$$

yielding $R _ { n } ( { \hat { f } } _ { n } ) = o _ { p } ( 1 )$ by incorporating with Lemma 3. We denote the metric

$$
d _ { 1 } ( f , m _ { 0 } ) = \operatorname* { m i n } _ { Q \in \mathcal { Q } } \| B _ { 0 } - \mathcal { T } ( f ) Q \| _ { 2 } \vee \| f _ { 0 } \circ Q - f \circ \mathcal { T } ( f ) \| _ { L ^ { 2 } ( \mu ) } .
$$

Here, $f \in \mathcal { F } _ { \mathcal { L } , \mathcal { M } , \mathcal { S } , \mathcal { R } } , m _ { 0 } = f _ { 0 } \circ B _ { 0 } ^ { \intercal }$ , $a \lor b$ means $\operatorname* { m a x } ( a , b )$ , and $\mu$ is the probability distribution of $\tau ( f ) ^ { \intercal } x$ . Recall that $\begin{array} { r } { d ( B , B _ { 0 } ) = \operatorname* { m i n } _ { Q \in \mathcal { Q } } { \| B _ { 0 } - B Q \| _ { 2 } } } \end{array}$ , where $B \in \Psi _ { d }$ .

For $f$ and $\delta > 0$ such that $d _ { 1 } ( f , m _ { 0 } ) \geq \delta$ , if

$$
d ( \mathcal { T } ( f ) , B _ { 0 } ) \leq \tilde { \delta } = \operatorname* { m i n } \{ [ \delta ^ { 2 } / ( 8 \mathcal { R } _ { 0 } \lambda ) ] ^ { 1 / \alpha } , \delta / 2 \} ,
$$

we have

$$
\begin{array} { r l } & { | f _ { 0 } ( B _ { 0 } ^ { \top } \boldsymbol { x } ) - f _ { 0 } ( Q ^ { \top } \mathcal { T } ( f ) ^ { \top } \boldsymbol { x } ) | \leq \lambda \| B _ { 0 } ^ { \top } \boldsymbol { x } - Q ^ { \top } \mathcal { T } ( f ) ^ { \top } \boldsymbol { x } \| _ { 2 } ^ { \alpha } } \\ & { \qquad \leq \lambda \| B _ { 0 } ^ { \top } - Q ^ { \top } \mathcal { T } ( f ) ^ { \top } \| _ { 2 } ^ { \alpha } } \\ & { \qquad \leq \frac { \delta ^ { 2 } } { 8 \mathcal { R } _ { 0 } } , } \end{array}
$$

for some orthogonal matrix $Q$ . Hence,

$$
\begin{array} { r l } & { R ( f ) = \mathbb { E } [ f _ { 0 } ( B _ { 0 } ^ { \top } x ) - f ( x ) ] ^ { 2 } } \\ & { \qquad \geq \mathbb { E } [ f _ { 0 } ( Q ^ { \top } \mathcal { T } ( f ) ^ { \top } x ) - f ( x ) ] ^ { 2 } } \\ & { \qquad + \ 2 \mathbb { E } [ f _ { 0 } ( B _ { 0 } ^ { \top } x ) - F _ { 0 } ] [ F _ { 0 } - f ( x ) ] } \\ & { \qquad \geq - \displaystyle \frac { \delta ^ { 2 } } { 2 } + d _ { 1 } ( f , m _ { 0 } ) ^ { 2 } \geq \displaystyle \frac { \delta ^ { 2 } } { 2 } > 0 , } \end{array}
$$

where $F _ { 0 } = f _ { 0 } ( Q ^ { \top } \mathcal { T } ( f ) ^ { \top } x )$ .

For the case that $d ( T ( f ) , B _ { 0 } ) > \tilde { \delta }$ , by Assumption 4, we have $R ( f ) \geq \xi > 0$ for some positive constant $\xi$ . To conclude, we obtain

$$
\operatorname* { i n f } _ { f : d _ { 1 } ( f , m _ { 0 } ) \geq \delta } R ( f ) > 0 ,
$$

for any $\delta > 0$ . Finally, applying Theorem 5.9 in Van der Vaart (2000), $R _ { n } ( { \hat { f } } _ { n } ) { \stackrel { \cdot } { = } } o _ { p } ( 1 )$ implies that $d _ { 1 } ( \hat { f } _ { n } , m _ { 0 } )  0$ in probability. Hence, $d ( \mathcal { T } ( \hat { f } _ { n } ) , B _ { 0 } ) \to 0$ in probability.

# Discussion

We have demonstrated that neural networks attain the capability of detecting the underlying low-dimensional structure that preserves all information in $x$ about the conditional mean function $\mathbb { E } ( y | x )$ . As a result, neural networks are suitable to be utilized for the estimation of central mean subspace $\Pi _ { B _ { 0 } }$ . The theoretical investigations sharpen our understanding of neural networks, while broadening the scope of SDR as well.

In the context of SDR, a more general scenario than sufficient mean dimension reduction emerges when considering

$$
y \perp \perp x | B _ { 0 } ^ { \top } x .
$$

And the column space spanned by $B _ { 0 }$ corresponds to the central subspace (Cook 1998a; Li 2018). It is clear that relation (4) is equivalent to that $p ( y | x ) = p ( y | B _ { 0 } ^ { \top } x )$ , where $p ( \cdot | \cdot )$ represents the conditional probability density function. Following the work of Xia (2007), we can further adapt neural networks to estimate the central subspace by modifying the loss function.

Under mild conditions, Xia (2007) showed that

$$
\mathbb { E } [ K _ { h } ( y - y _ { 0 } ) | x = x _ { 0 } ]  p ( y _ { 0 } | B _ { 0 } ^ { \top } x _ { 0 } ) , \quad \mathrm { a s ~ } h  0 ^ { + } .
$$

Here, $( x _ { 0 } , y _ { 0 } )$ is a fixed point, and $K _ { h } ( \cdot )$ is a suitable kernel function with a bandwidth $h$ . Such finding then implies that

$$
\begin{array} { r l r } {  { K _ { h } ( y - y _ { 0 } ) = \mathbb { E } [ K _ { h } ( y - y _ { 0 } ) | x ] + \mathrm { r e m a i n d e r ~ t e r m } } } \\ & { } & { \approx p ( y _ { 0 } | B _ { 0 } ^ { \top } x ) + \mathrm { r e m a i n d e r ~ t e r m } . } \end{array}
$$

Based on this discovery, it is natural to employ a neural network function $f ( B ^ { \top } \dot { x } , y _ { 0 } ) ~ : ~ \mathbb { R } ^ { d + 1 } \  ~ \dot { \mathbb { R } }$ to approximate $p ( y _ { 0 } | B _ { 0 } ^ { \top } x )$ , and obtain the estimate of $B _ { 0 }$ at the population level by solving the following problem

$$
( B ^ { * } , f ^ { * } ) = \underset { B , f } { \operatorname { a r g m i n } } \mathbb { E } [ K _ { h } ( y - \tilde { y } ) - f ( B ^ { \top } x , \tilde { y } ) ] ^ { 2 } ,
$$

where $\tilde { y }$ is an independent copy of $y$ . Empirically, we define the loss function as

$$
\tilde { L } _ { n } ( B , f ) = \frac { 1 } { n ^ { 2 } } \sum _ { i = 1 } ^ { n } \sum _ { j = 1 } ^ { n } [ K _ { h } ( y _ { j } - y _ { i } ) - f ( B ^ { \top } x _ { j } , y _ { i } ) ] ^ { 2 } .
$$

We provide some additional simulation results to verify the feasibility utilizing neural networks for the estimation of central subspace; see the Supplementary Material. Theoretical analysis of neural networks for the estimation of the central subspace, including unbiasedness and consistency, deserves further studies.