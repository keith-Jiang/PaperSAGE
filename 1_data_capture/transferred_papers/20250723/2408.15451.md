# Certified Causal Defense with Generalizable Robustness

Yiran Qiao1, Yu $\mathbf { Y _ { i n } } ^ { 1 }$ , Chen Chen2, Jing Ma1\*

1Case Wester Reserve University 2University of Virginia yxq350@case.edu, yxy1421@case.edu, zrh6du $@$ virginia.edu, jing.ma5@case.edu

# Abstract

While machine learning models have proven effective across various scenarios, it is widely acknowledged that many models are vulnerable to adversarial attacks. Recently, numerous efforts have emerged in adversarial defense. Among them, certified defense is well known for its theoretical guarantees against arbitrary adversarial perturbations on input within a certain range. However, most existing works in this line struggle to generalize their certified robustness in other data domains with distribution shifts. This issue is rooted in the difficulty of eliminating the negative impact of spurious correlations on robustness in different domains. To address this problem, in this work, we propose a novel certified defense framework GLEAN, which incorporates a causal perspective into the generalization problem in certified defense. More specifically, our framework integrates a certifiable causal factor learning component to disentangle the causal relations and spurious correlations between input and label, thereby excluding the negative effect of spurious correlations on defense. On top of that, we design a causally certified defense strategy to handle adversarial attacks on latent causal factors. In this way, our framework is not only robust against malicious noises on data in the training distribution but also can generalize its robustness across domains with distribution shifts. Extensive experiments on benchmark datasets validate the superiority of our framework in certified robustness generalization in different data domains.

Extended version — https://arxiv.org/pdf/2408.15451

ML models in critical applications where security and reliability are priorities, such as autonomous driving and healthcare.

In the past few decades, researchers have developed numerous defense methods to enhance the adversarial robustness of ML models. Many of these methods are based on adversarial training (Goodfellow, Shlens, and Szegedy 2014; Madry et al. 2017; Zhang et al. 2019; Athalye, Carlini, and Wagner 2018), which incorporates adversarial samples into model training. Despite its impressive performance, adversarial training is an empirical approach that lacks theoretical guarantees. That is, although it can enhance robustness against certain types of attacks, it may still be vulnerable to other unknown, or more potent adversarial perturbations. Differently, another line of work develops certified robustness. A certified robust classifier can theoretically guarantee that its prediction for a point $x$ remains constant within a certain specified range (a.k.a. radius) of perturbations on $x$ , regardless of the type of attack. Randomized smoothing-based certified defense (Lecuyer et al. 2019; Li et al. 2018; Cohen, Rosenfeld, and Kolter 2019) is one of the most representative methods in this area. Specifically, given an arbitrary base classifier $f$ this method can convert it to a certifiably robust classifier $g$ which is created by randomly sampling multiple noised versions of a given input and using the aggregated output from these variations to make final predictions. Inspired by this approach, many subsequent studies (Li et al. 2019; Jeong and Shin 2020; Jeong et al. 2021; Salman et al. 2019; Zhai et al. 2020) have expanded upon the basis of random smoothing.

# Introduction

Machine learning (ML) models, particularly deep neural networks (DNN), have demonstrated remarkable success across a diverse range of areas (Devlin et al. 2018; Silver et al. 2017; He et al. 2016). Despite their success, these models still exhibit significant vulnerabilities to adversarial perturbations on input (Szegedy et al. 2013; Goodfellow, Shlens, and Szegedy 2014; Biggio et al. 2013). A typical example in image classification is that a trained classifier that correctly classifies an image $x$ can be easily fooled by a perturbed image $x { + } \delta$ , where $\delta$ represents adversarial perturbations that are imperceptible to human perception. This weakness impedes the deployment of

Currently, most existing certified defense works focus on data in the same domain yet overlook other domains with distribution shifts. This limitation can result in a markedly degraded certified robustness performance when these methods are applied to the test domain (Sun et al. 2021). As discussed in previous work (Ilyas et al. 2019; Beery, Van Horn, and Perona 2018), such degradation of robustness lies in the fact that ML models tend to overfit spurious correlations between features and labels. As these spurious correlations often vary across different domains (Ye et al. 2022), fitting spurious correlations can easily lead $g$ to make incorrect predictions or correct predictions but with lower confidence levels. The former results in the certified radius being assigned zero, while the latter also leads to a reduced certified radius. Therefore, domain shifts can lead to weak generalization w.r.t. not only prediction performance but also certified robustness.

Different from ML models, humans can naturally capture the invariant relations between labels and their corresponding causal factors, various studies (Zhang, Zhang, and Li 2020; Tang, Huang, and Zhang 2020; Scho¨lkopf et al. 2021) argue that human’s inherent causal view brings a solution to avoid the domain generalization hurdle for robustness. Inspired by this, in this paper, we study the problem of generalizing the certified robustness under domain shifts from a causal view.

However, addressing this problem presents multifaceted challenges. Challenge 1: As aforementioned, spurious correlations varying across domains adversely affect robustness. To achieve robustness across domains, it is crucial to effectively remove the impact of these spurious correlations. However, identifying and eliminating the impact of spurious correlations on robustness in different domains presents a challenge. Challenge 2: Apart from spurious factors, the distribution shifts often bring challenges for the model to defend against the perturbations on the factors that causally determine the label in unseen domains, which leads to diminished certified robustness. Challenge 3: It is important to provide theoretical guarantees for robustness on other data domains, but most existing works remain in empirical observations and lack theoretical analysis. Although some works (Salman et al. 2019; Zhai et al. 2020) in certified defenses has provided upper bounds on perturbations while maintaining robustness, they were not designed to address certified robustness in the domain shift context.

In this work, to tackle these challenges, we propose a novel framework GeneraLizable cErtified cAusal defeNse (GLEAN) that enhances the certified robustness of models on data in different domains. To mitigate the influence of spurious correlations on robustness generalization (Challenge 1), we construct a causal model for data in different domains, and simultaneously conduct a causal analysis on model robustness and generalization. Based on the causal model, we filter out the impact from spurious correlations and enhance robustness across domains. This is different from most existing defense algorithms which take the same strategy indiscriminately towards all the input features. To achieve certified robustness through causal factors (Challenge 2), we utilize a certified causal factor learning module with Lipschitz constraint. This module enables certification through the latent representation space for high-level causal factors, conducting certified defense for perturbations on causal factors that determine the label in different domains. To bring theoretical guarantees for robustness on different data domains (Challenge 3), we derive a theoretical analysis by leveraging the theoretical support of certified defense and causal inference. Our main contributions can be summarized as:

• We investigate an important but underexplored problem of certified defense on data in different domains. We analyze the significance of this research problem and its corresponding challenges. • We propose a novel causality-inspired framework GLEAN for this problem, extending certified robustness across data domains. Specifically, we develop a certified defense strategy based on certifiable causal factor learning, which excludes spurious correlations and provides a certified radius for test data with a theoretical guarantee.

• We conduct extensive experiments to evaluate our framework on both synthetic and real-world datasets. The results show that our framework significantly outperforms the prevalent baseline methods.

# Preliminaries and Related Work

We consider a classification task with $x$ representing an input instance and $y$ denoting the corresponding label, where $x \in \mathcal { X } , y \in \mathcal { Y } : = \{ 1 , . . . , K \}$ , $\chi$ and $y$ represent the input space and the label space, respectively. A classifier trained for this task can be denoted by $f : \mathcal { X }  \mathcal { Y }$ . The data may be collected from different domains (i.e., environments). We use the superscript $( \cdot ) ^ { d }$ to denote the data in a certain domain $d$ .

# Certified Defense

Robust Radius The robust radius for an instance $x$ is the largest range (e.g., a $l _ { 2 }$ ball) centered at $x$ , within which $f$ provides a correct prediction for $x$ , and this prediction remains constant. It is defined as follows:

$$
\begin{array} { r } { R ( f ; x , y ) = \operatorname* { m i n } _ { f ( x ^ { \prime } ) \neq f ( x ) } \| x ^ { \prime } - x \| _ { 2 } . } \end{array}
$$

Unfortunately, calculating the robust radius for neural networks is proven to be an NP-complete problem (Katz et al. 2017; Sinha et al. 2017) thus both challenging and time-consuming.

Certified Radius Many previous works proposed certification methods to derive a certified radius that is the lower bound of the robust radius. Research in this area falls into two categories: exact methods and conservative methods. Exact methods (Ehlers 2017; Bunel et al. 2018; Tjeng, Xiao, and Tedrake 2017), usually based on Satisfiability Modulo Theories or mixed integer linear programming, guarantee the identification of a perturbation $\delta$ within a radius $r$ that can cause $f$ to change its prediction. However, they require the model to have a limited scale. Conservative methods (Wong and Kolter 2018; Wong et al. 2018; Gowal et al. 2018) ensure the detection of existing adversarial examples and, in addition, refuse to make certification for some vulnerable data points. These methods, though more scalable, impose specific assumptions on the model’s architecture.

Randomized Smoothing Randomized Smoothing (RS) is proposed to tackle the above limitations, which can be applied to any architectures (Cohen, Rosenfeld, and Kolter 2019). It constructs a smoothed classifier $g$ from an arbitrary base classifier $f$ . The definition of $g$ is as follows:

$$
\begin{array} { r } { g ( x ) = \arg \operatorname* { m a x } _ { y \in \mathcal { V } } P ( f ( x + \eta ) = y ) } \end{array}
$$

In this formula, $\eta \sim \mathcal { N } ( 0 , \sigma ^ { 2 } I )$ is the isotropic Gaussian noise with the noise level $\sigma$ as the hyperparameter. The smoothed classifier $g$ can be summarized as returning the class most likely to be predicted by $f$ when the input $x$ is sampled from Gaussian distributions. (Cohen, Rosenfeld, and Kolter 2019) provide the theoretical form of certified radius which is the lower bound of the robust radius:

$$
C R ( f ; x , y ) = \frac { \sigma } { 2 } ( \Phi ^ { - 1 } ( p _ { A } ) - \Phi ^ { - 1 } ( p _ { B } ) ) ,
$$

where $p _ { A } = P ( f ( x + \eta ) = y _ { A } )$ , $p _ { B } = \operatorname* { m a x } _ { y \neq y _ { A } } P ( f ( x +$ $\eta ) = y$ ), meaning that $f$ will mostly return class $y _ { A }$ with the probability $p _ { A }$ , and will return the runner-up class with the probability $\bar { p } _ { B } . \Phi ^ { - 1 }$ is the inverse of the standard Gaussian cumulative distribution function. Then $g ( x + \delta ) = y _ { A }$ for all $| | \delta | | _ { 2 } \le C R$ .

![](images/54fe681fd8b0296be94ef96dfc723e69647ec33b5c83df14e83d5765dc17c823.jpg)  
Figure 1: (a) Causal graph of data generation across domains; (b) A showcase of domain shift, here we use images in CMNIST as an example. (c) Two common cases of domain shifts leading to decreased ACR in certification. The pink area represents an incorrect decision area, green signifies the correct decision area, and circles represent a robust $l _ { 2 }$ ball.

# Reduced Certified Robustness in Unseen Domains

There widely exist distribution shifts between data in different domains, i.e., $P ^ { d } ( X , Y ) \neq P ^ { d ^ { \prime } } ( X , Y )$ , where $d \neq d ^ { \prime }$ . Inspired by (Zhang et al. 2013; Pearl, Glymour, and Jewell 2016), we use the causal graph shown in Figure 1 (a) to illustrate the causal model for data across domains. Specifically, as shown in Figure 1, we discuss the causal relations among five variables: label $Y$ (e.g., an object type), input features $X$ (e.g., an image), causal factors $C$ (e.g., the object shape in the image) that determine the label, non-causal factors $S$ (e.g., the background in the image), and domain variable $D$ (e.g., the data source). $C$ and $S$ are usually high-level latent concepts without observed supervision. Noticeably, $S$ often has spurious correlations with $Y$ , even if they are not causally related. Such spurious correlations often vary in different domains, i.e., $\bar { P ^ { d } } ( Y | S ) \neq P ^ { d ^ { \prime } } ( Y | S )$ .

Distribution shift often brings challenges in certified robustness (Sun et al. 2022). Here, we use a simple experiment to show the rapid deterioration of certified robustness on data in different domains, where the task is to classify the Colored-MNIST (CMNIST) dataset (Arjovsky et al. 2019). CMNIST is a modified version of the handwritten digit image dataset MNIST (LeCun et al. 1998), artificially constructed to include two colors, red and green. The colors are strongly but spuriously correlated with the label $Y$ $\boldsymbol { Y } = 0$ for digits $0 \sim 4$ , and $Y = 1$ for digits $5 \sim 9$ ). For this dataset, the color of the digits is a non-causal factor $S$ , while the shape of the digits is the causal factor $C$ . The spurious correlation between $Y$ and $S$ in the training domain is reversed in the test domain, as shown in Figure 1 (b).

Table 1: Comparison of the certified defense performance with/without domain shift. The metrics include the prediction accuracy and the Average Certified Radius (ACR).   

<html><body><table><tr><td>Dataset</td><td>Test Acc</td><td>ACR (σ = 0.25)</td></tr><tr><td>CMNIST</td><td>21.01%</td><td>0.07</td></tr><tr><td>MNIST</td><td>72.03%</td><td>0.37</td></tr></table></body></html>

In the CMNIST dataset, it is unsurprising that a classifier relying on the digit color would fail on the test domain due to the shift in the spurious correlation $P ( \boldsymbol { Y } | \boldsymbol { S } )$ . To show the negative impact of spurious correlation on certified robustness, we compare the Average Certified Radius (ACR, a metric used to evaluate certified robustness) of random smoothing-based certified defense on CMNIST with the results on MNIST (where there is no digit color and thus the above spurious correlations do not exist). As observed from the results in Table 1, there is a significant degradation of prediction accuracy and ACR on the test domain, indicating severe issues for certified defense under domain shift.

# L-Lipschitz Networks

Definition 1 (Lipschitz Continuity). A function $f : X \to Y$ is called Lipschitz continuous if there exists a non-negative constant $L$ (known as the Lipschitz constant) such that for all $x _ { 1 } , x _ { 2 } \in X$ the following condition is met:

$$
| | f ( x _ { 1 } ) - f ( x _ { 2 } ) | | _ { 2 } \leq L | | x _ { 1 } - x _ { 2 } | | _ { 2 } .
$$

Based on the definition, if a neural network $f$ is 1-Lipschitz, then for any input $x$ , the output $y$ satisfies $| | \mathbf { y } | | _ { 2 } \leq | | \mathbf { x } | | _ { 2 }$ . Equivalently, $| | \mathbf { y _ { 1 } } - \mathbf { y _ { 2 } } | | _ { 2 } \leq | | \mathbf { x _ { 1 } } - \mathbf { x _ { 2 } } | | _ { 2 }$ .

# GLEAN: Framework and Theories

In this section, we introduce the detailed technologies and theories in our proposed framework. We begin by proposing a causal view of robustness under domain shifts. Next, we introduce our design of a certifiable causal factor learning module to exclude the impact of spurious correlations on robustness. Then, we explain the whole certified defense process through the latent causal space, providing a theoretical guarantee for the certified robustness of data in different domains.

# Causal View of Robustness and Cross-Domain Generalization

As introduced in the last section, we illustrate our causal graph in Figure 1 (a). Noticeably, although the spurious correlations vary in different domains, since $C  Y$ has a direct causal link, the relationship between $C$ and $Y$ remains invariant across domains and is thus unaffected by domains. This inspired invariant learning based on the following causal invariance assumption (Li et al. 2022):

Assumption 1 (Causal Invariance over Domain Shifts) For any two domains $d$ and $d ^ { \prime }$ , the probability $P ( \boldsymbol { Y } | \boldsymbol { C } )$ is invariant to domain shifts, i.e.,:

$$
P ^ { d } ( \boldsymbol { Y } | \boldsymbol { C } ) = P ^ { d ^ { \prime } } ( \boldsymbol { Y } | \boldsymbol { C } ) , \forall d , d ^ { \prime } \in \mathbb { D } ,
$$

where $\mathbb { D }$ is the set of all possible domains.

Based on this assumption, a model that can identify causal factors and make predictions based on them can be generalized to unseen domains.

From a robustness perspective, distribution shifts introduce significant additional challenges. At a high level, robustness can be viewed as a generalization problem over an adversarial distribution (Xin et al. 2023). This adversarial distribution often differs from the unseen domains derived from natural distributions, necessitating more sophisticated methods to capture high-level causal factors in decision-making while filtering out the impact of adversarial perturbations. More specifically, for a target domain $d ^ { \prime }$ , we have:

$$
P ^ { d ^ { \prime } } ( Y | X ) = \int _ { c \in \mathbb { C } } P ^ { d ^ { \prime } } ( c | X ) P ^ { d ^ { \prime } } ( Y | c ) = \int _ { c \in \mathbb { C } } P ^ { d ^ { \prime } } ( c | X ) P ^ { d } ( Y | c ) ,
$$

$$
P ^ { d ^ { \prime } } ( Y | X ) = \int _ { s \in \mathbb { S } } P ^ { d ^ { \prime } } ( s | X ) P ^ { d ^ { \prime } } ( Y | X , s ) ,
$$

where $\mathbb { C }$ and $\mathbb { S }$ are the space of $C$ and $S$ , respectively. Here, $d \neq d ^ { \prime }$ . Each of the equations above decomposes ${ \dot { P } } ( Y | X )$ into two components. As shown in Eq. 6, the model can generalize to a different (or even adversarial) domain $d ^ { \prime }$ if it accurately captures the causal factors $C$ from $X$ . The other term $P ( \boldsymbol { Y } | \boldsymbol { C } )$ remains invariant across domains, which helps to mitigate the risk of increased vulnerability in new domains. However, as indicated by Eq. (7), $P ^ { d ^ { \prime } } ( Y | X , s )$ varies across domains, which can increase the vulnerability to adversarial perturbations. This increased vulnerability stems from two main issues, as illustrated in Figure 1 (c): 1) reduced accuracy in the test domain, leading to diminished prediction reliability even under slight perturbations; and 2) the change in $P ^ { d ^ { \prime } } ( Y | X , s )$ across domains increases decision uncertainty at each $X = x$ due to potential conflicts between $P ( \boldsymbol { Y } | \boldsymbol { C } )$ and $P ( \boldsymbol { Y } | \boldsymbol { S } )$ . These factors collectively complicate the task of achieving robustness across different domains. The above analysis indicates the importance of incorporating a causal view into the robustness problem across domains. In our framework, we identify the causal factors from input (i.e., modeling $P ( C | X ) )$ with certifiable robustness, and conduct certified defense based on an invariant predictor $P ( \boldsymbol { Y } | \boldsymbol { C } )$ .

# Causal Encoder with Lipschitz Constraint

Inspired by the above analysis and the observations of the aforementioned toy experiment, to achieve robustness in different domains, we develop a method to robustly identify causal factors from the input for downstream prediction. It is worth mentioning that, for many real-world scenarios, identifying causal factors in the input space (e.g., image pixels) is difficult without segmentation labels, and also less meaningful, because causal factors are often high-level concepts. Therefore, our method is built upon a representation space, where we conduct two main tasks: (1) learn the causal factors from the input features with an encoder $\Psi ( \cdot )$ ; (2) provide a certifiable guarantee for robustness in this process.

For the first task, encouraged by recent progress in causal generalization, we extract the causal factors of input features in the latent space through techniques in invariant learning (Krueger et al. 2021; Ahuja et al. 2020; Arjovsky et al. 2019; Mitrovic et al. 2020), which capture invariant factors across different domains. We can adopt one of the cutting-edge methods of this type for our invariant learning module. In this work, we leverage one of the most representative methods: invariant risk minimization (IRM) (Arjovsky et al. 2019) with the following optimization loss:

$$
\mathcal { L } _ { \mathrm { I R M } } = \sum _ { d \in \mathbb { D } _ { \mathrm { t r } } } R ^ { d } ( \beta \circ \Psi ) + \lambda \cdot \| \nabla _ { w | w = 1 . 0 } R ^ { d } ( w ( \beta \circ \Psi ) ) \| ^ { 2 } ,
$$

where $R ^ { d } ( \beta \circ \Psi ) \ : = \ : \mathbb { E } [ \mathcal { L } ( g ( \Psi ( x ) ) , y ) ]$ is the prediction loss in domain $d$ with an encoder $\Psi$ and classifier $\beta , w$ is a “dummy” classifier and can be fixed as a scalar 1.0, and $\mathbb { D } _ { \mathrm { t r } }$ is the set that includes all training domains. According to (Arjovsky et al. 2019), the gradient of $R ^ { d } ( w ( \beta \circ \Psi ) )$ reflects the invariance of the learned latent representations. The non-negative hyperparameter $\lambda$ controls the balance between the predictive ability and invariance.

Even though causal factor learning usually does not have specific restrictions regarding the encoder architecture, it is worth noting that an arbitrary architecture cannot provide certifiable robustness in the latent space. Therefore, for the second task, we adopt the 1-Lipschitz network (Trockman and Kolter 2021) to derive certifiable robustness across domains.

# Certified Robustness for Unseen Domains

While significant progress has been made in certified defenses when training and test data share the same distribution, there is still limited exploration and a lack of theoretical guarantees for certified robustness under domain shifts. In this subsection, we bridge this gap by utilizing the theoretical support from certified defense (Cohen, Rosenfeld, and Kolter 2019) and causal inference to derive necessary theorems in this setting.

According to previous discussions, we perform random smoothing for the causal factors in the latent space. Therefore, based on the calculation of the certified radius, we introduce the following Theorem 1:

Theorem 1. Suppose we have a causal encoder $\Psi : \mathcal { X }  \mathcal { Z }$ , and an arbitrary classifier $\beta : \mathcal { Z }  \mathcal { V }$ . Let $g _ { \beta }$ be defined as $g _ { \beta } ( z ) = \underset { y \in \mathcal { V } } { \arg \operatorname* { m a x } } P ( \beta ( z + \eta ) = y )$ , where $\eta \sim \mathcal { N } ( 0 , \sigma ^ { 2 } I )$ , $z = \Psi ( x )$ is the latent causal representation. Suppose $p _ { A }$ is the lower bound of $p _ { A } , \overline { { p _ { B } } }$ is the upper bound of $p _ { B }$ (here $p _ { A }$ and $\overline { { p _ { B } } }$ are obtained based on $g _ { \beta }$ with input in the representation space), $\underline { { p _ { A } } } , \overline { { p _ { B } } } \in [ 0 , 1 ]$ and satisfy:

$$
P ( \beta ( z + \eta ) = y _ { A } ) \ge \underline { { p _ { A } } } \ge \overline { { p _ { B } } } \ge \operatorname* { m a x } _ { y \ne y _ { A } } P ( \beta ( z + \eta ) = y ) .
$$

Then d, $d ^ { \prime } \in \mathbb { D }$ , $g _ { \beta } ( z ^ { d } + \delta _ { z } ) = g _ { \beta } ( z ^ { d ^ { \prime } } + \delta _ { z } ) = y _ { A }$ for all $\| \delta _ { z } \| _ { 2 } < C R _ { z }$ , where $\delta _ { z }$ is the perturbation applied to the latent causal representation $z$ and

$$
C R _ { z } ( \beta ; x , y ) = \frac { \sigma } { 2 } ( \Phi ^ { - 1 } ( \underline { { { p _ { A } } } } ) - \Phi ^ { - 1 } ( \overline { { { p _ { B } } } } ) ) .
$$

This theorem provides us a theoretical guarantee that any perturbation $\delta _ { z }$ within the range $C R _ { z }$ will not change the prediction of the smoothed classifier $g _ { \beta }$ . It also provides a theoretical guarantee for generalization: for any two instances from $d$ and $d ^ { \prime }$ respectively, if their causal latent representations (denoted by $z ^ { d }$ and $z ^ { d ^ { \prime } }$ ) learned from the causal encoder $\Psi$ are the same, then the predictions of $g _ { \beta }$ for them are consistent. Moreover, the certified radius for $z$ across these domains will also be consistent. Therefore, Theorem 1 provides theoretical support for achieving certified robustness on data in different domains by performing random smoothing in the latent space.

![](images/890d56239fc8e3fedcd995e84cfab9b6a9cf187bfd901b5b349c61e2567b913d.jpg)  
Figure 2: An overview of the proposed framework GLEAN. The upper part represents the training process, while the lower part depicts the certification process on the test domain. Here, we showcase two training domains and one test domain with two classes 0 and 1, where the color of the object is a spurious factor. We define $S = 0$ as orange and $S = 1$ as green. In training domain 1, the spurious distribution between color and category is $P ( Y = 0 | S = 0 ) = 0 . 9$ and $P ( Y = 1 | S = 1 ) = 0 . 1$ . These values change to 0.8 and 0.2, respectively in training domain 2, and then to 0.1 and 0.9 in the test domain. Thus, there is a correlation shift between the different domains of this dataset. The causal encoder is equipped with Lipschitz constraints with Lipschitz constant $L . Y _ { t r a i n }$ and $Y _ { t e s t }$ are ground truth labels. $\hat { Y }$ is the predicted label. $z$ is the causal latent representations and each $\eta$ is a Gaussian noise. $y _ { A }$ is the most probable class among all the $\hat { Y }$ after sampling with the probability $p _ { A }$ . Then we can leverage $p _ { A }$ to compute the certified radius in latent space $C R _ { z }$ and finally revert it back to get the certified radius $C R$ in input space.

Another significant problem left is that the certified radius $C R _ { z }$ mentioned in Theorem 1 is obtained by applying Gaussian noise within latent space and then performing Monte Carlo sampling. Thus, the robustness guarantee is only for $\beta$ However, in practice, attackers often directly perturb input features. Therefore, the certified radius obtained in the latent space needs to be mapped back to the input space to provide certified robustness for the entire classifier $f$ . Correspondingly, we have Theorem 2 as follows:

Theorem 2. Let the causal encoder $\Psi$ be $L$ -Lipschitz. Let $g$ be defined as $g ( x ) = \underset { y \in \mathcal { Y } } { \arg \operatorname* { m a x } } P ( \beta ( \Psi ( x + \eta ) ) = y )$ . Then $g ( x ^ { d } + \delta ) = g ( x ^ { d ^ { \prime } } + \delta ) = y _ { A }$ for all $\| \delta \| _ { 2 } < C R _ { z } / L ,$ , where $\delta$ is the perturbation applied to input features $x$ . Briefly, if we use an $L$ -Lipschitz neural network in the causal factor learning module, we can calculate the certified radius in the input space. This is because we can simply scale the certified radius in the latent space by the Lipschitz constant $L$ , such that $C R \geq C R _ { z } / L$ . If $L = 1$ , then $C R _ { z }$ will be the lower bound of $C R$ . With the aforementioned causal invariant assumption, the certified robustness for instances in one domain can also be propagated to instances in other domains with the same causal factors. Therefore, we are able to provide theoretical guarantees for cross-domain certified robustness. Detailed proofs can be found in the Appendix.

# Implementation

Overview of Framework We integrate the previous methods and theories to form our framework, which is demonstrated in Figure 2. In Figure 2, the gray path represents the training process. During training, we apply Gaussian augmentation to $z$ to enhance the prediction accuracy during the RS phase. The green path represents the certifying process. We first train the causal encoder $\Psi$ and classifier $\beta$ , then obtain robustness guarantees for the classifier $\beta$ by adding Gaussian noise to $z$ with Monte Carlo sampling. The bottom path represents the mapping process. Specifically, it involves multiplying the certified radius in the latent space by the mapping constant $1 / L$ , and reverting back to the input space to obtain robustness guarantees for the input feature $x$ .

Architecture As aforementioned, we use Lipschitz constraints in the causal factor learning module. We define the final linear layer as the classifier $\beta$ , with all preceding layers forming the encoder $\Psi$ . We apply the Cayley transform (Trockman and Kolter 2021) to achieve orthogonality, thereby ensuring that each linear layer has a Lipschitz constant of 1. For the activation functions, we employ GroupSort (Anil, Lucas, and Grosse 2019), which also has 1-Lipschitzness. More details on the implementation of 1-Lipschitz networks can be found in the Appendix.

Table 2: A comparison of certified test accuracy $( \% )$ and ACR between our framework and baselines. For each method, we recorded data for ten radii $r$ ranging from 0.00 to 0.45, with increments of 0.05. Every model is certified with $\sigma = 0 . 1 2$ . We highlight our results in bold whenever the value improves the baselines.   

<html><body><table><tr><td>Datasets</td><td>Models</td><td>r=0.00</td><td>0.05</td><td>0.10</td><td>0.15</td><td>0.20</td><td>0.25</td><td>0.30</td><td>0.35</td><td>0.40</td><td>0.45</td><td>ACR</td></tr><tr><td rowspan="5">CMNIST</td><td>Gaussian</td><td>18.1</td><td>14.8</td><td>12.9</td><td>10.5</td><td>9.0</td><td>8.6</td><td>8.1</td><td>7.8</td><td>7.3</td><td>6.0</td><td>0.0458</td></tr><tr><td>MACER</td><td>23.6</td><td>19.4</td><td>15.5</td><td>12.4</td><td>10.1</td><td>8.8</td><td>7.4</td><td>5.6</td><td>4.1</td><td>1.9</td><td>0.0482</td></tr><tr><td>SmoothAdv</td><td>27.2</td><td>22.7</td><td>16.9</td><td>13.6</td><td>10.5</td><td>8.5</td><td>7.3</td><td>5.8</td><td>3.6</td><td>1.3</td><td>0.0518</td></tr><tr><td>Consistency</td><td>12.1</td><td>11.3</td><td>10.8</td><td>10.7</td><td>10.6</td><td>10.5</td><td>10.4</td><td>10.4</td><td>10.3</td><td>10.3</td><td>0.0488</td></tr><tr><td>GLEAN (Ours)</td><td>64.3</td><td>62.7</td><td>60.5</td><td>58.0</td><td>55.8</td><td>53.2</td><td>51.4</td><td>47.9</td><td>44.9</td><td>38.6</td><td>0.2466</td></tr><tr><td rowspan="5">CelebA</td><td>Gaussian</td><td>31.0</td><td>31.0</td><td>31.0</td><td>29.0</td><td>28.0</td><td>26.0</td><td>26.0</td><td>24.0</td><td>22.0</td><td>17.0</td><td>0.1218</td></tr><tr><td>MACER</td><td>21.0</td><td>19.0</td><td>15.0</td><td>13.0</td><td>10.0</td><td>10.0</td><td>10.0</td><td>8.0</td><td>7.0</td><td>6.0</td><td>0.053</td></tr><tr><td>SmoothAdv</td><td>25.0</td><td>23.0</td><td>18.0</td><td>16.0</td><td>14.0</td><td>11.0</td><td>10.0</td><td>9.0</td><td>9.0</td><td>5.0</td><td>0.0623</td></tr><tr><td>Consistency</td><td>24.0</td><td>24.0</td><td>23.0</td><td>23.0</td><td>22.0</td><td>21.0</td><td>20.0</td><td>20.0</td><td>20.0</td><td>20.0</td><td>0.0989</td></tr><tr><td>GLEAN (Ours)</td><td>62.0</td><td>59.0</td><td>57.0</td><td>56.0</td><td>52.0</td><td>51.0</td><td>49.0</td><td>45.0</td><td>42.0</td><td>38.0</td><td>0.2326</td></tr><tr><td rowspan="5">DomainNet</td><td>Gaussian</td><td>33.0</td><td>32.0</td><td>32.0</td><td>31.0</td><td>28.0</td><td>25.0</td><td>24.0</td><td>23.0</td><td>22.0</td><td>19.0</td><td>0.1223</td></tr><tr><td>MACER</td><td>28.0</td><td>28.0</td><td>28.0</td><td>28.0</td><td>28.0</td><td>28.0</td><td>28.0</td><td>28.0</td><td>27.0</td><td>27.0</td><td>0.1272</td></tr><tr><td>SmoothAdv</td><td>27.0</td><td>27.0</td><td>26.0</td><td>26.0</td><td>25.0</td><td>24.0</td><td>23.0</td><td>23.0</td><td>20.0</td><td>18.0</td><td>0.1097</td></tr><tr><td>Consistency</td><td>27.0</td><td>27.0</td><td>27.0</td><td>27.0</td><td>27.0</td><td>27.0</td><td>27.0</td><td>27.0</td><td>27.0</td><td>27.0</td><td>0.1234</td></tr><tr><td>GLEAN (Ours)</td><td>63.0</td><td>61.0</td><td>55.0</td><td>54.0</td><td>48.0</td><td>43.0</td><td>41.0</td><td>37.0</td><td>30.0</td><td>23.0</td><td>0.2088</td></tr></table></body></html>

# Experiments

In this section, we conduct extensive experiments to evaluate our framework on one synthetic dataset and two real-world datasets. Specifically, we answer the following research question based on the experimental results. RQ1: How does GLEAN perform compared to the baselines of certified defense? RQ2: How do different components in GLEAN contribute to the performance? RQ3: How does GLEAN perform under different settings of hyperparameters?

# Datasets

We introduce the three datasets used in the experiments: CMNIST (Arjovsky et al. 2019), CelebA (Liu et al. 2015) and DomainNet (Peng et al. 2019). Detailed information on the domain construction and division of all these three datasets can be found in the Appendix.

# Experiment Settings

Baselines. We evaluate our framework by comparing it against several representative certified defense methods. All these methods are based on RS:

• Gaussian (Cohen, Rosenfeld, and Kolter 2019): Standard training with Gaussian noise based random smoothing. • MACER (Zhai et al. 2020): Add a regularization term that maximizes an approximate form of the certified radius. • SmoothAdv (Salman et al. 2019): Adversarial training is incorporated during the training of the smoothed classifier. • Consistency (Jeong and Shin 2020): The KullbackLeibler divergence between the mean of the classifier’s predictions after various perturbations and the prediction after a single perturbation was used as a regularization term. This term minimizes the variance in the classifier’s predictions after different perturbations, optimizing the objective for robust training of the smoothed classifier.

Evaluation Metrics. We consider two widely-used evaluation metrics: (1) certified accuracy at different radii, which is defined as the fraction of the test set that CERTIFY (Cohen, Rosenfeld, and Kolter 2019) classifies correctly. CERTIFY is a practical Monte Carlo-based certification procedure that offers the prediction of $g$ along with the lower bound of the certified radius or abstains the certification by sampling over $n$ Gaussian noises with the probability of at least $1 - \alpha$ , $\alpha$ is the significance level; (2) average certified radius (ACR), which is defined as $\begin{array} { r } { A C R \ = \ \frac { 1 } { | \mathbb { D } _ { \mathsf { t e s t } } | } \sum _ { ( x , y ) \in \mathbb { D } _ { \mathsf { t e s t } } } C R ( f ; \sigma , x , y ) \ . } \end{array}$ $\mathbf { 1 } _ { [ g ( x , \sigma ) = y ] }$ . Here, $\left| \mathbb { D } _ { \mathtt { t e s t } } \right|$ is the capacity of the test set, $C R$ is the certified radius returned by CERTIFY, 1 is the indicator function. We assign 0 to $C R$ for incorrect prediction of $g$ . We use the same settings in (Cohen, Rosenfeld, and Kolter 2019) with $n = 1 0 0 0 0 0 , n _ { 0 } = 1 0 0 , \alpha = 0 . 0 0 1$ to apply CERTIFY. Here $n _ { 0 }$ is the small number of samples to find $y _ { A }$ . Note that, for two different models, their certified accuracies sometimes cannot be directly compared. At a specific radius $r$ , one model may have a higher certified accuracy than the other, but the situation may be reversed at another radius. Therefore, ACR is a more suitable metric as it reflects “average robustness”. Training Details. We use a three-layer MLP for CMNIST and a four-layer CNN for CelebA and DomainNet. During inference, we apply RS with the noise level $\sigma = 0 . 1 2$ . The result of other $\sigma$ is shown in the Appendix. We set the parameter of the regularization term $\lambda = 1 0 0 0 0$ for all datasets.

# Experiment Results

Performance For all datasets, more detailed settings for the training parameters are provided in the Appendix. Table 2 shows a comparison of performance between our framework and baselines w.r.t. ACR and the certified test accuracy with different radii $r$ . We also plot the radius-certified accuracy curve in Figure 3. Note that ACR is equivalent to the area under the curve. From the results, we observe that our method achieves the highest certified accuracy and ACR (with a significant improvement compared with others) at almost all

CGoanusisitaen cyMAOCuErsR Smoothadv CGoanusisitaen cyMAOCuErsR Smoothadv CGoanusisitaen cyMAOCuErsR Smoothadv 1 舞 F 0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.1 0.2 0.3 0.4 0.5 Radius Radius Radius (a) CMNIST (b) CelebA (c) DomainNet

GR w/o Invariance & Lipschitz w/o Invariance & Lipschitz w/o ILnipvsarcihaintzce 0.8 w/o ILnipvsarcihaintzce Ours Ours 0.6   
0   
0.0 0.2 0.4 0.00 0.25 0.500.751.00 Radius Radius (a) σ = 0.12 (b) σ = 0.25   
C 𝜆 = 1𝑒1 𝜆 = 1𝑒2 𝜆 = 1𝑒3 𝜎 = 0.12 𝜆 = 1𝑒4 𝜆 = 1𝑒5 𝜆 = 1𝑒6 𝜎 = 0.25 𝜎 = 0.50 𝜎 = 1.00   
0   
0.0 0.2 0.4 0 1 2 3 Radius Radius (a) Vary λ (b) Vary σ

radii across the three datasets. Given that our training and testing data reside in different domains, the experimental results demonstrate that our approach significantly and consistently outperforms baselines in the generalization of certified robustness across domains. We omit the variance of the experimental results because it is far smaller than the performance gap between the methods. From Table 2, we can also observe that the ACR decreases progressively from CMNIST to CelebA to DomainNet. This decline is reasonable because the CMNIST dataset only involves spurious correlations between color and digits, whereas CelebA, in addition to the constructed spurious correlation between smiling and hair color, includes more complicated domain shifts regarding other facial features. For DomainNet, the complex variation in backgrounds makes the causal relationships within the data more difficult to capture.

Ablation Study To evaluate the effectiveness of each component in our method, we provide ablation study with the following variants: (1) w/o invariance: We remove the invariance regularization term in Eq.(8) and only use the first term as an ERM loss. (2) Network without Lipschitz Constraints: We replace the 1-Lipschitz layers in the network with the ones without any constraints. We conducted comparisons with two types of ablation studies simultaneously under $\sigma = 0 . 1 2$ and $\sigma = 0 . 2 5$ . As shown in Figure 4, our model undoubtedly outperforms the version without the invariant penalty since this variant cannot capture the causal factors effectively and thus fails to mitigate the influence of spurious correlations on robustness. For our model, the certified accuracy and ACR with Lipschitz constraints is slightly better than that of networks without any constraints. This is because Lipschitz constraints ensure that we use causal factors for certification. The results of the other two datasets are provided in the Appendix.

Parameter Study We set the hyperparameter $\begin{array} { r l } { \lambda } & { { } \in } \end{array}$ $\{ 1 0 , 1 0 ^ { 2 } , 1 0 ^ { 3 } , 1 0 ^ { 4 } , 1 0 ^ { 5 } , 1 0 ^ { 6 } \}$ , $\sigma \in \{ 0 . 2 5 , 0 . 5 0 , 1 . 0 0 \}$ . The results of the parameter study on CMNIST are shown in Figure 5. The results for the other two datasets are provided in the Appendix. We can observe in Figure 5 (a) that when $\lambda$ increases, the certified accuracy at the same radius also increases, this is because a higher $\lambda$ leads to stronger causal factor learning, and achieving stronger generalizable robustness. However, when $\lambda$ exceeds 10,000, the improvement in model performance becomes negligible as it has reached the bottleneck of the model’s ability to learn invariant causal factors. As shown in Figure 5 (b) $\sigma$ controls the level of noise. A higher noise level means that we can obtain a larger certified radius but at the cost of reduced certified accuracy.

# Conclusion

In this paper, we address the critical problem of generalizing certified robustness across different domains. We analyze the limitations of existing certified defense strategies and explore the challenges posed by robustness under domain shifts. To address this problem, we introduce a novel causality-inspired framework, GLEAN, designed to learn causal factors that mitigate the negative impact of spurious correlations on robustness, enabling a certifiable defense process across various domains. Extensive experiments on both synthetic and real-world benchmarks verify the effectiveness of our method. GLEAN can pave the path for future work that aims at further exploring causality-inspired defenses and any unified approaches for the generalization of adversarial robustness.