# Online and Streaming Algorithms for Constrained $k$ -Submodular Maximization

Fabian Christian Spaeh,1 Alina Ene,1 Huy Nguyen2

1Department of Computer Science, Boston University 2Khoury College of Computer Science, Northeastern University fspaeh $@$ bu.edu, aene $@$ bu.edu, hu.nguyen $@$ northeastern.edu

# Abstract

Constrained $k$ -submodular maximization is a general framework that captures many discrete optimization problems such as ad allocation, influence maximization, personalized recommendation, and many others. In many of these applications, datasets are large or decisions need to be made in an online manner, which motivates the development of efficient streaming and online algorithms. In this work, we develop single-pass streaming and online algorithms for constrained $k$ -submodular maximization with both monotone and general (possibly non-monotone) objectives subject to cardinality and knapsack constraints. Our algorithms achieve provable constant-factor approximation guarantees which improve upon the state of the art in almost all settings. Moreover, they achieve the fastest known running times and have optimal space usage. We experimentally evaluate our algorithms on instances for ad allocation and other applications, where we observe that our algorithms are practical and scalable, and construct solutions that are comparable in value even to offline greedy algorithms.

# 1 Introduction

We develop algorithms for maximizing a $k$ -submodular function $f$ subject to cardinality or knapsack constraints. $k$ - Submodular functions capture the property of diminishing returns under an allocation of elements from a ground set $V$ to $k$ parts. Specifically, we are trying to find $k$ disjoint subsets $( S _ { 1 } , \ldots , S _ { k } )$ of $V$ such that $f ( S _ { 1 } , \ldots , S _ { k } )$ is maximized. We consider constrained $k$ -submodular maximization where we enforce individual constraints for each part: Each part $a \in \{ 1 , \ldots , k \}$ has a specified budget $n _ { a }$ and we are only allowed to allocate at most $| S _ { a } | \le n _ { a }$ items.

$k$ -Submodular functions model several important applications such as ad allocation. In this problem, ad impressions arrive online and we have to allocate them immediately to one of $k$ advertisers. At the same time, each advertiser $a \in \{ 1 , \ldots , k \}$ is willing to pay for at most $n _ { a }$ ad impressions (Feldman et al. 2009). The advertising platform makes an allocation to maximize advertiser satisfaction, for which natural objectives such as user exposure show diminishing returns. Another important application is in personalized recommendation, which motivates the study of general (possibly non-monotone) objectives. Mirzasoleiman,

Badanidiyuru, and Karbasi (2016) show how to model a recommender system through cut functions in dissimilarity graphs, which are general $k$ -submodular. Related tasks such as document or image summarization (Lin and Bilmes 2011; Gomes and Krause 2010) can be modeled through similar objectives. Additional motivation on influence maximization, sensor placement, and video summarization is given in the works of Ohsaka and Yoshida (2015a) and Feldman, Karbasi, and Kazemi (2018). These further applications also motivate the study of more general packing constrains such as knapsack, where items take up non-uniform space in the budget constraints.

The datasets used in all of these applications are typically large and even offline greedy algorithms are often impractical. This necessitates streaming algorithms which only make one or multiple passes over the data and require little space. Furthermore, applications such ad allocation require us to make decisions online: Upon arrival, we need to immediately allocate a user to an advertiser without the chance of re-allocating later. Algorithms that work simultaneously in the online and streaming setting are therefore desirable and well-studied (Ene and Nguyen 2022; Feldman et al. 2021). We contribute to this line of work by designing algorithms with improved approximation guarantees. We make further progress with novel algorithms for more general settings.

Our Contributions and Techniques: Our key contributions for the online and streaming setting are:

1. We introduce a novel, general algorithm and analysis framework. Algorithms in our framework are combinatorial and very efficient, have optimal space and attain the best known running time for our setting. Our framework does not only allow us to improve upon prior results in a unified way, but also handles much more general non-monotone objectives and budget constraints.

2. Our framework allows us to address settings that were previously out of reach. For general (possibly nonmonotone) $k$ -submodular maximization, we are the first to obtain a constant-factor approximation, which was previously not even known in the offline setting (Xiao et al. 2022). We are also first to provide an algorithm for $k$ - submodular maximization subject to individual knapsack constraints (here, items do not have uniform size in the budget constraints), where prior work is restricted to a common constraint (Pham et al. 2022; Ha, Pham, and Tran 2024).

Table 1: Comparison of algorithms for $k$ -submodular maximization with cardinality constraints. We let $n = \operatorname* { m i n } _ { a \in [ k ] } n _ { a }$ denote the minimum budget, $\begin{array} { r } { r = \sum _ { a \in [ k ] } n _ { a } } \end{array}$ the total budget, and $m = | V |$ .   

<html><body><table><tr><td>Objective</td><td>Reference</td><td>Setting</td><td>Approx.</td><td>Time</td><td>Space</td></tr><tr><td rowspan="3">monotone</td><td>(Ohsaka and Yoshida 2015b)</td><td>offline</td><td>1</td><td>O(mkr)</td><td>0(m)</td></tr><tr><td>(Ene and Nguyen 2022)</td><td>online, streaming</td><td>~ 0.2953asn→8</td><td>O(mk)</td><td>0(r)</td></tr><tr><td>Theorem 1 (This paper)</td><td>online,streaming</td><td>≥ ≈ 0.3178 as n → 0</td><td>O(mk)</td><td>0(r)</td></tr><tr><td rowspan="2">general</td><td>(Xiao et al. 2022)</td><td>offline</td><td>4+maxqna</td><td>O(rmk)</td><td>0(r)</td></tr><tr><td>Thespem2)</td><td>online, streaming</td><td>~ 0.158asn→ 8</td><td>0(mk)</td><td>0(r)</td></tr></table></body></html>

3. Our algorithms achieve provable constant factor approximation guarantees that improve upon the state of the art for $k$ -submodular maximization (cf. Table 1). Due to their versatility, they also apply to the related and practically relevant setting of submodular maximization with a partition matroid. Here, we improve upon the guarantee of Feldman, Karbasi, and Kazemi (2018) for general objectives and close the gap to less efficient streaming algorithms (cf. Table 4). We achieve this by introducing new techniques that are suited for general (possibly non-monotone) objectives.

These contributions are in the context of individual constraints, which is what we discuss in the main body. Starting with Ohsaka and Yoshida (2015a), there has been a lot of work for a common constraint on the support, e.g. $| S _ { 1 } \cup S _ { 2 } \cup \dots \cup S _ { k } | \le n$ for a cardinality constraint Ha, Pham, and Tran (2024); Yu et al. (2023); Wang and Zhou (2021); Sakaue (2017); Nguyen and Thai (2020). Such constraints cannot capture individual constraints. However, applying our framework to the common budget setting yields efficient online and streaming algorithms, which we outline in Section 3.3 and formally discuss in Appendix D.

Related Work: The main focus of our work is on individual constraints and we therefore discuss only work related to this setting in the main body. However, our techniques also apply to the setting with a common constraint, and we also obtain further results for knapsack constraints. We discuss this in Appendix $\mathrm { ~ D ~ }$ along the relevant related work.

Offline algorithms for $k$ -submodular maximization. Most results for $k$ -submodular maximization are in the offline setting. For monotone objectives, Ohsaka and Yoshida (2015a) provide a greedy algorithm that achieves a $\textstyle { \frac { 1 } { 3 } }$ -approximation. The only prior work on general $k$ -submodular maximization is due to Xiao et al. (2022) and in the offline setting. Their greedy approach obtains a $\frac { 1 } { 4 + \operatorname* { m a x } _ { a } n _ { a } }$ -approximation, which decreases with the maximum budget.

Online and streaming algorithms for $k$ -submodular maximization. We consider the challenging and practically relevant online and streaming setting. The only work in this space is due to Ene and Nguyen (2022) and applies only to monotone objectives. For monotone objectives, our algorithms achieve an improved approximation guarantee as shown in Table 1. We give the first algorithm for general objectives with constant-factor approximation guarantees. Such a result was not even known in the offline setting.

Submodular maximization with a partition matroid. A related and important setting is submodular maximization with a partition matroid. Feldman et al. (2009) provide a seminal online algorithm for linear objectives based on the primal-dual approach. For general submodular objectives under a matroid constraint, Feldman et al. (2022) give a streaming algorithm based on the continuous extension of a submodular function, whose evaluation is costly. Their algorithm also maintains multiple solutions and is thus not suited for the online setting. Our discrete algorithms achieve the same guarantees when the minimum budget tends to infinity, which is a necessary assumption for the online setting (Feldman et al. 2009). In the online setting, Feldman, Karbasi, and Kazemi (2018) give a discrete algorithm for general objectives under $p$ -matchoid constraints. Their algorithm subsamples items which is also a technique we employ, but we obtain an improved approximation ratio.

# 2 Preliminaries

$k$ -Submodular functions. We define a partial allocation $( k + 1 ) ^ { V }$ as the set of all $k$ -tuples $( \bar { X _ { 1 } , \ldots , } \bar { X _ { k } } )$ of disjoint subsets $X _ { a } \subseteq V , X _ { a } \cap X _ { b } \ = \ \varnothing$ for all $a , b \in [ k ]$ where $[ k ] : = \{ 1 , 2 , \ldots , k \}$ . For a $k$ -tuple $\mathbf { X } \in ( k + 1 ) ^ { V }$ we define the support $\mathbf { s u p p } ( \mathbf { X } ) \ : = \ X _ { 1 } \cup \dots \cup \ X _ { k }$ as all allocated items. Given another $k$ -tuple $\mathbf { Y } \in ( k + 1 ) ^ { V }$ , we define the intersection $\mathbf { X } \sqcap \mathbf { Y }$ of two $k$ -tuples through $( \mathbf { X } \cap \Sigma ) _ { a } : = X _ { a } \cap Y _ { a }$ for all $a \in [ k ]$ , and the union as $( \mathbf { X } \sqcup \mathbf { Y } ) _ { a } : = ( X _ { a } \cup Y _ { a } ) \setminus \bigcup _ { b \neq a } ( X _ { b } \cup Y _ { b } )$ . Given these operations, we say $f \colon ( k + 1 ) ^ { V } \to { \mathbb { R } }$ is $k$ -submodular if

$$
f ( \mathbf { X } ) + f ( \mathbf { Y } ) \geq f ( \mathbf { X } \sqcap \mathbf { Y } ) + f ( \mathbf { X } \sqcup \mathbf { Y } )
$$

for all $\mathbf { X } , \mathbf { Y } \in ( k + 1 ) ^ { V }$ . We define the marginal gain of adding element $t$ to part $a$ of $\mathbf { X }$ as

$$
\Delta _ { t , a } f ( \mathbf { X } ) : = f \left( \left( X _ { 1 } , \ldots , X _ { a } \cup \{ t \} , \ldots , X _ { k } \right) \right) - f ( \mathbf { X } ) .
$$

We further write $\mathbf { X } \preceq \mathbf { Y }$ if $X _ { a } \subseteq Y _ { a }$ for all $a \in [ k ]$ and say $f$ is monotone if $f ( \mathbf { X } ) \leq f ( \mathbf { Y } )$ for all $\mathbf { X } \preceq \mathbf { Y }$ . To obtain a notion of diminishing returns, we say that $f$ is orthant submodular if

$$
\Delta _ { t , a } f ( \mathbf { X } ) \geq \Delta _ { t , a } f ( \mathbf { Y } )
$$

for all $\mathbf { X } \preceq \mathbf { Y }$ and $t \not \in \mathbf { \Gamma } { \mathbf { s u p p } } ( \mathbf { Y } )$ . We say $f$ is pairwise monotone if for all $t \not \in \mathbf { s u p p } ( \mathbf { X } )$ and $a \neq b$ ,

$$
\Delta _ { t , a } f ( \mathbf { X } ) + \Delta _ { t , b } f ( \mathbf { X } ) \geq 0 .
$$

We know that $f$ is $k$ -submodular if and only if $f$ is orthant submodular and pairwise monotone (Ward and Zivny´ 2016). A function is called submodular if it is 1-submodular.

Problem definition. In $k$ -submodular maximization, we are given a $k$ -submodular function and budgets $n _ { 1 } , \ldots , n _ { k }$ for every part. The goal is to find a solution that maximizes $f$ while allocating at most $n _ { a }$ items to every part $a$ , and we denote the optimum solution as $\mathbf { S } ^ { \ast }$ . More general knapsack constraints and the setting with a common constraint are defined in Appendix D.

We consider both problems in the (single-pass) streaming model. Here, all items of $V$ arrive in an arbitrary (possibly adversarial) order and the task is to generate a solution at the end of the stream, while using as little space as possible. Our algorithms simultaneously apply to the online setting with free disposal (Feldman et al. 2009). Here, items also arrive one at a time, but we are required to maintain a single solution after each arrival. Additionally, we are only allowed to add the arriving item to the solution or dispose (i.e. remove) an item from the solution, but cannot re-allocate.

Examples of $k$ -submodular functions. We now give examples of $k$ -submodular functions that arise in the applications to ad allocation and recommender systems discussed in the introduction and our experiments. The well-studied submodular welfare problem is a special case of $k$ -submodular maximization. Here, the goal is to maximize the welfare $\begin{array} { r } { f \left( \mathbf { X } \right) = \sum _ { a = 1 } ^ { k } h _ { a } ( X _ { \underline { { a } } } ) } \end{array}$ where $h _ { a } \colon 2 ^ { V } \to \mathbb { R } _ { + }$ is the valuation fo  agent $a \in [ k ]$ . If the $h _ { a }$ ’s are submodular then $f$ is orthant submodular and if they are monotone, then $f$ is monotone. Such instances appear for ad allocation where advertiser satisfaction can be modeled through a function $h _ { a }$ that expresses, for example, the coverage of an ad campaign. If $h _ { a } = h$ where $h$ is a submodular function that is symmetric (i.e., $h ( X ) = h ( V \backslash X )$ for all $X \subseteq V )$ , then $f$ is general $k$ -submodular (i.e., it is pairwise monotone and orthant submodular). Such instances arise from graph cut functions in applications such as recommender systems.

# 3 $k$ -Submodular Maximization

In the following, we illustrate our framework on monotone $k$ -submodular maximization, as it is a fundamental setting that allows us to convey the core ideas and novelties of our algorithm and analysis. Section 3.1 contains an algorithm description, intuition, and analysis overview. In Section 3.2, we give a short overview of how additional novel ideas allow us to adapt our framework to obtain the first constant factor approximation algorithm for general $k$ -submodular maximization. Algorithms and analysis for both settings are described in full detail in Appendix A. The algorithm and analysis framework that we develop in this section is general and with some additional work applies to related settings, and we outline these contributions in Section 3.3.

# 3.1 Monotone Objectives

Our algorithm for maximizing a monotone $k$ -submodular function is shown in Algorithm 1. As shown in Table 1, this improves upon the approximation guarantee of Ene and Nguyen (2022).

Parameters: coefficients {ga(i)}a [k],i [n ]   
Input: a monotone $k$ -submodular function $f$ and the bud  
gets {na}a [k]   
$\mathbf { S } = ( S _ { 1 } , \dots , S _ { k } )  ( \emptyset , \dots , \emptyset )$   
$\beta _ { a } \gets 0$ for all $a \in [ k ]$   
for $t = 1 , 2 , \dots , | V |$ : let $w _ { t , a } = \Delta _ { t , a } f$ (S) for all $a \in [ k ]$ let $a = \arg \operatorname* { m a x } _ { a \in [ k ] } \left\{ w _ { t , a } - \beta _ { a } \right\}$ if $w _ { t , a } - \beta _ { a } \geq 0$ : if $| S _ { a } | < n _ { a }$ : Sa ← Sa ∪ {t} else: let t′ = arg mini S wi,a $S _ { a } \gets ( S _ { a } \setminus \{ t ^ { \prime } \} ) \cup \{ t \} ^ { a }$ let $w _ { a } ( i )$ be the $i$ -th largest weight in $\{ w _ { t , a } \colon t \in S _ { a } \}$ and $w _ { a } ( i ) = 0$ for $i > | S _ { a } |$ $\begin{array} { r } { \beta _ { a }  \sum _ { i = 1 } ^ { n _ { a } } w _ { a } ( i ) g _ { a } ( i ) } \end{array}$   
return S

The algorithm maintains thresholds $\beta _ { a }$ that ensure high gain and balance the allocation. On arrival of each item $t$ , we evaluate its marginal gains for each part $a$ with respect to the current solution S. We denote these marginal gains as weights $w _ { t , a }$ . The algorithm decides whether to allocate the item and to which part using the discounted weights $w _ { t , a } - \beta _ { a }$ for all parts $a \in [ k ]$ . The algorithm chooses the part with largest discounted weight and allocates only if the discounted weight is non-negative. Thus, $\beta _ { a }$ acts as a threshold that the weight of item $t$ has to pass to be added to the solution. A critical design consideration is how to set the thresholds $\beta _ { a }$ , and we set them to a carefully designed linear combination of the weights of currently allocated items along the coefficients

$$
g _ { a } ( i ) : = \frac { c _ { a } } { n _ { a } } \bigg ( 1 + \frac { d _ { a } } { n _ { a } } \bigg ) ^ { i - 1 } \mathrm { f o r } c _ { a } : = \frac { 1 + d _ { a } } { \big ( 1 + \frac { d _ { a } } { n _ { a } } \big ) ^ { n _ { a } } - 1 }
$$

for all $i \in [ n _ { a } ]$ with constants $d _ { a }$ , which we will specify in Theorem 1 according to the budget $n _ { a }$ . If the chosen part is at capacity, we dispose of the item with lowest weight.

Intuition. Our basic design can be understood for linear objectives, where the weights $w _ { t , a }$ and the item values are identical. Let us consider this setting first. Here, a natural approach is a greedy allocation rule where the discounted weight is the value of the new item minus the value of the item that will be disposed (or 0 if no disposal is required). A crucial downside is that the greedy scheme fails to balance the allocation among the parts, which is necessary to defend against future inputs. We illustrate this with an example in Appendix A.2 due to Feldman et al. (2009). A solution is to consider the allocation by discounting the value of a new item by an exponential average over the values of all items currently allocated to a part.

In the submodular setting, an important difficulty is that the change in objective value when adding and disposing an item is no longer fixed after we modify the current solution. We therefore use the marginal gain $w _ { t , a }$ when an item arrives as a proxy for this gain or loss. To account for the fact that the loss of disposing an item might be larger than the proxy, we require new items to pass a higher threshold as in the linear setting. As such, it no longer suffices to set the thresholds $\beta _ { a }$ as a weighted average. Instead, we require a carefully-designed linear combination of the gains to set the thresholds $\beta _ { a }$ . The appropriate coefficients for each part $a$ are given through the functions $g _ { a } ( i )$ and we derive them via a careful and novel analysis which we outline below.

<html><body><table><tr><td colspan="3">Approximation guarantee mina Q</td></tr><tr><td>mina na</td><td>≤3 ≥4</td><td></td></tr><tr><td rowspan="2">approx</td><td>≥0.25 ≥0.3178</td><td>0.768</td></tr><tr><td>1-</td><td></td></tr></table></body></html>

Table 2: Parameter choices and approximation guarantee for monotone $k$ -submodular maximization. As shown in Equation (3), $Q _ { a }$ reflects the approximation ratio per part, and the overall approximation guarantee is thus given as $\operatorname* { m i n } _ { a } { \frac { 1 } { Q _ { a } } }$ .   

<html><body><table><tr><td>na</td><td>1</td><td>2</td><td>3</td></tr><tr><td>da</td><td>1</td><td>1.0642</td><td>≥4 1.0893 1.1461</td></tr><tr><td></td><td>0.25</td><td>≥ 0.2781</td><td>≥0.2896 ≥ 0.3178(1- 0.7681</td></tr></table></body></html>

Comparison to previous work. Our algorithm follows a primal-dual design and keeps a threshold for each part. Ene and Nguyen (2022) set thresholds depending on the marginal gains of all previously allocated items, even those that were already disposed. In contrast, we use a different scheme for setting the thresholds using linear combinations of the gains of only the items in the current solution. As such, our approach is related to Feldman et al. (2009) with the notable difference that we no longer use a convex combination of the gains, which is crucial for submodular objectives as discussed above. Our analysis is another significant departure from both prior works: The analysis of Feldman et al. (2009) strongly leverages the special structure of linear functions and does not apply to submodular objectives. Ene and Nguyen (2022) use a global analysis that is tailored to their specific threshold update scheme. In contrast, we use a different approach for updating the thresholds and analyze it via a novel local analysis outlined below. This makes our approach general and flexible, and it allows us to handle both monotone and non-monotone objectives as well as more general packing constraints. It further allows us to choose the coefficients that go into the thresholds, tailored to the specific budget in each part. Hence, we obtain better approximations in challenging settings when budgets are imbalanced. This was not considered in previous works but is important for applications such as ad allocation, which we also showcase experimentally in Table 3.

As is common in works on submodular optimization, we state the running time of our Algorithm 1 in terms of the number of function evaluations. We can easily see that this is $O ( | V | \cdot k )$ : we consider each item in $V$ exactly once and evaluate the $k$ -submodular function $k$ times per item. The algorithm only stores a single feasible solution in memory, and this dominates the space requirement. The space is therefore optimal, and the running time is the best-known for algorithms in our setting (Ene and Nguyen 2022). We obtain the following approximation guarantee.

Theorem 1. We make the following choices for the parameters $\{ d _ { a } \} _ { a \in [ k ] }$ . Let $d = 1 . 1 4 6 1$ , which is an approximate solution to the equation $e ^ { d } - d - 2 = 0$ . We set $d _ { a } = d$ if $n _ { a } > n _ { 0 } : = 3 ,$ , and we set $d _ { a }$ as shown in Table 2 if $n _ { a } \leq n _ { 0 }$ . We obtain the approximation guarantees shown in Table 2. Note that the approximation is at least 0.25 for any minimum budget and it tends $t o \ge 0 . 3 1 7 8$ as the minimum budget tends to infinity.

Overview of the analysis. We now provide an analysis overview for the approximation ratio of Algorithm 1. A complete analysis is in Section A.2 of the appendix. Analyses for other algorithms in this work follow the same proof framework, but require further non-trivial modifications.

We denote with superscript $\mathbf { \rho } ( t )$ all quantities of the algorithm at the end of iteration $t$ . We denote all quantities at the end of the stream without superscript. Let $\begin{array} { r } { T _ { a } ^ { \left( \dot { t } \right) } = \bigcup _ { i = 1 } ^ { t } S _ { a } ^ { ( i ) } } \end{array}$ be all items that were allocated to $a$ in the first $t$ i erations.

Lower bound on $f ( \mathbf { S } )$ . Our goal is to relate $f ( \mathbf { S } )$ to the optimum $f ( \mathbf { S } ^ { * } )$ . However, comparing both is difficult as there is no direct relationship between the allocation S created by our algorithm and the optimum solution $\mathbf { S } ^ { \ast }$ . What we can do is to relate both to marginal gains (weights) and thresholds used in the algorithm, and then leverage the algorithm’s structure to compare both. Using orthant submodularity, we can construct the following lower bound on the value of the solution S (Lemma 3):

$$
f ( \mathbf { S } ) \geq \sum _ { a } \sum _ { t \in S _ { a } } w _ { t , a } .
$$

Upper bound on $f ( \mathbf { S } ^ { * } )$ . An upper bound on the optimum value is harder to obtain, since our marginal gains are with respect to the current solution $\mathbf { S } ^ { ( t ) }$ , and it is unclear how to relate this to the optimum. For submodular functions $( k = 1 )$ , a common approach is to upper bound $f ( S ^ { * } )$ by $f ( S \cup S ^ { * } )$ and analyze the latter via the marginal gains. However, this strategy no longer works for $k$ -submodular functions since they are only defined on allocations where each item appears in at most one part. We thus consider a sequence of intermediate solutions $\mathbf { \bar { O } } ^ { ( t ) }$ that agree with $\mathbf { T } ^ { ( t ) }$ on items $\{ 1 , \ldots , t \}$ and with $\mathbf { S } ^ { \ast }$ on $\{ t + 1 , \ldots , | V | \}$ , and analyze the change in function value $f ( \mathbf { O } ^ { ( t - 1 ) } ) - f ( \mathbf { O } ^ { ( t ) } )$ for each iteration. With some additional care where we critically use the allocation choice of Algorithm 1, we obtain the following guarantee (Lemma 4):

$$
f ( \mathbf { S } ^ { \ast } ) \leq \sum _ { a } \left( \sum _ { t \in T _ { a } } \left( 2 w _ { t , a } - \beta _ { a } ^ { ( t - 1 ) } \right) + n _ { a } \beta _ { a } \right) .
$$

Due to Equations (1) and (2), we can compare terms on a per-part basis for each $a \in [ k ]$ , and provide a bound $Q _ { a }$ on the ratio between the two such that

$$
\sum _ { t \in T _ { a } } \left( 2 w _ { t , a } - \beta _ { a } ^ { ( t - 1 ) } \right) + n _ { a } \beta _ { a } \leq Q _ { a } \sum _ { t \in S _ { a } } w _ { t , a } .
$$

This gives us $f ( \mathbf { S } ^ { * } ) \leq Q f ( \mathbf { S } )$ where we try to make $Q : =$ $\operatorname* { m a x } _ { a \in [ k ] } Q _ { a }$ as small as possible. Our analysis yields

$$
\begin{array} { r } { Q _ { a } = \left( 1 + d _ { a } \right) \left( 1 + \frac { 1 } { \left( 1 + d _ { a } / n _ { a } \right) ^ { n _ { a } } - 1 } \right) } \end{array}
$$

for parameters $d _ { a }$ , which we can optimize according to the budgets as outlined below. First, however, we show how to obtain the per-part approximation ratios $Q _ { a }$ .

Deriving the approximation ratios $Q _ { a }$ . Note that the LHS of (3) has the weights $\{ w _ { t , a } : t \in T _ { a } \}$ of all of the items ever allocated to $a$ (including discarded items) and the thresholds. In contrast, the RHS of (3) has only the weights $\{ w _ { t , a } : t \in S _ { a } \}$ of the final solution. Thus we need to relate the weights of the discarded items and the thresholds to the items in the final solution. To this end, we use a primal potential tracking the lower bound (1) and a dual potential tracking the upper bound (2):

$$
P _ { t } : = \sum _ { i \in S _ { a } ^ { ( t ) } } w _ { i } , ~ D _ { t } : = \sum _ { i \in T _ { a } ^ { ( t ) } } \left( 2 w _ { a i } - \beta _ { a } ^ { ( i - 1 ) } \right) + n _ { a } \beta _ { a } ^ { ( t ) } .
$$

We interpret the dual $D _ { t }$ as follows: $2 w _ { a t } - \beta _ { a } ^ { ( t - 1 ) }$ is the cost of reallocating an item to the part chosen by the optimum solution, and we use $n _ { a } \beta _ { a } ^ { ( t ) }$ to account for items in $S _ { a } ^ { * }$ that have not arrived yet by paying the current threshold $\beta _ { a } ^ { ( t ) }$ for each of them. Our analysis relates the change in the dual to the change in the primal, in each iteration. If $t \notin T _ { a }$ , we experience no change in either primal nor dual. If $t \in T _ { a }$ , the change is

$$
\begin{array} { r l } & { \quad P _ { t } - P _ { t - 1 } = w _ { t , a } - \underset { i \in S _ { a } ^ { ( t - 1 ) } } { \operatorname* { m i n } } w _ { i , a } , } \\ & { \quad P _ { t } - P _ { t - 1 } = 2 w _ { t , a } - \beta _ { a } ^ { ( t - 1 ) } + n _ { a } \big ( \beta _ { a } ^ { ( t ) } - \beta _ { a } ^ { ( t - 1 ) } \big ) . } \end{array}
$$

To relate the two, we use several properties maintained by the algorithm: we only allocate the item if the discounted gain is non-negative (i.e., $w _ { t , a } \geq \beta _ { a } ^ { ( t - 1 ) } )$ and our threshold is a combination of the largest weights with exponential coefficients. We can then upper bound the change in thresholds $\beta _ { a } ^ { ( t ) } - \beta _ { a } ^ { ( t - 1 ) }$ (Lemma 6) using only the weights of the new item $w _ { t , a }$ and the disposed item $\mathrm { m i n } _ { i \in S _ { a } ^ { ( t - 1 ) } } w _ { i , a }$ , with appropriate coefficients. Setting $c _ { a }$ appropriately makes the two coefficients equal and gives us the desired comparison. This agrees with the intuition that $c _ { a }$ describes exactly how much additional gain we require from new items in order to account for the potential loss through the disposal, which is expressed in the dual potential. This definition results in the desired value of $Q _ { a }$ as stated in Equation 4.

Setting parameters $d _ { a }$ . It remains to choose the parameters $d _ { a }$ to optimize the approximation guarantee. In the large budget case, we can approximate $( 1 + \bar { d } _ { a } / n _ { a } ) ^ { n _ { a } } \approx \exp ( \bar { d _ { a } } )$ which does not depend on the budget. Thus we can use the same parameter $d$ for all parts and set it to the value that maximizes the approximation guarantee. In order to account for all budgets, including very small ones, we analyze the error incurred from approximating $( 1 + d _ { a } / n _ { a } ) ^ { n _ { a } }$ by $\exp ( d _ { a } )$ (Lemma 7) and derive appropriate choices $d _ { a }$ tailored to the budgets $n _ { a }$ . As a result, we can handle the challenging setting where budgets can be very different, and obtain approximations that improve with the budget.

# 3.2 Non-Monotone Objectives

The maximization of general (possibly non-monotone) $k$ - submodular functions is challenging even in the offline setting, where prior work does not achieve constant-factor approximation guarantees (Xiao et al. 2022). In this section we show how to use our framework along with fundamentally new techniques leveraging special properties of $k$ - submodular functions to obtain the first constant factor approximation guarantees for general objectives, even in the more difficult online and streaming setting. The problem is different from the monotone $k$ -submodular case as adding items with positive marginal gain may result in a decreased objective at the end. However, the core difficulty lies in the difference to non-monotone submodular maximization $( k \ = \ 1 )$ ) as approaches for this setting, such as subsampling and twice-greedy, do not have counterparts for nonmonotone $k$ -submodular maximization. This difference becomes apparent when comparing a solution created by our algorithm to the optimum solution: Even if the support of both solutions is identical, our solution can still have a lower objective due to misallocating items to the wrong parts.

Due to this difference, our approach is to consider the case $k = 1$ and $k \geq 2$ separately. For $k = 1$ , we use our algorithm for a partition matroid constraint (Section 3.3) and thus consider only the case $k \geq 2$ here. A key insight is that if the maximum budget $\operatorname* { m a x } _ { a } n _ { a }$ is not too large compared to the total budget $\sum _ { a } n _ { a }$ , we can use pairwise monotonicity in a delicate adaptation of Algorithm 1. We thus consider the regime when the maximum budget is not too large first. From this, we then derive an algorithm for all budgets.

Algorithm for $\begin{array} { r } { \operatorname* { m a x } _ { a } n _ { a } \le \frac { 1 } { 2 } \sum _ { a } n _ { a } } \end{array}$ . A serious complication of Algorithm 1 for non-monotone objectives it that we can no longer bound the difference in function value after re-allocating item $t$ according to the optimum solution. To hedge against the loss in objective due to misallocation, we also need to take the thresholds of all other parts into account (for more details, we refer the reader to the proof of Lemma 9 in the appendix), so we make a modification to the allocation: In each iteration $t$ , we choose the part that maximizes the following modified discounted gain:

$$
a \gets \arg \operatorname* { m a x } _ { a \in [ k ] } \Big \{ \Delta _ { t , a } f ( \mathbf { S } ^ { ( t - 1 ) } ) - \beta _ { a } ^ { ( t - 1 ) } - \operatorname* { m i n } _ { a ^ { \prime } \neq a } \beta _ { a ^ { \prime } } ^ { ( t - 1 ) } \Big \} .
$$

The full pseudocode and analysis can be found in Appendix B.1. We obtain:

Theorem 2. When setting the parameters $\{ d _ { a } \} _ { a \in [ k ] }$ as in Theorem $\jmath$ , the modified algorithm achieves an approximation guarantee that is $\textstyle { \frac { 1 } { 2 } }$ of the approximation in Theorem $\jmath$ .

Algorithm for All Budgets. If $\begin{array} { r } { \operatorname* { m a x } _ { a } n _ { a } > \frac { 1 } { 2 } \sum _ { a } n _ { a } } \end{array}$ , we can still obtain a constant-factor approximation (in expectation). Note that we either extract a lot of value from the part with maximum budget, or we can decrease the maximum budget and still obtain a good fraction of the original value. We mimic this idea by creating two solutions. For the first solution, we only allocate to the part with maximum budget while not exceeding the respective budget constraint. For the second solution, we solve the original problem, but reduce the budget of the maximum advertiser such that we can again apply Theorem 2. We select the better of the two solutions. This is only a streaming algorithm as we create multiple solutions, but we can also obtain an online algorithm by choosing a solution randomly. We defer a full description and analysis of this algorithm to Appendix B.2.

![](images/e4dad396f768c3ff60f3a762392d06250ee85c2b1bf19e4834988a4cb1f234db.jpg)  
Figure 1: Ad allocation on the iPinYou (two plots to the left) and Yahoo instance (two plots to the right). We report mean and standard deviation over all days in the datasets, while varying a uniform budget $n _ { a } = n$ for all $a \in [ k ]$ . Note that the online algorithms using modified parameter choices coincide with offline greedy on the Yahoo instance. Also, all algorithms except offline greedy use the same number of function evaluations. We indicate runs with the theoretical parameters (e.g. “Algorithm 1: Theory” is Algorithm 1 using the theoretically optimal parameters).

# 3.3 Additional Results

Our techniques can be applied in a unified way to obtain further results in the following settings.

Submodular maximization with a partition matroid (Appendix C). We close the gap between discrete and continuous methods which evaluate the multilinear extension and are therefore inefficient in practice (cf. Table 4). As Feldman, Karbasi, and Kazemi (2018), we subsample items to obtain a discrete algorithm, but improve upon their approximation guarantees by carefully choosing the subsampling probability in coordination with the coefficients $g _ { a } ( i )$ . Their algorithm also has the previously best approximation guarantees for $k = 1$ , and we even improve upon that.

$k$ -Submodular maximization with knapsack constraints (Appendix D.2). We extend our approach using the density $\rho _ { t , a } = \Delta _ { t , a } f ( \mathbf { S } ) / u _ { t , a }$ in place of the marginal gain $\Delta _ { t , a } f ( \mathbf { S } )$ . An immediate problem of having items of any size is that we are not able to fill up each constraint exactly. To obtain an efficient discrete algorithm using optimal space, we maintain an infeasible solution $\tilde { \bf S }$ that overflows by at most a single item per part. Our update rule for $\beta _ { a }$ becomes a careful generalization that accounts for irregular

item sizes and depends on $\tilde { \bf S }$

Common Constraints (Appendix D.3). Recall that for a common constraint, we have only a single constraint on the support, e.g. $| S _ { 1 } \cup \cdot \cdot \cdot \cup S _ { k } | \leq n$ or a single knapsack constraint. Our techniques also apply to this setting and we obtain improved online algorithms.

# 4 Experiments

We show the practicality of our algorithms for $k$ -submodular maximization on instances for ad allocation, influence maximization, and max-cut, exemplifying the applications mentioned in the introduction. Further results are in Appendix E. We demonstrate the performance of our algorithms, which almost close the gap to offline greedy algorithms in function value while retaining the efficiency of online algorithms such as the ones due to Ene and Nguyen (2022).

Instances. Here, we briefly discuss our experiments. We defer a detailed description to Appendix E.

Ad Allocation. We consider the problem of allocating ad impressions to $k$ advertisers (Mehta 2013). Ad impressions $t \in V$ arrive online and must be allocated immediately to advertisers $a \in [ k ]$ with budget constraints $| S _ { a } | \le n _ { a }$ . We use data from the iPinYou ad exchange (Zhang, Yuan, and Wang 2014) and a Yahoo dataset (Yahoo 2011). Our objective (total advertiser satisfaction) is a specific $k$ -submodular function which we detail in Appendix E. We also create an imbalanced instance on the iPinYou data by sampling advertiser budgets $n _ { a }$ uniformly from [10], with results in Table 3.

![](images/fcfae5bb4ac3e1a610af9908c22780d6809b1b43a14219123fa3178d2d5dafe1.jpg)  
Figure 2: Influence maximization with $k$ topics (top) and max- $k$ -cut on the Email instance (bottom). We vary a uniform budge $n _ { a } = n$ for $a \in [ k ]$ and report mean and standard deviation over 5 runs.

Table 3: Ad allocation on the iPinYou instance with imbalanced budgets. We report mean and standard deviation over 7 days. We use theoretical and modified parameter choices.   

<html><body><table><tr><td>Algorithm</td><td>Theory</td><td>Modified</td></tr><tr><td>Algorithm 1</td><td>7499.13 ± 68.22</td><td>10236.05 ± 220.22</td></tr><tr><td>(Ene and Nguyen 2022)</td><td>5698.33 ± 88.57</td><td>9681.85 ± 152.87</td></tr><tr><td>Offline Greedy</td><td>10427.58 ± 214.04</td><td></td></tr></table></body></html>

Influence Maximization with $k$ Topics and Sensor Placement with $k$ Measurements. We use the same experimental setup as Ene and Nguyen (2022) to create instances for monotone $k$ -submodular maximization. The results for influence maximization and sensor placement are in Figure 2 and Figure 3 of Appendix E, respectively.

Max- $k$ -Cut: The max- $k$ -cut problem asks, given a graph $\textit { G } ~ = ~ ( V , E )$ and cardinality constraints $n _ { 1 } , \ldots , n _ { k }$ to find $\textbf { S } \overset { \cdot } { \in } ( \overset { \cdot } { k } + 1 ) ^ { V }$ maximizing the total cut size defined as $\begin{array} { r c l } { f ( \mathbf { S } ) } & { \mathrel { \mathop : } = } & { \sum _ { a \in [ k ] } \delta _ { G } \bar { ( S _ { a } ) } } \end{array}$ where $\begin{array} { r l } { \delta _ { G } ( S ) } & { { } : = } \end{array}$ $| \{ \{ u , v \} \in E : u \in S , v \not \in S \} |$ . Note that $f$ is a general $k$ - submodular function since we use a single submodular and symmetric function for each part $\delta _ { G }$ . Our results on the Email network (Leskovec and Krevl 2014) are in Figure 2.

Algorithms. We use Algorithm 1 for the monotone instance ad allocation and Algorithm 2 for the general instance max- $k$ -cut. We use two parameter choices for the online algorithms: First, we set $\ { \dot { \{ \{ d _ { a } } \} }  _ { a \in [ k ] } , \ \{ c _ { a } \} _ { a \in [ k ] }$ to the optimal theoretical choice as the minimizer of $Q _ { a }$ in Lemma 5. Second, we modify these parameters by reducing each $c _ { a }$ to $\textstyle { \frac { 1 } { 4 } }$ of the the previous choice to make the algorithms less conservative. The problems we consider are NP-complete, so instead of reporting the approximation ratio, we compare our algorithms with the greedy algorithms of Ohsaka and Yoshida (2015a) for monotone and Xiao et al. (2022) for general objectives. Both greedy algorithms work in the offline setting and access elements in arbitrary order, as opposed to the more constrained streaming and online setting we consider. We implement both using lazy evaluations. We also run the algorithm of Ene and Nguyen (2022) on monotone instances. The theoretical and modified parameter choices coincide with the ones used in their experiments. Details about the machine we used are in Appendix E.

Discussion. Our algorithms with modified parameter choices are able to match or even outperform the objective value of offline greedy on the ad-allocation instances in Figure 1, with significantly less function evaluations. This is surprising as greedy is an offline algorithm, therefore less restricted than our streaming algorithms and thus expected to perform better. The instances on influence maximization and max- $k$ -cut (Figure 2) are more challenging, but we still almost match the objective value of offline greedy with about $8 \%$ less in objective. In the settings where the algorithms of Ene and Nguyen (2022) apply (i.e. the monotone instances in Figures 1 and 2 (top)), we recover their strong practical results. Table 3 shows that our algorithm also performs well if budgets are imbalanced and comes close to offline greedy. It outperforms the algorithm of Ene and Nguyen (2022) that does not adapt to the individual budgets in each part.