# Higher Order Structures For Graph Explanations

Akshit Sinha\*, Sreeram Vennam\*, Charu Sharma, Ponnurangam Kumaraguru

International Institute of Information Technology, Hyderabad akshit.sinha, sreeram.vennam @students.iiit.ac.in

# Abstract

Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations of graph-structured data, demonstrating remarkable performance across various tasks. Recognizing their importance, there has been extensive research focused on explaining GNN predictions, aiming to enhance their interpretability and trustworthiness. However, GNNs and their explainers face a notable challenge: graphs are primarily designed to model pair-wise relationships between nodes, which can make it tough to capture higher-order, multi-node interactions. This characteristic can pose difficulties for existing explainers in fully representing multi-node relationships. To address this gap, we present Framework For Higher-Order Representations In Graph Explanations (FORGE), a framework that enables graph explainers to capture such interactions by incorporating higher-order structures, resulting in more accurate and faithful explanations. Extensive evaluation shows that on average real-world datasets from the GraphXAI benchmark and synthetic datasets across various graph explainers, FORGE improves average explanation accuracy by $1 . 9 \mathrm { x }$ and $2 . 2 5 \mathrm { x }$ respectively. We perform ablation studies to confirm the importance of higher-order relations in improving explanations, while our scalability analysis demonstrates FORGE’s efficacy on large graphs.

# 1 Introduction

Graph Neural Networks (GNNs) (Kipf and Welling 2017) have become increasingly important in graph representation learning, as data in many real-world domains can be naturally modeled as graphs. GNNs have found applications in several sensitive fields, including information processing (Ying et al. 2018; Wang et al. 2019), criminal justice (Agarwal, Lakkaraju, and Zitnik 2021), molecular chemistry (Gilmer et al. 2017; Duvenaud et al. 2015), and bioinformatics (Zhang et al. 2021; Fout et al. 2017; Ragkousis 2022). In these sensitive domains, interpretability is crucial for ensuring transparent, justifiable, and ethical decisionmaking. As GNN usage expands, understanding their internal processes becomes vital for the effective and safe usage of these models in practical settings. To address these challenges, various graph explainers have been proposed. These graph explainers typically provide two types of explanations crucial for GNN prediction: (1) identification of subgraphs (Yuan et al. 2021; Schlichtkrull, Cao, and Titov 2022), and (2) determination of node features (Huang et al. 2020; Ying et al. 2019).

![](images/c98ef5d0c853847b66598fc469a3bf7bcebe7c49dda79c7260ac64477193d4d7.jpg)  
Figure 1: (a) Ground Truth for an example from BENZENE (Agarwal et al. 2023) (b) Explanation generated by GNNExplainer (Ying et al. 2019) (c) Explanation generated by using FORGE. Green nodes and edges signify the subgraph considered important for GNN prediction. By incorporating FORGE, we can capture important multi-node interactions, resulting in more accurate explanations.

However, graph explainers and GNNs face an inherent limitation in their ability to generate representations stemming from graphs’ ability to model only pair-wise interactions between nodes. This limitation has implications on the expressivity of GNNs, which is explored in detail by Xu et al. (2019). To address this, higher-order structures such as cell complexes (Hansen and Ghrist 2019) have been employed as they are a higher-order generalization of graphs, capable of modeling multi-node interactions. Neural networks designed for these higher-order structures (Bodnar et al. 2021; Ebli, Defferrard, and Spreemann 2020; Yang, Sala, and Bogdan 2022) have demonstrated significant performance improvements in graph learning tasks compared to traditional graph structures, and have been shown to be more expressive than traditional GNNs. Despite these advancements in higher-order graph representations, there exists a critical gap in the field of graph explainability: the potential of higher-order structures to enhance the interpretability of graph-based models remains unexplored.

In this work, we present Framework For Higher-Order Representations In Graph Explanations (FORGE), a novel, explainer-agnostic framework designed to enhance the capability of graph explainers by capturing multi-node interactions during the GNN learning process itself by internally representing the underlying graph data as a cell complex. To further refine the quality of explanations, as part of our framework we introduce Information Propagation algorithms that translate the explanations generated for cell complexes back to the underlying graph data, resulting in richer, more accurate, and faithful explanations with respect to the input data as well as the underlying GNN predictor. This is encapsulated by Figure 1, where our framework can capture the multi-node interactions of a benzene ring, which is the correct ground truth explanation. Without FORGE, the base explainer is unable to interpret these interactions, resulting in less accurate explanations.

The overall framework is described in Figure 2. We conduct extensive evaluations of FORGE on real-world datasets from the GraphXAI (Agarwal et al. 2023) benchmark, as well as on specially curated synthetic datasets. Our results demonstrate that incorporating FORGE consistently matches or improves the explanation accuracy and faithfulness of various graph explainers. To reaffirm our hypothesis, we perform rigorous ablations and find that each components of our framework contribute significantly to increased explainer performance. Additionally, our analysis of scalability reveals that FORGE efficiently handles dense, complex graph networks with only linear overhead in both time and space complexity, further emphasizing its practical applicability and computational efficiency.

# 2 Related Work

Higher-Order Representations of Graphs Hansen and Ghrist (2019) first explored relating higher-order structures to spectral graph theory. Following this, recent advancements in GNN architecture have explored the incorporation of higher-order topological structures, such as simplicial complexes (Ebli, Defferrard, and Spreemann 2020) and cell complexes (Bodnar et al. 2021), to successfully enhance the expressive power and representational capacity of these models. This line of work is motivated by the inherent limitations of traditional GNNs, particularly their constraints in capturing complex structural patterns (Xu et al. 2019) and effectively modeling long-range dependencies (Chen, Schulz, and Borgwardt 2024).

Explainability in GNNs The increasing adoption of GNNs in critical domains has resulted in significant research into methods for explaining their predictions. A comprehensive survey by Kakkad et al. (2023) provides an extensive overview of GNN explainability techniques.

Perturbation-based methods, such as GNNExplainer (Ying et al. 2019), PGExplainer (Luo et al. 2020), SubgraphX (Yuan et al. 2021), and GraphMask (Schlichtkrull, Cao, and Titov 2022), identify important subgraphs by systematically perturbing input graphs and analyzing the impact on model outputs. Gradient-based approaches, including Grad-CAM (Selvaraju et al. 2019) and Guided Backpropagation (Springenberg et al. 2015), leverage gradient information to assess the influence of input features on model predictions. Surrogate methods, exemplified by PGM-Explainer (Vu and Thai 2020) and GraphLIME (Huang et al. 2020), approximate complex GNN behavior with interpretable local models. Recent innovations have further expanded the landscape of GNN explainability. RGExplainer (Shan et al. 2021) employs reinforcement learning to construct explanatory subgraphs tailored to both model architecture and individual instances. MATE (Spinelli, Scardapane, and Uncini 2024) introduces a meta-learning framework that jointly optimizes GNN performance and explainability, resulting in intrinsically more interpretable representations. MotifExplainer (Yu and Gao 2022) introduces a motif-based approach for human interpretable explanations.

![](images/b668adaa91bda87c8f77c2282927a53c8eef78e5fc8cc7b5461602b9f3df2162.jpg)  
Figure 2: Visual representation of FORGE. The input graph is lifted to a cell complex, which is then given as input to (i) a GNN to train on, as well as to (ii) a graph explainer. Propagation is then done on the output cell complex explanation to map it to an explanation for the original graph. The green color on cells, nodes, and edges signifies the substructure considered important for GNN prediction (the explanation).

While MotifExplainer is most similar to our work, it is domain specific and suffers from $O ( n ^ { 3 } )$ complexity.

Although these approaches have significantly advanced GNN explainability, they primarily operate on traditional graph structures. Our proposed framework, FORGE, distinguishes itself by leveraging higher-order topological structures, specifically cell complexes, (1) to train GNNs to inherently create more interpretable representations during the learning process, and (2) utilize their topological properties to refine explanations post-hoc.

# 3 Cell Complexes

Graphs are powerful structures that excel in modeling relations between objects. However, they are limited to modeling pairwise relationships. To overcome this, graphs can be generalized to work in higher dimensions and model groupwise relationships. This generalization in algebraic topology is achieved by cell complexes (Hansen and Ghrist 2019).

A cell complex $X$ is constructed through a hierarchical assembly process. It begins with a set of vertices (0-cells), to which edges (1-cells) are attached by connecting these points with closed line segments, forming a graph. This structure is then expanded by attaching faces (2-cells) to corresponding 1-cells. While our focus remains within two dimensions, this process can be extended to higher dimensions1.

While our work does not go into detail about the algebraic topology ideas linked to cell complexes, it is important to provide some basic definitions (Hansen and Ghrist 2019) and notations (Table 1) that are essential to understand the framework we are proposing.

Table 1: Summary of notations used throughout the paper.   

<html><body><table><tr><td>Notation</td><td>Description</td></tr><tr><td>G</td><td>A general graph</td></tr><tr><td>V</td><td>Set of all vertices in a graph</td></tr><tr><td>E</td><td>Set of all edges in a graph</td></tr><tr><td>G(V,E)</td><td>A graph with vertices V and edges E</td></tr><tr><td>X</td><td>A general cell complex</td></tr><tr><td>X(p）</td><td>p-skeleton of a cell complex</td></tr><tr><td>C,c(p)</td><td>general cell, cell of dimension p</td></tr><tr><td>C(p)</td><td>A p-chain in a given cell complex</td></tr><tr><td>C</td><td>Set of all p-chains in a cell complex</td></tr><tr><td>Oc1,C2</td><td>A general boundary relation</td></tr><tr><td>£</td><td>Set of all boundary relations</td></tr><tr><td>X(C,Σ)</td><td>A cell complex defined by C and ∑</td></tr></table></body></html>

Definition 1 $\overset { \cdot } { p }$ -cell). A $p$ -cell $c ^ { ( p ) }$ in a cell complex refers to an element of dimension $p$ . In analogy to traditional graphs where we have vertices (0-dimensional) and edges (1-dimensional), cell complexes include these and extend to higher dimensions.

Definition 2 (boundary/coboundary). In a cell complex, a $p$ -cell $c ^ { ( p ) }$ is considered a face or boundary of a $( p + 1 )$ -cell $c ^ { ( p + 1 ) }$ if the set of points composing $c ^ { ( p ) }$ is a subset of those composing $c ^ { ( p + 1 ) }$ . Conversely, $c ^ { ( p + 1 ) }$ is referred to as the coface or coboundary of $c ^ { ( p ) }$ .

Definition 3 ( $\overset { \cdot } { p }$ -chain). In a given cell complex, a $p$ -chain $C ^ { ( { p } ) }$ is simply defined as the set of all $p$ -dimensional cells. The general set union of all such $C ^ { ( { p } ) }$ is denoted by $C$ .

Definition 4 ( $p$ -skeleton). The $p$ -skeleton of a cell complex $X$ is defined as the subcomplex $X ^ { ( p ) }$ consisting of cells of dimension at most $p$ . Using this definition, we see that $X ^ { ( 0 ) }$ is the set of all vertices and $X ^ { ( 1 ) }$ is the set of all vertices and edges which precisely make up the underlying graph.

Definition 5 (boundary relation). Within a cell complex, a boundary relation $\sigma$ is analogous to an edge in traditional graphs. It connects two cells, either of the same dimension (horizontal boundary relation) or different dimensions (vertical boundary relation). A horizontal boundary relation $\sigma _ { c _ { 1 } ^ { ( p ) } , c _ { 2 } ^ { ( p ) } }$ links two $p$ -cells that share a common boundary or coboundary, whereas vertical boundary relations σc(p),c(p+1) link a $p$ -cell to its corresponding boundaries or coboundaries. In this work, we further restrict the boundary relations to be undirected, implying σc1,c2 = σc2,c1 .

Using the definitions provided in this section, we can analogously represent a cell complex as $X ( C , \Sigma )$ , like we represent a graph as $G ( V , E )$ . The methodology we introduce to create a cell complex from a given graph is described in Section 4.

While cell complexes are adept at handling higher-order interactions, their conventional definition requires them to be closed under taking subsets, making them inefficient data structures for scalable computation (Yang, Sala, and Bogdan 2022). To overcome this, we present the following theorem and a subsequent modification in Equation (3) to achieve an efficient, restricted form of cell complexes that is linear in space complexity. The proof follows by construction and is deferred to the Appendix.

Theorem 3.1. For a graph $G ( V , E )$ with adjacency matrix $A$ having cycles of length at most $K$ , let $W _ { k }$ represent the number of closed walks of length $k$ which are not $k$ -cycles. Let $d e g ( v )$ represent the degree of a node $v$ in the graph. The corresponding cell complex $X$ will have cells $C$ and boundary relations $\Sigma$ such that

$$
| C | = | V | + | E | + \sum _ { k = 3 } ^ { K } { \frac { 1 } { 2 k } } [ t r ( A ^ { ( k ) } ) - W _ { k } ]
$$

$$
| \Sigma | \geq 3 | E | + \frac { 1 } { 2 } \sum _ { k = 3 } ^ { K } [ t r ( A ^ { ( k ) } ) - W _ { k } ] + \sum _ { v \in V } { \binom { d e g ( v ) } { 2 } }
$$

This theorem provides us with two important bounds. Equation (1) shows that the number of cells in cell complex $X$ grows linearly with the number of vertices and edges in the graph $G$ . Equation (2) shows that asymptotically, the size of $\Sigma$ grows quadratically with $| E |$ , due to the last term in the equation. This term is precisely the number of boundary relations that are present between two edges in the graph, which are represented by $\sigma _ { c _ { 1 } ^ { ( 1 ) } , c _ { 2 } ^ { ( 1 ) } } .$ To preserve the scalability and complexity of FORGE, we further restrict how we create cell complexes from graphs, and drop all boundary relations of the form $\sigma _ { c _ { 1 } ^ { ( p ) } , c _ { 2 } ^ { ( p ) } }$ where $p \geq 1$ from our construction of cell complexes. Doing this reduces Equation (2) to:

![](images/aba97a11b1f2d455b853422220913cac3872e42fc5def54a5bdb3a0912512516.jpg)  
Figure 3: The variation of $\left| \Sigma \right|$ for a conventional cell complex and our proposed reduced cell complex with increasing $| E |$ (Theorem 3.1) (left) and increasing $| V |$ (right), showing that our proposed variation is more space efficient.

$$
| \Sigma | = 3 | E | + \frac { 1 } { 2 } \sum _ { k = 3 } ^ { K } [ t r ( A ^ { ( k ) } ) - W _ { k } ]
$$

This operation ensures that $| \Sigma |$ increases linearly with $| E |$ , making cell complexes much more scalable. We empirically validate these results on large random graphs generated using the Erd˝os-Re´nyi method (Erdos, Re´nyi et al. 1960) and present the results in Figure 3.

Throughout the rest of this text, we use the term cell complexes to refer to these restricted/reduced cell complexes, unless stated otherwise.

# 4 Proposed Approach Lifting Graphs to Cell Complexes

In this section, we present the lifting algorithm we introduce to create a cell complex $X$ from its underlying graph $G$ . Using the lifting algorithm on a given graph $G ( V , E )$ , we construct $C ^ { ( 0 ) }$ , ${ \cal C } ^ { ( 1 ) }$ , and $C ^ { ( 2 ) }$ from $V$ , $E$ , and cycles present in the graph respectively, and add the corresponding $\Sigma$ to create $X ( C , \Sigma )$ . The lifting algorithm is described in Algorithm 1. For any given graph, the time complexity of the lifting algorithm is bounded by the time complexity of finding cycles in a graph2. Further details on the algorithm and how cell features are created to perform message passing are deferred to the Appendix.

After lifting the graph to its corresponding cell complex, the next step is to train a GNN on cell complexes. Because

# Algorithm 1: Lifting algorithm

Input: $G ( V , E )$   
Output: $\dot { X } ( C , \Sigma )$   
1: $C ^ { ( 0 ) } \gets \phi , C ^ { ( 1 ) } \gets \phi , C ^ { ( 2 ) } \gets \phi , \Sigma \gets \phi$   
2: for $e _ { u , v } \in E$ do   
3: Add $u , v$ to $C ^ { ( 0 ) }$   
4: Add $e _ { u , v }$ to $C ^ { ( 1 ) }$   
5: Add $\sigma _ { u , v }$ , $\sigma _ { u , e _ { u , v } }$ , $\sigma _ { v , e _ { u , v } }$ to $\Sigma$   
6: end for   
7: for $c \in \mathrm { S e t }$ of Cycles in $G$ do   
8: Add c to C(2)   
9: for $e _ { u , v } \in c$ do   
10: Add $\sigma _ { e _ { u , v } , c }$ to $\Sigma$   
11: end for   
12: end for   
13: $C  C ^ { ( 0 ) } \cup C ^ { ( 1 ) } \cup C ^ { ( 2 ) }$   
14: return $X ( C , \Sigma )$

they are analogous to graphs, we can provide $X$ as an input to any GNN, and obtain a trained model $\mathcal { F }$ which will generate representations for cells C.

# Generating Explanations for Cell Complexes

While graph explainers use diverse methods to generate explanations for GNN predictions, they can be abstracted as a function that takes in the trained GNN model and graph data and outputs a soft node or edge mask with values between 0 and 1 signifying the importance of each node and edge identified as the explanation. For simplicity, we continue the discussion by defining the explanation as a node mask, an analogous approach can be followed for an edge mask. Formally, let $\mathcal { M }$ be an explanation mask, then the output of a graph explainer $\mathcal { E }$ can be represented as

$$
\mathcal { M } = \mathcal { E } ( \mathcal { F } , G )
$$

Graph explainers generate explanations on the computation graph of a node, which is the node’s $k$ -hop neighborhood for a $k$ -layer GNN. Transitioning from traditional computation graphs, our approach utilizes cell complexes as the input to the function. This shift allows the integration of more complex structural data into the computational framework, through the introduction of vertical message passing happening through vertical boundary relations (Section 3) in the cell complex, in addition to the already existing (horizontal) message passing, shown in Figure 4. Consequently, the learned explanation mask is adapted to represent the computational complex, incorporating both the connectivity and hierarchical structure inherent in cell complexes. Formally, this can be represented by

$$
\mathcal { M } _ { X } = \mathcal { E } ( \mathcal { F } , X )
$$

By using a cell complex $X ( C , \Sigma )$ as input, the explainer outputs an explanation mask $\mathcal { M } _ { X }$ , which is an intermediate explanation mask over $C$ , indicating which cells are important for the model prediction.

![](images/b01c847c9b43c831ab62ecc7fd8de0603e96c36ed1bdef83498bb7f81921ec80.jpg)  
Figure 4: Example of a computation cell complex. The figure on the left shows 2-hop horizontal message passing, while the figure on the right represents 2-hop vertical message passing, introduced by FORGE.

# Information Propagation

With the explainer generating an explanation mask $\mathcal { M } _ { X }$ for the cell complex $X$ , we must propagate this information from the higher-order structures back to the base graph structure to produce the final explanation for $G$ . We term this process information propagation, as it involves transferring the learned importance values from the cell complex to the original domain.

We introduce multiple algorithms to perform information propagation, namely, (1) Hierarchical Propagation, (2) Direct Propagation, (3) Entropy Propagation, and (4) Activation Propagation. For brevity, we expand on (1) in detail and defer the description of other algorithms and optimizations to the Appendix. Hierarchical Propagation is described in Algorithm 2.

# Algorithm 2: Hierarchical Propagation

Input: $\mathcal { M } _ { X }$   
Parameters: $\alpha _ { c }$ , $\alpha _ { e }$   
Output: $\mathcal { M }$   
1: for $c ^ { ( 2 ) } \in C ^ { ( 2 ) }$ do   
2: for all $\sigma _ { c ^ { ( 1 ) } , c ^ { ( 2 ) } }$ containing $c ^ { ( 2 ) }$ do   
3: $\mathcal { M } _ { X } [ c ^ { ( 1 ) } ]  \mathcal { M } _ { X } [ c ^ { ( 1 ) } ] + ( \mathcal { M } _ { X } [ c ^ { ( 2 ) } ] - 0 . 5 ) \times \alpha _ { c }$   
4: end for   
5: end for   
6: for $c ^ { ( 1 ) } \in C ^ { ( 1 ) }$ do   
7: for all $\sigma _ { c ^ { ( 0 ) } , c ^ { ( 1 ) } }$ containing $c ^ { ( 1 ) }$ do   
8: $\mathcal { M } _ { X } [ c ^ { ( 0 ) } ]  \mathcal { M } _ { X } [ c ^ { ( 0 ) } ] + ( \mathcal { M } _ { X } [ c ^ { ( 1 ) } ] - 0 . 5 ) \times \alpha _ { e }$   
9: end for   
10: end for   
11: $\mathcal { M }  \mathcal { M } _ { X } [ C ^ { ( 0 ) } ]$   
12: return $\mathcal { M }$

The main idea behind Hierarchical Propagation is to aggregate the explanations of 2-cells with their 1-cell boundaries, then again aggregate explanations of 1-cells with their 0-cell boundaries. These 0-cells represent the underlying 0- skeleton of $X$ , which as mentioned in Section 3 is precisely the set of nodes $V$ of $G ( V , E )$ . The goal is to utilize cell explanations in a way that polarises important and unimportant nodes in the final explanation mask $\mathcal { M }$ . This is done in Algorithm 2 in steps 3 and 8, where we subtract 0.5 from the explanation masks of higher order cells to quantify how positively or negatively it should influence the overall explanation. They are further multiplied by parameters $\alpha _ { e }$ and $\alpha _ { c }$ , which are introduced to have fine-grained control over how much 1-cells and 2-cells, respectively, contribute to the graph explanation. By precomputing the required boundary relations, the time complexity of Information Propagation is reduced to $\mathcal { O } ( | E | )$ .

All the steps to obtain explanation $\mathcal { M }$ for a graph $G$ and trained model $\mathcal { F }$ comprising FORGE can be summarised formally using the following equations:

$$
\mathcal { M } = \mathrm { F O R G E } ( \mathcal { E } , \mathcal { F } , G )
$$

$$
\operatorname { F O R G E } ( \mathcal { E } , \mathcal { F } , G ) = \operatorname { P R O P } ( \mathcal { E } ( \mathcal { F } , \operatorname { L I F T } ( G ) ) )
$$

Where LIFT is Algorithm $1 , { \mathcal { E } }$ is any graph explainer, and PROP is any information propagation algorithm.

# 5 Experimental Settings

# Datasets

We evaluate FORGE on both real-world and synthetic datasets for a comprehensive evaluation across diverse conditions. We take real-world datasets from the graph explainability benchmark, GraphXAI (Agarwal et al. 2023), which includes Benzene, Mutagenicity, Alkyl Carbonyl, and Fluoride Carbonyl. For synthetic datasets, we generate random graphs with a distinct subgraph structure, known as a motif, which defines the ground truth for the graph. The task involves differentiating between two different motifs. Motifs we test include Bull, Square (4-cycle), Hexagon (6-cycle), Wheel, House, and Cube (Figure 5). The synthetic datasets are referred to as Motif1/Motif2 in the text based on the motifs present. Specific details about dataset generation can be found in the Appendix.

![](images/a4c8d7a3153f12615d42e2db32204694a97515ab8a86525d6861f30758b8b444.jpg)  
Figure 5: Different motifs used to generate synthetic graphs: (left to right) Bull, Wheel, Cube, and House.

# Evaluation Criteria

Graph explainer methods produce edge and node masks which represents the most important subgraph that resulted in a model prediction. To evaluate the correctness of this mask, we compare it to the ground truth by adopting two metrics from Agarwal et al. (2023), Graph Explanation Accuracy (GEA) and Graph Explanation Faithfulness (GEF).

Table 2: Graph Explanation Accuracy (GEA) ( ) scores for baseline explainers $\mathbf { \left( B \right) }$ against their FORGE variants $( \mathbf { F } )$ across all datasets, with FORGE improving performance across various baselines. The best result for each dataset is highlighted in bold. Underlined values represent the better result between a base explainer and its FORGE variant.   

<html><body><table><tr><td></td><td colspan="2">GNNExplainer</td><td colspan="2">GraphMask</td><td colspan="2">GradExplainer</td><td colspan="2">PGMExplainer</td><td colspan="2">SubgraphX</td><td rowspan="2">Random</td></tr><tr><td>Datasets</td><td>B</td><td>F</td><td>B</td><td>F</td><td>B</td><td>F</td><td>B</td><td>F</td><td>B</td><td>F</td></tr><tr><td>Benzene</td><td>0.456</td><td>0.772</td><td>0.276</td><td>0.378</td><td>0.167</td><td>0.353</td><td>0.109</td><td>0.198</td><td>0.450</td><td>0.614</td><td>0.194</td></tr><tr><td>AlkCarb</td><td>0.054</td><td>0.130</td><td>0.048</td><td>0.055</td><td>0.114</td><td>0.217</td><td>0.095</td><td>0.304</td><td>0.002</td><td>0.011</td><td>0.050</td></tr><tr><td>FluoCarb</td><td>0.207</td><td>0.441</td><td>0.077</td><td>0.230</td><td>0.233</td><td>0.439</td><td>0.115</td><td>0.300</td><td>0.079</td><td>0.095</td><td>0.154</td></tr><tr><td>Mutag</td><td>0.380</td><td>0.339</td><td>0.127</td><td>0.163</td><td>0.172</td><td>0.218</td><td>0.114</td><td>0.233</td><td>0.079</td><td>0.095</td><td>0.112</td></tr><tr><td>Bull/Square</td><td>0.126</td><td>0.433</td><td>0.082</td><td>0.241</td><td>0.179</td><td>0.298</td><td>0.080</td><td>0.150</td><td>0.355</td><td>0.447</td><td>0.145</td></tr><tr><td>House/Hex</td><td>0.107</td><td>0.478</td><td>0.115</td><td>0.271</td><td>0.199</td><td>0.410</td><td>0.088</td><td>0.158</td><td>0.346</td><td>0.617</td><td>0.165</td></tr><tr><td>Wheel/House</td><td>0.102</td><td>0.361</td><td>0.233</td><td>0.378</td><td>0.169</td><td>0.426</td><td>0.083</td><td>0.195</td><td>0.246</td><td>0.549</td><td>0.194</td></tr><tr><td>Cube/Wheel</td><td>0.114</td><td>0.339</td><td>0.183</td><td>0.402</td><td>0.119</td><td>0.494</td><td>0.091</td><td>0.178</td><td>0.397</td><td>0.489</td><td>0.221</td></tr></table></body></html>

GEA uses Jaccard Index (Taha and Hanbury 2015) to quantify explanation accuracy, and GEF measures how faithful the explanations are to the underlying GNN predictor using KL divergence (Kullback and Leibler 1951). Further details about the metrics are present in the Appendix.

# Baselines

We select a range of established explainer methods as baselines to evaluate FORGE. For perturbation methods, we select GNNExplainer (Ying et al. 2019), GraphMask (Schlichtkrull, Cao, and Titov 2022), and SubgraphX (Yuan et al. 2021). For surrogate methods, we evaluate PGMxplainer (Vu and Thai 2020), and for gradient-based methods, we select GradExplainer (Selvaraju et al. 2019). We use a random explainer as a naive baseline adapted from Agarwal et al. (2023).

# 6 Results

Table 2 presents the GEA scores for various explainers with and without our framework. The reported results are averaged over 10 different seeds, with standard deviations reported in the Appendix. Our experiments reveal several key insights into the performance of different GNN explainers across various datasets. FORGE consistently improves existing explainers across various datasets with improvements up to $31 5 \%$ (FORGE-enhanced GradExplainer on Cube/Wheel), except Mutag for GNNExplainer. The GEF scores are presented in Table 3, reinforcing the capabilities of our framework in creating explanations that are comparable or more faithful to the underlying GNN.

Our results reveal substantial variability in explainer effectiveness depending on the graph types and tasks. No single method consistently outperforms others across all datasets, suggesting the importance of choosing explainers tailored to specific problem domains. Perturbation-based methods benefit the most from our framework, with FORGE applied to GNNExplainer generally delivering the best performance on real-world datasets, while FORGE on SubgraphX excels in synthetic datasets. Surrogate and gradientbased methods also show notable improvements across all datasets when enhanced with FORGE. Interestingly, some explainers in their default form perform worse than the Random baseline, a finding supported by the GraphXAI benchmark. However, after applying FORGE, all explainers consistently surpass the Random baseline, achieving significantly improved performance.

Table 3: Graph Explanation Faithfulness (GEF) Scores for the AlkaneCarbonyl Dataset. FORGE-enhanced explainers generate explanations that are comparable or more faithful to the underlying GNN. Values in bold indicate best performance.   

<html><body><table><tr><td>Explainer</td><td colspan="2">GEF(↓)</td></tr><tr><td>GNNExplainer</td><td>B 0.189±0.04</td><td>F 0.083±0.02</td></tr><tr><td>GraphMask</td><td></td><td></td></tr><tr><td>GradExplainer</td><td>0.051±0.03</td><td>0.028±0.02</td></tr><tr><td>PGMExplainer</td><td>0.389±0.13 0.204±0.05</td><td>0.078±0.05</td></tr><tr><td>SubgraphX</td><td></td><td>0.234±0.03</td></tr><tr><td>Random</td><td>0.008±0.01 0.124±0.04</td><td>1 0.007±0.00</td></tr></table></body></html>

For reproducibility of our results, all implementation details are contained in the Appendix. Further experiments and additional results on all datasets for presented experiments are also presented in the Appendix.

# 7 Ablation Studies

# FORGE Components

FORGE comprises two key components: Lifting and Information Propagation. To evaluate their contributions, we compare FORGE’s performance against Base-LIFT (a version that applies regular explainers on lifted graphs), FORGE-LIFT (a version of FORGE that only performs lifting), and Base (which has neither Lifting nor Propagation). Figure 6 demonstrates that FORGE-LIFT, by itself, consistently outperforms the base explainers across all four methods. While Base-LIFT performs better than the baseline, we see that training the underlying GNN on the lifted graphs is important to achieve further performance gains. This improvement highlights the significant impact of the

![](images/fb3f9ef14117707e9548cefb109b5cd6ad5439080c17aed062d4c97e6bd03eb8.jpg)  
Figure 6: Results of ablations on FORGE Components for (a) GNNExplainer, (b) GraphMaskExplainer, (c) GradExplainer, (d) PGMExplainer on Synth-Wheel/Cube dataset. Both Lifting and Information Propagation contribute significantly to an increase in explanation accuracy.

Lifting component alone in generating more accurate explanations. Furthermore, when we incorporate Information Propagation to create the full FORGE framework, we observe an additional performance boost on top of Lifting. This enhancement is particularly noticeable in GNNExplainer and GradExplainer (subfigures (a) and (c)).

Interestingly, the impact of Information Propagation appears to vary across different explainers. For instance, its effect seems more pronounced in GNNExplainer and GradExplainer compared to GraphMaskExplainer and PGMExplainer. This variation suggests that the benefits of our proposed propagation strategies may depend on the underlying explanation mechanism. These findings collectively demonstrate that both Lifting and Information Propagation are crucial components of FORGE, each contributing significantly to the framework’s overall performance.

# Information Propagation Algorithms

Figure 7 presents our second ablation study, comparing four information propagation algorithms across four different datasets for all explainers, reporting the average GEA. This analysis reveals two key insights: (a) Algorithm Performance Variability: the effectiveness of propagation methods varies significantly across datasets, indicating that no single algorithm consistently outperforms the others in all scenarios, and (b) Dataset Dependency: the optimal choice of propagation algorithm seems to be heavily influenced by the specific dataset being analyzed. This suggests that the graph structure and data properties play an important role in determining the most effective propagation method.

![](images/463d1d40e6e9ee82616c54499bd2e7f862f7120ee8c9b16933fc23d99109a3b0.jpg)  
Figure 7: Ablation results for different propagation methods on various datasets, for all baseline explainers. The y-axis represents the average absolute increase in GEA over base explainers. (A) Hierarchical Prop, (B) Direct Prop, (C) Activation Prop, and (D) Entropy Prop.

While performance varies, Hierarchical and Direct Propagation methods tend to perform well across most datasets, suggesting they may be more versatile. Activation Propagation shows high effectiveness in certain cases, particularly noteworthy in the AlkaneCarbonyl dataset.

The findings from Figure 7 emphasize the importance of carefully selecting the propagation method based on the specific dataset and graph structure. They also highlight the need for adaptive or hybrid approaches that can leverage the strengths of different propagation algorithms depending on the context.

# 8 Conclusion and Future Work

We introduce FORGE, a novel framework that enhances GNN explainability by leveraging cell complexes. Our framework employs a novel lifting algorithm to convert graphs to cell complexes. Furthermore, we introduce information propagation algorithms to create more interpretable internal data representations. Our extensive evaluations demonstrate that FORGE consistently enhances explanation accuracy and faithfulness across both real-world and synthetic datasets. Future work could explore adaptive propagation approaches and investigate FORGE’s applicability to diverse graph learning tasks. We hope this work motivates further research into applying higher-order structures for enhanced interpretability.