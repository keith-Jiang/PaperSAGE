# Federated Unlearning with Gradient Descent and Conflict Mitigation

Zibin $\mathbf { P a n } ^ { 1 }$ , Zhichao Wang1, Chi Li1,5, Kaiyan Zheng4, Boqi Wang1, Xiaoying Tang\*1,2,3, Junhua Zhao∗1,2

1The School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China 2The Shenzhen Institute of Artificial Intelligence and Robotics for Society 3The Guangdong Provincial Key Laboratory of Future Networks of Intelligence 4Department of Mathematics and Department of Statistics, University of Michigan 5Shenzhen Research Institute of Big Data zibinpan $@$ link.cuhk.edu.cn, zhichaowang $@$ link.cuhk.edu.cn, chili $@$ link.cuhk.edu.cn, kaiyanz $@$ umich.edu, boqiwang $@$ link.cuhk.edu.cn, tangxiaoying $@$ cuhk.edu.cn, zhaojunhua $@$ cuhk.edu.cn

# Abstract

Federated Learning (FL) has received much attention in recent years. However, although clients are not required to share their data in FL, the global model itself can implicitly remember clients’ local data. Therefore, it’s necessary to effectively remove the target client’s data from the FL global model to ease the risk of privacy leakage and implement “the right to be forgotten”. Federated Unlearning (FU) has been considered a promising way to remove data without full retraining. But the model utility easily suffers significant reduction during unlearning due to the gradient conflicts. Furthermore, when conducting the post-training to recover the model utility, the model is prone to move back and revert what has already been unlearned. To address these issues, we propose Federated Unlearning with Orthogonal Steepest Descent (FedOSD). We first design an unlearning Cross-Entropy loss to overcome the convergence issue of the gradient ascent. A steepest descent direction for unlearning is then calculated in the condition of being non-conflicting with other clients’ gradients and closest to the target client’s gradient. This benefits to efficiently unlearn and mitigate the model utility reduction. After unlearning, we recover the model utility by maintaining the achievement of unlearning. Finally, extensive experiments in several FL scenarios verify that FedOSD outperforms the SOTA FU algorithms in terms of unlearning and model utility.

# Code — https://github.com/zibinpan/FedOSD

# 1 Introduction

Federated Learning (FL) has increasingly gained popularity as a machine learning paradigm in recent years (McMahan et al. 2017). It allows clients to cooperatively train a global model without sharing their local data, which helps address data island and privacy issues (Yu et al. 2022). But previous studies demonstrate that clients’ local training data is inherently embedded in the parameter distribution of the models trained on it (De and Pedersen 2021; Zhao et al. 2023). Therefore, in light of privacy, security, and legislation issues, it’s necessary to remove clients’ training data from the trained model (Zhang et al. 2023), especially when clients opt to withdraw from FL. This is known as the right to be forgotten (RTBF) (Liu et al. 2021), which is enacted by privacy regulations such as the General Data Protection Regulation (GDPR) (Voigt and Von Bussche 2017) and the California Consumer Privacy Act (CCPA) (Harding et al. 2019).

![](images/e78620b4d07589f97c580f77923c00a5ad3f024bc57f26e3dc83d933b930e6f0.jpg)  
Figure 1: A demo of three clients. $g _ { 1 } , g _ { 2 } , g _ { 3 }$ represent the gradient of clients. $d$ denotes the update direction for unlearning client 3, which is conflicting with $g _ { 1 }$ and $g _ { 2 }$ , i.e., $g _ { 1 } \cdot d < 0$ and $g _ { 2 } \cdot d < 0$ . $d _ { F e d O S D }$ represents the direction obtained by FedOSD, which doesn’t conflict with $g _ { 1 }$ and $g _ { 2 }$ .

A naive way to achieve this goal is to retrain the FL model. But it brings large computation and communication costs (Liu et al. 2023). In contrast, unlearning is a more efficient way, which has been well studied in centralized machine learning (Bourtoule et al. 2021). Inspired by it, Federated Unlearning (FU) has emerged, aiming to remove data from a trained FL model while trying to maintain model utility.

In this context, numerous FU techniques have been proposed. Federaser (Liu et al. 2021) leverages the norms of historical local updates in the previous FL training to accelerate retraining. FedKdu (Wu, Zhu, and Mitra 2022) and FedRecovery (Zhang et al. 2023) utilize the historical gradients to calibrate the model to erase the training data of the target client (i.e., the client that requests for unlearning). However, these methods require clients to continuously record historical information during FL training (Yang and Zhao 2023). Moreover, (Zhao et al. 2023) propose MoDe to unlearn the target client’s data by momentum degradation, but it requires simultaneously retraining a model for updating the unlearning model, which brings additional communication costs.

Among prior studies, Gradient Ascent (GA) is considered a viable and efficient method for FU (Liu et al. 2023), which formulates unlearning as the inverse process of learning and takes the inverse of the loss function to reduce the model performance on the target client. It can effectively achieve the unlearning goal in few communication rounds while not bringing extra storage costs (Halimi et al. 2022). However, we observe that there exist the following three primary challenges when performing GA in FU.

Challenge 1: Gradient explosion. Gradient explosion is a significant challenge for GA-based federated unlearning, necessitating a substantial reliance on experimental hyperparameter tuning. This is because the loss function generally has no upper bound (see Fig. 3(a)). Consequently, executing GA to unlearn results in gradient explosion and cannot converge. We delve further into this in Section 3.1. To this end, (Halimi et al. 2022) project model parameters to an $L _ { 2 }$ -norm ball of radius $\delta$ . But it requires experimentally tuning $\delta$ .

Challenge 2: Model utility degradation. Directly applying GA to unlearn would inevitably destroy the model utility (Yang and Zhao 2023), even leading to catastrophic forgetting (Liu et al. 2023). Specifically, the model performance on remaining clients (i.e., those that do not require unlearning) would decrease heavily. One direct cause is the gradient conflict (Pan et al. 2023), where the model update direction for unlearning a client conflicts with those of the remaining clients, directly leading to a reduction in model utility. Fig. 1 illustrates an example in which client 3 requests unlearning, but the model update direction conflicts with the gradients of client 1 and client 2. Consequently, the updated model would exhibit diminished performance on client 1 and 2.

Challenge 3: Model reverting issue in post-training. After unlearning, post-training is often conducted, where the target client leaves and the remaining clients continually train the FL global model cooperatively to recover the model utility that was reduced in the previous unlearning (Halimi et al. 2022; Wu, Zhu, and Mitra 2022). However, we observe that during this stage, the model tends to revert to its original state, resulting in the recovery of previously forgotten information and thus losing the achievement of unlearning. This issue is further explored in Section 3.3.

To handle the aforementioned challenges, we propose the Federated Unlearning with Orthogonal Steepest Descent algorithm (FedOSD). Specifically, to handle the gradient explosion inherent in GA, we modify the Cross-Entropy loss to an unlearning version and employ the gradient descent, rather than GA, to achieve the unlearning goal. Subsequently, an orthogonal steepest descent direction that avoids conflicts with retained clients’ gradients is calculated to better unlearn the target client while mitigating the model utility reduction. In post-training, we introduce a gradient projection strategy to prevent the model from reverting to its original state, thereby enabling the recovery of model utility without compromising the unlearning achievement.

Our contributions are summarized as follows:

1. We introduce an Unlearning Cross-Entropy loss that can overcome the convergence issue of Gradient Ascent.

2. We propose FedOSD that establishes an orthogonal steepest descent direction to accelerate the unlearning process while mitigating the model utility reduction.   
3. We design a gradient projection strategy in the posttraining stage to prevent the model from reverting to its original state for better recovering the model utility.   
4. We implement extensive experiments on multiple FL scenarios, validating that FedOSD outperforms the SOTA FL unlearning approaches in both unlearning performance and the model utility.

# 2 Background & Related Work

# 2.1 Federated Learning (FL)

The traditional $\mathrm { F L }$ trains a global model $\omega$ cooperatively by $m$ clients, which aims to minimize the weighted average of their local objectives (Li et al. 2020): $\begin{array} { r } { \operatorname* { m i n } _ { \omega } \sum _ { i = 1 } ^ { m } p _ { i } L _ { i } ^ { \setminus } ( \omega ) } \end{array}$ , where $p _ { i } { \geq } 0 , \textstyle { \sum } _ { i = 1 } ^ { m } p _ { i } { = } 1$ . $L _ { i }$ is the local objective of client $i$ , training data with $N _ { i }$ samples: $\begin{array} { r } { \bar { L _ { i } } ( \omega ^ { t } ) { = } \sum _ { j { = 1 } } ^ { N _ { i } } \frac { 1 } { N _ { i } } L _ { i _ { j } } ( \omega ^ { t } ) } \end{array}$ $L _ { i _ { j } }$ is the loss on sample $j$ , which is obtained by a specific loss function such as Cross-Entropy (CE) loss:

$$
L _ { C E } = - \sum _ { c = 1 } ^ { C } y _ { o , c } \cdot l o g ( p _ { o , c } ) ,
$$

where $C$ denotes the number of classes. $y _ { o , c }$ is the binary indicator (0 or 1) if class label $c$ is the correct classification for observation $o$ , i.e., the element of the one-hot encoding of sample $j$ ’s label. $p _ { o , c }$ represents the predicted probability observation $o$ that is of class $c$ , which is the $o ^ { t h }$ element of the softmax result of the model output.

# 2.2 Federated Unlearning

Federated Unlearning (FU) aims to erase the target training data learned by the FL global model, while mitigating the negative impact on the model performance (e.g., accuracy or local objective). Recognized as a promising way to protect ‘the Right to be Forgotten’ of clients, FU can also counteract the impact of data poisoning attacks to enhance the security (Yang and Zhao 2023; Liu et al. 2023).

FU has garnered increasing interest in recent years. (1) Some previous studies have leveraged the historical information of FL training to ease the target client’s training data, such as FedEraser (Liu et al. 2021), FedKdu (Wu, Zhu, and Mitra 2022), FedRecovery (Zhang et al. 2023), etc. (2) Besides, (Zhao et al. 2023) adopt momentum degradation to FU. (3) (Su and Li 2023) use clustering and (4) (Ye et al. 2024) employ distillation to unlearn. (5) A significant approach related to our work is Gradient Ascent, which utilizes the target client’s gradients for unlearning (Halimi et al. 2022; Wu et al. 2022).

Based on the types of client data that need to be forgotten, Federated Unlearning can be categorized into sample unlearning and client unlearning (Liu et al. 2023). We focus on client unlearning in this paper for two reasons. First, we can make a fair comparison with previous record-based FU methods such as FedEraser and FedRecovery. Since they rely on pre-recording information like model gradients on

Client 1 Client (a) FL Training Server g1 g2 g Aggregation Data   
Client 2 Loss CE Local Training   
Client Broadcasts Model Global Model   
Requests Unlearning (b) Unlearning   
Client UCE Loss Local Broadcasts Training wt+1=w²+ndt   
Remaining CELoSS Y Compute   
Clients g gt 92 by Eq.(6) (c) Post-training   
Remaining iu g't↑ gi Local 91t 92 Aggregation   
Clients ga Training Broadcasts Global Model

the target data that needed to be unlearned, they are not suitable for sample unlearning. Since in the sample unlearning, clients only request to unlearn partial training data. However, no one knows which data will be requested to unlearn during FL training, and thus preparing these records in advance for later unlearning is not feasible in practice.

Furthermore, for other FU algorithms that do not necessitate using historical training records, we can technically treat unlearning samples as belonging to a virtual client. Hence, the sample unlearning can be transferred to the client unlearning. For example, when a client requests to unlearn partial data $D _ { u }$ , we can form a new virtual client $u$ that owns $D _ { u }$ and unlearn it.

The formulation of unlearning the target client $u$ from the trained global model can be defined by:

$$
\operatorname* { m a x } _ { \omega } \ L _ { u } ( \omega ) ,
$$

where $L _ { u } ( \omega )$ represents the local objective of client $u$ in $\mathrm { F L }$

Federated Unlearning with Gradient Ascent. Gradient Ascent (GA) (Wu et al. 2022; Liu et al. 2023) is a proactive and efficient approach for solving Problem (2). At each communication round $t$ , it strives to maximize the empirical loss of the target client $u$ by updating the model according to $\omega ^ { t + 1 } = \omega ^ { \overline { { t } } } + \eta ^ { t } \nabla L _ { u } ( \dot { \omega } ^ { t } )$ with the step size (learning rate) $\boldsymbol { \eta } ^ { t }$ (Halimi et al. 2022). However, (Wu et al. 2022) suggest that this approach would fail because of destroying the global model performance for the remaining clients. To this end, they propose EWCSGA, which incorporates a regularization term to the cross entropy loss to mitigate the negative impact on the model utility. Besides, another approach computes an update direction $\Delta \omega$ orthogonal to the subspace of the model layer inputs $x$ , i.e., $\Delta \omega x = 0$ (Saha, Garg, and

# Algorithm 1: FedOSD

Require: Pretrained model $\omega ^ { 0 }$ , learning rate $\eta$ , FL client set   
$S$ , communication round $T$ , max unlearning round $T _ { u }$ .   
1: $u \in S \gets$ The client requests for unlearning.   
2: for $t = 0 , 1 , \cdots , T _ { u } - 1$ do   
3: Server broadcasts $\omega ^ { t }$ to all client $i \in S$ .   
4: $\boldsymbol { \omega } _ { i } ^ { t } \gets$ Each client $\mathbf { \chi } _ { i }$ performs local training, in which   
client $u$ switches to utilize UCE loss (Eq. (3)).   
5: Server receives $g _ { i } ^ { t } = ( \omega ^ { t } - \omega _ { i } ^ { t } ) / \eta$ from each client $i$ .   
6: $G \gets \mathrm { c o n c a t } ( g _ { 1 } ^ { t } , \cdot \cdot \cdot , g _ { i } ^ { t } , \cdot \cdot \cdot ) , \forall i \in S , i \neq u$ .   
7: Calculate orthogonal steepest direction $d ^ { t }$ by Eq. (6).   
8: $\omega ^ { t + 1 }  \omega ^ { t } + \bar { \eta d } ^ { t }$ .   
9: end for   
10: $S \gets S \backslash u$ , and start the post-training stage.   
11: for $t = T _ { u } , T _ { u } + 1 , \cdot \cdot \cdot , T$ do   
12: The server broadcasts $\omega ^ { t }$ to all client $i \in S$ .   
13: Each client $i$ performs local training to obtain $g _ { i } ^ { t }$ .   
14: $\begin{array} { r } { g _ { a } ^ { t }  \nabla _ { \omega ^ { t } } \frac { 1 } { 2 } \| \omega ^ { t } - \omega ^ { 0 } \| ^ { 2 } } \end{array}$ .   
15: if $g _ { i } ^ { t } \cdot g _ { a } ^ { t } > 0$ then   
16: $g _ { i } ^ { \prime t } \gets$ Project $g _ { i } ^ { t }$ to the normal plane of $g _ { a } ^ { t }$ .   
17: Rescale $g _ { i } ^ { \bar { \prime } t }$ by $\dot { g } _ { i } ^ { \prime t }  g _ { i } ^ { \prime t } / \| g _ { i } ^ { \prime t } \| \dot { \cdot } \| g _ { i } ^ { t } \|$ .   
18: else   
19: $g _ { i } ^ { \prime t } \gets g _ { i } ^ { t } .$   
20: end if   
21: Server receives $g _ { i } ^ { \prime t }$ and aggregates $\begin{array} { r } { \bar { g } ^ { \prime t } = \frac { 1 } { | S | } \sum _ { i } g _ { i } ^ { \prime t } } \end{array}$ .   
22: $\omega ^ { t + 1 }  \omega ^ { t } - \eta \bar { g } ^ { \prime t }$ .

Roy 2021; Li et al. 2023). This kind of method works well in protecting the model utility in centralized learning, however, it is not suitable for FL due to potential privacy leakage from uploading $x$ . SFU (Li et al. 2023) attempts to mitigate this issue by multiplying $x$ with a factor $\lambda$ before uploading, but attackers can easily recover the original data. Additionally, it would suffer model utility reduction during unlearning, because the derived model update direction is only orthogonal to a subset of the input data from the remaining clients, which cannot ensure the preservation of model utility. We validate these points through the experimental results presented in Table 2.

Our method draws from the idea of GA to achieve the goal of unlearning. Differently, we modify the CE loss function to an unlearning version to overcome the gradient explosion issue, and compute the steepest descent direction that not only aligns closely with the target client’s gradient but also avoids conflicts with the retained clients’ gradients. This approach enables more effective unlearning while mitigating the model utility degradation.

# 3 The Proposed Approach

Our proposed FedOSD aims to effectively remove the target client’s data from the FL global model while mitigating the model performance reduction across remaining clients. Fig. 2 demonstrates the framework of FedOSD, which includes two stages: unlearning (Fig. 2(b)) and post-training (Fig. 2(c)). $\omega ^ { 0 }$ is the global model previously trained through Federated Learning across $m$ clients (Fig. 2(a)). When client $u$ requests for unlearning, it utilizes the proposed Unlearning Cross-Entropy loss to conduct the local training. After collecting local gradients $g _ { i } ^ { t }$ , the server calculates a direction $d ^ { t }$ that is closest to client $u$ ’s gradient while orthogonal to remaining clients’ gradients, and then updates the model by $\omega ^ { t + 1 } = \bar { \omega } ^ { t } + \eta ^ { t } d ^ { t }$ . In the post-training stage, a gradient projection strategy is performed to prevent the model from reverting to $\omega ^ { 0 }$ . Detailed steps of FedOSD can be seen in Algorithm 1. In Appendix.A.2, we prove the convergence of FedOSD in the unlearning and post-training stages.

![](images/c88fb285e05acdf9f2e008eaf3904c42924c882790e24c47432d14e2b8bd8901.jpg)  
Figure 3: A comparison between (a) Cross-Entropy and (b) the proposed Unlearning Cross-Entropy. When using CE loss and GA to unlearn, it needs to drive $p _ { o , c }$ to 0, leading to gradient explosion and non-convergence. When the target client switches to utilize UCE, it adopts the gradient descent to drive $p _ { o , c }$ to 0 and wouldn’t bring the convergence issue.

# 3.1 Unlearning Cross-Entropy Loss

We first take a brief review of how Gradient Ascent can drive the model to unlearn. As shown in Fig. 3(a), by updating the global model with $\omega ^ { t + 1 } = \omega ^ { t } + \eta \bar { \nabla } L _ { u } ( \omega ^ { t } )$ , the local loss increases and $p _ { o , c }$ approaches 0, thus degrading the model’s prediction accuracy on the target client’s data and achieving unlearning. However, the CE Loss (Eq.(1)) has no upper bound. As seen in Fig. 3(a), when $p _ { o , c }$ is getting quite close to 0, $\partial L _ { C E } / \partial p _ { o , c }$ would suffer the explosion and thus the local gradient of the target unlearning client explodes. That’s why directly applying GA to unlearn would make the model similar to a random model (Halimi et al. 2022). One conventional solution is to project the model back to an $L _ { 2 }$ - norm ball of radius $\delta$ (Halimi et al. 2022). But it brings a hyper-parameter that requires experimentally tuning, and a fixed $\delta$ cannot guarantee the convergence.

To address this issue, we modify CE loss to an unlearning version named Unlearning Cross-Entropy (UCE) loss:

$$
L _ { U C E } = - \sum _ { c = 1 } ^ { C } y _ { o , c } \cdot l o g ( 1 - p _ { o , c } / 2 ) .
$$

By minimizing Eq.(3), we can drive the predicted probability $p _ { o , c }$ to be closer to 0 (as seen in Fig. 3(b)), thereby diminishing the prediction ability of the model on the target client’s data to unlearn it. Note that before unlearning, the model $\omega ^ { 0 }$ often performs well on clients, where $p _ { o , c }$ is close to 1 and the model update step for the global model is already quite small. Hence, the constant “ $2 ^ { , , }$ in Eq.(3) is set to ensure that the gradient norm of the target client does not exceed those of the remaining clients. This can prevent the unlearning process from being unstable or even directly damaging the model utility. We verify this in Appendix.B.2.

Hence, when client $u$ requests for unlearning, it no longer applies GA on the CE loss. Instead, it switches to utilize UCE loss and performs gradient descent to train the model. Since UCE loss has the lower bound 0, it can achieve the goal of unlearning client $u$ ’s data without bringing issues of gradient explosion and convergence difficulties. Denote ${ \tilde { L } } _ { u }$ as the local objective of the target client $u$ by using UCE loss, then the unlearning formulation (2) is transferred to:

$$
\operatorname* { m i n } _ { \omega } \tilde { L } _ { u } ( \omega ) .
$$

# 3.2 Orthogonal Steepest Descent Direction

In FedOSD, we solve Problem (4) to unlearn the target client $u$ by iterating $\omega ^ { t + 1 } = \omega ^ { t } + \eta ^ { t } d ^ { t }$ , where $d ^ { t }$ is an orthogonal steepest descent direction at $t ^ { t h }$ round. In this section, we discuss how to obtain such an update direction and analyze how it can accelerate unlearning while mitigating the negative impact on the model utility. We start by introducing the gradient conflict, which is a direct cause of model performance degradation on FL clients (Wang et al. 2021).

Definition 1 (Gradient Conflict): The gradients of client $i$ and $j$ are in conflict with each other iff $g _ { i } \cdot g _ { j } < 0$ .

In each communication round $t$ , denote $g _ { i } ^ { t } , i \neq u$ as the local gradient of remaining clients, and $g _ { u } ^ { t }$ as the gradient of the target client $u$ for unlearning. If we directly adopt $- g _ { u } ^ { t }$ as the direction to update the model to unlearn client $u$ , i.e., $\boldsymbol \omega ^ { t + 1 } = \boldsymbol \omega ^ { t } - \boldsymbol \eta ^ { t } \boldsymbol g _ { u } ^ { t }$ , the model performance on the remaining clients would easily suffer reduction because $g _ { u } ^ { t }$ would conflict with some $g _ { i } ^ { t }$ . The experimental results of Table 3 corroborate the presence and the impact of such gradient conflicts in FU.

Hence, mitigating gradient conflicts can help alleviate decreases in the model utility. One ideal solution would be identifying a common descent direction $d ^ { t }$ that satisfies $d ^ { t } { \cdot } g _ { u } ^ { t } { < } 0$ and $\dot { } d ^ { t } { \cdot } g _ { i } ^ { t } { < } 0$ . However, such a strategy could lead to the model becoming prematurely trapped in a local Pareto optimum, which remains far from the optimum of Problem (4). We verify it in the ablation experiments (Section 4.3).

To this end, we mitigate the gradient conflict by computing a model update direction $d ^ { t }$ orthogonal to the gradient of the remaining clients, i.e., $d ^ { t } \cdot g _ { i } ^ { t } = 0 , \mathsf { \bar { \forall } } i \neq u$ . Although $d ^ { t }$ is not a common descent direction, it helps slow down the performance reduction of the model on the remaining clients. However, in FL, the number of remaining clients (i.e., $m { - } 1 )$ is significantly smaller than $D$ (the dimension of model parameters), implying rank $( \forall g _ { i } ^ { t } , i \neq u ) \ \le \ m - 1 \ < < \ \bar { D }$ . Consequently, there are numerous orthogonal vectors $d$ that satisfy $\dot { d } \cdot g _ { i } ^ { t } = 0$ . Therefore, if the obtained direction differs significantly from $- g _ { u } ^ { t }$ , it would impede the unlearning process and potentially exacerbate the degradation of model utility. We verify it in the ablation study in Section 4.3.

Denote $G \in \mathbb { R } ^ { ( m - 1 ) \times D }$ as a matrix where each row represents a gradient of a remaining client, the key idea is to find a $d ^ { t }$ that satisfies $G d ^ { t } = { \vec { 0 } }$ while being closest to $- g _ { u } ^ { t }$ to accelerate unlearning, i.e., $\begin{array} { r } { d ^ { t } = \arg \operatorname* { m i n } _ { d ^ { t } } c o s ( g _ { u } ^ { t } , d ^ { t } ) } \end{array}$ . To maintain the direction’s norm, we fix $\| d ^ { t } \| = \| g _ { u } ^ { t } \|$ , then the problem is equivalent to:

![](images/68b1de4d9b4c8912327290c70467e1c2c79c97fd1c2d4c97ffbeb3bbf91e7b07.jpg)  
Figure 4: A demo depicting the model reverting issue in post-training. The contour map denotes the local loss of the model on a remaining client. $\bar { \omega } ^ { 0 }$ is the original model before unlearning. $\omega ^ { T _ { u } }$ is the model after unlearning. The dashed arrow depicts the path of the model update in post-training, where $\omega ^ { \dot { T } _ { u } }$ moves to $\boldsymbol { \bar { \omega } } ^ { T _ { u } + 1 }$ and is closer to $\omega ^ { \overset { \cdot } { 0 } }$ . The red arrows indicate a better path obtained by FedOSD.

$$
\begin{array} { r l } & { \underset { d ^ { t } \in \mathbb { R } ^ { D } } { \operatorname* { m i n } } ~ \frac { g _ { u } ^ { t } \cdot d ^ { t } } { \| g _ { u } ^ { t } \| ^ { 2 } } , } \\ & { s . t . ~ G d ^ { t } = \vec { 0 } , } \\ & { ~ \| d ^ { t } \| = \| g _ { u } ^ { t } \| , } \end{array}
$$

which is a linear optimization problem and the solution is:

$$
d ^ { t } = \frac { 1 } { 2 \Vert g _ { u } ^ { t } \Vert ^ { 2 } \mu } \left( G ^ { T } U \Sigma ^ { + } V ^ { T } G g _ { u } ^ { t } - g _ { u } ^ { t } \right) ,
$$

where $\mu$ is a related scalar that can make $\| d ^ { t } \| = \| g _ { u } ^ { t } \|$ , i.e., $\begin{array} { r l r } { \mu } & { { } = } & { \| G ^ { T } U \Sigma ^ { + } V ^ { T } G g _ { u } ^ { t } - g _ { u } ^ { t } \| / ( 2 \| g _ { u } ^ { t } \| ^ { 4 } ) } \end{array}$ . The matrices $V , { \boldsymbol \Sigma } , U ^ { T }$ are the singular value decomposition of $G G ^ { T } \in \mathbb { R } ^ { ( m - 1 ) \times ( m - 1 ) }$ , i.e., $G G ^ { T } = V \Sigma U ^ { T }$ , which are not time-consuming to obtain. $\Sigma ^ { + }$ is the Moore-Penrose pseudoinverse of $\Sigma$ , i.e., $\begin{array} { r } { \Sigma ^ { + } = \mathrm { d i a g } ( \frac { 1 } { { s } _ { 1 } } , \frac { 1 } { { s } _ { 2 } } , \cdots , \frac { 1 } { { s } _ { r } } , 0 , \cdots , 0 ) } \end{array}$ , where $s _ { 1 } , s _ { 2 } , \cdots , s _ { r }$ are the non-zero singular values of $G G ^ { T }$ . The detailed proof of Eq.(6) is presented in Appendix.A.1, where we also report the actual computation time of FedOSD. The obtained $\dot { \boldsymbol { d } } ^ { t }$ is closest to $- g _ { u } ^ { t }$ and satisfies $G d ^ { t } = 0$ , so that it can accelerate unlearning and mitigate the model utility reduction.

# 3.3 Gradient Projection in Post-training

After unlearning, the target client $u$ leaves the FL system, and the remaining clients undertake a few rounds of FL training to recover the model utility. This phase is referred to as the “post-training” stage (Halimi et al. 2022; Zhao et al. 2023). However, we observe that not only is the model performance across remaining clients recovered, but unexpectedly, the performance on the forgotten data of the target client $u$ also improves. It looks like the model remembers what has been forgotten.

One possible case is that the data from the target client $u$ share a similar distribution with the remaining clients’ data. Hence, with the model utility being recovered, the model can generalize to client $u$ ’s data, thereby enhancing the model performance on client $u$ . In general, this issue does not require intervention, because it can even happen on a retrained model without the participation of the target client $u$ .

However, we observe that there is another case called model reverting that requires intervention. As seen in Fig. 4, with the model utility being reduced, the local loss of the remaining clients increased after unlearning. Besides, many previous FU algorithms do not significantly deviate the model from the original model $\omega ^ { 0 }$ during unlearning. Subsequently, when starting post-training, the local gradient $g _ { i } ^ { t }$ does not conflict with $g _ { a } ^ { t }$ (i.e., $g _ { i } ^ { t } \cdot g _ { a } ^ { t } > 0 \rangle$ ), where $g _ { a } ^ { t }$ is defined by $\begin{array} { r } { g _ { a } ^ { t } = \nabla _ { \omega ^ { t } } \frac { 1 } { 2 } \| \bar { \omega ^ { t } } - \omega ^ { 0 } \| ^ { 2 } } \end{array}$ . Therefore, the model is driven back to the old local optimal region where $\omega ^ { 0 }$ also resides, so that the model directly recovers what has been forgotten. The experimental results of Table 1 and Fig. 5 substantiate this observation, showing a decreased distance between the model and $\omega ^ { 0 }$ during post-training.

To address this issue, when $\breve { g _ { i } ^ { t } } \cdot g _ { a } ^ { t } > 0$ , we project the local gradient $g _ { i } ^ { t }$ to the normal plane of $g _ { a } ^ { t }$ :

$$
{ g ^ { \prime } } _ { i } ^ { t } = g _ { i } ^ { t } - { \frac { g _ { i } ^ { t } \cdot g _ { a } ^ { t } } { \| g _ { a } ^ { t } \| ^ { 2 } } } \cdot g _ { a } ^ { t } .
$$

Subsequently, each remaining client uploads $g _ { i } ^ { \prime t }$ instead of $g _ { i } ^ { t }$ to the server for the aggregation, i.e., $\begin{array} { r } { \bar { g } ^ { \prime t } = \frac { 1 } { | S | } \sum _ { i } g _ { i } ^ { \prime t } } \end{array}$ . And the global model is updated by $\omega ^ { t + 1 } = \omega ^ { t } - \eta \bar { g } ^ { \prime t }$ . Given that ${ g ^ { \prime } } _ { i } ^ { t } \cdot g _ { a } ^ { t } = 0 , \forall i , \bar { g } ^ { \prime t }$ satisfies ${ \bar { g } } ^ { \prime t } \cdot g _ { a } ^ { t } = 0$ , ensuring that the updated model would not revert towards $\omega ^ { 0 }$ . Thus, it addresses the reverting issue in the post-training stage. It’s worth noting that the gradient projection method still works when the loss surface is complex. This is because it can always identify a direction that prevents the model from reverting to the original model, while guiding it towards other local optima.

# 4 Experiments

# 4.1 Experimental Setup

We adopt the model test accuracy on the retained clients (denoted as R-Acc) to evaluate the model utility. To assess the effectiveness of unlearning, we follow (Halimi et al. 2022; Li et al. 2023; Zhao et al. 2023) to implant backdoor triggers into the model by poisoning the target client’s training data and flipping the labels (more details can be seen in Appendix.B.1). As a result, the global model becomes vulnerable to the backdoor trigger. The accuracy of the model on these data measures the attack success rate (denoted as ASR), and the low ASR indicates the effective unlearning performance of the algorithm.

Baselines and Hyper-parameters. We first consider the retraining from scratch (denoted as Retraining) and FedEraser (Liu et al. 2021), which is also a kind of retraining but leverages the norms of the local updates stored in the preceding FL training to accelerate retraining. We then encompass well-known FU algorithms including FedRecovery (Zhang et al. 2023), MoDe (Zhao et al. 2023), and the gradient-ascent-based FU methods: EWCSGA (Wu et al. 2022) and FUPGA (Halimi et al. 2022). We follow the settings of (Halimi et al. 2022; Zhang et al. 2023) that all clients utilize Stochastic Gradient Descent (SGD) on local datasets with local epoch $E = 1$ . We set the batch size as 200 and the learning rate $\eta \in \{ 0 . 0 0 5 , 0 . 0 2 5 , 0 . 0 0 1 , 0 . 0 0 0 5 \}$ decay of

Table 1: The ASR, the mean R-Acc (and the std.) of the model. The row of $\omega ^ { 0 }$ denotes the initial state before unlearning. The ‘1’ marked following the algorithm name represents the results after unlearning, while $\cdot _ { 2 } ,$ denotes the results after post-training. The signal $\cdot _ { r } ,$ in the columns of ASR ignifies an increase of the ASR value because of the model reverting during post-training.   

<html><body><table><tr><td></td><td colspan="6">FMNIST</td><td colspan="6">CIFAR-10</td></tr><tr><td></td><td colspan="2">Pat-20</td><td colspan="2">Pat-50</td><td colspan="2">IID</td><td colspan="2">Pat-20</td><td colspan="2">Pat-50</td><td colspan="2">IID</td></tr><tr><td>Algorithm</td><td>ASR</td><td>R-Acc</td><td>ASR</td><td>R-Acc</td><td>ASR</td><td>R-Acc</td><td>ASR</td><td>R-Acc</td><td>ASR</td><td>R-Acc</td><td>ASR</td><td>R-Acc</td></tr><tr><td></td><td>.991</td><td>.852(.113)</td><td>.957</td><td>.869(.013)</td><td>.893</td><td>.898(.010)</td><td>.897</td><td>.589(.115)</td><td>.754</td><td>.658(.016)</td><td>.243</td><td>.731(.013)</td></tr><tr><td>Retraining</td><td>.004</td><td>.760(.228)</td><td>.002</td><td>.817(.025)</td><td>.002</td><td>.840(.015)</td><td>.047</td><td>.507(.106)</td><td>.009</td><td>.583(.149)</td><td>.022</td><td>.511(.013)</td></tr><tr><td>FedEraser</td><td>.005</td><td>.763(.171)</td><td>.011</td><td>.810(.102)</td><td>.002</td><td>.872(.009)</td><td>.098</td><td>.454(.158)</td><td>.026</td><td>.571(.128)</td><td>.016</td><td>.683(.012)</td></tr><tr><td>FedRecovery1</td><td>.637</td><td>.761(.279)</td><td>.693</td><td>.823(.092)</td><td>.498</td><td>.871(.012)</td><td>.156</td><td>.454(.337)</td><td>.102</td><td>.476(.346)</td><td>.015</td><td>.692(.017)</td></tr><tr><td>MoDe1</td><td>.003</td><td>.667(.246)</td><td>.005</td><td>.777(.046)</td><td>.002</td><td>.792(.012)</td><td>.145</td><td>.256(.162)</td><td>.066</td><td>.199(.119)</td><td>.025</td><td>.481(.018)</td></tr><tr><td>EWCSGA1</td><td>.000</td><td>.255(.259)</td><td>.000</td><td>.233(.261)</td><td>.101</td><td>.126(.009)</td><td>.000</td><td>.199(.372)</td><td>.000</td><td>.381(.426)</td><td>.018</td><td>.259(.010)</td></tr><tr><td>FUPGA1</td><td>.000</td><td>.227(.254)</td><td>.000</td><td>.178(.200)</td><td>.101</td><td>.105(.008)</td><td>.000</td><td>.202(.373)</td><td>.000</td><td>.388(.433)</td><td>.019</td><td>.271(.013)</td></tr><tr><td>FedOSD1</td><td>.000</td><td>.757(.187)</td><td>.000</td><td>.806(.042)</td><td>.000</td><td>.884(.011)</td><td>.000</td><td>.549(.185)</td><td>.000</td><td>.602(.175)</td><td>.000</td><td>.696(.016)</td></tr><tr><td>FedRecovery2</td><td>.960"</td><td>.857(.112)</td><td>.873T</td><td>.876(.013)</td><td>.806</td><td>.898(.011)</td><td>.785T</td><td>.607(.119)</td><td>.598</td><td>.643(.138)</td><td>.155T</td><td>.737(.016)</td></tr><tr><td>MoDe2</td><td>.007</td><td>.744(.252)</td><td>.003</td><td>.816(.028)</td><td>.002</td><td>.843(.014)</td><td>.060</td><td>.519(.117)</td><td>.035</td><td>.582(.173)</td><td>.016</td><td>.703(.016)</td></tr><tr><td>EWCSGA²</td><td>.935T</td><td>.836(.173)</td><td>.400</td><td>.869(.013)</td><td>.378r</td><td>.896(.012)</td><td>.581r</td><td>.591(.194)</td><td>.592T</td><td>.652(.118)</td><td>.140"</td><td>.736(.016)</td></tr><tr><td>FUPGA²</td><td>.857T</td><td>.837(.185)</td><td>.745T</td><td>.875(.013)</td><td>.199</td><td>.894(.009)</td><td>.662r</td><td>.599(.157)</td><td>.602</td><td>.658(.091)</td><td>.144"</td><td></td></tr><tr><td>FedOSD²</td><td>.023</td><td>.851(.105)</td><td>.021</td><td>.874(.014)</td><td>.004</td><td>.897(.011)</td><td>.027</td><td>.606(.101)</td><td>.016</td><td>.659(.017)</td><td>.030</td><td>.737(.014) .734(.015)</td></tr></table></body></html>

Table 2: The performance of SFU on FMNIST in Pat-20, Pat-50, and IID scenarios. All settings are the same as Table 1. SFU1 represents the results after unlearning, while $\mathrm { \Delta { S F U ^ { 2 } } }$ denotes the results after post-training. The signal $^ { \bullet } r ^ { \cdot }$ ’ in the columns of ASR signifies an increase in the ASR value because of the model reverting during post-training.   

<html><body><table><tr><td>一</td><td colspan="2">Pat-20</td><td colspan="2">Pat-50</td><td colspan="2">IID</td></tr><tr><td></td><td>ASR</td><td>R-Acc</td><td>ASR</td><td>R-Acc</td><td>ASR</td><td>R-Acc</td></tr><tr><td>SFU1</td><td>.000</td><td>.345(.27)</td><td>.198</td><td>.218(.02)</td><td>.103</td><td>.169(.01)</td></tr><tr><td>SFU2</td><td>.547T</td><td>.792(.18)</td><td>.563T</td><td>.846(.02)</td><td>.386T</td><td>.893(.01)</td></tr></table></body></html>

0.999 per round, where the best performance of each method is chosen in comparison. Prior to unlearning, we run FedAvg (McMahan et al. 2017) for 2000 communication rounds to generate the original model $\omega ^ { 0 }$ for unlearning. The max unlearning round is 100, while the max total communication round (including unlearning and post-training) is 200.

Datasets and Models. We follow (Zhao et al. 2023) to evaluate the algorithm performance on the public datasets MNIST (LeCun et al. 1998), FMNIST (Xiao, Rasul, and Vollgraf 2017), and CIFAR-10/100 (Krizhevsky and Hinton 2009), where the training/testing data have already been split. To evaluate the effectiveness of unlearning across varying heterogeneous local data distributions, we consider four scenarios to assign data for clients: (1) Pat-20: We follow (McMahan et al. 2017) to build a pathological non-IID scenario where each client owns the data of $20 \%$ classes. For example, in a dataset like MNIST with 10 classes, each client has two classes of the data. (2) Pat-50: It constructs a scenario where each client has $50 \%$ classes. (3) Pat-10: It’s an extreme data-island scenario where each client has $10 \%$ of distinct classes. (4) IID: The data are randomly and equally separated among all clients. We utilize LeNet-5 (LeCun et al. 1998) for MNIST, Multilayer perception (MLP)

Table 3: ASR, R-Acc, and $N C$ , the mean number of retained clients per round whose gradients conflict with the model update direction in Pat-20 on MNIST and CIFAR-100.   

<html><body><table><tr><td></td><td colspan="3">MNIST</td><td colspan="3">CIFAR-100</td></tr><tr><td>Algorithm</td><td>ASR</td><td>R-Acc</td><td>NC</td><td>ASR</td><td>R-Acc</td><td>NC</td></tr><tr><td></td><td>.997</td><td>.963</td><td></td><td>.584</td><td>.394</td><td></td></tr><tr><td>FedRecovery</td><td>.038</td><td>.716</td><td>3.00</td><td>.000</td><td>.214</td><td>1.00</td></tr><tr><td>MoDe</td><td>.039</td><td>.723</td><td>3.47</td><td>.027</td><td>.160</td><td>3.13</td></tr><tr><td>EWCSGA</td><td>.000</td><td>.527</td><td>7.45</td><td>.000</td><td>.093</td><td>7.95</td></tr><tr><td>FUPGA</td><td>.000</td><td>.535</td><td>7.38</td><td>.000</td><td>.090</td><td>7.92</td></tr><tr><td>FedOSD</td><td>.000</td><td>.924</td><td>0.00</td><td>.000</td><td>.369</td><td>0.00</td></tr></table></body></html>

(Popescu et al. 2009) for FMNIST, CNN (Halimi et al. 2022) with two convolution layers for CIFAR-10, and NFResNet18 (Brock, De, and Smith 2021) for CIFAR-100.

# 4.2 Evaluation of Unlearning and Model Utility

We first evaluate the ASR and R-Acc of the model at the end of both the unlearning stage and post-training stage on FMNIST and CIFAR-10. One of the ten clients is randomly selected as the target client requesting for unlearning.

Table. 1 lists the comparison results. It can be seen that in the unlearning stage, the gradient-ascent-based FU algorithms such as EWCSGA and FUPGA achieve more complete unlearning in non-IID scenarios, evidenced by their ASR reaching 0, but they experience a more pronounced reduction in R-Acc. What’s worse, on FMNIST, the models of EWCSGA and FUPGA after the unlearning stage are nearly equivalent to a randomly initialized model. This is because their gradient constraint mechanisms, which aim to handle the gradient explosion issue, rely on fixed hyper-parameters. Since the optimal hyper-parameters cannot be determined in advance, these methods inevitably become ineffective. Benefiting from the UCE loss and the orthogonal steepest de

R-Acc ASR Distance Retraining Unlearning Stage Post-training Stage (a) FedEraser (b) FedRecovery (c) MoDe 60 0.4   
2460 240 40 0.23 460 230 10 0 0 50 100 150 200 0 0 01 50 100 150 200 0.0 0 0 50 100 150 200 0 Communication Round Communication Round Communication Round (d) EWCSGA (e) FUPGA (f) FedOSD 0.6 0.8 0.8   
60 0.64 40 60 0.20.40.6   
240 240 0 0 50 100 150 200 0.0 0 0 + 50 100 150 200 0.0 0 0 50 100 150 200 0.0 Communication Round Communication Round Communication Round

<html><body><table><tr><td></td><td colspan="4">m=10</td><td colspan="4">m=20</td><td colspan="4">m=50</td></tr><tr><td>Algorithm</td><td>ASR</td><td>R-Acc</td><td>Worst</td><td>Best</td><td>ASR</td><td>R-Acc</td><td>Worst</td><td>Best</td><td>ASR</td><td>R-Acc</td><td>Worst</td><td>Best</td></tr><tr><td>c0</td><td>.754</td><td>.658(.016)</td><td>.629</td><td>.683</td><td>.226</td><td>.678(.019)</td><td>.644</td><td>.712</td><td>.085</td><td>.712(.069)</td><td>.570</td><td>.830</td></tr><tr><td>Retraining</td><td>.009</td><td>.583(.149)</td><td>.435</td><td>.770</td><td>.006</td><td>.499(.227)</td><td>.260</td><td>.768</td><td>.069</td><td>.408(.272)</td><td>.090</td><td>.730</td></tr><tr><td>FedEraser</td><td>.026</td><td>.571(.128)</td><td>.399</td><td>.693</td><td>.045</td><td>.441(.253)</td><td>.158</td><td>.712</td><td>.018</td><td>.437(.057)</td><td>.325</td><td>.555</td></tr><tr><td>FedRecovery1</td><td>.102</td><td>.476(.346)</td><td>.082</td><td>.794</td><td>.012</td><td>.548(.115)</td><td>.400</td><td>.696</td><td>.000</td><td>.344(.348)</td><td>.000</td><td>.740</td></tr><tr><td>MoDe1</td><td>.066</td><td>.199(.119)</td><td>.062</td><td>.321</td><td>.026</td><td>.121(.063)</td><td>.054</td><td>.214</td><td>.116</td><td>.114(.056)</td><td>.035</td><td>.205</td></tr><tr><td>EWCSGA1</td><td>.000</td><td>.381(.426)</td><td>.000</td><td>.880</td><td>.000</td><td>.425(.439)</td><td>.000</td><td>.904</td><td>.000</td><td>.476(.329)</td><td>.085</td><td>.850</td></tr><tr><td>FUPGA1</td><td>.000</td><td>.388(.433)</td><td>.000</td><td>.889</td><td>.000</td><td>.421(.443)</td><td>.000</td><td>.914</td><td>.000</td><td>.435(.368)</td><td>.045</td><td>.855</td></tr><tr><td>FedOSD1</td><td>.000</td><td>.602(.175)</td><td>.433</td><td>.803</td><td>.000</td><td>.658(.103)</td><td>.526</td><td>.798</td><td>.000</td><td>.707(.067)</td><td>.575</td><td>.830</td></tr><tr><td>FedRecovery2</td><td>.598</td><td>.643(.138)</td><td>.477</td><td>.785</td><td>.074r</td><td>.667(.075)</td><td>.552</td><td>.766</td><td>.009</td><td>.644(.064)</td><td>.515</td><td>.770</td></tr><tr><td>MoDe²</td><td>.035</td><td>.582(.173)</td><td>.361</td><td>.747</td><td>.026</td><td>.503(.141)</td><td>.328</td><td>.666</td><td>.075</td><td>.368(.286)</td><td>.055</td><td>.710</td></tr><tr><td>EWCSGA²</td><td>.592</td><td>.652(.118)</td><td>.497</td><td>.781</td><td>.223r</td><td>.680(.042)</td><td>.610</td><td>.752</td><td>.052</td><td>.711(.028)</td><td>.635</td><td>.780</td></tr><tr><td>FUPGA²</td><td>.602</td><td>.658(.091)</td><td>.538</td><td>.766</td><td>.220r</td><td>.679(.041)</td><td>.614</td><td>.748</td><td>.046r</td><td>.708(.029)</td><td>.640</td><td>.775</td></tr><tr><td>FedOSD²</td><td>.016</td><td>.659(.017)</td><td>.627</td><td>.689</td><td>.005</td><td>.678(.029)</td><td>.618</td><td>.730</td><td>.002</td><td>.710(.050)</td><td>.595</td><td>.820</td></tr></table></body></html>

Table 4: The ASR, the mean R-Acc (and the std.), as well as the worst and best R-Acc across the remaining clients on CIFAR-10 with Pat-50 under $m = 1 0$ , $m = 2 0$ , and $m = 5 0$ . The row of $\omega ^ { 0 }$ denotes the initial state before unlearning. The ‘1’ marked following the algorithm name represents the results after unlearning, while $^ { \cdot } 2 ^ { \cdot }$ denotes the results after post-training. The signal $\cdot _ { r } ,$ in the columns of ASR signifies an increase in the ASR value because of the model reverting during post-training.

scent update direction, the proposed FedOSD does not bring extra hyper-parameters and can successfully unlearn the target client data while suffering less utility reduction than others. Besides, since FedRecovery performs unlearning relies solely on the pre-stored historical FL training information, it cannot guarantee the unlearning effect in all scenarios.

During the post-training stage, FedRecovery, EWCSGA, and FUPGA can recover the R-Acc to a level comparable to or exceeding that of the initial state. However, their models gravitate towards the initial $\omega ^ { 0 }$ , leading to the models remembering what has been erased, and thus the ASR values rise significantly. In comparison, FedOSD can recover the model utility without suffering the model reverting issue. More experimental results on MNIST and CIFAR-100 are available in Appendix.B.2.

We also replicate SFU (Li et al. 2023) discussed in Section 2.2 and test its performance on FMNIST (see Table 2). For each remaining client, one batch of data samples is selected to compute the representation matrix. However, we find this process to be highly time-consuming due to the high dimensionality of the representation matrix, which complicates the computation of the SVD. The results depict that it cannot achieve the unlearning goal, suffering significant RAcc reduction during the unlearning process, as well as the model reverting issue during post-training.

![](images/29873b59b164482033f22b420f0927859bdd040db97de3e2ab04742c8178910d.jpg)  
Figure 6: The best, the average, and the worst R-Acc across clients in Pat-10 on (a) CIFAR-10 and (b) CIFAR-100.

Furthermore, we present the experimental results for different client numbers: $m = 1 0$ , $m = 2 0$ , and $m = 5 0$ in Table 4. These results verify the superior performance of FedOSD in terms of the unlearning effectiveness and the model utility in scenarios with more client participation.

To elucidate the negative impact of gradient conflicts on the model utility during unlearning, we report ASR, R-Acc, and the average number of retained clients experiencing gradient conflicts with the model update direction $d ^ { t }$ in Table. 3. The results demonstrate that mitigating the conflict between $d ^ { t }$ and the remaining clients’ gradients can significantly alleviate reductions in the model utility.

Besides, Fig. 6 depicts the results in Pat-10 to evaluate the effect of unlearning on the model utility when some classes of data are completely removed. Compared with FedOSD, the model utility reduction on previous FU methods is considerably unfair, where the R-acc values are even approaching 0. In contrast, FedOSD more effectively maintains the model’s performance on the remaining clients.

Moreover, we visualize the curves of ASR, R-Acc, and the distance between $\omega ^ { t }$ and $\omega ^ { 0 }$ during unlearning and posttraining in Fig. 5. Notably, the unlearning stage of FedRecovery only comprises a single round, as it performs unlearning relying solely on the historical information of the previous FL training. The results demonstrate that FedOSD successfully achieves a zero ASR while maintaining the highest model utility during unlearning. The distance curve in the post-training stage verifies that the models of FedRecovery, EWCSGA, and FUPGA tend to revert towards $\omega ^ { 0 }$ , evidenced by the decreasing distance, thereby leading to an increase in ASR, which suggests a recovery of previously unlearned information. In contrast, FedOSD prevents the model from moving back, thereby ensuring the recovery of model utility without suffering the model reverting issue during post-training.

# 4.3 Ablation Experiments

In Table. 5, we evaluate the performance of several variants of FedOSD (M1 to M5) to study the effect of each part.

<html><body><table><tr><td></td><td colspan="2">FMNIST</td><td colspan="2">CIFAR-10</td><td colspan="2">CIFAR-100</td></tr><tr><td>Method</td><td>ASR</td><td>R-Acc</td><td>ASR</td><td>R-Acc</td><td>ASR</td><td>R-Acc</td></tr><tr><td></td><td>.957</td><td>.869</td><td>.754</td><td>.658</td><td>.433</td><td>.437</td></tr><tr><td>FedOSD1 M11 M21 M31</td><td>.000 .000 .000 .835</td><td>.806 .163 .317 .886</td><td>.000 .000 .000 .331</td><td>.602 .224 .391 .697</td><td>.000 .000 .000 .159</td><td>.399 .014 .267 .476</td></tr></table></body></html>

Table 5: The ASR, the mean R-Acc of the model in the ablation studies. ‘1’ marks the unlearning stage and $' 2 ^ { \prime }$ denotes post-training. $\cdot _ { r } ,$ means suffering the model reverting issue.

M1: Do not use the UCE loss. Instead, the target client utilizes Gradient Ascent on the CE loss to unlearn. The results demonstrate that GA would destroy the model utility.

M2: Replace the orthogonal steepest descent direction $d ^ { t }$ to $- g _ { u } ^ { t }$ for updating the unlearning model, which would conflict with retained clients’ gradients. As a result, the model utility suffers more reduction than FedOSD.

M3: During unlearning, using Multiple Gradient Descent algorithm (Fliege and Svaiter 2000; Pan et al. 2024) to obtain a common descent direction $d ^ { t }$ that satisfies $d ^ { t } \cdot g _ { i } ^ { t } < 0 , \forall i \neq$ $u$ , which can both reduce the UCE loss of the target client and the CE loss of remaining clients in unlearning. The results depict that while this strategy does not compromise model utility, it fails to achieve the unlearning goal, verifying the analysis in Section 3.2.

M4: Randomly select a solution $d ^ { t }$ from the solutions to $G \cdot d ^ { t } { = } \vec { 0 }$ that also satisfies $d ^ { t } { \cdot } g _ { u } ^ { t } { < } 0$ to update the model for unlearning. Since the obtained $d ^ { t }$ would deviate a lot from $- g _ { u } ^ { t }$ , the result of ASR is higher than that of FedOSD. If we tune a larger learning rate to enhance the unlearning performance, it would further harm the model utility.

M5: Remove the gradient projection strategy in the posttraining stage. It results in the model reverting issue, with a significant increase in ASR, verifying it’s necessary to prevent the model from moving back to $\bar { \omega } ^ { 0 }$ during post-training.

# 5 Conclusion and Future Work

In this work, we identify the convergence issue of Gradient Ascent and demonstrate the necessity of mitigating the gradient conflict in Federated Unlearning. Moreover, we highlight the issue of model reverting during post-training, which adversely affects the unlearning performance. To address these issues, we propose FedOSD, which modifies the Cross-Entropy loss to an unlearning version and achieves an orthogonal steepest descent model direction for unlearning. Extensive experiments verify that FedOSD outperforms SOTA FU methods in terms of the unlearning effect and mitigating the model utility reduction. A number of interesting topics warrant future exploration, including the design of the unlearning version of other loss functions such as MSE loss, and further enhancing fairness and privacy protection in FU.