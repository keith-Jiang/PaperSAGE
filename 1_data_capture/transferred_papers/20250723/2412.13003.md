# Boosting Test Performance with Importance Sampling–a Subpopulation Perspective

Hongyu Shen, Zhizhen Zhao

Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Champaign, IL, 61820, U.S.A. hongyu2, zhizhenz @illinois.edu

# Abstract

Despite empirical risk minimization (ERM) is widely applied in the machine learning community, its performance is limited on data with spurious correlation or subpopulation that is introduced by hidden attributes. Existing literature proposed techniques to maximize group-balanced or worst-group accuracy when such correlation presents, yet, at the cost of lower average accuracy. In addition, many existing works conduct surveys on different subpopulation methods without revealing the inherent connection between these methods, which could hinder the technology advancement in this area. In this paper, we identify important sampling as a simple yet powerful tool for solving the subpopulation problem. On the theory side, we provide a new systematic formulation of the subpopulation problem and explicitly identify the assumptions that are not clearly stated in the existing works. This helps to uncover the cause of the dropped average accuracy. We provide the first theoretical discussion on the connections of existing methods, revealing the core components that make them different. On the application side, we demonstrate a single estimator is enough to solve the subpopulation problem. In particular, we introduce the estimator in both attribute-known and -unknown scenarios in the subpopulation setup, offering flexibility in practical use cases. And empirically, we achieve state-of-theart performance on commonly used benchmark datasets.

# Code — https://github.com/skyve2012/DBA Extended version — https://arxiv.org/abs/2412.13003

# 1 Introduction

Empirical risk minimization (ERM) often struggles with distribution shifts that manifest when the training and test distributions differ (Bickel, Br¨uckner, and Scheffer 2007; Quionero-Candela et al. 2009; Shimodaira 2000). One ubiquitous type of distribution shift is subpopulation shift, which describes a scenario where the portion of the subpopulations may vary between training and testing sets. See Figure 1 for an example. This consequently leads to degraded performance when a trained model is applied to production/testing environments (Yang et al. 2023). Ensuring that machine learning models are robust against these distribution shifts hence is crucial for their reliability and safe real-world application.

w/ Subpopulation Shift w/o Subpopulation Shift 22 1 1 55 s5

Existing works proposed different methods in the forms of auxiliary losses (Li et al. 2018; Arjovsky et al. 2019; Alshammari et al. 2022), data augmentations (Zhang et al. 2018; Yao et al. 2022; Han et al. 2022), modeling objectives (Liu et al. 2021; Sagawa et al. 2020; Japkowicz 2000; Wu et al. 2023; Nam et al. 2020; Asgari et al. 2022; Rudner et al. 2024; Han and Zou 2024; Hong et al. 2023; Tsirigotis et al. 2024; Menon et al. 2021; Lin et al. 2017) and data sampling techniques (LaBonte, Muthukumar, and Kumar 2024; Izmailov et al. 2022; Japkowicz 2000). They all exhibit superior performance on worst group accuracy while maintaining high accuracy in the overall set. However, two recent works experimentally observed that most models experience a drop in average accuracy performance compared to the ERM setup despite the high worst group accuracy (Tsirigotis et al. 2024; Yang et al. 2023). Nonetheless, none of the papers is able to provide rigorous explanations on the answer to why. The lack of clarity in understanding can impede the development of appropriate models and methods, potentially stalling progress in the field.

In this work, we propose a systematic dataset bias analysis (DBA) framework that is rooted in importance sampling. With this framework, we reveal the cause of the lower-thanERM average accuracy is the mismatch between the learning objective and the testing dataset. Moreover, we identify the flexibility of this framework in interpreting the formulation of some of the existing works that focus primarily on statistical heuristics and do not specify the underlying assumptions of the models or data. The DBA framework, on the other hand, can close the gap, allowing us to explicitly discuss assumptions systematically and compare different existing works with the same language. We believe this analysis offers a comprehensive and theoretically grounded view to people who wish to proceed with the study of subpopulation methods.

Practically, we propose to estimate a single distribution given the conducted analysis using the DBA framework and prove that this is enough for solving the subpopulation problem under certain assumptions. Subsequently, we propose 3 different methods for estimating the distribution given different access levels to data and attributes. Empirically, we demonstrate the framework improves the test performance under subpopulation setups and achieves state-of-the-art (SOTA) results for both average and worst group accuracy while avoiding the lower-to-ERM performance.

# 2 Related Work

Here we cover related works about importance sampling, the survey papers of the subpopulation shift, and the associated SOTA methods. An extension is included in Appendix.

# 2.1 Importance Sampling

DBA interprets distributional shift as a mismatch of the weight function from an importance sampling perspective. Although primarily focused on subpopulation setups, the method’s formulation applies broadly to distributional shift problems. Early works on importance sampling (Shimodaira 2000; Huang et al. 2006) address dataset shifts but lack realworld experiments and clarity for subpopulation cases. In contrast, DBA systematically formulates the application to subpopulation problems, explicitly stating assumptions and identifying key components like distributions leading to such issues. Other studies (Kanamori, Hido, and Sugiyama 2009; Fang et al. 2020) propose weight estimation methods requiring partial test set access, unlike DBA. Additionally, DBA considers the weight function as the ratio of joint distributions of $x$ and $y$ , addressing subpopulation and covariate shifts more realistically.

# 2.2 Subpopulation Survey

Yang et al. (2023) provides the first comprehensive experimental study on subpopulation methods. It uses Bayes’ theorem to decompose $y | x .$ , accounting for attributes (spurious features), and categorizes datasets into four classes with varying label-attribute correlations. The paper benchmarks 20 subpopulation methods across these datasets but lacks statistical quantification of performance differences. Other surveys (Yu et al. 2024; Zhang et al. 2023) cover broader out-ofdistribution (OOD) and domain generalization (DG) methods. While Yu et al. (2024) focuses on applications, Zhang et al. (2023) quantifies error inflation due to distribution shifts but doesn’t address correction via model design. Our work extends prior studies by providing formal statistical analysis to quantify errors from both data and modeling perspectives. DBA also explains why some methods trade worst-case accuracy for lower average test accuracy.

# 2.3 Subpopulation Method

We categorize subpopulation methods into four classes: auxiliary losses, data augmentations, modeling objectives, and data sampling techniques. Auxiliary loss methods (Li et al. 2018; Arjovsky et al. 2019; Alshammari et al. 2022) aim to mitigate the impact of spurious backgrounds via adversarial training, gradient regularization, or class-balanced adjustments. Data augmentation methods (Zhang et al. 2018; Han et al. 2022; Yao et al. 2022) use convex combinations of samples to reduce background effects. Data sampling methods (LaBonte, Muthukumar, and Kumar 2024; Izmailov et al. 2022) identify class-balanced subsets with independent spurious features for finetuning. Modeling objective methods (Sagawa et al. 2020; Wu et al. 2023; Lin et al. 2017; Rudner et al. 2024) focus on robust feature learning, subpopulation correction, or tailored loss terms like KL divergence or mutual information.

DBA stands out by explicitly stating data assumptions and connecting existing methods under a unified statistical framework (see Sec. 4). For instance, it highlights that augmentation methods (Zhang et al. 2018; Yao et al. 2022; Han et al. 2022) assume conditional similarity across subpopulations. DBA also identifies a universal assumption of identical conditional generative models across methods, which previous works did not explicitly address. Empirically, DBA outperforms SOTA methods on three datasets, confirming its effectiveness and simplicity, and leveraging importance sampling for practical implementation.

# 3 Method

In this section, we first describe the framework, followed by the introduction of the proposed methods.

# 3.1 Dataset Bias Analysis Framework

Throughout the paper, we consider the following notations: $x \in \mathcal { X }$ and $y \in \mathcal { V }$ indicate the random variables for the data and labels, respectively. $\chi$ and $y$ refer to their corresponding spaces. We denote $y$ as a discrete random variable. We use $p ( \cdot )$ to denote the probability distribution and $q ( \cdot )$ or $\hat { p } ( \cdot )$ to represent the estimates. Subscripts $\mathrm { ^ { * } t r ^ { * } }$ , “va”, and “te” indicate concepts associated with train, validation, and test datasets, respectively. We use $\mathcal { D }$ to refer to the datasets. We let $\mathcal { M } _ { \mathrm { t r } } : = \dot { \{ \boldsymbol { q } ( \cdot ) | \boldsymbol { q } ( \cdot ) } $ estimated with data in ${ \mathcal { D } } _ { \mathrm { t r } } \}$ denote the model spaces for the general learning problem. $s$ denotes the attributes/spurious variables that are present in the datasets. This is also the root of the subpopulation. And $I$ refers to the dataset indicator, which is the abstract variable that has no real values (i.e. $I _ { \mathrm { t r } } , I _ { \mathrm { v a } }$ , and $\boldsymbol { I _ { \mathrm { t e . } } }$ ). We use $\operatorname { S u p p } ( \cdot )$ to indicate the support set. We also use the notation $^ { \bullet } \sim ^ { \mathfrak { v } }$ on two datasets (e.g. $\mathcal { D } _ { \mathrm { t r } } \sim \mathcal { D } _ { \mathrm { t e . } }$ ) to represent the same data distributions for the given datasets.

The DBA framework is formulated by initially asking the question: Which model do we pick after training? Conventional approaches consider ERM over ${ \mathcal { D } } _ { \mathrm { t r } }$ , stop the training, and choose the model with the lowest loss value on $\mathcal { D } _ { \mathrm { v a } }$ . Usually, the losses are implicitly assumed to be identical across ${ \mathcal { D } } _ { \mathrm { t r } }$ , $\mathcal { D } _ { \mathrm { v a } }$ , and $\mathcal { D } _ { \mathrm { t e } }$ . There are two drawbacks to this inattentive assumption. First, it does not properly characterize the difference across different datasets. Second, it does not naturally take into account how people make choices on the model. As a remedy, we propose the following objective (Eq. (1)) as the foundation for the DBA framework:

$$
\mathbb { E } _ { ( x , y ) \sim p ( x , y | I _ { \mathrm { v a } } ) } [ \log q ( y | x , I _ { \mathrm { t r } } ) ] .
$$

The maximization of the objective (Eq. (2)) hence provides an intuitive view of how people choose the final model after the optimization:

$$
\operatorname* { m a x } _ { q \in \mathcal { M } _ { \mathrm { t r } } } \mathbb { E } _ { ( x , y ) \sim p ( x , y | I _ { \mathrm { v a } } ) } [ \log q ( y | x , I _ { \mathrm { t r } } ) ] .
$$

In this paper, we consistently focus on the predictive modeling setup (i.e. $y | x )$ , which is aligned with existing works. Intuitively, Eq. (2) describes the scenario where we find the best conditional predictive model $q$ according to the highest log likelihood measured over $\mathcal { D } _ { \mathrm { v a } }$ . Eq. (2) differs from ERM by explicitly considering the inherent difference between different datasets. In most cases, we seek for models to perform well on the unseen ${ \mathcal { D } } _ { \mathrm { t e } }$ . To characterize this, we apply a similar logic as in Eq. (1) and focus on measuring the difference between validation and test sets. We make the following universal assumption 1.

Assumption 1. The supports of $x$ , y on $\mathcal { D } _ { t r }$ , $\begin{array} { r } { \mathcal { D } _ { \nu a } , } \end{array}$ , and $\mathcal { D } _ { t e }$ follow the relationship:   
$\mathrm { \dot { S u p p } } _ { t r } ( x , y ) \supset \mathrm { S u p p } _ { \nu a } ( x , y )$ , $\mathrm { S u p p } _ { t r } ( x , y ) \supset \mathrm { S u p p } _ { t e } ( x , y ) ,$ and $\mathrm { S u p p } _ { \nu a } ( x , y ) \supset \mathrm { S u p p } _ { t e } ( x , y )$ .

The inclusion relationship described in the Assumption 1 essentially ensures a well-defined weight function (i.e., the denominator of the weight function is not zero) in the importance sampling setup in the proposed DBA framework. With this assumption, we make the following claim on the performance of the picked model (from Eq. (2)) with ${ \mathcal { D } } _ { \mathrm { t e } }$ : How does the picked model perform on the test set?

Claim 1. Given Assumption $\boldsymbol { { \mathit { 1 } } }$ holds and let $q ^ { * }$ denote the best model obtained from Eq. (2). The likelihood evaluated with the test set $\mathcal { D } _ { t e }$ for the model $q ^ { * }$ can be viewed as the importance sampling version of $\mathbb { E } _ { ( x , y ) \sim p ( x , y | I _ { \nu a } ) } [ z ( x , y , I _ { \nu a } , \bar { I } _ { t e } ) \log q ^ { * } ( y | x , I _ { t r } ) ]$ over the validation set with the function $z ( \cdot )$ defined below:

$$
z ( x , y , I _ { \nu a } , I _ { t e } ) : = \frac { p ( x , y | I _ { t e } ) } { p ( x , y | I _ { \nu a } ) } .
$$

We defer this and all the following proof details in Appendix. Claim 1 informs that the only way to guarantee the best testing performance for the picked model $q ^ { * }$ is to have access to the distribution $p ( x , y \vert I _ { \mathrm { t e } } )$ . This points out a hidden pitfall that commonly exists, yet overlooked, in the current machine learning optimizations with ERM— people choose a model with the best validation performance and report the corresponding testing performance. By Claim 1, we know that this general setup is true only in the case where $p ( x , y | I _ { \mathrm { v a } } ) = \bar { p ( x , y | I _ { \mathrm { t e } } ) }$ . Otherwise, one needs to provide an accurate estimation on $z ( x , y , I _ { \mathrm { v a } } , I _ { \mathrm { t e } } )$ and pick the training model via a weighted likelihood, $\begin{array} { r } { \mathbb { E } _ { ( x , y ) \sim p ( x , y | I _ { \mathrm { v a } } ) } [ z ( x , y , I _ { \mathrm { v a } } , I _ { \mathrm { t e } } ) \log q ^ { * } ( y | x , I _ { \mathrm { t r } } ) ] } \end{array}$ , on the validation set, to achieve optimal performance on the test set.

Simply put, Eq. (2) describes the way people pick the model during optimization, and Claim 1 points out the correct picking criterion for maximum test set performance. A natural follow-up question on these two arguments is: Can we combine the notion of training and picking, and directly optimize q to maximize the testing performance? The answer is affirmative under some additional assumptions. To explain, we first claim an optimization equivalence, providing the general form with which the optimization on the training set is identical to the optimization on the testing set (Claim 2). Then we derive another objective in the setup where we obtain a closed-form $g ( x , y , I _ { \mathrm { t r } } , I _ { \mathrm { t e } } )$ (see Claim 2) after making several assumptions on the structure of the test data (Theorem 1).

Claim 2. Given Assumption $\jmath$ holds we obtain the following equality on the objective:

$$
\begin{array} { r l } & { \mathbb { E } _ { ( x , y ) \sim p ( x , y | I _ { t r } ) } [ \log q ( y | x , I _ { t r } ) ] } \\ & { = \mathbb { E } _ { ( x , y ) \sim p ( x , y | I _ { t r } ) } [ g ( x , y , I _ { t r } , I _ { t e } ) \log q ( y | x , I _ { t r } ) ] , } \end{array}
$$

where the weight function g(x, y, Itr, Ite) := p(x,y|Itre) .

The proof is similar to Claim 1 and can be found in Appendix. Note that in the language of importance sampling, the weight function $g ( x , y , \bar { I } _ { \mathrm { t r } } , \bar { I } _ { \mathrm { t e } } )$ consists of the proposal distribution $p ( x , y | I _ { \mathrm { t r } } )$ and the data distribution $p ( x , y \vert I _ { \mathrm { t e } } )$ in our setup. Compared to Eq. (1), Eq. (4) offers an objective that can be optimized with ${ \mathcal { D } } _ { \mathrm { t r } }$ as the expectation is taken over the training set—the same space defined for models $q \in \mathcal { M } _ { \mathrm { t r } }$ Claim 2 also confirms that one must know $p ( x , y | I _ { \mathrm { t e } } )$ to improve the testing performance of $q$ when optimizing a model.

In this paper, we consider a uniform attribute setup that assumes the uniform distribution on the attribute/spurious variable $s \in { \mathcal { S } }$ , which is a discrete random variable and $\operatorname { S u p p } ( s ) = \operatorname { S u p p } ( y )$ . $s$ represents the cause of the subpopulation in our study. Formally speaking, this paper considers the following subpopulation shift:

Definition 1. The subpopulation shift is defined as the distributional difference between $p ( x , y | I _ { \mathrm { t r } } )$ and $p ( x , y \vert I _ { \mathrm { t e } } )$ that is introduced by the spurious variable $s$ w.r.t. the response $y$ . Namely, $p ( s , \dot { y } | I _ { \mathrm { t r } } ) \ne \bar { p ( s , y | I _ { \mathrm { t e } } ) }$ .

Specifically, we decompose the joint distribution of $x$ and $y$ through $\begin{array} { r } { \sum _ { s } \dot { p } ( x , y , s | I _ { \mathrm { t r } } ) \dot { = } \sum _ { s } \dot { p } ( x | y , s , I _ { \mathrm { t r } } ) p ( y , s | I _ { \mathrm { t r } } ) } \end{array}$ , and $\begin{array} { r } { \sum _ { s } p ( x , y , s | I _ { \mathrm { t e } } ) = \sum _ { s } p ( x | y , s , I _ { \mathrm { t e } } ) p ( y , s | I _ { \mathrm { t e } } ) } \end{array}$ . And the difference between datasets is on $p ( s , y | I _ { \mathrm { t r } } ) \neq p ( s , y | I _ { \mathrm { t e } } )$ . In the following, we describe several assumptions that lead to the major result of the paper—Theorem 1:

Assumption 2. A universal data generator given the dataset information $I$ , the label $y$ , and the attribute $s$ for the training and test sets: $p ( x | y , s , I _ { t r } ) = p ( x | y , s , I _ { t e } )$ .

Assumption 3. The attribute variable $s$ follows a uniform distribution, conditional on $y$ and $I _ { t e } \colon p ( s | y , I _ { t e } ) = 1 / L ,$ , where $L$ is the number of outcomes for the discrete random variable s.

Assumption 2 requires identical generative processes for $x$ across training and testing. This can be seen as a specific type of covariate shift, attributing shifts in $p ( x , y )$ to variations in $p ( y , s )$ given the attribute $s$ , rather than $p ( x )$ . Such an assumption is common in conformal analysis and causal inference (Yang, Kuchibhotla, and Tchetgen Tchetgen 2024; Suter et al. 2019; Lei and Cand\`es 2021). Assumption 3 imposes a weaker assumption compared to the literature, where uniformity and independence are generally assumed for both $y$ and $s$ (Tsirigotis et al. 2024). Compared to the existing work, we only assume $s$ to follow a uniform distribution and there is no constraint on the distribution of $y$ . The latter makes this approach applicable to class-imbalanced test data.

We further make two additional assumptions (Assumption 4 and 5) that reflect the nature of the considered subpopulation problems. This starts with studying the composition of the shifted datasets. Specifically, we introduce a random variable $m$ that explicitly describes the substructure of the given data (i.e. ${ \mathcal { D } } _ { \mathrm { t r } }$ , ${ \mathcal { D } } _ { \mathrm { v a } }$ , and $\mathcal { D } _ { \mathrm { t e } . }$ ). Most existing works only consider the attribute variable $s$ and its relation to labels $y$ and data $x$ . However, we realize that simply introducing this attribute is not enough to quantify the subpopulation as different subpopulations may have distinct relationships between $s , y$ , and $x$ . Therefore, the presence of $m$ enables the quantification of such differences, making the proposed framework more flexible.

In particular, we consider $m$ to be a binary random variable that takes values $m _ { 0 }$ or $m _ { 1 }$ . And $m _ { 0 }$ refers to the conceptual minority group in ${ \mathcal { D } } _ { \mathrm { t r } }$ that shares the same statistics for $s , y$ and $x$ in ${ \mathcal { D } } _ { \mathrm { t e } }$ , whereas $m _ { 1 }$ denotes the majority group that has distinct statistics of $s , y$ , and possibly $x$ —this explicitly characterizes the prevalent subpopulation in ${ \mathcal { D } } _ { \mathrm { t r } }$ that causes the underperformance in $\mathcal { D } _ { \mathrm { t e } }$ . One may question the soundness of why we claim it is possible to find such a minority group in ${ \mathcal { D } } _ { \mathrm { t r } }$ . An intuitive, yet not strict, proof is to consider the established Assumption 1 that constrains inclusive supports across datasets. With Assumption 1, we can always find a subset of ${ \mathcal { D } } _ { \mathrm { t r } }$ whose data statistics are close to that of ${ \mathcal { D } } _ { \mathrm { t e } }$ for any possibly large enough datasets. This leads to the following assumption:

# Assumption 4. $\begin{array} { r } { p ( y | I _ { t e } ) = p ( y | m _ { 0 } , I _ { t r } ) = p ( y | I _ { t r } ) . } \end{array}$

Assumption 4 describes the scenario where there is no subpupolation on $y$ between ${ \mathcal { D } } _ { \mathrm { t r } }$ and ${ \mathcal { D } } _ { \mathrm { t e } }$ . This assumption indicates that the subpopulation is introduced by the association between $s$ and $y$ , or $x$ and $y$ , but not solely by $y$ itself. Since the minority group $m _ { 0 }$ shares same data statistics as $y$ , it is natural to have the equality $p ( y | I _ { \mathrm { t e } } ) = p ( y | m _ { 0 } , I _ { \mathrm { t r } } )$ . It is noteworthy that there is no constraint on the number of groups specified by $m$ . The size of 2 is considered in this paper due to its simplicity and high performance in practice (see Sec. 5).

Assumption 5, on the other hand, quantifies explicitly that there is a portion (i.e. $m _ { 1 }$ ) of samples in ${ \mathcal { D } } _ { \mathrm { t r } }$ whose attributes $s$ are identical to the labels $y$ . Rather than treating it as an assumption, it is more of a characterization on the subpopulation that widely presents in the real-world data (e.g., Waterbirds and ColorMNIST, or others described in (Yang et al. 2023)), where attributes strongly mislead the model prediction by such correlation.

Assumption 5. $p ( s | y , m _ { 1 } , I _ { t r } ) = \mathbf { 1 } _ { \{ y = s \} }$ , where $\scriptstyle \mathbf { 1 } _ { \{ y = s \} }$ is the indicator function.

With all ingredients, we propose the following theorem on the modeling objective:

Theorem 1. Given Assumption 1, 2, 3, 4, and 5 hold, the optimization of Eq. (4) with the following weight function $g ( x , y , I _ { t r } , I _ { t e } )$ directly maximizes the testing performance:

$$
\begin{array} { r l } & { \quad g ( x , y , I _ { t r } , I _ { t e } ) ^ { - 1 } : = p ( m _ { 0 } | I _ { t r } ) } \\ & { + \frac { p \left( m _ { 1 } | I _ { t r } \right) \cdot \frac { L } { p \left( y | I _ { t r } \right) } \cdot p \left( y | m _ { 1 } , I _ { t r } \right) } { 1 + \left[ \frac { p \left( m _ { 0 } | I _ { t r } \right) \cdot p \left( y | I _ { t r } \right) / L + p \left( y | m _ { 1 } , I _ { t r } \right) } { p \left( m _ { 0 } | I _ { t r } \right) \cdot p \left( y | I _ { t r } \right) / L } \right] \cdot \frac { 1 - p \left( s = y | y , x , I _ { t r } \right) } { p \left( s = y | y , x , I _ { t r } \right) } } , } \end{array}
$$

where $\begin{array} { r } { p ( y | m _ { 1 } , I _ { t r } ) = \frac { p ( y | I _ { t r } ) - p ( m _ { 0 } | I _ { t r } ) \cdot p ( y | I _ { t r } ) } { p ( m _ { 1 } | I _ { t r } ) } } \end{array}$ . $p ( m _ { 0 } | I _ { t r } )$ and $p ( m _ { 1 } | I _ { t r } )$ represent the probability of a binary random variable m taking the value $m _ { 0 }$ or $m _ { 1 }$ , respectively. Namely, the random variable m denotes the split of $\mathcal { D } _ { t r }$ into the majority and minority groups.

The corresponding proof can be found in Appendix. With this formulation, $p ( \bar { s } \dot { = } y | y , x , I _ { \mathrm { t r } } )$ is the only unknown term to be estimated. Theorem 1 provides a closed form objective with which models trained with ${ \mathcal { D } } _ { \mathrm { t r } }$ perform optimally on ${ \mathcal { D } } _ { \mathrm { t e } }$ . In the following, we consider 3 different setups on the accessibility of $s$ and the relationship between ${ \mathcal { D } } _ { \mathrm { t r } }$ and $\mathcal { D } _ { \mathrm { v a } }$ In each setup, we provide a method to estimate Eq. (5). We further showcase the performance of the proposed methods in the experiment section (Sec. 5). In Appendix, we include a discussion on the limitations of this approach concerning the restriction and possible relaxation of the assumptions.

# 3.2 Dataset Bias Correction Method

In this section, we provide 3 different approaches to estimate $E q$ . (5). We summarize these approaches with a general name: dataset bias correction method (DBCM). The 3 approaches essentially provide different ways of estimating the only missing term $p ( s | y , x , I _ { \mathrm { t r } } )$ in Eq. (5). Once the term is estimated, we employ a universal algorithm (see Algorithm 1) to train the model with ${ \mathcal { D } } _ { \mathrm { t r } }$ . To facilitate the use of this approach in more real-world applications, we describe the scenarios where the three following approaches can be applied in Appendix.

Attribute $s$ is Known When we have access to the attribute $s$ , we can make a direct estimation on the only unknown term $p ( s = y | y , x , I _ { \mathrm { t r } } )$ using the data $( x , y , s ) \in \mathcal { D } _ { \operatorname { t r } }$ and apply Algorithm 1 therein. As $p ( s = y | y , x , I _ { \mathrm { t r } } )$ increases, the weight function $g$ decreases (see Eq. (5)), because stronger spurious correlations make $p ( s = y | y , x , I _ { \mathrm { t r } } )$ larger. Down-weighting these samples during training helps performance by reducing reliance on spurious correlations.

Attribute $s$ is Unknown and $\mathcal { D } _ { \mathbf { t r } } \sim \mathcal { D } _ { \mathbf { v a } }$ When we do not have access to the attribute $s$ and $\mathcal { D } _ { \mathrm { t r } } \sim \mathcal { D } _ { \mathrm { v a } }$ , we propose to use the following term to estimate $p ( s | y , x , I _ { \mathrm { t r } } )$ :

$$
\begin{array} { r l } & { \hat { p } ( s = y \vert y , x , I _ { \mathrm { t r } } ) } \\ & { \propto \exp \left( \frac { \displaystyle \vert \log \hat { p } ( y \vert x , I _ { \mathrm { t r } } ) - \log \hat { p } ( y \vert x , I _ { \mathrm { v a } } ) \vert } { \tau } \right) ^ { - 1 } , } \end{array}
$$

where $\hat { p } ( y | x , I _ { \mathrm { t r } } )$ and $\hat { p } ( y | x , I _ { \mathrm { v a } } )$ are the predictive models learned with ${ \mathcal { D } } _ { \mathrm { t r } }$ and $\mathcal { D } _ { \mathrm { v a } }$ , respectively. And $\tau$ is the temperature hyperparameter. In practice, we find $\tau = 1$ consistently produces good results. We explicitly introduce $\tau$ to allow flexibility in the control of the estimation in Eq. (6). Specifically, we first overfit two independent predictive models on both ${ \mathcal { D } } _ { \mathrm { t r } }$ and ${ { \mathcal { D } } _ { \mathrm { { v a } } } }$ and then measure the difference on the two approximate laws with the training data. Note that $p ( s = y | y , x , I _ { \mathrm { t r } } )$ captures how likely $s$ shares the same label as $y$ , which is the only unknown term evaluated in Eq. (5). Therefore, we do not need to recover the full distribution $p ( s | y , x , I _ { \mathrm { t r } } )$ . Instead, we only need to quantify $\hat { p } ( s = y | y , x , I _ { \mathrm { t r } } )$ —“how likely the bias is biased towards the true label $y$ .” This is captured by Eq. (6), as if two models (trained separately on training and validation data) produce similar likelihoods (i.e. the difference in Eq. (6) is smaller) on a given input, then the input must associate with the attribute $s$ that is same as $y$ . To summarize this approach in one line: two overfitted models act as a bias corrector!

Attribute $s$ is Unknown and $\mathcal { D } _ { \mathbf { t r } } \ \sim \ \mathcal { D } _ { \mathbf { v a } }$ On the other hand, when $\mathcal { D } _ { \mathrm { t r } } \nsim \mathcal { D } _ { \mathrm { v a } }$ , we cannot utilize the predictive model estimated with $\mathcal { D } _ { \mathrm { v a } }$ . Instead, we propose to use the following term as an alternative,

$$
\hat { p } ( s = y | y , x , I _ { \mathrm { t r } } ) \propto \exp \left( - \frac { \log \hat { p } ( y | x , I _ { \mathrm { t r } } ) } { \tau } \right) ^ { - 1 } .
$$

This is according to the observation that machine learning models tend to learn the correlated attributes $s$ with $y$ easily (Asgari et al. 2022). In our case, we simply use $\hat { p } ( y | x , I _ { \mathrm { t r } } )$ as the proxy to characterize such correlation. In this case, samples with high accuracy should be down-weighted, as the model easily learns spurious correlations.

# 3.3 Choose Models

Similarly, we discuss different approaches for choosing a model. Unlike conventional methods that consistently use $\mathcal { D } _ { \mathrm { v a } }$ to decide which model to choose, we propose to consider different ways for choosing a model when relationships between $\mathcal { D } _ { \mathrm { v a } }$ and ${ \mathcal { D } } _ { \mathrm { t e } }$ are different. When $\mathcal { D } _ { \mathrm { v a } } \sim \mathcal { D } _ { \mathrm { t e } }$ , according to Eq. (3), $z ( x , y , I _ { \mathrm { v a } } , I _ { \mathrm { t e } } ) = 1$ . This indicates that evaluating models on validation set is equivalent to evaluating on the test set, which corresponds to the conventional approach. However, things change when $\mathcal { D } _ { \mathrm { v a } } \sim \mathcal { D } _ { \mathrm { t e } }$ . This suggests that ${ { \mathcal { D } } _ { \mathrm { { v a } } } }$ is not sufficient in measuring the model performance for the test set as $z ( x , y , I _ { \mathrm { v a } } , I _ { \mathrm { t e } } ) \neq 1$ In this case, we can adopt the similar approach outlined in Sec. 3.2 to estimate $z ( x , y , I _ { \mathrm { v a } } , I _ { \mathrm { t e } } )$ , which focuses on ${ { \mathcal { D } } _ { \mathrm { { v a } } } }$ and ${ \mathcal { D } } _ { \mathrm { t e } }$ , rather than ${ \mathcal { D } } _ { \mathrm { t r } }$ and $\mathcal { D } _ { \mathrm { t e } }$ .

# 4 DBA Interpretation on Existing Work

In this section, we showcase how some representative existing works can be related to the DBA framework. Such discussion should complement the existing survey papers on subpopulation/distributional shifts and provide insights on the methodological development in the future. We follow the previously introduced categorization.

<html><body><table><tr><td>Algorithm 1 The universal algorithm for optimizing q(y|x,Itr).</td></tr><tr><td>Input The initialized model q(y|x,Itr); dataset Dtr; The esti- mation p(sly, x,Itr).</td></tr><tr><td>Output: the optimized q(y|x,Itr). 1: Obtain g(x,y,Itr,Ite) given p(s = yly,x,Itr) (see</td></tr><tr><td>Eq. (5)). 2:Perform the following optimization using Dtr:</td></tr><tr><td>max E(x,y)~p(x,ylIr)[g(x,y,Itr,Ite)logq(y|x,Itr)].(8) q∈Mtr</td></tr></table></body></html>

The model objective class: Liu et al. (2021) and Nam et al. (2020) can be viewed as proposing different forms of the $p ( s | y , x , I _ { \mathrm { t r } } )$ estimation, where the former utilizes the classification accuracy and the latter considers generalized cross-entropy. Sec. 3.2 and 3.2 provide rationale on the validity of these terms—essentially they characterize the probability $p ( s = y | y , x , I _ { \mathrm { t r } } )$ . Besides the variants of $\hat { p } ( s = y | y , x , I _ { \mathrm { t r } } )$ , they propose different training schemes to correct. Liu et al. (2021) subsamples the training set with their $\hat { p } ( s = y | y , x , I _ { \mathrm { t r } } )$ and Nam et al. (2020) proposes a parallel model to reweight samples according to $\hat { p } ( s = y | y , x , I _ { \mathrm { t r } } )$ from the generalized cross entropy. Nonetheless, none of them is alike DBCM, which is statistically consistent in directly improving the testing performance.

The data sampling class: The methods in the data sampling class share great similarity to ours, as the proposed DBCM is essentially an importance sampling (reweighing) mechanism. ReWeight and ReSample (Japkowicz 2000) can be treated as variants of the sampling technique. Precisely, ReWeight adjusts each sample weight according to the class ratio, in order to recover the class-balanced setup. Similarly, ReSample bootstraps the dataset with class-balanced weights. Essentially, they can be treated as the direct estimation of $\begin{array} { r } { g ( x , y , I _ { \mathrm { t r } } , I _ { \mathrm { t e } } ) \ \stackrel { \cdot } { = } \ \frac { p ( x , y | I _ { \mathrm { t e } } ) } { p ( x , y | I _ { \mathrm { t r } } ) } } \end{array}$ p(x,y|Itter) , assuming p(y|Itr) is uniform. When considering the presence of attribute $s$ , $g ( x , y , I _ { \mathrm { t r } } , I _ { \mathrm { t e } } )$ becomes,

$$
g ( x , y , I _ { \mathrm { t r } } , I _ { \mathrm { t e } } ) : = \frac { p ( x , y | I _ { \mathrm { t e } } ) } { p ( x , y | I _ { \mathrm { t r } } ) } = \frac { \sum _ { s } p ( x | y , s , I _ { \mathrm { t e } } ) p ( y , s | I _ { \mathrm { t e } } ) } { \sum _ { s } p ( x | y , s , I _ { \mathrm { t r } } ) p ( y , s | I _ { \mathrm { t r } } ) } .
$$

Their setups, in this case, further assume $p ( y , s | I _ { \mathrm { t e } } )$ is uniform and $\bar { p } ( y , s | I _ { \mathrm { t r } } ) = p ( y , s | I _ { \mathrm { t e } } )$ , which is a stronger assumption compared to the proposed.

The auxiliary loss class: Tsirigotis et al. (2024) and Menon et al. (2021) are commonly used logit adjustment methods. With the DBA framework, they can be viewed as a two-step method. First, both methods propose an estimation of $p ( \bar { y } , s = y | , x , I _ { \mathrm { t r } } )$ . Then the estimates are used as a penalty term to regularize the ERM of the predictive model $q ( y | x , I _ { \mathrm { t r } } )$ . In the first step, Tsirigotis et al. (2024) applies a similar approach to one described in (Liu et al. 2021). Both share conceptual similarity to the DBCM variant in Sec. 3.2. Menon et al. (2021), on the other hand, simply enforces the uniform class balance assumption. Once $\hat { p } ( y , s = y | x , I _ { \mathrm { t r } } )$ is obtained, they optimize w.r.t.

$$
\mathbb { E } _ { ( x , y ) \sim p ( x , y | I _ { \mathrm { t r } } ) } [ \log q ( y | x , I _ { \mathrm { t r } } ) + \log \hat { p } ( y , s = y | x , I _ { \mathrm { t r } } ) ] .
$$

To compare the difference between Eq. (10) and the optimal objective (Eq. (4)), we prove the following theorem with two additional assumptions on the label $y$ .

Assumption 6. The label y given $I _ { t r }$ follows a uniform distribution.

Assumption 7. The training set contains only the dominant group $m _ { 1 }$ : $p ( m _ { 1 } | I _ { t r } ) = 1$ .

Theorem 2. Given Assumption $I , 2 , 3 \ 6 ,$ , and 7 hold, the optimization of Eq. (4) with the following weight function $g ( x , y , I _ { t r } , I _ { t e } )$ directly maximizes the testing performance:

$$
g ( x , y , I _ { t r } , I _ { t e } ) ^ { - 1 } : = L \cdot p ( y , s = y | x , I _ { t r } ) .
$$

And the objective Eq. (4) is of form:

$$
\begin{array} { r l } & { \mathbb { E } _ { ( x , y ) \sim p ( x , y | I _ { t r } ) } \left[ g ( x , y , I _ { t r } , I _ { t e } ) \big ( \log q ( y | x , I _ { t r } ) \right. } \\ & { ~ + \log p ( y , s = y | x , I _ { t r } ) \big ) } \\ & { ~ \left. + g ( x , y , I _ { t r } , I _ { t e } ) \log L \cdot g ( x , y , I _ { t r } , I _ { t e } ) \right] . } \end{array}
$$

The proof is in Appendix. Eq. (10) differs Eq. (12) by 2 aspects. First, Eq. (10) ignores the weight function $\dot { g ( x , y , I _ { \mathrm { t r } } , I _ { \mathrm { t e } } ) }$ before the summation. Second, the regularization $g ( x , y , I _ { \mathrm { t r } } , I _ { \mathrm { t e } } ) \log L \cdot g ( x , y , I _ { \mathrm { t r } } , I _ { \mathrm { t e } } )$ in Eq. (12) is missing. Without these terms, Eq. (12) is not guaranteed to optimize for a class-balance dataset, as indicated in (Tsirigotis et al. 2024; Menon et al. 2021). Consequently, these methods may underperform.

The augmentation class: Despite existing works provide augmentation techniques in the form of linear combination (Zhang et al. 2018; Yao et al. 2022; Han et al. 2022), none of the papers provides statistical interpretation on why such techniques work better than ERM. We see our DBA framework as the first to provide support for the soundness of the augmentation technique. In short, the augmentation to combine data samples can be viewed as variations of the direct recovery of $\begin{array} { r } { \dot { g ( x , y , I _ { \mathrm { t r } } , I _ { \mathrm { t e } } ) } : = \frac { p ( x , y | I _ { \mathrm { t e } } ) } { p ( x , y | I _ { \mathrm { t r } } ) } } \end{array}$ under a different set of assumptions. Specifically, we provide the following Theorem 3 to support this statement. The proof can be found in Appendix. In the following theorem, $m _ { 0 }$ and $m _ { 1 }$ are identical to the terms introduced in Theorem 1. We first describe the assumptions.

Assumption 8. The data generator of $\mathcal { D } _ { t r }$ are conditionally identical given different group information $m$ : $p ( x | m _ { 0 } , I _ { t r } ) = p ( x | m _ { 1 } , I _ { t r } ) = p ( x | I _ { t r } )$ .

Assumption 9. The predictive model on $\mathcal { D } _ { t e }$ shares the same law with the model that is conditioned on the group $m _ { 0 }$ for $\begin{array} { r } { \mathsf { J } _ { t r } \colon p ( y | x , I _ { t e } ) = p ( y | x , m _ { 0 } , I _ { t r } ) } \end{array}$ .

Note that Assumption 9 is conceptually similar to the setup for Theorem 1.

Theorem 3. Given Assumption $I , \delta _ { ; }$ , and $^ { g }$ hold, the weight function $g ( x , y , I _ { t r } , I _ { t e } )$ has the following form:

$$
\begin{array} { r l } & { g ( x , y , I _ { t r } , I _ { t e } ) ^ { - 1 } : = \lambda _ { 0 } ( x , I _ { t r } , I _ { t e } ) \cdot p ( x | I _ { t r } ) } \\ & { ~ + \lambda _ { 1 } ( x , y , I _ { t r } , I _ { t e } ) \cdot p ( x | I _ { t r } ) . } \end{array}
$$

$\begin{array} { r l r } { \lambda _ { 0 } ( x , I _ { \mathrm { t r } } , I _ { \mathrm { t e } } ) } & { { } : = } & { \frac { p ( m _ { 0 } | I _ { \mathrm { t r } } ) } { p ( x | I _ { \mathrm { t e } } ) } } \end{array}$ and $\begin{array} { r l } { \lambda _ { 1 } ( x , y , I _ { \mathrm { t r } } , I _ { \mathrm { t e } } ) } & { { } : = } \end{array}$ p(m1|Itr)p(y|x,m1,Itr) . This means the weight function $g ( x , y , I _ { \mathrm { t r } } , I _ { \mathrm { t e } } )$ is a reweighing of the original $p ( x | I _ { \mathrm { t r } } )$ . The commonly used augmentation can be viewed as a samplelevel adjustment to the weight function. From Theorem 3 we know that the sum of $\lambda _ { 0 }$ and $\lambda _ { 1 }$ need not be 1, which is different from some existing augmentation approaches (Zhang et al. 2018; Yao et al. 2022) 1. The theorem also offers statistical rationale on why the weighted linear combination works (Han et al. 2022). Since both $\lambda _ { 0 }$ and $\lambda _ { 1 }$ depend on the data statistics from the testing set, methods that utilize sample-independent coefficient (Zhang et al. 2018; Yao et al. 2022) should experience degraded performance. We believe this provides insights into the advancement of augmentationbased techniques in the future.

# 5 Experiment

We compare different DBCM variants (see Sec. 3.2) benchmarking models with three benchmarking datasets. We showcase the SOTA performance of our models, to demonstrate the consistency of the theory developed in Sec. 3. In addition, we provide experimental evidence that complements the theory on explaining why existing works would sacrifice average accuracy for higher worst group accuracy.

# 5.1 Experimental Setup

To ensure a fair comparison, we consider models and datasets prepared by Yang et al. (2023). Specifically, we consider two vision datasets: Waterbirds (Sagawa et al. 2020) and ColorMNIST (Nam et al. 2020; Tsirigotis et al. 2024), and one language dataset: CivilComments (Borkan et al. 2019), in order to cover the two popular data types. We modify the ColorMNIST dataset such that it aligns with the setup in Tsirigotis et al. (2024), which is a harder setup. This is because the vanilla version in Yang et al. (2023) consists of only two types of attributes, whereas the version in Nam et al. (2020); Tsirigotis et al. (2024) contains 10 attributes. The modified ColorMNIST contains a “ratio” indicator that specifies the portion of samples that do not correlate with labels and attributes. In our experiment, we consider ratios $2 \%$ and $0 . 5 \%$ , as they are the intermediate and the hardest setups. In practice, we also treat $p ( m _ { 1 } | I _ { \mathrm { t r } } )$ and $p ( m _ { 0 } \vert I _ { \mathrm { t r } } )$ serve as prior knowledge/hyperparameters of training composition. Specifically for ColorMNIST, where spurious sample ratio is known, we directly assign $0 . 5 \%$ or $2 \%$ for $p ( \bar { m } _ { 0 } \vert I _ { \mathrm { t r } } )$ (i.e., $1 - p ( m _ { 1 } | I _ { \mathrm { t r } } ) )$ . When the composition ratio is unknown, $p ( m _ { 0 } \vert I _ { \mathrm { t r } } )$ is treated as a hyperparameter and empirically we identify $p ( m _ { 0 } | I _ { \mathrm { t r } } ) = 0 . 8 5$ performed well across datasets.

The evaluation consists of 8 benchmarking models from Yang et al. (2023) that fall into the 4 different classes (see Sec. 1 and 4): Mixup (Zhang et al. 2018); LISA (Yao et al. 2022); JTT (Liu et al. 2021); Focal Loss (Lin et al.

<html><body><table><tr><td rowspan="2"></td><td colspan="2">ColorMNIST(0.5%)</td><td colspan="2">ColorMNIST(2%)</td><td colspan="2">Waterbirds</td><td colspan="2">CivilComments</td><td rowspan="2">Availability of s</td></tr><tr><td>average</td><td>worst</td><td>average</td><td>worst</td><td>average</td><td>worst</td><td>average</td><td>worst</td></tr><tr><td>ERM</td><td>81.69 ± 0.10</td><td>1.14 ± 0.40</td><td>95.23 ± 0.07</td><td>56.82 ± 0.23</td><td>88.25 ±0.16</td><td>67.76 ± 0.30</td><td>87.59 ± 0.38</td><td>48.17 ± 2.61</td><td rowspan="8"></td></tr><tr><td>Mixup (Zhang et al.2018)</td><td>81.12 ± 2.20</td><td>0.00 ±0.00</td><td>96.09 ±0.20</td><td>80.00± 2.22</td><td>88.52 ± 0.22</td><td>59.97 ± 2.01</td><td>87.67 ± 0.12</td><td>53.10 ± 2.11</td></tr><tr><td>LISA(Yao et al. 2022)</td><td>89.45 ± 1.57</td><td>21.50 ± 8.51</td><td>97.32 ± 0.37</td><td>87.27 ± 6.55</td><td>93.63± 0.66</td><td>76.95 ± 4.25</td><td>87.22 ± 0.13</td><td>40.62 ± 4.32</td></tr><tr><td>JTT (Liu et al. 2021)</td><td>81.98 ± 1.17</td><td>2.00 ±0.08</td><td>95.03 ± 0.10</td><td>56.82 ± 2.21</td><td>88.32 ±0.20</td><td>68.80 ± 2.99</td><td>87.78 ± 0.29</td><td>47.06 ± 2.94</td></tr><tr><td>FroaD(igtal.272020)</td><td></td><td>9.00± 0.00</td><td>95.69 ± .25</td><td>43.91 ± 2.33</td><td>97.75±0.36</td><td>54.67 ± 2.67</td><td></td><td></td></tr><tr><td></td><td>67.37±0.4</td><td></td><td></td><td></td><td></td><td></td><td>87.74± 0.16</td><td>43.73 ± 3.6</td></tr><tr><td>MMD (Li et al.2018)</td><td>11.35 ± 1.30</td><td>0.00 ±0.00</td><td>11.35 ± 2.26</td><td>0.00±0.00</td><td>88.33 ± 0.51</td><td>53.58 ± 2.38</td><td>82.08 ± 0.63</td><td>0.00 ±0.00</td></tr><tr><td>ReSample (Japkowicz 2000)</td><td>94.95± 0.19</td><td>66.37 ± 2.33</td><td>98.34 ±0.23</td><td>92.00 ± 1.66</td><td>93.72 ± 0.22</td><td>80.69 ± 1.86</td><td>84.59 ± 1.23</td><td>62.17 ± 1.72</td><td rowspan="2"></td></tr><tr><td>ReWeight (Japkowicz 2000)</td><td>92.43 ± 0.21</td><td>57.84 ± 1.78</td><td>97.83 ± 0.19</td><td>91.46 ± 1.80</td><td>93.86±0.30</td><td>81.15 ± 2.20</td><td>87.04 ± 0.74</td><td>58.27 ± 2.14</td></tr><tr><td>DBCM(Sec.3.2, known s)</td><td>96.67 ± 0.27</td><td>84.62 ± 2.02</td><td>98.76 ±0.20</td><td>92.31 ± 1.73</td><td>94.01 ± 0.19</td><td>83.18 ± 2.00</td><td>87.85± 0.15</td><td></td><td>43.33 ± 2.15</td></tr><tr><td>ERM</td><td>81.69 ± 0.10</td><td>1.14 ± 0.40</td><td>95.23 ± 0.07</td><td>56.82 ± 0.23</td><td>88.25 ± 0.16</td><td>67.76 ± 0.30</td><td>87.59 ± 0.38</td><td>48.17 ± 2.61</td><td rowspan="8"></td></tr><tr><td>Mixup (Zhang et al.2018)</td><td>81.03 ± 2.30</td><td>0.00 ±0.00</td><td>95.26 ± 0.17</td><td>42.05 ± 3.61</td><td>90.65 ±0.30</td><td>67.29 ± 1.93</td><td>87.48 ± 0.11</td><td>54.84 ± 2.13</td></tr><tr><td>LISA(Yao et al. 2022)</td><td>68.09 ± 2.06</td><td>0.00 ±0.00</td><td>94.46 ± 0.53</td><td>15.91 ± 13.11</td><td>89.80 ± 1.11</td><td>66.82 ± 3.87</td><td>87.18 ±0.28</td><td>49.21 ± 2.11</td></tr><tr><td>JTT (Liu et al. 2021)</td><td>81.80 ± 0.19</td><td>2.00 ±0.08</td><td>95.42 ± 0.02</td><td>48.86 ± 1.85</td><td>88.83 ±0.28</td><td>66.36 ± 3.10</td><td>87.78 ± 3.84</td><td>47.06 ± 8.09</td></tr><tr><td>Focal Loss (Lin et al. 2017)</td><td>67.12 ± 0.50</td><td>0.00 ±0.00</td><td>94.53 ± 0.32</td><td>37.00 ± 4.10</td><td>89.92 ± 0.43</td><td>61.68 ± 3.01</td><td>87.74 ± 0.12</td><td>50.08± 4.10</td></tr><tr><td>ReSample (Japkowicz 2000)</td><td>81.55 ± 0.21</td><td>0.00 ±0.00</td><td>95.70 ± 0.15</td><td>65.00 ± 1.63</td><td>87.99 ± 0.12</td><td>64.17 ± 1.98</td><td>83.24 ± 1.73</td><td>68.91 ± 4.51</td></tr><tr><td>ReWeight (Japkowicz 2000)</td><td>76.77 ± 0.37</td><td>0.00 ± 0.00</td><td>94.93 ± 0.08</td><td>54.55 ± 0.10</td><td>87.81 ± 0.18</td><td>67.60 ± 1.67</td><td>87.02 ±1.12</td><td>58.73 ± 4.60</td></tr><tr><td>DBCM(Sec. 3.2,D~Dva）</td><td>94.63 ± 0.35</td><td>57.95 ± 2.30</td><td>97.64 ± 0.10</td><td>81.00 ± 1.40</td><td>88.25 ± 0.05</td><td>70.56 ± 0.12</td><td>87.86± 0.30</td><td></td></tr><tr><td>DBCM(Sec.3.2,DrDva)</td><td>86.12 ± 0.29</td><td>3.41 ±0.70</td><td></td><td>96.08 ±0.20</td><td>61.36 ± 1.90</td><td>91.04 ± 0.07</td><td>62.77 ± 0.10</td><td>87.62 ± 0.20</td><td>43.41 ± 2.20 53.89 ± 2.16</td><td></td></tr></table></body></html>

Table 1: Results on the three benchmarking datasets with both cases where $s$ is known and unknown, respectively. We report both average and worst group accuracy, with mean and standard deviation $( ^ { 6 6 } \pm 7 )$ for each of the considered methods after 3 independent runs. The boldfaced values indicate the highest accuracy in comparison.

2017); GroupDRO (Sagawa et al. 2020); MMD (Li et al. 2018); ReSample (Japkowicz 2000); ReWeight (Japkowicz 2000). For each model, we consider two setups, where the first allows the presence of attributes and the second does not. We retrain all the considered models from Yang et al. (2023) and pick the best models according to the average validation accuracy, which is different from the worst-group-accuracy criterion in Yang et al. (2023) to match the objective in Eq. 4 (i.e. the framework considers on average accuracy by design). For model optimization, we consider default optimizers and learning rates in Yang et al. (2023). Details are in Appendix.

Attribute $s$ is Known This section presents results with accessible attribute $s$ . In addition to the 8 benchmarking models, we also include results with ERM as the baseline. We consider the DBCM variant in Sec. 3.2. Results are summarized in the upper half of Table 1. It is clear that when the attribute $s$ presents, the proposed DBCM(Sec. 3.2, known $s$ ) achieves the highest average accuracy among all the considered datasets. And all the accuracy of our model exceeds the ERM baseline. This provides the empirical evidence for Theorem 2 and 1. Although there is no theoretical quantification on the worst group accuracy, DBCM achieves two highest and one competing (i.e., Waterbirds) worst group accuracy.

Attribute $s$ is Unknown This section presents results without accessing the attribute $s$ . We omit results for GroupDRO (Sagawa et al. 2020), MMD (Li et al. 2018) as both methods naturally require the knowledge of $s$ (Yang et al. 2023). DBCM(Sec. 3.2, $\mathcal { D } _ { \mathrm { t r } } \sim \mathcal { D } _ { \mathrm { v a } } )$ ) and DBCM(Sec. 3.2, $\mathcal { D } _ { \mathrm { t r } } \sim \mathcal { D } _ { \mathrm { v a } } )$ are two variants of the proposed method. Results are summarized in the lower half of Table 1. We observe that the proposed DBCM variants achieve the highest average accuracy among all the compared datasets, and 3 out of 4 highest worst group accuracy, suggesting the validity of the methods when $s$ is unknown. And in the case of the worst group accuracy for ColorMNIST $( 0 . 5 \% )$ , almost all but DBCM cannot correctly classify the worst group samples (i.e.

worst group accuracy $\mathbf { \epsilon } = 0$ ), suggesting that DBCM method is robust to the change of spurious association between the attributes and the labels.

# 5.2 Observation on the Degraded Average Accuracy

From Table 1 we empirically identify an interesting phenomenon—compared to all other methods, DBCM is the only model that consistently outperforms the results of ERM. This observation is aligned with Yang et al. (2023); Tsirigotis et al. (2024). Yet the previous work did not provide systematic reasoning on why. We argue that the cause is an incorrect model objective that is different from the data composition in ${ \mathcal { D } } _ { \mathrm { t e } }$ . Specifically, the reduced average accuracy is the result of the misspecified $p ( x , y \vert I _ { \mathrm { t e } } )$ in $g ( \overline { { x } } , y , I _ { \mathrm { t r } } , I _ { \mathrm { t e } } )$ . For the full explanation, please refer to Appendix. It is noteworthy that we are the first to provide such a statistical interpretation of the degradation phenomenon.

# 6 Conclusion

In summary, we present the DBA framework to identify the true model objective that improves the test performance. The paper proposes different DBCM variants with weaker assumptions compared to the existing works and demonstrates the SOTA performance. Additionally, we reinterpret the existing work with the proposed framework, which explains the issue of the degraded average accuracy. With the analysis, we convey a message that to achieve decent test performance (even without the access to test during training), one must comprehensively investigate the relationship between those datasets and the model objective. For this purpose, we hope the proposed framework could act as a complementary tool to all the existing work, help people analyze such gaps, and facilitate the development of the corresponding model solutions.