# TimePFN: Effective Multivariate Time Series Forecasting with Synthetic Data

Ege Onur Taga, Muhammed Emrullah Ildiz, Samet Oymak

University of Michigan, Ann Arbor egetaga@umich.edu, eildiz@umich.edu, oymak@umich.edu

# Abstract

The diversity of time series applications and scarcity of domain-specific data highlight the need for time-series models with strong few-shot learning capabilities. In this work, we propose a novel training scheme and a transformer-based architecture, collectively referred to as TimePFN, for multivariate time-series (MTS) forecasting. TimePFN is based on the concept of Prior-data Fitted Networks (PFN), which aims to approximate Bayesian inference. Our approach consists of (1) generating synthetic MTS data through diverse Gaussian process kernels and the linear coregionalization method, and (2) a novel MTS architecture capable of utilizing both temporal and cross-channel dependencies across all input patches. We evaluate TimePFN on several benchmark datasets and demonstrate that it outperforms the existing state-of-the-art models for MTS forecasting in both zero-shot and few-shot settings. Notably, fine-tuning TimePFN with as few as 500 data points nearly matches full dataset training error, and even 50 data points yield competitive results. We also find that TimePFN exhibits strong univariate forecasting performance, attesting to its generalization ability. Overall, this work unlocks the power of synthetic data priors for MTS forecasting and facilitates strong zero- and few-shot forecasting performance.

# Code — https://github.com/egetaga/TimePFN

# 1 Introduction

Natural language processing has achieved remarkable success driven by advances in neural architectures and data pipelines. These advances underlie modern language and vision-language models that exhibit remarkable zero-shot and few-shot learning capabilities. Inspired by these, researchers have started exploring whether such methods and ideas could be extended to time series forecasting. For instance, a notable line of work (Zhou et al. 2021; Wu et al. 2021; Zhou et al. 2022; Zhang and Yan 2023) examine the use of transformer architecture (Vaswani et al. 2017) in timeseries forecasting. More recently, there is also a push toward building foundation models for time series tasks (Ansari et al. 2024). However, the heterogeneous nature of time series data brings additional complications. As shown by (Zeng et al. 2023), even simple linear models are shown to outperform most existing transformer-based models in univariate and multivariate time-series forecasting. This could be attributed to the heterogeneous nature of time-series data and relatively naive tokenization methods, underscoring the need for richer datasets as well as more effective architectures that can capture both temporal and cross-channel dependencies.

![](images/0a4f6d9a4173260280ed36e9841bde6e154f2e646f07647defe0ca11a69a5142.jpg)  
Figure 1: Average forecasting performance (MSE) of TimePFN. MSE values are averaged over all data budgets.

In language models, the discrete nature of the problem makes the tokenization fairly straightforward, which is in contrast to the continuous time series data. Additionally, the scalar value of a time series datapoint have no clear meaning, unlike words, where vector embeddings can capture semantic similarity. To address these problems, PatchTST (Nie et al. 2023) proposed using patching with overlapping strides and demonstrated its benefit for univariate forecasting. While PatchTST treats multivariate forecasting as multiple univariate problems, iTransformer (Liu et al. 2023) proposes representing each channel as a single token, resulting in an architecture that intuitively augments simple linear layers with a transformer architecture.

In this work, we approach MTS forecasting from a data

Var 1 Var 1 Var 2 Var 2 time time Transformer Convolutional Filtering + FFN Encoder Var 1   
Var1 o Conv1 Patch   
Var1 o Conv2 Patch Embeddings Var 2 Embeddings Positional   
Var2 o Conv1   
Var2 o Conv2 Embeddings time tokens

centric perspective. While various architectural considerations have been incorporated into the forecasting process, we argue that the data aspect is relatively underappreciated. Existing transformer-based MTS approaches focus on the classical learning setup where a model is trained and tested on the same task. Although this often results in satisfactory performance for large datasets, it is likely to underperform in real-world applications where the training set is small or test set is out-of-distribution. This is especially so for modern sequence/transformer models that involve complex architectures and naturally require a substantial amount of data to operate at optimal performance.

Our approach TimePFN brings two key innovations: (1) Generating realistic and diverse large-scale multivariate time series data, where inter- and intra-channel dependencies are common, and (2) Developing an architecture capable of extracting time series features from this large-scale synthetic dataset. The architecture also allows for transfer learning to novel tasks with arbitrary number of channels. Overall, empowered by large amount of synthetic data (on the order of millions of samples), TimePFN facilitates state-of-the-art zero-shot and few-shot accuracy on benchmark datasets.

The strong zero-shot performance of our model, along with its superior performance in few-shot settings, supports the importance of the data-centric perspective. Evaluations demonstrate that our model, when fine-tuned on as few as 50 to 500 samples, is competitive with the performance of alternative methods trained on the entire dataset. More specifically, we make the following contributions:

• We present a new method to generate synthetic multivariate time series data using Gaussian processes with kernel compositions and a linear coregionalization model. • We propose a variation of PatchTST (Nie et al. 2023) for multivariate forecasting. Unlike PatchTST, our architecture incorporates channel mixing and employs a convolutional embedding module for patch embeddings. This allows it to effectively extract cross-channel relations and generate more representative embeddings, as demonstrated by experiments. • TimePFN is the first multivariate time-series PFN. No

Input: Number of variates $N$ , time-series length $T$ , Weibull   
shape parameter $\beta$ , Weibull scale parameter $\lambda$ , (min, max)   
value of dirichlet concentration parameter $( d _ { m i n } , d _ { m a x } )$ ,   
minimum number of latent functions $m$ , maximum number   
of kernel composition in KernelSynth $J$   
Output: Synthetic MTS $C$ with $N$ variates and length   
$T$ 1: $L \sim m a x ( m i n ( W e i b u l l ( \beta , \alpha ) , N ) , m )$ 2: $d \sim U ( d _ { m i n } , d _ { m a x } )$ 3: for $\mathrm { j } \in \{ 1 \ldots L \}$ do   
4: $l _ { j } ( t ) \gets K e r n e l S y n t h ( J , T )$ 5: end for 6: for $\mathbf { i } \in \{ 1 \ldots N \}$ do 7: $[ \alpha _ { i , 1 } \dots \alpha _ { i , L } ] \sim D i r ( d )$ 8: Ci(t) ← PjL=1 αi,jlj(t) 9: end for   
10: return $\{ C _ { i } ( t ) \} _ { i = 1 } ^ { N }$

tably, TimePFN demonstrates strong zero-shot and fewshot performance and consistently outperforms comparable models/methods across various benchmarks.

• We find that TimePFN also exhibits strong univariate forecasting performance, although it is explicitly trained with synthetic multivariate data. This attests to the flexibility and generalization capability of our approach.

# 2 Related Work

Transformers (Vaswani et al. 2017) have revolutionized NLP, significantly advancing zero-shot and few-shot capabilities in language and vision models. This has led researchers to explore the application of transformers to timeseries forecasting, leading to a substantial body of work including but not limited to (Zhou et al. 2021; Wu et al. 2021; Zhou et al. 2022; Li et al. 2019; Nie et al. 2023; Liu et al. 2022; Zhang and Yan 2023). Informer by (Liu et al. 2023) introduces the ProbSparse attention mechanism, which alleviates the quadratic complexity of the naive transformer to log-linear complexity, to mitigate the scalability issues in long sequence time-series forecasting (LSTF). (Zhou et al. 2022) uses the sparsity of the time-series in the fourier domain to enhance the performance in LSTF. PatchTST (Nie et al. 2023) uses patching with overlapping strides as a tokenization mechanism to address issues associated with naive tokenization of time-series data. This approach yields patch-based tokens that are interpretable while maintaining channel independence, treating each channel as univariate but facilitating joint learning across all channels through the same set of shared weights. Our architecture deviates from PatchTST by incorporating convolutional layers before patching and using channel-mixing to capture interactions between tokens from different channels. The advantages of convolutions are highlighted in the speech literature by (Baevski et al. 2020; Hsu et al. 2021). On the other hand, iTransformer (Liu et al. 2023) treats each variate as a single token, showing the potential benefits of utilizing inter

channel relationships.

In zero-shot forecasting, a line of work has emerged (Orozco and Roberts 2020; Oreshkin et al. 2021; Jin et al. 2022; Dooley et al. 2023; Ansari et al. 2024). More recently, (Ansari et al. 2024) has developed novel tokenization methods, employed quantization, and made time series data resemble language, enabling the training of LLM architectures for probabilistic univariate forecasting in a framework called Chronos. Chronos employs a data augmentation technique called KernelSynth, which generates synthetic timeseries data using Gaussian processes to improve the generalization capability of the model. Meanwhile, another line of work, ForecastPFN (Dooley et al. 2023), is trained entirely on a synthetic dataset following the framework of Priordata Fitted Networks (PFNs). Initially proposed by (Mu¨ller et al. 2022), PFNs are designed to approximate Bayesian inference. Another study (Verdenius, Zerio, and Wang 2024) integrates Joint-Embedding Predictive Architectures with PFNs for zero-shot forecasting. In addition to the mentioned models, Mamba4Cast (Bhethanabhotla et al. 2024) is also trained entirely on synthetic data using the Mamba architecture as its backbone (Gu and Dao 2024). While the mentioned literature addresses univariate settings, our work introduces the first multivariate Prior-data Fitted Network, to the best of our knowledge, featuring an architecture that enables strong zero-shot and few-shot performances on MTS forecasting.

# 3 Proposed Method

This work relies on two key aspects: a multivariate synthetic time series data generation mechanism that encapsulates inter- and intra-channel dependencies common across real time series data, and an architecture capable of generalization to real datasets when trained on such a dataset.

In the following section, we introduce the main concept behind our synthetic MTS data generation and training mechanism: Prior-data Fitted Networks (PFNs).

# Prior-data Fitted Networks for MTS Forecasting

Let $\mathcal { D } \ : = \ \{ t , X _ { t } \} _ { t = 1 } ^ { T }$ represent an N-channel multivariate time series data spanning a time horizon $T$ , where $X _ { t } : = [ x _ { t , 1 } , \dots , x _ { t , N } ]$ . Each $x _ { t , i }$ is potentially causally dependent on previous time steps and on one another. Given the data $\{ t , \bar { X } _ { t } \} _ { t = 1 } ^ { \tilde { T } }$ where $\smash { \widetilde { T } < T }$ , the task is to forecast $X _ { \widetilde { T } + 1 } , \dots , X _ { T }$ .  We tackle tehis problem using a Bayesian fra emework. Assuming a hypothesis space $\Omega$ with a prior distribution $p ( \omega )$ , each hypothesis $\boldsymbol \omega \in \Omega$ models a multivariate time series (MTS) generating process, i.e., $X _ { t } ~ = \omega ( t )$ . For example, $\Omega$ could represent the space of hypotheses for vector autoregression (VAR) models, where a particular instance $\boldsymbol { \omega } \in \Omega$ corresponds to a specific VAR process, such as VAR(2), and data $\mathcal { D }$ can be generated via this process. Now, given a data ${ \mathcal { D } } : = \{ t , X _ { t } \} _ { t = 1 } ^ { \bar { \widetilde { T } } }$ where $\tilde { T } < T$ , the posterior predictive distribution (PP D) of $\pmb { x } \in \mathbb { R } ^ { N }$ at time $T$ is $p ( \cdot \mid \bar { T } , \mathcal { D } )$ . By Bayes’ theorem,

$$
p ( \pmb { x } \mid T , \mathcal { D } ) \propto \int _ { \Omega } p ( \pmb { x } \mid T , \omega ) p ( D \mid \omega ) p ( \omega ) d \omega
$$

As shown by (Mu¨ller et al. 2022; Hollmann et al. 2023; Dooley et al. 2023), the posterior predictive distribution (PPD) is approximated using prior fitting networks (PFNs) as follows: We iteratively sample a hypothesis $\omega$ from the hypothesis space $\Omega$ according to the probability $p ( \omega )$ . Next, we generate a prior dataset $\mathcal { D }$ from this hypothesis, denoted as $\bar { \mathcal { D } } \sim p ( \mathcal { D } \mid \omega )$ . We then optimize the parameters of the PFN on these generated datasets using standard methods. Twhe rtie $\mathcal { D } _ { i n p u t } : = \{ t , X _ { t } \} _ { t = 1 } ^ { \tilde { T } }$ daend $\mathcal { D } _ { o u t p u t } : = \{ t , X _ { t } \} _ { t = \widetilde { T } + 1 } ^ { T }$ Subsequently, we train the PFN to forecast $\mathcal { D } _ { o u t p u t }$ ferom $\mathscr { D } _ { i n p u t }$ using standard time-series transformer training techniques, aiming to minimize the mean-squared error loss as our optimization objective, following the setting of (Dooley et al. 2023).

In our work, we define the hypothesis space $\Omega$ as consisting of single-input, multi-output Gaussian processes represented by the linear model of coregionalization (LMC) (Journel and Huijbregts 2003). Our choice is driven by the representational power of Gaussian processes and their ability to generate a diverse range of time series through the LMC framework.

# Synthetic MTS Data Generation

In synthetic MTS (multivariate time series) data generation, our goal is twofold. First, we strive to create variates that are realistic, exhibiting periodic patterns, trends, and other common features found in real-world data. Second, we aim for these variates to be correlated with one another, which better represents MTS data characteristics. Fortunately, our first goal is addressed by a method called KernelSynth. Chronos (Ansari et al. 2024) uses KernelSynth to enrich its training corpus by randomly composing kernels using binary operators (such as addition and multiplication) to generate diverse, univariate synthetic time-series data. This method is essentially the inverse of the kernel composition approach described in (Duvenaud et al. 2013), where kernel compositions are used for structure discovery in nonparametric regression. In contrast, KernelSynth focuses on generating realizations from these kernels. For example, combining a linear kernel with a periodic kernel results in a pattern that exhibits both a linear trend and sinusoidal seasonality. Similarly, multiplying a squared-exponential kernel with a periodic kernel creates locally periodic patterns (Duvenaud et al. 2013). Chronos aggregates kernels of various types—such as Linear, Periodic, Squared-Exponential, Rational, and Quadratic—and with different parameters (such as daily, weekly, and monthly periodic kernels) in a kernel bank, composing them as described above to define the Gaussian processes.

However, generating a MTS time-series data is yet to be addressed. To address the second goal, generating variates that are correlated in a realistic manner, we use a generative Gaussian modelling, called linear model of coregionalization (LMC), which is developed initially in the field of geostatistics (Journel and Huijbregts 2003). For ease of understanding, we adopt the time-series notation we used above. In LMC, the outputs are obtained as linear combinations of independent latent random functions. In other words, given $\pmb { t } \in \mathbb { R } ^ { T }$ , the outputs in each channel $\{ C _ { i } ( t ) \} _ { i = 1 } ^ { N }$ is the linear combination of $L$ latent functions

$$
C _ { i } ( t ) = \sum _ { j = 1 } ^ { L } \alpha _ { i , j } l _ { j } ( t )
$$

Observe that, since latent functions are independent with zero-mean the resulting output covariance is a well-defined PSD function with zero-mean ( ´Alvarez, Rosasco, and Lawrence 2012). In our synthetic data generation algorithm, to avoid scaling issues, we restrict ourselves to convex combinations. Thus, for each $i$ , we have $\alpha _ { i , 1 } + \ldots \alpha _ { i , L } =$ 1 with $\alpha _ { i , j } \geq 0$ , meaning that the outputs lie in the convex hull of latent functions $l _ { j }$ ’s. We generate the latent functions based on KernelSynth’s algorithm due to it’s extensive descriptive value. Note that the LMC formulation encapsulates the cases where the correlations between different variates are small or nonexistent. Specifically, the case where each variate is independent from the rest corresponds to $L = N$ with $C _ { i } ( t ) = \bar { l } _ { i } ( t )$ . Such a modelling is important, as some MTS data have strong correlation between different variates, whereas others have small or non-existent correlation.

In our algorithm, LMC-Synth, we sample the number of latent functions from a Weibull distribution and $\left[ \alpha _ { i , 1 } \ldots \alpha _ { i , L } \right]$ from a Dirichlet distribution. To avoid highly skewed cases, we impose upper and lower bounds on the possible number of latent functions. Since the uncorrelated setting of $L = N$ with $C _ { i } ( t ) = l _ { i } ( t )$ is crucial for modeling MTS problems with low correlation among variates, we also generate data under this setting. Incorporating this extra setting is shown to yield the strongest performance in zero-shot settings.

# Architecture for TimePFN

In designing the architecture, our main principle was to create a system capable of extracting time-series features useful for MTS forecasting. Through this, we aimed for the architecture to achieve better generalization when applied to real-world datasets. The primary advantage of the PFN framework in our case is that, since synthesizing large-scale synthetic MTS data is feasible with LMC-Synth, we are no longer constrained by data scarcity. Previous MTS models were compelled to balance model complexity with their datasets due to limited data, often restricting the use of certain components or their quantity (such as the number of transformer layers) to avoid overfitting. However, with access to large-scale MTS data, we can expand our architecture and freely incorporate additional components that we believe will improve forecasting performance on new datasets. In light of this, we proceed to explain our architecture and design choices.

The TimePFN model resembles PatchTST (Nie et al. 2023) in several aspects when processing MTS data, but it differs significantly in two areas: our convolutional filtering of the variates prior to patching and channel-mixing.

Convolutional Filtering. Before patching, consider an MTS dataset $X ~ = ~ [ x _ { 1 } \ldots \bar { x _ { N } } ]$ in $\mathbb { R } ^ { L \times N }$ , where $L$ is the length and $N$ is the number of variates. We apply learnable 1D convolution operations to each variate, with convolutional weights shared across all variates. After convolutions, we apply 1D magnitude max pooling to each newly generated variate, followed by a new set of 1D convolutions. In TimePFN, each $\boldsymbol { x } _ { i } ~ \in ~ \mathbb { R } ^ { L }$ is transformed into $\bar { \boldsymbol { x } } _ { i } ~ \in ~ \mathbb { R } ^ { ( C + 1 ) \times L }$ , where $C$ rows come from 1D convolutional operations and magnitude max pooling, whereas one row is the original $x _ { i }$ . We keep the original $x _ { i }$ to not loose any information, analogous to skip connections used in NLP (He et al. 2015). In practice, we used $C = 9$ . Filtering with convolutions is a valuable tool in time-series analysis. Many operations, such as differencing to de-trend data, can be effectively represented by convolutions. We utilized this approach to extract common time-series features across various datasets, thereby improving the generalization capability of our model.

Patch Embeddings. Given $\bar { \boldsymbol { x } } _ { i } \in \mathbb { R } ^ { ( C + 1 ) \times L }$ , we extract overlapping patches of size $P$ with a stride of $S$ , following the settings described in (Nie et al. 2023). Each patch thus has dimensions $\mathbb { R } ^ { ( C + 1 ) \times P }$ , and a total of $\lfloor \frac { L - P } { S } + 2 \rfloor$ patches are extracted from a single variate. In total, we get $\begin{array} { r } { \dot { \boldsymbol { N } } \times \left\lfloor \frac { \boldsymbol { L } - \boldsymbol { P } } { S } + 2 \right\rfloor } \end{array}$ patches. Each patch is then flattened and fed into a 2-layer feedforward neural network to be mapped to embedding dimension $D$ . We add 2D sinusoidal positional encodings (Wang and Liu 2019) to the embeddings to correctly capture channel-wise and temporal information. In practice, we used $( P = 1 6 , S = 8 )$ ), similar to (Nie et al. 2023).

Channel-mixing. Unlike PatchTST (Nie et al. 2023), where the tokens from each channel are fed independently into a transformer encoder, we input all tokens into the transformer encoder after applying the positional encodings described above. Consequently, tokens from different variates can attend to each other.

Transformer Encoder. We employ a naive multihead transformer encoder, incorporating layer normalizations (Ba, Kiros, and Hinton 2016) and skip connections (He et al. 2015) to improve training stability. After feeding the tokens into the multilayer encoder, we rearrange them into their respective channels and apply a channel-wise flattening operation. This is followed by a two-layer feedforward network that processes the flattened variate representations using shared weights (single FFN is applied to all variates).

Normalization. We normalize each variate $x _ { i }$ to have zero mean and unit standard deviation prior to any other process described above, as recommended by (Kim et al. 2021), to alleviate the impact of distribution shifts between our synthetic dataset and test examples (Liu et al. 2023; Nie et al. 2023). Before forecasting, we revert the time series to its original scale by de-normalizing.

Architectural Details. Due to our architectural specifications, TimePFN has fixed input sequence and forecasting lengths. However, it can accept an arbitrary number of variates. Thus, although we trained TimePFN with a synthetic dataset of a fixed channel size $C = 1 6 0 ^ { \circ } ,$ ), it can forecast with both fewer and more channels than those used in its training data. When forecasting with a number of channels $\bar { C } \leq \breve { C }$ , we directly input the data to TimePFN. To mitigate the effects of distribution shifts when forecasting with more channels at test time, we process the data by splitting it into non-overlapping channels of size at most $C$ . If the test data has $\bar { C }$ channels, we divide it into $\begin{array} { r } { \left\lfloor { \frac { \bar { C } } { C } } \right\rfloor + 1 } \end{array}$ segments, input them separately, and then stack them afterwards.

Table 1: MTS forecasting results of TimePFN and comparable architectures with best results in bold. Input and forecast lengths are set to be 96. SeasonalN. stands for Seasonal Naive. TimePFN demonstrates remarkable performance in budget-limited settings, as well as with the full dataset, particularly in scenarios involving a large number of variates.   

<html><body><table><tr><td></td><td>Dataset</td><td>ECL</td><td>Weather</td><td>Traffic</td><td>Solar</td><td>Exchange</td><td>ETTh1 ETTh2</td><td>ETTm1</td><td>ETTm2</td></tr><tr><td rowspan="3"></td><td>Models</td><td>MSE MAEMSE MAEMSE MAEMSE MAE MSE MAEMSE MAEMSE MAE MSE MAEMSE MAE</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>TimePFN</td><td>0.315 0.3830.209 0.2551.108 0.6130.9410.7300.105 0.2290.453 0.4390.328 0.362 1.587 0.9450.259 0.254</td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.637 0.5120.212 0.291</td></tr><tr><td>Naive SeasonalN. Mean</td><td>1.618 0.9640.268 0.2 0.845 0.761</td><td></td><td>141.07 5390.815</td><td>0.081 0.196</td><td>2940.7</td><td>0.42</td><td>2270.67</td><td>1.213 0.6640.266 0.327 2740.334</td></tr><tr><td>Os=</td><td>TimePFN iTransformer0.2780.360 PatchTST DLinear FEDFormer Informer</td><td>0.2350.322 0.190 0.667 0.646 0.406 0.4630.742 0.612 0.908 0.7580.306 0.381 1.2260.8960.4640.51</td><td></td><td>.8880.93 .5870.87</td><td></td><td>4040.88 6760.5</td><td>.9281.383 240.468</td><td>0.6930.54 0.745 0.5890.291 0.387</td><td>.229 0.307 0.419 0.4180.195 0.276 .884 0.6080.268 0.337 2310.310 1.332 0.846|3.484 1.290</td></tr><tr><td>00s= A</td><td>Autoformer TimePFN iTransformer0.20 0.284 DLinear FEDformer Informer Autoformer</td><td>0.729 0.675 0.190 0.2830.178 0.2 0.2350.3280.335 0.3 0.317 0.4070.265 0.34 0.869 0.760 0.3200. 0.303 0.396 0.138 0.137</td><td>0.3220.401 600 48</td><td>888</td><td></td><td></td><td></td><td>8170.62 .6740.542</td><td>0030.745 .590 0.995 .316 0.407 0.360 0.3860.185 0.268 31 0.310 0.870 0.626 .238 0.322 210.803 230.308</td></tr><tr><td>=</td><td>iTransformer|0.147 0.239 PatchTST DLinear FEDformer Informer Autoformer # of Variates</td><td>0.1850.26 0.195 0.2780.341 0.196 0.310 0.327 0.4130.455 0.4 0.214 0.327 321</td><td>0.2730.344 21</td><td>0.6050.376 862</td><td>4550.48 137</td><td>8</td><td>4400.44</td><td>3640.4</td><td>850.272 70.260 3440.37 0.1950.293 3630.4080.1910.286 .3960.474 5200.490 0.2330.311</td></tr></table></body></html>

# 4 Experiments

In all MTS evaluations, our primary objective is to forecast a horizon of 96 time steps using an MTS input of 96 time steps. We trained a single TimePFN model on a large-scale, multivariate synthetic dataset generated by LMC-Synth and conducted all experiments using this model. We generated 15,000 synthetic datasets with a length of 1024 and a channel size of 160 from LMC-Synth, further augmenting with datasets having independent variates as in the case $\bar { C _ { i } } ( t ) =$ $l _ { i } ( t )$ . The independent data comprises approximately $2 5 \%$ of the purely correlated data. During training, we extracted time-series input and output pairs using a sliding window of size 192 (96 for input, 96 for output), resulting in approximately 1.5 million synthetic data points. We trained the model to forecast the MTS output based on the given input using MSE loss with our 160 channel synthetic dataset. Training a single TimePFN of 8 transformer layers takes around 10 hours on L40S GPU.

In the few-shot evaluations, we fine-tuned TimePFN using the specified data budget. We did not perform any hyperparameter tuning on TimePFN, and the same set of hyperparameters was used in all few-shot settings. Details about the model hyperparameters are provided in the appendix.

Benchmark Datasets. We evaluated TimePFN on nine widely-used, real-world benchmark datasets for MTS forecasting. These datasets include ETTh1, ETTh2, ETTm1, ETTm2 (collectively referred to as ETT, representing Electricity Transformer Temperature), Weather, Solar Energy, ECL (Electricity Consuming Load), Exchange, and Traffic. The Solar Energy dataset was introduced by (Lai et al. 2018), while the others were introduced by (Wu et al. 2021). We provide the specifications of these datasets in the appendix section Datasets.

Baselines. Since no MTS PFN is available, we compared TimePFN with state-of-the-art transformer-based MTS forecasting models, including FEDformer (Zhou et al. 2022), Autoformer (Wu et al. 2021), Informer (Zhou et al. 2021), PatchTST (Nie et al. 2023), and iTransformer (Liu et al. 2023). We evaluated these models across the entire dataset and at various data budgets, including 50, 100, 500, and 1000 data points. For instance, at a data budget of 500, the model is trained using 500 MTS input and output pairs. Additionally, we included DLinear (Zeng et al. 2023), a linear model, as part of our baseline. Given its lower complexity, we consider it a strong baseline for smaller data budgets.

For smaller data budgets and our zero-shot evaluations, we incorporated three algorithmic baselines as suggested by (Dooley et al. 2023) and (Ansari et al. 2024): Mean, Naive, and Seasonal Naive. These baselines are applied independently to each variate. The Mean baseline forecasts by repeating the mean value of the input variate. The Naive approach forecasts by repeating the last value of the input variate. In the Seasonal Naive method, we assume a periodicity of seven.

Table 2: Zero-shot results of TimePFN on univariate time-series forecasting with input length $= 3 6$ . TimePFN-96 has input length of 96. The errors are averaged over forecasting lengths of $\{ 6 , 8 , 1 4 , 1 8 , 2 4 , 3 6 , 4 8 \}$ . Chronos-s stands for Chronos-small. The best results are in bold (excluding TimePFN-96).   

<html><body><table><tr><td>Dataset</td><td>TimePFN-36</td><td>TimePFN-96</td><td></td><td>ForecastPFN</td><td>Chronos-s</td><td>SeasonalN.</td><td></td><td>Naive</td><td></td><td>Mean</td></tr><tr><td>Models</td><td>MSE MAE</td><td>MSE MAE</td><td></td><td>MSE MAE</td><td>MSE MAE</td><td></td><td>MSE MAE</td><td></td><td>MSE MAE</td><td>MSE MAE</td></tr><tr><td>ECL</td><td>0.752 0.703</td><td>.509</td><td>.549</td><td>.416 0.958</td><td>.152 0.792</td><td>559</td><td>.995</td><td>.211</td><td>0.829</td><td>0.963 0.805</td></tr><tr><td>Weather×10²</td><td>0.042 1.381</td><td>0.046</td><td>1.47</td><td>0.084 1.999</td><td>0.036 1.136</td><td>0.045</td><td>1.352</td><td>0.035</td><td>1.123</td><td>0.069 1.893</td></tr><tr><td>Traffic</td><td>1.503 1.032</td><td>0.414</td><td>0.503</td><td>4.521 1.742</td><td>3.103 1.364</td><td>.301</td><td></td><td>.330</td><td>1.463</td><td>2.125 1.256</td></tr><tr><td>Exchange</td><td>0.027 0.125</td><td>0.034</td><td>0.139</td><td>0.057 0.180</td><td>0.049 0.113</td><td>0.028</td><td>0.128</td><td>0.022</td><td>0.107</td><td>0.040 0.156</td></tr><tr><td>ETTh1</td><td>0.029 0.130</td><td>0.030</td><td>133</td><td>0.102 0.237</td><td>0.061 0.155</td><td>0.039</td><td></td><td>.031</td><td>0.128</td><td>0.040 0.154</td></tr><tr><td>ETTh2</td><td>0.126 0.273</td><td>0.086</td><td>0.224</td><td>0.434 0.517</td><td>0.207 0.321</td><td>0.279</td><td>0.408</td><td>0.215</td><td>0.336</td><td>0.168 0.321</td></tr></table></body></html>

Although TimePFN is specifically trained for multivariate time-series forecasting, we also evaluated its performance on univariate forecasting $\scriptstyle ( \mathbf { C } = 1 )$ to demonstrate its robust generalization capabilities. In this context, we compared it with ForecastPFN (Dooley et al. 2023) and Chronos (Ansari et al. 2024), two state-of-the-art univariate zero-shot forecasters. ForecastPFN utilizes an input sequence length of 36, while TimePFN operates with a sequence length of 96. To accommodate this discrepancy, we padded the additional 60 time steps with the mean value of the input sequence when running TimePFN. These models are evaluated over forecast lengths of 6, 8, 14, 18, 24, 36, and 48. We used the smaller version of Chronos. The full results are detailed in the appendix, while in the main text, we report the averaged MSE and MAE values across these forecast lengths. Furthermore, to showcase the complete forecasting performance of TimePFN, we also conducted runs with a non-padded sequence length of 96. We evaluated all results ourselves.

Experimental Setting. When comparing TimePFN with the aforementioned baselines, we use the hyperparameters reported in their official codebases. We re-run the experiments with limited budgets and by utilizing the entire training dataset. (Liu et al. 2023) presents the forecasting results for the mentioned transformer-based MTS architectures using the full training dataset. We re-run all the experiments and selected the best results from both our run and their report to ensure that the performance of other architectures is not underreported when we use the entire training set. Our unaltered results are included in the appendix.

In TimePFN, we use a single model with fixed hyperparameters that is trained only once on our large-scale multivariate synthetic dataset. In few-shot evaluations, we finetune TimePFN with a given data budget, maintaining the same hyperparameters across different datasets. In all evaluations except for univariate cases, we report the forecasting errors for the next 96 time steps, given a multivariate time series (MTS) of sequence length 96. Our implementation details and further experimental settings such as hyperparemeters are reported in the Appendix: Implementation Details.

# Main Results

In MTS forecasting, we compared TimePFN with various baselines in zero-shot settings, as well as with different data budgets, and by utilizing the entire dataset. Table 1 presents our results for zero-shot settings, data budgets of 50 and 500, and scenarios using the entire dataset. Our comprehensive results, which also include data budgets of 100 and 1000, can be found in the Appendix under the section Extended Results. With a data budget of 50, TimePFN outperforms all transformer-based architectures and DLinear. However, with a data budget of 500, it surpasses all baselines except for PatchTST (Nie et al. 2023) in the exchange dataset, closely competing with it. When utilizing the entire dataset, TimePFN achieves the best results in four datasets, equaling the performance of PatchTST. Given that we fine-tuned TimePFN with fixed hyperparameters across all datasets, and selected the best results from the baselines and the findings reported in (Liu et al. 2023), the performance of TimePFN is noteworthy. We observe that TimePFN excels in datasets with a greater number of variates and a more multivariate nature, while PatchTST primarily excels in ETT datasets and Exchange. This outcome is anticipated, as TimePFN is designed to incorporate channel mixing, whereas PatchTST is designed with channel independence. Indeed, the lower forecasting performance of PatchTST on Traffic dataset supports this hypothesis.

In zero-shot settings, TimePFN outperforms all zero-shot baselines except on the Solar-Energy and Exchange datasets, with Solar-Energy being in close proximity. In fact, the Exchange dataset is highly non-trivial, as simply using the last value as the forecast outperforms all baselines, except for PatchTST in the entire budget case. We observed that the Solar-Energy data exhibits sudden spikes e.g. as a function of sun rising or going down. Our model, based on its training data from the LMC-Synth prior, fails to anticipate such sudden spikes. However, these spiky behavior is well within the capabilities of changepoint kernels in Gaussian processes, suggesting a clear path for future improvements.

# Univariate Time-Series Forecasting

Although TimePFN was specifically trained for MTS forecasting using a synthetic dataset with a channel size of 160, we also tested it in a zero-shot scenario for univariate timeseries forecasting where $C = 1$ . Moreover, we used the sequence length of 36 that ForecastPFN (Dooley et al. 2023) was specifically trained on. To accommodate this sequence length, we padded the remaining $9 6 - 3 6 = 6 0$ sequence lengths with the mean value of the input time-series to mitigate any scaling issues, and named this model configuration TimePFN-36. To demonstrate the full performance of our model, we included results for TimePFN without padding using a sequence length of 96, referred to as TimePFN-96 in Table 2. All other results were reported with a sequence length of 36.

Table 3: In TimePFN-w/o-Convolution, we eliminate the convolutional operator that is normally applied to the initial input variates. In PatchTST-PFN, we train a PatchTST model to evaluate the significance of channel-mixing and the appropriateness of our architecture for PFNs. Both the sequence length and the forecasting length are set to 96.   

<html><body><table><tr><td rowspan="2">Dataset Models</td><td rowspan="2">ECL</td><td rowspan="2">Weather</td><td rowspan="2">Traffic</td><td rowspan="2">Solar</td><td rowspan="2">Exchange</td><td rowspan="2">ETTh1</td><td rowspan="2">ETTh2</td><td rowspan="2">ETTm1</td><td rowspan="2">ETTm2</td></tr><tr><td>MSE MAE MSE MAEMSE MAEMSE MAE MSE MAEMSE MAEMSE MAEMSE MAE MSE MAE</td></tr><tr><td>TimePFN</td><td>0.315 0.3830.209 0.2551.108 0.6130.941 0.7300.105 0.2290.4530.4390.328 0.3620.637 0.5120.212 0.291</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>TimePFN-w/Conv0.6530.670.2210711.2870.7571.1970.8290.1110.2370.60805170.3380.740.7710.565040.7</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Z PatchTST-PFN</td><td></td><td></td><td></td><td></td><td>0.470 0.5220.212 0.2621.172 0.702|1.014 0.7870.108 0.2310.554 0.5010.322 0.3660.746 0.5600.215 0.301</td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

<html><body><table><tr><td>Dataset Models</td><td>ECL MSE MAEMSE MAE</td><td>Weather</td><td>Traffic MSE MAE</td><td>Solar MSE MAE</td><td>Exchange MSE MAE</td><td>ETTh1 MSE MAEMSE MAEMSE MAE MSE MAE</td><td>ETTh2</td><td>ETTm1</td><td>ETTm2</td></tr><tr><td>TimePFN</td><td>0.315 0.3830.209 0.255</td><td></td><td>1.108 0.613</td><td>0.941 0.730</td><td>0.105 0.229</td><td></td><td></td><td>0.453 0.4390.328 0.3620.637 0.5120.212 0.291</td><td></td></tr><tr><td>S TimePFN-Ind0.350 0.4160.214 0.260</td><td></td><td></td><td>1.180 0.651</td><td>1.197 0.829</td><td>0.113 0.238</td><td></td><td></td><td>0.468 0.4470.326 0.3630.761 0.5420.215 0.295</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

Table 4: TimePFN-ind is the model trained using only independent variates, while the other model is our standard one, which we used throughout the experiments. Both models have a sequence and forecasting length of 96.

As demonstrated in Table 2, TimePFN outperforms models that were specifically trained for univariate time series forecasting, which attests to its robust generalization and zero-shot performance. Our extensive evaluations, which detail the errors for different sequence lengths, can be found in the appendix under the section Extended Results.

# Ablation Study

Training a single TimePFN model requires approximately 10 hours on a single L40S GPU, which limited our capacity for ablation studies. Nevertheless, we conducted two types of key ablations: the first type focused on the architecture, while the second type focused on the synthetic data generation.

Architectural Ablation. In the first part, we first aim to understand the impact of our 1D convolutional operation applied to time-series variates before any patching. To do this, we remove the operation and train a TimePFNconvolutionless model, then report the zero-shot results in Table 3. We observe that without the convolutional operation, the zero-shot performance significantly decreases. Additionally, since our architecture differs from that of (Nie et al. 2023) particularly in terms of channel mixing, we trained the PatchTST architecture to assess the impact of channel mixing on zero-shot forecasting performance. As seen in Table 3, both of our ablation experiments supports our model design principles and underscores the usefulness of TimePFN’s architecture for synthetic time series learning.

Synthetic Dataset Ablation. To understand whether the synthetic data generation algorithm LMC-Synth gives any benefits over just using the variates generated by KernelSynth (Ansari et al. 2024) independently in each channel, we trained TimePFN with using data where each channel is generated independently. This case, as we described previously, corresponds to the case where $C _ { i } ( t ) = l _ { i } ( t )$ with number of channels equaling to number of latent functions. We see in Table 4 that using generative coregionalization provides clear benefits.

# 5 Conclusion

In this work, we demonstrate that with large-scale synthetic training and a suitable architecture for extracting useful time series features, fine-tuning with as few as 50 to 500 examples are sufficient to achieve competitive performance in multivariate time series forecasting. To this end, we present a novel method for generating large-scale synthetic MTS data with realistic intra- and inter-channel dependencies, called LMC-Synth, utilizing Gaussian processes and linear coregionalization model. Simultaneously, we developed an architecture capable of transfer learning, utilizing 1D convolutions applied to time series variates and channel-mixed patching. TimePFN exhibits strong zero-shot performance, and although it is explicitly trained for MTS forecasting, it also excels in zero-shot univariate forecasting, demonstrating the flexibility and generality of our framework. To the best of our knowledge, TimePFN is the first multivariate time-series PFN. For future work, we aim to improve our synthetic data-generation mechanism to better model sudden changes and multi-scale challenges that are prevalent in many time-series tasks. Additionally, integrating time series PFNs with tabular models presents an intriguing avenue for research. Moreover, we plan to extend our efforts into developing foundation models for multivariate time series.