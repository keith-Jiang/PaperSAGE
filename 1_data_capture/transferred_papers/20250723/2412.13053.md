# SMOSE: Sparse Mixture of Shallow Experts for Interpretable Reinforcement Learning in Continuous Control Tasks

M´aty´as Vincze1,2, Laura Ferrarotti2, Leonardo Lucio Custode1 Bruno Lepri2, Giovanni Iacca1

1 University of Trento, Italy 2 Fondazione Bruno Kessler, Italy mvincze, ferrarotti, lepri $@$ fbk.eu, leonardo.custode, giovanni.iacca @unitn.it

# Abstract

Continuous control tasks often involve high-dimensional, dynamic, and non-linear environments. State-of-the-art performance in these tasks is achieved through complex closed-box policies that are effective, but suffer from an inherent opacity. Interpretable policies, while generally underperforming compared to their closed-box counterparts, advantageously facilitate transparent decision-making within automated systems. Hence, their usage is often essential for diagnosing and mitigating errors, supporting ethical and legal accountability, and fostering trust among stakeholders. In this paper, we propose SMOSE, a novel method to train sparsely activated interpretable controllers, based on a top-1 Mixture-of-Experts architecture. SMOSE combines a set of interpretable decisionmakers, trained to be experts in different basic skills, and an interpretable router that assigns tasks among the experts. The training is carried out via state-of-the-art Reinforcement Learning algorithms, exploiting load-balancing techniques to ensure fair expert usage. We then distill decision trees from the weights of the router, significantly improving the ease of interpretation. We evaluate SMOSE on six benchmark environments from MuJoCo: our method outperforms recent interpretable baselines and narrows the gap with noninterpretable state-of-the-art algorithms.

# Introduction

Over the last decade, Artificial Intelligence (AI) has achieved dramatic success. However, the increasing adoption of AI systems in real-world use cases has been raising concerns related to the trustworthiness of these systems (Wexler 2017; McGough 2018; Varshney and Alemzadeh 2017; Rudin, Wang, and Coker 2019; Huang et al. 2020; Smyth et al. 2021; He 2021). One of the most promising paths toward trustworthy AI involves developing methods to enhance our understanding of AI decision-making processes. In this context, eXplainable AI (XAI) introduces AI systems enabling the generation of post-hoc explanations for their behavior (Dwivedi et al. 2023). Although such methods do provide insights into the decision-making processes, their post-hoc analysis explains just an approximation of the original closed-box behavior. This approximation cannot be trusted in high-stakes scenarios, which are frequent for instance in robot control, autonomous driving, emergency response, and public decision-making. Indeed, given that interpretable policies distilled for explanation are typically less complex than closed-box ones, they might require adopting entirely different strategies, disrupting the link between the original behavior and its interpretation (Rudin 2019). To overcome this issue, research efforts have been channeled towards Interpretable AI (IAI) (Rudin 2019; Akrour, Tateo, and Peters 2021). Interpretable AI focuses on systems whose decision-making process is inherently understandable to humans, without the need for distilled explanations (Arrieta et al. 2020). These models prioritize clarity and simplicity, enabling users to directly comprehend how inputs are transformed into outputs, thus facilitating transparency, trust, and validation ease (Rudin 2019).

Due to its sequential nature, Reinforcement Learning (RL) benefits more from directly interpretable policies than post-hoc explanations (Rudin et al. 2021). Discrepancies between a policy and its interpretable approximation can grow over time due to state distribution drift, making local explanations less meaningful (Akrour, Tateo, and Peters 2021). Moreover, post hoc XAI methods, when used in RL to tackle goal misalignment problems, might mislead observers into believing that closed-box agents select the correct actions for the appropriate reasons, even though their decision-making process may actually be misaligned (Chan, Kong, and Liang 2022; Delfosse et al. 2024). This paper addresses the challenge of Interpretable Reinforcement Learning (IRL) in continuous action spaces. In particular, the main advances presented in this work can be summarized as follows:

• We propose SMOSE, a novel, high-performing method, that exploits a sparse, interpretable Mixture-of-Experts (MoE) architecture. Our method simultaneously learns a set of simple yet effective continuous sub-policies, and an interpretable router. The sub-policies are sequentially selected by the router, one per each decision step, to control the system, based on the current state. • We showcase the capabilities of SMOSE through results obtained on six well-known continuous control benchmarks from the MuJoCo environment (Todorov, Erez, and Tassa 2012). For all the considered environments we analyze performance both in training and in evaluation. • We include a full interpretation for all the trained poli

cies, demonstrating the effectiveness of SMOSE in providing extensive insight on the learned controllers. By combining router and the experts interpretation, we analyze the high-level and low-level alignment of the policy. • We compare SMOSE with interpretable and noninterpretable state-of-the-art RL models, considering both larger model size competitors, and models with comparable size (in terms of the number of active and overall parameters). Results highlight that the proposed architecture, while retaining interpretability, improves the performance w.r.t. other interpretable methods, tightening the gap with non-interpretable approaches.

The paper is structured as follows: the next two sections summarize the relevant background and related contributions, while the Methods section details SMOSE, our proposed approach. Then, the Results section presents the experimental setup and the performance achieved by our method, including the interpretation of the learned policies. Finally, we draw the conclusions and discuss the future directions of this work.

# Background

As recently surveyed in (Glanois et al. 2024), several works study IRL models. Some approaches exploit neural logicbased policies to achieve interpretability (Jiang and Luo 2019; Kimura et al. 2021; Delfosse et al. 2023; Sha et al. 2024). Older approaches (McCallum 1996; Pyeatt, Howe et al. 2001) use existing methods for decision tree (DT) induction, adapting them to the RL domain. In (Roth et al. 2019), the authors introduce a heuristic to keep the size of the trees smaller while still achieving good performance. However, these algorithms suffer from the curse of dimensionality, i.e., they do not scale well with the dimensionality of the state space. More recent approaches address this issue. In (Silva et al. 2020), the authors employ soft DTs (Irsoy, Yıldız, and Alpaydın 2012) as policies for RL agents. This simplifies training and allows the use of widely known deep RL algorithms. However, soft trees are difficult to interpret, and discretizing them into “hard” DTs, the policies obtained can suffer from a significant loss in performance.

Other approaches make use of evolutionary principles to optimize DTs. (Dhebar et al. 2020) propose a method for learning a non-linear DT from a neural network trained on the target environment. This allows for choosing the desired properties of the resulting DT (e.g., the depth). On the other hand, this methodology hinders online learning (and thus adaptation to novel scenarios). In (Custode and Iacca 2023), the authors combine evolutionary techniques with $Q$ -Learning to produce DTs that can learn online, while still being interpretable. The DTs produced by this approach achieve performance comparable to non-interpretable stateof-the-art algorithms in a number of simple benchmarks. However, this approach has an extremely high computational cost, requiring a large number of interactions with the environment. Finally, in (Custode and Iacca 2024), the authors leverage principles from social learning to significantly improve both the computational complexity and the performance of evolutionary methods.

# Related Work

IRL methods tailored to environments with continuous action space are heavily needed in a wide variety of real-world scenarios, e.g., robot manipulation and control, as showcased by many benchmarking examples (Todorov, Erez, and Tassa 2012). So far, however, only a few works have investigated the use of IRL for continuous control. A branch of research is dedicated to the learning of interpretable programs as RL policies (Verma et al. 2019, 2021; Liu et al. 2023; Kohler et al. 2024). In (Custode and Iacca 2021) the authors propose a cooperative co-evolutionary approach in order to independently evolve a population of binary DTs (generated via Grammatical Evolution) and a population of sets of actions, both optimized w.r.t. the fitness associated with the combined use of the two. (Videau et al. 2022) explore methods for constructing symbolic RL controllers, utilizing parse trees and Linear Genetic Programming (LGP) to represent the programs as a vector of integers. Additionally, a multiarmed bandit strategy distributes the computational budget across generations. LGP is also used in (Nadizar, Medvet, and Wilson 2024), along with Cartesian Genetic Programming (CGP), where programs are instead represented as directed acyclic graphs. In (Paleja et al. 2023), an IRL algorithm for continuous control is introduced, exploiting fuzzy DTs combined with nodes and leaves with differentiable crispification, that can hence be directly learned via gradient descent. As previously mentioned, our method takes advantage of an interpretable MoE architecture for the control policy. MoEs can be found in RL literature, employed to tackle different problems, such as parameter scaling in deep RL (Obando-Ceron et al. 2024), handling of multiple optimal behaviors (Ren et al. 2021), and multi-task learning (Cheng et al. 2023; Willi et al. 2024), to name a few. In the realm of interpretability, a kernel-based method employing MoE is proposed in (Akrour, Tateo, and Peters 2021). In this work, the selection of a set of centroids from trajectory data is optimized. Each state is associated with an expert policy modeled as a Gaussian distribution around a linear policy, while retaining an internal complex function approximator. According to this approach, a learned combination of experts handles the control task. This is obtained by considering fuzzy memberships to clusters and employing a learned set of cluster weights. In our method, instead, we can tune the number of experts employed at every timestep for the decision, and, for instance, force the policy to exploit only one expert, for maximum interpretability (as explained in more detail in the Methods section). Moreover, while the policies in (Akrour, Tateo, and Peters 2021) are updated via approximate policy iteration, the centroids, which must be elements of the replay buffer, require separate iterations of discrete optimization. SMOSE, instead, learns a router that distributes the control tasks among experts. This compact representation, while being fully interpretable, can be learned via backpropagation, simultaneously to the experts.

# Method

We seek to solve RL problems with continuous actions structured as Markov Decision Processes (MDPs), i.e., tuples $( S , { \mathcal { A } } , { \mathcal { P } } , { \mathcal { R } } , \gamma , S _ { 0 } )$ , where $s$ is the set of the states in the problem, $\mathcal { A } \in \mathbb { R } ^ { n _ { a } }$ is the set of (continuous) actions, $\mathcal { P } ( s , a , s ^ { \prime } ) : \mathcal { S } \times \mathcal { A } \times \mathcal { S } \  \ [ 0 , 1 ]$ associates a probability to each transition from $( s , a )$ to each state $s ^ { \prime }$ ; $\bar { \mathcal { R } } ( s , a , s ^ { \prime } ) : \mathcal { S } \times \mathcal { A } \times \mathcal { S } \to \mathbb { R } ^ { + }$ assigns a reward to each triplet $( s , a , s ^ { \prime } )$ ; $\gamma$ is a discount factor, used for denoting the importance of future rewards w.r.t. the current one, and $ { \boldsymbol { S } } _ { 0 }$ is the set of the initial states. Solving such a problem requires the design of a learning strategy to fit a policy function $\pi : S \times \mathcal { A } \stackrel { \textstyle - } { \to } [ 0 , 1 ]$ , optimizing:

![](images/c3036014e2c9c91ee3447eba79b24a30720e5ee44d1d229bea0cf697090a429a.jpg)  
Figure 1: SMOSE. Schematic summary of the proposed architecture.

$$
\mathbb { E } _ { a _ { t } \sim \pi _ { t } } \Big [ \sum _ { t = 0 } ^ { \infty } \gamma ^ { t } \mathcal { R } _ { t } \Big | s _ { 0 } \sim \mathcal { S } _ { 0 } , s _ { t + 1 } \sim \mathcal { P } _ { t } \Big ] ^ { 1 }
$$

through interaction with the environment. Good results tackling this problem in non-trivial environments are often achieved by complex, not directly interpretable policies.

The main idea behind our method is to decompose a complex behavior by identifying a set of basic skills (or decision strategies) that are easier to interpret when combined with a policy capable of selecting which skill to employ in each state. Decomposing the decision process with this divideand-conquer approach can provide insight into the interpretation of the learned controller’s behavior. Thus, we structure the control policy as a composition of:

• A set $\{ \pi _ { m } ( \cdot | \theta _ { m } ) : S \to A | m = 1 , \ldots , M \}$ of $M$ parameterized continuous policy functions, representing $M$ decision-makers, each trained to be expert in an individual useful basic skill. To ensure overall interpretability, we consider continuous policies $\pi _ { m }$ that are shallow and directly interpretable, such as linear ones. • A planner, capable of assigning to each state $s \in { \mathcal { S } }$ an expert policy among the available set, in order to attain optimal behavior. This module is a parameterized router $g \dot { ( \boldsymbol { s } | \boldsymbol { \Theta } ) } : \mathcal { S }  [ 0 , 1 ] ^ { M }$ producing a one-hot encoded $M$ -dimensional vector as output.

SMOSE, summarized in Figure 1, combines the experts and the router according to a MoE architecture of the form:

$$
\pi ( s ) = \sum _ { m = 1 } ^ { M } \left[ g ( s | \Theta ) \right] _ { m } \pi _ { m } ( s | \theta _ { m } ) ,
$$

$$
^ { 1 } \pi _ { t } \doteq \pi \bigl ( s _ { t } , \cdot \bigr ) , \mathcal { R } _ { t } \doteq \mathcal { R } \bigl ( s _ { t } , a _ { t } , s _ { t + 1 } \bigr ) , \mathcal { P } _ { t } \doteq \mathcal { P } \bigl ( s _ { t } , a _ { t } , \cdot \bigr )
$$

where $\big [ g \big ( s | \Theta \big ) \big ] _ { m }$ is the $m$ -th component of vector:

$$
g \big ( s | \Theta \big ) = \mathrm { T O P } _ { 1 } \big ( \operatorname { s o f t m a x } \big ( \hat { g } \big ( s | \Theta \big ) \big ) .
$$

In the equation above, we indicate as $\hat { g } ( s | \Theta ) \in \mathbb { R } ^ { M }$ the inner parameterization of the router that produces as output an $M$ -dimensional vector. The softmax function then transforms $\hat { \boldsymbol g } \big ( \boldsymbol s | \Theta \big )$ in a preference vector, where the $m$ -th element measures the preference in choosing the $m$ -th expert as actor in state $s$ . The function $\mathrm { T O P _ { 1 } }$ assigns value 1 to the vector component with maximum preference, and 0 to all the others, allowing for an extremely sparse MoE structure in which only one expert is activated in each state.

Remark. The proposed method can also accommodate the use of $T O P _ { k }$ with $k > 1$ , replacing $T O P _ { 1 }$ in Eq. (2) and thereby enabling the selection of a combination of $k$ experts at each timestep to make control decisions. While the resulting policy would remain interpretable, in this work we intentionally set $k = 1$ to evaluate the performance of a truly sparse MoE architecture and to maximize interpretability within this framework.

This architecture takes inspiration from (Shazeer et al. 2017; Riquelme et al. 2021), where sparse MoE neural layers are stacked to obtain both computational efficiency in inference and parameters specialized on subsets of states. Here, such architecture is tuned to retain interpretability in continuous control, by removing the non-linearities and activation functions, employing a single-layer shallow structure, and shaping the inner router $\hat { g }$ and the experts $\pi _ { m }$ as linear functions, that is:

$$
\pi _ { m } ( s | \theta _ { m } ) = \theta _ { m } \cdot s \quad { \mathrm { a n d } } \quad { \hat { g } } ( s | \Theta ) = \Theta \cdot s
$$

with $\theta _ { m } \in \mathbb { R } ^ { n _ { a } \times n _ { s } }$ and $\boldsymbol \Theta \in \mathbb { R } ^ { M \times n _ { s } }$ .

The control policy in Eq. (1) is trained via RL. The parameters characterizing $\pi _ { m }$ and $g$ are hence simultaneously learned. It is important to notice that composing the $\mathrm { T O P _ { 1 } }$ and softmax functions in the order expressed in Eq. (2) permits a larger propagation of the gradients among the router weights, if compared with the opposite ordering. At every update, indeed, each of the $M$ components of softmax $\big ( \hat { g } \big ( s | \Theta \big )$ ) depends on all the weights $\Theta$ , due to the action of softmax. Hence, all the components in $\Theta$ will be updated, not only the ones associated with the expert selected by $\mathrm { T O P _ { 1 } }$ (Riquelme et al. 2021). We ensure to explore the action space of each expert $\pi _ { m }$ by injecting stochasticity in the choices in training, i.e.,

$$
\pi _ { m } ( s | \theta _ { m } , \sigma _ { m } ) = \mathcal { N } ( \theta _ { m } \cdot s , \sigma _ { m } ^ { 2 } ) .
$$

Our method can be seamlessly integrated with any RL algorithm for the learning of continuous controllers. In this work, we rely for exploration on Soft Actor-Critic (SAC) (Haarnoja et al. 2018), a state-of-the-art algorithm that balances the objective function with a term promoting higher entropy policies. We structure the actor module according to the interpretable architecture in Eq. (1), while we maintain the classic neural critic introduced in (Haarnoja et al. 2018).

In order to ensure balanced workloads among the $M$ experts, we augment the SAC actor objective function with additional penalties introduced in (Riquelme et al. 2021), weighted by a tunable parameter $\lambda$ . In particular, per each mini-batch $\mathrm { \bar { S } } = \{ \mathrm { s _ { k } } \} \subseteq \mathcal { S }$ of states, we consider its importance for $m = 1 , \ldots , M$ :

$$
\mathrm { I m p } _ { m } \mathrm { ( S ) } = \sum _ { s _ { \mathrm { k } } \in \mathrm { S } } \mathrm { s o f t m a x } \bigl ( \pi _ { m } \bigl ( s _ { \mathrm { k } } \mid \theta _ { m } , \sigma _ { m } \bigr ) \bigr )
$$

and we compute the importance loss:

$$
f _ { \mathrm { i m p } } ( \mathrm { S } ) = \frac { 1 } { 2 } \Big ( \frac { \mathrm { s t d } ( \mathrm { I m p ( \mathrm { S } ) } ) } { \mathrm { m e a n } ( \mathrm { I m p ( \mathrm { S } ) } ) } \Big ) ^ { 2 } .
$$

Then, we consider the load of S for $m = 1 , \ldots , M$ :

$$
\mathrm { L o a d } _ { m } ( \mathrm { S } ) = \sum _ { s _ { \mathrm { k } } \in \mathrm { S } } \mathbb { P } ( \epsilon _ { \mathrm { n e w } } \geq \tau ( s _ { \mathrm { k } } ) - \pi _ { m } ( s _ { \mathrm { k } } \mid \theta _ { m } , \sigma _ { m } ) )
$$

with $\begin{array} { r } { \tau ( s _ { \mathrm { k } } ) = \operatorname* { m a x } _ { m } \Bigl ( \pi _ { m } \bigl ( s _ { \mathrm { k } } | \theta _ { m } , \sigma _ { m } \bigr ) \Bigr ) } \end{array}$ , and compute the load-balancing loss:

$$
f _ { \mathrm { l o a d } } ( \mathrm { S } ) = \frac { 1 } { 2 } \Big ( \frac { \mathrm { s t d } ( \mathrm { L o a d } ( \mathrm { S } ) ) } { \mathrm { m e a n } ( \mathrm { L o a d } ( \mathrm { S } ) ) } \Big ) ^ { 2 } .
$$

When computing the load-balancing loss on each mini-batch S, noise to the inner router, i.e., $\hat { g } ( s | \Theta ) = \Theta \cdot s + \epsilon$ , with $\epsilon \sim \mathcal { N } ( 0 , 1 / M ^ { 2 } )$ , in order to ensure proper exploration of experts employment. Finally, once the policy training is complete, we produce a useful support for interpretation by distilling a multiclass classifier using Decision Trees (DTs), which are trained on a router-labeled replay buffer. By decomposing the original router into $M$ binary classifiers of limited depth, we create an easily readable representation of the interpretable router’s decision-making process, where each DT determines whether a specific expert should control the task for a given state. After balancing the data, we train the DTs in a supervised manner, using the CART algorithm (Timofeev 2004). More details on this can be found in the Appendix.

# Results

In this section, we present the evaluation of SMOSE on six widely-known continuous-control environments from MuJoCo (Todorov, Erez, and Tassa 2012), namely Walker2dv4, Hopper-v4, Ant-v4, HalfCheetah-v4, Reacher-v4, and

![](images/c6f8ab8eb3bd15b5a1a7af06a9e5d9949db2495d979d6444b84ffe0147d03f76.jpg)  
Figure 2: Performance in training. SMOSE compares to non-interpretable models of the same size, considering the number of overall (SAC-M) and active (SAC-S) parameters.

Swimmer-v4 (described in Appendix), which are commonly used as benchmarks in the RL field. The following subsections detail the experimental setup, performance evaluation, and comparative analysis with interpretable and non-interpretable baselines, highlighting the effectiveness of SMOSE in delivering both interpretability and high performance. As it is common in RL literature, performance is measured considering the attained episodic rewards (ER), i.e., the accumulated rewards achieved at each timestep of an episode, in order to compare well with the state of the art.

# Training Performance

In this section, we include the results in terms of performance achieved in training by SMOSE, tuned with $M = 8$ experts, and weighing the load-balancing losses in Eq.s (3)- (4) with $\lambda = 0 . 1$ . The value for the $M$ parameter has been empirically tuned to strike a balance between agents’ performance and easiness of interpretation, as shown in the ablation study included in Appendix. The weight $\lambda$ was tuned between 0.01 and 1.0 on a three-dimensional grid, to ensure fair load and reduce expert collapse. Details on the computational setup are available in the Appendix. To achieve statistical reliability in our results, we perform $N _ { \mathrm { t r a i n } } = 1 0$ independent training runs, seeded from 0 to 9. Every training run consists of a sequence of episodes with a maximum horizon of $H \ : = \ : 1 0 0 0$ interactions with the environment, which is achieved if the controller does not incur catastrophic failure.

We train for a total of one million environmental interactions (i.e., timesteps), to be comparable with the closed-box methods literature. The ER achieved by SMOSE in training over the six mentioned environments is represented in Figure 2, where we plot the mean and standard deviation of episodic returns over the environment interactions. We assign the ER to all the timesteps of the same episode, and then we plot the ER at each timestep, to make episodes with different lengths comparable. In the same figure, the ER achieved by closed-box policies is included as a reference. For a fair comparison, we consider neural policies trained with SAC (the same RL algorithm we use to learn our policy), tuned with the same set of hyperparameters, included in the Appendix. Moreover, we consider non-linear structures that exploit a comparable amount of parameters with our interpretable policy. In particular, our model has a total number $N _ { \mathrm { t o t } }$ of parameters, considering both the router and the $M$ experts; however, for each decision, only $N _ { \mathrm { { a c t } } }$ active parameters are used (i.e., those associated with the router and the selected expert), which alleviates the computational cost of the decision-making process. The figure includes the results in training associated with a first neural architecture, indicated as SAC-M, that exploits $N _ { \mathrm { t o t } }$ parameters for each decision. Additionally, the figure shows as well the results of a second neural architecture, indicated as SACS, that exploits $N _ { \mathrm { a c t } }$ parameters for each decision, and hence fully comparable in inference to our policy. Both $N _ { \mathrm { t o t } }$ and $N _ { \mathrm { a c t } }$ for our method are detailed in Tables 1-2. From Figure 2, we can see that SMOSE’s performance is close to the one of SAC-S on three over six environments (HalfCheetahv4, Hopper-v4, and Reacher-v4), and sensibly better than it on one over six environments (Walker2d-v4), while it is close to the result achieved by SAC-M on two over six environments (Walker2d-v4 and Reacher-v4). These evaluations are affected by the performance on Ant-v4, where we note that the initial behavior, similar again to the one of SACS, is worsened by a small number of final under-average training realizations (3 over 10), while higher performance is achieved in the majority of the cases.

Table 1: Performance and model size comparison. SMOSE outperforms interpretable methods.   

<html><body><table><tr><td>Environment</td><td>Algorithm</td><td>avg.ER</td><td>Nact(Ntot)</td></tr><tr><td rowspan="4">Walker2d-v4</td><td>CGP</td><td>1090.00*± 59.50*</td><td rowspan="4"></td></tr><tr><td>LGP</td><td>1080.00*± 14.00*</td></tr><tr><td>Metric-40</td><td>775.00*±115.50*</td></tr><tr><td>SMoSE (ours)</td><td>4224.29 ± 25.96</td></tr><tr><td rowspan="4">Hopper-v4</td><td>CGP</td><td>1150.00*± 92.50*</td><td rowspan="4"></td></tr><tr><td>LGP</td><td>1120.00*± 87.50*</td></tr><tr><td>Metric-40</td><td>2005.00*± 295.00*</td></tr><tr><td>SMOSE (ours)</td><td>2816.08 ± 445.57</td></tr><tr><td rowspan="4">Ant-v4</td><td>CGP</td><td>1130.00*± 222.50*</td><td rowspan="4"></td></tr><tr><td>LGP</td><td>1210.00*± 390.00*</td></tr><tr><td>Metric-40</td><td>2210.50*±175.50*</td></tr><tr><td>SMOSE (ours)</td><td>3245.43 ± 380.93</td></tr><tr><td rowspan="4">HalfCheetah-v4</td><td>CGP</td><td></td><td rowspan="4">672 (3808)</td></tr><tr><td>LGP</td><td>6375.00*± 496.50* 6388.50*± 296.50*</td></tr><tr><td>Metric-40</td><td>2210.50*±175.50*</td></tr><tr><td>SMOSE (ours)</td><td>7310.17 ± 131.57</td></tr><tr><td rowspan="3">Reacher-v4</td><td></td><td></td><td rowspan="3">360 (1872)</td></tr><tr><td>CGP LGP</td><td>-68.50*± 43.75* -58.50*± 11.10*</td></tr><tr><td>SMoSE (ours)</td><td>- 5.49 士 2.32</td></tr><tr><td rowspan="3">Swimmer-v4</td><td>CGP</td><td>280.00*±</td><td>360 (1872) 7.50*</td></tr><tr><td>LGP</td><td>278.50*±</td><td>14.00*</td></tr><tr><td>SMoSE (ours)</td><td>45.40 士</td><td>1.62 108 (360)</td></tr></table></body></html>

‘\*’ = visually derived from the plots reported in the original papers ‘—’ $\mathbf { \tau } = \mathbf { \tau }$ number of employed parameters not specified in literature magenta $\mathbf { \tau } = \mathbf { \tau }$ best score per environment

Table 2: Performance and model size comparison. SMOSE narrows the gap with non-interpretable methods.   

<html><body><table><tr><td>Environment</td><td>Algorithm</td><td>avg. ER</td><td>Nact(Ntot)</td></tr><tr><td rowspan="5">Walker2d-v4</td><td>SAC-L</td><td>4358.06 ± 582.94</td><td>73484</td></tr><tr><td>SAC-M</td><td>4020.51 ± 192.75</td><td>1842</td></tr><tr><td>SAC-S</td><td>2967.14± 77.18</td><td>372</td></tr><tr><td>PPO</td><td>3362.16± 793.40</td><td>5708</td></tr><tr><td>SMoSE (ours)</td><td>4224.29 ± 25.96</td><td>360 (1872)</td></tr><tr><td rowspan="5">Hopper-v4</td><td>SAC-L</td><td>2636.49± 424.21</td><td>70406</td></tr><tr><td>SAC-M</td><td>3224.25 ± 177.90</td><td>672</td></tr><tr><td>SAC-S</td><td>3076.09± 178.37</td><td>186</td></tr><tr><td>PPO</td><td>2311.90 ± 654.90</td><td>5126</td></tr><tr><td>SMoSE (ours)</td><td>2816.08 ± 445.57</td><td>168 (672)</td></tr><tr><td rowspan="5">Ant-v4</td><td>SAC-L</td><td>5255.46 ± 1070.65</td><td>77072</td></tr><tr><td>SAC-M</td><td>4894.18 ± 599.64</td><td>3800</td></tr><tr><td>SAC-S</td><td>4162.97± 298.12</td><td>720</td></tr><tr><td>PPO</td><td>2327.12 ± 871.63</td><td>6480</td></tr><tr><td>SMOSE (ours)</td><td>3245.43± 380.93</td><td>672 (3808)</td></tr><tr><td rowspan="5">HalfCheetah-v4</td><td>SAC-L</td><td>11809.87 ± 256.10</td><td>73484</td></tr><tr><td>SAC-M</td><td>8992.22± 80.58</td><td>1842</td></tr><tr><td>SAC-S</td><td>7214.30 ± 87.29</td><td>372</td></tr><tr><td>PPO</td><td>2308.29 ±1526.87</td><td>5708</td></tr><tr><td>SMoSE (ours)</td><td>7310.17 ± 131.57</td><td>360 (1872)</td></tr><tr><td rowspan="5">Reacher-v4</td><td>SAC-L</td><td>- 3.75 ±</td><td>69892</td></tr><tr><td>SAC-M</td><td>- 4.02 ±</td><td>484</td></tr><tr><td>SAC-S</td><td>1.61 - 4.82 ± 1.81</td><td>148</td></tr><tr><td>PPO</td><td>-6.57± 2.37</td><td>4930</td></tr><tr><td>SMoSE (ours)</td><td>- 5.49 ± 2.32</td><td>144 (480)</td></tr><tr><td rowspan="5">Swimmer-v4</td><td>SAC-L</td><td>68.59±</td><td>69124</td></tr><tr><td>SAC-M</td><td>71.94 ±</td><td>355</td></tr><tr><td>SAC-S</td><td>59.42± 2.89</td><td>108</td></tr><tr><td>PPO</td><td>93.26 ± 19.90</td><td>4868</td></tr><tr><td>SMoSE (ours)</td><td>45.4±</td><td>108 (360)</td></tr></table></body></html>

magenta $\mathbf { \tau } = \mathbf { \tau }$ best score per environment

# Policy Evaluation

After training, we evaluate the performance of the learned policies in Eq. (1) once deployed, and compare it with interpretable and not-interpretable methods on the six environments. We test deterministic linear experts obtained discarding the learned standard deviations $\sigma _ { m }$ as it is often done with SAC (Haarnoja et al. 2018), while employing the coefficients $\{ \theta _ { m } | m = 1 , \ldots , M \}$ in combination with the router $g$ in Eq. (2), with the learned parameters $\Theta$ . In our evaluation campaign, per each environment, we test all the $N _ { \mathrm { t r a i n } } ~ = ~ 1 0$ policies on $N _ { e } ~ = ~ 1 0 0$ independent episodes with maximum horizon $H = 1 0 0 0$ (1000 episodes in total), and we measure the average ER (avg. ER) achieved by the method. Numerical results on this are included in Tables 1-2.

For comparison, we consider the results reported in (Nadizar, Medvet, and Wilson 2024) for CGP and LGP, and the ones achieved by Metric-40 in (Akrour, Tateo, and Peters 2021) (where only 4 out of the 6 environments were tested, i.e., all except Reacher-v4 and Swimmer-v4), the best-performing policy trained in that work, characterized by a MoE architecture tuned with 40 experts. The three methods are briefly discussed in the Related Work section. Moreover, we consider as non-linear competitors the classic version of Proximal Policy Search (PPO) (Schulman et al. 2017), and three neural architectures trained with SAC (Haarnoja et al. 2018), characterized by different numbers of parameters. In particular, we consider SAC-M and SAC-S, previously described in the analysis of training performance, and we add a third, larger network, SAC-L. The architecture underlying SAC-L corresponds to the neural architecture employed in (Haarnoja et al. 2018). For PPO, we consider the performance benchmarked in (Huang et al. 2022), while, for the SAC-based methods, we train and evaluate them with the same procedure described for SMOSE.

We can see in Table 1 that SMOSE consistently outperforms its competitors in five over six environments (all except Swimmer-v4). Additionally, Table 2 underlines that on average, our method’s performance is closer to the one of the non-interpretable competitors, showing how SMOSE is performing consistently better than PPO (on five environments over six, all but Swimmer-v4), and on average close to the SAC-based approaches of comparable size, namely, SAC-M and SAC-S. Table 2 shows that SAC-L often achieves the best performance. This architecture is advantaged not only by exploiting non-linearity but also by the high number of parameters at its disposal (see Table 2, last column), which, however, makes it computationally more expensive, both in training and in inference.

As mentioned, Swimmer-v4 appears to be the most difficult environment for SMOSE, being the only one in which SMOSE does not outperform any of the interpretable methods, as well as PPO. To further comment on this, we can notice in Table 2 that the performance in this environment by all the SAC-based policies (including SMOSE) is weaker.

# Policy Interpretation

This section includes the interpretation of the best policy learned for the Reacher-v4 environment. We include interpretations for the other five environments in the Appendix. Figure 3 presents a graphical representation of the weights of the MoE policy, including both the router and the experts. Details on both are included in the following, but we can already notice from the figure that the controller employs almost exclusively a small subset of variables:

• coordinates of the target (T): $x _ { \mathrm { T } } , y _ { \mathrm { T } }$ • coordinates’ difference between the fingertip $( f )$ and T: $\Delta _ { x } = x _ { f } - x _ { \mathrm { T } } , \Delta _ { y } = y _ { f } - y _ { \mathrm { T } }$

The two control variables, the torques applied to the first and second joint, are indicated as $\tau _ { 1 }$ and $\tau _ { 2 }$ , respectively. We indicate as $S _ { m }$ the score of the $m$ -th expert, i.e., the “weight” computed by the corresponding column of the router.

Expert 1 $( S _ { 1 } \approx 2 . 7 y _ { \mathrm { T } } - 5 . 6 \Delta _ { y } )$ Expert 1 is called when its score $S _ { 1 }$ is greater than all the other scores. Its policy can be described as:

$$
\left\{ \begin{array} { l l } { \tau _ { 1 } \approx x _ { \mathrm { T } } , } \\ { \tau _ { 2 } \approx y _ { \mathrm { T } } - 4 \Delta _ { x } . } \end{array} \right.
$$

According to this, the first joint is strongly accelerated in a direction that is opposite to that of $x _ { \mathrm { T } }$ , while the second joint’s control signal is composed of two terms, one that moves the joint in the opposite direction of $y _ { \mathrm { T } }$ , and the other that tries to minimize the distance on the $\mathbf { X }$ -axis between the fingertip and the target.

Expert 2 $( S _ { 2 } \approx - 3 . 5 y _ { \mathrm { T } } + 8 . 3 \Delta _ { y } )$ It can be noted that Expert 2’s score $S _ { 2 }$ has opposite signs w.r.t. $S _ { 1 }$ , indicating that these two experts are likely working in opposite states. Its policy can be described as follows:

$$
\begin{array} { r } { \left. \tau _ { 1 } \approx 3 . 1 y _ { \mathrm { T } } - 2 . 7 \Delta _ { x } - 4 . 5 \Delta _ { y } , \right. } \\ { \left. \tau _ { 2 } \approx 2 . 8 y _ { \mathrm { T } } + 3 . 0 \Delta _ { x } - 3 . 4 \Delta _ { y } , \right. } \end{array}
$$

shows that the two joints’ control signals have partially similar behaviors. Indeed, both of them have a dependency on $y _ { \mathrm { T } }$ , which can be seen as a feed-forward control scheme combined with a feed-back control scheme (Tao, Kosut, and Aral 1994). This is suggested by the presence, in both controllers, of an amplified version of $y _ { \mathrm { T } }$ , and a negative dependency on $\Delta _ { y }$ (please, note that in this environment $\Delta$ can be seen as the opposite of the control error). We note, though, a strong difference between the two joints’ controllers: while $\tau _ { 1 }$ has a negative dependency on $\Delta _ { x }$ (suggesting that it tries to reach the target also on the $\mathbf { \boldsymbol { x } }$ -axis), $\tau _ { 2 }$ shows the opposite. Interestingly, the magnitude of the two contributions is comparable, suggesting that these terms are used to balance the fingertip by simultaneously moving the two joints.

Expert 3 $( S _ { 3 } \approx 3 . 9 y _ { \mathrm { T } } - 5 . 8 \Delta _ { y } )$ The score $S _ { 3 }$ has similar coefficients to $S _ { 1 }$ , but with higher weight to $y _ { \mathrm { T } }$ , meaning that Expert 3 is preferred over Expert 1 when $y _ { \mathrm { T } }$ has a high magnitude. The policy of this expert can be summarized as:

$$
\left\{ \begin{array} { l l } { \tau _ { 1 } \approx 2 . 9 y _ { \mathrm { T } } + 4 . 8 \Delta _ { x } , } \\ { \tau _ { 2 } \approx - 3 . 6 x _ { \mathrm { T } } + 2 . 4 \Delta _ { x } + 5 . 0 \Delta _ { y } . } \end{array} \right.
$$

Most of the terms in (7) try to move away from the target (i.e., positive dependencies on $\Delta _ { x }$ and $\Delta _ { y }$ , as well as a negative dependency on $x _ { \mathrm { T } }$ . However, the positive dependency on $y _ { \mathrm { T } }$ in $\tau _ { 1 }$ , shows an attempt to strike a balance between moving towards the desired $y _ { \mathrm { T } }$ and moving away from $x _ { \mathrm { T } }$ . This likely builds momentum for reaching the target in the following steps, through the use of other experts.

Expert 4 $( S _ { 4 } \approx 4 . 1 y _ { \mathrm { T } } + 2 . 1 \Delta _ { x } - 6 . 0 \Delta _ { y } )$ Expert 4 implements a simple policy, described by:

$$
\left\{ \begin{array} { l l } { \tau _ { 1 } \approx - 3 . 9 x _ { \mathrm { T } } , } \\ { \tau _ { 2 } \approx 2 . 9 \Delta _ { y } . } \end{array} \right.
$$

This policy tends to move away from the target. In fact, in $\tau _ { 1 }$ we have a negative dependency on $x _ { \mathrm { T } }$ , and in $\tau _ { 2 }$ a positive one w.r.t. $\Delta _ { y } = y _ { f } - y _ { \mathrm { T } }$ , thus a positive weight pushing $y _ { f }$ farther away from $y _ { \mathrm { T } }$ .

Expert 5 $( S _ { 5 } \approx 3 . 5 \Delta _ { x }$ ) This expert’s policy is:

$$
\left\{ \begin{array} { l l } { \tau _ { 1 } \approx 5 . 9 y _ { \mathrm { T } } - 5 . 2 \Delta _ { y } , } \\ { \tau _ { 2 } \approx - 3 . 4 x _ { \mathrm { T } } + 1 3 y _ { \mathrm { T } } . } \end{array} \right.
$$

The torque $\tau _ { 1 }$ aims for $y _ { f }$ to reach $y _ { \mathrm { T } }$ through combined feed-forward and feed-back control approaches, while $\tau _ { 2 }$

Router Router Router Router col. 1 Expert 1 col. 2 Expert 2 col. 3 Expert 3 col. 4 Expert 4 cosine of the angle of the first arm cosine of the angle of the second arm sine of the angle of the first arm sine of the angle of the second arm $\times$ -coordinate of the target -7.1 -3.6 -3.9 y-coordinate of the target 2.7 -8.2 -3.5 3.1 2.8 3.9 2.9 4.1 10 angular velocity of the first arm angular velocity of the second arm   
x-value of fingertip position - target position -4.0 -2.7 -3.0 4.8 2.4 2.1 5   
y-value of fingertip position - target position -5.6 8.3 -4.5 3.4 -5.8 5.0 -6.0 2.9   
z-value of fingertip position - target position Router Router Router Router 0 col. 5 Expert 5 col. 6 Expert 6 col. 7 Expert 7 col. 8 Expert 8 cosine of the angle of the first arm cosine of the angle of the second arm 5 sine of the angle of the first arm sine of the angle of the second arm $\times$ -coordinate of the target 1-3..40 -4.5 -56..42 53.21 2.5 3.6 10 5.9 -2.5 -2.6 angular velocity of the first arm angular velocity of the second arm   
x-value of fingertip position - target position 3.5 -2.6 -2.4 -2.2   
y-value of fingertip position - target position -5.2 -4.0 -6.1 5.2 6.4 3.8 10.3 4.4   
z-value of fingertip position - target position 1 2 1 2 1 2 1 2

pushes the fingertip to reach $y _ { \mathrm { T } }$ only by using feed-forward control. Moreover, $\tau _ { 2 }$ has a non-negligible negative dependency on $x _ { \mathrm { T } }$ , contributing to keeping the joint away from it.

Expert 6 $( S _ { 6 } \approx - 2 . 6 \Delta _ { x } - 4 . 0 \Delta _ { y } )$ Score $S _ { 6 }$ , suggests that Expert 6 is called when the difference between the fingertip’s and the target’s coordinates have large negative values. The policy works as follows:

$$
\begin{array} { r } { \left\{ \tau _ { 1 } \approx - 4 . 5 x _ { \mathrm { T } } - 6 . 1 \Delta _ { y } , \right. \qquad } \\ { \left. \tau _ { 2 } \approx - 6 . 2 x _ { \mathrm { T } } + 5 . 4 y _ { \mathrm { T } } + 5 . 2 \Delta _ { y } . \right. } \end{array}
$$

While this policy is more intricate, we can interpret all the individual contributions, which will be combined at runtime (akin to the superposition principle in linear systems). In $\tau _ { 1 }$ , we have a negative contribution from $x _ { \mathrm { T } }$ , pushing the joint away from it, and a negative contribution w.r.t. $\Delta _ { y }$ , moving the joint in such a way that the distance between the fingertip and the target (on the y-axis) is reduced. On $\tau _ { 2 }$ we have a negative dependency on $x _ { \mathrm { T } }$ , and a positive dependency on $y _ { \mathrm { T } }$ (akin to feed-forward control), moving the joint in the same direction of $y _ { \mathrm { T } }$ , plus a positive dependency on $\Delta _ { y }$ , which makes the fingertip move away from the target on the y-axis. While these two effects seem opposite, it is worth noticing that we can rework the equation as follows: $\tau _ { 2 } \approx$ $- 6 . 2 x _ { \mathrm { T } } + 5 . 4 y _ { \mathrm { T } } + 5 . 2 \Delta _ { y } = - 6 . 2 x _ { \mathrm { T } } + 5 . 4 y _ { \mathrm { T } } + 5 . 2 y _ { f } -$ $5 . 2 y _ { \mathrm { T } } \approx - 6 . 2 x _ { \mathrm { T } } + 5 . 2 y _ { f } .$ . This revised equation can be easily interpreted as: $\tau _ { 2 }$ tends to (i) move away from $x _ { \mathrm { T } }$ , and (ii) move towards the positive side of the y-axis.

Expert 7 $( S _ { 7 } \approx - 2 . 5 y _ { \mathrm { T } } + 6 . 4 \Delta _ { y } )$ Score $S _ { 7 }$ suggests that Expert 7 is queried when the target is on the negative side of the y-axis, and the distance (also on the y-axis) has a large value (implying that the fingertip’s y coordinate is larger than $y _ { \mathrm { T } } .$ ). The policy is:

$$
\left\{ \begin{array} { l l } { \tau _ { 1 } \approx 5 . 2 x _ { \mathrm { T } } + 3 . 1 y _ { \mathrm { T } } , } \\ { \tau _ { 2 } \approx 2 . 5 x _ { \mathrm { T } } - 2 . 4 \Delta _ { x } + 3 . 8 . \Delta _ { y } . } \end{array} \right.
$$

We can see that $\tau _ { 1 }$ uses a kind of feed-forward control to move towards both $x _ { \mathrm { T } }$ and $y _ { \mathrm { T } }$ . Instead, $\tau _ { 2 }$ , moves joint 2 towards $x _ { \mathrm { T } }$ (also exploiting $\Delta _ { x }$ ), simultaneously moving it away from $y _ { \mathrm { T } }$ , through a positive contribution w.r.t. $\Delta _ { y }$

Expert 8 $( S _ { 8 } \approx - 2 . 6 y _ { \mathrm { T } } + 1 0 . 3 \Delta _ { y } )$ Score $S _ { 8 }$ , similarly to $S _ { 7 }$ , suggests that Expert 8 is called when $y _ { \mathrm { T } } \le 0$ and $y _ { f } \geq$ $y _ { \mathrm { T } }$ . However, the larger weights suggest that this expert is called way more often than Expert 7. Its policy is:

$$
\left\{ \begin{array} { l l } { \tau _ { 1 } \approx 3 . 6 x _ { \mathrm { T } } - 2 . 2 \Delta _ { x } , } \\ { \tau _ { 2 } \approx 4 . 4 \Delta _ { y } . } \end{array} \right.
$$

Here $\tau _ { 1 }$ can be interpreted as simply performing combined feed-forward and feed-back control, while $\tau _ { 2 }$ tends to move the joint farther from the target (on the y-axis).

# Conclusions

In this paper, we introduced SMOSE, a novel approach for training sparsely activated and interpretable controllers using a top-1 Mixture-of-Experts architecture. By integrating interpretable shallow decision-makers, each specializing in different basic skills, and an interpretable router for task allocation, SMOSE strikes a balance between performance and interpretability. The evaluation of SMOSE presented in this work demonstrates its competitive performance, outperforming existing interpretable baselines and narrowing the performance gap with non-interpretable state-of-the-art methods. The transparency of SMOSE is also showcased in this work, through an in-depth interpretation. Additionally, by distilling DTs from the learned router, we supply an additional tool to facilitate the interpretation of the trained models, making SMOSE a compelling choice for scenarios where both high performance and interpretability are required. As future work, we plan to explore the potential of SMOSE in more complex environments and extend it to multi-agent scenarios, exploiting socially-inspired reward designs to achieve interpretable cooperative and coordinated AI policies.