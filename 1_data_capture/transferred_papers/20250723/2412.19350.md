# On the Expressiveness and Length Generalization of Selective State-Space Models on Regular Languages

Aleksandar Terzi´c1, 2, Michael Hersche1, Giacomo Camposampiero1, 2, Thomas Hofmann2, Abu Sebastian1, Abbas Rahimi1

1IBM Research - Zurich 2ETH Z¨urich aleksandar.terzic1, michael.hersche, giacomo.camposampiero1 @ibm.com, thomas.hofmann $@$ inf.ethz.ch, ase, abr @zurich.ibm.com

# Abstract

Selective state-space models (SSMs) are an emerging alternative to the Transformer, offering the unique advantage of parallel training and sequential inference. Although these models have shown promising performance on a variety of tasks, their formal expressiveness and length generalization properties remain underexplored. In this work, we provide insight into the workings of selective SSMs by analyzing their expressiveness and length generalization performance on regular language tasks, i.e., finite-state automaton (FSA) emulation. We address certain limitations of modern SSM-based architectures by introducing the Selective Dense State-Space Model (SD-SSM), the first selective SSM that exhibits perfect length generalization on a set of various regular language tasks using a single layer. It utilizes a dictionary of dense transition matrices, a softmax selection mechanism that creates a convex combination of dictionary matrices at each time step, and a readout consisting of layer normalization followed by a linear map. We then proceed to evaluate variants of diagonal selective SSMs by considering their empirical performance on commutative and non-commutative automata. We explain the experimental results with theoretical considerations.

Code — https://github.com/IBM/selective-dense-statespace-model

# 1 Introduction

Large language models (LLMs) are most often based on the Transformer architecture (Vaswani et al. 2017), a neural network that is highly parallelizable across a sequence of tokens. The parallelizability, coupled with hardware-aware implementations of the model (Dao 2024), have allowed for efficient training over large corpora of long sequences. However, despite empirical breakthroughs in natural language processing (NLP), recent theoretical studies demonstrate that the Transformer has limited expressiveness, in particular when faced with state-tracking problems such as deciding the truth value of regular language expressions, i.e., emulating finite-state automata (FSA) (Hahn 2020; Bhattamishra, Ahuja, and Goyal 2020; Merrill and Sabharwal 2023).

On the other hand, nonlinear recurrent neural networks (RNNs) can emulate any FSA; this can be seen by considering explicit mappings of FSA dynamics onto RNN weights, as surveyed in (Svete and Cotterell 2023). In practice, RNNs learn to emulate various FSA and often generalize to sequences much longer than those seen in training (Dele´tang et al. 2023). However, in contrast to the Transformer, RNNs cannot be parallelized across the sequence length.

Recently, a novel family of sequence models has emerged: linear state-space models (SSMs) (Gu, Goel, and Re´ 2022; Gupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023; Orvieto et al. 2023). SSMs provide an alternative sequence processing backbone that can be executed in parallel during training and sequentially during inference. As a key driver for higher computational efficiency, many (selective) SSMs utilize diagonal rather than dense transition matrices (Gupta, Gu, and Berant 2022; Gu et al. 2022; Smith, Warrington, and Linderman 2023; Orvieto et al. 2023; Gu and Dao 2023; De et al. 2024), allowing parallel scans for efficient training while remaining effective in many tasks of interest. The most recent variants based on diagonal selective SSMs outperform the Transformer on several benchmarks, including language modeling (Gu and Dao 2023).

Although formal limits on the expressiveness of selective SSMs have recently been derived in the literature (Zubic´ et al. 2024; Orvieto et al. 2024; Merrill, Petty, and Sabharwal 2024; Cirone et al. 2024; Sarrof, Veitsman, and Hahn 2024; Grazzi et al. 2024), the performance of selective SSMs on FSA emulation has not been sufficiently explored. In this work, we experimentally and analytically study the capabilities of SSMs and selective SSMs to generalize to longer sequences than seen during training on a set of various FSA emulation tasks. Our contributions are as follows:

In Sec. 3, we introduce the first selective SSM capable of perfect $( \geq 9 9 . 9 \% )$ length generalization on FSA emulation using a single layer. We call this model SD-SSM, the Selective Dense State-Space Model. SD-SSM utilizes a dictionary of dense unstructured transition matrices, a softmax selection mechanism that creates a convex combination of a fixed number of transition matrices at each step, and finally applies a readout consisting of layer normalization followed by a linear map. We identify that a common design choice, the presence of a nonlinear readout, prevents SDSSM from achieving full accuracy on a challenging FSA emulation task. We compare the model with the standard RNN and the LSTM (Hochreiter and Schmidhuber 1997)

in terms of the minimal sample length required to generalize in length and find that SD-SSM exhibits better length generalization. Moreover, running SD-SSM with a parallel algorithm yields a notable speed-up over its sequential implementation.

In Sec. 4, we take a closer look at selective SSMs with diagonal complex transition matrices. We evaluate them on a set of FSA emulation tasks and analyze the effects of different architectural design choices on their performance on a commutative and non-commutative automaton. We find that perfect in-domain accuracy can be achieved on both automata, but length generalization is significantly worse on the non-commutative one. We explain our experimental results with such systems by demonstrating that, under an assumption on the mapping of FSA to selective SSMs, singlelayer selective diagonal SSMs obey a tighter upper bound on expressiveness than the one shown in (Merrill, Petty, and Sabharwal 2024).

# 2 Background

In this section we provide an overview of selective SSMs and FSA, and present an exact mapping of any FSA to the weights of a selective SSM.

# State-Space Models (SSMs) and Selective SSMs

As their backbone, SSMs implement the standard linear time-invariant system of equations:

$$
\begin{array} { l } { x _ { t } = A x _ { t - 1 } + B u _ { t } } \\ { y _ { t } = C x _ { t } + D u _ { t } } \end{array}
$$

With $A \in \mathbb { R } ^ { n \times n }$ , $B \in \mathbb { R } ^ { n \times d }$ , $C \in \mathbb { R } ^ { d \times n }$ and $D \in \mathbb { R } ^ { d \times d }$ Since any real $n \times n$ matrix is diagonalizable up to an arbitrarily small perturbation of its entries, the above system can be equivalently represented using complex diagonal transition matrices (Orvieto et al. 2023). The diagonal form is significantly more efficient to evaluate. Because the system is linear in the hidden state $\boldsymbol { x } _ { t }$ , the sequence $( x _ { 1 } , . . . , x _ { T } )$ can be computed using parallel algorithms (Martin and Cundy 2018; Gu et al. 2021).

Selective SSMs ( $\mathrm { G u }$ and Dao 2023) implement the following system of equations:

$$
\begin{array} { l } { x _ { t } = A ( u _ { t } ) x _ { t - 1 } + b ( u _ { t } ) } \\ { y _ { t } = c ( x _ { t } ) + d ( u _ { t } ) } \end{array}
$$

With $A ( u _ { t } ) \ \in \ \mathbb { R } ^ { n \times n } , \ b \ : \ \mathbb { R } ^ { d } \ \to \ \mathbb { R } ^ { n } , \ c \ : \ \mathbb { R } ^ { n } \ \to \ \mathbb { R } ^ { d }$ , and $d : \mathbb { R } ^ { d }  \mathbb { R } ^ { d }$ . In contrast to standard SSMs, selective SSMs generate the matrix $A$ dynamically as a function of the input $\boldsymbol { u } _ { t }$ . While most SSMs can be diagonalized, selective SSMs can only be diagonalized if all $A ( u _ { t } )$ matrices are simultaneously diagonalizable. This is a more restrictive condition than diagonalizability, as it requires that the product $A ( u _ { t } ) A ( u _ { t ^ { \prime } } )$ commutes $\forall t , t ^ { \prime } \in [ 1 , T ]$ . This system can also be evaluated in parallel using the parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Gu and Dao 2023).

![](images/33b66ac8f43500b96ca9a0aa4fc9bbda9d1a2596fff053a43e7cddc92d9f0f47.jpg)  
Figure 1: The parity automaton with $\scriptstyle { \mathrm { Q = } } \{ E \nu e n , O d d \}$ and $\Sigma \ = \ \{ 0 , 1 \}$ . The automaton starts in the Even state, toggles on input 1, and makes no transition on input 0.

# Finite-State Automata (FSA)

A deterministic finite-state automaton (FSA) is an abstract model of computation defined as a 5-tuple $( Q , \Sigma , \delta , q _ { \mathrm { i n i t } } , F )$ , where $Q$ is a finite set of states, $\Sigma$ is a finite input alphabet, $\delta : Q \times \Sigma  Q$ is the transition function, $q _ { \mathrm { i n i t } } \in Q$ is a designated initial state, and $F \subseteq Q$ is the set of accepting states. In this work, we are not interested in the set $F$ , and $q _ { \mathrm { i n i t } }$ is only of limited interest. This leads us to the definition of a semi-automaton, which is a 3-tuple $( Q , \Sigma , \delta )$ with $Q , \Sigma$ , and $\delta$ defined as above.

A rich body of work connects semiautomata with algebraic semigroups (Straubing 1994; Krohn and Rhodes 1965; Liu et al. 2023; Merrill, Petty, and Sabharwal 2024). A semigroup is a set with an associative binary operation defined on it. Every semiautomaton induces a transformation semigroup consisting of a set of functions $\rho : Q  Q$ defined for each $\sigma \in \Sigma$ by the transition function $\delta ( \cdot , \sigma )$ . The associative binary operation on this set is function composition. See, for example, the parity automaton shown in Figure 1.

The corresponding transformation semigroup consists of two elements, these being the transition functions corresponding to the two inputs, $\delta ( \cdot , 0 )$ and $\delta ( \cdot , 1 )$ . The transition function $\delta ( \cdot , 0 )$ corresponds to the identity operation, since no matter in which state $q$ the automaton is in, $\delta ( q , 0 ) = q$ . If the states are one-hot encoded, $\delta ( \cdot , 0 )$ is equivalently to the $2 \times 2$ identity matrix. Meanwhile, $\delta ( \cdot , 1 )$ can be equivalently represented as a $2 \times 2$ matrix with zeros on the diagonal and ones in the off-diagonal entries. This matrix toggles the onehot encoded state. Using matrix representation, the function composition can be equivalently represented as matrix multiplication. Therefore, the final state of the automaton can be obtained by evaluating the chain of matrix products corresponding to the given sequence of inputs. For a deeper discussion of the topic, we recommend (Liu et al. 2023).

# Mapping an FSA to a Selective SSM

Any FSA can be mapped to a selective SSM. To see this, we show a mapping procedure that is conceptually equivalent to those of (Merrill, Petty, and Sabharwal 2024; Liu et al. 2023). For each $q \in Q$ , encode it using $e n c : Q \to \mathbb { R } ^ { | Q | }$ such that the encodings of different states are orthogonal. Orthogonality is a sufficient, but not a necessary, condition for mapping an FSA to a selective SSM. Given the encoding of the states, we can now map the transition function $\delta : Q \times \Sigma \ \to \ Q$ to the transition matrices $A ( u _ { t } )$ from Eq. (3). In this mapping, each symbol in the input alphabet $\sigma \in \Sigma$ has an associated transition matrix $A ( \sigma )$ defined by the transition function $\delta ( \cdot , \sigma ) : Q \to Q$ . The mapping between this function and $A ( \sigma )$ is defined via the sum $\begin{array} { r } { \dot { \boldsymbol { A } } ( \bar { \boldsymbol { \sigma } } ) = \sum _ { \boldsymbol { q } \in \boldsymbol { Q } } e n c ( \delta ( \boldsymbol { q } , \boldsymbol { \sigma } ) ) \cdot e n c ( \boldsymbol { q } ) ^ { T } } \end{array}$ .

We now have all the ingredients needed to map any FSA to Eq. (3). This is achieved by setting $x _ { 0 } = q _ { i n i t }$ , identifying the inputs $\boldsymbol { u } _ { t }$ as elements of the alphabet $\Sigma$ and thus setting $A ( u _ { t } ) = A ( \sigma )$ as above, and setting $B = 0$ . By induction, one can see that $x _ { t } = e n c ( q _ { t } )$ with $q _ { t }$ achieved by $t$ -fold repeated application of the transition function $\delta$ onto $q _ { i n i t }$ given a sequence of inputs $( \sigma _ { 1 } , . . . , \sigma _ { t } )$ .

# 3 Experimental Analysis of SD-SSM on Regular Languages

This section presents our first contribution. We start with an empirical study of different sequence models on various FSA emulation tasks. The models are evaluated in terms of their length generalization capabilities. We then propose a novel selective SSM, SD-SSM, that successfully learns to emulate the dynamics of complex FSAs using a single layer.

# Task Description and Experimental Setup

The investigated tasks involve tracking the state transitions of different FSAs. The experimental code is based on (Dele´tang et al. 2023) and (Liu et al. 2023). We evaluate our models on seven different FSAs. Parity, Even Pairs, Cycle, and Arithmetic are taken from (Dele´tang et al. 2023). We further define three automata based on the Cayley diagrams of different algebraic groups. $C _ { 2 } \times C _ { 4 }$ , the direct product of cyclic groups $C _ { 2 }$ and $C _ { 4 }$ , is a commutative, solvable group with eight states. $D _ { 4 }$ , the dihedral group with eight elements, is a non-commutative, solvable group. $A _ { 5 }$ is the group of even permutations of five elements, a nonsolvable group with 60 states. The tasks are described in Appendix $\mathbf { A } ^ { 1 }$ .

At each training step, we uniformly sample a sequence length $l$ between 1 and the maximum training length $L$ . We then generate a random input sequence $( \sigma _ { 1 } , \dots , \sigma _ { l } )$ and use it to emulate the automaton. The models are trained to minimize the cross-entropy loss between their output at the final step and the final state of the emulated automaton. As in (Dele´tang et al. 2023), we train the model for a fixed number of steps and report the test accuracy of the model at the final training step. The hyperparameters for reproducing the experiments are reported in Appendix B.

# Modern SSMs Fail to Emulate FSAs

We start the discussion by evaluating prominent sequence models from the literature on various tasks using the experimental setup described above. Our evaluation includes the standard nonlinear RNN (Elman 1990), the Transformer (Vaswani et al. 2017; Ruoss et al. 2023), S4D (Gu et al. 2022), H3 (Fu et al. 2023), Mamba (Gu and Dao 2023), as well as the block-diagonal selective SSM RegularLRNN (Fan, Chi, and Rudnicky 2024). All models are trained on sequences of length up to 40 and are tested on sequences of length 1 up to 500. We train the models using 3 random seeds and report the maximum/average area under the accuracy vs. length curve. Except for the Transformer, we report all results using a single layer.

![](images/84b560681590a7f3b03e6f51d69a8185c348579cdf45edd0bf8123143c19279a.jpg)  
Figure 2: The SD-SSM model consists of three main steps. Firstly, the dense transition matrices for all time steps are generated using a softmax selection mechanism and operator normalization inspired by (Fan, Chi, and Rudnicky 2024). The second step is the linear recurrence from Eq. (3). The final step is the readout, consisting of layer normalization followed by a linear map.

As shown in Table 1, none of the aforementioned models achieve perfect length generalization on all of the tasks. In fact, the models often fail already on in-domain lengths. As a particularly important example consider Mamba, the most recently investigated model and one that has shown the most promise as an alternative to the Transformer as LLM backbone. On the Arithmetic task, the best Mamba model achieves an in-domain accuracy of $9 1 . 6 \%$ . It exhibits an accuracy of below $9 9 \%$ for the first time with sequences of length 22. The performance then rapidly drops with longer sequences, yielding the low accuracy $( 2 0 . 1 \% )$ reported in Table 1. We ran hyperparameter search in which we varied the state size, the learning rate, the weight decay factor, and trained for $1 0 ^ { 6 }$ steps with a batch size of 128. The Mamba model described above is the best out of 96 runs.

Additional results with two layers of S4D, H3, Mamba, as well as S4 (Gu, Goel, and Re´ 2022) and Hyena (Poli et al. 2023) are reported in Appendix C. While the models often exhibit better length generalization with two layers than with a single one, none of them achieve significant length generalization on FSA emulation.

# SD-SSM Learns to Emulate FSA

We now present an architecture that successfully learns to emulate a wide range of different FSA dynamics using a single layer. We call the model SD-SSM, standing for Selective Dense State-Space Model. The model architecture is presented in Figure 2. It can be conceptually separated into

<html><body><table><tr><td>Task</td><td>RNN</td><td>Transformer</td><td>S4D</td><td>H3</td><td>Mamba</td><td>RegularLRNN</td><td>C Diagonal</td><td>SD-SSM (ours)</td></tr><tr><td colspan="9">(Delétang et al. 2023)</td></tr><tr><td>Parity</td><td>100 /100</td><td>52.3 /50.4</td><td>50.1/50.0</td><td>50.0 / 50.0</td><td>50.3 / 50.1</td><td>100 /100</td><td>99.3 / 72.4</td><td>100 /100</td></tr><tr><td>Even Pairs</td><td>100 /100</td><td>100/100</td><td>50.4 / 50.3</td><td>51.0 / 50.5</td><td>100 /100</td><td>100 /100</td><td>54.5 / 54.3</td><td>100 /100</td></tr><tr><td>Cycle</td><td>100 /100</td><td>73.6 /52.9</td><td>33.6 /29.2</td><td>20.1/20.0</td><td>21.1 / 21.0</td><td>100 /100</td><td>99.6 / 90.4</td><td>100 /100</td></tr><tr><td>Arithmetic</td><td>100 /100</td><td>25.5 /23.5</td><td>20.1/20.0</td><td>20.1/20.0</td><td>20.1/20.1</td><td>33.3/30.2</td><td>22.1/21.7</td><td>99.9 / 98.5</td></tr><tr><td colspan="9">(Liu et al. 2023)</td></tr><tr><td>C2 × C4</td><td>100/100</td><td></td><td></td><td></td><td></td><td>100 /99.4</td><td>79.2/ 60.4</td><td>100 /93.3</td></tr><tr><td>D4</td><td>100 /100</td><td></td><td></td><td></td><td></td><td>100 /100</td><td>32.6 / 29.8</td><td>99.9 / 99.9</td></tr><tr><td>A5</td><td>100 /100</td><td></td><td></td><td></td><td></td><td>100 /100</td><td>8.3/ 8.2</td><td>100 /100</td></tr></table></body></html>

Table 1: Maximum/average length generalization accuracy $( \% )$ on FSA emulation tasks over three random seeds using singlelayer models, except the Transformer, which uses five layers. The Transformer results are taken from (Ruoss et al. 2023) and use randomize RoPE positional encodings. While they evaluate on sequences of length 50 to 500 and we evaluate on length 1 to 500, the failure of the model is still evident. We denote the selective SSM defined by (Fan, Chi, and Rudnicky 2024) as RegularLRNN. The complex $( \mathbb { C } )$ diagonal model is a diagonal selective SSM which we defined in Sec. 4. Our SD-SSM achieves near-perfect average accuracy on all investigated automata.

three different phases:

$$
\begin{array} { l l } { \displaystyle { \cal A } ( u _ { t } ) = \mathrm { O p N o r m } ( \sum _ { i = 1 } ^ { k } \mathrm { s o f t m a x } ( S u _ { t } ) [ i ] A _ { i } ) } \\ { \displaystyle x _ { t + 1 } = A ( u _ { t } ) x _ { t } + B u _ { t } } \\ { \displaystyle y _ { t } = C \mathrm { ( L a y e r N o r m } ( x _ { t } ) ) } \end{array}
$$

In the first phase, the transition matrices $A ( u _ { t } )$ are generated for each input $u _ { t }$ in parallel. This is achieved by passing the input embeddings through a linear layer $( S )$ and then processing the resulting vector using the softmax function. The outputs of the softmax are used to weigh a dictionary of $k$ - many trainable dense transition matrices labeled $A _ { 1 } , . . . , A _ { k }$ . In our experiments, we use between 5 and 20 transition matrices (see Appendix B).

The weighted matrices are summed together and then modified using an operator normalization procedure (OpNorm). Operator normalization is required for the stability of the system, preventing the eigenvalues of the transition matrices from growing beyond 1. The avenue we pursue is heavily inspired by the concurrent work of (Fan, Chi, and Rudnicky 2024), which normalizes the generated $A ( u _ { t } )$ matrices before applying the recursion in Eq. (3). Their normalization scheme consists of normalizing the columns of $A ( u _ { t } )$ by setting each column vector $a _ { i }$ thereof to $a _ { i } / \mathrm { m a x } ( 1 , l _ { p } ( a _ { i } ) )$ , with $\bar { l } _ { p } ( \cdot )$ denoting the standard $l _ { p }$ - norm operation. We adopt a version of the column-wise normalization scheme in our SD-SSM. Concretely, we divide each column of $A$ by its $l _ { p }$ -norm, $a _ { i } ~  ~ a _ { i } / l _ { p } ( a _ { i } )$ . While (Fan, Chi, and Rudnicky 2024) finds that $p \ = \ 1 . 2$ works well across all tasks, we varied $p \in [ 1 . 0 , 1 . 5 ]$ across the tasks via a hyperparameter search.

The final two phases consist of executing the recurrence in Eq. (3), followed by a readout of the state value. The readout consists of Layer Normalization (Ba, Kiros, and Hinton 2016) followed by a linear layer. We find the design of the readout to be especially important for length generalization. In the experiments that we present in the following subsection, we see that the typically used MLP readout, such as what (Fan, Chi, and Rudnicky 2024) used, has a negative impact on the generalization properties of our model.

We compare our matrix generation with RegularLRNN, which generates transition matrices as $A ( u ) = \bar { W } _ { 2 } \sigma ( W _ { 1 } u )$ . Their transition matrices are block-diagonal, and we assume that the block size equals the square root of the state dimensionality, as was also the case in their experiments. The total number of parameters in their transition matrix generator is then $d n { \sqrt { n } } + n ^ { 3 }$ , while our generation incurs a cost of $k ( d + n ^ { 2 } )$ . For a fixed $k$ , our generation method is more parameter efficient.

The results on FSA emulation using a one-layer SD-SSM are reported in Table 1. As shown, on all of the tasks we investigate, the best SD-SSM achieves perfect $( \geq 9 9 . 9 \% )$ accuracy using only one layer. We do observe that SD-SSM exhibits higher variability on the $C _ { 2 } \times C _ { 4 }$ automaton compared to RegularLRNN. RegularLRNN with one layer does however not perform well on Arithmetic.

# Nonlinear Readout Hurts State Tracking

We additionally ablate the SD-SSM readout. Apart from the desire for simplicity, the SD-SSM’s readout also emerged from the observation that a more complex readout has detrimental effects on the length generalization. On the Arithmetic task, we conducted extensive experiments in which the linear layer in SD-SSM’s readout was replaced by a standard two-layer MLP with the ReLU non-linearity. The hidden layer of the MLP was configured to consist of 64 units, equal to the state size of the model. We varied the learning rate in $\{ 2 \mathrm { e } { - } 5 , 1 \mathrm { e } { - } 4 , 5 \mathrm { e } { - } 4 \}$ , the $p$ parameter in $l _ { p }$ normalization in $\{ 1 . 1 , 1 . 2 , 1 . 3 \} $ , and we experimented with two regularization techniques, weight decay in $\{ 0 , 1 { \mathrm { e } } { - } 4 , 1 { \mathrm { e } } { - } 3 \}$ and dropout in $\{ 0 . 1 , 0 . \bar { 2 } , 0 . 5 \}$ on the intermediate activations of the MLP readout. The best accuracy was achieved by using weight decay. The model achieves an accuracy of $7 1 . 9 \%$ , significantly below $9 9 . 9 \%$ achieved by the linear readout SD-SSM. Further results are shown in Appendix C.

Table 2: Maximum length generalization accuracy $( \% )$ on sequences up to length 500 over three random seeds. The models were trained to emulate the $A _ { 5 }$ automaton with very short sequences (4 to 8) using a state size of 128.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="4">Training Length</td></tr><tr><td>4</td><td>5</td><td>6 7</td><td>8</td></tr><tr><td>RNN</td><td>7.1</td><td>26.1</td><td>85.4 99.4</td><td>99.9</td></tr><tr><td>LSTM</td><td>14.7</td><td>66.9</td><td>97.6 99.9</td><td>100</td></tr><tr><td>SD-SSM (ours)</td><td>31.5</td><td>83.3</td><td>97.2 99.4</td><td>100</td></tr></table></body></html>

Table 3: Forward $^ +$ backward pass runtime (seconds) required to evaluate one batch (16) with SD-SSM using a sequential or a parallel algorithm. The model uses one layer and a state size of 64.   

<html><body><table><tr><td rowspan="2">Compute Mode</td><td colspan="4">Sequence Length</td></tr><tr><td>64</td><td>128</td><td>256</td><td>512</td></tr><tr><td>Recurrent</td><td>1.82 s</td><td>6.40 s</td><td>21.93 s</td><td>77.27 s</td></tr><tr><td>Parallel</td><td>1.91 s</td><td>4.55 s</td><td>11.67 s</td><td>26.68 s</td></tr></table></body></html>

# Length Efficiency Analysis

We further consider a more demanding experimental setup. We compare how well RNN, LSTM, and SD-SSM extrapolate to longer sequences when trained on very short sequences. Concretely, the initial state of the network is chosen uniformly at random from the set of automaton states, and the model is trained to predict the automaton state after consuming an input of a short length, up to 8. As the training sequences are very short, we observe overfitting: the training loss will often notably increase after having plateaued at a low value. Thus, we validate the models on sequences up to length 40 and report the accuracy obtained on sequences up to length 500. Further details on this experimental setup are outlined in Appendix B. Table 2 shows favorable results for SD-SSM in this challenging task. For very short sequences, it exhibits better length generalization compared to the RNN and the LSTM with the same state size. Further results are provided in Appendix C.

# SD-SSM Can Leverage Parallel Scans

SD-SSM’s linear recurrence over the hidden state allows us to use the parallel scan algorithm to compute the sequence of states $x _ { 1 } , \ldots , x _ { t }$ . Table 3 reports the combined forward and backward pass runtime (in seconds) of a single layer SDSSM on an NVIDIA V100 with different sequence lengths $( \mathrm { L } )$ , a state size of 64, and a batch size of 16 in PyTorch. We use the implementation of the parallel scan algorithm from (Fan, Chi, and Rudnicky 2024). The parallel algorithm allows SD-SSM to be trained more efficiently than in sequential mode, despite its use of unstructured matrices.

# 4 A Limitation of Diagonal Selective SSMs

To better understand the need for dense transition matrices, we start this section by evaluating diagonal selective SSMs on the previously used set of regular language tasks. We observe that it exhibits significantly lower scores than its dense counterpart, SD-SSM. We then take a closer look at the performance of different architectural variants of a diagonal selective SSM on two selected automata, one being commutative and the other being non-commutative with respect to the inputs. We find that diagonal selective SSMs tend to perform significantly better on the commutative automaton. We explain our experimental findings by drawing a connection between the presented $\mathrm { F S A } $ selective SSM mapping and results from linear system theory.

# Diagonal Selective SSMs Fail to Emulate FSAs

We first present and evaluate a selective SSM that utilizes complex-valued diagonal transition matrices on the full set of FSA tasks, motivated by the prominence of such models in modern literature (Gupta, Gu, and Berant 2022; Orvieto et al. 2023). The models we use in our experiments are variations on the following, general model:

$$
\begin{array} { r l } & { \tilde { A } _ { \mathrm { R e , I m } } ( u _ { t } ) = W _ { \mathrm { R e , I m } } ^ { o } ( \sigma ( W _ { \mathrm { R e , I m } } ^ { i } ( u _ { t } ) ) ) } \\ & { \tilde { A } ( u _ { t } ) [ m ] = ( \tilde { A } _ { \mathrm { R e } } ( u _ { t } ) [ m ] + i \tilde { A } _ { \mathrm { I m } } ( u _ { t } ) [ m ] ) } \\ & { A ( u _ { t } ) [ m ] = \beta \tilde { A } ( u _ { t } ) [ m ] / | \tilde { A } ( u _ { t } ) [ m ] | } \\ & { x _ { t + 1 } = A ( u _ { t } ) \odot x _ { t } + B u _ { t } } \\ & { y _ { t } = W _ { o } ^ { y } ( \sigma ( W _ { i } ^ { y } ( \mathrm { R e } ( x _ { t } ) \oplus \mathrm { I m } ( x _ { t } ) ) ) } \end{array}
$$

with $W _ { R e , I m } ^ { i } \in \mathbb { R } ^ { n \times d }$ , $W _ { R e , I m } ^ { o } \in \mathbb { R } ^ { n \times n }$ , $W _ { i } ^ { y } \in \mathbb { R } ^ { 2 n \times 2 n }$ , $W _ { o } ^ { y } ~ \in ~ \mathbb { R } ^ { d \times 2 n }$ , $B \ \in \ \mathbb { C } ^ { n \times d }$ , $\beta \in \ [ 0 , 1 ]$ , $\odot$ denoting the element-wise product, $\sigma ( \cdot )$ an element-wise activation function, in our case ReLU, $| \cdot |$ the element-wise absolute value function, and $a \oplus b$ denoting the concatenation of the two vectors $a$ and $b$ .

The entries along the diagonal of the presented model’s transition matrices are complex numbers whose real and imaginary parts are generated as functions of the input using a two-layer MLP. They are constrained to be on the complex circle of radius $\beta$ , where $\beta$ is a hyperparameter.

At the readout, we concatenate the real and the imaginary parts of the state vector and then further process this vector using a two-layer MLP. While the nonlinear readout was detrimental in SD-SSM, we find that the $\mathbb { C }$ diagonal model exhibits better accuracy with a nonlinear readout instead.

We evaluate the $\mathbb { C }$ Diagonal model on the previously used tasks and report the results in Table 1. On Parity and Cycle, the model achieves almost perfect length generalization on the evaluated sequences, significantly better than Mamba which uses real-valued diagonal transition matrices. However, on Even Pairs, the $\mathbb { C }$ diagonal model achieves perfect accuracy on in-domain lengths, but fails drastically as soon as the sequence length is increased beyond the training domain. On Arithmetic, it fails on in-domain lengths, dropping below $9 9 \%$ already at sequence length 8.

![](images/cdb1c63b4a94d640e1f28dda2ca26ade588fc70b2940485440ab7275761f37cb.jpg)

Figure 3: Cayley diagrams of $C _ { 2 } \times C _ { 4 }$ and the $D _ { 4 }$ automata, both with two actions: toggle (blue) and move (red) (Carter 2009). $C _ { 2 } \times C _ { 4 }$ is commutative. Starting at any state, applying toggle followed by move results in the same state as move followed by toggle. The same does not hold for $D _ { 4 }$ .   
Table 4: Maximum length generalization accuracy $( \% )$ of variants of the $\mathbb { C }$ diagonal selective SSM and our SD-SSM.   

<html><body><table><tr><td colspan="4"></td><td colspan="2">Variants of C Diagonal SD-SSM</td></tr><tr><td>B=0?</td><td colspan="2">Yes</td><td colspan="2">No</td><td>No</td></tr><tr><td>Readout</td><td>Linear</td><td>Nonlin.</td><td>Linear</td><td>Nonlin.</td><td>Linear</td></tr><tr><td>C2 ×C30</td><td>100</td><td>87.6</td><td>65.8</td><td>81.7</td><td>100</td></tr><tr><td>D30</td><td>8.35</td><td>8.35</td><td>11.3</td><td>61.0</td><td>100</td></tr></table></body></html>

# Performance on Commutative and Non-Commutative Automata

We investigated in more depth the behavior of diagonal selective SSMs on the $C _ { 2 } \times C _ { 3 0 }$ and $D _ { 3 0 }$ automata. Both $C _ { 2 } \times C _ { 3 0 }$ and $D _ { 3 0 }$ are solvable groups, but $C _ { 2 } \times C _ { 3 0 }$ is commutative and $D _ { 3 0 }$ is not. According to recent theoretical results on the expressive capacity of diagonal selective SSMs (Merrill, Petty, and Sabharwal 2024; Sarrof, Veitsman, and Hahn 2024), both can be emulated by diagonal selective SSMs. Smaller versions of the automata, each with 8 instead of 60 states, are shown in Figure 3. As the automata have long diameters, i.e., the expected number of random actions required to visit each state of the automaton is large, we train the models on these two tasks with sequences up to length 90 and report the average accuracy on sequences up to length 600.

We train four different complex diagonal selective SSM variants on $C _ { 2 } \times C _ { 3 0 }$ and $D _ { 3 0 }$ FSA emulation. The two central architectural choices we ablate are the use of the $B$ matrix in the transition as well as the use of a nonlinear readout. We report the results obtained with the best-performing seed for each model in Table 4.

Firstly, we observe that all models learn to emulate $C _ { 2 } \times$ $C _ { 3 0 }$ perfectly on in-domain lengths. Their length generalization is however significantly affected by the architectural choices. Models that do not utilize the $B$ matrix tend to learn solutions that exhibit better length generalization than their counterparts which include the $B$ matrix.

The results are significantly different on $D _ { 3 0 }$ . Without the $B$ matrix, the models completely fail to learn the dynamics of the automaton, exhibiting very low in-domain accuracy.

The model exhibits significantly better length generalization once the $B$ matrix is introduced, although it is only with a nonlinear readout that the model learns to emulate the automaton even on in-domain lengths. The best length generalization accuracy on $D _ { 3 0 }$ is significantly lower than what could be achieved on the $C _ { 2 } \times C _ { 3 0 }$ automaton. Further results with more layers of the $C$ diagonal SSM with nonlinear readout and $B \neq 0$ are provided in Appendix C. Introducing more layers did not improve the model’s length generalization. In contrast, SD-SSM achieves perfect length generalization on both tasks.

# Theoretical Characterization of Diagonal Selective SSMs

Various recent works have derived different bounds on the computational capacity of diagonal selective SSMs (Merrill, Petty, and Sabharwal 2024; Sarrof, Veitsman, and Hahn 2024). However, an explanation for the behavior on commutative vs. non-commutative FSAs, as shown in Table 4, is missing. We present an analysis of systems with diagonal transition matrices using a restrictive assumption. The assumption is that models implement a mapping consistent with the one described in Sec. 2, for which the $B$ matrix is irrelevant. Single-layer diagonal selective SSMs that do not utilize the $B$ matrix are restricted to commutative automata:

Proposition 1. Given a sequence of inputs $( u _ { 1 } , . . . , u _ { T } )$ , let the transition matrices $( A ( u _ { 1 } ) , . . . , A ( u _ { T } ) )$ be simultaneously diagonalizable. Under the described mapping of $F S A$ to a single-layer selective SSMs which sets $x _ { 0 } = e n c ( q _ { i n i t } ) .$ , $b ( u _ { t } ) ~ \bar { = } ~ 0$ , and whose transition matrices are simultaneously diagonalizable, the selective SSM can only emulate commutative automata.

Proof. If the selective SSM is parametrized according to the mapping shown in Sec. 2, then it is equivalent to the following system:

$$
x _ { t + 1 } = A ( u _ { t } ) x _ { t }
$$

We assumed that the matrices $A ( u _ { t } )$ are simultaneously diagonalizable. This means that there exist a single invertible matrix $W \in \mathbb { C } ^ { n \times n }$ such that all transition matrices can be expressed as $A ( u _ { t } ) = W \Lambda ( u _ { t } ) W ^ { - 1 }$ , with the diagonal matrix $\Lambda ( u _ { t } ) \in \mathbb { C } ^ { n \times n }$ . If we insert the above decomposition into the reduced system $x _ { t \pm 1 } ~ = ~ A ( u _ { t } ) x _ { t }$ , we obtain the form $x _ { t + 1 } = W \dot { \Lambda ( u _ { t } ) } W ^ { - 1 } x _ { t }$ . The dynamics of this system are unchanged if we change the representation basis by multiplying the system from the left with $W ^ { - 1 }$ . By setting $\tilde { x } _ { t } = \dot { W } ^ { - 1 } x _ { t }$ , we see that the above system is equivalent to the diagonal system $\tilde { x } _ { t + 1 } = \Lambda ( u _ { t } ) \tilde { x } _ { t }$ . Therefore, if we interpret Eq. (13) as implementing the dynamics of an FSA, if this system admits an equivalent diagonal representation then the final automaton state is invariant to the order in which the inputs are presented.

The mapping we impose is reminiscent of several other mappings from literature (Merrill, Petty, and Sabharwal 2024; Liu et al. 2023). In fact, if a model based on Eq. (3) implements a mapping different from the one we describe in

Sec. 2, then it is not necessarily commutative. This can be seen by unrolling Eq. (1) for several time steps.

$$
\begin{array} { l } { x _ { 1 } = A _ { 1 } x _ { 0 } + b ( u _ { 1 } ) } \\ { x _ { 2 } = A _ { 2 } A _ { 1 } x _ { 0 } + A _ { 2 } b ( u _ { 1 } ) + b ( u _ { 2 } ) } \\ { x _ { 3 } = A _ { 3 } A _ { 2 } A _ { 1 } x _ { 0 } + A _ { 3 } A _ { 2 } b ( u _ { 1 } ) + A _ { 3 } b ( u _ { 2 } ) + b ( u _ { 3 } ) } \end{array}
$$

with the abbreviation $A ( u _ { t } ) \ = : \ A _ { t }$ . Since the product of diagonal matrices commutes, it is exactly the terms containing $b ( u _ { t } )$ that break the commutativity. However, while the model that utilizes the $B$ matrix learns to emulate the noncommutative automaton in our experiments, it only generalizes to a limited degree.

# 5 Related Work State-Space Models

Early SSMs build on the HiPPO theory of optimal projections (Gu et al. 2020). The S4 model (Gu, Goel, and Re´ 2022) is an early example of an SSM used in a deep neural network, and it significantly advanced the state-of-theart on a collection of long-range modeling tasks compared to the Transformer. Diagonal SSMs emerged from a desire for more efficient parallelizable computation in the form of DSS (Gupta, Gu, and Berant 2022) and S4D (Gu et al. 2022). S5 introduces effective simplified MIMO SSMs (Smith, Warrington, and Linderman 2023). The LRU (Orvieto et al. 2023) is a simplified and effective linear SSM utilizing complex-valued transition matrices. The H3 (Fu et al. 2023) presents advancements towards realistic language modeling using SSMs, but shows that such models perform best when interleaved with attention layers. Mamba (Gu and Dao 2023) is the first selective SSM to outperform the Transformer (Vaswani et al. 2017) in a range of important NLP tasks including language modeling. (Fan, Chi, and Rudnicky 2024) presents a block-diagonal selective SSM which achieves perfect length generalization on three out of the four regular language tasks from (Dele´tang et al. 2023). Compared to previous work, we are the first to demonstrate that all finite-state automata from (Dele´tang et al. 2023), and others from (Liu et al. 2023), can be emulated with single layer selective SSM utilizing a linear readout. We additionally provide experimental results with various single- and multi-layer complex-valued diagonal SSMs on FSA emulation.

# Formal Analysis of Sequence Models

The ability of neural networks to model various formal models of computation is a long-standing area of research (Siegelmann and Sontag 1995; Minsky 1967). One of the first models studied was the RNN, which can implement the dynamics of any FSA, with (Svete and Cotterell 2023) reviewing three different exact mappings of FSA to RNNs. The presented mappings are due to (Minsky 1954; Dewdney 1977; Indyk 1995). Recently, many such studies of the Transformer model have emerged. A survey of various bounds on the Transformer’s expressiveness can be found in (Strobl et al. 2024). Particularly interesting is the study due to (Merrill and Sabharwal 2023), which conjectures that any model architecture as parallelizable as the Transformer will obey limitations similar to it. (Dele´tang et al. 2023) presents experimental study of the Transformer (Vaswani et al. 2017), RNN (Elman 1990), LSTM (Hochreiter and Schmidhuber 1997), Stack-RNN (Joulin and Mikolov 2015), TapeRNN (Suzgun et al. 2019) and other architectures in formal language transduction. We extend their analysis by considering SSM-based architectures. (Merrill, Petty, and Sabharwal 2024) derives a bound on diagonal selective SSMs with logarithmic precision representation, placing them in the $T \bar { C } ^ { 0 }$ circuit complexity class. This complexity class encompasses the presented $C _ { n } \times C _ { m }$ and $D _ { n }$ groups. Their experimental results do not evaluate the length-generalization aspect of SSMs and selective SSMs. (Sarrof, Veitsman, and Hahn 2024) show that a stack of complex diagonal SSM layers can emulate any automaton in $T \bar { C ^ { 0 } }$ . Their experimental results only evaluate Mamba and the Transformer, and the commutativeness of the automata is not a central aspect of their work.

# 6 Conclusion

In this work, motivated by the inability of a wide range of sequence models to emulate arbitrary automata, we have presented SD-SSM. It utilizes a dictionary of dense transition matrices, combined at each time step using a softmax selection mechanism and operator normalization, and a readout which consists of layer normalization followed by a linear map. SD-SSM is the first selective state-space model to achieve perfect length generalization on a diverse set of FSA emulation tasks using a single layer.

We then evaluated more efficient selective SSMs with diagonal complex valued transition matrices on a set of FSA emulation tasks. We observed that they exhibit significantly worse length generalization than their dense counterparts. We probed deeper into this result by investigating their performance on two similar automata which differ in one crucial property: commutativity with respect to the inputs. Our experimental analysis confirms that diagonal selective SSMs exhibit a significantly higher degree of length generalization on the commutative automaton compared to the noncommutative one. We explain the results by drawing a connection between a general mapping of FSA dynamics onto selective SSM weights and linear system theory. Assuming that the selective SSMs do not implement an unintuitive mapping of FSA dynamics, we observe that they indeed cannot model non-commutative automata.

We list some potential avenues for future work. Firstly, SD-SSM’s softmax selection mechanism allows the use of temperature scaling and annealing strategies, which could lead to more interpretable and efficient models. Secondly, general mappings of $T C ^ { 0 }$ non-commutative automata to diagonal selective SSMs can be investigated further. Finally, the model could be evaluated on more natural data to reveal whether the increased formal expressiveness translates to other real-world applications.