# A Plug-and-Play Bregman ADMM Module for Inferring Event Branches in Temporal Point Processes

Qingmei Wang1, Yuxin $\mathbf { W } \mathbf { u } ^ { 1 }$ , Yujie $\mathbf { L o n g } ^ { 2 }$ , Jing Huang2, Fengyuan $\mathbf { R a n } ^ { 2 }$ , Bing $\mathbf { S u } ^ { 1 }$ , Hongteng $\mathbf { X } \mathbf { u } ^ { 1 * }$

1Gaoling School of Artificial Intelligence, Renmin University of China 2School of Cyber Science and Engineering, Wuhan University

# Abstract

An event sequence generated by a temporal point process is often associated with a hidden and structured event branching process that captures the triggering relations between its historical and current events. In this study, we design a new plugand-play module based on the Bregman ADMM (BADMM) algorithm, which infers event branches associated with event sequences in the maximum likelihood estimation framework of temporal point processes (TPPs). Specifically, we formulate the inference of event branches as an optimization problem for the event transition matrix under sparse and low-rank constraints, which is embedded in existing TPP models or their learning paradigms. We can implement this optimization problem based on subspace clustering and sparse group-lasso, respectively, and solve it using the Bregman ADMM algorithm, whose unrolling leads to the proposed BADMM module. When learning a classic TPP (e.g., Hawkes process) by the expectation-maximization algorithm, the BADMM module helps derive structured responsibility matrices in the Estep. Similarly, the BADMM module helps derive low-rank and sparse attention maps for the neural TPPs with selfattention layers. The structured responsibility matrices and attention maps, which work as learned event transition matrices, indicate event branches, e.g., inferring isolated events and those key events triggering many subsequent events. Experiments on both synthetic and real-world data show that plugging our BADMM module into existing TPP models and learning paradigms can improve model performance and provide us with interpretable structured event branches.

Code — https://github.com/qingmeiwangdaily/BADMM TPP

# Introduction

Temporal point processes (TPPs) are powerful tools for modeling events that occur sequentially in continuous-time domain (Kingman 1992; Ross et al. 1996). They have been extensively used to capture the dynamics of the events in various domains, such as earthquake prediction (Lewis and Shedler 1979), financial analysis (Bacry, Mastromatteo, and Muzy 2015), social network modeling (Zhou, Zha, and Song 2013a), recommendation systems (Xu, Carin, and Zha

2018), and so on. Typically, for an event sequence generated by a TPP, there exists a branching process capturing the event-level triggering patterns hidden in the sequence (Dion and Yanev 1994; Møller and Rasmussen 2006). As illustrated in Figure 1(a), the branching process is often represented as a transition matrix of events, which provides insights into the causal relationships between events and helps identify the cascades of subsequent events. Therefore, inferring such event branches from observed event sequences is crucial for us to reveal the underlying mechanism of event generation and information diffusion, e.g., identifying the source node in an information diffusion network (Farajtabar et al. 2015; Zhang et al. 2018) and controlling the diffusion of specific information (Farajtabar et al. 2014, 2017a).

Despite its significance, inferring event branches is challenging because they are latent variables unobservable in practice. For some classic TPPs like Hawkes process (Zhou, Zha, and Song 2013b), we can learn the TPPs based on observed event sequences in the expectation-maximization (EM) framework, in which the event branches are often inferred as the responsibilities of the events (i.e., the posterior probabilities of historical events given the current ones). For neural TPPs like transformer Hawkes process (THP) (Zuo et al. 2020) and self-attentive Hawkes process (SAHP) (Zhang et al. 2020), the attention maps within the models indicate the impacts of historical events on the current ones, and thus, can be treated as evidence of event branches. However, both methods often suffer the oversmoothness issue and lead to unstructured event branches. How to infer interpretable and structured event branches effectively for various TPP models is still an open problem.

In this study, we introduce a novel plug-and-play module based on the Bregman Alternating Direction Method of Multipliers (Bregman ADMM or BADMM for short) algorithm (Wang and Banerjee 2014), which helps infer event branches in the maximum likelihood estimation framework of TPPs. As illustrated in Figure 1(b), given the responsibility matrices of classic TPPs or the attention maps of neural TPPs, the BADMM module imposes low-rank and sparse structures on the matrices by solving a subspace clustering problem (Elhamifar and Vidal 2013) or a sparse group-lasso problem (Simon et al. 2013). Applying the BADMM module to the EM algorithm of classic TPPs, we can optimize the responsibility matrices iteratively to guide the learning of

eEnvteSnetqSuenqcuesnces EM for Classic TPP MLE of Neural TPP B> A qt ga d q 4t qt qat q At MLE of Model MLE of Model 0□○△□○△□ 品 ： M-step 。 1 △ □ E-step o △ Responsibility Matrix Attention Map □ aTnrsaitniosintiMoantriMxaotfrix of Implementations Based on eEnvteBnrtanBcrhaensches Our Bregman ADMM Module (a) Event branches (b) Plug-in strategies of BADMM module model parameters. For neural TPPs, we unroll the iterations of the Bregman ADMM algorithm and build the BADMM module to obtain a new attention layer. With the help of the BADMM module, we effectively penalize dense triggering patterns among events for both classic and neural TPPs, leading to structured event transition matrices and, accordingly, interpretable event branches.

Through extensive experiments on both synthetic and real-world datasets, we evaluate different implementations of the BADMM module and demonstrate their effectiveness. Experimental results show that incorporating the BADMM module into existing TPP models improves their performance and interpretability. In particular, the BADMM module not only enhances the predictive accuracy of TPP models but also provides valuable insights into the hidden event branching processes, e.g., identifying the isolated events within event sequences and those key events triggering multiple subsequent events.

# Related Work and Preliminaries

# Temporal Point Processes

Temporal point process is a classic statistical tool to model the event sequences in continuous-time domain (Kingman 1992; Wang et al. 2023). Suppose that we observe an event sequence $\grave { s } ~ = ~ \{ ( t _ { n } , c _ { n } ) \} _ { n = 1 } ^ { \tilde { N } ^ { \star } }$ , where $( t _ { n } , c _ { n } )$ is the $n$ -th event, $t _ { n } \in [ 0 , T ]$ is its timestamp, and $c _ { n } \in \mathcal { C } = \{ 1 , . . . , C \}$ is its event type. A TPP often models the dynamics of the events by a parametric multivariate intensity function, denoted as $\lambda ( t ; \theta ) = \{ \lambda _ { c } ( t ; \theta ) \} _ { c \in \mathcal { C } , t \in [ 0 , T ] }$ , where

$$
\lambda _ { c } ( t ; \theta ) \mathrm { d } t = \mathrm { d } \mathbb { E } [ N _ { c } ( t ; \theta ) | \mathcal { H } _ { t } ^ { c } ] , \forall c \in \mathcal { C } ,
$$

represents the expected instantaneous rate of the type- $c$ event happening at time $t$ given historical events. Here, $N _ { c } ( t ; \theta )$ is the counting process of the event type, which records the number of the type- $c$ events happening till time $t$ , and $\mathcal { H } _ { t } ^ { \mathcal { C } } = \{ ( t _ { n } , c _ { n } ) \in \bar { s } \vert \bar { t } _ { n } < t \}$ is the history till time $t$ . The parameters of the intensity function (and the counting process) are denoted as $\theta$ . We can learn the TPP in the maximum likelihood estimation (MLE) framework, maximizing the likelihood of the event sequence shown below:

$$
L ( \boldsymbol { s } ; \boldsymbol { \theta } ) = \prod _ { n = 1 } ^ { N } \lambda _ { c _ { n } } ( t _ { n } ; \boldsymbol { \theta } ) \exp \Bigl ( - \sum _ { c \in \mathcal { C } } \int _ { 0 } ^ { T } \lambda _ { c } ( \boldsymbol { s } ; \boldsymbol { \theta } ) \mathrm { d } s \Bigr ) .
$$

Classic TPP models, like Poisson process (Kingman 1992), Hawkes process (Hawkes 1971), self-correcting process (Isham and Westcott 1979), and their multivariate versions (Zhou, Zha, and Song 2013b; Xu et al. 2016), often apply manually designed intensity functions, which simplifies the learning problem but suffers a high risk of model misspecification. To overcome this challenge, some neural TPPs have been proposed to model intensity functions by neural networks, which can enhance model capabilities significantly. The early neural TPPs are built based on recurrent neural networks, e.g., RMTPP (Du et al. 2016) and NHP (Mei and Eisner 2017). Recently, some Transformerbased TPP models are proposed, e.g., SAHP (Zhang et al. 2020) and THP (Zuo et al. 2020).

# Inference of Event Branches

For the event sequence whose events have inter-dependency, there always exists a branching process associated with the corresponding TPP (Møller and Rasmussen 2006; Farajtabar et al. 2014). Typically, we can represent the branches of $N$ events by a lower triangular matrix, denoted as $B \ =$ $[ b _ { n n ^ { \prime } } ] ~ \in ~ [ \dot { 0 } , \infty ) ^ { N \times N }$ , where $b _ { n n ^ { \prime } } \geq 0$ if $n \geq n ^ { \prime }$ and $b _ { n n ^ { \prime } } = 0$ otherwise. This matrix often works as a “transition” matrix of events — the nonzero $b _ { n n ^ { \prime } }$ indicates that the $n ^ { \prime }$ -th event contributes to the happening of the $n$ -th event. Ideally, a sparse and structured transition matrix helps reveal the underlying generative mechanisms of events (Dion and Yanev 1994; Gonza´lez et al. 2013; Champion et al. 2022), which enhances the interpretability of the TPP model. Therefore, in this study, we aim to infer the event branch matrix $B$ simultaneously when learning the TPP model $\theta$ .

Inferring event branches for arbitrary TPPs is challenging due to the latent nature of transition matrix. Focusing on Hawkes process (Zhou, Zha, and Song 2013b), whose intensity function encodes the branching process explicitly (Møller and Rasmussen 2006), some attempts have been made to infer event branches. For example, an information source identification method is proposed in (Farajtabar et al. 2015), which traces back to information sources in diffusion networks from partially observed cascades, crucial for applications like epidemiology and social networks. A related study (Farajtabar et al. 2017b) designs interventions to mitigate fake news spread using point process models, demonstrating the real-world application of branch process inference in misinformation control. Focusing on Hawkes processes with textual covariates, the work in (Zhang et al. 2018) identifies root sources in textual conversation threads, providing insights into propagation patterns within text data. However, the above methods often lead to non-structured and over-dense event branches because they seldom consider imposing structural regularization on the transition matrix of events. Moreover, these methods only apply to Hawkes process, which cannot be extended to other complicated TPP models.

# Optimization-driven Model Design

In this study, we aim to design a plug-and-play BADMM module that effectively infers event branches for both classic and neural TPPs. Our work can be treated as a new attempt at optimization-driven model design. In this field, some methods have been made to implement implicit neural network layers based on optimization algorithms. For example, the Sinkhormer in (Sander et al. 2022) and the ROTP in (Xu and Cheng 2023) apply Sinkhorn-scaling algorithm (Cuturi 2013) to improve attention maps and global pooling layers, respectively. An ADMM-based neural network is proposed to solve compressive sensing problems (Yang et al. 2018). These methods can embed optimization algorithms into model architectures or learning paradigms and, accordingly, impose structural constraints on model parameters or intermediate outputs. However, none of the existing methods consider inferring event branches for TPPs.

# Proposed Method Transition Matrices within TPPs

As aforementioned, the inference of event branches corresponds to learning structured transition matrix of events. This matrix is intrinsic for many existing TPP models.

Hawkes process The intensity function of Hawkes process encodes event branches explicitly (Møller and Rasmussen 2006), which is constructed as

$$
\begin{array} { r } { \lambda _ { c } ( t ; \theta ) = \mu _ { c } + \sum _ { t _ { n } < t } a _ { c c _ { n } } \kappa ( t - t _ { n } ) , } \end{array}
$$

in which $\mu _ { c }$ is time-invariant exogenous intensity of the type- $c$ event and $\begin{array} { r } { \sum _ { t _ { n } < t } a _ { c c { n } } \kappa ( t - t _ { n } ) } \end{array}$ is endogenous intensity recording the accumulative impacts of historical events at time $t . \kappa ( \bar { t } ) , t \geq 0$ , is a predefined time-decay function. As a result, the model parameters $\theta = \{ \mu , A \}$ include the exogenous intensity vector ${ \pmb \mu } = [ \mu _ { c } ] \in [ 0 , \infty ) ^ { C }$ and the infectivity matrix ${ \pmb A } = [ a _ { c c ^ { \prime } } ] \in [ 0 , \infty ) ^ { C \times C }$ capturing the pairwise impacts between different event types.

We often apply the EM algorithm (Zhou, Zha, and Song 2013b) to achieve the MLE of Hawkes process. In the $t$ -th iteration, given the current parameter $\bar { { \boldsymbol \theta } ^ { ( t ) } } = \{ { \boldsymbol \mu } ^ { ( t ) } , { \boldsymbol A } ^ { ( t ) } \}$ , we update the parameter by the following two steps:

E-step: We construct a lower bound of the log-likelihood in (2) based on Jensen’s inequality:

$$
\begin{array} { l } { { \displaystyle \log L ( s ; \theta ) \geq Q ( \theta , \theta ^ { ( t ) } ) } \ ~ } \\ { { \displaystyle = \sum _ { n = 1 } ^ { N } \Big ( r _ { n n } ^ { ( t ) } \log \frac { \mu _ { c _ { n } } } { r _ { n n } ^ { ( t ) } } + \sum _ { n ^ { \prime } = 1 } ^ { n - 1 } r _ { n n ^ { \prime } } ^ { ( t ) } \log \frac { a _ { c _ { n } c _ { n ^ { \prime } } } \kappa ( t _ { n } - t _ { n ^ { \prime } } ) } { r _ { n n ^ { \prime } } ^ { ( t ) } } \Big ) } \ ~ } \\ { { \displaystyle ~ - \sum _ { c \in \mathcal { C } } \Big ( T \mu _ { c } + \sum _ { n = 1 } ^ { N } a _ { c c _ { n } } \int _ { 0 } ^ { T - t _ { n } } \kappa ( s ) \mathrm { d } s \Big ) , } \ ~ } \end{array}
$$

where $R ^ { ( t ) } = [ r _ { n n ^ { \prime } } ^ { ( t ) } ]$ is the responsibility matrix in the $t$ -th iteration. Its element is

$$
\begin{array} { r } { r _ { n n ^ { \prime } } ^ { ( t ) } = \left\{ \begin{array} { l l } { \frac { \mu _ { c n } ^ { ( t ) } } { \lambda _ { c n } \left( t _ { n } ; \theta ^ { ( t ) } \right) } , } & { n = n ^ { \prime } , } \\ { \frac { a _ { c n c _ { n ^ { \prime } } } ^ { ( t ) } \kappa \left( t _ { n } - t _ { n ^ { \prime } } \right) } { \lambda _ { c n } \left( t _ { n } ; \theta ^ { ( t ) } \right) } , } & { n > n ^ { \prime } , } \\ { 0 , } & { n < n ^ { \prime } , } \end{array} \right. } \end{array}
$$

which corresponds to the posterior probability of the $n ^ { \prime }$ -th event given the $n$ -th event.

M-step: We update the model parameter by minimizing the surrogate function $Q \big ( \theta , \theta ^ { ( t ) } \big )$ , i.e.,

$$
\boldsymbol { \theta } ^ { ( t + 1 ) } = \arg \operatorname* { m a x } _ { \boldsymbol { \theta } } Q ( \boldsymbol { \theta } , \boldsymbol { \theta } ^ { ( t ) } ) ,
$$

which can be solved in a closed form (Zhou, Zha, and Song 2013b; Xu, Farajtabar, and Zha 2016).

We can find that the responsibility matrix in (4) works as the transition matrix of events, which is estimated by the Estep iteratively. Therefore, the inference of event branches corresponds to estimating a structured responsibility matrix.

Transformer-based neural TPPs For Transformer-based neural TPPs (Zuo et al. 2020; Zhang et al. 2020), their intensity functions encode historical impacts by a self-attention mechanism. Given the event embeddings of a sequence, denoted as $\pmb { X } = [ \pmb { x } _ { 1 } , . . . , \pmb { x } _ { N } ] \in \mathbb { R } ^ { D \times N }$ , the attention map is

$$
\tilde { A } = \sigma _ { r } \Big ( L \odot \frac { ( Q X ) ^ { \top } K X } { \sqrt { D } } \Big ) ,
$$

where $Q , K \in \mathbb { R } ^ { d \times D }$ maps the event embeddings to latent spaces. $\mathbf { { \cal L } } ~ = ~ \left[ \ell _ { n n ^ { \prime } } \right]$ is a lower triangular masking matrix, whose element $\ell _ { n n ^ { \prime } } = 1$ if $n \geq n ^ { \prime }$ , otherwise, $\ell _ { n n ^ { \prime } } = \infty$ . $" \textcircled { \cdot } ? ^ { , , }$ denotes the Hadamard product of matrix, and $\sigma _ { r }$ means applying the Softmax operation to each row of input matrix. As shown in (Zuo et al. 2020), the sparsity of $\bar { \tilde { A } }$ indicates event branches — $\boldsymbol { a } _ { n n ^ { \prime } }$ with a large value implies that the $n$ -th event is likely to be triggered by the $n ^ { \prime }$ -th event.

Over-smoothness issue The above two examples demonstrate that many TPP models estimate transition matrices intrinsically either in their learning paradigms or their model architectures. Unfortunately, these matrices often suffer the over-smoothness issue, i.e., having too many nonzero elements and leading to over-dense and unstructured event branches. For the $R ^ { ( t ) }$ in (4), its structure is determined by the sparsity of $\mu$ and $A$ , which requires additional sparse regularization (Xu, Farajtabar, and Zha 2016; Zhou, Zha, and Song 2013b). For the $\tilde { A }$ in (6), its lower-triangular part is always nonzero because of using the softmax operation. Therefore, we need to impose more structural constrains when estimating the transition matrices, which motivates the design of our BADMM module.

# BADMM Module for Structured Event Branches

Imposing sparse and low-rank structures Taking the $R ^ { ( t ) }$ in (4) or the $\tilde { A }$ in (6) as the initial transition matrix, denoted as $B _ { 0 }$ , our Bregman ADMM module imposes sparse and low-rank structures on the matrix by solving the following optimization problem:

$$
\operatorname* { m i n } _ { B \in \Omega } \underbrace { \mathrm { K L } ( B \parallel B _ { 0 } ) } _ { \mathrm { P r i o r } } + \lambda \underbrace { ( \alpha \parallel B \parallel _ { 1 } + ( 1 - \alpha ) R ( B ) ) } _ { \mathrm { S t r u c t u r a l ~ r e g u l a r i z e r } } .
$$

The feasible domain $\Omega = \{ B = [ b _ { n n ^ { \prime } } ] | B \mathbf { 1 } _ { N } = \mathbf { 1 } _ { N } , b _ { n n ^ { \prime } } =$ 0 for $n < n ^ { \prime } , b _ { n n ^ { \prime } } \geq 0$ for $n \geq n ^ { \prime } \}$ , ensuring $B$ is a rownormalized lower triangular matrix. In the objective function, the first term penalizes the KL-divergence between $B$ and the initial $B _ { 0 }$ , which requires the target transition matrix inherits the prior knowledge provided by $B _ { 0 }$ to some eulxatreinzta. iTohne, isnecwonhidctherthme $\ell _ { 1 }$ a-njorinmt $\begin{array} { r } { \| B \| _ { 1 } = \sum _ { n , n ^ { \prime } = 1 } ^ { N } | b _ { n n ^ { \prime } } | } \end{array}$ enhances the sparsity of the event branch matrix and $R ( B )$ penalizes the rank of $B$ . In this study, we consider two implementations of $R ( B )$ :

• Subspace clustering. We can implement $R ( B )$ as the nuclear norm, i.e., ∥B∥∗ = PnN=1 σN (B). It computes the summation of $B$ ’s singu ar values, which encourages sparse singular values and results in a low-rank matrix. As a result, the regularizer $\alpha \| B \| _ { 1 } + ( 1 - \alpha ) \| B \| _ { * }$ corresponds to the subspace clustering method in (Elhamifar and Vidal 2013; Liu, Lin, and $\mathrm { Y u } 2 0 1 0 \AA$ ). • Sparse group-lasso. We can also implement $R ( B )$ as the $\ell _ { 1 , 2 }$ norm, i.e., $\begin{array} { r } { \| B \| _ { 1 , 2 } = \sum _ { n = 1 } ^ { N } \| \pmb { b } _ { n } \| _ { 2 } } \end{array}$ , where $\pmb { b } _ { n }$ is the $n$ -th column of $B$ . Th  term encourages $B$ to have a group sparse structure. As a result, the regularizer $\alpha \| B \| _ { 1 } + ( 1 - \alpha ) \| B \| _ { * }$ corresponds to the sparse group-lasso in (Simon et al. 2013).

The significance of the regularization term is controlled by $\lambda > 0$ , and the trade-off between the sparse and low-rank terms is achieved by $\alpha \in [ 0 , 1 ]$ .

Remark. It should be noted that the utilization of the regularization is motivated by the event branching structure we desire. As illustrated in Figure 1(a), an event branching process often consists of some isolated events independent of other events and some key events that trigger subsequent events. In addition, an event’s impact always decays over time, so it can only trigger a subset rather than all subsequent events. As a result, the corresponding event branch matrix is sparse and low-rank, and the proposed regularizer enhances such structures accordingly.

Implementation details Applying Bregman ADMM algorithm (Wang and Banerjee 2014), we can solve the above optimization problem iteratively. In particular, we first rewrite (7) in the following equivalent format:

$$
\begin{array} { r l } & { \underset { \boldsymbol { B } , \boldsymbol { X } _ { 1 } , \boldsymbol { X } _ { 2 } } { \operatorname* { m i n } } \mathrm { K L } ( \boldsymbol { B } \| \boldsymbol { B } _ { 0 } ) + \lambda ( \alpha \| \boldsymbol { X } _ { 1 } \| _ { 1 } + ( 1 - \alpha ) R ( \boldsymbol { X } _ { 2 } ) ) } \\ & { \quad \mathrm { s . t . } \quad \boldsymbol { B } = \boldsymbol { X } _ { 1 } = \boldsymbol { X } _ { 2 } , \quad \boldsymbol { B } \in \Omega . } \end{array}
$$

Here, $X _ { 1 }$ and $X _ { 2 }$ are two auxiliary variables helping decouple the three terms in the objective function. Then, we can further rewrite (8) in its augmented Lagrangian form, i.e.,

$$
\begin{array} { r l } & { \operatorname* { m a x } _ { Z _ { 1 } , Z _ { 2 } } \operatorname* { m i n } _ { B \in \Omega , { \pmb X } _ { 1 } , { \pmb X } _ { 2 } } \mathrm { K L } ( B \parallel B _ { 0 } ) } \\ & { \quad + \lambda ( \alpha \parallel X _ { 1 } \parallel _ { 1 } + ( 1 - \alpha ) R ( { \pmb X } _ { 2 } ) ) } \\ & { \quad + \rho \sum _ { i = 1 , 2 } \Bigl ( \langle Z _ { i } , B - { \pmb X } _ { i } \rangle + B _ { \phi } ( { \pmb B } , { \pmb X } _ { i } ) \Bigr ) . } \end{array}
$$

where $Z _ { 1 }$ and $ { \boldsymbol { Z } } _ { 2 }$ are dual variables, $B _ { \phi }$ denotes the Bregman divergence term determined by a convex function $\phi$ . When $\begin{array} { r } { \phi = \| \cdot \| _ { 2 } , B _ { \phi } ( B , \pmb { X } _ { i } ) = \frac { 1 } { 2 } \| \pmb { \bar { B } } - \pmb { X } _ { i } \| _ { F } ^ { 2 } } \end{array}$ . When $\phi$ is the entropy function, $B _ { \phi } ( B , X _ { i } ) \bar { = } \mathrm { K L } ( B \| X _ { i } )$ .

After initializing R(0) = X1(0) ${ \pmb R } ^ { ( 0 ) } \ = \ { \pmb X } _ { 1 } ^ { ( 0 ) } \ = \ { \pmb X } _ { 2 } ^ { ( 0 ) } \ = \ B _ { 0 }$ and ${ \pmb Z } _ { 1 } ^ { ( 0 ) } = { \pmb Z } _ { 2 } ^ { ( 0 ) } = { \pmb 0 } _ { N \times N }$ , we update the variables iteratively by alternating optimization. In the $t$ -th iteration, we have

![](images/1f1190e22f27ab89ab5821336f0cfa300ce6999898a56b920b82dde6ff410245.jpg)  
Figure 2: An illustration of the feed-forward step of our Bregman ADMM module.

• Update $B$ by solving the following problem:

$$
\operatorname* { m i n } _ { B \in \Omega } \mathrm { K L } ( B \| B _ { 0 } ) + \rho \sum _ { i = 1 , 2 } ( \langle \pmb { Z } _ { i } ^ { ( t ) } , \pmb { B } \rangle + \mathrm { K L } ( \pmb { B } \| \pmb { X } _ { i } ^ { ( t ) } ) ) ,
$$

where the Bregman divergence is set to be KLdivergence. This problem has a closed form solution:

$$
\pmb { B } ^ { ( t + 1 ) } = \sigma _ { r } \Big ( \frac { \log \pmb { B } _ { 0 } + \rho \sum _ { i = 1 , 2 } ( \log \pmb { X } _ { i } ^ { ( t ) } - \pmb { Z } _ { i } ^ { ( t ) } ) } { 1 + 2 \rho } \Big ) .
$$

• Update $X _ { 1 }$ and $X _ { 2 }$ via soft-thresholding: Setting the Bregman divergence to be the squared $\ell _ { 2 }$ -norm, we have

$$
\begin{array} { r l } & { \underset { \pmb { X } _ { 1 } } { \operatorname* { m i n } } \frac { \rho } { 2 } \| \pmb { X } _ { 1 } - \pmb { B } ^ { ( t + 1 ) } \| _ { 2 } ^ { 2 } - \rho \langle \pmb { Z } _ { 1 } ^ { ( t ) } , \pmb { X } _ { 1 } \rangle + \lambda \alpha \| \pmb { X } _ { 1 } \| _ { 1 } } \\ & { \Rightarrow \pmb { X } _ { 1 } ^ { ( t + 1 ) } = S _ { \frac { \lambda \alpha } { \rho } } \big ( \pmb { B } ^ { ( t + 1 ) } + \pmb { Z } _ { 1 } ^ { ( t ) } \big ) , } \end{array}
$$

where $S _ { \tau } ( a ) = \mathrm { s i g n } ( a ) ( | a | - \tau ) _ { + }$ is the elementwise soft-thresholding function.

$$
\begin{array} { r l } & { \underset { \pmb { X } _ { 2 } } { \operatorname* { m i n } } \frac { \rho } { 2 } \| \pmb { X } _ { 2 } - \pmb { B } ^ { ( t + 1 ) } \| _ { 2 } ^ { 2 } - \rho \langle \pmb { Z } _ { 2 } ^ { ( t ) } , \pmb { X } _ { 2 } \rangle + \lambda \alpha R ( \pmb { X } _ { 2 } ) } \\ & { \Rightarrow \{ R ( \cdot ) = \| \cdot \| _ { 1 , 2 } : \pmb { x } _ { n , 2 } ^ { ( t + 1 ) } = \tau _ { n } S _ { \frac { \lambda \alpha } { \rho } } \big ( \pmb { b } _ { n } ^ { ( t + 1 ) } + \pmb { z } _ { n , 2 } ^ { ( t ) } \big )  } \\ & {  \pmb { R } ( \cdot ) = \| \cdot \| _ { * } : \pmb { X } _ { 2 } ^ { ( t + 1 ) } = \pmb { U } S _ { \frac { \lambda ( 1 - \alpha ) } { \rho } } \big ( \pmb { \Sigma } \big ) \pmb { V } ^ { \top } , } \end{array}
$$

where we denote $\pmb { x } _ { n , 2 } ^ { ( t + 1 ) }$ $z _ { n , 2 } ^ { ( t ) }$ , and ${ \pmb b } _ { n } ^ { ( t + 1 ) }$ as the $n$ - th column of X(t+1), $X _ { 2 } ^ { ( t + 1 ) } , \ Z _ { 2 } ^ { ( t ) }$ , and $B ^ { ( t + 1 ) }$ , respectively. $\begin{array} { r } { \tau _ { n } = ( 1 - \frac { ( 1 - \alpha ) \lambda } { \rho \| S _ { \frac { \lambda \alpha } { \rho } } ( b _ { n } ^ { ( t + 1 ) } + z _ { n , 2 } ^ { ( t ) } ) \| _ { 2 } } ) + } \end{array}$ . $U { \pmb \Sigma } V ^ { \top }$ is the singular value decomposition (SVD) of $B ^ { ( t + 1 ) } + Z _ { 2 } ^ { ( t ) }$ . The closed-form solutions are derived based on the implementations of $R ( \cdot )$ .

• Update $\scriptstyle { Z _ { 1 } }$ and $ { \boldsymbol { Z } } _ { 2 }$ by the following step:

$$
\begin{array} { r } { Z _ { 1 } ^ { ( t + 1 ) } = Z _ { 1 } ^ { ( t ) } + ( B ^ { ( t + 1 ) } - X _ { 1 } ^ { ( t + 1 ) } ) , } \\ { Z _ { 2 } ^ { ( t + 1 ) } = Z _ { 2 } ^ { ( t ) } + ( B ^ { ( t + 1 ) } - X _ { 2 } ^ { ( t + 1 ) } ) . } \end{array}
$$

Repeating the above updates $T$ times till the objective function converges, we obtain the final event branch matrix.

For Hawkes process, we can apply the above algorithm directly in the E-step and obtain a structured responsibility matrix accordingly. For neural TPPs, we follow the work in (Xu and Cheng 2023), unrolling the iterations as $T$ layers. Accordingly, the proposed Bregman ADMM module can be implemented as the stack of the $T$ layers, as illustrated in Figure 2. Plugging the Bregman ADMM module into the attention layer leads to a new attention mechanism. Note that, the $T$ layers are involved in the back-propagation when learning the neural TPPs. When $R ( \cdot ) = \| \cdot \| _ { * }$ , we detach the gradient of $X _ { 2 }$ such that the SVD operation will not be considered and the memory cost can be saved.

# Comparisons with Existing Methods

Our BADMM module is applicable to improve both traditional EM paradigms and Transformer-based TPP models for inferring event branches. To our knowledge, another plug-and-play module driven by optimization algorithm and with a similar functionality is the Sinkhorn-based module proposed in (Mena et al. 2020; Sander et al. 2022). Specifically, focusing on the EM algorithm of Gaussian mixture models, the work in (Mena et al. 2020) revisits the E-step of the algorithm and computes the responsibility matrices by the Sinkhorn-scaling algorithm (Sinkhorn and Knopp 1967). Similarly, the Sinkformer in (Sander et al. 2022) improves the attention layer of Transformer encoder, applying the Sinkhorn-scaling algorithm to derive attention maps. These methods impose a doubly-stochastic constraint on the responsibility matrices and attention maps, treating the matrices as entropic optimal transport plans (Cuturi 2013).

Note that although the Sinkhorn-based module can also derive structured matrices, it is inapplicable for inferring event branches of TPPs. For the EM algorithm of Hawkes processes, the responsibility matrix is a lower-triangular matrix. Applying the Sinkhorn-scaling algorithm to the matrix leads to a diagonal matrix. It means that all the events are independent, which is unreasonable in practice. For the attention maps in neural TPPs, although we can apply the Sinkhorn-based attention layer first and then mask the doubly-stochastic attention map to a lower-triangular matrix, this operation breaks the row-wise normalization structure of the attention map. As a result, we can no longer interpret the attention map as the posterior probabilities of historical events given the current ones.

# Experiments

We conducted comprehensive experiments on both synthetic and real-world datasets and evaluate the usefulness of our module. In addition, we analyzed the interpretabilty of inferred event branches and demonstrated the robustness of our module to its hyperparameters. All the experiments are run on two 3090 GPUs, and we record each method’s averaged performance and standard deviation in three trials.

# Implementation Details

Datasets We apply one synthetic event sequence dataset and four real-world ones in our experiments, whose statistics are shown in Table 1. The synthetic dataset Conttime (Mei and Eisner 2017) is simulated by Hawkes process, while the real-world datasets, including Retweet (Zhao et al. 2015), StackOverflow (SO) (Jure 2014), Amazon (Xue et al. 2023), and Taobao (Xue et al. 2023), contain user behaviors on different platforms, which are commonly used for evaluating TPP models. In addition, to evaluate the rationality of inferred event branches, we consider the dialogue data in the movie “12 Angry Men” (Zhang et al. 2018), in which each juror is treated as an event type and the timestamped sentences of the jurors are events.

Table 1: The statistics of datasets   

<html><body><table><tr><td>Dataset</td><td>#Event Types</td><td># Sequences</td><td>Max.length</td></tr><tr><td>Conttime</td><td>5</td><td>10,000</td><td>100</td></tr><tr><td>Taobao</td><td>17</td><td>2.000</td><td>94</td></tr><tr><td>Retweet</td><td>3</td><td>24,000</td><td>264</td></tr><tr><td>StackOverflow</td><td>22</td><td>6.633</td><td>736</td></tr><tr><td>Amazon</td><td>16</td><td>12,556</td><td>282</td></tr><tr><td>12 Angry Men</td><td>12</td><td>1</td><td>587</td></tr></table></body></html>

Baselines We plug our BADMM module into existing TPPs and evaluate its impacts accordingly. Here, we consider the classic Hawkes process (HP in (Zhou, Zha, and Song 2013b)) and two Transformer-based TPPs (THP in (Zuo et al. 2020) and SAHP in (Zhang et al. 2020)). For HP, we apply our BADMM module to estimate its responsibility matrix in the EM algorithm. For THP and SAHP, we replace their self-attention layer with our BADMM module. The baselines of our method include: $i$ ) the HP learned by EM algorithm, $i i { \bf \Gamma }$ ) the HP-based source tracking (HPST) in (Zhang et al. 2018), iii) the THP and SAHP learned by SGD, and $_ { i v }$ ) the THP and SAHP applying the Sinkhornbased attention layers in (Sander et al. 2022).

Hyperparameter settings For our BADMM module, we consider two implementations, denoted as $\mathbf { B A D M M } _ { * }$ (i.e., $R ( \cdot ) = \| \cdot \| _ { * } )$ and $\mathbf { B A D M M } _ { 1 , 2 }$ (i.e., $R ( \cdot ) = \| \cdot \| _ { 1 , 2 } )$ , respectively. The weight $\rho$ of Lagrangian term only affects the convergence rate, so we simply set it to be 1 in our experiments. For the weight of regularizer, we set $\alpha \in ( 0 , 1 )$ and $\lambda \in \{ 0 . 0 1 , 0 . 1 , 1 , \bar { 1 } 0 , 1 0 0 \}$ . Applying grid search, we find their optimal configurations. For the number of iterations $T$ , we follow the setting of Sinkhorn-based module in (Sander et al. 2022) and set $T = 2$ , which achieves a trade-off between the model complexity and capability.

Evaluation metrics When evaluating each learned model in a testing set, we record $i$ ) the log-likelihood per event (ELL) and $i i$ ) the prediction accuracy of event types (ACC). In addition, to evaluate the rationality of inferred event branches, we visualize the transition matrices learned by different methods and check the learned important events and their triggering patterns qualitatively.

# Effectiveness and Rationality

Impacts on Model Performance We conducted a thorough evaluation of BADMM-enhanced methods against various baselines across multiple datasets. The results in Table 2 demonstrate the consistent effectiveness of our BADMM module across all datasets and backbone models. In most situations, applying our BADMM module significantly improves the performance of both classical and neural TPPs. In our opinion, this phenomenon is because the event sequences in practice are driven by hidden branching processes, so that inferring event branches during training does not do harm to the learning of TPPs. Moreover, the existing TPPs, especially those neural ones, may suffer from the over-fitting issue during training. Our BADMM module imposes structural constraints on their transition matrices, regularizing model parameters implicitly to mitigate the issue.

Table 2: Comparisons for various methods on model performance.   

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Method</td><td colspan="2">ELLTaobaACC ↑</td><td colspan="2">ELL etweeACC ↑</td><td colspan="2">LtackOverAC↑</td><td colspan="2">ELLAmazACC ↑</td><td colspan="2">ELLC ntimAc t</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="4">HP</td><td>BADMM1,2</td><td>Classic EM|-0.2620.014 0.4630.001 0.2200.000 0.4760.001</td><td></td><td>-8.9020.0010.5640.000</td><td></td><td>-2.7580.012 0.4520.002</td><td></td><td>-9.022001705908|428943902|-2.56003502-16010 -2.5610.000 0.3660.001</td><td></td><td></td><td>-1.0730.000 0.3400.001</td></tr><tr><td>BADMM*</td><td>0.2230.000 0.4800.001</td><td></td><td>-8.9020.000</td><td>0.5650.000</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>-2.7440.010 0.4540.002</td><td></td><td>-2.5610.0000.3680.000</td><td></td><td></td><td>-1.0730.000 0.3400.001</td></tr><tr><td>Softmax</td><td>-9.8790.8560.4340.000</td><td></td><td>-7.8780.045</td><td>0.5370.001</td><td>-3.6140.2470.3530.012</td><td></td><td>-3.2230.031</td><td>0.2950.052</td><td></td><td>-1.4160.104 0.2640.037</td></tr><tr><td rowspan="4">SAHP</td><td>BADMM1.2</td><td>9.7670590.43400</td><td></td><td>-7.702048</td><td>0.5410.00</td><td></td><td>-3.450.470.356012</td><td>-3.137021 0.3096036</td><td></td><td></td><td>-1.3630.182 0.2720026</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>BADMM*</td><td>-9.5940.119 0.4340.000</td><td></td><td>-7.6860.090</td><td>0.5420.003</td><td>-3.3740.062</td><td>0.3650.003</td><td>-3.1270.282</td><td>0.3070.009</td><td></td><td>-1.2350.086 0.2780.022</td></tr><tr><td>Softmax</td><td>0.1700.006</td><td>0.4360.000</td><td>-9.3730.716</td><td>0.5110.013</td><td>-0.6980.007</td><td>0.4500.002</td><td>0.5420.001</td><td>0.3510.001</td><td></td><td>-0.3900.0430.6960.051</td></tr><tr><td rowspan="3">THP</td><td>BAiDMM1.2</td><td>0.1780.03</td><td>0.436.0000</td><td>-11.020340500</td><td></td><td></td><td>07000.43.03</td><td>0.2820.02</td><td>0.3310.02</td><td></td><td>-0.3940.0390.7037.036</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>BADMM*</td><td>0.1930.012</td><td>0.4360.000</td><td>-8.7530.046</td><td>0.5380.001</td><td>-0.6960.002 0.4530.002</td><td></td><td>0.5430.000</td><td>0.3560.001</td><td></td><td>-0.3850.0410.7480.025</td></tr></table></body></html>

Table 3: Top-5 most influential jurors in “12 Angry Men”   

<html><body><table><tr><td>Rank</td><td>GPT-40</td><td>HPST</td><td>HP</td><td>BADMM THP</td><td>SAHP</td></tr><tr><td>1</td><td>Juror 8</td><td>Juror 8</td><td>Juror 8</td><td>Juror 8</td><td>Juror 8</td></tr><tr><td>2</td><td>Juror 3</td><td>Juror 3</td><td>Juror 3</td><td>Juror 3</td><td>Juror 3</td></tr><tr><td>3</td><td>Juror 7</td><td>Juror 7</td><td>Juror 7</td><td>Juror 7</td><td>Juror 7</td></tr><tr><td>4</td><td>Juror 10</td><td>Juror 1</td><td>Juror 10</td><td>Juror 10</td><td>Juror 4</td></tr><tr><td>5</td><td>Juror 4</td><td>Juror 10</td><td>Juror 12</td><td>Juror 1</td><td>Juror 10</td></tr></table></body></html>

Qualitative comparisons In Figure 3, we visualize the transition matrices inferred by different methods. We can find that the original transition matrices are over-smoothed, which contain too many nonzero elements. In contrast, applying our BADMM module indeed leads to sparse and lowrank transition matrices for both classic and neural TPPs, which enhances the structures of transition matrices significantly. For neural TPPs, we further compare the transition matrices derived by our BADMM module with those derived by the Sinkhorn module (Sander et al. 2022). The Sinkhorn module first derives a doubly stochastic matrix and then masks it to a lower triangular matrix. Although it can achieve low-rank and sparse transition matrices to some extent, it breaks the row-normalization constraint of the transition matrix and thus results in inferior performance compared to our method (as shown in Table 2).

In addition, the two implementations of BADMM module often lead to different event branches. Because of us

0 3 6 9.12.15.18 0 3 6 9 121518 0 3 6 9 121518   
0 □ 0 1 0 1   
3 3 3 ：   
6 6 6   
9 9 9   
123 GE 1R 中   
18 ： ： Classic EM BADMM1,2 BADMM (a) R in HP 0481216 04 8 1216 0481216 0481216   
4 4 1 4 1 4   
8 8 8 8   
1 E 1 1   
16 1 1 ： Attention Sinkhorn $\mathbf { B A D M M } _ { 1 , 2 }$ BADMM (b) $\tilde { A }$ in SAHP 0481216 0481216 0481216 0481216   
4 十 4 4 ■ I   
8 8 8 8   
1 1 1 1 ■   
1 1 Attention Sinkhorn $\mathbf { B A D M M } _ { 1 , 2 }$ BADMM (c) $\tilde { A }$ in THP

ing sparse group-lasso regularization, $\mathbf { B A D M M } _ { 1 , 2 }$ tends to very sparse transition matrices in which only some columns have dense nonzero elements. It means that $\mathbf { B A D M M } _ { 1 , 2 }$ attributes the generation of the event sequence from few globally-significant events. On the other side, $\mathbf { B A D M M _ { * } }$ preserves the structures of the initial transition matrices better and tends to identify the events having local impacts on its subsequent events. According to the results in Table 2, we can find that $\mathbf { B A D M M _ { * } }$ often outperforms $\mathbf { B A D M M } _ { 1 , 2 }$ on data fitness and prediction accuracy, which means that the transition matrices of $\mathbf { B A D M M _ { * } }$ are often associated with

0 0.3 0.5 0.7 1 Accu	ra	cy   
0 0.356 0.378 0.371 0.378 0.353 0.360   
1 0.356 0.376 0.362 0.355 0.344 0.35
8   
1 0.356 0.339 0.316 0.313 0.325 0.35
6   
10 0.356 0.339 0.332 0.330 0.328 0.35
4   
100 0.356 0.341 0.330 0.330 0.328 00 20 4	0 60 80 100 α Iterations (a) ACC of HP (b) Iterations v.s. ACC 83838388888888838333 83838388888888838333   
88   
8 3 3 3 (a) $\tilde { A }$ derived by Softmax (b) $\tilde { A }$ derived by $\mathbf { B A D M M _ { * } }$ hTehyeycacamemieninfofuonudnd IdodnoI'tnd'tkonkno'ntwokwn hTaht'ast snontotwwhyhy hteheotlodhledmomaldnanmwiawtnihthawiat whehtehwtehreIrthIbebrleiIleivbeevlieiteitv ew're'eresitstiittnignghehrere nkinfiefkieni fnheihsiinschceihsetscth rornontotMayabyebIe Idodno'tn't nIvnovkoiknigng I' Im'mnontotrtyriynigngtot   
here's what I think and chcahnagnegeyoyuorurmimnidn I have no personal WelelltlWhtehlrlertehwewrrerweveovrtoetsevso It'Ist'jsujsutsthtahtatWe're'er   
feelings about this I just ofrorgugiultilytyIt'Ist'snontoetaesaysytoto tatlakliknigngabaobuotu   
want to talk about facts ariasiesemymyhahnadndanadndsesnednd sosomembeobdoyd'sy lsiflief Number one The old abobyoyofoffftotodideiewiwtihtohuotut hehrereWe ecacna'tn   
man lived downstairs atlakliknigngabaobuotuit iftifrisrtst dedceicdiedietiitninfifviev under the room.... mimniuntuetsesSuSpupopsoisnign wew're'erewrwornogn oNwowthtehsesearaere IdodnoI'tnd'tkonkno'ntwokwnMoMawyabyMebaenyonboreernaesoaosrnoena oComemeononThTihsiissins'tn't afcatcstsYoYuoucacna'tn't oLoko kthtihsisk ikdi'sd sbebenenk ikcikcekded uSnudnSaduyanyWdaWeyedodWnoe'tnd'to erfeuftuetefafcatcsts raoruonudndalal llofofhihsilsiflief..e... .H.eH'se's enededa asesremrmonon ahdada aprpertettytymimsiesrearbalbeleyeyaerasr.s. ... (c) Triggering patterns among key dialogues

the models with low risks of misspecification. Considering this fact, we think $\mathbf { B A D M M _ { * } }$ can generate more reasonable event branches and reveal triggering patterns among events better. In the following experiments, we mainly test the performance of $\mathbf { B A D M M _ { * } }$ .

Rationality of Inferred Event Branches For the dialogue in the movie “12 Angry Men”, we follow the HPST method (Zhang et al. 2018), learning a transformer hawkes process and inferring a transition matrix for the jurors’ sentences. According to the learned transformer hawkes process, we can rank the 12 jurors in the movie based on the overall impacts of their sentences (i.e., $\mathbf { 1 } ^ { \top } B S$ , where $\mathbf { 1 } ^ { \top } B \in \mathbb { R } ^ { N }$ records the impacts of $N$ sentences and $\pmb { S } \in$ $\{ 0 , 1 \} ^ { N \times 1 2 }$ maps the sentences to corresponding juro )∈. Ideally, we hope that the ranking result can reflect the significance of the jurors in the movie. Taking the ranking result derived by GPT-4o as the ground truth (which is checked by five volunteers watched the movie before), we compare the ranking results derived by the TPPs learned with our BADMM module with that achieved by HPST. Table 3 lists the top-5 most influential jurors proposed by different methods. Both our method and HPST can detect the top-2 most influential jurors (i.e., Jurors 8 and 3, who insist on acquittal and conviction, respectively). Compared to HPST, the ranking results of our method are more consistent with that of GPT-4o. For example, the SAHP learned with our BADMM module considers Juror 4 in the top-5 list. This juror appears in the list of GPT-4o but is missed by HPST. Note that both

GPT-4o and HPST leverage textual information when ranking the jurors, while our method purely relies on the timestamps and the jurors’ IDs within the event sequence. In other words, by applying our BADMM module, we can learn reliable event branches that are consistent with those estimated by the text-based models.

To further verify the rationality of inferred event branches, we sample the rows and columns of the transition matrix relevant to Jurors 8 and 3 and visualize it in Figure 4. Similar to the results in Figure 3, we can find that compared to the Softmax operation of THP, our BADMM module can derive a transition matrix with better sparsity, whose event branches are more clear. In addition, based on the inferred transition matrix, we show the triggering patterns hidden the dialogue between the two jurors. We can find that the triggering patterns are consistent with the semantics of the sentences.

# Robustness to Hyperparameters

The impacts of key hyperparameters $( \alpha , \lambda$ , and the number of iterations $T$ ) on model performance are shown in Figure 5. Specifically, Figure 5(a) shows the ACC of the models learned with different configurations of $\alpha$ and $\lambda$ . The results show that our BADMM module consistently performs well within a broad range of the hyperparameter settings, indicating that our method is robust to moderate changes in these hyperparameters. Figure 5(b) shows the effect of the number of iterations on the performance of our method. We can find that our BADMM module quickly converges within a few iterations, with marginal gains observed beyond a certain threshold. This rapid convergence implies that, in practical scenarios, we can implement our BADMM module using limited iterations, which reduces computational costs while maintaining high performance.

# Conclusion

We introduced a novel BADMM module, which helps infer structured event branches for various TPPs. By integrating this module into both classic and neural TPPs, we addressed the common over-smoothness issue of event branch inference, providing an effective and robust solution to impose sparse and low-rank structures on transition matrices. Our method improves the interpretability of learned TPPs and enhances their model performance.