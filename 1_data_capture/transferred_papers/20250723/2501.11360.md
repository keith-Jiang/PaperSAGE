# Federated Learning with Sample-level Client Drift Mitigation

Haoran $\mathbf { X } \mathbf { u } ^ { 1 * }$ , Jiaze $\mathbf { L i } ^ { 1 \ast }$ , Wanyi $\mathbf { W } \mathbf { u } ^ { 2 }$ , Hao Ren3†

1Zhejiang University, Hangzhou, China 2Shandong University, China 3School of Cyber Science and Engineering, Sichuan University, Chengdu 610065, China xhaoran $@$ zju.edu.cn, Jiaze Li $@$ zju.edu.cn, wanyi wu $@$ mail.sdu.edu.cn, hao.ren $@$ scu.edu.cn

# Abstract

Federated Learning (FL) suffers from severe performance degradation due to the data heterogeneity among clients. Existing works reveal that the fundamental reason is that data heterogeneity can cause client drift where the local model update deviates from the global one, and thus they usually tackle this problem from the perspective of calibrating the obtained local update. Despite effectiveness, existing methods substantially lack a deep understanding of how heterogeneous data samples contribute to the formation of client drift. In this paper, we bridge this gap by identifying that the drift can be viewed as a cumulative manifestation of biases present in all local samples and the bias between samples is different. Besides, the bias dynamically changes as the FL training progresses. Motivated by this, we propose FedBSS that first mitigates the heterogeneity issue in a sample-level manner, orthogonal to existing methods. Specifically, the core idea of our method is to adopt a bias-aware sample selection scheme that dynamically selects the samples from small biases to large epoch by epoch to train progressively the local model in each round. In order to ensure the stability of training, we set the diversified knowledge acquisition stage as the warmup stage to avoid the local optimality caused by knowledge deviation in the early stage of the model. Evaluation results show that FedBSS outperforms state-of-the-art baselines. In addition, we also achieved effective results on feature distribution skew and noise label dataset setting, which proves that FedBSS can not only reduce heterogeneity, but also has scalability and robustness.

# Introduction

Federated Learning (FL), which enables collaborative training of models by sharing parameters among clients without exchanging raw data, has garnered significant attention for its ability to leverage vast amounts of data on clients while preserving privacy. The basic steps of FL are to iteratively execute local model training on multiple clients individually and subsequently aggregate all updated models at the server. Despite its implementation simplicity, the challenge arises from the inherently heterogeneous distribution of FL’s training data across clients—characterized by non-identical and independent (Non-IID) data—substantially deteriorating the performance of the obtained global model.

Many approaches have been devoted to addressing the data heterogeneity problem of FL. One of the representative categories is to consider that the essence of performance degradation is due to the client drift caused by the data heterogeneity where the local updates of each client greatly deviate from the aggregated global one (Karimireddy et al. 2020; Li et al. 2020; Zhang et al. 2022; Li and Zhan 2021; Gao et al. 2022; Louizos, Reisser, and Korzhenkov 2024). To mitigate this drift, many methods are proposed to calibrate the local update from the perspective of optimization (Gao et al. 2022; Karimireddy et al. 2020). For example, some methods leverages the difference between the local update and the global update in old rounds to compensate the local update in current round (Karimireddy et al. 2020; Gao et al. 2022) and other works add regularization on the local loss function to facilitate the local update to approach the global one (Zhang et al. 2022; Li and Zhan 2021; Louizos, Reisser, and Korzhenkov 2024). Besides a few works explore different aggregation strategies on the server (Chan et al. 2024).

Although these approaches have made great achievements, they lack a deep understanding of how the heterogeneous data samples contribute to the formation of client drift.

In this paper, we seek to tackle the Non-IID challenge of FL from the sample level by delving into the impact of each local sample on the drift of local update. Specifically, we identify that there exists substantial bias in each sample and the client drift can be viewed as a cumulative manifestation of the biases of all samples. Besides, the bias between samples is different from each other where the sample with large loss is considered to have a relatively large impact on the client drift. Also, we find that the bias of each sample dynamically changes during the FL training process.

Motivated by this finding, we propose a simple yet effective method, FedBSS, which includes two stages: Diversified knowledge acquisition stage and Progressive knowledge learning stage. The diversified knowledge acquisition phase acts as a model’s cold start, warming it up by introducing varied knowledge through standard aggregation techniques. This step prevents early-stage deviations caused by limited knowledge scope, essentially initializing the global model with a diverse set of preliminary information guided by the principle of diversity. The second stage is to dynamically select samples based on their bias to train the local model. Specifically, we measure the bias of each sample by the loss of the global model. Then, during each round, each client sorts the bias of all samples and selects the samples with small loss to train the local model. Considering that filtering out samples with large biases may reduce the Knowledge acquisition efficiency, we further propose progressively adding samples with large biases to the training process. Finally, in order to give the partition boundary of samples with different biases adaptively, we introduce the concept of uncertainty.

![](images/8fb26af1e5549eebb5f64bf9c0fae769a690733e0a3e9f784957328a3a54c85f.jpg)  
Figure 1: Illustration of FedBSS. Our approach includes local and global progressive learning, shown by green and pink arrows. In the client’s local learning, there are three steps: 1) Samples are rated by loss; higher $i$ in $S _ { i }$ signifies greater loss (redder), lower means lesser (bluer). 2) Loss-based sorting identifies an adaptive threshold for classifying samples into ’unbiased’ (low loss) or ’bias’ (higher loss) sets. 3) Training begins with ’unbiased’ samples, then integrates ’bias’ ones over time, enabling comprehensive learning. Meanwhile, the global model learns biases across rounds, shrinking the ’bias’ set.

Experiment results on different datasets and models show that FedBSS can significantly improve the performance of the global model under label distribution skew, feature distribution skew and noise data settings compared to baselines. Our contributions can be summarized as:

• To the best of our knowledge, this paper is the first to consider to solve client drift issues in the sample level. Our findings reveal that the biases of client samples are different from each other which cumulatively formalizes the client drift. • We propose a novel and fine-grained two-stage approach, FedBSS, which has the diversified knowledge acquisition stage and the progressive knowledge learning stage. The former is based on the principle of diversity and infuses diversified knowledge into the global model. On the basis of the former, the latter sorts the loss of client sample based on the current global model, adaptively divides the client sample by uncertainty, and guides the global model from unbiased sample to biased samples learning through progressive knowledge learning strategies.

• To validate the efficiency of the proposed method, we compare FedBSS with state-of-the-art methods on NonIID data settings. By conducting extensive experiments over various deep-learning models and datasets, we show that FedBSS can not only reduce data heterogeneity but also has scalability and robustness.

# Related Works

Many approaches (Li, He, and Song 2021; Zhu, Hong, and Zhou 2021; Lee et al. 2022; Acar et al. 2021a; Wang et al. 2024b) have been dedicated to addressing the issue of data heterogeneity in Federated Learning (FL). These methods can be classified into two primary categories: those that calibrate local updates from an optimization standpoint and those that devise aggregation strategies. The specifics of these related works are elaborated upon as follows.

Calibrating Local Update Previous research has tackled the challenge of Non-IID data in federated learning by calibrating the local model updates. Works such as SCAFFOLD (Karimireddy et al. 2020) address client drift by employing control variates for gradients, albeit without directly considering inconsistencies between local and global objectives. FedDC (Gao et al. 2022), in a different approach, utilizes auxiliary drift variables to monitor and mitigate discrepancies between local and global model parameters. Regarding statistical heterogeneity, FedProx (Li et al. 2020) integrates an additional term into the local model’s objective function, suggesting that over-updating the local model could hinder global convergence. This proximal term serves as a penalty, discouraging significant deviations of the local model from the global one. Another strategy, presented by FedLC (Zhang et al. 2022), introduces a fine-tuned calibrated cross-entropy loss for local updates. It does so by incorporating a pairwise label margin, enhancing the sensitivity of the learning process to inter-class differences. Similarly, FedRS (Li and Zhan 2021) addresses the challenge of missing classes by proposing a ”Restricted Softmax” mechanism, which constrains the updating of weights associated with missing classes during local training. FedDyn (Acar et al. 2021b), on the other hand, introduces a dynamic regularizer tailored to each client to align individual client models with the global model, thereby reducing communication overhead. More recently, integrating contrastive learning and mutual information into the loss function has emerged as another optimization strategy, as illustrated by Louizos et al. (Louizos, Reisser, and Korzhenkov 2024). Strategies such as incorporating additional regularizers into loss functions or leveraging gradient control mechanisms are all aimed at mitigating the drift of local models.

Aggregation Strategy The parameters of client models are inherently prone to drift, making server aggregation strategies another pivotal approach to mitigating the effects of Non-IID data. One such strategy, InCo (Chan et al. 2024)Aggregation, harnesses internal cross-layer gradients—combining gradients from both shallow and deep layers within a server model—to enhance similarity in the deeper layers without necessitating extra client-server communication.Recently, FedCDA (Wang et al. 2024a), on the other hand, employs a selective aggregation of cross-round local models, effectively minimizing disparities between the global and local models.

# Problem Formulation

In this setting, we have $N$ clients with their private datasets $\mathcal { D } _ { n } = \{ ( \boldsymbol { x } _ { i } , y _ { i } ) \} _ { i = 1 } ^ { S _ { n } }$ mwbher eo $x _ { i }$ aiisnitnhge sda tmaplseaomnp -, $y _ { i }$ cliise ints. $S _ { n }$ $n$ The objective of federated learning framework is to learn the global model parameter $\vartheta$ which minimizes the loss function $F ( \vartheta )$ on training data of all clients without access to original data:

$$
\begin{array} { l } { \displaystyle { \mathit { m i n } } \ F ( \vartheta ) = \displaystyle { \frac { 1 } { N } } \sum _ { n = 1 } ^ { N } F _ { n } ( \vartheta ) } \\ { \displaystyle F _ { n } ( \vartheta ) = \mathbb { E } _ { \pmb { x } \sim \mathcal { D } _ { n } } f _ { n } ( \vartheta ; \pmb { x } ) } \end{array}
$$

![](images/fe21b5e708666444f6f95df7b79217a126742a5bb2ddc448e41fd19e0271bde6.jpg)  
Figure 2: Different samples on client have a different degree of drift to the model.

$f _ { n } ( \vartheta ; \pmb { x } )$ denotes the loss value with respect to model $\vartheta$ and random data sample $\scriptstyle { \pmb x }$ .

# Motivation

In this section, we revisit the principles of client drift and ask three questions:

# Q1: What is the impact of different local samples on client drift?

This question stems from that previous studies did not study the impact of local samples on client drift. We investigate this by profiling the training process of clients over local client samples and identify that different client samples have different drift degrees to the training of the local model. Figure 2 shows the above motivation. In fact, this naturally holds because the loss of the global model is different for each client sample where loss is a measure of the gap between the knowledge of local data and the knowledge of the global data. Therefore, we take loss value as the basis to measure the degree of the sample’s influence on the client drift. To verify the above viewpoint, we track the training progress of different samples on a single client with ResNet18 (He et al. 2016) model as an example. Based on loss, we sort client samples and extract the top $50 \%$ sample, the bottom $50 \%$ sample and all the samples respectively for training. Figure 3a is divided into four parts, which are the origin sample, $100 \%$ sample training, top $50 \%$ sample training and bottom $50 \%$ sample training according to the order of top left, top right, bottom left and bottom right. We can draw the following conclusion: the degree of the client model drift is also small when the loss of samples is small and vice versa. Therefore, we can partition the client sample into two sets based on the loss value, namely, the set of biased samples and unbiased samples which indicates samples contribute large and small to the degree of model drift separately.

# Q2: How to identify biased and unbiased samples from local samples?

A natural observation is that the loss of each sample in different rounds varies, causing a fixed loss threshold or ratiobased biased sample identification scheme to not apply to the scenario, which can also be observed from Figure 3c. To solve this, inspired by previous work (Fuchsgruber et al. 2024; Zhu et al. 2008) we identify that the uncertainty, i.e., the confidence of the model on the probability of sample classification, can adaptively give the partition boundary of unbiased samples and biased samples.

![](images/1c26522bc7a1f9440270189d1bdc40480a5076d015d0ed6430df8f83198ce10c.jpg)  
Figure 3: (a) Different impacts of various local samples on model drift. (b) The relationship between loss and uncertainty changes. (c) The relationship between uncertainty and loss as local samples vary. (d) The abrupt changes in the adaptive classification points in each round when a diversified knowledge acquisition stage is absent.

# Q3: How to mitigate the client drift with identified sample bias?

The core idea is that unbiased samples can reduce the drift degree of the model and increase the stability of training compared with biased samples. The intuitive idea is to filter out the biased sample part during client model training and only adopt the unbiased samples. However, the limit is that biased samples, as a part of all training samples, often contain information necessary for model training. Directly filtering out biased samples for training will affect the model’s knowledge acquisition of this part of the data, and ultimately affect the model’s performance. To this end, the method should satisfy two requirements: 1. minimized bias to reduce client drift; 2. minimized knowledge sacrification. Built upon the above analysis, we introduce a progressive learning concept, which gradually adds biased samples to the local training process. The intuition behind this is that when we gradually add biased samples, we can gradually guide the model from local to global stability to learn the

knowledge in biased samples.

# Methodology

Motivated by the above findings, we propose a novel and fine-grained framework named FedBSS which is illustrated in Figure 1. Our method FedBSS has the following innovations.

First, considering that different samples on client have a different degree of drift to the local updates, we evaluate and sort the loss set $S _ { n }$ of all the samples on nth client by global model from the server as follows:

$$
S _ { n } = \{ ( { \pmb x } _ { i } ^ { n } , S _ { k } ^ { n } ) \} _ { i = 1 } ^ { N _ { n } } , \quad \mathrm { w i t h } \ : S _ { i } ^ { n } < S _ { j } ^ { n } \quad \mathrm { i f } \ : i < j
$$

where $S _ { n }$ is the evalution loss set of $n$ -th client and $S _ { k } ^ { n }$ denotes the bottom $k$ th loss in all sample from $n$ th client. Second, in order to divide all sample into biased samples and unbiased sample from nth client, we need to set a classification sample $S _ { \mathrm { m i d } } ^ { n }$ which when the loss value $S _ { i } ^ { n } \leq S _ { \mathrm { m i d } } ^ { n }$ , $x _ { k } ^ { n }$ sample corresponding to the loss value $S _ { i } ^ { n }$ is unbiased sample while the rest is biased samples. However, as mentioned above, different rounds, different clients, and different samples will lead to different thresholds of classification points. Thus, we introduce the uncertainty to adaptively set the classification point for all clients on each round:

$$
\alpha ( x _ { i } ^ { n } ; \boldsymbol { \vartheta } ) ) = 1 - ( \mathbf { m a x } ( p ( x _ { i } ^ { n } ; \boldsymbol { \vartheta } ) ) - \mathbf { m i n } ( p ( x _ { i } ^ { n } ; \boldsymbol { \vartheta } ) ) )
$$

where $\alpha ( x _ { i } ^ { n } ; \vartheta ) )$ denotes the uncertainty of the model $\vartheta$ for the sample $ { \boldsymbol { x } } _ { i } ^ { n }$ on nth client.

We plot the relationship between loss and uncertainty in the model’s assessment of the sample as Figure 3b. With the increase of sample loss value, the model’s perception of the sample is essentially divided into three stages from figure. Specifically, the model’s judgment of sample is certain and accurate, the model’s judgment of sample is uncertain and inaccurate, and the model’s judgment of sample is certain but inaccurate. Based on the above properties, we can adaptively divide the sample into biased and unbiased sample by the highest uncertainty. Therefore, we take the sample with the highest uncertainty as the classification point at the beginning of client model training:

$$
\boldsymbol \alpha ( S _ { \mathrm { m i d } } ^ { n } ) = \mathbf { m a x } ( \boldsymbol \alpha ( x _ { i } ^ { n } ; \boldsymbol \vartheta ) ) )
$$

Then, we propose progressively adding biased samples along during local training process in a epoch-by-epoch manner as follows:

$$
\mathbf { X } _ { t , e } ^ { n } = \mathbf { X } _ { n o } ^ { n } + \alpha \mathbf { X } _ { b i a s } ^ { n } , \quad \alpha = \frac { 1 - \cos { \left( \frac { e } { e _ { \mathrm { t o t a l } } } \pi \right) } } { 2 }
$$

where $\mathbf { X } _ { t , e } ^ { n }$ denotes the $n$ th client training sample on client epoch e and global communication round t, $\mathbf { X } _ { n o }$ is the unbiased sample on $n$ th client, $\mathbf { X } _ { b i a s }$ is biased samples on nth client, $E _ { \mathrm { e } }$ is current client training epoch and $E _ { \mathrm { t o t a l } }$ is total epoch in the client training. Note that these samples are in the global round communication on the nth client.

One detail is that we do not adopt the scheme of linearly increasing biased samples epoch by epoch, such as $\begin{array} { r } { \mathbf { X } _ { t , e } ^ { n } = \mathbf { X } _ { n o } ^ { n } + \frac { e } { e _ { \mathrm { t o t a l } } } \mathbf { X } _ { b i a s } ^ { n } } \end{array}$ . The specific comparison results are in section . Concretely, we can discover that the samples near the classification point are relatively dense, while the samples at the edge of loss are relatively scattered, and the span of loss is large from figure 3c. This may result in the linear increment method not fitting the sample curve well. More discussion is demonstrated in section .

In addition to the above, we also need an extra first stage as the warmup. This is because the model’s initial judgment of the point of uncertainty fluctuates greatly. Figure 3d has shown the point. The intuition behind this is that the initial knowledge of the model is not enough, and it cannot make a relatively stable judgment on the client sample. Thus in order to ensure the stability of the initial training stage, warmup is used as the cold start stage of the model, which is to enable the model to acquire more diversified knowledge at the initial stage. Note that Figure 1 only describes the second stage.

In summary, our algorithm is divided into two stages: the first stage is Diversified knowledge acquisition stage, at which the client normally selects all the samples. And the second stage is Progressive knowledge learning stage. At this time, the algorithm first divides adaptive samples according to our above description, and then progressively adds unbiased samples during local client training.

# Experiments

# Experiment Setting

Datasets and models. We evaluate the performance of the proposed FedBSS over two models and four mainstream datasets. In Non-IID setting with label distribution skew, we consider Fashion-MNIST (Xiao, Rasul, and Vollgraf 2017), CIFAR-10 (Krizhevsky, Hinton et al. 2009) and CIFAR100 (Krizhevsky, Hinton et al. 2009), which contains 10, 10, 100 classes respectively. For CIFAR-10 and CIFAR-100 datasets, we use ResNet-50 (He et al. 2016) as the backbone to train and test the performance while for Fashion-MNIST we use a simple CNN instead. The simple CNN has two 5x5 convolution layers (the first with 32 channels, the second with 64, each followed with 3x3 max pooling), a fully connected layer with 512 units and ReLU activation. In Non-IID setting with feature distribution skew, we utilize DomainNet dataset.

Data Partition. Follow the setting (Kairouz et al. 2021), we adopt two classic data partitioning strategies, namely label distribution skew and feature distribution skew.

• Label Distribution Skew: To evaluate the performance of our work in a heterogeneous scenario with label distribution skew, we specify two Non-IID data partition methods called Shards (McMahan et al. 2017) and Dirichlet (Lin et al. 2020). In the Shards setting, the sorted samples are shuffled into $N * S$ shards, and assigned to $N$ clients randomly. Each client owns an equal number of pieces. In the second setting, data distribution over clients satisfies the Dirichlet distribution by using $\alpha$ to characterize the degree of heterogeneity. We set $\alpha$ of Dirichlet: $\{ 0 . 1 , 0 . 3 , 0 . 5 \}$ and shards for each client: $\{ 2 , 4 , 8 \}$ .

• Feature Distribution Skew: In this setting, clients share the same label space while different feature distribution, which has been extensively studied in previous works (Li et al. 2021; Peng et al. 2019; Yang et al. 2023). We conduct the classification task on natural images sourced from DomainNet (Peng et al. 2019), which consists of diverse distributions of images from six distinct data sources. we utilize all six distinct data domains. We selected ten categories—airplane, clock, axe, ball, bicycle, bird, strawberry, flower, pizza, and bracelet—for the classification task. Each client operates exclusively with the data from one domain, and in this setting, all six clients participated in the aggregation of the model.

Baselines. Beside of FedAvg, we also compare against various types of Non-IID federated learning approaches with the proposed method in our experiments. The first main type includes typical non-aggregation methods that calibrate local update, including Scaffold (Karimireddy et al. 2020), FedProx (Li et al. 2020), FedExP (Jhunjhunwala, Wang, and Joshi 2023),FedLC (Zhang et al. 2022) and FedRS (Li and Zhan 2021). Besides, another representative strategy is to select aggregation on the sever, such as InCo (Chan et al. 2024) and FedCDA (Wang et al. 2024a).

Table 1: The comparison of test accuracy of different methods. The best results are bolded.   

<html><body><table><tr><td>Method</td><td colspan="3">Fashion-MNIST(%)</td><td colspan="3">CIFAR-10(%)</td><td colspan="3">CIFAR-100(%)</td></tr><tr><td>Shards (S)</td><td>2</td><td>4</td><td>8</td><td>2</td><td>4</td><td>8</td><td>2</td><td>4</td><td>8</td></tr><tr><td>FedAvg</td><td>62.68±6.30</td><td>67.31±4.10</td><td>73.64±2.16</td><td>23.56±3.54</td><td>32.17±2.52</td><td>36.18±2.25</td><td>2.99±0.26</td><td>4.95±0.37</td><td>7.95±0.76</td></tr><tr><td>FedProx</td><td>63.42±5.23</td><td>68.71±3.55</td><td>72.58±3.54</td><td>20.65±1.84</td><td>27.18±3.13</td><td>31.89±1.17</td><td>2.34±0.21</td><td>4.47±0.48</td><td>6.30±0.62</td></tr><tr><td>FedExP</td><td>54.84±4.80</td><td>65.58±3.54</td><td>70.19±4.22</td><td>21.74±2.74</td><td>29.90±1.83</td><td>35.86±2.39</td><td>2.71±0.21</td><td>5.08±0.68</td><td>8.40±0.42</td></tr><tr><td>FedLC</td><td>64.67±0.25</td><td>74.20±0.39</td><td>76.64±0.44</td><td>21.28±0.23</td><td>25.19±0.35</td><td>32.07±0.27</td><td>4.33±0.09</td><td>7.06±0.11</td><td>9.18±0.12</td></tr><tr><td>FedRS</td><td>62.17±0.25</td><td>66.23±0.69</td><td>68.64±0.27</td><td>19.28±0.13</td><td>23.47±0.17</td><td>28.66±0.32</td><td>4.01±0.07</td><td>7.32±0.10</td><td>8.80±0.08</td></tr><tr><td>Scaffold</td><td>65.72±1.11</td><td>75.16±1.33</td><td>82.51±0.99</td><td>23.37±0.35</td><td>27.08±5.67</td><td>31.52±2.67</td><td>5.01±0.24</td><td>6.20±0.12</td><td>8.18±0.32</td></tr><tr><td>FedCDA</td><td>66.47±0.12</td><td>74.81±0.17</td><td>80.54±0.09</td><td>22.33±0.12</td><td>31.38±0.21</td><td>36.04±2.34</td><td>5.52±0.13</td><td>5.06±0.32</td><td>7.51±0.57</td></tr><tr><td>InCo</td><td>65.66±4.85</td><td>73.32±2.09</td><td>75.68±1.53</td><td>23.21±0.54</td><td>29.94±0.70</td><td>31.12±0.65</td><td>6.02±0.07</td><td>9.11±0.24</td><td>10.11±0.15</td></tr><tr><td>Ours</td><td>67.10±0.07</td><td>76.01±0.33</td><td>79.31±0.87</td><td>25.97±0.33</td><td>32.81±0.28</td><td>34.04±0.23</td><td>6.21±0.13</td><td>10.45±0.26</td><td>10.96±0.31</td></tr><tr><td>Dirichlet (α)</td><td>0.1</td><td>0.3</td><td>0.5</td><td>0.1</td><td>0.3</td><td>0.5</td><td>0.1</td><td>0.3</td><td>0.5</td></tr><tr><td>FedAvg</td><td>69.48±2.59</td><td>74.50±2.07</td><td>76.41±0.94</td><td>27.72±1.73</td><td>35.07±1.80</td><td>37.88±1.58</td><td>11.79±0.34</td><td>14.00±0.35</td><td>14.50±0.18</td></tr><tr><td>FedProx</td><td>61.39±6.17</td><td>70.65±3.38</td><td>73.87±2.37</td><td>23.36±2.57</td><td>26.86±2.92</td><td>30.16±2.47</td><td>7.99±0.55</td><td>10.70±0.33</td><td>11.03±0.18</td></tr><tr><td>FedExP</td><td>68.08±4.56</td><td>74.29±2.11</td><td>77.14±1.17</td><td>27.78±3.44</td><td>35.00±1.93</td><td>39.06±1.24</td><td>11.61±0.52</td><td>13.53±0.29</td><td>14.54±0.25</td></tr><tr><td>FedLC</td><td>67.11±0.31</td><td>69.27±0.16</td><td>74.19±1.09</td><td>27.99±0.28</td><td>32.42±1.25</td><td>40.76±1.02</td><td>11.88±0.26</td><td>13.00±0.12</td><td>14.17±0.58</td></tr><tr><td>FedRS</td><td>62.67±0.25</td><td>64.20±0.09</td><td>68.64±0.86</td><td>26.34±0.23</td><td>30.47±0.17</td><td>38.07±0.67</td><td>14.02±0.12</td><td>15.06±0.11</td><td>16.77±0.37</td></tr><tr><td>Scaffold</td><td>73.43±1.76</td><td>74.95±0.76</td><td>76.96±0.69</td><td>14.54±2.22</td><td>18.38±1.43</td><td>19.26±2.63</td><td>12.96±0.18</td><td>13.20±0.26</td><td>14.20±0.40</td></tr><tr><td>FedCDA</td><td>70.66±4.85</td><td>71.11±1.34</td><td>73.68±1.24</td><td>29.21±0.15</td><td>38.94±0.34</td><td>39.68±0.04</td><td>12.34±0.42</td><td>14.81±0.14</td><td>15.44±0.16</td></tr><tr><td>InCo</td><td>69.66±1.13</td><td>73.32±2.09</td><td>75.68±1.53</td><td>28.21±4.55</td><td>36.94±2.71</td><td>38.69±1.65</td><td>11.52±0.67</td><td>13.81±0.24</td><td>14.22±0.15</td></tr><tr><td>Ours</td><td>76.24±0.16</td><td>80.34±0.15</td><td>82.11±0.17</td><td>38.49±0.22</td><td>43.11±0.25</td><td>45.27±0.19</td><td>18.37±0.27</td><td>19.11±0.12</td><td>19.08±0.11</td></tr></table></body></html>

<html><body><table><tr><td>Method</td><td colspan="3">Fashion-MNIST(%)</td><td colspan="3">CIFAR-10(%)</td><td colspan="3">CIFAR-100(%)</td></tr><tr><td>Noisy Label Radio</td><td>0.1</td><td>0.3</td><td>0.5</td><td>0.1</td><td>0.3</td><td>0.5</td><td>0.1</td><td>0.3</td><td>0.5</td></tr><tr><td>FedAvg</td><td>80.19±0.13</td><td>78.44±0.11</td><td>75.28±0.31</td><td>42.15±0.27</td><td>36.78±0.42</td><td>28.23±0.35</td><td>13.70±0.16</td><td>11.13±0.23</td><td>8.08±0.16</td></tr><tr><td>FedProx</td><td>79.94±0.10</td><td>78.17±0.15</td><td>75.60±0.27</td><td>37.96±0.24</td><td>32.67±0.20</td><td>26.30±0.53</td><td>10.76±0.27</td><td>8.75±0.15</td><td>6.39±0.14</td></tr><tr><td>FedExP</td><td>80.27±0.11</td><td>78.28±0.13</td><td>75.74±0.25</td><td>41.82±0.18</td><td>36.60±0.24</td><td>29.32±0.50</td><td>13.44±0.23</td><td>11.11±0.18</td><td>7.51±0.24</td></tr><tr><td>FedLC</td><td>80.11±0.11</td><td>78.18±0.09</td><td>76.98±0.12</td><td>39.23±0.26</td><td>35.14±0.30</td><td>30.41±0.18</td><td>15.08±0.39</td><td>11.34±0.21</td><td>10.39±0.17</td></tr><tr><td>FedRS</td><td>79.86±0.22</td><td>77.59±0.11</td><td>74.88±0.13</td><td>37.88±0.24</td><td>34.28±0.25</td><td>27.68±0.38</td><td>12.09±0.18</td><td>10.09±0.17</td><td>9.21±0.28</td></tr><tr><td>Scaffold</td><td>78.65±0.09</td><td>78.88±0.09</td><td>76.40±0.17</td><td>40.72±0.36</td><td>37.14±0.19</td><td>29.36±0.41</td><td>12.37±0.24</td><td>9.94±0.15</td><td>7.39±0.06</td></tr><tr><td>FedCDA</td><td>79.63±0.15</td><td>78.20±0.12</td><td>75.74±0.30</td><td>39.83±0.15</td><td>35.27±0.21</td><td>30.49±0.28</td><td>14.18±0.22</td><td>10.33±0.21</td><td>9.10±0.21</td></tr><tr><td>InCo</td><td>79.89±0.56</td><td>77.27±0.60</td><td>75.16±0.46</td><td>38.12±1.13</td><td>34.55±0.99</td><td>24.78±1.75</td><td>11.02±0.34</td><td>9.54±0.29</td><td>8.28±0.54</td></tr><tr><td>Ours</td><td>80.63±0.14</td><td>79.91±0.13</td><td>78.88±0.08</td><td>44.46±0.22</td><td>40.27±0.29</td><td>38.96±0.19</td><td>16.01±0.17</td><td>15.47±0.28</td><td>12.13±0.34</td></tr></table></body></html>

Table 2: The comparison of test accuracy on noisy label datasets. The best results are bolded.

Implementation Details. We implement the whole experiment in a simulation environment based on RTX 3090 GPUs. We use 100 clients in total and randomly choose $10 \%$ each round for local training. We set the local epoch to 10, batch size to 64, and learning rate to $1 e \mathrm { ~ - ~ } 3$ . We employ SGD optimizer with momentum of $1 e - 4$ and weight decay of $1 e \mathrm { ~ - ~ } 5$ for all methods and datasets. At the same time, we set the number of global communication rounds to 200. For evaluation, we compute the average accuracy and standard deviation over the final 10 rounds of each run. For our method, we set the number of warmup rounds to 50.

# Result with Label Distribution Skew

We report the comparison results with other baselines in Table 1. In order to demonstrate the generalization of our method, we compare them on two different Non-IID settings, Shards and Dirichlet distribution. We apply different data distributions on different datasets. We can see that our proposed FedBSS achieves the best performance on almost all settings. It demonstrates the effectiveness and benefit of FedBSS. Specifically, on relatively larger datasets such as CIFAR-10 Dirichlet 0.5, FedBSS with ResNet-50 achieves $4 5 . 2 7 \%$ accuracy whereas the best baseline method FedLC achieves $4 0 . 7 6 \%$ accuracy. In addition, FedBSS with simple CNN also makes improvements on relatively smaller datasets, and the improvement is not less than in large models. At the same time, we can also see that the results of our method on relatively small datasets and simple CNN are not the best, which may be because the client biased samples of different rounds is less drift to the global model on small datasets and simple models, and can not provide better performance of the global model by progressive learning. In conclusion, we can notice our FedBSS makes more improvements on the large model, complex datasets than small

model, simple datasets.

Note that the baseline approaches we compare can actually be divided into two categories. One is calibrating local update, and the other is setting aggregating strategy. The performance of InCo in the table is not good enough. The main reason is that its perception of the client is relatively limited, so the improvement is small. Furtherly, we can see that scaffold outperforms our method FedBSS on some datasets, while it has not the good performance on some other datasets. This indicates that Scaffold is not stable and always sensitive to datasets and hyperparameters. The performance of our method FedBSS is usually more prominent when there is high heterogeneity setting. This may be because the biased samples drifts the model to a large extent when there is high heterogeneity. Therefore, our method can more effectively improve its performance in such scenarios. Besides, our experimental results show that the server aggregating strategy-based methods also improve less on small models and simple datasets. We consider that the main reason is that small models have little parameter, which the effect of directly improving the model parameters is not obvious in this scenario.

# Result with Feature Distribution Skew

In the above experiments, we define data heterogeneity with label distribution skew. In this section, we test our approach on feature Non-IID setting with feature distribution skew. We utilize domainNet dataset which contains six domains: clipart, infograph, painting, quickdraw, real, and sketch. We selected ten categories for the classification task and ResNet-18 as backbone. Each client operates exclusively with the data from one domain, and in this setting, all six clients participated in the aggregation of the model. Figure 4 is the result. We can find that our approach FedBSS outperforms FedAvg’s performance not only on six very different domains, but also on the final global model.

# Result on Noise Label Datasets

In real scenarios, data labels are often noisy or absent. In this section, we employ FedBSS on noise label dataset to test the robustness of our method. Table 2 shows results of our method FedBSS. We set different noise label radio which is 0.1, 0.3 and 0.5 on three datasets. We can find that our method achieve better performance compared with other Non-IID methods. Specifically, our approach offers a significant improvement over other approaches for both relatively smaller dataset Fashion-MNIST and relatively larger datasets CIFAR-100. With the increase of noise label ratio, our method improves more and more significantly, while FedBSS can still maintain good performance even under the most severe noise label ratio condition which is 0.5. It proves that our approach is robust and scalable.

# More discussion and experiments

In this section, we discuss two parts which are mentioned on section in our method FedBSS. The first one is about first warmup stage. The second one is about sample selection strategy.

![](images/3af40970e616168ae33117a2e291e92688a7f6cfa3374ca732fa9f872f1b635f.jpg)  
Figure 4: Result on DomainNet dataset

Warmup Stage For the first one, we compare with three baselines. Our method sets the warmup rounds as 50. We compare with the baseline which varies the warmup rounds as $\{ 0 , 2 5 , 1 0 0 \}$ . We can see that our method which sets the warmup rounds as 50. Note that this does not mean that 50 rounds is always optimal. However, we can discover that when the warmup round is zero which means the first warmup stage doesn’t exist, the performance of the model is always worse. This may be due to the lack of diversity knowledge in the first stage. Certainly, when warmup rounds are too high, the performance is worse too. This is because the first stage is too long, resulting in too many global model drifts, which makes the progressive learning effect of the second stage not good. Therefore, the warmup rounds are a relatively sensitive parameter. We think the first warmup phase is a good trade-off between minimizing client drift and minimizing knowledge sacrification. Specifically, the first phase helps the model acquire diversified knowledge, while the second phase mitigated the phenomenon of client draft.

Sample Selection Strategy We have discussed the biased sample selection strategy in section . To summarize, there are three strategies: filter, linearly addition and our method. We compared these three strategies. Our method proves the superiority and it outperforms the other two strategies. It proves that filtering biased samples may cause the model to lose part of its knowledge and fall into local optimal solutions or slow convergence. On the other hand, the effect of linear inclusion of biased sample is not good, which may be due to the sample distribution, which is more concentrated in the middle part and dispersed in the edge part. This may result in a linear function that does not fit well.

# Conclusion

This paper focuses on Non-IID setting in federated learning. We propose a novel two-stage method for FL called FedBSS that exploits the drift properties of different samples. Our method is to adopt a bias-aware sample selection scheme that dynamically selects the samples from small biases to large epoch by epoch to train progressively the local model in each round. We demonstrate its effectiveness and robustness.