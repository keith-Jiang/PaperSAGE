# Memory-Reduced Meta-Learning with Guaranteed Convergence

Honglin Yang1,2, Ji $\mathbf { M } \mathbf { a } ^ { 1 , 2 * }$ , Xiao $\mathbf { Y } \mathbf { u } ^ { 2 , 3 }$

1Department of Automation, Xiamen University, Xiamen, China 2Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen, China 3Institute of Artificial Intelligence, Xiamen University, Xiamen, China yanghonglin $@$ stu.xmu.edu.cn, maj $. 0 8 @$ xmu.edu.cn, xiaoyu $@$ xmu.edu.cn

# Abstract

The optimization-based meta-learning approach is gaining increased traction because of its unique ability to quickly adapt to a new task using only small amounts of data. However, existing optimization-based meta-learning approaches, such as MAML, ANIL and their variants, generally employ backpropagation for upper-level gradient estimation, which requires using historical lower-level parameters/gradients and thus increases computational and memory overhead in each iteration. In this paper, we propose a meta-learning algorithm that can avoid using historical parameters/gradients and significantly reduce memory costs in each iteration compared to existing optimization-based meta-learning approaches. In addition to memory reduction, we prove that our proposed algorithm converges sublinearly with the iteration number of upper-level optimization, and the convergence error decays sublinearly with the batch size of sampled tasks. In the specific case in terms of deterministic meta-learning, we also prove that our proposed algorithm converges to an exact solution. Moreover, we quantify that the computational complexity of the algorithm is on the order of $\mathcal { O } ( \dot { \epsilon } ^ { - 1 } )$ , which matches existing convergence results on meta-learning even without using any historical parameters/gradients. Experimental results on meta-learning benchmarks confirm the efficacy of our proposed algorithm.

Code — https://github.com/yangxiaolin9527/RM-MetaL Extended version — https://doi.org/10.48550/arXiv.2412.12030

# 1 Introduction

Meta-learning has emerged as a promising paradigm in modern machine learning. Unlike traditional machine learning methods that require large amounts of data points for model training (He et al. 2016; Vaswani et al. 2017), meta-learning leverages prior experience from previously learned tasks to quickly adapt to new tasks with limited data. This feature makes meta-learning particularly appealing for applications where data are scare or expensive to obtain (Denevi et al. 2019; Wang et al. 2020). To date, numerous meta-learning methods have been proposed, such as metric-based approaches (Vinyals et al. 2016; Snell, Swersky, and Zemel 2017), optimization-based approaches (Finn, Abbeel, and Levine 2017; Raghu et al. 2019), and modelbased approaches (Santoro et al. 2016; Li et al. 2019). Recently, with the rapid progress of optimization theory in machine learning, optimization-based meta-learning approaches are gaining increased traction (Bertinetto et al. 2018; Rajeswaran et al. 2019; Lee, Yoo, and Kwak 2023; Wu et al. 2023; Chen and Wang 2024a).

Optimization-based approaches often formulate metalearning as a bilevel programming problem, where the lower-level optimization is dedicated to adapting the model to a given task, and the upper-level optimization aims to train meta-parameters that can enable the model to generalize to new tasks (Maclaurin, Duvenaud, and Adams 2015; Zhang et al. 2024). Over the past decades, numerous optimization-based meta-learning approaches have been proposed, with typical examples including Model-Agnostic Meta-Learning (MAML) (Finn, Abbeel, and Levine 2017) and its variants (Finn et al. 2017; Zhou, Wu, and Li 2018; Nichol, Achiam, and Schulman 2018; Raghu et al. 2019; Oh et al. 2021; Chi et al. 2022; Kirsch et al. 2022), and bilevel-optimization-based approaches (Franceschi et al. 2018; Bertinetto et al. 2018; Zhou et al. 2019; Ji, Yang, and Liang 2020). However, traditional optimization-based metalearning approaches employ iterative differentiation to estimate upper-level gradients, i.e., hypergradient estimation, which requires the use of historical lower-level parameters/gradients, leading to increased computational and memory overhead, as evidenced by our experimental results in Figure 2 and Table 2. Although recent approaches, such as FOMAML (Finn, Abbeel, and Levine 2017) and others (Nichol, Achiam, and Schulman 2018; Zhou et al. 2019; Fallah, Mokhtari, and Ozdaglar 2020), mitigate the memory costs in each iteration by avoiding the use of historical lower-level parameters/gradients, they suffer from decreased learning accuracy due to the lack of information from these lower-level parameters/gradients.

# 1.1 Contributions

Our contributions can be summarized as follows.

1. We propose an optimization-based meta-learning algorithm that eliminates the need for using any historical parameters/gradients, and thus ensures nearly invariant memory costs over iterations. This is fundamentally different from traditional MAML-based and iterative-differentiation-based meta-learning approaches such as Finn, Abbeel, and Levine (2017); Raghu et al. (2019); Oh et al. (2021), where memory costs grow to infinity when the iteration number of lowerlevel optimization tends to infinity. In fact, our experimental results in Figure 2 show that our algorithm achieves at least a $5 0 \%$ reduction in memory costs compared to ANIL in Raghu et al. (2019) and the iterative-differentiation-based approach in Ji, Yang, and Liang (2020).

2. In addition to reducing memory costs, we also establish the convergence rate and computational complexity of our algorithm for both stochastic and deterministic metalearning, which is different from the existing bilevel programming literature, e.g., Franceschi et al. (2018); Shaban et al. (2019); Ji, Yang, and Liang (2020), that focus solely on deterministic cases. For stochastic meta-learning, we prove that our algorithm converges sublinearly with the number of iterations, and the convergence error decays sublinearly with the batch size of sampled tasks, which is consistent with the nonconvex nature of the stochastic meta-learning objective functions. For deterministic meta-learning, we prove that our algorithm can converge sublinearly to an exact solution.

3. We quantify that the computational complexity of our algorithm is on the order of $\mathsf { \bar { O } } ( \epsilon ^ { - 1 } )$ for both stochastic and deterministic meta-learning, which matches the currently well-known results on optimization-based metalearning (Rajeswaran et al. 2019; Ji et al. 2020).

4. Furthermore, since our algorithm estimates the hypergradient using the Hessian-inverse-vector-product without computing the full Hessian or Jacobian matrices, we improve the computational complexity of the hypergradient estimation in previous meta-learning approaches (Park and Oliva 2019; Hiller, Harandi, and Drummond 2022) from $\mathcal { O } ( q ^ { 2 } )$ (or $\mathcal { O } ( p q ) )$ to $\mathcal { O } ( \operatorname* { m a x } \{ p , q \} )$ per iteration.

5. We conduct experimental evaluations using several meta-learning benchmark datasets, including the “CIFARFS”, “FC100”, “miniImageNet”, and “tieredImageNet” datasets. The results confirm the efficiency of our algorithm in terms of both learning accuracy and memory reduction.

# 1.2 Related Work

The earliest optimization-based meta-learning approach is Model-Agnostic Meta-Learning (MAML), which is an initial parameter-transfer method where the objective is to learn a good “optimal initial model parameters” (Finn, Abbeel, and Levine 2017). Despite the widespread use of MAML and its variants such as DEML (Zhou, Wu, and Li 2018), ANIL (Raghu et al. 2019), and BOIL (Oh et al. 2021) in image classification (Chi et al. 2022; Ullah et al. 2022), reinforcement learning (Vuorio et al. 2019; Kirsch et al. 2022), and imitation learning (Finn et al. 2017; Li et al. 2021), these approaches rely on iterative differentiation for hypergradient estimation, which requires using historical lower level parameters/gradients, leading to high computational and memory overhead. To solve this issue, Finn, Abbeel, and Levine (2017) proposed a simplified MAML algorithm that avoids leveraging historical lower-level parameters/- gradients and uses only the last parameters in lower-level optimization for hypergradient updates, thereby reducing computational and memory overhead. However, the memory reduction achieved by this approach comes at the expense of degrading learning accuracy, which is undesirable in accuracy-sensitive meta-learning applications (Zhou et al. 2019; Peng 2020).

Driven by the need of reducing computational and memory costs in optimization-based meta-learning approaches, plenty of bilevel-optimization-based algorithms have recently been proposed (Bertinetto et al. 2018; Ji, Yang, and Liang 2020; Chen and Wang 2024a) (which aim to learn good embedding model parameters, rather than focusing on a good initialization as MAML-based approaches do). Initially, these algorithms addressed meta-learning problems by treating the lower-level optimality condition as constraints to the upper-level problem. However, this method is inapplicable to the scenario where the lower-level problem is large scale or the lower-level objective function has a complex structure. More recently, Rajeswaran et al. (2019); Ji, Yang, and Liang (2020) proposed an approximate-implicitdifferentiation-based approach to solve meta-learning problems. However, this approach requires a common optimal solution for lower-level optimization, which is often difficult to satisfy in many meta-learning applications (note that in meta-learning, the lower-level optimization aims to adapt the model to different tasks, leading to heterogeneous optimal solutions for lower-level optimization problems). Moreover, the convergence analysis in Ji, Yang, and Liang (2020) is based on deterministic gradients, and it is not clear if the deterministic convergence analysis methods can be extended to stochastic meta-learning, where the task distribution is unknown in practice.

# 2 Preliminaries

Notations We use $\mathbb { R }$ and $\mathbb { N } ^ { + }$ to represent the sets of real numbers and positive integers, respectively. We abbreviate with respect to by $w . r . t .$ . We denote $\nabla F ( { \dot { \boldsymbol { \theta } } } ) ~ \in ~ \mathbb { R } ^ { p }$ as the gradient of $F ( \theta )$ . We use $\nabla _ { \boldsymbol { \theta } } g ( \boldsymbol { \theta } , \boldsymbol { \phi } )$ and $\nabla _ { \phi } g ( \theta , \phi )$ to represent the gradients of $\textit { g } w . r . t . \textit { \theta }$ and $\phi$ , respectively. We write $\nabla _ { \theta \phi } ^ { 2 } \breve { g ( \theta , \phi ) } \in \mathbb { R } ^ { p \times q }$ for the Jacobian matrix of $g$ and $\nabla _ { \phi } ^ { 2 } g ( \theta , \phi ) \in \mathbb { R } ^ { q \times q }$ for the Hessian matrix of g w.r.t. $\phi$ . For vectors $\phi _ { 1 } , \cdot \cdot \cdot , \phi _ { n }$ , we use $\phi = \mathrm { c o l } \{ \phi _ { 1 } , \cdot \cdot \cdot , \phi _ { n } \}$ to represent their stacked column vector. For a set $\boldsymbol { B }$ , we denote the number of its elements by $| B |$ .

# 2.1 Meta-Learning

We consider a meta-learning problem with a series of tasks $\{ \mathcal { T } _ { i } , \textit { i } \in \mathbb { N } ^ { + } \}$ . All tasks are drawn from an unknown task distribution $\mathcal { P } ( \tau )$ . Each task $\mathcal { T } _ { i }$ has a loss function ${ \mathcal { L } } ( \theta , \phi ; \xi _ { i } )$ over each data point $\xi _ { i }$ , where $\theta$ represents the meta-parameters of an embedding model shared by all tasks, and $\phi$ represents the task-specific parameters. The goal of meta-learning is to find common optimal parameters $\theta ^ { * }$ that benefit all tasks, and based on the parameters $\theta ^ { * }$ , the model can quickly adapt its own parameters $\phi$ to a new task $\mathcal { T } _ { i }$ using only a few data points and training iterations. Typically, the meta-learning problem can be formulated as the following bilevel programming problem:

$$
\begin{array} { r l } & { \underset { \theta \in \mathbb { R } ^ { p } } { \operatorname* { m i n } } F ( \theta ) = \mathbb { E } _ { \mathcal { T } _ { i } \sim \mathcal { P } ( \mathcal { T } ) } [ f _ { i } ( \theta , \phi _ { i } ^ { * } ( \theta ) ) ] , } \\ & { \mathrm { s . t . } \quad \phi _ { i } ^ { * } ( \theta ) = \underset { \phi \in \mathbb { R } ^ { q } } { \operatorname { a r g m i n } } g _ { i } ( \theta , \phi ) . } \end{array}
$$

In task $\mathcal { T } _ { i }$ , the upper-level objective function $f _ { i } ( \theta , \phi )$ is given by fi(θ, ϕ) ≜ |Di1,f | P|jD=i1,f | L(θ, ϕ; ξij,f ), where data $\xi _ { i , f } ^ { j }$ , $j \in \{ 1 , \cdot \cdot \cdot , | \mathcal { D } _ { i , f } | \}$ is sampled from the validation dataset $\mathcal { D } _ { i , f }$ , and the objective function $g _ { i } ( \theta , \phi )$ is given by $\begin{array} { r } { g _ { i } ( \theta , \phi ) \triangleq \frac { 1 } { | { \mathcal D } _ { i , g } | } \sum _ { j = 1 } ^ { | { \mathcal D } _ { i , g } | } { \mathcal L } ( \theta , \phi ; \xi _ { i , g } ^ { j } ) + R ( \phi ) } \end{array}$ , where data $\xi _ { i , g } ^ { j } , \ j \in \{ 1 , \cdot \cdot \cdot , | \mathcal { D } _ { i , g } | \}$ is sampled from training dataset $\mathcal { D } _ { i , g }$ and $R ( \phi )$ is a strongly-convex regularizer w.r.t. $\phi$ .

Remark 1. Compared with existing bilevel-programming frameworks which use common lower-level optimal parameters $\phi ^ { * }$ to enable upper-level optimization (see, e.g., Ji, Yang, and Liang (2020)), our meta-learning framework in (1) allows different tasks $\mathcal { T } _ { i }$ to have different lower-level optimal parameters $\phi _ { i } ^ { * }$ . This heterogeneity significantly complicates our convergence analysis.

Remark 2. The meta-learning formulation in (1) involves bilevel-programming objectives. This is fundamentally different from conventional single-level machine learning, which focuses on learning optimal parameters $\phi ^ { * }$ for a specific task using a large number of data points (Soydaner 2020; Chen et al. 2024; Chen and Wang 2024b). However, in many applications, collecting data is costly and time consuming, and may even be infeasible due to physical system constraints (Wang et al. 2020). Therefore, meta-learning is typically regarded as a more general framework than conventional single-level machine learning (Shu, Meng, and $X u$ 2021). In the special case where the upper-level objective function is absent, the meta-learning problem can be reduced to a single-level machine learning problem.

The objective functions in (1) satisfy the following standard assumptions.

Assumption 1. $F ( \theta )$ is nonconvex and $g _ { i } ( \theta , \phi )$ is $\mu$ - strongly convex w.r.t. $\phi$ .

Assumption 2. We denote $f ( \theta , \phi ) \triangleq \mathbb { E } _ { \mathcal { T } _ { i } \sim \mathcal { P } ( \mathcal { T } ) } [ f _ { i } ( \theta , \phi ) ]$ and $g ( \theta , \phi ) \triangleq \mathbb { E } _ { \mathcal { T } _ { i } \sim \mathcal { P } ( \mathcal { T } ) } [ g _ { i } ( \theta , \phi ) ]$ . Then, the following statements are satisfied: (i) Functions $f , \nabla f , \nabla g , \nabla _ { \theta \phi } ^ { 2 } g ,$ , and $\nabla _ { \phi } ^ { 2 } g$ are $l _ { f , 0 } , \ l _ { f } , \ l _ { g } , \ l _ { g , 1 }$ , and $l _ { g , 2 }$ Lipschitz continuous, respectively. (ii) Gradients $\nabla f _ { i } ( \theta , \phi )$ , $\nabla _ { \phi } ^ { 2 } g _ { i } ( \theta , \phi )$ and $\nabla _ { \theta \phi } ^ { 2 } g _ { i } ( \theta , \phi )$ are unbiased and have bounded variances $\sigma _ { f 1 } ^ { 2 }$ , $\sigma _ { g 1 } ^ { 2 }$ , and $\sigma _ { g 2 } ^ { 2 }$ , respectively.

Assumptions 1 and 2 are standard in meta-learning literature; see, for example Ji et al. (2020). Note that we do not impose that the lower-level function $g _ { i }$ is Lipschitz w.r.t. $\phi$ , which is required in iMAML in Rajeswaran et al. (2019). Note that iMAML implicitly assumes the search space of parameters $\phi _ { i }$ to be bounded such that function $g _ { i }$ is bounded. Moreover, since each task $\mathcal { T } _ { i }$ , $i \in \{ 1 , 2 , \cdots , | \boldsymbol { B } | \}$ is randomly sampled from the task distribution $\mathcal { P } ( \tau )$ , gradients $\nabla f _ { i } ( \theta , \phi )$ and $\nabla g _ { i } ( \theta , \phi )$ are stochastic, and thus we assume that they satisfy Assumption 2-(ii).

In practice, the task distribution $\mathcal { P } ( \tau )$ is usually unknown, making it impossible to directly solve problem (1). To circumvent this problem, a common approach is to reformulate (1) as the following empirical risk minimization problem:

$$
\begin{array} { l } { \displaystyle \underset { \theta \in \mathbb { R } ^ { p } } { \operatorname* { m i n } } F _ { \mathcal { B } } ( \theta ) = f _ { \mathcal { B } } \left( \theta , \phi ^ { * } ( \theta ) \right) = \frac { 1 } { | \mathcal { B } | } \displaystyle \sum _ { i = 1 } ^ { | \mathcal { B } | } f _ { i } ( \theta , \phi _ { i } ^ { * } ( \theta ) ) , } \\ { \mathrm { s . t . } \quad \phi _ { i } ^ { * } ( \theta ) = \underset { \phi \in \mathbb { R } ^ { q } } { \operatorname { a r g m i n } } g _ { i } ( \theta , \phi ) , } \end{array}
$$

where $\phi ^ { * } ( \theta )$ represents a stacked vector of task-specific optimal parameters $\phi _ { i } ^ { * }$ and $| B |$ denotes the batch size of sampled tasks.

# 2.2 Main Challenges in Meta-Learning

The main challenge in solving problem (2) lies in computing the hypergradient $\nabla F _ { B } ( { \boldsymbol { \theta } } )$ of the upper-level function w.r.t. $\theta$ . From (2), the hypergradient computation requires knowledge of $\phi ^ { * } ( \theta )$ . However, obtaining $\phi ^ { * } ( \theta )$ is often difficult in large-scale meta-learning, especially when the lower-level objective function $g _ { i } ( \theta , \phi )$ has a complex structure.

To solve this issue, a commonly used approach is to employ iterative differentiation for hypergradient estimation; see, for example, Domke (2012); Maclaurin, Duvenaud, and Adams (2015); Franceschi et al. (2017). More specifically, the iterative-differentiation-based approach first executes a $K$ -step gradient descent to solve the lower-level optimization problem in (2) and obtain an approximate solution $\phi _ { i } ^ { K } ( \theta )$ for each task $\mathcal { T } _ { i }$ . Then, building on the solution $\phi _ { i } ^ { K } ( \theta )$ , this approach estimates the hypergradient by using the following relation (Ji, Yang, and Liang 2020):

$$
\begin{array} { r l } & { \frac { \partial f _ { B } ( \theta , \phi ^ { K } ( \theta ) ) } { \partial \theta } = \displaystyle \frac { 1 } { | \mathcal { B } | } \sum _ { i = 1 } ^ { | \mathcal { B } | } \frac { \partial f _ { i } ( \theta , \phi _ { i } ^ { K } ( \theta ) ) } { \partial \theta } } \\ & { \quad = \displaystyle \frac { 1 } { | \mathcal { B } | } \sum _ { i = 1 } ^ { | \mathcal { B } | } \left( \nabla _ { \theta } f _ { i } ( \theta , \phi _ { i } ^ { K } ( \theta ) ) - \lambda _ { \phi } \sum _ { k = 0 } ^ { K - 1 } \nabla _ { \theta \phi } ^ { 2 } g _ { i } ( \theta , \phi _ { i } ^ { k } ( \theta ) ) \times \right. } \\ & { \qquad \left. \prod _ { q = k + 1 } ^ { K - 1 } \left( I - \lambda _ { \phi } \nabla _ { \phi } ^ { 2 } g _ { i } ( \theta , \phi _ { i } ^ { q } ( \theta ) ) \right) \nabla _ { \phi } f _ { i } ( \theta , \phi _ { i } ^ { K } ( \theta ) ) \right) . \quad ( 3 ) } \end{array}
$$

It is clear that the hypergradient estimation in (3) relies on historical lower-level parameters ${ \phi } _ { i } ^ { k } ( \theta ) , k = 0 , \cdot \cdot \cdot , K$ . As the iteration number of lower-level optimization grows, this dependency significantly increases both computational and memory overhead, as evidenced by our experimental results in Figure 2 and Table 2. Moreover, obtaining a good approximation for $\phi _ { i } ^ { * } ( \theta )$ often requires numerous iterations in lower-level optimization, which inevitably extends the backpropagation chain and results in gradient problems such as vanishing or exploding hypergradient estimation (Ji et al. 2020; Jamal, Wang, and Gong 2021).

Motivated by these observations, we aim to propose a meta-learning algorithm that can avoid using historical lower-level parameters/gradients for hypergradient estimation while still ensuring comparable convergence compared with existing optimization-based meta-learning approaches.

# 3 Methodology

In this section, we propose a memory-reduced algorithm for meta-learning with provable convergence to a solution $\theta ^ { * }$ to problem (2). Before presenting our algorithm, we first introduce our hypergradient-estimation approach.

# 3.1 Hypergraident Estimation

Inspired by the recent results on stochastic bilevel programming (Ghadimi and Wang 2018; Lorraine, Vicol, and Duvenaud 2020), we estimate the hypergradient by using the following relation:

$$
\begin{array} { r l r }   { \nabla F _ { \mathcal { B } } ( \theta ) = \nabla _ { \theta } f _ { \mathcal { B } } ( \theta , \phi ^ { * } ( \theta ) ) - \frac { 1 } { | \mathcal { B } | } \sum _ { i = 1 } ^ { | \mathcal { B } | } \big ( \nabla _ { \theta \phi } ^ { 2 } g _ { i } ( \theta , \phi _ { i } ^ { * } ( \theta ) ) } \\ & { } & { \times [ \nabla _ { \phi } ^ { 2 } g _ { i } ( \theta , \phi _ { i } ^ { * } ( \theta ) ) ] ^ { - 1 } \nabla _ { \phi } f _ { i } ( \theta , \phi _ { i } ^ { * } ( \theta ) ) \big ) . \quad ( 4 , 4 , 5 ) } \end{array}
$$

From (4), it can be seen that obtaining $\nabla F _ { B } ( { \boldsymbol { \theta } } )$ requires computing the inverse of Hessian matrix $[ \mathsf { \bar { V } } _ { \phi } ^ { 2 } g _ { i } ( \dot { \theta } , \dot { \phi } _ { i } ^ { * } ( \dot { \theta } ) ) ] ^ { - 1 }$ and Jacobian matrix $\nabla _ { \theta \phi } ^ { 2 } g _ { i } ( \theta , \phi _ { i } ^ { * } ( \theta ) )$ . To avoid computing the full Hessian/Jacobian matrix, we aim to estimate the Hessian-inverse-vector product:

$$
\boldsymbol { v } _ { i } ^ { * } = [ \nabla _ { \phi } ^ { 2 } g _ { i } ( \theta , \phi _ { i } ^ { * } ( \theta ) ) ] ^ { - 1 } \nabla _ { \phi } f _ { i } ( \theta , \phi _ { i } ^ { * } ( \theta ) ) .
$$

Based on (4), the hypergradient can be rewritten as

$$
\nabla F _ { \mathcal { B } } ( \theta ) = \nabla _ { \theta } f _ { \mathcal { B } } \big ( \theta , \phi ^ { * } ( \theta ) \big ) - \frac { 1 } { | \mathcal { B } | } \sum _ { i = 1 } ^ { | \mathcal { B } | } \nabla _ { \theta \phi } ^ { 2 } g _ { i } \big ( \theta , \phi _ { i } ^ { * } ( \theta ) \big ) v _ { i } ^ { * } ,
$$

where $\nabla _ { \theta \phi } ^ { 2 } g _ { i } ( \theta , \phi _ { i } ^ { * } ( \theta ) ) v _ { i } ^ { * }$ will be referred to as the Jacobianvector product. It follows from (6) that if the estimation of $\nabla _ { \boldsymbol { \theta } } \bar { f _ { B } } ( \boldsymbol { \theta } , \boldsymbol { \phi ^ { * } } ( \boldsymbol { \theta } ) ) ,$ $v _ { i } ^ { * }$ , and $\nabla _ { \theta \phi } ^ { 2 } g _ { i } ( \theta , \phi _ { i } ^ { * } ( \theta ) ) v _ { i } ^ { * }$ is accurate enough, a good estimation of the hypergradient is obtained.

Note that estimating the Hessian-inverse-vector product $v _ { i } ^ { * }$ and further Jacobian-vector product $\nabla _ { \theta \phi } ^ { 2 } g _ { i } ( \theta , \phi _ { i } ^ { * } ( \theta ) ) v _ { i } ^ { * }$ circumvents the requirement on estimating the full Hessian and Jacobian matrices, and hence, reduces the computational complexity from the order of $\mathcal { O } ( q ^ { 2 } )$ (or $\mathcal { O } ( p q ) )$ to the order of $\mathcal { O } ( \operatorname* { m a x } \{ p , q \} )$ per iteration. This is different from existing results on bilevel programming and metalearning (Park and Oliva 2019; Chen et al. 2022; Hiller, Harandi, and Drummond 2022), which estimate the hypergradient by computing full matrices directly, leading to heavy computational overhead.

# 3.2 Memory-Reduced Meta-Learning Algorithm Design

In this section, we first introduce an approach to estimate the Hessian-inverse-vector product $\boldsymbol { v } _ { i } ^ { * }$ , which is necessary for estimating the hypergradient according to (6). Using it as a subroutine, we will then propose our memory-reduced meta-learning algorithm.

Approximating $\boldsymbol { v } _ { i } ^ { * }$ in (5) equals to solving the equation $\nabla _ { \phi } ^ { 2 } { \bar { g _ { i } } } ( \theta , \phi _ { i } ^ { * } ( \theta ) ) { \bar { v _ { i } } } = \nabla _ { \phi } f _ { i } ( \theta , { \bar { \phi } } _ { i } ^ { * } ( \theta ) )$ , which is the optimality condition of the following optimization problem:

$$
\operatorname* { m i n } _ { \boldsymbol { v } \in \mathbb { R } ^ { q } } \varphi ( \boldsymbol { v } ) , \quad \varphi ( \boldsymbol { v } _ { i } ) = \frac { 1 } { 2 } \boldsymbol { v } _ { i } ^ { T } H _ { i } \boldsymbol { v } _ { i } - b _ { i } ^ { T } \boldsymbol { v } _ { i } ,
$$

Subroutine 1: Estimating Hessian-inverse-vector product at the $t$ -th outer loop iteration

1: Input: Parameters $\theta _ { t }$ and $\phi _ { i , t } ^ { K }$ ; integer $N$ .   
2: $\ v _ { i , t } ^ { 0 } = v _ { i , t - 1 } ^ { N }$ if $t > 0$ , and $\mathbf { \nabla } v _ { i , t } ^ { 0 } = \mathbf { 0 } _ { q }$ otherwise.   
$\boldsymbol { r } _ { i , t } ^ { 0 } = \boldsymbol { p } _ { i , t } ^ { 0 } = \nabla _ { \phi } f _ { i } ( \theta _ { t } , \phi _ { i , t } ^ { K } ) - \nabla _ { \phi } ^ { 2 } g _ { i } ( \theta _ { t } , \phi _ { i , t } ^ { K } ) \boldsymbol { v } _ { i , t } ^ { 0 }$ .   
4: for $n = 0 , 1 . . . , N - 1$ do   
5: Get Hessian-vector product $h _ { i , t } ^ { n } = \nabla _ { \phi } ^ { 2 } g _ { i } ( \theta _ { t } , \phi _ { i , t } ^ { K } ) p _ { i , t } ^ { n }$ .   
6: ηin,t prin,tT rhin,t .   
7: $\boldsymbol { v } _ { i , t } ^ { n + 1 } = \boldsymbol { v } _ { i , t } ^ { n } + \eta _ { i , t } ^ { n } \boldsymbol { p } _ { i , t } ^ { n } .$   
8: $r _ { i , t } ^ { n + 1 } = r _ { i , t } ^ { n } - \eta _ { i , t } ^ { n } h _ { i , t } ^ { n } .$   
9: $\begin{array} { r } { \zeta _ { i , t } ^ { n } = \frac { { ( r _ { i , t } ^ { n + 1 } ) } ^ { T } r _ { i , t } ^ { n + 1 } } { r _ { i , t } ^ { n } T _ { i , t } ^ { n } } } \end{array}$   
10: $p _ { i , t } ^ { n + 1 } = r _ { i , t } ^ { n + 1 } + \zeta _ { i , t } ^ { n } p _ { i , t } ^ { n }$ .   
112: eOnudtpfourt: $\boldsymbol { v } _ { i , t } ^ { N }$ .

where $H _ { i }$ and $b _ { i }$ are given by $H _ { i } = \nabla _ { \phi } ^ { 2 } g _ { i } ( \theta , \phi _ { i } ^ { * } ( \theta ) )$ and $b _ { i } = \nabla _ { \phi } f _ { i } ( \theta , \phi _ { i } ^ { * } ( \theta ) )$ , respectively. We present Subroutine 1 to find the optimal solution $v _ { i } ^ { * }$ to problem (7).

Building on Subroutine 1, we can estimate the hypergradient in (6) by using the following equality:

$$
\nabla \widehat { F } _ { \mathcal { B } } ( \theta _ { t } ) = \nabla _ { \theta } f _ { \mathcal { B } } ( \theta _ { t } , \phi _ { t } ^ { K } ) - \frac { 1 } { | \mathcal { B } | } \sum _ { i = 1 } ^ { | \mathcal { B } | } \nabla _ { \theta \phi } ^ { 2 } g _ { i } ( \theta , \phi _ { i , t } ^ { K } ) v _ { i , t } ^ { N } .
$$

With the hypergradient estimation (8), we propose a memory-reduced algorithm for solving the meta-learning problem (1) in Algorithm 1.

In Algorithm 1, we use parameters $\phi _ { i , t } ^ { K }$ at the last innerloop iteration for hypergradient estimation. This is different from existing iterative-differentiation-based meta-learning approaches, such as Finn, Abbeel, and Levine (2017); Zhou, Wu, and Li (2018); Oh et al. (2021); Antoniou, Edwards, and Storkey (2019); Raghu et al. (2019); Baik et al. (2021); Yao, Zhang, and Finn (2022), which employ historical lowerlevel parameters $\phi _ { i } ^ { k } ( \theta ) , k \in \{ 0 , \cdot \cdot \cdot , \bar { K } \}$ for hypergradient estimation; see Eq. (3) for details. Our Algorithm 1 significantly reduces memory costs in each outer-loop iteration $t$ , which is evidenced by our experimental results in Figure 2. Moreover, the avoidance of long-distance backpropagation in Algorithm 1 also reduces the risk of exploding or vanishing gradients, leading to better learning accuracy than existing iterative-differentiation-based meta-learning approaches in Finn, Abbeel, and Levine (2017); Raghu et al. (2019); Ji, Yang, and Liang (2020), as evidenced by our experimental results in Figure 1 and Table 1.

Inspired by the linear acceleration capability of conjugate gradient approaches in solving the quadratic programming problem (Grazzi et al. 2020), we leverage the conjugate gradient in our Subroutine 1. However, different from the existing conjugate gradient-based algorithm in Ji, Yang, and Liang (2020) which relies on deterministic gradients, we employ stochastic gradients, which complicates our convergence analysis.

1: Input: The batch size of sampled tasks $| B |$ ; random initialization $\theta _ { 0 }$ and $\phi _ { i , 0 }$ for all $i \in \{ 1 , \cdots , | B | \}$ ; stepsizes $\lambda _ { \phi }$ and $\lambda _ { \theta }$ ; integers $T$ and $K$ .   
2: for $t = 0 , 1 , 2 , . . . , T - 1$ do   
3: Sample a task batch $B \sim \mathcal { P } ( \tau )$ .   
4: $u _ { 0 } = \mathbf { 0 } _ { p }$ .   
5: for $i = \bar { 1 } , \cdots , | \boldsymbol { B } |$ do   
6: Set $\phi _ { i , t } ^ { 0 } = \phi _ { i , t - 1 } ^ { K }$ if $t > 0$ , and $\phi _ { i , 0 }$ otherwise.   
87: fo ${ \phi } _ { i , t } ^ { k + 1 } = { \phi } _ { i , t } ^ { k } - \lambda _ { \phi } \nabla _ { \phi } g _ { i } \big ( \theta _ { t } , \phi _ { i , t } ^ { k } \big ) ,$ $k = 0 , 1 , . . . , K - 1$ .   
9: end for   
10: Run Subroutine 1 and obtain Hessian-inversevector product $\boldsymbol { v } _ { i , t } ^ { N }$ .   
11: Accumulate Jacobin-vector products $\nabla _ { \theta \phi } ^ { 2 } g _ { i } ( \theta _ { t } , \phi _ { i , t } ^ { K } ) v _ { i , t } ^ { N }$ . $u _ { i } = u _ { i - 1 } +$   
12: end for   
13: Compute $\begin{array} { r } { \nabla \widehat { F } _ { \mathcal { B } } ( \theta _ { t } ) = \nabla _ { \theta } f _ { \mathcal { B } } ( \theta _ { t } , \phi _ { t } ^ { K } ) - \frac { 1 } { | \mathcal { B } | } u _ { | \mathcal { B } | } . } \end{array}$   
14: $\theta _ { t + 1 } = \theta _ { t } - \lambda _ { \theta } \nabla \widehat { F } _ { B } ( \theta _ { t } )$ .   
15: end for   
16: Output: $\theta _ { T }$

Remark 3. To deal with the bilevel programming objectives (where upper-level optimality relies on lower-level optimality), we employ nested-loop iterations in Algorithm 1. Note that nested-loop iterations are commonly used in metalearning algorithms, such as Raghu et al. (2019); Rajeswaran et al. (2019). The iteration numbers $K$ and $N$ in Algorithm 1 are fixed constants, which are independent of the outer-loop iteration number T . This is different from existing bilevel programming approaches in Chen et al. (2022); Chen, Sun, and Yin (2021) which have the innerloop iteration number increasing with the outer-loop iteration, and hence have a heavier computational overhead. In addition, different from existing meta-learning algorithms in Hiller, Harandi, and Drummond (2022); Park and Oliva (2019) which estimate the full Hessian matrix or Jacobian matrix, Algorithm 1 only estimates a vector of dimension $\operatorname* { m a x } \{ p , q \}$ , and thus reduces computational complexity.

# 4 Convergence Analysis

Algorithm 1 can ensure sublinear convergence with the number $T$ of outer-loop iteration, and the convergence error decreases sublinearly with the batch size of the sampled tasks. The results are summarized in Theorem 1, whose proof is given in Section B of the supplementary material.

Theorem 1. Under Assumptions $\boldsymbol { { \mathit { 1 } } }$ and 2, if the iteration numbers $K$ and $N$ satisfy $K \ge K _ { 0 }$ and $N \geq N _ { 0 }$ with detailed forms of $K _ { 0 }$ and $N _ { 0 }$ given in Section $_ { B . 2 }$ of the supplementary material, the iterates $\theta _ { t }$ generated by Algorithm $\boldsymbol { { \mathit { 1 } } }$ satisfy

$$
\frac { 1 } { T + 1 } { \sum _ { t = 0 } ^ { T } } \mathbb { E } [ \left. \nabla F ( \theta _ { t } ) \right. ^ { 2 } ] \leq \mathcal { O } \left( \frac { 1 } { T } \right) + \mathcal { O } \left( \frac { 1 } { | \mathcal { B } | } \right) .
$$

Theorem 1 proves that Algorithm 1 converges to a stable solution to problem (1) with the optimization error decreasing as the batch size of sampled tasks increases. We would like to point out that the bound $\begin{array} { r } { \mathcal { O } ( \frac { 1 } { \vert \boldsymbol { B } \vert } ) } \end{array}$ in Theorem 1, caused by finite batch sizes of sampled tasks, inherently exists in all stochastic optimization approaches with finite samples (Gower et al. 2019). Although the variancereduction technique (Reddi et al. 2016; Fang et al. 2018) can be used to mitigate the influence of this term in singlelevel stochastic optimization, the extension of this approach to meta-learning/bilevel programming is hard to implement, since it is difficult to derive unbiased estimators of hypergradient, let alone variance reduction ones; see Dagre´ou et al. (2022) for details.

Moreover, to give a more intuitive description of the computational complexity, we define an $\epsilon$ -solution to problem (1) as follows.

Definition 1. (Lian et al. 2017) For some positive integer $T$ , $\begin{array} { r } { i f ~ \frac { 1 } { T } \sum _ { t = 0 } ^ { T - 1 } \mathbb { E } [ \| \nabla F ( \theta _ { t } ) \| ^ { 2 } ] \le \epsilon } \end{array}$ holds, then we say that the sequence $\{ \theta _ { t } \} _ { t = 0 } ^ { T }$ can reach an $\epsilon$ -solution to problem (1).

Building on Theorem 1, we have the following corollary.

Corollary 1. Under the conditions of Theorem $\boldsymbol { { \mathit { 1 } } }$ , for any $\epsilon > 0$ , Algorithm $\boldsymbol { { \mathit { 1 } } }$ requires at most $\mathrm { \bar { \mathcal { O } } ( ( 3 + 2 K _ { 0 } ) } \mathrm { \bar { | } } B { \left| \epsilon ^ { - 1 } \right. }$ gradient evaluations on $\phi$ and $\mathcal { O } ( ( 1 + K _ { 0 } \vert B \vert ) \epsilon ^ { - 1 } )$ gradient evaluations on $\theta$ to obtain an $\epsilon$ -solution.

In Corollary 1, the inner-loop iteration number $K _ { 0 }$ in Algorithm 1 is a fixed constant, which is different from the existing bilevel programming approaches in Chen et al. (2022); Chen, Sun, and Yin (2021) which have the inner-loop iteration number increasing with the outer-loop iteration, and hence have a higher computational complexity of the order of $\mathcal { O } ( \epsilon ^ { - 2 } )$ . Moreover, the computational complexity of our Algorithm 1 matches the convergence results for MAML and ANIL in Ji et al. (2020), even when we use less memory in each outer iteration.

In Theorem 1, we consider a stochastic scenario in which tasks are drawn from an unknown task distribution $\mathcal { P } ( \tau )$ . Next, we consider a deterministic scenario in which we iterate over all sample elements in the task space. In this case, Assumption 2 becomes a deterministic version as follows.

Assumption 3. For each task $\textstyle { \mathcal { T } } _ { i } ,$ , functions $f _ { i } , \nabla f _ { i } , \nabla g _ { i } ,$ , $\nabla _ { \theta \phi } ^ { 2 } g _ { i }$ , and $\nabla _ { \phi } ^ { 2 } g _ { i }$ are $l _ { f , 0 } , l _ { f } , l _ { g } , l _ { g , 1 } ,$ , and $l _ { g , 2 }$ Lipschitz continuous, respectively.

Theorem 2. Under Assumptions $\jmath$ and $_ 3$ , if the iteration numbers $K$ and $N$ satisfy $K \geq \mathcal { O } ( \kappa )$ and $\mathrm { ~ \bar { ~ } { ~ N ~ } ~ } \geq \mathcal { O } \left( \sqrt { \kappa } \right)$ with $\begin{array} { r } { \kappa = \frac { l _ { g } } { \mu } } \end{array}$ , then the iterates $\theta _ { t }$ generated by Algorithm $\boldsymbol { { \mathit { 1 } } }$ with deterministic gradients satisfy

$$
{ \frac { 1 } { T + 1 } } { \sum _ { t = 0 } ^ { T } } \left\| \nabla F ( \theta _ { t } ) \right\| ^ { 2 } \leq \mathcal { O } \left( { \frac { 1 } { T } } \right) .
$$

Theorem 2 demonstrates that when we consider deterministic meta-learning, Algorithm 1 converges to an exact solution to problem (1) with a sublinear convergence rate. To achieve an $\epsilon$ -solution, Algorithm 1 with deterministic gradients requires at most $\mathcal { O } ( \bar { \kappa } \epsilon ^ { - 1 } )$ gradient evaluations in both $\phi$ and $\theta$ , which is consistent with the convergence results for the iterative-differentiation-based meta-learning algorithm in Ji, Yang, and Liang (2020).

45670 CIFAR-FS FC100 miniImageNet tieredImageNet 45 WW 60 WMM 60 MWMVW 40 小 WW YMM WMMIW 50 50 AA 35 Algorithm 1 Algorithm 1 40 Algorithm 1 40 Algorithm 1 ITD-BiO 30 ITD-BiO ITD-BiO ITD-BiO   
300 MAML MAML 30 MAML 30 MAML ANIL 25 ANIL W ANIL ANIL 20 20 500 1000 1500 2000 0 500 1000 1500 2000 0 500 1000 1500 2000 0 500 1000 1500 2000 Iteration Iteration Iteration Iteration

<html><body><table><tr><td>Algorithm</td><td>Backbonea</td><td>CIFAR-FS Accuracy(%)</td><td>FC100 Accuracy(%)</td><td>Backbone</td><td>miniImageNet Accuracy(%)</td><td>tieredImageNet Accuracy(%)</td></tr><tr><td>MAML</td><td>64-64-64-64</td><td>54.85 ± 1.23</td><td>47.50 ± 1.06</td><td>32-32-32-32</td><td>43.33 ± 0.95</td><td>48.37 ± 1.17</td></tr><tr><td>ANIL</td><td>64-64-64-64</td><td>63.52 ± 1.31</td><td>47.70 ± 0.95</td><td>32-32-32-32</td><td>57.80 ± 1.14</td><td>58.16 ± 0.94</td></tr><tr><td>ITD-BiO</td><td>64-64-64-64</td><td>63.69 ± 1.10</td><td>47.88 ± 0.74</td><td>32-32-32-32</td><td>55.58 ± 1.31</td><td>60.78 ± 0.10</td></tr><tr><td>Algorithm 1(ours)</td><td>64-64-64-64</td><td>64.24 ± 1.41</td><td>48.52 ± 1.13</td><td>32-32-32-32</td><td>59.58 ± 1.08</td><td>61.97 ± 0.86</td></tr></table></body></html>

a The “Backbone” represents the remaining layers in the model except for the last linear layer. The numbers in “64-64-64-64” and “32-32- 32-32” represent the number of filters in each convolutional layer in the backbone.

Table 1: Test accuracies by fine-tuning the model learned by Algorithm 1, MAML, ANIL, and ITD-BiO using 20-step gradien descent on the “CIFAR-FS”, “FC100”, “miniImageNet”, “tieredImageNet” datasets, respectively.

# 5 Experiments

In this section, we evaluate the performance of our proposed Algorithm 1 by using a few-shot image classification problem on the “CIFAR-FS” dataset (Bertinetto et al. 2018), the “FC100” dataset (Oreshkin, Rodr´ıguez Lo´pez, and Lacoste 2018), the “miniImageNet” dataset (Vinyals et al. 2016), and the “tieredImageNet” dataset (Ren et al. 2018), respectively. In all experiments, we compare our Algorithm 1 with other optimization-based meta-learning approaches, including the bilevel optimization-based algorithm with iterative differentiation (ITD-BiO) in Ji, Yang, and Liang (2020), MAML in Finn, Abbeel, and Levine (2017), and ANIL in Raghu et al. (2019). Due to space limitations, we leave the experimental setup in neural network training in Appendix C.2.

# 5.1 Evaluation on Meta-Learning Accuracy

Following Finn, Abbeel, and Levine (2017), we consider a meta-learning problem with $| B |$ tasks $\{ \mathcal { T } _ { i } , i = 1 , \cdots , | \boldsymbol { B } | \}$ in each iteration. Each task $\mathcal { T } _ { i }$ has a loss function $\mathcal { L } ( \theta , \phi _ { i } ; \xi )$ over each data sample $\xi$ , where $\theta$ represents the parameters of an embedding model shared by all tasks, and $\phi _ { i }$ represents the task-specific parameters in a given task $\mathcal { T } _ { i }$ . The goal of our meta-learning problem (1) is to find the optimal parameters $\theta ^ { * }$ that benefit all tasks, and based on $\theta ^ { * }$ , the model can quickly adapt its own parameters $\phi _ { i }$ to any new task $\mathcal { T } _ { i }$ using only a few data points and training iterations.

In this experiment, all comparative meta-learning algorithms are trained using a four-convolutional-layer convolutional neural network (CNN) architecture given in Ravi and Larochelle (2016). The network is trained using a crossentropy loss function in PyTorch. For our Algorithm 1 and ITD-BiO in Ji, Yang, and Liang (2020), $\phi _ { i }$ corresponds to the parameters of the last linear layer of the CNN model and $\theta$ represents the parameters of the remaining layers. This setup ensures that the lower-level objective function $g _ { i } ( \theta , \phi )$ is strongly convex with respect to $\phi$ , while the upper-level objective function $F ( \theta )$ is generally nonconvex with respect to $\theta$ . For MAML and ANIL, we employ their default initialization-based learning paradigms given in Finn, Abbeel, and Levine (2017) and Raghu et al. (2019), respectively. All algorithms are executed in $| B | = 3 2$ batches of tasks over 2, 000 iterations with each task involving a training dataset $\mathcal { D } _ { i } ^ { \mathrm { t r a } }$ and a validation dataset $\mathcal { D } _ { i } ^ { \mathrm { v a l } }$ , both designed for 5-way classification with 5 shots for each class. For the experiments conducted on the “CIFAR-FS” and “FC100” datasets, we set the step sizes to $\lambda _ { \theta } = 0 . 0 0 0 1$ and $\lambda _ { \phi } = 0 . 1$ ; and for the experiments conducted on the “miniImageNet” and “tieredImageNet” datasets, we set $\lambda _ { \theta } ~ = ~ 0 . 0 0 1$ and $\lambda _ { \phi } = 0 . 0 5$ . The iteration numbers $K$ and $N$ are configured as $K = N = 2 0$ . In our comparison, the near-optimal stepsizes and inner-loop iteration numbers are applied to ITDBiO, MAML and ANIL, such that doubling them leads to divergent behavior. Moreover, for ITD-BiO, MAML, and ANIL, we used Adam (Kingma and Ba 2017) for upperlevel optimization and gradient descent for lower-level optimization, following the guidance from Ji, Yang, and Liang (2020), Finn, Abbeel, and Levine (2017), and Raghu et al. (2019), respectively.

Note that evaluating the performance of a meta-learning algorithm involves two stages: learning-accuracy evaluation

miniImageNet/tieredImageNet CIFAR-FS/FC100 6000 5894.1 866.6 345000 IATlgDo-rBitihOm 1 4521.7 800 IATlgDo-rBitihOm 1 665.1 ANIL 3149.3 3490.8 3490.9 ANIL 463.6 491.7 491.8 2693.1 2693.2 400 377.5 377.5 1776.9 1892.7 1892.7 262 263.3 263.3 1094.1 1094.1 200 149.1 149.1 495.8 496.6 497.4 498.1 77.5 77.7 78 78.2 0 0 5 10 15 20 5 10 15 20 Number of inner-loop iterations K Number of inner-loop iterations K

Figure 2: Memory costs of Algorithm 1, MAML, ANIL, and ITD-BiO under different numbers of inner-loop iterations $K$ using the “CIFAR-FS”, “FC100”, “miniImageNet”, “tieredImageNet” datasets, respectively.   

<html><body><table><tr><td>Algorithm</td><td>CIFAR-FS wallclock time(s)a</td><td>FC100 wallclock time(s)</td><td>miniImageNet wallclock time(s)</td><td>tieredImageNet wallclock time(s)</td></tr><tr><td>MAML</td><td>_b</td><td>2638.45</td><td>10065.83</td><td>9453.75</td></tr><tr><td>ANIL</td><td>1130.16</td><td>879.33</td><td>1639.85</td><td>1417.64</td></tr><tr><td>ITD-BiO</td><td>1740.76</td><td>1607.74</td><td>3031.51</td><td>2059.43</td></tr><tr><td>Algorithm1 (ours)</td><td>832.40</td><td>621.67</td><td>739.62</td><td>693.46</td></tr></table></body></html>

a The wallclock time represents the time spent to achieve a certain validation accuracy of $0 . 9 0 \times A c c _ { \mathrm { m a x } }$ , where $A c c _ { \mathrm { m a x } }$ denotes the highest validation accuracy achieved among all comparison algorithms. b Since MAML can never reach $0 . 9 0 \times A c c _ { \mathrm { m a x } }$ , we have not provided its wallclock time here.

Table 2: Comparison with MAML, ANIL, and ITD-BiO in terms of wallclock time using the “CIFAR-FS”, “FC100”, “miniImageNet”, “tieredImageNet” datasets, respectively.

in the meta-learning stage and generalization-ability evaluation in the meta-test stage. The learning accuracy in the meta-learning stage is shown in Figure 1. These results confirm the advantage of the proposed algorithm over existing meta-learning algorithms on various datasets. To show that Algorithm 1’s generalization ability to a new task which is comprised of some images unseen in the meta-learning stage, we list the test accuracies by fine-tuning the model learned by Algorithm 1, MAML, ANIL, and ITD-BiO using a 20-step gradient descent. The results are summarized in Table 1, which shows that our proposed Algorithm 1 has better generalization ability than existing counterparts.

# 5.2 Evaluation on Memory Costs

In optimization-based meta-learning algorithms, a significant portion of memory costs come from storing historical lower-level parameters/gradients for hypergradient estimation (see Eq.(3)). To show that Algorithm 1 can ensure nearly time-invariant memory usage over iterations, we compare the memory needed under different numbers of inner-loop iterations. The results in Figure 2 clearly show that the memory costs of Algorithm 1 remain consistently stable regardless of the number of inner-loop iterations. In contrast, the memory costs for MAML, ANIL, and ITD-BiO increase with the number of inner-loop iterations. Moreover, even when the number of inner-loop iterations is set to 5, the memory consumption of our algorithm is more than $5 0 \%$ lower compared to MAML, ANIL, and ITD-BiO.

# 5.3 Evaluation on Wallclock Time

To compare the convergence rate, i.e., computational complexity, of our Algorithm 1 with existing counterparts, we summarize their wallclock time to achieve a certain $0 . 9 0 \times$ $A c c _ { \operatorname* { m a x } }$ validation accuracy in Table 2, where $A c c _ { \operatorname* { m a x } }$ represents the highest validation accuracy achieved among all comparison algorithms. The experimental results validate that our algorithm is still faster than MAML, ANIL, and ITD-BiO in wallclock time even without using any historical lower-level parameters/gradients.

# 6 Conclusions

In this paper, we propose a meta-learning algorithm that can simultaneously reduce memory costs and ensure sublinear convergence. More specifically, our proposed approach avoids using any historical lower-level parameters/- gradients, and hence, ensures nearly invariant memory costs over iterations. This is in sharp contrast to most existing iterative-differentiation-based algorithms for meta-learning, which use historical lower-level parameters/gradients to ensure learning accuracy, implying growing memory costs when the inner-loop iteration number increases. In addition, we systematically characterize the convergence performance of our algorithm for both stochastic and deterministic meta-learning, and quantify the computational complexities for gradient evaluations on both upper-level and lowerlevel parameters. Experimental results on various benchmark datasets in few-shot meta-learning confirm the advantages of the proposed approach over existing counterparts.