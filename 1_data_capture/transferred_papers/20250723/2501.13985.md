# Pilot: Building the Federated Multimodal Instruction Tuning Framework

Baochen Xiong1,2,3, Xiaoshan $\mathbf { Y a n g } ^ { 1 , 2 , 3 }$ , Yaguang $\mathbf { S o n g ^ { 2 } }$ , Yaowei Wang2,4, Changsheng $\mathbf { X } \mathbf { u } ^ { 1 , 2 , 3 ^ { * } }$

1MAIS, Institute of Automation, Chinese Academy of Sciences 2Pengcheng Laboratory 3School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS) 4Harbin Institute of Technology (Shenzhen) xiongbaochen2022 $@$ ia.ac.cn, xiaoshan.yang, csxu @nlpr.ia.ac.cn, songyg01, wangyw @pcl.ac.cn

# Abstract

In this paper, we explore a novel federated multimodal instruction tuning task(FedMIT), which is significant for collaboratively fine-tuning MLLMs on different types of multimodal instruction data on distributed devices. To solve the new task, we propose a federated multimodal instruction tuning framework(Pilot). Our framework integrates two stages of “adapter on adapter” into the connector of the vision encoder and the LLM. In stage 1, we extract task-specific features and client-specific features from visual information. In stage 2, we build the cross-task Mixture-of-Adapters(CT-MoA) module to perform cross-task interaction. Each client can not only capture personalized information of local data and learn taskrelated multimodal information, but also learn general knowledge from other tasks. In addition, we introduce an adaptive parameter aggregation strategy for text training parameters, which optimizes parameter aggregation by calculating weights based on the euclidean distance between parameters, so that parameter aggregation can benefit from positive effects to the greatest extent while effectively reducing negative effects. Our framework can collaboratively exploit distributed data from different local clients to learn cross-task knowledge without being affected by the task heterogeneity during instruction tuning. The effectiveness of our method is verified in two different cross-task scenarios.

# Introduction

The emergence of Multimodal Large Language Models (MLLMs) (Li et al. 2023; Liu et al. 2024; Driess et al. 2023; Dai, Li et al. 2023) has significantly advanced the field of artificial intelligence. MLLMs have shown excellent ability in processing and integrating various modalities information (especially text and image), and has achieved remarkable performance in tasks such as text generation, machine translation and question answering. Enhancing the zero-shot generalization ability of MLLMs on novel multimodal tasks is a key goal driving its development. Multimodal instruction tuning has been shown to be highly effective in improving the zero-shot generalization ability of models to unseen multimodal problems (Xu et al. 2022; Ye et al. 2023; Sun et al. 2024; Chen et al. 2024a; Xiao et al. 2024b).

↑ Client 1 Client 2 Client K   
Traditional FedIT task: Brainstorming Task Classification Task   
Input: List 6 different types Input:Identify which Input: When was Tomoaki   
of rain in Seattle instrument is string or Komorida born? Context:   
Output: 1. Mist 2. Drizzle 3. percussion. ‘Komorida was born in...   
Sleet 4.Downpour... Output:Gudok is string.. Output: He was born...   
FedMIT task: GQATask CaptionCOCOTask Grounding RefCOCO Task Input:Is the sky Input: Summary of Input:the coordinates dark? what you see. of yellowboat. Output: Yes, the Output: A sea plane Output: [0.011,0.15, sky is dark. in the water with. 0.358,0.283]. Sending local model Sending back global model (a)   
0. A\W\\W WWWW AIIIIY   
Brainstorming Closed QA Classification GQA Caption Grounding (b) COCo RefCOCO

Current instruction tuning methods are usually implemented by centralized learning paradigm, where one central party collects a substantial amount of data to train the model, which may lead to privacy security issues. Federated Learning (FL) is currently the primary framework for distributed training of models while maintaining data security and privacy. McMahan et al. (2017) proposed the first federated learning algorithm, FedAvg, which aggregates model gradients from local clients to a central server without data sharing. Afterwards, FL is also successfully used for instruction tuning of pre-trained models (Zhang et al. 2024b; Ye et al. 2024), called Federated Instruction Tuning (FedIT). These frameworks can effectively leverage locally private instruction-following data.

Although existing FedIT approaches have made important progress, they only consider the case where different clients perform different instruction-based natural language processing(NLP) tasks, and there is little exploration in the multimodal field. The integration of vision and language plays a crucial role in machine perception and understanding of the world. Language is the basis of cognitive processing, while vision provides the necessary sensory information (Xu et al. 2024; Xiong et al. 2024). In this context, we attempt to transfer the success of FedIT in multimodal tasks. Therefore, in this paper, we explore a novel Federated Multimodal Instruction Tuning task (FedMIT).

Compared with traditional FedIT, FedMIT focuses on the client containing different multimodal instruction tuning tasks (e.g., visual question answering and image captioning), as shown in Figure 1(a). In our preliminary study, we first distributed different task instruction data to each client, including “Caption (e.g., COCO (Lin et al. 2014))” for image description, “VQA (e.g., GQA (Hudson 2019))” for visual question answering, and “Grounding (e.g., RefCOCO (Kazemzadeh et al. 2014))” for visual localization. Then, we directly apply the representative method Shepherd (Zhang et al. 2024b) in the FedIT task to the FedMIT task. Figure 1(b) shows the relative scores of Shepherd on two tasks, where the relative scores are the comparison scores between the results of the Shepherd method and the local training results. The results show that the traditional FedIT method does not perform well on the FedMIT task. Since the diversity of multimodal tasks greatly increases the heterogeneity between clients, we believe that traditional FedIT methods cannot adequately address this kind of task heterogeneity.

Compared with the traditional FedIT task, each client in FedMIT task not only needs to capture the personalized information of local data and task-related multimodal information, but also needs to be able to accommodate the differences between different tasks to avoid parameter conflicts. This requires the model to maintain its understanding of the task and local data while also being able to learn general knowledge from other tasks to improve model performance and cross-task ability.

Inspired by these observations, we introduce the Federated Multimodal Instruction Tuning framework: Pilot. Our framework integrates two stages of “adapter on adapter” into the connector of the vision encoder and the LLM, and adopts an adaptive parameter aggregation strategy for the training parameters of the LLM. First we introduce the two-stage training of the client. Stage 1: Task-specific feature mining. We hope to extract client-specific and task-specific features from the client’s visual information. We propose taskspecific adapter to extract task-specific visual features that is only important for one task, and client-specific adapter to extract specific visual features of the client’s unique data distribution. To encourage the client-specific adapter to produce features that are more refined than the task-specific adapter, a difference loss is used to ensure the orthogonality of their output features. Stage II: Cross-task visual interaction. We integrate the Cross-task Mixture-of-Adapters(CTMoA) module with the task-specific adapter. By interacting with the server, each adapter in CT-MoA is initialized from the task-specific adapter of the corresponding task. We hope that the CT-MOA module can learn general knowledge from other tasks to improve model performance and cross-task capabilities. Therefore, we added cross-task adapters to the task-specific adapters of other tasks in CT-MOA, where cross-task adapter on other taskspecific adapter. Cross-task adapter aims to extract crosstask collaboration visual features. In addition, the CT-MOA module also contains a router that selects adapters during the stage II with auxiliary losses on the router to maintain a balanced loading of adapters. Considering the computation and communication requirements, we adopt text-adapter-based parameter-efficient tuning techniques to train LLM to reduce the amount of trainable parameters on each device. For the server side, it collects all client visual and text training parameters. For visual training parameters, we adopt the taskaware aggregation strategy. For text training parameters, we introduce adaptive parameter aggregation, which optimizes parameter aggregation by calculating weights based on the euclidean distance between parameters, so that parameter aggregation can benefit from positive effects to the greatest extent while effectively reducing negative effects. Combining federated optimization with two-stage local updates, our framework can collaboratively exploit distributed data from different local clients to learn cross-task knowledge without being affected by the task heterogeneity during instruction tuning.

Our contributions are summarized as follows: (1) We propose to explore a new task of federated multimodal instruction tuning, which is significant for collaboratively finetuning MLLMs on different types of multimodal instruction data on distributed devices. (2) To solve the new task, we propose a Federated Multimodal Instruction Tuning framework(Pilot). Our framework builds two stages “adapter on adapter” strategy. In stage 1, the model extracts clientspecific and task-specific features, and in stage 2, we construct CT-MOA modules to learn cross-task interactions. We adopt an adaptive aggregation strategy for the LLM training parameters. With the above approach, our method can learn cross-task knowledge without being affected by task heterogeneity during instruction tuning. (3) We verify the effectiveness of Pilot on the state-of-the-art LLaVA (Liu et al. 2024) in two different cross-task scenarios.

# Related Work

# Federated Learning

The earliest FL algorithm is FedAvg (McMahan et al. 2017), which builds the global model by averaging the local updates obtained by Stochastic Gradient Descent (SGD) (Gorbunov, Hanzely, and Richta´rik 2021). However, FedAvg inevitably experiences performance degradation on non-IID data (Yang et al. 2024b; Xiong et al. 2023). To deal with this problem, FedProx (Li et al. 2020) adds a proximal term to local targets to minimize the distance between the global model and the local model for non-IID data. PerAvg (Fallah, Mokhtari, and Ozdaglar 2020) uses popular meta-learning framework MAML (Finn, Abbeel, and Levine 2017), which allows each client to quickly adapt to local data by finding a suitable initialization. TAPPFL (Arevalo et al. 2024) designs a task-agnostic and provably privacy-preserving federated learning framework. FedTGP (Zhang et al. 2024a) uses adaptive-margin-enhanced contrastive learning to learn trainable global prototypes on the server to solve the model heterogeneity problem. FedLPS (Jia et al. 2024) uses an

Client 1: Caption COCO Task Client 1 Server Client 1: Caption COCO Task TextLLM ATexter Adapter Avg LLM Texter Task-specific   
□□□□□□□□□□□ Adapter Vg □□□□□□□ + → Client 2 电 →   
Client-specific Text- Avg Client-specific/ Adapter Word Adapter Adapter Word T . Clent estapter 三 ↑ Adapter Task-specific 宫日 Adapter Adaptive Text-adapter Aggregation Stage 1 Communication Stage 2

adaptive channel model pruning method to enable clients to participate in federated learning training with heterogeneous tasks. However, the above methods only consider the heterogeneity on single-modal clients. In contrast, our method focuses on addressing the heterogeneity of cross-task instruction data in the MLLM-based federated multimodal instruction tuning task.

# Multimodal Instruction Tuning

Instruction tuning (Wei et al. 2021; Xiao et al. 2024a) significantly improves the generalization ability of large language models to unseen tasks based on natural language instructions. With the advent of MLLMs, the scope of instruction tuning has been expanded to include multimodal and visual tasks. MiniGPT-4 (Zhu et al. 2023) and LLaVA (Liu et al. 2024) keep the visual encoder frozen and adjust the language model, extending instruction tuning to multimodal. InstructBLIP (Dai, Li et al. 2023) enhances zeroshot capabilities by performing instruction tuning on multiple datasets. Shikra (Chen et al. 2023) extend MLLMs to visual grounded tasks using instructions with bounding box coordinates. mPLUG-Owl (Ye et al. 2023) fine-tunes the visual and text encoders in two stages. EProj (He et al. 2023) address the catastrophic forgetting of the model in continuous instruction tuning. MixLoRA (Shen et al. 2024) uses conditional mixed LoRA to address the task interference caused by LoRA during multimodal instruction tuning. Although these studies demonstrate excellent zero-shot capabilities, they still suffer from the risk of privacy leakage.

# Federated Instruction Tuning

There has been some research in federated learning based on Parameter-Efficient Fine-tuning(PEFT) (Shi et al. 2023; Su et al. 2024; Chen and Zhang 2024). FedCLIP (Lu et al. 2023) and pFedprompt (Guo et al. 2023) finetune the CLIP model (Radford et al. 2021) by adapting and conveying only small number of learnable prompts. FedDISC (Yang et al. 2024a) first integrated the pre-trained diffusion model (Dhariwal and Nichol 2021) into the FL framework. FedDAT (Chen et al. 2024b)utilizes a dual-adapter teacher architecture to address the efficient fine-tuning of parameters of multimodal base models in heterogeneous federated learning. However, the application of instruction tuning in FL has not been fully explored. Federated instruction tuning (Zhang et al. 2024b) provides a simple and effective method for supporting distributed client privacy-preserving instruction tuning through the FL protocol. Fatellm (Fan et al. 2023) and OpenFedLLM (Ye et al. 2024) built a concise and easy-to-research framework for fine-tuning federal instructions. Recently, FedDPA (Yang et al. 2024c) explored the problem of heterogeneity in NLP tasks. In our work, we attempt to address the unexplored task of multimodal instruction tuning for MLLMs in federated learning.

# Methodology Problem Definition

We assume that there is a set of multimodal instruction tuning data $\mathcal { D } \ = \ \{ ( \mathcal { D } _ { k } , t _ { k } ) \} _ { k = 1 } ^ { K }$ from $K$ clients, where $\mathcal { D } _ { k } = \{ X _ { i , k } ^ { v } , \mathbf { x } _ { i , k } ^ { i n s } , \mathbf { y } _ { i , k } ^ { a n s } \} _ { i = 1 } ^ { n _ { k } }$ is the set of $n _ { k }$ data pairs on the $k$ -th client. The total number of data pairs is $\begin{array} { r } { n = \sum _ { k = 1 } ^ { K } n _ { k } } \end{array}$ $X ^ { v } , { \bf x } ^ { i n s }$ and $\mathbf { y } ^ { a n s }$ indicate the image, instruct on tokens and answer tokens, respectively. $t _ { k } \in \{ 1 , . . . , T \}$ denotes the task type of the $k$ -th client. $T$ is the total number of tasks and $T \leqslant K$ .

In the FedMIT task, all clients obtain the pre-trained MLLM from the server. Generally, MLLMs contain a visual encoder $f$ , a connector $\psi$ , and LLM $L$ . Specifically, for a given input image $X ^ { v }$ , the visual encoder extracts visual features $H ^ { v } = f ( X ^ { v } )$ . The connector is used to align the visual encoder with the LLM. Connector transforms $H ^ { v }$ into a language embedding tokens $\mathbf { x } ^ { i m g } \in \mathbb { R } ^ { N \times C }$ , effectively facilitating the integration of multimodal information within the LLM framework, where $N$ is the number of tokens and $C$ is the hidden size.

$$
\begin{array} { r } { \mathbf { x } ^ { i m g } = \psi ( H ^ { v } ) , \mathrm { w i t h } H ^ { v } = f ( X ^ { v } ) . } \end{array}
$$

Finally, we input $\mathbf { x } ^ { i m g }$ and $\mathbf { x } ^ { i n s }$ into the LLM to generate response.

The FedMIT task aims to perform instruction tuning in a distributed manner, where each client can increment the local model by leveraging cross-task knowledge learned on other clients without the need for centralized data collection. The overall optimization goal is defined as follows:

$$
\sum _ { k = 1 } ^ { K } \frac { n _ { k } } { n } \sum _ { i = 1 } ^ { n _ { k } } \frac { 1 } { n _ { k } } \sum _ { j = 1 } ^ { L } - \log p _ { \theta } \left( \mathbf { y } _ { i , k } ^ { j , a n s } \mid \mathbf { x } _ { i , k } ^ { i m g } , \mathbf { x } _ { i , k } ^ { i n s } , \mathbf { y } _ { i , k } ^ { < j , a n s } \right)
$$

where $L$ represents the answer length, $\theta$ is the trainable parameters. y ij,kans indicates the j-th answer token and yi<,kj, $\mathbf { y } _ { i , k } ^ { < j , a n s }$ indicates all answer tokens before the index $j$ .

# Federated Multimodal Instruction Tuning Framework

FedMIT task has greater heterogeneity between tasks, and traditional FedIT methods cannot be used directly to solve the problem. As shown in Figure 2, we propose the Federated Multimodal Instruction Tuning framework (Pilot) to address the task heterogeneity between clients. In our framework, we integrate a two-stage “adapter on adapter” method into the connector of the visual encoder and LLM. In stage 1, we extract task-specific features and client-specific features from visual information. Through federated aggregation, in stage 2, we build a CT-MOA module to perform cross-task interaction. We hope that each client not only needs to capture personalized information from local data and learn taskrelated multimodal information, but also needs to be able to learn general knowledge from other tasks to improve model performance and cross-task capabilities.

Stage 1: Task-specific Feature Mining. At stage 1, we hope to extract client-specific and task-specific features from the client’s visual information. We propose task-specific adapter $\psi ^ { t }$ to extract task-specific visual features that is only important for one task, and client-specific adapter $\psi ^ { s }$ to extract specific visual features of the client’s unique data distribution. We define these two adapters as two-layer of perceptrons. Finally, the image tokens represent: $\mathbf { x } ^ { i m g } = \mathbf { x } ^ { \dot { t } } + \mathbf { \bar { x } } ^ { \dot { s } }$ , where $\mathbf { x } ^ { t } = \mathsf { \bar { \psi } } ^ { t } ( H ^ { v } ) \in \mathbb { R } ^ { N \times C }$ , $\mathbf { x } ^ { s } \bar { } = \psi ^ { s } ( H ^ { v } ) \in \mathbb { R } ^ { N \times C }$ .

To encourage the client-specific adapter to produce features that are more refined than the task-specific adapter, a difference loss is used to ensure the orthogonality of their output features. Inspired by the domain separation network (Bousmalis et al. 2016), we adopt a soft subspace orthogonality constraint:

$$
\mathcal { L } _ { d } = \| \mathbf { x } ^ { t ^ { \top } } \mathbf { x } ^ { s } \| _ { F } ^ { 2 } ,
$$

where $\| \cdot \| _ { F }$ is Frobenius norm. Hence, the Stage 1 total loss is

$$
\mathcal { L } = \mathcal { L } _ { c e } + \lambda _ { 0 } \mathcal { L } _ { d } ,
$$

where $\mathcal { L } _ { c e }$ represents the language modeling loss, which computes the cross-entropy of next-token predictions. $\lambda _ { 0 }$ denote coefficients for difference loss. After stage 1, each client sends the LLM training parameters text-adapter $\Theta _ { l }$ and task-specific adapter $\Theta _ { a }$ parameters to the server.

Stage 2: Cross-task Visual Interaction. At stage 2, the client obtains the updated task-specific adapters for $T$ tasks from the server. In other words, each client obtains $( T { - } 1 )$ task-specific adapters of other tasks in addition to its local task-specific adapter. To learn general knowledge from other tasks to improve model performance and crosstask capabilities, we integrate the local task-specific adapter with other task-specific adapters to construct a Cross-task Mixture-of-Adapters (CT-MoA) module, as shown in Figure 3. In the CT-MoA module, we assume that the local taskspecific adapter index is 1 $( \psi _ { 1 } ^ { t } )$ , and there are $T$ adapters in total. However, directly loading task-specific adapters of distinct tasks may cause the model of the current task to be unable to fully utilize these adapters due to task heterogeneity. Therefore, we added cross-task adapters to the task-specific adapters of other tasks in CT-MOA. For taskspecific adapters $\mathbf { \psi } _ { { i } } ^ { t } , i = 2 , . . . , T$ , we add cross-task adapter $\psi ^ { c }$ on it to alleviate the discrepancy between other taskspecific adapters and the local task-specific adapter due to task heterogeneity. Cross-task adapter aims to extract crosstask collaboration knowledge. It has the same structure as the task-specific adapter and is initialized by the local taskspecific adapter $\psi _ { 1 } ^ { t }$ parameters. In addition, the CT-MOA module includes a router for predicting the probability of selecting and activating each adapter from $T$ total task-specific adapters. The router network $( \phi )$ has a linear layer for calculating the normalized weight matrix using $H ^ { v }$ for voting, and producting $\mathcal { P }$ :

![](images/c46948e393c54fed3e328e7fa1fbabbf51ebad3a4c9be71f7115a1edb3281ec9.jpg)  
Figure 3: Cross-task Mixture-of-Adapters (CT-MoA).

$$
\mathcal { P } = \mathrm { S o f t m a x } ( \phi ( H ^ { v } ) ) \in \mathbb { R } ^ { T } .
$$

Finally, the output of CT-MOA module is expressed as:

$$
{ \bf x } ^ { t } = \mathcal { P } [ 1 ] \psi _ { 1 } ^ { t } ( H ^ { v } ) + \sum _ { i = 2 } ^ { T } \mathcal { P } [ i ] \left( \psi _ { i } ^ { t } ( H ^ { v } ) + \psi _ { i } ^ { c } ( H ^ { v } ) \right) .
$$

Following (Zoph et al. 2022), we adopt an auxiliary losses based on the language modeling cross-entropy loss to maintain a load balance between task-specific adapters in the Cross-task MoA module. The auxiliary losses comprise load balancing loss and router $\mathbf { z }$ -loss. The load balancing loss is defined as:

$$
\mathcal { L } _ { b } = T \sum _ { i = 1 } ^ { T } \mathcal { D } _ { i } \mathcal { R } _ { i } ,
$$

where $\begin{array} { r } { \mathcal D _ { i } \ = \ \frac { 1 } { N } \sum _ { j = 1 } ^ { N } 1 \left\{ \mathrm { a r g m a x } ( \phi ( H ^ { v } [ j ] ) ) = i \right\} } \end{array}$ represents the proportion of tokens distributed to adapter $i , 1 \{ \cdot \}$ is an indicator function. While $\begin{array} { r } { \mathcal { R } _ { i } = \frac { 1 } { N } \sum _ { j = 1 } ^ { N } \phi _ { i } ( H ^ { v } [ j ] ) } \end{array}$ denotes the proportion of the probability assigned to adapter $i$ . $\phi _ { i } ( H ^ { v } [ j ] )$ is the probability of routing token $j$ to adapter $i$ . The router $\mathbf { z }$ -loss is defined as:

$$
\mathcal { L } _ { z } = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \left( \log \sum _ { j = 1 } ^ { T } e ^ { g _ { j } ^ { ( i ) } } \right) ^ { 2 } ,
$$

where $g ~ \in ~ \mathbb { R } ^ { N \times T }$ are the logits obtained by the router. Hence, the Stage 2 total loss is:

$$
\begin{array} { r } { \mathcal { L } = \mathcal { L } _ { c e } + \lambda _ { 1 } \mathcal { L } _ { b } + \lambda _ { 2 } \mathcal { L } _ { z } , } \end{array}
$$

$\lambda _ { 1 }$ and $\lambda _ { 2 }$ denote coefficients for load balancing loss and router $\textbf { \em z }$ -loss. During stage 2 training, the model only updates the local task-specific adapter, the cross-task adapter, and the text-adapter.

# Federated Optimization

The proposed method is collaboratively optimized via local two stages instruction tuning and global aggregation. We train the Pilot with two alternate steps of local update and global aggregation for $R$ rounds. In each round, the client receives the global model and updates the local model $E$ epochs on the local data. In the first round, the server sends the base MLLM model to each client. Each client performs local instruction tuning of stage 1, then send the taskspecific adapter and text-adapter parameters to the server. After global aggregation, the server sends $T$ task-specific adapters and text-adapter parameters back to the client. Each client updates the model architecture and performs local instruction tuning of stage 2, and finally send the local taskspecific adapter and text-adapter parameters to the server.

Task-aware Visual-adapter Aggregation. After the server collects the task-specific adapter parameters of all clients, we adopt a task-aware aggregation method.

Task-specific adapter mainly learns task-specific visual features and has the general understanding ability of this task. Therefore, the parameters learned in one client can also be shared by clients of the same task. We use weighted average to aggregate the task-specific adapter parameters $\Theta _ { a }$ of the same task:

$$
\bar { \Theta } _ { a } ^ { t } = \sum _ { k \in \mathcal { K } _ { t } } \frac { n _ { k } } { k ^ { \prime } \in \mathcal { K } _ { t } } \Theta _ { a , k } ^ { t } , t = 1 , . . . , T ,
$$

where $\textstyle { \mathcal { K } } _ { t }$ is a set of clients with task type $t$ . Finally, we obtain $T$ number of task-specific adapters and send them to each client.

Adaptive Text-adapter Aggregation. For all collected text-adapter parameters, compared with the fully weighted aggregation method used in the traditional FedIT method, we expect that the text-adapter parameters of each client can benefit from the positive impact to the greatest extent during the aggregation process, while effectively reducing the negative impact. Therefore, we propose an adaptive textadapter aggregation method. We first calculate the Euclidean distance between each text-adapter parameter and the other $( K { - } 1 )$ text-adapter parameters. Then select the nearest $M$ points for weighted aggregation based on the distance. For example, text-adapter parameters $\Theta _ { l , k }$ for client $k$ :

$$
d _ { k , i } = E ( \Theta _ { l , k } , \Theta _ { l , i } ) , { \mathrm { ~ f o r ~ } } i \in \{ 1 , 2 , . . . , K \} \backslash \{ k \} ,
$$

where $E ( \cdot )$ is the Euclidean distance formula. We obtain the distance $D _ { k } \in \mathbb { R } ^ { K - 1 }$ between the $\Theta _ { l , k }$ and other textadapter parameters, and then we select the Top- $M$ closest parameters for weighted aggregation:

$$
\bar { \Theta } _ { l , k } = \frac { n _ { k } } { n _ { k } + \sum _ { k ^ { \prime } \in \mathcal { K } _ { m } } n _ { k ^ { \prime } } } \Theta _ { l , k } + \sum _ { k ^ { \prime } \in \mathcal { K } _ { m } } \frac { n _ { k ^ { \prime } } \cdot w _ { k ^ { \prime } } } { n _ { k } + \sum _ { k ^ { \prime } \in \mathcal { K } _ { m } } n _ { k ^ { \prime } } } \Theta _ { l , k ^ { \prime } } ,
$$

where $\begin{array} { r } { w _ { k ^ { \prime } } = \frac { 1 / d _ { k , k ^ { \prime } } } { \sum _ { k ^ { \prime } \in \mathcal { K } _ { m } } 1 / d _ { k , k ^ { \prime } } } } \end{array}$ , $\kappa _ { { \scriptscriptstyle m } }$ is the set of $M$ clients closest to parameter $\Theta _ { l , k }$ .

# Experiment

# Experimental Setups

Now, we introduce how to construct the federated multimodal instruction tuning task scenario. To ensure the diversity of instruction tuning datasets, we collect various publicly available and commonly used visual-language datasets. These instruction tuning datasets cover a wide range of tasks, including knowledge-based image question answering, image question answering reading comprehension, and optical character recognition VQA. The selected datasets include ScienceQA (Lu et al. 2022), GQA (Hudson 2019), and OCRVQA (Mishra et al. 2019). However, we observe that these datasets are limited to traditional QA tasks in visuallanguage tasks. Therefore, to enrich the diversity of tasks, we introduce the image caption dataset COCO (Lin et al. 2014) for image description task and the grounding dataset RefCOCO (Kazemzadeh et al. 2014) for visual localization task. For all the above datasets, we construct two different federated instruction tuning scenarios. FL-oriented visual understanding scenario: We use the GQA, COCO, and RefCOCO datasets. We randomly divide each dataset into 3 subsets, and each subset is regarded as a client. Then, we obtain a FedMIT task scenario with 9 clients and 3 different visual understanding tasks(9-client, 3-task). FL-oriented general VQA scenario: We use the ScienceQA, GQA, and OCRVQA datasets. Similar to the above operations, we obtain a FedMIT task scenario with 9 clients and 3 different VQA tasks(9-client, 3-task). For more data information, please refer to the supplementary materials.

# Baselines

We compare our framework Pilot with 5 state-of-theart FL algorithms: FedAVG (2017), FedProx (2020), FedAdam (2020), Shepherd (2024b), and FedDPA (2024c). FedAvg takes the weighted average of all training parameters as a standard optimization method. FedProx focuses on local model correction, and FedAdam focuses on introducing momentum on the server side to stabilize global model updates. Shepherd and FedDPA are FedIT task method. We also show local training and centralized training as references, where local training is trained by using one client’s dataset without collaboration. Centralized training is training all datasets centrally.

Table 1: Comparison with state-of-the-art methods under FL-oriented visual understanding scenario.   

<html><body><table><tr><td rowspan="2">Methods</td><td colspan="3">GQA</td><td colspan="3">Caption COCO</td><td colspan="3">Grounding RefCOCO</td></tr><tr><td>Client 1</td><td>Client 2</td><td>Client 3</td><td>Client 4</td><td>Client 5</td><td>Client 6</td><td>Client 7</td><td>Client 8</td><td>Client 9</td></tr><tr><td>Centralized training</td><td>Centralized test result: 52.8</td><td></td><td></td><td></td><td>Centralized test result: 130.3</td><td></td><td></td><td>Centralized test result: 65.3</td><td></td></tr><tr><td>Local training</td><td>48.8</td><td>47.5</td><td>48.4</td><td>117.6</td><td>122.3</td><td>120.9</td><td>29.1</td><td>35.1</td><td>31.9</td></tr><tr><td>FedAvg</td><td>47.2</td><td>46.6</td><td>46.4</td><td>86.5</td><td>102.5</td><td>104.2</td><td>25.4</td><td>30.2</td><td>32.4</td></tr><tr><td>FedProx</td><td>48.1</td><td>46.4</td><td>47.8</td><td>98.5</td><td>101.2</td><td>113.2</td><td>35.8</td><td>45.5</td><td>39.6</td></tr><tr><td>FedAdam</td><td>50.8</td><td>48.8</td><td>50.5</td><td>117.3</td><td>119.2</td><td>122.0</td><td>47.5</td><td>45.9</td><td>45.1</td></tr><tr><td>FedDPA</td><td>49.3</td><td>48.2</td><td>49.3</td><td>112.2</td><td>121.4</td><td>113.6</td><td>48.2</td><td>47.3</td><td>46.2</td></tr><tr><td>Shepherd</td><td>43.8</td><td>39.6</td><td>44.7</td><td>75.1</td><td>86.2</td><td>101.2</td><td>24.1</td><td>30.7</td><td>28.5</td></tr><tr><td>Pilot</td><td>51.4</td><td>49.7</td><td>50.2</td><td>120.3</td><td>126.4</td><td>125.1</td><td>49.6</td><td>52.4</td><td>50.9</td></tr></table></body></html>

Table 2: Comparison with state-of-the-art methods under FL-oriented general VQA scenario.   

<html><body><table><tr><td rowspan="2">Methods</td><td colspan="3">GQA</td><td colspan="3">ScienceQA</td><td colspan="3">OCRVQA</td></tr><tr><td>Client 1</td><td>Client 2</td><td>Client 3</td><td>Client 4</td><td>Client 5</td><td>Client 6</td><td>Client 7</td><td>Client 8</td><td>Client 9</td></tr><tr><td>Centralized training</td><td></td><td>Centralized test result: 56.7</td><td></td><td></td><td>Centralized test result: 58.0</td><td></td><td></td><td>Centralized test result: 52.3</td><td></td></tr><tr><td>Local training</td><td>46.1</td><td>50.2</td><td>49.6</td><td>50.2</td><td>49.6</td><td>50.0</td><td>40.62</td><td>46.33</td><td>37.10</td></tr><tr><td>FedAvg</td><td>44.1</td><td>48.3</td><td>47.1</td><td>44.2</td><td>40.5</td><td>42.6</td><td>35.6</td><td>38.1</td><td>34.2</td></tr><tr><td>FedProx</td><td>44.5</td><td>48.3</td><td>46.6</td><td>48.0</td><td>46.3</td><td>43.9</td><td>34.2</td><td>37.5</td><td>34.6</td></tr><tr><td>FedAdam</td><td>45.6</td><td>49.6</td><td>50.2</td><td>50.1</td><td>51.2</td><td>49.0</td><td>42.9</td><td>49.5</td><td>42.8</td></tr><tr><td>FedDPA</td><td>47.3</td><td>50.2</td><td>50.6</td><td>52.3</td><td>51.9</td><td>50.4</td><td>43.5</td><td>49.6</td><td>43.2</td></tr><tr><td>Shepherd</td><td>43.4</td><td>45.0</td><td>46.2</td><td>37.6</td><td>36.8</td><td>39.5</td><td>35.6</td><td>35.3</td><td>30.7</td></tr><tr><td>Pilot</td><td>49.2</td><td>52.8</td><td>52.3</td><td>54.7</td><td>53.2</td><td>53.7</td><td>45.7</td><td>50.2</td><td>44.9</td></tr></table></body></html>

# Implementation Details

In all experiments, we use LLaVA 1.5 (Liu et al. 2024) as multimodal Large Language Model. LLaVA utilizes the pretrained CLIP visual encoder ViT-L/14 (Radford et al. 2021) to extract visual features, and LLM utilizes Vicuna-V1.5- 7b (Chiang et al. 2023). The client-specific adapter and taskspecific adapter parameters in stage 1 are initialized by the pre-trained MLP connector parameters in LLaVA. All crosstask adapter parameters in stage 2 are initialized by the local task-specific adapter obtained in stage 1. For all client text-adapter, we use the LoRA (Hu et al. 2021) parameter efficient tuning technique to train LLM. The rank of LoRA is 64 with a scalar $\alpha = 1 2 8$ . During instruction tuning, we only fine-tune the parameters of the connector and LoRA while keeping the rest of the LLM frozen. The learning rates of stage 1 and stage 2 are set to 2e-5 and 4e-5, respectively. The number of text-adapter parameter aggregates Top- $M$ is set to 6 The coefficients of $\lambda _ { 0 } , \lambda _ { 1 }$ , and $\lambda _ { 2 }$ are 0.1, 0.1, and 0.01, respectively. The number of local cycles $E$ is set to 1 and the number of communication rounds $R$ is 3. We train Pilot on 8 A100 GPUs (40G), with an effective batch size of 16 per GPU. To ensure fairness, for all baselines, all additional hyperparameters involved in the compared methods use their best settings.

# Experimental Results

For the VQA task (including ScienceQA, GQA, and OCRVQA), we calculate the accuracy of predicting answers against ground truth. For the caption task, we report the CIDEr score. For the grounding task, we employ Intersection-over-Union (loU) as the evaluation metric. Specifically, a prediction is deemed accurate only when its loU exceeds or equals 0.5. Table 1 and Table 2 show the comparison of Pilot with other five methods in the federated scenarios of visual understanding and General VQA. Our framework outperforms all baselines in two task scenarios. We found that the performance of FedAVG method is lower than local training. This shows that the heterogeneity of multimodal tasks leads to greater parameter conflicts, and simple aggregation has a negative impact on local models. At the same time, we observed that FedAvg method is better than Shepherd. The former aggregates all training parameters, while the latter only aggregates LLM training parameters. This result also indirectly reflects the importance of visual information to the FedMIT task. Compared with FedDPA, only improving the LLM training parameters does not achieve the desired effect, which also illustrates the need to consider the differences between different tasks and the necessity of learning general knowledge from other tasks. Finally, the results prove that our method can not only overcome the heterogeneity between tasks, but also collaborate with all clients to improve the performance of local models.

# Ablation Studies

Here, we show the results of several variants of our method in the FL-oriented visual understanding scenario to demonstrate the effectiveness of the main modules of our method. We first analyze the impact of the four components of our framework (i.e., cross-task adapter, difference loss, auxiliary loss, and adaptive text-adapter aggregation (ATA)) on the model performance, and further evaluate the effectiveness of the proposed method. Table 3 shows the average score of all clients for the same task. We found that without using the above methods, the performance of our method is lower than that of local training, which does not have the ability to overcome task heterogeneity. Then we add adaptive text-adapter aggregation, and the performance of our method is improved and outperforms local training, which demonstrates that this module can effectively alleviate the impact of task heterogeneity. When we add the cross-task adapter, we observe that the model performance improves on all clients. Through auxiliary loss optimization, the performance of our framework can be further improved. The results show that the CT-MOA module is able to learn general knowledge from other tasks to improve model performance and cross-task capabilities. Removing the difference loss, our connector has only one and no longer distinguishes between client-specific adapter and task-specific adapter. The performance of the model has decreased, indicating that it is necessary to maintain the personalized information of the client. The above results demonstrate the importance of each component in our method.

<html><body><table><tr><td rowspan="2">ATA</td><td rowspan="2"></td><td colspan="2">Loss</td><td rowspan="2">GQA</td><td rowspan="2">CoCo</td><td rowspan="2">RefCOCO</td></tr><tr><td>Lb+Lz</td><td>Ld</td></tr><tr><td>×</td><td>×</td><td>×</td><td>×</td><td>46.9</td><td>105.7</td><td>30.6</td></tr><tr><td>√</td><td>×</td><td>×</td><td>×</td><td>47.5</td><td>110.6</td><td>39.8</td></tr><tr><td>√</td><td>√</td><td>×</td><td>×</td><td>49.3</td><td>114.5</td><td>47.5</td></tr><tr><td>√</td><td>√</td><td>√</td><td>×</td><td>49.8</td><td>120.7</td><td>48.2</td></tr><tr><td>√</td><td>√</td><td>√</td><td>√</td><td>50.4</td><td>124.0</td><td>51.0</td></tr></table></body></html>

Table 3: Ablation studies under FL-oriented visual understanding scenario.   

<html><body><table><tr><td>Methods</td><td>GQA</td><td>CoCo</td><td>RefCOCO</td><td>AP</td><td>CP</td></tr><tr><td>Pilot</td><td>50.4</td><td>124.0</td><td>51.0</td><td>0.5B</td><td>0.3B</td></tr><tr><td>+ CT-CLIP</td><td>51.6</td><td>126.1</td><td>52.7</td><td>1.75B</td><td>0.5B</td></tr></table></body></html>

# Further Remarks

Building Cross-task CLIP. Our method modifies the MLLM connector to learn general knowledge from other tasks to improve model performance and cross-task capabilities. But the natural thought is: why not use CLIP? To answer this question, we unfreeze CLIP and train all MLP layers in CLIP in the same way as the connector, called CTCLIP. We conduct experiments on the visual understanding scenario for federated learning. Table 4 shows the average scores of all clients for the same task, where AP denotes local model Activation Params and CP denotes Communication Params. Experimental results show that for the FedMIT task, learning cross-task visual information from different tasks is an effective solution. We observe that although CTCLIP outperforms the Pilot, it comes at the expense of additional training parameters and communication parameters. Compared with Pilot, the communication parameters sent to the server increase by 0.2B, and the local client activation parameters increase by 1.25B. With the increase of tasks, the computational cost is unacceptable. Therefore, we give priority to the more simple and efficient method.

Table 5: Compare different text-adapter parameter aggregation strategies.   

<html><body><table><tr><td>Selection Strategy</td><td>GQA</td><td>CoCo</td><td>RefCOCO</td></tr><tr><td>Same task client</td><td>49.6</td><td>120.2</td><td>50.3</td></tr><tr><td>All clients</td><td>47.9</td><td>109.7</td><td>45.2</td></tr><tr><td>Pilot (M=5)</td><td>49.7</td><td>122.8</td><td>50.6</td></tr><tr><td>Pilot (M=6)</td><td>50.4</td><td>124.0</td><td>51.0</td></tr><tr><td>Pilot (M=7)</td><td>50.3</td><td>123.2</td><td>51.7</td></tr></table></body></html>

Table 4: Impact of vision encoder improvements on model performance.   
Table 6: Compare different cross-task adapter parameters initialization strategies.   

<html><body><table><tr><td>InitializationStrategy</td><td>GQA</td><td>CoCo</td><td>RefCOCO</td></tr><tr><td>Random</td><td>50.1</td><td>122.7</td><td>50.3</td></tr><tr><td>Pilot</td><td>50.4</td><td>124.0</td><td>51.0</td></tr></table></body></html>

Different Text-adapter Parameter Aggregation Strategies. In our framework, we adopt the adaptive text-adapter aggregation method. For all text adapter parameters, we adopt a weighted optimization aggregation strategy based on euclidean distance and select the Top- $M$ parameters for weighted averaging. We compared different aggregation strategies, such as aggregating only parameters of clients with the same task (same task client), aggregating parameters of all clients (all clients), and different Top- $M$ selections. As shown in Table 5, due to the heterogeneity between tasks, aggregating on all client will lead to parameters conflicts. In addition, aggregating only on the same task is a suboptimal solution because it does not utilize the semantic knowledge of other tasks. At the same time, we tested the impact of different top- $\mathbf { \nabla } \cdot M$ on model performance, and the results showed that the adopted adaptive aggregation method can benefit from positive influences while reducing negative interference.

Cross-task Adapter Initialization Strategy. In our framework, the role of the cross-task adapter is to extract cross-task knowledge. We compared initialization with taskspecific adapter parameters with training from scratch (random). As shown in Table 6, training from scratch leads to performance degradation for all tasks. Using task-specific adapter parameters initialization provides a good starting point for the module and can help the local client better extract cross-task knowledge.

# Conclusion

In this paper, we propose a federated multimodal instruction tuning framework to solve the new task of federated multimodal instruction tuning by collaboratively utilizing distributed data from different local clients to learn crosstask knowledge without being affected by task heterogeneity during instruction tuning. Through two stages “adapter on adapter” strategy, our model can capture the personalized information of local data and the task-related multimodal information, and can also adapt to the differences between different tasks. Our method achieves state-of-the-art results in two cross-task scenarios.