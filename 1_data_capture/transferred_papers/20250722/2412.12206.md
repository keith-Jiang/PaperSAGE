# Provably Secure Robust Image Steganography via Cross-Modal Error Correction

Yuang Qi, Kejiang Chen\*, Na Zhao, Zijin Yang, Weiming Zhang

Anhui Province Key Laboratory of Digital Security, University of Science and Technology of China, Hefei, Anhui, China qiyuang@mail., chenkj $@$ , znzhaona $@$ mail., bsmhmmlf@mail., zhangwm@ ustc.edu.cn

# Abstract

The rapid development of image generation models has facilitated the widespread dissemination of generated images on social networks, creating favorable conditions for provably secure image steganography. However, existing methods face issues such as low quality of generated images and lack of semantic control in the generation process. To leverage provably secure steganography with more effective and high-performance image generation models, and to ensure that stego images can accurately extract secret messages even after being uploaded to social networks and subjected to lossy processing such as JPEG compression, we propose a high-quality, provably secure, and robust image steganography method based on state-of-the-art autoregressive (AR) image generation models using Vector-Quantized (VQ) tokenizers. Additionally, we employ a cross-modal error-correction framework that generates stego text from stego images to aid in restoring lossy images, ultimately enabling the extraction of secret messages embedded within the images. Extensive experiments have demonstrated that the proposed method provides advantages in stego quality, embedding capacity, and robustness, while ensuring provable undetectability.

# Introduction

Steganography (Cachin et al. 2005) is a science and art of covert communication that hides secret messages in covers, which needs to avoid arousing suspicion from steganalysis. In terms of security, steganography is divided into empirically secure steganography (ESS) and provably secure steganography (PSS). While empirically secure steganography has been developed for many years (Sedighi, Cogranne, and Fridrich 2015; Wang et al. 2019, 2020), there is relatively little research on PSS. Actually, PSS also has a long history. Cachin (1998) and Hopper et al. (2002) have proposed the definitions of information-theoretic security and computational security for steganography, respectively.

For a long time, PSS has been lacking due to the lack of precise samplers and the inability to obtain a definite cover distribution. It was not until the emergence of generative artificial intelligence that efficient, efficient PSS became possible (Chen et al. 2018), where generative image steganography was in the vanguard. The image generation model gives

1100 message 1110 message 0001 OSN Warden damage 0110 A suspicion generate extract T   
‚ñ° upload download n„ÄÇ   
Previous PSIS low quality lossy stego images Among with high quality stego images generated images

an explicit distribution of pixels (Van den Oord et al. 2016; Tulsiani and Gupta 2021), or a sampler corresponding to the distribution (Song and Ermon 2019; Goodfellow et al. 2020), which meets the requirements of PSS.

Yang et al. (2018) proposed the first provably secure steganography method based on image generative model, utilizing PixelCNN (Van Den Oord, Kalchbrenner, and Kavukcuoglu 2016) for message embedding. Ding et al. (2023) proposed a provably secure steganography contruction based on distribution copies and deployed it on ImageGPT (Chen et al. 2020). These two methods can only perform steganography at the pixel level, resulting in stego images with low resolution and poor quality. In the context where high-resolution image generation models have become increasingly widespread (Zhang et al. 2022; Du et al. 2024), transmitting such low-resolution generated images is no longer an entirely innocent act; this does not align with the covert pursuit of steganographic behavior, as depicted in Figure 1.

Additionally, in practical applications, digital images are widely disseminated through social networks, and steganographic images are no exception. Therefore, the ability to withstand lossy processing by social networks is also an important criterion for evaluating image steganography. Unfortunately, for the aforementioned provably secure image steganography method, lossy processing can cause the receiver to lose synchronization, leading to heavy message damage. To resist lossy processing, Yang et al. (2023) proposed PARIS, a provably secure robust image steganography method using inverse sampling based on generative adversarial networks (GANs), where the message is encoded into a latent vector, and then generating the stego image. GAN inversion (Xia et al. 2022) is utilized to reconstruct the latent vector and then extract the secret message. As the GAN network structure deepens, the inversion accuracy decreases rapidly and the message is difficult to extract. Therefore, the inversion-based method can only be limited to low-quality small GAN.

Su et al. proposed StegaStyleGAN, achieving provably security and higher resolution (Su, Ni, and Sun 2024). They used a message mapping method similar to that of in PARIS to map the message into random noise of StyleGAN (Karras, Laine, and Aila 2019), and trained a CNN for message extraction. Although StegStyleGAN has the capability to generate stegos with resolutions of $2 5 6 \times 2 5 6$ , it is specifically designed for StyleGAN, and the quality of the image is limited by the upper bounds of GAN‚Äôs generative capabilities.

Large language models (LLMs) offer remarkable performance in solving language tasks (Vaswani et al. 2017; Radford et al. 2019; Achiam et al. 2023) and showing potential towards achieving general artificial intelligence (Ge et al. 2024; Almeida et al. 2024), which inspired researchers to explore the possibility of developing autoregressive (AR) models in the field of image generation. AR image generation models, represented by Vector-Quantized-VAE (VQVAE) (Van Den Oord, Vinyals et al. 2017), VQGAN (Esser, Rombach, and Ommer 2021), DALL-E (Ramesh et al. 2021), and LlamaGen (Sun et al. 2024), may also become mainstream in the future, just like LLMs. Moreover, advanced generative models can use labels or descriptive text to conveniently control the semantics of the generated images, enabling the generated images that better fit the steganographic scenario. However, novel AR models are quite different from traditional models like PixelCNN. Is it possible to design steganography methods for existing AR models with VQ tokenizers that achieve high quality, provable security, and robustness?

In this paper, we affirm the above question. A provably secure robust steganography based on a semantic controllable AR image generative model, LlamaGen, is proposed. Considering the requirement of security and robustness, we design three modules, which are the secure message embedding module, the discrete token optimization module, and the cross-modal error correction module. The first module is based on the AR model, which embeds the secret message into an image token sequence in a distribution-preserving manner. Subsequently, the token indices are decoded into an image by the image tokenizer. The sender can utilize this module to generate high-quality secure stego images.

The receiver still faces challenges. The image tokenizer cannot accurately encode the image into correct stego tokens, and lossy social network processing exacerbates the discrepancy. The second and third module are introduced to address these two problems. In the second module, an optimization process for discrete image tokens is employed to assist in the recovery of the tokens. As for the design of a cross-modal error-correction module, with the aid of an image-to-text model, compressed error-correction information is embedded into a descriptive text about the stego image using provably secure linguistic steganography. The stego image is finally transmitted to the receiver along with the error-correction text, achieving provably secure robust image steganography through cross-modality errorcorrection.

We conducted experiments and demonstrated that our method can achieve provably secure, high-quality image robust steganography. The experimental results indicate that the proposed method significantly enhances the image quality and embedding capacity of stego images while ensuring the security and robustness of message extraction.

The main contributions of this paper are summarized:

‚Ä¢ We propose a provably secure robust image steganography method based on an auto-regressive generative model, LlamaGen, capable of generating high-quality stego images while preserving distribution. ‚Ä¢ We design a robust enhancement mechanism, which includes a discrete token optimization module and a crossmodal error-correction module, to strengthen the provably secure steganography against lossy channels. ‚Ä¢ Experiments verify the provable security and robustness of the proposed steganography method, and the visual effects demonstrate our significant advantage over existing methods in terms of the quality of the generated images.

# Related Work

There are two common definitions of steganographic security. Cachin (1998) first proposed an information-theoretic model for steganography with passive adversaries. The adversary‚Äôs task of distinguishing between an innocent cover $c$ and a stego $s$ containing a secret message is interpreted as a ‚Äúhypothesis testing‚Äù problem. The security of a stegosystem can be quantified by Kullback-Leibler divergence between the cover distribution $P _ { c }$ and the stego distribution $P _ { s }$ ,

$$
D _ { K L } ( P _ { c } | | P _ { s } ) = \sum _ { x \in \mathcal { C } } P _ { c } ( x ) \log \frac { P _ { c } ( x ) } { P _ { s } ( x ) } ,
$$

where $x$ is the object transmitted in the channel with the alphabet $\nu$ . If $\mathbf { \bar { \Gamma } } { \cal D } _ { K L } ( P _ { c } | | P _ { s } ) ~ = ~ 0$ , the stegosystem is called perfectly secure. Another definition is based on computational complexity theory, proposed by Hopper et al. (2002). Computational security in steganography is established through a probabilistic game that distinguishes the outputs of a oracle $\mathcal { O } _ { D }$ that can randomly sample from the channel distribution $\mathcal { D }$ and a steganographic encoding algorithm $\operatorname { E N C O D E } _ { \mathcal { D } }$ . The attacker‚Äôs advantage is defined as the difference between the probability of correctly identifying the stego and the probability of incorrectly identifying a cover as a stego. The stegosystem is called secure if all probabilistic polynomial time (PPT) adversaries $\mathcal { A }$ ‚Äôs advantage against the stegosystem is negligible with respect to a security parameter $\kappa$ , that is:

$$
\left| \operatorname* { P r } \left[ \boldsymbol { A } _ { \mathcal { D } } ^ { \mathrm { E N C O D E } _ { \mathcal { D } } ( K , \cdot , \cdot ) } = 1 \right] - \operatorname* { P r } \left[ \boldsymbol { A } _ { \mathcal { D } } ^ { O _ { \mathcal { D } } ( \cdot , \cdot ) } = 1 \right] \right| < \operatorname { n e g l } \left( \kappa \right) ,
$$

where negl $( \kappa )$ is a negligible function concerning $\kappa$ .

Secure Message Embedding DiscreteToken Optimization Cross-modal Error Correction (CMEC) computeerror   
Message Bits recoveredtokens ‚ñ≥q= llq-qll Predecessor Priority Provably Secure 2. Relative Coordinate Steganography Embedding   
10001010101100101110010010 q ‰∏≠ 3. Vector Proximity   
Condition q Nn-Zer 11100101 PRNG Key PRNG @ $\delta _ { q }$   
C quantization L stegoNo Encoder Q-Frmer 1 E stego image 4 5961 2 4 15 33[Ôºü] qt A DND Stego text for EC   
Stego Image Tokensq<t Auto-regressiveGeneration p(q)= p(qtlq<t,c) recovered optimize with Œîx ÂÖ¨ x 8 a golden retriever laying in the grass... AR Model DQ Iitilizetheimzaionvctorusingencoder. Emcoder Q-Former LLM Âìà image autoregressive model with VQ tokenizer back-propagation vison-language generative modelpre-argreed channel

Based on the aforementioned security definitions, Hopper et al. (2002) proposed a construction based on rejection sampling. Le et al. (2003) leveraged the duality between steganography and source coding (e.g. arithmetic coding) to encode and decode encrypted messages during the sampling process from channel distribution $\mathcal { D }$ . These classic constructions needs implicit samplers or even explicit representations of $D$ , which is satisfied by deep learning generative models (Chen et al. 2018). Due to the exponential time complexity of rejection sampling-based algorithms, researchers focus on implementing or improving efficient arithmetic coding-based algorithms (Yang et al. 2018; Chen et al. 2020; Kaptchuk et al. 2021). However, their implementation inevitably distort the distribution. Zhang et al. (2021) proposed ADG (adaptive dynamic grouping), grouping candidate signals with ‚Äúequal probability sums‚Äù and encoding messages using the group index. Ding et al. (2023) proposed Discop, constructing multiple ‚Äúdistribution copies‚Äù during signal generation and encoding messages using the copy index, thereby avoiding distortion of the distribution.

A suitable generative model allows these PSS constructions to be applied across various signal cover, i.e., text, audio, and images. While research into PSS for text is already well-established, its application to image cover is limited. Therefore, this paper aims to explore the potential for applying PSS to high-quality images with semantic control.

# Methodology

# Approach Overview

We propose CMSTEG, a novel provably secure robust image STEGanography via Cross-Modal error-correction. As shown in Figure 2, CMSTEG comprises three modules: Secure Message Embedding, Discrete Token Optimization, and Cross-modal Error Correction.

r(ùë°)(mi = 0) 0.4 rotate 0.6 Ps a b 0 0.4 1.0

# Secure Message Steganography Module $M _ { 1 }$

This module is capable of generating a stego image with height $H$ and width $W$ , which is deployed with the sampling process of a pre-trained AR model with a Vector Quantised (VQ) tokenizer. The AR generative model is trained to generate a sequence of discrete image tokens q ‚àà Qh√ów, where $h = \bar { H } / p$ , $w = W / p$ , $p$ is the downsample ratio of the image tokenizer, every $\pmb q ^ { ( i , j ) }$ is a indice of a image codebook. The sequence of tokens starts from a given conditional embedding $\mathcal { H }$ and stops at the location of the pre-defined maximum length $h \cdot w$ . Image tokens $( q _ { 1 } , q _ { 2 } , \dots , q _ { h \cdot w } )$ are sampled by AR models in the way of next-token prediction. Utilizing the probability distribution $p ( q _ { t } | q _ { < t } , \mathcal { H } )$ predicted by the AR model, PSS constructions such as Meteor, Discop can be deployed during the sampling phase of image token generation. At each time step $t$ , the stego image token is generated as follows:

$$
q _ { t } = { \mathrm { E N C O D E } } _ { p ( q _ { t } | q _ { < t } , \mathcal { H } ) } \left( K , m _ { t } \right) ,
$$

where ENCODE denotes the steganographic embedding algorithm used in practical deployment. Using Discop as an example, ENCODE first constructs several distribution copies based on the probability distribution, then selects the one that represents the message bits $m _ { t }$ from the distribution copies according to the secret message to be embedded, and finally chooses the stego token $q _ { t }$ for this time step based on the random number controlled by the steganographic key $K$ . Figure 3 provides a simple example of using Discop to select a token from a distribution based on a message bit.

![](images/08c6a111845cc144031ac5f18657e08143a0091a0fc28f9b198807dfa57308ae.jpg)  
Figure 4: Flowchart of the discrete token optimization module used in the proposed provably secure and robust image steganography method.

Then, a VQ tokenizer consisting of an encoder $E$ and a decoder $G$ is used to remap the code indices $\pmb q$ into the corresponding feature vectors $z _ { q }$ in a discrete codebook $\mathcal { Z }$ , where $\mathcal { Z } \in \mathbb { R } ^ { N \times d }$ is with $N$ learnable vectors and pre-trained as well as the image tokenizer $E$ and $G , d$ is the dimension of $z _ { q }$ . Then decoder $G$ converts the vectors back into image pixels $\pmb { x } \in \mathbb { R } ^ { H \times W \times 3 }$ by:

$$
\begin{array} { r } { \pmb { x } = G ( \pmb { z } _ { q } ) , } \end{array}
$$

where $\scriptstyle { \pmb x }$ is the generated stego image.

# Discrete Token Optimization Module $M _ { 2 }$

The sender then uploads the stego image to a pre-agreed communication channel with the receiver.

Assuming the channel is lossless, the receiver can directly obtain the original generated stego image from the channel. However, the receiver cannot directly extract the secret message from the image pixels and must re-encode it into image tokens. Specifically, the encoder $E$ takes $\scriptstyle { \mathbf { { \vec { x } } } }$ as input and first outputs a set of continuous vectors:

$$
\hat { \boldsymbol { z } } = \boldsymbol { E } ( \boldsymbol { x } ) \in \mathbb { R } ^ { h \times w \times d } .
$$

In the subsequent element-wise quantization $\mathscr { Q } ( \cdot )$ , each vector $\hat { \boldsymbol { z } } ^ { ( i , j ) } \in \mathbb { R } ^ { d }$ is quantized to its closest codebook entry $z _ { n }$

$$
\hat { \pmb q } = \pmb { \mathcal { Q } } ( \hat { \pmb z } ) : = \left( \arg \operatorname* { m i n } _ { n \in N } \lVert \hat { \pmb z } ^ { ( i , j ) } - \pmb z _ { n } \rVert \right) ,
$$

where $\hat { \pmb q } \in \mathbb { Q } ^ { h \times w }$ are the indices corresponding to quantized vectors zqÀÜ Rh√ów√ód.

Unfortunately, the VQ tokenizers do not guarantee consistency on the vectors before and after passing through

a Decoder-Encoder structure. Formally, the loss between stego tokens $\pmb q$ and re-encoded tokens $\hat { \pmb q }$ can be denoted as:

$$
\Delta _ { q } = \lVert q - \hat { \pmb q } \rVert .
$$

To relatively accurately extract the secret message, it is necessary to make $\Delta _ { q }$ as small as possible. We use the reversed tokens $\hat { \pmb q }$ to regenerate a recovered image for optimization. A differentiable noise layer $\mathcal { N }$ designed to simulate the noise attack $\mathcal { N } _ { C }$ of the channel $C$ is introduced, ensuring that the recovered image undergoes lossy operations similar to those experienced by the stego image. Then the receiver can get a lossy recovered image:

$$
\hat { \pmb { x } } ^ { \prime } = \mathcal { N } ( \hat { \pmb { x } } ) = \mathcal { N } \left( G \left( \pmb { z } _ { \hat { \pmb { q } } } \right) \right) .
$$

The difference between the lossy recovered image $\hat { \pmb x } ^ { \prime }$ and the lossy stego image $ { \boldsymbol { { x } } } ^ { \prime }$ can be denoted as:

$$
\begin{array} { r l } & { \Delta _ { { \pmb x } } = \| { \pmb x } ^ { \prime } - \hat { { \pmb x } } ^ { \prime } \| _ { 2 } } \\ & { \qquad = \| \mathcal { N } _ { C } ( G ( { \boldsymbol z } _ { { \pmb q } } ) ) - \mathcal { N } \left( G \left( { \boldsymbol z } _ { { \hat { \pmb q } } } \right) \right) \| _ { 2 } , } \end{array}
$$

If $\hat { \pmb q }$ is identical to $\pmb q$ , then $z _ { \hat { q } }$ is the same as $z _ { q }$ , and $\Delta _ { x }$ will be quite small. Let $\Delta _ { x }$ be a loss function of $\pmb q$ , and an optimization method can be used to obtain a $\hat { \pmb q }$ that is as close as possible to $\pmb q$ .

Since the tokens are discrete integers, there is no gradient at $\pmb q$ and $\hat { \pmb q }$ . For generation images of larger sizes, the discrete optimization of the $h \times w \times N$ dimensions is quite challenging. Therefore, we use the differentiable continuous vectors $\hat { \pmb { z } } \in \mathbb { R } ^ { h \times w \times d }$ to replace $\hat { \pmb q }$ for optimization based on gradient descent:

$$
\hat { z } \gets \left[ \hat { z } - \gamma _ { \hat { z } } \frac { \partial \Delta _ { x } } { \partial \hat { z } } \right] ,
$$

where $\gamma _ { \hat { z } }$ denotes the learning rate of gradient descent.

Ultimately, after the optimization process, we convert $\hat { z }$ back into discrete tokens $\hat { \pmb q } = \pmb { \mathcal { Q } } ( \hat { \pmb z } )$ , which is more similar to $\pmb q$ than re-encode $\mathbf { { x } ^ { \prime } }$ directly. The whole discrete token optimization module is shown in Figure 4.

# Cross-Modal Error-Correction Module $M _ { 3 }$

Upon observation, we found that even if $\Delta _ { x }$ converges to a considerably low level during the optimization process, there are still some recovered tokens that differ from the original stego tokens. We introduce additional error-correction mechanisms to enhance the robustness of steganography in this module.

Once the sender uploads the generated stego image $\scriptstyle { \mathbf { { \vec { x } } } }$ to the selected channel, the sender has the ability to carry out a complete discrete token optimization process, just as the receiver would do during extraction. If there remains some tokens that cannot be recovered, the sender can supplement this part to the receiver in some other way. Specifically, let $\delta _ { q }$ be a set that represents the non-zero elements from $\Delta _ { q }$ ,

$$
\delta _ { q } = \{ \left( ( i , j ) , \pmb { q } ^ { ( i , j ) } \right) \mid \Delta _ { \pmb { q } } ^ { ( i , j ) } \neq 0 \} .
$$

The error-correction module embeds $\delta _ { q }$ as a secret message into a piece of text using a PSS method based on generative models. The stego text is then conveyed to the receiver.

To further strengthen the semantic connection between the stego text used for error correction and the original stego image, we opt to utilize a pre-trained vision-language model, which consists of a frozen image encoder $E _ { B }$ , a pre-trained querying transformer (Q-Former) $Q F _ { B }$ used for bridging the modality gap, and a large language model $L L M$ for generation. Firstly, the image encoder $E _ { B }$ and the Q-Former $Q F _ { B }$ jointly take responsibility for extracting the lossy stego image $ { \boldsymbol { { x } } } ^ { \prime }$ that has been processed by the channel into a visual representation $\mathcal { H } _ { x ^ { \prime } }$ that can be understood by the $L L M$ , which can be denoted as:

$$
\mathcal { H } _ { \pmb { x } ^ { \prime } } = Q F _ { B } ( E _ { B } ( \pmb { x } ^ { \prime } ) , \mathcal { H } _ { t } ) ,
$$

where $\mathcal { H } _ { t }$ represents the instruction text or question that can be input during the Q-Former encoding process. Then the $L L M$ generates a corresponding descriptive text for $\mathbf { { x } ^ { \prime } }$ with $\mathcal { H } _ { x ^ { \prime } }$ as the context, while steganographic methods like Discop are employed to embed $\delta _ { q }$ within it. Due to the limited carrying capacity of text, to ensure complete error correction as much as possible, we also need to compress $\delta _ { q }$ . To send as little additional information as possible while achieving the strongest robustness, three principles are adhered to when embedding error-correction information, namely:

Predecessor Priority. Errors that appear early in the image token sequence can affect subsequent tokens, necessitating the prioritization of error correction for preceding ones.

Relative Coordinate. Most token reconstruction errors tend to cluster. Except for the first token, we represent the occurrence location of each erroneous token using relative coordinates $\delta _ { 1 }$ from the position where the previous erroneous token appeared. To reduce the volume of error correction information, we set a maximum relative coordinate threshold $\lambda _ { 1 }$ . Tokens exceeding the maximum relative coordinate will not be corrected.

Vector Proximity. During the generation of image tokens, the sampling is restricted to the top- $k$ tokens. By calculating the distance between all the top- $k$ vectors corresponding to samplable tokens and the vector corresponding to the incorrect reconstructed token after optimization, only the sorted sequence numbers $\delta _ { 2 }$ corresponding to the correct tokens are transmitted during error correction. Given a set of vectors $z ~ = ~ \{ z _ { 1 } , z _ { 2 } , \ldots , z _ { k } \}$ after an optimization process, where each $z _ { i }$ corresponds to a samplable top- $k$ token $q _ { i }$ . Let $z _ { e }$ be the vector corresponding to the incorrectly reconstructed token. We calculate the distance between each $z _ { i }$ and $z _ { e }$ , $\Delta _ { z , i } = \| z _ { i } - z _ { e } \|$ , $i = 1 , 2 , \ldots , k$ . The distances $\Delta _ { z , 1 } , \Delta _ { z , 2 } , \ldots , \Delta _ { z , k }$ are then sorted, and the corresponding indices are $o _ { 1 } , o _ { 2 } , \ldots , o _ { k }$ . During error correction, only the sequence number $o _ { j }$ corresponding to the correct token is transmitted, that is $\bar { \delta _ { 2 } } \ : = \ : o _ { j }$ where $z _ { j } = z _ { q }$ . Similar to the maximum relative coordinate value, a maximum relative sequence threshold $\lambda _ { 2 }$ will also be set; tokens exceeding this will not be corrected.

The compressed error-correction $\delta _ { q }$ can be denoted as:

$$
\delta _ { q } = \{ \left( \left( \delta _ { 1 } , \delta _ { 2 } \right) ^ { \left( i , j \right) } \right) \mid 1 \leq i \leq h , 1 \leq j \leq w , \Delta _ { q } ^ { \left( i , j \right) } \neq 0 \} ,
$$

where $0 ~ \le ~ \delta _ { 1 } ~ < ~ 2 ^ { \lambda _ { 1 } }$ , $0 ~ \leq ~ \delta _ { 2 } ~ < ~ 2 ^ { \lambda _ { 2 } }$ . Every $( \delta _ { 1 } , \delta _ { 2 } )$ is encoded into binary numbers and encrypted, waiting for steganographic embedding. At each time step $t$ of sampling process of the vision-language model, the stego text token $l _ { t }$ for error-correction is generated as follows:

$$
l _ { t } = \mathrm { E N C O D E } _ { p ( l _ { t } | l _ { < t } , \mathcal { H } _ { \mathbf { x } ^ { \prime } } ) } \left( K , \delta _ { t } \right) .
$$

Assuming ENCODE has an embedding rate of $\rho$ bits per token on the LLM, then the number $\tau$ of erroneous image tokens that the stego text tokens of length $\ell$ can correct can be calculated as:

$$
\tau = \left\lfloor 1 + { \frac { \rho \cdot \ell - \lfloor \log _ { 2 } ( h \cdot w ) \rfloor + \lambda _ { 2 } } { \lambda _ { 1 } + \lambda _ { 2 } } } \right\rfloor .
$$

After the steganographic process is completed, the stego text corresponding to stego text tokens $l \doteq ( l _ { 1 } , l _ { 2 } , \ldots , \bar { l _ { \ell } } )$ , along with the stego image $\scriptstyle { \mathbf { { \vec { x } } } }$ , is transmitted to the receiver. The receiver can then extract error-correction information from the stego text to assist in message extraction from the stego image. Ultimately, robust provably secure image steganography is achieved.

# Complexity

The time complexity of our method can be evaluated in three modules. In $M _ { 1 }$ , the time to generate the stego image includes the predicting time of the token distribution, the embedding time of secret message, and the generating time of the image from the tokens. The embedding time depends on the algorithm used. The complexity of optimizing in $M _ { 2 }$ is $O ( T ( 3 \cdot H \cdot W + d )$ , where $T$ is the numbers of iterations, $d$ is the dimension of vector. $M _ { 3 }$ ‚Äôs time includes the time to compute error correction information and to generate the stego text. The first time is related to the number of tokens, the top- $k$ value, and the dimension of the vectors. The time complexity is $O ( h \cdot w ( k \cdot d + k \log k ) )$ .

# Proof of Security

In our method, both the stego image with embedded secret message and the stego text with embedded error correction information are transmitted through public channels, and security needs to be guaranteed at the same time. For stego text, since the security of the embedding algorithm used has been proven, in this paper we only discuss the security of the image steganographic embedding algorithm, that is, the undetectability of the stego image from the normal generated image.

Assume that a PPT adversary $\mathcal { A }$ possesses a nonnegligible advantage to distinguish the generated stego image $\scriptstyle { \mathbf { { \boldsymbol { x } } } }$ from a randomly sampled cover image $\scriptstyle { \pmb x } _ { c }$ by the same model, which can be defined as:

$$
| \mathrm { P r } [ \boldsymbol { A } ( \boldsymbol { x } ) = 1 ] - \mathrm { P r } [ \boldsymbol { A } ( \boldsymbol { x } _ { c } ) = 1 ] | = \epsilon ,
$$

where $\epsilon$ denotes a non-negligible quantity relative to the length of shared key $K$ , indicating that $\mathcal { A }$ is able to distinguish between $\pmb { x } _ { c }$ and $\scriptstyle { \mathbf { { \vec { x } } } }$ . In this paper, an image is generated by $\begin{array} { r } { \pmb { x } = G ( \pmb { z _ { q } } ) } \end{array}$ . We denote the sequence of tokens used to generate the cover image as $\pmb q _ { c }$ and the tokens used to generate the stego image as $\pmb q$ . Hence, the advantage of $\mathcal { A }$ can be calculated as:

$$
\left| \operatorname* { P r } [ A ( G ( z _ { q } ) ) = 1 ] - \operatorname* { P r } [ A ( G ( z _ { q _ { c } } ) ) = 1 ] \right| = \epsilon .
$$

![](images/9939f87478d8f29252f9f9bf00f9dae19d1b6aa71b3bcad1630e1f9e3ed800a2.jpg)  
Figure 5: Visual results of generated stego images. All images are scaled to a suitable display size at the same ratio. (a) Ours; (b) Discop-ImageGPT (Ding et al. 2023); (c) PARIS (Yang et al. 2023).

That is, $\mathcal { A }$ ‚Äôs advantage to distinguish between $\scriptstyle { \mathbf { { \vec { x } } } }$ and $\scriptstyle { \pmb x } _ { c }$ can be reduced to an advantage to distinguish between $\pmb q$ and $\pmb q _ { c }$ . For each time step $t$ , $q _ { t }$ is obtained by the steganographic embedding algorithm ENCODE based on shared key $K$ and message bits $m _ { t }$ , while $q _ { c , t }$ is determined by a random sampling algorithm SAMPLE with a random number $\boldsymbol { r } _ { t }$ . Hence, the advantage is:

$$
\begin{array} { r l } & { \left| \operatorname* { P r } [ A ( \operatorname { E N C O D E } _ { p ( q _ { t } | q _ { < t } , \mathcal { H } ) } \left( K , m _ { t } \right) ) = 1 ] \right. } \\ & { \left. \qquad - \operatorname* { P r } [ A ( \operatorname { S A M P L E } _ { p ( q _ { t } | q _ { < t } , \mathcal { H } ) } \left( r _ { t } \right) ) = 1 ] \right| = \epsilon . } \end{array}
$$

Based on the previously proposed PSS constructions (Hopper, Langford, and Von Ahn 2002; Kaptchuk et al. 2021; de Witt et al. 2022; Ding et al. 2023), the aforementioned advantages can be reduced to $\mathcal { A }$ ‚Äôs ability to distinguish between a uniformly distributed random number obtained by encrypting with an encryption algorithm and a random number directly sampled from a uniform distribution in polynomial time. However, during steganography, a computationally secure symmetric encryption scheme is utilized. Therefore, the non-negligible advantage cannot hold, indicating that the cover and the stego are indistinguishable in polynomial time, validating the computational security of the proposed image steganography method. Q.E.D.

# Experiments

In this section, we conduct experiments to present the performance of CMSTEG mainly in terms of visualization and robustness, and compare CMSTEG with previous provably secure image steganography methods. The platform is Pytorch 2.3.1 and NVIDIA A6000.

Secure Message Steganography Module ${ \bf { M } } _ { 1 }$ In our experiments, we utilize a VQGAN with a downsampling rate of 16 as the VQ tokenizer. The codebook vector dimension is 8, codebook size is 16384. We employ an AR model with 3 billion parameters based on the Llama architecture for generating image tokens. The training of both VQ tokenizer and AR model is on ImageNet train set, using the resolution of $2 5 6 \times 2 5 6$ and random crop data augmentation. Top- $k$ is set to 2000. In the steganography experiments, we directly use the pre-trained models for generation without retraining them. The generated image size is set to $3 8 4 \times 3 8 4$ . The category labels used for generating cover and stego images are also sourced from ImageNet. Therefore, each cover or stego image corresponds to a token sequence of length 576.

<html><body><table><tr><td>Method</td><td>Model</td><td>Semantic Robust Resolution</td><td></td></tr><tr><td>Discop-ImageGPT</td><td>AR</td><td>weak</td><td>weak 32 √ó32</td></tr><tr><td>PARIS</td><td>GAN</td><td>weak</td><td>strong 64 √ó64</td></tr><tr><td>StegaStyleGAN</td><td>GAN</td><td>weak</td><td>strong 256√ó256</td></tr><tr><td>CMSTEG</td><td>AR</td><td>strong</td><td>strong 384√ó384</td></tr></table></body></html>

Discrete Token Optimization Module $\mathbf { M } _ { 2 }$ When simulating a lossy channel with a noise layer, JPEG-SS is used to simulate JPEG noise in a differentiable manner since it performs better than JPEG-MASK according to Yang et al. (2023). The momentum-based optimizer Adam is adopted with an initial learning rate of 0.002. The number of optimization steps is set to 10, 000.

Cross-Modality Error Correction Module ${ \bf { M } } _ { 3 }$ As for the image-to-text model, a InstructBLIP (Dai et al. 2023) model with a 7 billion parameter Vicuna language model is used. $\lambda _ { 1 }$ is set to 8, as is $\lambda _ { 2 }$ . Max token length is set to 200.

# Experimental Results

Visual Quality We focus on two aspects of visual quality: one is the comparison of the quality of stego images that different steganographic methods can generate, and the other is the comparison of quality between randomly sampled cover images and stego images generated by the provably secure and robust steganographic method. For the first aspect, we are mainly concerned with the resolution of the images. Table 1 presents a comparison of the resolutions of the stego images that CMSTEG and other methods can generate. As illustrated in Figure 5, our CMSTEG can generate stego images with higher resolution, greater diversity, and better visual quality. Figure 6 shows the stego image and its corresponding error-correcting text, both of which have consistent semantics.

In the image,a parrot is colored primarily with yellow and blue feathers.Thiscolorcombination makes it stand out amidst its green background since both of these colorsaretypically associated in tropical...

Figure 6: Example of stego image and its corresponding semantically consistent stego text for error correction.   

<html><body><table><tr><td rowspan="2">Noise</td><td colspan="2">w/M1</td><td>w/ M1,2</td><td>w/ M1,2.3</td></tr><tr><td colspan="2">Rq Cap</td><td>Cap Rq</td><td colspan="2">Rq Cap</td></tr><tr><td colspan="2">QF95 89.56</td><td colspan="2">91.89 106 99.89</td><td>3803 99.98</td><td colspan="2">4285 3936</td></tr><tr><td colspan="2">JPEG QF 85 QF 75 G.N. 0.01</td><td colspan="2">66 84.60 47 78.65 51</td><td>99.30 2793 98.28 2551 97.39 1935</td><td colspan="2">99.81 99.34 3861 98.42 3293</td></tr></table></body></html>

Robustness We evaluate the robustness mainly using token recovery rate $R _ { q }$ . Effective capacity $( C a p )$ is calculated as the maximum number of message bits that can be successfully decoded and extracted with the error-correcting capability of the system before encountering an error that exceeds the system‚Äôs correction threshold. We generate 50 stego images using CMSTEG with random message and attempt to extract the message from it. Specifically, we extract the message immediately after re-encoding the stego images, after the optimization process, and following the error correction. The results of these three extractions are recorded as w/ $\mathbf { M } _ { 1 }$ , w/ $\mathbf { M } _ { 1 , 2 }$ , and w/ $\mathbf { M } _ { 1 , 2 , 3 }$ , respectively, as shown in Table 2. It can be seen that CMSTEG can almost achieve lossless embedding and extraction of high-capacity messages over a lossless channel after passing through all three modules. It is also worth noting that due to the characteristics of AR models, preceding errors will affect the extraction of subsequent messages. Therefore, $R _ { q }$ is not entirely proportional to the effective capacity.

Security To verify the security of the algorithm, three deep-learning-based steganalyzers, namely ConvNet (Deng et al. 2019), SRNet (Boroumand, Chen, and Fridrich 2018), and LWENet (Weng et al. 2022), are employed to distinguish the cover image and stego image. In the experiment, the detection error rate $\begin{array} { r } { \bar { P } _ { E } = \frac { \bar { P } _ { F A } + \bar { P } _ { M D } } { 2 } } \end{array}$ is tested respectively, where $P _ { F A }$ denotes the false alarm rate and $P _ { M D }$ denotes the missed detection rate. For training three steganalyzers, 4000 randomly sampled generated images are selected as covers, and 4000 secret message-driven generated images are used as stegos. 1000 covers and 1000 stegos are used for the final test. The experimental results are shown in Table 3. Remarkably, the detection error rates remain closely to 0.5, indicating that the cover and stego images are indistinguishable. Our comparison is also within provably secure steganographic methods, which similarly provide theoretical guarantees; as shown in their papers (Ding et al. 2023; Yang et al. 2023; Su, Ni, and Sun 2024), their detection error rates are also around 0.5. Experimental results show that our proposed method, like previous secure image steganography methods, can resist detection by existing steganalyzers.

Table 3: Detection error rate $\bar { P } _ { E }$ against different steganalyzers.   

<html><body><table><tr><td>Steganalyzer</td><td>ConvNet</td><td>LWENet</td><td>SRNet</td></tr><tr><td>PE</td><td>0.5014</td><td>0.5043</td><td>0.4980</td></tr></table></body></html>

Table 2: Performance of the proposed CMSTEG against JPEG compression and other noise of different strengths.   
Table 4: Robustness of CMSTEG under JPEG Compression $( \mathrm { Q F = 9 5 } )$ ) with different max token lengths.   

<html><body><table><tr><td>l (token)</td><td>50</td><td>100</td><td>200</td><td>500</td></tr><tr><td>Payload (bit)</td><td>75</td><td>217</td><td>628</td><td>2037</td></tr><tr><td>Rq(%)</td><td>99.54</td><td>99.67</td><td>99.81</td><td>99.80</td></tr><tr><td>Cap (bit)</td><td>3608</td><td>3903</td><td>3935</td><td>3986</td></tr></table></body></html>

Effectiveness of Error Correction Table 4 shows the number of bits that can be embedded in the text, the reconstruction accuracy of image tokens after text error correction, and the effective capacity of secret messages, all varying with the maximum text token length. It can be observed that as the text length increases, the number of bits that can be embedded in the text increases significantly. We believe this is because, with the increase in text sequence length, the constraints imposed by the image and the initial prompt on text generation become weaker. Increasing the length of the error-correcting stego text can enhance the robustness of CMSTEG, which aligns with our expectations.

# Conclusion

In this paper, we propose CMSTEG, for the first time, achieving provably secure and robust image steganography on AR image generation models with VQ tokenizer. CMSTEG comprises three modules. The first secure message embedding module embeds secret message into stego images without altering any distribution. The second discrete token optimization module helps to recover the lost stego tokens during re-encoding and the lossy channel. The third cross-modal error-correction module utilizes an image-totext model to generate semantically consistent stego text corresponding to the stego image with error-correction message embedded in it. Experiments on LlamaGen demonstrate that CMSTEG can generate high-quality stego images. We have provided theoretical proofs for the security of the proposed image steganography method and experimental validation against steganalyzers. The designed cross-modality errorcorrection module effectively enhances the robustness of steganography, ensuring that the method can extract secret messages with a high payload under various types of noise.