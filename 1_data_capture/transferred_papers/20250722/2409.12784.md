# Evaluating Image Hallucination in Text-to-Image Generation with Question-Answering

Youngsun Lim\*, Hojun Choi\*, Hyunjung Shim

Kim Jaechul Graduate School of AI, KAIST youngsun ai, hchoi256, kateshim @kaist.ac.kr

# Abstract

Despite the impressive success of text-to-image (TTI) models, existing studies overlook the issue of whether these models accurately convey factual information. In this paper, we focus on the problem of image hallucination, where images created by TTI models fail to faithfully depict factual content. To address this, we introduce I-HallA (Image Hallucination evaluation with Question Answering), a novel automated evaluation metric that measures the factuality of generated images through visual question answering (VQA). We also introduce I-HallA v1.0, a curated benchmark dataset for this purpose. As part of this process, we develop a pipeline that generates high-quality question-answer pairs using multiple GPT-4 Omni-based agents, with human judgments to ensure accuracy. Our evaluation protocols measure image hallucination by testing if images from existing TTI models can correctly respond to these questions. The I-HallA v1.0 dataset comprises 1.2K diverse image-text pairs across nine categories with 1,000 rigorously curated questions covering various compositional challenges. We evaluate five TTI models using I-HallA and reveal that these state-of-the-art models often fail to accurately convey factual information. Moreover, we validate the reliability of our metric by demonstrating a strong Spearman correlation $\scriptstyle ( \rho = 0 . 9 5 )$ with human judgments. Our benchmark dataset and metric can serve as a foundation for developing factually accurate TTI models.

#

(a) A molecule of methane Q: How many hydrogen atoms are connected to the central atom   
R iCnhtohieceism:a{g"eT?w  o", "Three", "Four", "Five", "None of the above"} VQA for Factual image: "Four" VQA for Hallucinated image: "None of the above" 三   
Hallucinated Q: Is there a molecule in the image? Choices: {"Yes", "No"} VQA for Factual image: "Yes" VQA for Hallucinated image: "Yes" 7 (b) Comb-pattern pottery BCE 8000 Q: What features mostly depicted from the object in the image?   
Choices: {"Scratches", "Tiny holes", "Smooth Surface", "Circular Motifs", "None of the above"} VQA for Factual image: "Scratches" VQA for Hallucinated image: "Circular Motifs" N   
Hallucinated Q: What is the pattern on the pottery in the image? Choices: {"Comb", "Wavy", "Spiral", "Floral", "None of the above"} VQA for Factual image: "Comb" VQA for Hallucinated image: "Comb" 7 I(-OHuarlsl)A TIFA 7 Alignment SVeismualntic EKxntoerwnlaeldge

# Introduction

As generative models (Reimers and Gurevych 2019; Rombach et al. 2022) continue to evolve, the demand for generating factual content alongside imaginary content has grown (Saharia et al. 2022; Chen et al. 2022). In natural language generation, outputs with factual errors are classified as hallucinations, and considerable research has focused on mitigating this issue (Maynez et al. 2020; Min et al. 2023).

Current text-to-image (TTI) models also struggle to accurately reflect factual information, generating incorrect images for given prompts, as illustrated in Figure 1. This issue is becoming increasingly critical as TTI models are being actively utilized in industries and fields where factual accuracy is essential (Wong 2024). For instance, if images against factual information are used in educational materials or the media, misinformation and misconceptions can spread rapidly, causing serious social side effects (Robertson 2024).

While hallucinations have been primarily discussed in the language domain, relatively little research has addressed this issue in the context of image generation. This paper focuses on the unexplored issue of “image hallucination,” where generated images fail to reflect factual information (Lim and Shim 2024). To understand image hallucination in TTI models and guide future research directions, well-defined evaluation protocols and benchmark datasets are essential. Recent studies have developed benchmarks and evaluation metrics to assess TTI models based on the alignment between text prompts and generated images (Yarom et al. 2024). These evaluation protocols, such as TIFA (Hu et al. 2023), fo

History Science 安 Reliable The Statue of Oil and water TTI Q: Howare oil and water positioned Websites Liberty in 1890 dn't mix ncase Models Dataset )Mit Corpus of Image-Text Pairs LLM B)Oil aboveve itl Textbooks Reasoning D)Sidebysideindifferentcontainers Text Prompt Gold Answer: B Dataset CoI:relation Text Prompts Factual Hallucinated Corpus of Prompt Tuples Question-Answer Sets   
(b) Enhancing Dataset (d) Evaluating TTI Models on Image Hallucination The yellowish upper layer is oil. 图 Theclear lower layer is water. Dataset 学 Oilislessdense. VLM × I-HallA Score: 0.2 Oil floats on top of water. · 紫 Factual Reasoning Question-Answer × VLM Sets   
Dataset Reasoning 紫 Answer Set Oil should beontopof the water MTdels Automated Watea adilo ng nianig Dataset Text Prompt Human Annotation Generated Corpus of Prompt Tuples

cus only on the elements explicitly mentioned in the text prompt. However, verifying the factual alignment of generated images requires external knowledge beyond the prompt. As shown in Figure 1-(a), the number of hydrogen atoms, though not mentioned in the prompt, is important factual information. Additionally, current metrics struggle to distinguish between images that accurately represent factual information and those that simply match the text prompt, especially when polysemy introduces various interpretations. As shown in Figure 1-(b), while the generated image captures the idea of a “comb pattern,” it also includes several incorrect details, like small circles and decorations, which are not part of the factual image.

To address these limitations, we propose a three-stage pipeline to construct a new benchmark, I-HallA v1.0, with a new evaluation metric, I-HallA. Unlike existing protocols, we leverage the vast knowledge base of GPT-4 Omni (GPT4o) (OpenAI 2024) to assess factual information not mentioned in the prompt, such as the number of hydrogen and carbon atoms, molecular structure, and more. Additionally, we utilize the visual understanding capabilities of GPT-4o to discern factual visual semantics from potentially false ones, which is difficult to do with text prompts alone.

First, the dataset includes 200 prompts based on content from five science and history textbooks (Jackson J. Spielvogel 2008; Danzer et al. 2008; Urone et al. 2020; O’Grady et al. 2021; Neser 2023) to address factual information. Textbooks, meticulously edited for educational purposes, represent years of accumulated knowledge and are among the most authoritative sources, making them a primary basis for this dataset. Specifically, we use the textbooks’ captions and corresponding figures as prompts representing factual information and factual images. This is because textbook figures are carefully curated, highly aligned with their captions, and thoroughly validated for factual accuracy. The hallucinated images generated from these prompts in five TTI models (Rombach et al. 2022; StabilityAI 2022; Podell et al. 2023; OpenAI 2023) are compared against the factual images. In total, we gather 1,200 images for all prompts, consisting of both factual and hallucinated images.

Secondly, we enhance the dataset by inputting each prompt and its corresponding image into GPT-4o to obtain factual information, referred to as “reasoning,” relevant to the prompt. This process leverages GPT-4o’s external knowledge beyond the prompt and considers visual semantics to distinguish details difficult to discern from text alone. Lastly, we construct I-HallA, consisting of 1,000 multiplechoice question-answer (QA) sets to evaluate the extent of image hallucination in TTI models, using reasoning as a key input. With GPT-4o as our VQA model, we input the generated image and corresponding questions for each prompt. The accuracy of the answers is then scored, with higher accuracy indicating fewer hallucinations. We average QA scores across all prompts to evaluate TTI models on I-HallA v1.0. In all three stages, a thorough human review validates the metric’s legitimacy, though future use won’t require it.

By applying our metric to various TTI models, we quantitatively measure the extent of image hallucination. Experimental results show a strong correlation between our metric and human evaluation, with Spearman’s $\scriptstyle \rho = 0 . 9 5$ , indicating close alignment in assessing hallucination. Our benchmark effectively addresses image hallucination, paving the way for further advancements in mitigating this issue.

# Related Works

# Hallucination in Language Generation

In language models, hallucination refers to the generation of unfaithful content to the given source material (Ji et al. 2023). As large language models (LLMs) increasingly produce text that closely resembles human writing, there has been a growing emphasis on developing benchmarks to evaluate and distinguish hallucinated content. For instance, FEVER (Thorne et al. 2018) is a dataset used for factchecking that utilizes Wikipedia as its knowledge source. HaluEval (Li et al. 2023a) combines automated generation with human annotation to detect hallucinations.

Hallucination also occurs in large vision-language models (VLMs) such as LLaVA (Liu et al. 2024a), where visual features are input into LLMs to generate textual descriptions. In VLMs, hallucination refers to a mismatch between the factual details of images (e.g., object presence, attributes, spatial relations) and the corresponding generated text (Liu et al. 2024b). Various studies evaluate this using metrics like BLEU (Papineni et al. 2002) or CIDEr (Vedantam, Zitnick, and Parikh 2015), or by querying VLMs about object presence (Li et al. 2023b). In contrast, we focus on evaluating hallucinations in image generation, where generated images fail to depict factual information accurately. While hallucinations can occur in our pipeline during text generation, we mitigate this issue through rigorous human reviews.

# Common Sense Reasoning in VLMs

Some studies use benchmark datasets to assess whether VLMs possess commonsense knowledge when interpreting images. For instance, the WHOOPS (Guetta et al. 2023) and ROME (Zhou et al. 2023) datasets are created by inputting intentional text prompts that defy common sense into TTI models, resulting in odd and unconventional images.

These studies differ from our focus, as their benchmarks evaluate the language generated by VLMs, rather than assessing TTI models. Furthermore, by using counter-intuitive prompts to intentionally generate weird images, they do not address image hallucination, where TTI models fail to reflect factual information when given factually accurate prompts. Moreover, their concept of common sense differs from factual accuracy. For example, one prompt in WHOOPS is “A little girl standing in front of a blackboard with math formulas on it.” While this scenario is factual, as a young child can solve math problems, WHOOPS argues that this prompt defies common sense. Therefore, existing research on common sense does not fully address image hallucination.

# Evaluating Text-to-Image Generation with Question Answering

CLIPScore (Hessel et al. 2021) and DALL-Eval (Cho, Zala, and Bansal 2023), which are early studies measuring the alignment between generated images and text prompts to evaluate TTI models, commonly exhibit limitations due to the inherent constraints of CLIP (e.g., inability to count objects) or the restricted scope of evaluation criteria.

With the growing capabilities of foundation models, a new approach has developed that validates alignment in textto-image generation by using VQA models on questions derived from the prompt. For instance, TIFA (Hu et al. 2023) classifies elements of the text prompt into 12 categories and generates a set of questions and answers using GPT3 (Brown et al. 2020). These sets are used to evaluate the image by inputting both the image and questions into the VQA model like mPLUG (Li et al. 2022). ${ \dot { \mathrm { V } } } \mathrm { Q } ^ { 2 }$ (Yarom et al. 2024) extracts key information from the text prompt, generates related questions, and evaluates the text-image alignment by checking whether the image provides correct answers. VPEval (Cho, Zala, and Bansal 2024) enhances these alignment evaluation methods by incorporating object detection and optical character recognition, allowing for a more precise assessment. Davidsonian Scene Graph (Cho et al. 2024) breaks down the prompt into small propositions and represents the dependencies between these propositions in a graph, ensuring that the generated questions are not redundant. However, existing benchmarks that focus solely on evaluating the alignment between text prompts and images often fail to detect external knowledge beyond the text and the factual visual semantics embedded in the image, which is required to address image hallucination.

# Methodology

# Image Hallucination

Factual information refers to data that can be objectively verified and proven true based on evidence or reliable sources. It is an important evaluation criterion in fields that require reliable and accurate information, such as education (Hew et al. 2014). In this paper, we focus on image hallucination, a phenomenon where the images generated by TTI models fail to accurately reflect factual information.

Existing benchmarks that merely evaluate the alignment between the text prompt and the generated image are inadequate for properly assessing image hallucination. Their limitations in evaluating image hallucination can be summarized in two key points: The inability to evaluate factual information beyond the prompt, and the difficulty in identifying accurate visual semantics.

As shown in Figure 1, existing evaluation metrics, such as TIFA (Hu et al. 2023), rely solely on text prompts, which limit their ability to consider factual information not explicitly stated in the prompt. In contrast, our metric leverages GPT-4o’s ability to generate questions and answers based on external factual content not in the prompt but learned by the model. This allows us to assess whether crucial factual information, though not mentioned in the prompt, is accurately reflected in the generated image.

Additionally, existing metrics cannot evaluate whether the visual semantics within an image are hallucinated. This is because the polysemy of text prompts can generate images that are not visually factual while reflecting the word’s meaning. In such cases, metrics based solely on the text prompt fail to distinguish these visual semantics, making it impossible to assess image hallucination. For instance, in the case of “Comb-pattern pottery BCE 8000,” the factual and hallucinated images of the comb-pattern are represented in Figure 1-(b). However, the word “comb-pattern” corresponds to various visual designs and is not confined to a single, unique pattern. Consequently, visual representations that do not match the specific form intended— potentially contradicting historical fact—can still be described as “comb-pattern.” This ambiguity complicates current evaluation metrics to assess the factual information. In contrast, our pipeline utilizes the visual capabilities of GPT-4o by inputting both the prompt and image together to generate an evaluation metric based on factual information. From factual images, we obtain the reasons why these images are considered factual, while from hallucinated images, we acquire discriminative information about how incorrect semantics differ from those in corresponding factual images, as illustrated in Figure 2-(b). This enables us to distinguish accurate visual semantics that reflect factual information among the many possible choices corresponding to the prompt. Therefore, our approach allows for evaluating factual information, including visual semantics like patterns that cannot be fully conveyed through the text prompt alone.

Table 1: Statistical Analysis of I-HallA v1.0 Benchmark Dataset by Domain, Category, Type, and Difficulty Level. Each prompt’s category is based on corresponding textbooks, broadly divided into science and history. The “Type” refers to whether each prompt’s image is factual or hallucinated. The “Difficulty Level” is determined by making five type predictions for each prompt and its corresponding image using GPT-4o. Based on the number of correct predictions, the difficulty is categorized as follows: 0-1 correct predictions are classified as “Hard,” 2-3 correct predictions as “Medium,” and 4-5 correct predictions as “Easy.”   

<html><body><table><tr><td rowspan="2">Domain</td><td rowspan="2">Category</td><td rowspan="2">Type</td><td colspan="3">Level</td><td rowspan="2">Total</td></tr><tr><td>Easy</td><td>Medium</td><td>Hard</td></tr><tr><td rowspan="5">Science</td><td>Physics</td><td>Haucinaled</td><td>2625</td><td>34</td><td>44</td><td>33</td></tr><tr><td>Biology</td><td>Hallucinated</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td>17</td><td>12</td><td>1-2</td><td></td></tr><tr><td>Earth Science</td><td>Factual Hallucinated</td><td>40</td><td>2</td><td>4</td><td>46</td></tr><tr><td></td><td></td><td>33 160</td><td>4</td><td>9</td><td>46</td></tr><tr><td rowspan="7">History</td><td>Western&Africa</td><td>Factual</td><td>14</td><td>16</td><td>24</td><td>：</td></tr><tr><td>Ancient Western&Africa</td><td>Hallucinated</td><td>5</td><td>1 2</td><td>0 8</td><td>15 15</td></tr><tr><td>Medieval</td><td>Factual Hallucinated</td><td>7 5</td><td>0 2</td><td>0 0</td><td>7 7</td></tr><tr><td>Western&Africa Modern</td><td>Factual Hallucinated</td><td>35 24</td><td>1</td><td>3</td><td>39</td></tr><tr><td>Eastern</td><td>Factual</td><td>9</td><td>2 0</td><td>13 1</td><td>39 10</td></tr><tr><td>Ancient</td><td>Hallucinated</td><td>2</td><td>4</td><td>4</td><td>10</td></tr><tr><td>Eastern Medieval</td><td>Factual Hallucinated</td><td>16 4</td><td>0</td><td>0</td><td>16</td></tr><tr><td></td><td>Eastern</td><td>Factual</td><td>12</td><td>3 一</td><td>9 0</td><td>16 13</td></tr><tr><td></td><td>Modern</td><td>Hallucinated</td><td>10</td><td>1</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>2</td><td>13</td></tr><tr><td></td><td></td><td></td><td>143</td><td>17</td><td>40</td><td>：</td></tr></table></body></html>

# I-HallA v1.0: Benchmark for Evaluating Image Hallucination

We propose a curated benchmark, I-HallA v1.0, and an evaluation metric, I-HallA, to assess image hallucination in TTI models. To our knowledge, this is the first benchmark to evaluate image hallucination in TTI-generated images.

Table 2: Statistical Analysis of the I-HallA Metric by Compositions of Interest (CoIs). The dataset includes $1 { , } 0 0 0 \mathrm { Q A }$ sets with their corresponding CoI; 500 each for science and history. “Others” includes 16 in science and 9 in history.   

<html><body><table><tr><td>Domain</td><td>Color</td><td>Counting</td><td>Existence</td><td>Posture</td><td>Relation</td><td>Scene</td><td>Shape</td><td>Size</td></tr><tr><td>Science</td><td>30</td><td>49</td><td>106</td><td>11</td><td>91</td><td>100</td><td>73</td><td>24</td></tr><tr><td>History</td><td>64</td><td>53</td><td>110</td><td>40</td><td>45</td><td>72</td><td>84</td><td>23</td></tr></table></body></html>

As a pioneering effort, our benchmark focuses on the educational domain, where the accuracy of factual information is crucial. Education serves as an ideal starting point for this benchmark study due to the broad and diverse use of factual data. Textbooks are specifically chosen as they encapsulate knowledge accumulated over time, providing wellstructured and reliably categorized content. Within this domain, we choose science and history, two subjects that heavily rely on images for effective learning. As shown in Table 1, in science, our benchmark covers Physics, Biology, and Earth Science. In history, it is organized by geographic regions—Eastern and Western & African—and by periods, such as Ancient, Medieval, and Modern.

We introduce a three-stage pipeline to construct our benchmark dataset and evaluation metric. First, we collect a dataset to address the image hallucination in the educational domain. Next, we enhance this dataset by leveraging GPT-4o’s pre-trained knowledge and visual understanding capabilities. Finally, based on the enhanced dataset, we develop a metric to evaluate image hallucination.

Collecting Our Dataset The first stage for creating a benchmark is the collection process for initial datasets. We engage 10 graduate students to curate prompts from three science (Urone et al. 2020; O’Grady et al. 2021; Neser 2023) and two history textbooks (Jackson J. Spielvogel 2008; Danzer et al. 2008). For each chapter in textbooks, participants extract up to 10 prompts and their corresponding factual images. The prompt $P$ is either derived from textbook figures or selected based on unanimous agreement among participants that it represents key content in the chapter, such as important events, phenomena, or artworks. If a figure corresponding to the prompt $P$ is in the textbook, it is collected as a factual image $I _ { f }$ . In rare cases where no such figure exists, we search for an image related to $P$ on verified websites, such as government-operated ones, and collect it as $I _ { f }$ .

Each participant inputs the curated prompt $P$ into five TTI models, generating 10 images per model. All participants evaluate these images to identify image hallucination that contradicts factual information. The image with the most pronounced hallucination, unanimously agreed upon by all 10 participants, is selected as the representative hallucinated image $I _ { h }$ for that prompt and model, highlighting the most evident hallucination and revealing each model’s limitations.

To conclude, our dataset consists of 200 tuples $\{ P , I _ { f } , I _ { h _ { i } } \} _ { i = 1 } ^ { 5 }$ , each containing a prompt $P$ , a factual image $I _ { f }$ , and five hallucinated images $I _ { h _ { i } }$ , where $i$ represents each TTI model. We collect 100 tuples from the science domain, and the remaining 100 are from history. In total, we assemble a set of 1,200 pairs, each consisting of a prompt and either a factual or hallucinated image.

Enhancing Our Dataset The second stage involves enhancing the collected dataset using GPT-4o to evaluate factual information better. Leveraging GPT-4o, pre-trained on vast data, and equipped with visual understanding, we develop a dataset incorporating external knowledge and visual semantics beyond the text prompt. For each prompt $P$ , we input two sets— $. ( P , I _ { f } )$ and $( P , I _ { h } )$ —into GPT-4o to obtain three aspects: responses, reasonings, and difficulty levels. We use hallucinated images $I _ { h }$ generated by DallE-3 (OpenAI 2023) to capture hallucinations produced even by the latest TTI model. To determine the difficulty levels, we performed the same process five times independently.

The response, determined by GPT-4o, categorizes the input image as either “factual” if accurate or “hallucinated” if not. The reasoning provides the factual justification for this response, incorporating external knowledge beyond the prompt and visual semantic information that supports this assessment. Thus, for each prompt, two types of reasoning are provided: one for correctly identifying a “factual” image and another for identifying a “hallucinated” image. Difficulty levels are determined by evaluating GPT-4o’s accuracy across five inference attempts in discerning the factual information related to the given prompt and image. Using the initial dataset labels as the ground truth, we compare GPT4o’s response to determine correctness. Prompts and images are classified as “Hard” if GPT-4o answered correctly 0-1 times, “Medium” if correct 2-3 times, and “Easy” if correct 4-5 times. We store the reasonings only when GPT-4o provides the correct response.

10 human annotators review and refine all reasonings into a final, well-expressed version, retaining only the unanimously agreed-upon parts. In cases where GPT-4o fails to provide any correct response, the reasoning is developed through group discussion and consensus among all participants. Consequently, our dataset consists of 200 tuples $\{ \boldsymbol { P } , \boldsymbol { I } _ { f } , \boldsymbol { I } _ { h _ { i } } , \boldsymbol { R } , \boldsymbol { D } \} _ { i = 1 } ^ { \tilde { 5 } }$ , where the reasoning $R$ and difficulty levels $D$ have been added to the previously collected dataset.

I-HallA: An Evaluation Metric Using QuestionAnswering The final stage involves developing the evaluation metric, I-HallA, to evaluate the factual accuracy of images generated by TTI models. It employs five multiple-choice QA sets to assess image hallucination based on the curated dataset from previous stages. To analyze the benchmark and results, we introduce classification criteria called Compositions of Interests (CoIs) to categorize the QA sets. We select the most relevant compositions from existing TTI evaluation studies (Hu et al. 2023; Li et al. 2024) that are closely related to image hallucination: color, counting, existence, others, posture, relation, scene, shape, and size. The “others” category applies when a given QA set does not fit into the other CoIs.

To generate the QA sets, we input the prompt $P$ , reasoning $R$ , and CoIs into GPT-4o. Based on the reasoning, GPT4o generates five multiple-choice QA sets per prompt. Each QA set consists of a question targeting factual information,

Prompt: Betsy Ross Flag Factual /Difficulty:Easy Domain/Category: History/Western& Africa Modern Reasoning: The Betsy Ross Flaghas 13alternating red and whitehorizontal stripesandabluecantonwith13 1.0 five-pointed white stars arranged ina circle.... Hallucinated /Difficulty: Easy Q: How many stars are on the flag in the image? A)1 Gold Answer:B B)13 CoI: Counting D)23 I-HallA 0.6

five answer choices (with the fifth option being “None of the above”), and the correct factual answer as the gold answer. Simultaneously, GPT-4o generates the most relevant CoI for each QA set. The QA set generation follows two key guidelines: First, the more factual information an image contains, the more correct answers it should provide, thereby yielding higher scores for more factual images. Second, qualitative information that cannot be visually verified through the image should be excluded in the questions. The 10 participants then review the generated QA sets to ensure that they adhere to these two guidelines. Any disagreements among participants lead to revisions. This process results in $1 { , } 0 0 0 \mathrm { Q A }$ sets and their corresponding CoIs across 200 prompts.

To calculate the score using I-HallA (I-HallA score), we input the image to evaluate, along with the question and five answer choices, into a VQA model and compare the model’s response with the gold answer. There are five questions per image, and the I-HallA score ranges from 0 to 1. The formula can be expressed as follows:

$$
\frac { 1 } { | \mathcal { P } | } \sum _ { p \in \mathcal { P } } \left( \frac { 1 } { | Q ^ { p } | } \sum _ { ( q , c , g ) \in ( Q ^ { p } , C ^ { p } , G ^ { p } ) } \mathbb { I } ( \operatorname { V } \mathbb { Q } \mathbb { A } ( p , q , c ) = g ) \right)
$$

$\mathbb { I } ( \cdot )$ is an indicator function that returns 1 if the condition is satisfied. VQA is the VQA model’s prediction for the given QA. $\mathcal { P }$ is a set of prompts. For the given prompt $p , Q ^ { p } , C ^ { p }$ , and $G ^ { p }$ represent the sets of questions, choices, and gold answers, respectively. $| \cdot |$ is the number of elements in a set.

# Experiments

In this section, we first analyze the statistical characteristics of I-HallA v1.0 across various categories, difficulty levels, and a list of CoIs. We evaluate the recent five text-to-image models with I-HallA, emphasizing our metric robustly and accurately assesses image hallucination. The models include DallE-3 (OpenAI 2023), Stable Diffusion v1.4, Stable Diffusion v1.5 (Rombach et al. 2022), Stable Diffusion $\mathrm { v } 2 . 0$ (StabilityAI 2022), and Stable Diffusion XL-base v1.0 (Podell et al. 2023). Additionally, through human evaluation, we demonstrate that our method strongly correlates with human judgments on evaluating image hallucination. For all experiments, we utilize GPT-4o as the VQA model for the I-HallA.

Table 3: Our benchmark evaluation results on existing TTI models and factual images; We compute the I-HallA score by averaging ratio-based scores across 100 prompts per category (science and history). $\dagger$ means scoring as incorrect if even one out of five QA sets is wrong for each prompt. Each experiment is conducted three times.   

<html><body><table><tr><td rowspan="3">Models</td><td colspan="2">I-HallA Score</td><td colspan="2">I-HallA Score†</td></tr><tr><td>Science</td><td>History</td><td>Science</td><td>History</td></tr><tr><td>SD v1.4</td><td>0.353 ± 0.002</td><td>0.535 ± 0.013</td><td>0.033 ± 0.012</td><td>0.110 ± 0.010</td></tr><tr><td>SD v1.5</td><td>0.309 ± 0.011</td><td>0.533 ± 0.004</td><td>0.030 ± 0.017</td><td>0.117 ± 0.021</td></tr><tr><td>SD v2.0</td><td>0.336 ± 0.006</td><td>0.540 ± 0.014</td><td>0.027 ± 0.021</td><td>0.120 ± 0.010</td></tr><tr><td>SD XL</td><td>0.398 ± 0.015</td><td>0.579 ± 0.012</td><td>0.077 ± 0.050</td><td>0.110 ± 0.066</td></tr><tr><td>DallE-3</td><td>0.661 ± 0.020</td><td>0.666 ± 0.003</td><td>0.227 ± 0.029</td><td>0.133 ± 0.031</td></tr><tr><td>Factual</td><td>0.856±0.002</td><td>0.873 ± 0.006</td><td>0.517 ± 0.038</td><td>0.533±0.015</td></tr></table></body></html>

# Benchmark Analysis

To illustrate the comprehensive scope of I-HallA v1.0, we provide an analysis spanning all categories, difficulty levels, and compositions. Additionally, we demonstrate that GPT4o can be effectively used to develop I-HallA v1.0.

Statistics and diversity As shown in Table 1, in the science domain, we collect 33, 21, and 46 prompts from physics, biology, and earth science textbooks, respectively. In the history domain, we collect prompts from two textbooks. Specifically, we gather 15, 7, 39, 10, 16, and 13 prompts from the Western & African/Ancient, Western & African/Medieval, Western & African/Modern, Eastern/Ancient, Eastern/Medieval, and Eastern/Modern sections, respectively. The number of images collected per prompt includes one factual image and five hallucinated images from different TTI models, totaling six images per prompt. Therefore, a total of 1,200 images are included in I-HallA v1.0.

Additionally, I-HallA provides 1,000 questions with their corresponding compositions of interest, categorized into 9 types, as shown in Table 2. In science, the occurrences are color (30), counting (49), existence (106), others (16), posture (11), relation (91), scene (100), shape (73), and size (24); while in history, they are color (64), counting (53), existence (110), others (9), posture (40), relation (45), scene (72), shape (84), and size (23).

GPT-4o’s ability on image hallucination In our study, we employ GPT-4o to generate reasoning and analyze the difficulty for I-HallA v1.0. GPT-4o assesses whether an image is factual or hallucinated based on the prompt, with hallucinated images generated by the DALL-E 3. Based on the three difficulty levels, GPT-4o classified 160 image-prompt pairs as Easy, 16 as Medium, 24 as Hard in the science domain; 143 as Easy, 17 as Medium, and 40 as Hard in history. When treating “Hard” pairs as incorrect, accuracy is $8 8 \%$ in science and $80 \%$ in history. If cases, where GPT-4o fails to answer any questions correctly, are treated as incorrect, accuracy increases to $91 . 5 \%$ in science and $8 4 . 5 \%$ in history.

The higher number of “Easy” pairs suggests GPT-4o’s strong ability to judge factual information, with “Easy” pairs being $\times 4$ in science and $\times 2 . 5$ in history compared to the total of others. Moreover, as the reasoning and QA sets are thoroughly refined through human review, our benchmark, developed collaboratively by GPT-4o and humans, is wellequipped to evaluate image hallucination.

# Evaluating Text-to-Image Models

Using our I-HallA v1.0, we demonstrate that all five latest TTI models suffer from image hallucination. By analyzing I-HallA scores, we quantitatively show how each model reflects factual information differently, proving that our benchmark objectively evaluates image hallucination. Furthermore, by categorizing the I-HallA scores across different categories and CoIs, we analyze specific situations where each model tends to produce image hallucinations.

Table 3 presents the average I-HallA score and standard deviation from three trials for various TTI models on IHallA v1.0. Higher scores indicate better performance in reflecting factual information without image hallucination. DallE-3 outperforms the Stable Diffusion models in mitigating hallucination across all subjects. Even under the strict standard ( ), where one incorrect answer results in failure, DallE-3 remains the top performer. I-HallA scores are generally higher in history than in science. These scores allow us to assess how effectively current TTI models handle image hallucination. Factual images score in the high 80s, much higher than the average of 0.411 in science and 0.570 in history for the five TTI models, demonstrating that our metric effectively measures factual information. Still, they fall short of perfect, likely due to noise in I-HallA creation or VQA model limitations, which we aim to address in future work.

Figure 4 shows the impact of various TTI models on image hallucination across different categories and compositions. The top graph displays average scores by category, while the middle and bottom graphs represent the average scores for each composition in science and history. Models with larger parameters and newer architectures tend to have higher scores, with DallE-3 generally outperforming other models. However, exceptions exist, such as in the “Eastern Ancient” category, where Stable Diffusion $\mathrm { v } 2 . 0$ scores higher than Stable Diffusion XL-base v1.0 or DallE-3.

In the science and history domains, the I-HallA score for “Posture” and “Size” compositions, respectively, is highest in the Stable Diffusion models, despite their generally lower image quality. This suggests that these models effectively reflect factual information, as our metric evaluates factual accuracy rather than image quality. Therefore, even TTI models that generate high-quality images can score lower if they fail to meet factual criteria. For the “Others” composition in the history domain, the high I-HallA scores of Stable Diffusion v1.4 might be influenced by a smaller sample size.

Stable Diffusion v1.4 Stable Diffusion v1.5 Stable Diffusion v2.0 Stable Diffusion XL-Base-1.0 DallE-3 TTI accuracy (by category) I-HallA v1.0   
S 0.6   
0.24 ■ 0.0 Biology Physics Earth Earth Eastern Eastern Western & Africa Western & Africa Western & Africa Science Ancient Medieval Modern Ancient Medieval Modern TTI accuracy (by composition) on the science category I-HallA v1.0 0.8   
0.246 1 Ldod 1 1 0.0 Color Counting Existence Others Posture Relation Scene Shape Size TTI accuracy (by composition) on the history category I-HallA v1.0 0.8   
I-HallA Score 0.2 0.46 00000 0.0 Color Counting Existence Others Posture Relation Scene Shape Size

![](images/0f516e346335f533691873f088f623929e1fc97d6860a9a9a45b24d207bee7b9.jpg)  
Figure 4: I-HallA scores from five different TTI models across different categories and compositions. The factual information of images generated by TTI models using the prompts from I-HallA v1.0 is evaluated using the I-HallA metric. The I-HallA scores in this figure represent the average scores of each TTI model, calculated across different categories and compositions.   
Figure 5: Plot of I-HallA scores from GPT-4o and human evaluations. Blue circles indicate average scores from 53 participants per question, with score distributions depicted via violin plots. Stars emphasize GPT-4o’s results, which closely align with human judgments, demonstrating a strong correlation between the model and human evaluators.

# Exploring the Reliability of I-HallA Through Human Evaluation

By calculating the correlation between the previously mentioned experimental results and the human evaluation of IHallA v1.0, we demonstrate that our benchmark aligns well with human judgment in assessing image hallucination.

We randomly sample 10 prompts from I-HallA v1.0, including 4 factual and 3 hallucinated images from each of two great TTI models in I-HallA: DallE-3 and Stable Diffusion XL-base v1.0. We collect I-HallA scores from 53 participants, guiding them to answer questions based on the images provided, following the same approach as GPT-4o. Figure 5 shows the average I-HallA scores and standard deviations for each image. Even the image with the largest score difference shows a variance of only about 0.2, indicating that the results are very similar to human evaluations.

To verify this quantitatively, we calculate correlations between GPT-4o’s I-HallA scores and human judgments on image hallucination. For the three metrics—Pearson’s $r$ , Spearman’s $\rho$ , and Kendall’s $\tau$ —we observe very high correlations of 0.952, 0.950, and 0.889, respectively.

# Conclusion

In conclusion, we propose the I-HallA v1.0 benchmark as the first to address image hallucination in text-to-image generation by evaluating factual information. This benchmark overcomes the limitations of previous methods, which could not accurately assess factual information. Developed through a three-stage pipeline using GPT-4o and thorough human review, it evaluates hallucination in 200 factual and 1,000 hallucinated images from five text-to-image models. Our results confirm that the benchmark effectively measures image hallucination and aligns well with human judgment. We hope that our benchmark and evaluation metric will be instrumental in resolving image hallucination in the future.