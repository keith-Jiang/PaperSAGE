# Toward Falsifying Causal Graphs Using a Permutation-Based Test

Elias Eulig1,2,\*,†, Atalanti A. Mastakouri3, Patrick Bl¨obaum3, Michaela Hardt4,\*, Dominik Janzing3

1German Cancer Research Center (DKFZ) 2Heidelberg University 3Amazon Research Tu¨bingen 4University Hospital Tu¨bingen elias.eulig@dkfz.de, {atalanti,janzind}@amazon.de, bloebp $@$ amazon.com, michaela.hardt $@$ uni-tuebingen.de

# Abstract

Understanding causal relationships among the variables of a system is paramount to explain and control its behavior. For many real-world systems, however, the true causal graph is not readily available and one must resort to predictions made by algorithms or domain experts. Therefore, metrics that quantitatively assess the goodness of a causal graph provide helpful checks before using it in downstream tasks. Existing metrics provide an absolute number of inconsistencies between the graph and the observed data, and without a baseline, practitioners are left to answer the hard question of how many such inconsistencies are acceptable or expected. Here, we propose a novel consistency metric by constructing a baseline through node permutations. By comparing the number of inconsistencies with those on the baseline, we derive an interpretable metric that captures whether the graph is significantly better than random. Evaluating on both simulated and real data sets from various domains, including biology and cloud monitoring, we demonstrate that the true graph is not falsified by our metric, whereas the wrong graphs given by a hypothetical user are likely to be falsified.

Project — https://eeulig.github.io/dag-falsification

Call DAG of microservices Domain expert Causal DAG of latencies B B A C (a) Domain expert estimates causal DAG $\hat { \mathcal G }$   
1. Unidentified confounding 2. Unknown type I error rate Cluster workload see Figure 2 B A   
3. B's latency increases $\star$ A skips call to $\mathrm { C } \to \mathrm { C ^ { \prime } s }$ latency decreases $\underbrace { \mathbb { O } ^ { 3 } } _ { \texttt { \tiny B } ^ { 0 } } \underbrace  \mathrm { ~ \mathbb { A } ~ } _ { \texttt { \tiny B } } \mathrm { ~ \texttt ~ { ~ C ~ } ~ } _ { \texttt { \tiny B } } ^ { \texttt { \tiny B } ^ { * } } \mathrm { ~ \Pi ~ } _ { \mathrm { l e a d s ~ t o } } \mathrm { ~ \mathbb { B } _ { \Xi } \underbrace { \right. ~ \mathrm { { A } _ { \Xi } } \left. ~ \mathrm { { C _ { \Xi } } ~ } } _ { \texttt { \tiny A } } ~ } _ { \texttt { \tiny B } }$ (b) Possible sources of CI violations of $\hat { \mathcal G }$

# 1 Introduction

Directed Acyclic Graphs (DAGs) are a core concept of causal reasoning as they form the basis of structural causal models (SCMs) which can be used to predict the effect of interventions in a causal system or even to answer counterfactual questions. For that reason, they have numerous applications, including biology (Sachs et al. 2005; Huber et al. 2007; Pingault et al. 2018), medicine (Shrier and Platt 2008) and computer vision (Wang and Sahbi 2013; Chalupka, Perona, and Eberhardt 2015; Wang et al. 2020).

Nevertheless, for many real-world systems the true causal relationships, represented in the form of a DAG, are often not readily available. If randomized controlled trials are not possible, inferring the DAG from passive observational data alone is a hard problem that rests on strong assumptions on statistical properties (e.g. causal faithfulness), functional relationships, noise distributions, or graph constraints (Shimizu et al. 2006; Peters and Bu¨hlmann 2014; Chickering 1996; Chickering, Heckerman, and Meek 2004; Claassen, Mooij, and Heskes 2013; Mooij et al. 2016). These assumptions are often violated in practice. Instead of relying on discovery from observational data, domain experts can describe known dependencies in a system (Fig. 1; (a)). This approach, however, is subject to human error and incomplete knowledge. This is particularly problematic for applications in causal inference, where a wrong graph structure will lead to wrong conclusions about the effect of interventions.

Efforts towards quantitatively evaluating the consistency of a given graph (either originating from a domain expert or a causal discovery algorithm) using observational data alone are of utmost importance. However, existing attempts towards this direction report the raw number of inconsistencies between DAG and observed data without a baseline of how many such inconsistencies are to be expected in the first place (Textor et al. 2016; Reynolds et al. 2022). Striving for zero is unrealistic and the acceptable number of violations depends on many factors including the size and complexity of the graph. On real-world data, the fraction of conditional independence violations for expert-elicited graphs can be surprisingly high due to unidentified confounding, high type I error rates of the conditional independence tests (Shah and Peters 2020), or complex interactions between the variables (Fig. 1; (b)). Other methods require the existence and knowledge of DAGs from a related system (Pitchforth and Mengersen 2013), or are similarly difficult to interpret due to a missing baseline (Madigan, York, and Allard 1995; Friedman and Koller 2003).

![](images/4100e8e7d20c7cdadf728e1e03a7301b8505f5a922894533516e9d4f865bbfc2.jpg)  
Figure 2: Type I error rate at $\alpha = 5 \%$ for different sizes $D$ of the conditioning set for one parametric (partial correlation) and two nonparametric CI tests, KCI (Zhang et al. 2011) and GCM (Shah and Peters 2020). Data (solid: $N = 1 0 0$ , dashed: $N = 5 0 0$ ) were sampled from gaussian-linear conditionals. More details are given in Supp. A.1.

To overcome these drawbacks, in this work we develop a novel metric1 to evaluate a given graph using observational data alone. Aiming to understand if a number of violations of a user-specified graph is high or low, we compare it against a baseline that we construct by randomly permuting its nodes. Through this comparison we can shed light onto the question if the violations of a user-specified graph are false-positives or point to real deficiencies of the graph.

# 2 Related Work

In this section, we review existing works that aim to quantify the consistency of a given DAG with observed data.

The R package dagitty (Textor et al. 2016) implements functions to evaluate DAG-dataset consistency by testing conditional independence (CI) relations implied by the graph structure via the d-separation (Pearl 1988) and global Markov condition. However, directly using the number of violations of graph-implied CIs as a metric to evaluate a given graph is not suitable for real-world applications because there exists no nonparametric CI test with valid level2 over all distributions and thus the probability of type I errors remains unknown in practice (Shah and Peters 2020, Th. 2), c.f. Fig. 2. Therefore, without a baseline comparison, the raw number of violations (absolute or fraction) does not provide the user with a meaningful measure of whether or not the observed inconsistencies of the given graph are significant.

Reynolds et al. (2022) validate a given DAG (6 nodes, 8 edges) that relates exposure to spaceflight environment to the performance and health outcomes of rats and mice. Together with testing CIs implied by d-separations using dagitty, they test whether dependencies implied by the graph lead to marginal dependencies in the observations (faithfulness). Similar to (Textor et al. 2016) interpreting the results of those tests is challenging, without a baseline comparison. Our metric overcomes this drawback by providing such a baseline comparison to estimate whether the observed inconsistencies are significant.

Pitchforth and Mengersen (2013) suggest a number of questions to validate expert-elicited Bayesian Networks (BNs). In particular, the authors propose to validate a given BN by verifying that it is similar to BNs from the same domain already established in the literature. Nevertheless, the framework does not provide a quantified measure and many of the questions assume the existence of comparison models in the literature, which may not be available in many domains. In contrast, we propose a quantitative metric, constructed via a surrogate baseline and not reliant on the existence of similar models from the same domain.

Finally, numerous works exist on Bayesian structure learning (e.g., Madigan, York, and Allard 1995; Friedman and Koller 2003; Giudici and Castelo 2003) where the posterior $p ( \mathcal { G } \mid \mathcal { D } ) \propto p ( \mathcal { D } \mid \mathcal { G } ) p ( \mathcal { G } )$ of a graph $\mathcal { G }$ is estimated given some data $\mathcal { D }$ . While the likelihood $p ( \mathcal { D } \mid \mathcal { G } )$ could be used as a measure for the goodness of $\mathcal { G }$ , the problem of a missing baseline remains.

# 3 Background

# 3.1 Notation

This work aims to evaluate the consistency of a given DAG $\hat { \mathcal { G } } ~ = ~ ( V , \hat { \mathcal { E } } )$ with vertices $V = \{ 1 , . . . , n \}$ , random variables $X = \{ X _ { i } : i \in V \}$ , and edges $\hat { \mathcal { E } } \subseteq V ^ { 2 }$ using observations $\mathcal { D } = \{ { \cal X } ^ { ( 1 ) } , . . . , { \cal X } ^ { ( N ) } \}$ i.i.d. sampled from the joint distribution3 $\mathsf { \bar { P } } ( { \pmb X } )$ . We assume the existence of some unknown true causal DAG $\mathcal G ^ { * }$ and that $P ( \pmb { X } )$ satisfies the causal Markov condition relative to $\mathcal G ^ { * }$ .

In the following, we refer to $X \ \underline { { { \mathrm { 1 1 } } } } \mathbf { \mathcal { D } } \ Y | Z$ as the outcome of a CI test using $\mathcal { D }$ , where $X$ and $Y$ are conditionally independent given $Z$ (and likewise $X ~ \not \sim \not \sim Y | Z$ to denote conditional dependency). Note that whether independence is rejected is both a property of $\mathcal { D }$ and the choice of a particular CI test. In this work, to denote that a set of nodes $Z$ d-separates (Pearl 1988) $X$ from $Y$ , we write: $X$ 1 $_ { \mathcal { G } } Y | Z$ .

For some graph $\mathcal { G } = ( V , \mathcal { E } )$ , we call a node $i$ a parent of node $j$ if $( i , j ) \in \mathcal { E }$ and denote with $\mathrm { P a } _ { j } ^ { \mathcal { G } }$ the set of all parents of $j \in \mathcal G$ $\mathrm { P a } _ { j } ^ { \mathcal { G } } = \varnothing$ iff $j$ is a root node). Furthermore, for nodes $i , j$ if there exists a direct path $i  \cdots  j$ we call $i$ an ancestor of node $j$ and $j$ a descendant of $i$ . We denote with $\operatorname { A n c } _ { j } ^ { \mathcal { G } }$ the set of all ancestors of $j \in \mathcal G$ and with $\mathrm { N D } _ { i } ^ { \mathcal { G } }$ the set of all non-descendants of $i \in \mathcal G$ .

# 3.2 Validating Local Markov Conditions

One common approach to evaluate a DAG using observed data is by means of statistical testing of independence relations implied by $\hat { \mathcal G }$ (e.g. Textor et al. 2016; Reynolds et al. 2022). In the following, we will formally introduce this test and elaborate on why it is unsuitable to use as a metric directly. In Sec. 4 we will then present a baseline to overcome said drawbacks.

One of the standard assumptions in causal inference is the causal Markov condition, which allows us to factorize a joint probability distribution $P ( \pmb { X } )$ over the variables $X$ of $\mathcal { G }$ into the product

$$
P ( X ) = P ( X _ { 1 } , X _ { 2 } , . . . , X _ { n } ) = \prod _ { i = 1 } ^ { n } P ( X _ { i } | \mathbf { P } \mathbf { a } _ { i } ^ { \mathcal { G } } ) \ .
$$

We can equivalently formulate this as the parental Markov condition:

Theorem 1 (Parental Markov condition (Pearl 2009)). A probability distribution $P$ is Markov relative to a DAG $\mathcal { G } =$ $( V , \mathcal { E } )$ , iff $X _ { i }$ 上 $_ P \ N D _ { i } ^ { \mathcal { G } } \setminus P a _ { i } ^ { \mathcal { G } } \mid P a _ { i } ^ { \mathcal { G } }$ .

For the remainder of this work we will use the terms parental Markov condition and local Markov condition (LMC) interchangeably. To test whether a distribution satisfies the parental Markov condition relative to a DAG we can thus list the CIs entailed by the DAG via the graphical ${ \mathrm { d } }$ -separation criterion (Pearl 1988) and test whether those are satisfied or violated by the distribution at hand.

Definition 1 (Parental triples). For some graph $\mathcal { G }$ , we refer to the ordered triple $( i , j \in \mathbf { N D } _ { i } ^ { \mathcal { G } } \backslash \mathbf { P a } _ { i } ^ { \mathcal { G } }$ , $Z = \bar { \mathrm { P a } _ { i } ^ { \mathcal { G } } } ^ { \cdot }$ ) as parental triple and denote the set of all such triples implied by $\mathcal { G }$ as $\mathrm { T _ { P a } ^ { \bar { \mathcal { G } } } }$

According to Theorem 1 every parental triple $( i , j , Z ) \in \mathrm { T } _ { \mathrm { P a } } ^ { \mathcal { G } }$ implies the CI $X _ { i } ~ \bot \bot P ~ X _ { j } ~ \mid ~ Z$ . Similar to Textor et al. (2016) we will now introduce the notion of violations of LMC:

Definition 2 (Violations of LMCs). We denote with VLGˆM,CD the set of triples $( i , j , Z ) \in \mathrm { T } _ { \mathrm { P a } } ^ { \hat { \mathcal { G } } }$ , for which $( i , j )$ is an ordered pair, and we observe LMC violations on data $\mathcal { D }$ , i.e.

$$
\begin{array} { r } { V _ { \mathrm { L M C } } ^ { \hat { \mathcal G } , \mathcal D } = \Big \{ ( i , j , Z ) \in \mathrm { T } _ { \mathrm { P a } } ^ { \hat { \mathcal G } } : X _ { i } \ \mathcal { N } _ { \mathcal D } X _ { j } \ | \ Z \Big \} \ . } \end{array}
$$

Furthermore, we denote the fraction of LMC violations with

$$
\phi _ { \mathrm { L M C } } ^ { \hat { \mathcal { G } } , \mathcal { D } } = \frac { \vert V _ { \mathrm { L M C } } ^ { \hat { \mathcal { G } } , \mathcal { D } } \vert } { \vert \mathrm { T } _ { \mathrm { P a } } ^ { \hat { \mathcal { G } } } \vert } .
$$

Using the two metrics from Definition 2 directly to measure the goodness of a user-given graph is not suitable for realworld applications due to the reasons detailed in Sec. 2, particularly the unknown type I error rate of a $\mathrm { C I }$ test (c.f. Shah and Peters (2020) & Fig. 2).

# 4 A Baseline for Violations of Local Markov Conditions

sacruesisnesduifnfitchienptrteovimoeuasssuercetitohnesc,othnesismtetnrciycs $V _ { \mathrm { L M C } } ^ { \hat { \mathcal { G } } , \mathcal { D } }$ apnhd. $\phi _ { \mathrm { L M C } } ^ { \hat { \mathcal { G } } , \mathcal { D } }$ In the following, we therefore derive a baseline to compare the number of LMC violations $V _ { \mathrm { L M C } } ^ { \hat { \mathcal { G } } , \mathcal { D } }$ to.

# 4.1 Finding a Suitable Baseline

Motivated by a very skeptical view whether the user specified graph is related to the observed independence structure at all, we are interested in a baseline that is a random draw of a set of conditional independence statements. For example, consider the dependence structure of microsvervices in a distributed system, where multiple effects can lead to violations of independence statements on observed data (Fig. 1).

In general, there can be different reasons why the pattern of observed conditional independences appears unrelated to $\hat { \mathcal G }$ . On the one hand, the domain expert who provided $\hat { \mathcal G }$ may have messed up causal links and directions entirely. But even if all the links of $\hat { \mathcal G }$ are correct, additional confounding and violations of faithfulness4 can mess up the independence structure. In both cases, DAG and independences appear random relative to each other regardless of whether we think the DAG or the pattern of independences to be random. Our experiments show that ‘better than random’ is a surprisingly high bar, and both domain experts and causal discovery algorithms fail to meet it in many settings.

We now introduce a set of properties that draws from the random baseline should satisfy:

P1: They should infer the same number $m$ of conditional independences as the given graph $\hat { \mathcal G }$ .   
P2: They should partition $m$ into $m _ { r }$ conditional independences with conditioning sets of size $r$ with the same numbers $m _ { r }$ as $\hat { \mathcal G }$ does.   
P3: Conditional independences inferred by draws from the baseline should be closed under the semi-graphoid axioms (Pearl and Paz 1986; Geiger, Verma, and Pearl 1990).

P1 ensures that the number of observed LMC violations on the baseline are comparable to those observed on $\hat { \mathcal G }$ . E.g. a random baseline inferring fewer CIs than $\hat { \mathcal G }$ will also result in much fewer LMC violations.5 If $\hat { \mathcal G }$ is sparse because it has been drawn with the implicit intention of explaining only highly significant dependences, we should not reject it just because it shows many LMC violations with respect to our significance level. Instead, we should rather benchmark $\hat { \mathcal G }$ against a random guess that infers equally many independences. P2 ensures that type I and type II errors, when testing implied CIs on the baseline, are comparable to those on $\hat { \mathcal G }$ .

E.g. a random baseline inferring mostly CIs with small conditioning sets may exhibit fewer LMC violations than $\hat { \mathcal G }$ by virtue of a smaller type I error rate (c.f. Fig. 2). Lastly, P3 ensures that the CI statements from the baseline do not imply additional CIs (which are not already in the set) via the semi-graphoid axioms. For example, due to decomposition, $X \perp Y \cup W | Z \Rightarrow X \perp Y | Z \& X \perp W | Z .$

A natural choice for a baseline that satisfies all the requirements above can be constructed by sampling nodepermutations of $\hat { \mathcal G }$ . More formally, let $S _ { n }$ denote the set of permutations $\pi$ on the vertices $\{ 1 , . . . , n \}$ of some graph $\mathcal { G }$ . For any permutation $\pi \in S _ { n }$ we denote with $\sigma _ { \pi } ( \mathcal { G } )$ the graph for which the edge $i  j$ exists iff $\pi ( i ) \to \pi ( j )$ exists in $\mathcal { G }$ . Because of the one-to-one correspondence between $\pi$ and $\sigma$ we will drop the subscript and in the following refer to $\sigma \in S _ { \mathcal { G } }$ as one node-permutation of $\mathcal { G }$ .

We can then construct our baseline by sampling random node-permutations $\sigma \in S _ { \hat { \mathcal { G } } }$ of the given graph $\hat { \mathcal G }$ . Let $O ( \hat { \mathcal { G } } )$ define the orbit of $\hat { \mathcal G }$ under $S _ { \hat { \mathcal { G } } }$ , i.e., the set of DAGs obtained via permutations. Since all DAGs in $O ( \hat { \mathcal { G } } )$ imply the same CIs as $\hat { \mathcal G }$ up to renaming of variables, they satisfy P1 and P2. Furthermore, they satisfy P3 since the set of CIs entailed by a graph are closed under the semi-graphoid axioms.6,7

Note that the mapping $S _ { \hat { \mathcal { G } } }  O ( \hat { \mathcal { G } } )$ defined by $\sigma \mapsto \sigma ( \mathcal G )$ is in general not one-to-one because there will often be a non-trivial subgroup that leaves $\hat { \mathcal G }$ invariant (the stabilizer subgroup $\operatorname { S t a b } ( \hat { \mathcal { G } } )$ of $S _ { \hat { \mathcal { G } } } .$ ).8

Proposition 1. Uniform sampling of permutations from the set of all node permutations $S _ { \hat { \mathcal { G } } }$ results in uniform sampling from the DAGs in the orbit $O ( \hat { \mathcal { G } } )$ .

A proof is provided in Supp. D.1. Further, $O ( \hat { \mathcal { G } } )$ decomposes into Markov equivalence classes of equal size. This can easily be seen by the same argument when we consider the action of $S _ { \hat { \mathcal { G } } }$ on the set of Markov equivalence classes and introduce the corresponding (larger) stabilizer subgroup. Thus, uniform sampling of permutations from the set of all node permutations $S _ { \hat { \mathcal { G } } }$ also results in uniform sampling from the Markov equivalence classes in $O ( \hat { \mathcal { G } } )$ .

# 4.2 A Permutation Test to Evaluate DAGs

Using the above baseline we state the following null hypothesis:

Hypothesis $H _ { 0 }$ : The DAG $\hat { \mathcal G }$ is drawn uniformly at random from some distribution $Q$ on the set of DAGs that is invariant under permutations of nodes, that is $Q ( \mathcal G ) = Q ( \sigma ( \mathcal G ) )$ for all $\sigma \in S _ { \mathcal { G } }$ .

To test this hypothesis we consider the number of violations VLGˆM,CD as test statistics and build the null via

$$
\begin{array} { r l } & { V _ { \mathrm { L M C } } ^ { \sigma ( \hat { \mathcal G } ) , \mathcal D } \big | \colon } \\ & { \qquad p _ { \textsc { l M C } } ^ { \hat { \mathcal G } , \mathcal D } = \operatorname* { P r } \left( \big | V _ { \mathrm { L M C } } ^ { \sigma ( \hat { \mathcal G } ) , \mathcal D } \big | \leq \big | V _ { \mathrm { L M C } } ^ { \hat { \mathcal G } , \mathcal D } \big | \right) \ . } \end{array}
$$

Proposition 2. pLMC is a valid $p$ -value, i.e. if $H _ { 0 }$ is true, then $P r \left( p _ { L M C } ^ { \hat { \mathcal { G } } , \mathcal { D } } \leq \alpha \right) \leq \alpha$ .

A proof is provided in Supp. D.2. Computing the quantity in (4) using all $n !$ permutations is infeasible for large $n$ . Therefore, we approximate it via Monte Carlo sampling with $T$ random permutations: $\{ \sigma _ { i } \sim S _ { \hat { \mathcal { G } } } \} _ { i = 0 } ^ { T }$ . For an estimated pvalue we can also report binomial proportion confidence intervals.

Another quantity that proves to be useful in practice is the fraction of DAGs in $O ( \hat { \mathcal { G } } )$ that are Markov equivalent to $\hat { \mathcal G }$ . To this end we first define

$$
V _ { \mathrm { T P a } } ^ { \mathcal { G } ^ { \prime } , \mathcal { G } } = \{ ( X _ { i } , X _ { j } , Z ) \in \mathbf { T } _ { \mathbb { P a } } ^ { \mathcal { G } ^ { \prime } } : X _ { i } \ \mathcal { H } _ { \mathcal { G } } \ X _ { j } \ | \ Z \}
$$

as the set of all ordered triples that are parentally ${ \mathrm { d } }$ -separated in $\mathcal { G } ^ { \prime }$ but not d-separated in $\mathcal { G }$ .

Proposition 3. Suppose some given graph Gˆ. If VTσP(aGˆ),Gˆ $\varnothing _ { ; }$ , then $\sigma ( \hat { \mathcal { G } } )$ and $\hat { \mathcal G }$ are Markov equivalent.

The proof is provided in Supp. D.3. Using this graphical criterion we can define a second metric

$$
\begin{array} { r l } & { p _ { \mathrm { T P a } } ^ { \hat { \mathcal { G } } } : = \mathrm { P r } \left( | V _ { \mathrm { T P a } } ^ { \sigma ( \hat { \mathcal { G } } ) , \hat { \mathcal { G } } } | \leq | V _ { \mathrm { T P a } } ^ { \hat { \mathcal { G } } , \hat { \mathcal { G } } } | \right) } \\ & { \qquad = \mathrm { P r } \left( | V _ { \mathrm { T P a } } ^ { \sigma ( \hat { \mathcal { G } } ) , \hat { \mathcal { G } } } | = 0 \right) ~ , } \end{array}
$$

twurheicohf $\hat { \mathcal G }$ nisbeaubsoeudt tohempeoassuirbel ehocawuisnaflorormdaetriivnegtsh.eI $\mathrm { C I }$ $p _ { \mathrm { T P a } } ^ { \mathcal { G } } >$ $\alpha$ , for some prespecified threshold $\alpha$ , then the number of Markov equivalent DAGs in $O ( \hat { \mathcal { G } } )$ is large and consequently $\hat { \mathcal G }$ provides us with limited information about the true graph in the sense of testing of CIs. For an information-theoretic interpretation of our test see Supp. F.

# 4.3 Interpretation of pLMC and $p _ { \mathbf { T P a } }$

We note that p-values are often misinterpreted and misused in practice (Vidgen and Yasseri 2016; Wasserstein and Lazar 2016; Amrhein, Greenland, and McShane 2019) and therefore provide the following interpretation based on Popper’s theory of falsification (Popper 2005). According to this, every scientific theory must admit potential falsifiers, i.e. measurable observations that would falsify the theory. Increasing confidence in such theory can then only come from observations that it permits, i.e. as it withstands attempts to falsify it.

The test $\bar { p } _ { \mathrm { T P a } } ^ { \hat { \mathcal { G } } }$ provides us with a measure of the falsifiability of a given graph $\hat { \mathcal G }$ . If the number of random DAGs that are Markov equivalent to $\hat { \mathcal G }$ is large (and consequently $p _ { \mathrm { T P a } } ^ { \hat { \mathcal { G } } }$ is large) this limits the falsifiability of $\hat { \mathcal G }$ via CI testing. Contrary, if $p _ { \mathrm { T P a } } ^ { \hat { \mathcal { G } } }$ is small, the CIs entailed by $\hat { \mathcal G }$ are ‘characteristic’ and it admits many potential falsifiers. Based on these considerations, we propose the following interpretation of our tests for practitioners:

(a) If $p _ { \mathrm { T P a } } ^ { \hat { \mathcal { G } } } \leq \alpha$ , $\hat { \mathcal G }$ is falsifiable by testing implied CIs.

(b) If (a) and further $p _ { \mathrm { L M C } } ^ { \hat { \mathcal { G } } , D } > \alpha$ , then $\hat { \mathcal G }$ is falsified. (c) If (a) and further $p _ { \mathrm { L M C } } ^ { \hat { \mathcal { G } } , \mathcal { D } } \leq \alpha$ , then there is no CI-based evidence against $\hat { \mathcal G }$ and we cannot falsify $\hat { \mathcal G }$ using our test. Consequently, $\hat { \mathcal G }$ is corroborated (but not verified).

# 4.4 Relating $p _ { \mathbf { L M C } }$ to the Identification of Cause-Effect Pairs

We now want to present an interpretation of $p _ { \mathrm { L M C } } $ which is more closely related to causal questions. To this end, consider the following task.

Task 1 (Identification of unconfounded cause-effect pairs). Identify all ordered pairs $( X _ { i } , X _ { j } )$ such that there is a directed path from $X _ { i }$ to $X _ { j }$ and no $X _ { k }$ with $k \neq i , j$ that has a directed path to $X _ { i }$ and a directed path to $X _ { j }$ that does not go through $X _ { i }$ .

We can then ask whether $\hat { \mathcal G }$ performed better at Task 1 than a random baseline. However, in analogy to P1 above, the baseline should satisfy the following property:

P4: Draws from the random baseline should infer the same number of unconfounded cause-effect pairs as the given graph $\hat { \mathcal G }$ .

Likewise to P1 & P2, all DAGs in $O ( \hat { \mathcal { G } } )$ satisfy P4 since they preserve $\hat { \mathcal G }$ up to a renaming of variables. By defining the number of wrongly inferred causal effects as test statistics and building the null via the node-permutation baseline, we can compute a $\boldsymbol { \mathrm { p } }$ -value $p _ { \mathrm { C E } }$ .

We will now see that under the assumption of faithfulness and knowledge of a single non-effect node, our metric $p _ { \mathrm { L M C } } $ is loosely related to $p _ { \mathrm { C E } }$ .

Theorem 2. Suppose an additional node $X _ { 0 }$ for which we know (e.g. through strong domain knowledge) that it is not an effect of any of the $1 , \ldots , n$ nodes, but satisfies $X _ { 0 } ~ \not \perp \ X _ { l }$ , $\forall l = 1 , \ldots , n$ . Further, let the joint distribution $P ( X _ { 0 } , X _ { 1 } , . . . , X _ { n } )$ be Markov and faithful relative to some DAG $\mathcal { G }$ with these $n + 1$ nodes. Then $( X _ { i } , X _ { j } )$ with $i , j = 1 , \ldots , n$ is an unconfounded cause-effect pair if and only if the following two conditions hold:

$$
\begin{array} { l } { { X _ { i } \not \downarrow _ { P } X _ { j } } } \\ { { X _ { 0 } \not \perp \underline { { { \vert } } } _ { P } X _ { j } \vert X _ { i } . } } \end{array}
$$

The proof is provided in Supp. D.4. We conclude that in our idealized scenario, testing whether $\hat { \mathcal G }$ is better than random at Task 1, is similar to testing whether $\hat { \mathcal G }$ performs better than random in identifying all pairs $( X _ { i } , X _ { j } )$ for which both conditions (7) and (8) hold. Here, the baseline is generated by the permutation group $S _ { n - 1 }$ , that is, the stabilizer group of $X _ { 0 }$ in $S _ { n }$ .

# 5 Experiments

In the following we evaluate our proposed metric on synthetic and real data for which the true DAG is known or a consensus graph is established in the literature. Additionally, we introduce a novel dataset from cloud monitoring where the reversed call graph provides an estimate of the true causal graph (Fig. 1; (a)), thus providing a useful test beyond synthetic and existing real-world datasets.

# 5.1 Experimental Setup

We conduct experiments with two different sources of given graphs: Emulated domain experts with partial knowledge of either a subset of nodes or a subset of edges of the true DAG $\mathcal { G } ^ { * } = ( V , \mathcal { E } ^ { * } )$ and causal discovery algorithms, which are a popular choice to estimate a DAG in the absence of domain expertise. Note that all $\hat { \mathcal G }$ in our experiments differ from $\mathcal { G } ^ { * }$ in a structured, i.e. non-random, way.

Node Domain Expert (DE- $V$ ) This model mimics the situation where a domain expert knows all causal edges for some subset $K \subseteq V$ of the nodes in the system and knows the overall sparsity of the DAG. For all the other nodes, however, the expert has no domain knowledge and thus assigns edges randomly between pairs of nodes not both in $K$ . We define different levels of DE- $V$ to emulate the fraction of nodes for which there exists domain knowledge, i.e. $| K | / | V |$ , where $| K | / | V | = 0$ corresponds to the situation where the domain expert has no knowledge and $| K | / | V | = 1$ corresponds to $\hat { \mathcal { G } } = \mathcal { G } ^ { * }$ . More details are given in Supp. A.2.

Edge Domain Expert (DE- $\mathcal { E }$ ) This model mimics a domain expert with edge-specific knowledge about $\mathcal { G } ^ { \ast }$ . To construct $\hat { \mathcal G }$ , we randomly remove and flip some of the true edges and add some new ones. By construction $\hat { \mathcal G }$ of a DE- $\mathcal { E }$ is related to the Structural Hamming Distance (SHD) (Acid and de Campos 2003; Tsamardinos, Brown, and Aliferis 2006) and thus the desired similarity of a given graph can be controlled by means of the SHD. We characterize different DE- $\mathcal { E }$ by the $\operatorname { S H D } ( \hat { \mathcal { G } } , \mathcal { G } ^ { * } )$ they entail (or $\mathrm { S H D } ( \hat { \mathcal { G } } , \mathcal { G } ^ { * } ) / | \mathcal { E } ^ { * } |$ to compare systems with different sparsity), where $\mathrm { S H D } ( \hat { \mathcal { G } } , \mathcal { G } ^ { * } ) / | \mathcal { E } ^ { * } | = 0$ corresponds to $\hat { \mathcal G } = \overline { { \mathcal G } } ^ { * }$ . More details are given in Supp. A.3.

Causal Discovery Algorithms The proposed test can also be applied to DAGs inferred by causal discovery algorithms. However, as many algorithms use (some of the) CIs either explicitly (constraint-based) or implicitly (scorebased) for constructing the DAG, evidence in favor of an inferred DAG can only come from those CIs that were neither used by the algorithm, nor implied, via semi-graphoid axioms (Pearl and Paz 1986; Geiger, Verma, and Pearl 1990), by those CIs used by the algorithm. Nonetheless, we evaluate our test on graphs inferred by LiNGAM (Shimizu et al. 2006), CAM (Bu¨hlmann, Peters, and Ernest 2014), and NOTEARS (Zheng et al. 2018). We chose those algorithms as they are not solely based on CIs (note, however, that e.g. in LinGAM independence of noise entails CI). More details are given in Supp. C.3.

# 5.2 Synthetic Data and Graphs

For evaluating our method on synthetic data we sample random $\mathcal { G } ^ { \ast }$ under the Erdo˝s-R´enyi model (Erdo˝s and Re´nyi 1959) with $n \in \{ 1 0 , 2 0 , 3 0 \}$ nodes and an expected degree $d \in \{ 1 , 2 , 3 \}$ , denoted as ER-n- $d$ . To generate synthetic data from $\mathcal { G } ^ { \ast }$ , conditionals are modeled as additive noise models $X _ { i } = \overline { { f _ { i } ( \mathbf { P a } _ { i } ^ { \mathcal { G } ^ { * } } ) } } + N _ { i }$ with $N _ { i }$ sampled from a normal distribution and $f _ { i }$ either being random (nonlinear) MLPs, or a linear combination of the node’s parents. The exogenous variables are sampled from a standard normal, uniform, or Gaussian mixture distribution.

For all experiments on synthetic data we sample $T = 1 0 ^ { 3 }$ node permutations and use datasets with $N ~ { \mathit { \Phi } } = ~ 1 0 ^ { 3 }$ observations. To investigate the effect of $N$ and $T$ on $p _ { \mathrm { L M C } }$ we run ablation studies on nonlinear data with $N , T \in$ $\{ 1 0 ^ { 1 } , 1 0 ^ { 2 } , 1 0 ^ { 3 } , 1 0 ^ { 4 } \}$ . More information on implementation and parameter choices is provided in Supp. A.4.

# 5.3 Real Data

To evaluate our proposed metric on real-world data, we consider three datasets with established consensus graphs serving as ground truth. We provide further details on the data in Supp. A.5.

Protein Signaling Network (Sachs et al. 2005) This open dataset contains quantitative measurements of the expression levels of $n = 1 1$ phosphorylated proteins and phospholipids in the human primary T cell signaling network. The $N = 8 5 3$ observational measurements, corresponding to individual cells, were acquired via intracellular multicolor flow cytometry (Sachs et al. 2005). The consensus DAG contains 19 edges $d \approx 3 . 4 5$ ).

Auto MPG (Quinlan 1993) The Auto MPG dataset contains eight attributes (three multivalued discrete and five continuous) with the fuel consumption in miles per gallon (mpg) for $N = 3 9 8$ unique car models. While the original use of the data was to predict mpg of a car, in line with previous works on causal inference (e.g. Wang and Mueller 2017; Teshima and Sugiyama 2021), we use a consensus network ( ${ \mathrm { \dot { \ell } } } n = 6$ , 9 edges, $d = 3$ ).

Application Performance Monitoring (APM) We collect trace data of microservices in a distributed system hosted on Amazon Web Services (AWS). The traces contain latency information on incoming and outbound requests of each service, averaged over $2 0 \mathrm { { m i n } }$ , making observations approximately i.i.d. In total we ran the application for six days, leading to $N = 4 3 2$ observations. We test the working hypothesis that the transpose of the dependency graph ( ${ \mathrm { ' } n = 3 9 }$ , 40 edges, $d \approx 2 . 0 5 )$ ) of the application is the true causal DAG (for a discussion of this hypothesis, see Sec. 4).

# 6 Results

# 6.1 Simulated Graphs

Nonlinear Mechanisms Figure 3a depicts mean $p _ { \mathrm { L M C } }$ for 50 synthetic graphs of various size and sparsity with nonlinear mechanisms modeled using random MLPs. As expected, we find that the average $p _ { \mathrm { L M C } } $ monotonically decreases with increasing amount of domain knowledge for both models of domain experts. When the domain expert has complete knowledge $( \hat { \mathcal { G } } = \mathcal { G } ^ { * }$ , corresponding to $| K | / | V | = 1$ for DE- $V$ and $\mathrm { S H D } / | \mathcal { E } | = 0$ for DE- $\mathcal { E }$ ), we reject the hypothesis that the DAG is as bad as a random node permutation with significance level $\alpha = 1 \%$ for all configurations $( p _ { \mathrm { L M C } } < 0 . 0 0 5 )$ .

Table 1: Mean $p _ { \mathrm { L M C } } $ and standard deviation (over 50 trials) for the consensus graphs of the real-world data sets with $9 5 \%$ confidence interval. For $\alpha = 5 \%$ we reject the hypotheses that the graphs are as bad as random ones, despite high fractions of violations $\phi _ { \mathrm { L M C } } ^ { \mathcal { G } ^ { \ast } , \mathcal { D } }$ (3).   

<html><body><table><tr><td></td><td>Sachs</td><td>Auto MPG</td><td>APM</td></tr><tr><td>9*,D 95% conf. int. PLMC</td><td>0.031 ± 0.006 [0.020, 0.042]</td><td>0.03± 0.0052 [0.019, 0.04]</td><td>0.00</td></tr><tr><td>M</td><td>0.08</td><td>0.50</td><td>0.22</td></tr></table></body></html>

Table 2: $p _ { \mathrm { L M C } } $ and SHD for graphs inferred by causal discovery on the Sachs et al. (2005) data.   

<html><body><table><tr><td></td><td>NOTEARS</td><td>CAM</td><td>LiNGAM</td></tr><tr><td>,D</td><td></td><td></td><td>0.548±0.237 0.0724±0.0866 0.0362±0.1290</td></tr><tr><td>SHD/ε PLMC</td><td></td><td>2.780±0.241 1.880 ±0.198</td><td>1.0600±0.0806</td></tr></table></body></html>

Gaussian-Linear Mechanisms In the supplemental, Fig. A3 we report $p _ { \mathrm { L M C } }$ for synthetic graphs of various size and sparsity and with linear-gaussian mechanisms. Similar to DAGs with nonlinear mechanisms, we observe that our metric strictly decreases with increasing amount of domain knowledge for both models of domain experts. If $\hat { \mathcal G } = \mathcal G ^ { * }$ , we would not falsify the true graph using our metric and significance level $\alpha \overset { \cdot } { = } 1 \%$ .

Effect of Number of Sampled Permutations and Number of Observations Further, we investigate the effect of the number of permutations $T$ and sample size $N$ on $p _ { \mathrm { L M C } } $ for synthetic DAGs with nonlinear mechanisms (Tab. A1). To limit the running time of the experiment, we only evaluate $p _ { \mathrm { L M C } } $ for the true graph, i.e. $\hat { \mathcal { G } } = \mathcal { G } ^ { * }$ . Here, we notice that on average our metric is consistently below 0.05, and therefore we would not reject the true graph with significance level $\alpha \ : = \ : 5 \%$ . The only exception to this are graphs with few nodes (ER-10- $d$ ), evaluated on very few ( $N = 1 0$ ) samples.

# 6.2 Real World Applications

Figure 3b shows mean $p _ { \mathrm { L M C } } $ over 50 sampled given DAGs for the three real-world datasets. Similar to the experiments with synthetic data, we find that with increasing amount of domain knowledge (higher $| K | / | V |$ , lower $\mathrm { S H } \bar { \mathrm { D } } / | \mathcal { E } | ,$ $p _ { \mathrm { L M C } } $ is strictly decreasing. When $\hat { \mathcal { G } } = \mathcal { G } ^ { * }$ we reject the hypotheses that the given graphs are as bad as a random node permutation at $\bar { \alpha } = 5 \%$ for all datasets.

Furthermore, we find that for all real-world datasets the ferxapceticotendotfypLeMICe rvoiro ratitoenos $\phi _ { \mathrm { L M C } } ^ { \mathcal { G } ^ { \ast } , \mathcal { D } }$ or 3t)h s hgingihfiecr ntcheanl tvhel $5 \%$ $\alpha ~ \stackrel { - } { = } ~ 5 \%$ we used for all conditional independence tests throughout this work (c.f. Tab. 1). Thus, using φLG⇤M,C as a metric, we would falsely reject the true causal graph, na¨ıvely assuming our CI tests would have valid level.

![](images/260b77bb58594ab14cb68e12abf6660c0ec42194eb49c5668ac47420a6d5691f.jpg)  
Figure 3: Mean $p _ { \mathrm { L M C } }$ for two types of domain experts, simulated via DE- $V$ (left; smaller numbers correspond to less domain knowledge) and DE- $\mathcal { E }$ (right; smaller numbers correspond to more domain knowledge). On synthetic data (a) for the true DAG $( | K | / | V | = 1$ ; $\mathrm { S H D } / | \mathcal { E } | = 0 ,$ ), we reject the null that the DAG is as bad as random with $\alpha = 1 \%$ for all configurations. $\hat { \mathcal G }$ is falsified with the same $\alpha$ if $| K | / | V | \leq 0 . 6$ or $\mathrm { S H D } / | \mathcal { E } | \ge 1 . 5$ . On real-world data (b), for the true DAG, we reject the null that it is as bad as random with $\alpha = 5 \%$ and $\hat { \mathcal G }$ is falsified with the same $\alpha$ for $| K | / | V | \leq 0 . 8$ or SHD/ $| \mathcal { E } | \geq 0 . 5$ for all datasets.

$$
\frac { 1 0 \times \ \phantom { 1 0 } { 1 0 } } { \mathbf { R u n t i m e } \ [ \mathbf { s } ] } \ \stackrel { 1 0 } { 5 } \ \stackrel { 5 0 } { 3 } \ \frac { 1 0 0 } { 5 0 7 \pm 1 8 \ 3 . 3 8 6 \pm 8 7 }
$$

Table 3: Runtime of $p _ { \mathrm { L M C } } $ for large graphs with up to 200 nodes. All graphs were modeled as ER- $n { - } 1$ , $\textbf { \textit { n } } \in$ $\{ 1 0 , 5 0 , 1 0 0 , 2 0 0 \}$ . Data were generated using nonlinear conditionals and $N = 1 0 0 0$ samples. For each test we sample 100 permutations, sufficient to reject the null at $\alpha = 1 \%$ . As CI test we employed the GCM with boosted decision trees as regressor. See Supp. A.6 for more details.

# 6.3 Causal Discovery Algorithms

While the main scope of this work is to evaluate user-given graphs, we conduct additional experiments with $\hat { \mathcal G }$ inferred via causal discovery. On the Sachs et al. (2005) data we find that graphs inferred by NOTEARS and CAM are not significantly better than random, whereas graphs inferred by LiNGAM are not falsified using our metric at $\alpha \ : = \ : 5 \%$ (Tab. 2). Furthermore, a ranking based on our metric is in accordance with an SHD ranking $( \mathrm { N O T E A R S } > \mathrm { C A M } >$ LiNGAM). Both inequalities are significant with $p < 0 . 0 0 1$ for SHD and $p _ { \mathrm { L M C } } $ when tested using a Wilcoxon signedrank test. For further experimental results see Supp. C.3.

# 6.4 Runtime

Large graphs may entail thousands of CIs that need to be tested in order to compute our metric. Therefore, we evaluated the feasibility of applying $p _ { \mathrm { L M C } } $ to graphs with up to 200 nodes (Tab. 3). We find that runtimes are reasonably fast $( < \mathrm { { 1 h } ) }$ even for very large graphs. Note, that domain experts and causal discovery methods would likely take much longer to come up with a DAG $\hat { \mathcal G }$ in the first place. E.g., on DAGs with 50 and 100 nodes CAM and NOTEARS are about an order of magnitude slower, respectively (Rolland et al. 2022; Lachapelle et al. 2020). More information on the algorithmic complexity of our metric is provided in Supp. B.

# 7 Discussion

In this work we addressed the lack of a suitable metric to evaluate an estimated DAG on observed data. To this end we discussed an existing absolute metric that, without a baseline comparison, is difficult to interpret.

We defined a set of properties that a suitable baseline should satisfy and found that sampling random node permutations of the given DAG is a natural way to satisfy these requirements. Using this baseline, we derived a novel metric which comprises two tests that measure first, how characteristic a given graph is in the sense that it is falsifiable by testing CIs and second, whether the given graph is significantly better than a random one in terms of CIs. Using graphs originating from emulated domain experts and causal discovery algorithms we evaluated our method on two types of data. Synthetic data with known true DAG and real-world data with a consensus graph established in the literature. Furthermore, we contributed a novel data set from cloud monitoring where an estimate of the true causal graph is given by the inverted dependency graph.

We acknowledge that our baseline may seem like a low bar to clear and other baselines could be considered in future work. However, we argue that besides being a natural choice that satisfies a number of desirable properties (P1 – P3 in Sec. 4) our experiments show that ‘better than random’ is a surprisingly high threshold that simulated domain experts and causal discovery algorithms fail to meet in many settings. We also note that applying our algorithm to DAGs inferred by causal discovery algorithms should be done with caution as elaborated in Sec. 5.1 and other approaches may be better suited for this purpose (Faller et al. 2024). Furthermore, in certain applications, specific suggestions for local edge improvements that go beyond the simple report of the triplets that result in LMC violations may be desired, and we leave this interesting direction for future work. Another promising direction could be to extend our nodepermutation baseline to the likelihood $p ( \mathcal { D } | \hat { \mathcal { G } } )$ (c.f. Sec. 2).