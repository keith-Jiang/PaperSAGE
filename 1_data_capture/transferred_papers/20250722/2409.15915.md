# Planning in the Dark: LLM-Symbolic Planning Pipeline Without Experts

Sukai Huang1, Nir Lipovetzky1 and Trevor $\mathbf { C o h n } ^ { 1 , 2 * }$

1The University of Melbourne 2 Google sukaih@student.unimelb.edu.au, {nir.lipovetzky, trevor.cohn}@unimelb.edu.au

# Abstract

Large Language Models (LLMs) have shown promise in solving natural language-described planning tasks, but their direct use often leads to inconsistent reasoning and hallucination. While hybrid LLM-symbolic planning pipelines have emerged as a more robust alternative, they typically require extensive expert intervention to refine and validate generated action schemas. It not only limits scalability but also introduces a potential for biased interpretation, as a single expert’s interpretation of ambiguous natural language descriptions might not align with the user’s actual intent. To address this, we propose a novel approach that constructs an action schema library to generate multiple candidates, accounting for the diverse possible interpretations of natural language descriptions. We further introduce a semantic validation and ranking module that automatically filter and rank the generated schemas and plans without expert-in-the-loop. The experiments showed our pipeline maintains superiority in planning over the direct LLM planning approach. These findings demonstrate the feasibility of a fully automated end-to-end LLM-symbolic planner that requires no expert intervention, opening up the possibility for a broader audience to engage with AI planning with less prerequisite of domain expertise.

Code — https://github.com/Sino-Huang/Official-LLMSymbolic-Planning-without-Experts Extended version — https://arxiv.org/abs/2409.15922

# 1 Introduction

The advent of Large Language Models (LLMs) has opened new avenues for solving natural language-described planning tasks (Kojima et al. 2022). However, direct plan generation using LLMs, while seemingly straightforward, has been criticized for inconsistent reasoning and hallucination, which undermines their reliability in critical planning scenarios (Valmeekam et al. 2022, 2023; Huang et al. 2024). In response, researchers have advocated for more robust approaches that combine the flexibility of LLMs with the correctness of symbolic planning to solve planning tasks (Pallagani et al. 2024; Oswald et al. 2024). To improve the soundness of generated plans, a hybrid LLM-symbolic planning

Initial state desc. Domain descriptions   
Goal state desc. Predicate descriptions Action descriptions   
Book 31 Book 2 1 Direct LLM Planning Table initial state Plan Candidates Large Language Model Book 12 Problem Spec. Book 3 Action Schema(s) Predicate List 1 Table Symbolic Planner goal state 2 LLM-Symbolic Planning Pipeline

pipeline has emerged. As shown in Figure 1, instead of relying solely on LLMs to generate sequences of action plans through in-context learning, this pipeline begins by leveraging LLMs to extract abstract symbolic action specifications from natural language descriptions, known as action schemas. These schemas define the essential components of an action in a structured format understandable by symbolic planners. Once these schemas are generated, a classical planner can take over to search for feasible plans that fulfill the task specifications (Liu et al. 2023; Silver et al. 2024; Guan et al. 2023; Kambhampati et al. 2024).

Yet, this method is brittle, as a single missing or contradictory predicate in an action schema can prevent the planner from finding a valid plan. Thus, current pipelines often require multiple iterations of expert intervention to refine and validate the generated action schemas. For instance, Guan et al. (2023) reported that the expert took 59 iterations to fix schema errors for a single task domain. This process demands substantial time and expertise, which significantly hinders the scalability of the method. More critically, due to budget constraints, often only one expert is involved in the process. This creates a critical vulnerability: the potential for interpretation mismatch between the expert and the user. Experts, while knowledgeable, inevitably bring their own subjective interpretations to the task descriptions, often formal

Domain NLN-NLdL-e-dseescsccrriibeedd LLM bRSoleliypicrePmPslleanbntnaniotininloginsc Expert planning tasks 日 ((eRe..ge..,,pArcttieiosnescnhetemamata)i)ons 9 E Bottleneck: Review & Correction   
Limitation 2: Subjective and Limited Interpretation User Domain 25 Mbanoaogkisng Expert ?   
New Books Shelve quickly, Categorize and any available space. arrange neatly.

izing them in a single, specific way. This limits the system to a single perspective of the task. However, unlike formal language designed to have an exact, context-independent meaning, natural language inherently contains ambiguities that yield diverse valid interpretations of the same description. This ambiguity suggests that a straightforward, one-to-one mapping from natural to formal languages – a typical case when relying on a single expert – risks overlooking the interpretation that the user actually intended (Moravcsik 1983) (see Figure 2).

Regarding the issue with reliance on expert intervention, we propose a novel pipeline that eliminates this dependency. Specifically, our approach introduces two key innovations:

(1): We construct an action schema library to generate multiple candidates, a strategy that has been overlooked in prior work despite being a natural fit for capturing the inherent ambiguity in natural language. By leveraging this library, we also increase the likelihood of obtaining solvable action schema sets – those have at least one valid plan that can be found by a planner.

(2): We leverages sentence encoders1 to automatically validate and filter generated action schemas. This module ensures that the generated schemas closely align with the task descriptions in the semantic space, effectively acting like expert feedback. Our experiments demonstrate that without expert intervention, our pipeline generates sound action plans competitive with direct LLM-based plan generation, even in short-horizon planning tasks. Importantly, our approach offers multiple schema sets and plan candidates, preserving the diversity of interpretations inherent in ambiguous natural language descriptions.

# 2 Related Work

Direct Plan Generation with LLMs: The use of LLMs for direct action plan generation has been explored across various domains, including embodied tasks (Wang et al.

2023; Xiang et al. 2024), and other language grounding environments (Ahn et al. 2022; Huang et al. 2022). These approaches are built upon the idea that LLMs’ reasoning capabilities can be effectively elicited through in-context learning techniques, particularly the Chain-of-Thought (CoT) approach. CoT prompts the model to generate a series of intermediate reasoning steps before arriving at the final answer, resulting in more coherent and logically sound reasoning (Wei et al. 2022). Building upon CoT, Yao et al. (2024) proposed Tree-of-Thought (ToT) framework, which explores multiple reasoning pathways, generating diverse plans and ranking them based on self-verification heuristics. These heuristics are verbalized confidence scores produced by LLMs themselves, a method supported by studies showing that LLMs are effective as zero-shot ranking models (Lin et al. 2022; Hou et al. 2023; Zhuang et al. 2023).

Criticism and Hybrid Planning: Despite the promising results, researchers have raised concerns about the reliability and soundness of LLM-generated plans (Valmeekam et al. 2022, 2023; Huang et al. 2024). A critical issue highlighted by Kambhampati et al. (2024) is that planning and reasoning tasks are typically associated with System 2 competency, which involves slow, deliberate, and conscious thinking (Sloman 1996; Kahneman 2011). However, LLMs, being essentially text generators, exhibit constant response times regardless of the complexity of the question posed. This behavior suggests that no first-principle reasoning is occurring, contradicting the expectations for true planning capabilities. To this end, researchers have explored hybrid approaches. For instance, Thought of Search (Katz et al. 2024) involves the generation of successor function and goal test code by LLMs, followed by their execution within an external execution environment. The approach we focus on involves utilizing LLMs to generate symbolic representations of tasks, which are then processed by external symbolic planners to search for feasible plans (Liu et al. 2023; Guan et al. 2023). However, existing pipelines emphasize the necessity of expert intervention for action schema validation and refinement. While Kambhampati et al. (2024) proposed using LLMs as semi-expert critics to assess output quality, this approach still necessitates expert involvement for final decision-making. In contrast, our work strives to reduce the dependency on expert intervention, offering a more accessible approach to hybrid LLM-symbolic planning that also addresses the inherent ambiguity in natural language descriptions.

# 3 Problem Setting and Background

We consider a scenario where an agent generates action plans for natural language-described planning tasks. A task description typically consists of: (1) a domain description outlining general task information and possible high-level actions, and (2) a problem instance description specifying the initial and goal states. The study of LLM-symbolic planning pipelines is grounded in the formal framework of classical planning, which relies on symbolic representations of planning tasks. These representations are typically expressed using the Planning Domain Definition Language (PDDL) (Aeronautiques et al. 1998; Haslum et al. 2019).

Step 1: 1   
Building a Diverse Schema Library N M N 1 the same entity, but they are referring to PNlaLn-ndiensgcrTiabsekds LLMs generate SAchtieomna 1 encoded differently Z(D),P 4 N Task Entity O in symbolic in free form   
Step 2: (Configured for High Diversity) Diverse action schema candidates   
Semantic Coherence Filtering representation representation M N Score: 0.37 PDDL domain NatdueraslcrLipatnigounage Sentence PDDL Action Schema Encoder Score: 0.46 Score: 0.17 × Action Schema PSrcehdeimcateSeLtist SEenctteoendncecere Score: 0.15 ×   
SPltaenp  S3:coring and Ranking Conformal Prediction (a)2y (0) (mi) seexpmeacntteidc tvoecotuotrpruetpsriemsielnatrations Similar semantic embeddings Symbolic Sentence Cumul. Score: 0.78 PDDL Planner Plalananssns Encoder Cumul. Score: 0.64 alanRnssanked PSrcehdeimcateSeLtist {π}K Cumul. Score: 0.47 Sorted Assumption in the filtering mechanism L (mi

Figure 3: An overview of the proposed pipeline, it first constructs diverse action schema candidates to cover various interpretations of the natural language descriptions. Then, it filters out low-confidence candidates to ensure the generation candidates are semantically aligned with the descriptions. Lastly, it produces and ranks multiple plans using a symbolic planner. The filtering mechanism is grounded in the concept of semantic equivalence across different representations of the same content.

In brief, a PDDL description is defined by $\langle \mathcal { D } , \Pi _ { \mathcal { D } } \rangle$ , where:

• $\mathcal { D } = \langle \mathcal { P } , \mathcal { A } \rangle$ is the domain specification: $\mathcal { P }$ is the set of predicates that can either hold true or false, and $\mathcal { A }$ is the set of action schemas. Each action schema $\alpha \in { \mathcal { A } }$ is defined as a tuple $\alpha = \langle p a r , p r e , e f f \rangle$ , where par details the parameters, and pre and $e f f$ are the preconditions and effects, respectively. Both pre and $e f f$ are typically expressed as conjunctive logical expressions using predicate logic. • $\Pi _ { \mathcal { D } } = \langle \mathcal { O } , \mathcal { Z } , \mathcal { G } \rangle$ is the problem instance: $\mathcal { O }$ is the set of objects to interact with, $\boldsymbol { \mathcal { T } }$ is the initial state, and $\mathcal { G }$ is the goal state that the agent needs to achieve.

A solution to the planning task is a sequence of grounded actions $( \pi = ( a _ { 0 } , { \bar { . . . , a _ { n } } } ) )$ that transforms the initial state $\boldsymbol { \mathcal { T } }$ to the goal state $\mathcal { G }$ . Each grounded action $a _ { i }$ is an instantiation of an action schema $\alpha \in { \mathcal { A } }$ and predicates, where the parameters in $\alpha$ are replaced with specific objects from $\mathcal { O }$ .

To bridge natural language descriptions and formal planning representations, we introduce a natural language proxy layer, denoted as $\mathcal { Z } ( \cdot )$ , for these task specifications. For example, $\mathcal { Z } ( \mathcal { D } )$ represents the natural language equivalent of the domain specification $\mathcal { D }$ . The two approaches, direct LLM planning and LLM-symbolic planning, can then be expressed in Eq 1 and Eq 2, respectively:

$$
\begin{array} { r } { \pi \sim P _ { \mathrm { L L M } } ( \cdot \mid \mathcal { Z } ( \mathcal { D } ) , \mathcal { Z } ( \Pi _ { \mathcal { D } } ) ) \qquad } \\ { \hat { \mathcal { A } } \sim P _ { \mathrm { L L M } } ( \cdot \mid \mathcal { Z } ( \mathcal { D } ) ) ; \Pi _ { \mathcal { D } } \sim P _ { \mathrm { L L M } } ( \cdot \mid \mathcal { Z } ( \Pi _ { \mathcal { D } } ) ) } \\ { \pi = f \left( \langle \mathcal { P } , \hat { \mathcal { A } } \rangle , \Pi _ { \mathcal { D } } \right) \qquad } \end{array}
$$

In these equations, $P _ { \mathrm { L L M } } ( \cdot )$ represents the generation process of LLMs, and $f$ is the symbolic planner that search for sound plans. While we largely adhere to the problem setting of previous research (e.g., Liu et al. (2023), Guan et al. (2023)), we introduce a crucial refinement by specifying a precise predicate set $( \mathcal { P } )$ for each domain descriptions. This controlled setting addresses a key challenge in evaluating across different methodologies. Without a standardized predicate set, variations in domain understanding can lead to diverse and potentially incomparable outputs, hindering meaningful evaluation.

# 4 Methodology

As illustrated in Figure 3, the proposed pipeline stands in contrast to existing expert-dependent approaches and consists of three key steps: (1) Building a Diverse Schema $L i$ - brary $( \ S 4 . I ) ,$ , (2) Semantic Coherence Filtering $( \ S \ 4 . 2 )$ and (3) Plan Scoring and Ranking $( \ S \ 4 . 4 )$ .

# 4.1 Building a Diverse Schema Library

A key challenge in translating natural language descriptions into symbolic action schemas is the inherent ambiguity of language itself. Different interpretations of the same description can lead to variations in action schemas, impacting the downstream plan generation process. To ensure we explore a wide range of interpretations and effectively cover the user’s intent, we utilize multiple LLM instances, denoted as $\{ P _ { \mathrm { L L M } } ^ { 1 } , P _ { \mathrm { L L M } } ^ { 2 } , . . . , P _ { \mathrm { L L M } } ^ { N } \}$ , and set their temperature hyperparameter high to encourage diverse outputs. Each will then generate its own set of action schemas $\bar { \mathcal { A } } _ { i } \sim P _ { \mathrm { L L M } } ^ { i } ( \cdot \ |$ $\mathcal { Z } ( \mathcal { D } ) )$ , where $\hat { \mathcal { A } } _ { i } = ( \hat { \alpha } _ { i 1 } , \hat { \alpha } _ { i 2 } , . . . , \hat { \alpha } _ { i M } )$ . Here, $\hat { \alpha } _ { i j }$ , where $i \in [ \dot { 1 } , . . N ]$ and $j \in [ 1 , . . . , M ]$ , represents the generated action schema of $j$ -th action in the domain by the $i$ -th LLM instance.

The generated schemas $\hat { \alpha } _ { i j }$ from all models are then aggregated into a single library. Since each domain comprises $M$ actions, a “set” of action schemas refers to a complete collection where each action in the domain is associated with one corresponding schema. Therefore, all possible combination of action schemas within the library can generate approximately $\binom { N } { 1 } ^ { M }$ different sets of action schemas.

In addition, existing pipelines rely heavily on expert intervention, partly because individual LLMs struggle to generate solvable sets of schemas – those that a planner can successfully use to construct a plan. This reliance becomes even more pronounced as the number of actions increases, with the probability of obtaining a solvable set of schemas from a single LLM diminishing exponentially. In contrast, our approach, by constructing a diverse pool of action schema sets, substantially improves the probability of finding a solvable set. Our analysis (detailed in Appendix A) demonstrates that, under reasonable assumptions, this probability can increase from less than $0 . 0 0 0 1 \%$ with a single LLM to over $9 5 \%$ when using multiple LLM instances.

Note that the solvability of a set of action schemas can be efficiently verified by leveraging the completeness feature of modern symbolic planners. If a plan can be found for a given problem using the generated schemas, the set is deemed solvable. Importantly, modern symbolic planners have advanced capabilities that allow them to efficiently reject unsolvable schema sets. This is achieved by the ability to prove delete-free reachability in polynomial time (Bonet and Geffner 2001). Furthermore, modern planners are designed to operate efficiently on multithread CPU and the efficiency of the process should not be a cause for concern. See Appendix D for more details.

# 4.2 Semantic Coherence Filtering

The previous method alone faces two limitations. First, as task complexity grows, the “brute-force” approach of combining and evaluating all possible sets becomes increasingly inefficient. Second, solvability does not guarantee semantic correctness – schemas may not accurately reflect the task descriptions, potentially leading to incorrect or nonsensical plans. Therefore, it is crucial to implement a filtering mechanism that autonomously assesses the semantic correctness of individual action schemas, filtering out low-quality candidates before they enter the combination process.

Our approach is grounded in the concept of semantic equivalence across different representations of the same content, as discussed by Weaver (1952) in his memorandum “Translation.” Weaver emphasized that the most effective way to translate between languages is to go deeper to uncover a shared “common base of meaning” between language representations, illustrating this by noting that “a Russian text is really written in English, but it has been encoded using different symbols.” This principle is crucial in our context, where task descriptions in natural language and their corresponding structured symbolic representations should exhibit high semantic similarity, reflecting the same shared meaning despite different syntactic forms (see right side of Figure 3).

Recent developments in language models as code assistants (Chen, Tworek et al. 2021; Rozie\`re, Gehring et al. 2024) further support this assumption, demonstrating that these models can decode the underlying semantics of structured symbolic representations. Inspired by this, we propose a filtering step that leverages a sentence encoder $E ( \cdot )$ to generate embeddings for both the action descriptions $\dot { E } ( \mathcal { Z } ( \alpha ) )$ and the generated schemas $E ( \hat { \alpha } )$ . Then, we compute the cosine similarity between these embeddings to quantify semantic relatedness and filter out action schemas with low scores.

Specifically, we employ a conformal prediction (CP) framework (see Appendix B) to statistically guarantee that true positive action schema candidates have a high probability of being preserved while minimizing the size of the filtered set (Sadinle et al. 2019). In this process, a threshold $\hat { q }$ will be calculated based on a user-specified confidence level $1 - \epsilon$ . Action schemas with cosine similarity scores below this threshold are filtered out from the library.

This process (illustrated in step 2 of Figure 3) significantly reduces the number of candidate sets of action schemas to $\Pi _ { i = 1 } ^ { M } ( m _ { i } )$ , where $m _ { i }$ is the number of action schemas that pass the semantic validation for the $i$ -th action. This prefiltering approach not only reduces the computational load on the symbolic planner, increasing efficiency, but also ensures that generated schemas closely align with the semantic meaning of the task descriptions.

# 4.3 Finetuning with Manipulated Action Schemas

Hard negative samples have been shown to enhance representation learning by capturing nuanced semantic distinctions (Robinson et al. 2023). In our context, we found that structured action schemas are particularly ideal for generating hard negatives. By manipulating predicates in the precondition or effect expressions of true action schemas, we create hard negatives with subtle differences. During finetuning, a triplet loss function is employed, where each training sample consists of a triplet: the natural language description of an action $\scriptstyle ( { \mathcal { Z } } ( \alpha ) )$ , the true action schema $( \alpha )$ , and a negative sample $( \alpha ^ { \mathrm { n e g } } )$ (see Figure 4). A negative sample is of three types – (1) Easy Negatives: action schemas from other planning domains (inter-domain mismatch); (2) SemiHard Negatives: action schemas from the same domain but referring to different actions (intra-domain mismatch); and (3) Hard Negatives: As shown in Table 1, we employ four types of manipulations – swap, negation, removal, and addition – to manipulate the reference2 action schema of the domain.

easy negative a	
c aneg   
desc. of action   
place-on-table 0 ne qneg Q hard negative semi-hard positive (ilat) negative place-on-table pl
c-
-hf

Table 1: Types of Manipulations for Generating Synthesized Hard Negative Action Schemas in Training Data. Mutexes are predicates that cannot be true simultaneously, e.g., one cannot hold a book and have it on a table simultaneously.   

<html><body><table><tr><td>Manipulation Type Description</td><td></td></tr><tr><td>Swap</td><td>Exchanges a predicate between preconditions and effects</td></tr><tr><td>Negation</td><td>precotesitipredieate inether</td></tr><tr><td>Removal</td><td>Removes a predicate from either preconditions or effects</td></tr><tr><td>Addition</td><td>Adds mutually exclusive (mutex) predicates to preconditions or effects (Helmert 2009)</td></tr></table></body></html>

Through this process, the sentence encoder learns to embed natural language descriptions closer to their corresponding action schemas while distancing them from negative samples in the semantic space.

# 4.4 Plan Generation and Ranking

Action schemas that more accurately represent the intended tasks described in natural language are likely to yield higherquality, more reliable plans. Leveraging this causal relationship, we assess and rank the generated plans based on the cumulative semantic similarity scores of their constituent action schemas. Specifically, we feed each solvable set of action schemas into a classical planner, which generates a calculated as $\textstyle \sum _ { i = 1 } ^ { M } { \frac { E ( { \mathcal { Z } } ( \alpha _ { i } ) ) \cdot E ( \hat { \alpha _ { i } } ) } { \| E ( { \mathcal { Z } } ( \alpha _ { i } ) ) \| \| E ( \hat { \alpha _ { i } } ) \| } }$ , where $\mathcal { Z } ( \alpha _ { i } )$ is the $i$ main, $\hat { \alpha _ { i } }$ is the corresponding generated action schema and $E ( \cdot )$ is already defined in Sec 4.2. It ensures that the structured symbolic model comprising the plans are semantically aligned with the descriptions of the planning domain (see step $3$ in Figure 3). Furthermore, this approach allows for optional lightweight expert intervention as a final, noniterative step. By presenting the ranked schema sets and their corresponding plans, experts can determine the most appropriate one, providing a balance between autonomy and expert guidance.

Overall, our pipeline bridges the gap between ambiguous task descriptions and the precise requirements of symbolic planners. By generating a diverse pool of action schemas and leveraging semantic similarity for validation and ranking, we achieve two key advancements. First, we reduce the dependency on expert intervention, making the process more accessible and efficient. Second, we preserve the inherent ambiguity of natural language, offering users multiple valid interpretations of the task and their corresponding plans.

# 5 Experiments

Our experiments test the following hypotheses: (H1) Semantic equivalence across different representations, as discussed by Weaver, holds true in our context. $( \mathbf { H } 2 )$ Ambiguity in natural language descriptions leads to multiple interpretations. (H3) Our pipeline produces multiple solvable candidate sets of action schemas and plans without expert intervention, providing users with a range of options. $\mathbf { \left( H 4 \right) }$ Our pipeline outperforms direct LLM planning approaches in plan quality, demonstrating the advantage of integrating LLM with symbolic planning method. See Appendix for other experiments outside the scope of these hypotheses.

# 5.1 Experimental Setup

Task and Model Setup. We introduces several key enhancements that distinguish it from previous work. (1) Novel Test Domains: We carefully selected three test domains ensuring they are unfamiliar to LLMs – Libraryworld: a modified version of the classic Blockworld domain; Minecraft: resource gathering and crafting domain inspired by the game Minecraft; and Dungeon: a domain originally proposed by Chrpa et al. (2017). This approach addresses a significant issue: many $\mathrm { I P C } ^ { 3 }$ domains have likely been leaked into LLM training data (see Appendix C). For training and calibration of the sentence encoder, we used domains from IPC and PDDLGym (Silver and Chitnis 2020). (2) LLM Selection: We use the open-source GLM (Hou et al. 2024) over proprietary models like GPT-4, aligning with our commitment to accessible planning systems. (3) Ambiguity Examination: We tested our pipeline on two types of task descriptions to assess the impact of ambiguity – (a) detailed descriptions following the established style of Guan et al. (2023), and (b) layman descriptions provided by five non-expert participants4 who, unfamiliar with PDDL, described the domains and actions based on reference PDDL snippets. (4) Symbolic Planner: We used DUAL-BWFS (Lipovetzky and Geffner 2017) planner for plan generation as well as checking if the generated schema sets are solvable. (5) LLM Prompt Engineering: We use the CO-STAR and CoT framework to guide LLMs in generating outputs (see Appendix E).

Baselines. We evaluate our pipeline against two key baselines: (1) The previous LLM-symbolic planning pipeline proposed by Guan et al. (2023), which involves expert intervention for action schema validation and refinement; and $( 2 ) D i$ - rect LLM-based planning using Tree-of-Thought (ToT) (Yao et al. 2024), which generates multiple plans and ranks them based on self-verification heuristics.

![](images/e9b4891fe1347282921ce7e9c4c0387bf02a5b7dbc6cb303086c821edbdcbd7e.jpg)  
Figure 5: The sentence encoder enhances the identification of mismatched pairs by fine-tuning with negative samples.

# 5.2 Semantic Equivalence Analysis

To investigate H1, we initially assessed the cosine similarity of sentence embeddings for pairs of action schemas and their corresponding natural language descriptions, both when they were matched and when they were mismatched. We employed two pre-trained, extensive sentence encoders: text-embedding-3-large and sentence-t5-xl. These models, without any fine-tuning, demonstrated higher cosine similarity for matched pairs compared to mismatched ones. This finding suggests that the ability to detect such equivalence is an inherent feature of high-quality sentence embedding models, not merely an artifact of fine-tuning. However, OpenAI text-embedding-3-large model is bad for its accessibility, a lightweight encoder all-roberta-large- $\cdot \nu I$ allows for better speed and improved accuracy through finetuning, which is good in practice. The performance of the fine-tuned roberta model is shown in Figure 5. The substantial improvement in the model’s capacity to identify hard negatives – mismatched pairs with subtle differences – is a direct result of our dedicated training weights allocation. We deliberately designed our training data selection to include a ratio of easy, semi-hard, and hard negatives as [0.0, 0.4, 0.6], respectively (see Appendix E.7). This ratio was strategically chosen to concentrate on hard negatives, as LLMs are more likely to make hard-negative mistakes when generating action schemas. By prioritizing hard negatives in our training dataset, we aimed to enhance the model’s ability to filter out low-quality action schemas during the semantic coherence filtering step.

# 5.3 Pipeline Performance and Efficiency

Our pipeline’s performance and efficiency are highlighted through several key observations. Firstly, the use of action schema library effectively produces solvable action schema sets without requiring expert-in-the-loop. Notably, deploying 10 LLM instances is sufficient to generate solvable schema sets for all test domains, supporting H3. Secondly,

Table 2: Contrasts Our Pipeline with Existing Works. Note that the property of generating sound (logical correct) plans has been highlighted as a feature of the hybrid planner in prior work (Liu et al. 2023; Guan et al. 2023). However, there is no guarantee that the schemas are fully correct w.r.t. what the user actually wants. Thus, we are weakening the property to soundness w.r.t. schemas.   

<html><body><table><tr><td>Model</td><td>Mech.</td><td>Expuert</td><td>Heyistic</td><td>w.t.dhemas</td></tr><tr><td>Tree-of-Thought (Yao et al. 2024)</td><td>LLM</td><td>0</td><td>Self Verification</td><td>No</td></tr><tr><td>Guan et al.(2023) Hybrid</td><td></td><td>~59</td><td>Expert Validation</td><td>Yes</td></tr><tr><td>Ours</td><td>Hybrid</td><td>≤1</td><td>Sim.anties</td><td>Yes</td></tr></table></body></html>

Figure 6 reveals a clear pattern: when confronted with inherently ambiguous layman descriptions from non-expert participants, our pipeline generates a significantly increased number of distinct solvable schema sets (e.g., from 3419 to 8039 when $\mathrm { L L M } \# = 1 0 ~ \mathrm { w } / \mathrm { o } ~ \mathrm { C P } )$ , thereby supporting H2. This increase is primarily attributed to the diverse selection of predicates within the action schemas. Each predicate selection reflects a different interpretation of the problem, with each schema set emphasizing distinct features deemed critical for planning.

For instance, in the Libraryworld domain, we observed that some schema sets generated by some LLM instances take into account the ‘category’ property of books when constructing actions such as stacking books on a shelf. This means that, according to these schema sets, only books within the same category can be stacked together, which is a more organized way of arranging books. Consequently, this leads to different planning outcomes that reflect the varied interpretations of the user query at hand, which are a direct result of the ambiguity present in the layman’s description and the flexibility it provides to LLMs in making such choices.

The pipeline’s ability to generate a range of potential interpretations in response to ambiguous inputs is a critical advantage. It ensures that all intended aspects of the user’s description can be captured, even when the description is imprecise or incomplete.

Thirdly, the integration of conformal prediction in the filtering step demonstrates a significant improvement in efficiency, as evidenced by Figure 6. With the confidence level $1 - \epsilon$ set to 0.8, the pipeline filtered out a large number of candidates, reducing the total number of combinations to $3 . 3 \%$ of the original (1051 out of 31483) but meanwhile, the ratio of solvable schemas (verified by the planner) increased from $1 0 . 9 \%$ to $23 . 0 \%$ . This result strongly supports H3, highlighting the pipeline’s ability to efficiently generate solvable and semantically coherent schema sets. See Table 2 for a comprehensive comparison of our pipeline with existing LLM-based planning approaches. Notably, the initial low ratio of solvable schema sets $( 1 0 . 9 \% )$ underscores the challenge faced within the LLM-symbolic planning paradigm, which may explain why expert intervention has been a common practice in the past.

![](images/75ac661f3b05ebdf925ebf4fdfc512a1217f4d28a52a9cb7f3a7f0dc5a6c12ca.jpg)  
Total vs. Viable Combinations (With CP Filtering, $1 - \epsilon = 0 . 8$ )   
Figure 6: With CP, a large number of candidates are pruned, thereby improving efficiency.

Table 3: Blind plan ranking eval.: Four assessors compared the top two plans from each approach to gold plans.   

<html><body><table><tr><td></td><td>Rank 1st</td><td>Rank 2nd</td><td>Rank 3rd</td><td>Rank 4th</td><td>Rank 5th</td><td>Avg.</td></tr><tr><td>Gold</td><td>14</td><td>4</td><td>4</td><td>1</td><td>1</td><td>1.79</td></tr><tr><td>Ours</td><td>4</td><td>18</td><td>11</td><td>5</td><td>10</td><td>2.97</td></tr><tr><td>ToT</td><td>6</td><td>2</td><td>9</td><td>18</td><td>13</td><td>3.62</td></tr></table></body></html>

# 5.4 Human Evaluation on Plan Quality

To further validate our approach, we conducted a human evaluation comparing the top two plan candidates generated by our pipeline against those from the ToT framework and a gold-standard plan derived from the reference PDDL domain model. Four expert assessors with extensive PDDL experience ranked the plans based on their feasibility in solving the given problems. The results, summarized in Table 3, clearly support H4.

For a deeper insight into our pipeline’s capabilities, we specifically tested the Sussman Anomaly, a well-known planning problem that requires simultaneous consideration of multiple subgoals, as solving them in the wrong order can undo previous progress (see Figure 1). Our results showed that ToT-style approaches using various LLMs, including state-of-the-art models like GPT-4o, consistently fail to solve this problem. The failure arises from the mistaken assumption that the first subgoal mentioned (i.e., placing book 1 on top of book 2) should be addressed first, leading to incorrect plans. Interestingly, GPT-3.5 and GPT4o exhibited different behaviors when faced with this problem. While GPT-3.5 consistently, yet incorrectly, asserted it had completed the problem, GPT-4o occasionally exhibited awareness of the plan’s incompleteness. However, even with this heightened awareness, GPT-4o was unable to identify the correct path within the given depth limit. Notably, ToTstyle approaches reveals a critical limitation where high verbalized confidence scores does not necessarily translate to plan validity. In contrast, our pipeline generates a range of plans, including suboptimal ones, but excels at identifying and prioritizing the most promising candidates through its ranking process that is based on the cumulative cosine similarity scores of generated action schemas. By strictly adhering to semantic alignment between these schemas and natural language descriptions, and by using a symbolic planner, the system avoids being misled by the tendency – observed in both humans and LLMs – to reason in a linear manner. This tendency involves prioritizing subgoals based on their order of appearance rather than considering their underlying logical dependencies. Such linear reasoning can lead to noninterleaved planning, where subgoals are tackled in the order they are presented and each must be fully completed before addressing the next one, which is a pitfall in complex planning problems like the Sussman Anomaly.

# 5.5 Failure Case Analysis

Schema Set with No Plan Found: We encountered instances where no solvable action schema set was generated, primarily due to limitations in the LLM’s reasoning capabilities. The use of open-source LLMs, while more accessible, may result in a lower success rate compared to more advanced proprietary models like GPT-4o. Specifically, with 7 LLM instances, we observed occasional failures of generating solvable sets action schemas for the libraryworld and minecraft domains. Nevertheless, solvable schema sets were consistently obtained across all domains when the number of LLM instances was increased to 10 (see Appendix F for a breakdown of schema set yield by LLM instance count).

Unexpected Preference: In the Dungeon domain, human assessors unexpectedly preferred ToT-generated plans over both the reference plan and the proposed pipeline’s plans. Further analysis revealed that the ToT plans consistently included a step: grabbing a sword. Interestingly, grabbing a sword was not a necessary step for solving the given problem. Consequently, symbolic planners, focused on optimal pathfinding, excluded this step from their plans. However, this “unnecessary” step of acquiring a sword aligns with common strategies in Dungeon games, where players typically prioritize preparedness. Thus, this action strongly appealed to human assessors, causing them to rank the ToTgenerated plans higher.

# 6 Conclusion

Our work presents a 3-step pipeline that learn symbolic PDDL models over ambiguous natural language descriptions and generated multiple ranked plan candidates. Our findings demonstrate that a full end to end hybrid planner is possible without expert intervention, paving the way for democratizing planning systems for a broader audience. One limitation in this work is the lack of direct evaluation methods for assessing the quality of generated action schema sets. Metrics like “bisimulation” (Coulter et al. 2022) or “heuristic domain equivalence” (Oswald et al. 2024) require the generated schema sets to have the same action parameters as a predefined reference set. This approach doesn’t suit our context, where action parameters are flexible and inferred in real-time from natural language descriptions.