# Fair Text-to-Image Diffusion via Fair Mapping

Jia $\mathbf { L i } ^ { 1 , 2 , 4 * }$ , Lijie $\mathbf { H } \mathbf { u } ^ { 1 , 2 , 3 * }$ , Jingfeng Zhang2, 5, Tianhang Zheng6,7, Hua Zhang4, Di Wang1, 2, 3 †

1Provable Responsible AI and Data Analytics (PRADA) Lab 2King Abdullah University of Science and Technology 3SDAIA-KAUST 4Institute of Information Engineering, Chinese Academy of Sciences 5University of Auckland   
6The State Key Laboratory of Blockchain and Data Security, Zhejiang University   
7Hangzhou High-Tech Zone (Binjiang) Institute of Blockchain and Data Security di.wang@kaust.edu.sa

# Abstract

In this paper, we address the limitations of existing text-toimage diffusion models in generating demographically fair results when given human-related descriptions. These models often struggle to disentangle the target language context from sociocultural biases, resulting in biased image generation. To overcome this challenge, we propose Fair Mapping, a flexible, model-agnostic, and lightweight approach that modifies a pre-trained text-to-image diffusion model by controlling the prompt to achieve fair image generation. One key advantage of our approach is its high efficiency. It only requires updating an additional linear network with few parameters at a low computational cost. By developing a linear network that maps conditioning embeddings into a debiased space, we enable the generation of relatively balanced demographic results based on the specified text condition. With comprehensive experiments on face image generation, we show that our method significantly improves image generation fairness with almost the same image quality compared to conventional diffusion models when prompted with descriptions related to humans. By effectively addressing the issue of implicit language bias, our method produces more fair and diverse image outputs.

# Introduction

Text-to-image diffusion models [Ho, Jain, and Abbeel 2020, Song, Meng, and Ermon 2021] have achieved remarkable performance in various applications [Dhariwal and Nichol 2021, Wu et al. 2022, Ceylan, Huang, and Mitra 2023, Blattmann et al. 2023, Poole et al. 2023, Tevet et al. 2023] by involving incorporation of textual conditional elements with language models [Ho and Salimans 2022]. With the increasing popularity among the public, the generation of diverse images [Ouyang, Xie, and Cheng 2022] about human-related description becomes crucial, yet it remains a challenging task [Ning, Li, and Jianlin $\mathtt { S u } 2 0 2 3$ , Schramowski et al. 2023, Bansal et al. 2022]. For example, when prompted to "An image of a computer programmer" and "An image of a confident person" to Stable Diffusion, as exemplified in Figure 1, it is obvious that the majority of results are white male, even if there are no specified gender or race descriptions in textual condition.

![](images/3747e73397fcb31b02b18ad9df1168fa068e1dad15bbabc27a49957c96320cca.jpg)  
Figure 1: Fair Mapping balances demographic biases in textto-image models by minimally adjusting training to generate more diverse images compared to outputs of Stable Diffusion.

This phenomenon demonstrates that text-to-image diffusion models learned implicit correlations between demographic visual representations and textual descriptions during training. Real-world data is often biased and incomplete, reflecting inherent stereotypes in human perceptions [Makady et al. 2017, Bansal et al. 2022]. When training on a dataset that includes pairs of images and texts [Schuhmann, Beaumont, and Richard Vencu 2022, Schuhmann, Vencu, and Romain Beaumont 2021], text-to-image diffusion models face challenges in disentangling sensitive attributes such as gender or race, from specific prompts, which potentially introduce during denoising and lead to societal biases in generation.

Consequently, the pressing question remains: How can unbiased inferences be made in the face of training data that is inherently biased? Nowadays, [Orgad, Kawar, and Belinkov 2023, Gandikota, Orgad, and Belinkov 2024] highlight text-to-image models implicitly assume sensitive attributes from the language model, where textual condition embeddings demonstrate an inclination towards specific demographic groups. As a result, some work [Shen, Du, and Pang 2023, Fraser, Kiritchenko, and Nejadgholi 2023] takes fine-tune methods to control the distribution of generated images. However, updating the parameters of the diffusion model with expensive data collection potentially results in knowledge forgetting, ultimately detrimental to stable generative ability. [Friedrich, Schramowski, and Manuel Brack 2023] allows human instruction, enabling to guide output based on desired criteria. These post-processing methods require significant computational resources and time, which is not efficient enough without a robust framework.

In this paper, we propose a novel post-processing, modelagnostic, and lightweight method namely Fair Mapping. Briefly speaking, there are two additional components in Fair Mapping compared to vanilla diffusion models: The first one is a linear mapping network which is strategically designed to rectify the implicit bias in the representation space of text encoder in text-to-image diffusion models. It addresses the disentanglement of the target language context from implicit language biases by introducing a fair penalty mechanism. This mechanism fosters a harmonious representation of sensitive information within word embeddings via a flexible linear network with only a modest addition of new parameters. At the inference stage, Fair Mapping introduces a detector, which aims to detect input containing potential biased content for a robust generation. To summarize, our contributions are three-fold.

• Analysis of Language Bias and Proposed Fairness Evaluation Metric: We first quantitatively explain the bias issue in generated images caused by the textual condition within text-guided diffusion models, providing insights into the contributing dynamics. We introduce evaluation metrics designed to assess the language bias and diffusion bias of diffusion models in generating text-guided images. These metrics provide a systematic and objective measure for quantifying the reduction of bias in the generative process, enabling a more precise evaluation of fairness outcomes. • Innovative Fair Mapping Module: To mitigate language bias, we develop a model-agnostic and lightweight method namely Fair Mapping. Generally speaking, Fair Mapping introduces a linear network before the Unet structure, which optimizes minimal extra parameters for training, enabling seamless integration into any text-toimage generative models while keeping their parameters unchanged. At the inference stage, there is an additional detector compared to conventional diffusion models, which aims to detect whether input text prompt given by users pass through the linear network for debias.

• Comprehensive Experimental Evaluations: Finally, we conduct comprehensive experiments of our methods to ensure our generated images’ fairness and quality. Specifically, our experiments demonstrate that our method improves performance and outperforms several text-toimage diffusion models based on human descriptions for fairness, while the image quality is very close to and even better than that of the base diffusion models.

# Related Work

Text-guided Diffusion Models. Text-guided diffusion models merge textual descriptions with visual content to create high-resolution, realistic images that align with the semantic guidance provided by the accompanying text prompts [Ramesh, Dhariwal, and Alex Nichol 2022, Saharia, Chan, and Saurabh Saxena 2022, El-Nouby et al. 2018, Kim et al. 2023, Avrahami et al. 2023, Balaji, Nah, and Xun Huang 2022, Feng et al. 2023, He, Salakhutdinov, and Kolter 2023]. However, this fusion of modalities also brings to the forefront issues related to bias and fairness [Struppek, Hintersdorf, and Kersting 2022, Bansal et al. 2022], which have prompted extensive research efforts to ensure that the generated outputs do not perpetuate societal inequalities or reinforce existing biases. In this paper, we explore these challenges and the state-of-the-art solutions aimed at enhancing the fairness and equity of text-guided diffusion models.

Bias in Diffusion Models. While large datasets are commonly used in data-driven generative models, they often contain social and cultural biases [Birhane, Prabhu, and Kahembwe 2021, Schuhmann, Beaumont, and Richard Vencu 2022, Schuhmann, Vencu, and Romain Beaumont 2021]. Previous efforts have addressed this challenge by optimizing model parameters after training [Shen, Du, and Pang 2023, Fraser, Kiritchenko, and Nejadgholi 2023], while [Jiang, Lyu, and Ma 2023] accomplish distributional control by updating the latent code. [Friedrich, Schramowski, and Manuel Brack 2023] introduces a post-processing mechanism based on human instructions. [Chuang, Jampani, and Li 2023] remove biased directions in text embeddings to mitigate bias in visionlanguage. Similarly, [Orgad, Kawar, and Belinkov 2023, Gandikota, Orgad, and Belinkov 2024] update the model’s cross-attention layers to achieve concept editing in certain text for debiasing. In our work, we also seek to bridge this gap by addressing language biases in semantic representation space, thereby contributing to a more comprehensive understanding and mitigation of biases in generative data by employing an end-to-end framework.

Bias in Language Models. The Transformer structure of language models is capable of storing and encoding knowledge, including societal biases reflected in the training data. This capability is also extended to text-to-image diffusion models through the incorporation of attention layers [Meng, Sharma, and Andonian 2022, Arad, Orgad, and Belinkov 2023, Berg, Hall, and Yash Bhalgat 2022]. Ensuring fairness in these models has been extensively studied, especially in the context of large-scale models [Gehman et al. 2020, Abid, Farooqi, and Zou 2021, Bender et al. 2021, Zhang, Wang, and Sang 2022, Radford, Kim, and Chris Hallacy 2021, Tian, Lai, and Moore 2018, Ding et al. 2021]. Efforts have been made to address these biases with approaches [Wang, Zhang, and Sang 2022, Dehouche 2021] aiming to mitigate the impact of bias. In our work, we explore the intersection of bias mitigation efforts in language models, which is a critical juncture in the pursuit of fairness and ethics in artificial intelligence.

# Language Bias in Text-to-Image Diffusion Models

For starters, we list key notations that will be used throughout the paper.

Notations. Consider a keyword set $C$ such as a set of different occupations. For each keyword $c$ , such as "doctor", it has a set $A$ of possible sensitive attributes such as "male" or "female". Note that here we suppose the attribute set is the same for all keywords for convenience. For language bias, we denote $p r o m p t ( a , c )$ for each $a \in A , c \in C$ as a prompt in a uniform and specific format. For example, prompt(male, doctor) $\ c =$ "an image of a male doctor". We also denote prompt $( ^ { \cdot \cdot , } , c )$ as the prompt where there is no sensitive attribute, such as $p r o m p t ( ^ { \cdot } \cdot , \mathrm { d o c t o r } ) = " \mathrm { a n }$ image of a doctor". Given the text encoder for textual conditioning in a text-to-image diffusion model, we extract text representations $f$ and $\{ f _ { j } \} _ { j = 1 } ^ { | A | }$ from prompt $( \cdot \cdot , c )$ and $\{ p r o m p t ( a _ { j } , c ) \} _ { a _ { j } \in A }$ respectively. These representations are essential in conventional text-guided diffusion models for generating coherent and contextually relevant text samples.

Before showing our method for mitigating language bias, it is necessary to address the following fundamental question: Whether there indeed exists implicit language bias even $i f$ there is no explicit sensitive attribute in the textual information of prompt? Based on the above notation, we propose the bias metric for the input prompts and the generated outputs of text-to-image diffusion models.

1) Diffusion Bias. We propose a novel evaluation metric based on group fairness to robustly assess fairness in generative results of diffusion models across diverse groups. This metric captures variations in generated outcomes among demographic attributes [Hardt, Price, and Srebro 2016], such as gender, and quantifies fairness by evaluating equilibrium. Our study adopts a highly specific and constrained definition of fairness in the evaluation process. A diffusion model is absolutely fair if it satisfies that for any keyword $c _ { k } \in C$

$$
\begin{array} { r } { P ( s = a _ { i } | c = c _ { k } ) = P ( s = a _ { j } | c = c _ { k } ) , \mathrm { f o r ~ a l l ~ } a _ { i } , a _ { j } \in A , } \end{array}
$$

where $s$ is the random variable of the sensitive attribute for the output of the diffusion model, $c$ is the random variable of the keyword in conditional textual information, $P ( s = a _ { i } | c = c _ { k } )$ represents the probability of the sensitive attribute $s$ of generative images expressing $a _ { i }$ given a specific conditional prompt with keyword $c _ { k }$ . We define our diffusion bias evaluation criteria towards attribute $a _ { i }$ for $c _ { k }$ as follows:

$$
\begin{array} { l } { { \cal D } B i a s _ { a _ { i } } ( c _ { k } ) = P \left( s = a _ { i } \mid c = c _ { k } \right) } \\ { \displaystyle - \frac { 1 } { \mid A \mid } \sum _ { a _ { j } \in A } P \left( s = a _ { j } \mid c = c _ { k } \right) } \end{array}
$$

Thus, based on (1), for a keyword $c _ { k }$ , our metric on the diffusion bias is defined as follows:

$$
{ \mathrm { B i a s S c o r e } } ( c _ { k } ) = { \sqrt { \frac { 1 } { \mid A \mid } \sum _ { a _ { i } \in A } \left( D B i a s _ { a _ { i } } ( c _ { k } ) \right) ^ { 2 } } } .
$$

Thus, for a dataset $C$ that contains keywords, our fair evaluation metric on the diffusion bias is $\begin{array} { r } { \frac { 1 } { | C | } { \bf \dot { \sum } } _ { c _ { k } \in C } \bf B i a s S c o r e ( } c _ { k } )  \end{array}$ . A smaller value indicates that the method is fairer.

2) Language Bias. We assess language bias by incorporating semantic similarity calculation [Chen, Kornblith, and Mohammad Norouzi 2020, Mikolov, Chen, and Greg Corrado 2013] between prompt $( a , c )$ and prompt $( " , c )$ . Specifically, we use Euclidean distance to evaluate the distance between prompt with and without explicit sensitive attributes. The closer distance indicates a potential bias towards one specific sensitive attribute. We define our language bias evaluation

Occupation Diffusion Bias Towards male aerospace engineer, accountant, aide, author, T   
Male- baker, bus driver, career counselor, claims plumber   
biased appraiser, cleaner, clerk, cook, compliance Language BiasTowards male officer, computer support specialist… artist, author, childcare worker, claims   
Female- appraiser, customer service representative,   
biased dental assistant, designer, event planner, executive assistant, financial manager… waitress

criteria towards attribute $a _ { i }$ for keyword $c _ { k }$ and our input prompts as $L B i a s _ { a _ { i } } ( c _ { k } )$ :

$$
L B i a s _ { a _ { i } } ( c _ { k } ) = - \| f _ { j } - f \| _ { 2 } + \frac { 1 } { | A | } \sum _ { a _ { j } \in A } \| f _ { j } - f \| _ { 2 } ,
$$

where $\| f _ { j } - f \| _ { 2 }$ represents the Euclidean distance between the prompt generated with the sensitive term $a _ { i }$ and the keyword $c _ { k }$ , compared to the prompt generated with no sensitive term. Thus, the total language bias for keyword $c _ { k }$ and our prompt is $\begin{array} { r } { \frac { 1 } { \left| A \right| } \sum _ { a _ { i } \in A } L \bar { B } i a \bar { s _ { a _ { i } } } ( c _ { k } ) } \end{array}$ .

Based on the above two metrics, we conduct following experiments in the occupation keyword set listed in Appendix: 1) First, we calculate language bias on every occupation over sensitive attribute gender. 2) Then, for each occupation c, we use the following prompt format prompt $( \cdot \cdot , c )$ for guiding the stable diffusion model to generate 100 images and measure the diffusion bias.

Figure 2 shows the experimental results. In the left region of Figure 2, we analysed language bias on specific occupations and provided examples to illustrate some samples aligning with societal stereotypes. For example, our analysis of the term "aerospace engineer" highlights a clear gender bias favoring males, reflecting the gender stereotype associated with this profession in the real world. The right-hand side of Figure 2 displays the biases associated with various occupations by creating a scatter plot with diffusion bias on the y-axis and language bias on the x-axis. Moreover, we discovered the majority of data are concentrated in where a male bias was revealed in both diffusion and language bias. This research implies that language bias and diffusion bias are mutually reinforcing, which infuse in the cross-attention layers of the UNet. In summary, implicit language bias is one of the direct factors leading to diffusion bias. From the view of language assumption, it is reasonable to reduce the impact it has on diffusion bias and promote more equitable and unbiased generative outcomes.

# Mitigating Implicit Bias via Fair Mapping

Motivated by above analysis, we introduce Fair Mapping to disentangle the implication of language from diffusion generation. Generally speaking, Fair Mapping introduces two additional components as a post-processing method on well-trained text-to-image diffusion models: A linear network named Fair Mapping and a detector that will be activated during the inference stage. Training and inference procedures of

Fair Mapping are elucidated in Figure 3. We implement Fair Mapping consisting of linear stacking networks, drawing inspiration from StyleGAN [Karras, Laine, and Aila 2018] and MixFairFace [Wang et al. 2022], which enables the correction of native language semantic features, ultimately leading to their debiasing and alignment with the balanced embedding space. The detector at the inference stage is used for robust generation to decide whether the input will be debiased. We provide details on the training and inference stages hereafter.

# Training Fair Mapping Network

Recall that our linear network Fair Mapping, denoted as $M$ , is to transform the representation of the input prompt to a debiased space. Our idea is to maintain the representation aligned with a balanced state of sensitive attributes, so they can introduce little assumption to visual representation. First, given a keyword dataset and group of sensitive attributes, we will construct two distinct types of prompts that have a consistent and uniform format for each keyword $c$ . The first type constitutes the original input prompt, denoted as $p r o m p t ( ^ { \cdot } , c )$ , where we explicitly exclude any sensitive attributes (represented as ‘ ’). It does not prioritize explicit sensitive attribute information. In contrast, the second type of prompt, prompt $( a _ { j } , c )$ , where $a _ { j } \in A$ , incorporates specific sensitive words. These prompts are designed to quantitatively explore the language relationship between sensitive attributes and the target keyword. Moreover, we have language representation vectors $f$ and $\{ f _ { j } \} _ { j = 1 } ^ { | A | }$ from prompt $( \cdot \cdot , c )$ and $\{ p r o m p t ( a _ { j } , c ) \} _ { a _ { j } \in A }$ respectively. Given these, our Fair Mapping is after the pretrained text encoder and further transforms these representation vectors: $v = M ( f ) , v _ { j } = M ( f _ { j } )$ for $a _ { j } \in A$ .

Our aim with this transformation process is to ensure that the representations are fair and unbiased, and exhibit equitable treatment across sensitive attributes. In detail, the objectives of $v$ and $\{ v _ { j } \} _ { j }$ are two-fold: 1) They should maintain semantic consistency akin to $f$ and $\{ f _ { j } \} _ { j }$ respectively, serving as keeping the original contextual information. 2) More importantly, $v$ should equalize the representation of different demographic groups and prevent the encoding of implicit societal biases. Therefore, we employ bias-aware objectives and regularization techniques to guide representations to be balanced from sensitive information. Below we will discuss how to achieve the above two goals.

Keeping semantic consistency. Our general idea is designed to maintain consistency and semantic coherence between the original embeddings and mapped embeddings. To achieve this objective, we employ a strategy for minimizing the disparity between pre-transformed and post-transformed features in the embedding space. Specifically, we adopt the mean squared error (MSE) as a metric to measure the reconstruction error, drawing inspiration from [Kingma and Welling 2014]. By applying this metric, we compute a semantic consistency loss for each keyword:

$$
\mathcal { L } _ { t e x t } = \frac { 1 } { | A | + 1 } \left( | | \boldsymbol { v } - \boldsymbol { f } | | _ { 2 } ^ { 2 } + \sum _ { a _ { j } \in A } | | \boldsymbol { v } _ { j } - \boldsymbol { f } _ { j } | | _ { 2 } ^ { 2 } \right) .
$$

Through the minimization of this loss over all keywords, we can ensure that the mapped embeddings preserve the crucial information and semantic attributes inherent in the original embeddings. This process safeguards the fidelity and integrity of the data throughout the mapping transformation.

Fairness loss. Distances of embeddings to groups with sensitive attributes can inadvertently encode demographic information. For example, if the word "doctor" is closer to "male" than "female" in the representation space of language models, it may inherently convey gender bias [Chen, Kornblith, and Mohammad Norouzi 2020]. To mitigate this issue, we employ an invariant loss that entails the adjustment of associations between sensitive attributes and naive prompts during the training process using mapping offsets. The primary objective is to diminish associations of implicit sensitive attributes with text embeddings through the application of mapping offsets, promoting a more unbiased representation.

To equalize the representations of attributes, we ensure that representations of prompts have balanced distances from the representations of prompts with specific sensitive attributes, thereby reducing the bias assumptions in the semantic space. In the case where the size of the sensitive group $A$ is 2, we can minimize the difference in distance between the native embeddings, expressed as $| d ( v , v _ { 1 } ) - d ( v , v _ { 2 } ) |$ . Here, $d ( \cdot , \cdot )$ represents the Euclidean distance [Dokmanic et al. 2015] between embeddings. To address the computational complexity when dealing with a large attribute set containing multiple sensitive groups, we can optimize representation bias by reducing the variance in the distance between embeddings instead of calculating the difference in the distance for each pair. The fairness loss term denoted as $\mathcal { L } _ { f a i r }$ can be formulated as follows:

$$
\mathcal { L } _ { f a i r } = \sqrt { \frac { 1 } { \vert A \vert } \sum _ { a _ { j } \in A } \left( d ( v , v _ { j } ) - \overline { { d ( v , \cdot ) } } \right) ^ { 2 } } .
$$

Here, $d ( v , v _ { i } )$ represents the Euclidean distance between the native embedding $v$ and the specific sensitive attribute embedding $\boldsymbol { v } _ { i }$ . $\overline { { { d ( v , \cdot ) } } }$ refers to the average distance between the native embedding $v$ and all the sensitive attribute embeddings $v _ { j }$ . By incorporating this fairness loss term into the training objective, we aim to minimize the variance in the distance between $\boldsymbol { v }$ and $v _ { j }$ with sensitive attributes. To optimize the overall objective, we combine the semantic consistency loss, denoted as $\mathcal { L } _ { t e x t }$ (from (5)), with the estimated bias difference from the fairness penalty (from (6)). This results in the following combined loss function for each keyword:

$$
\begin{array} { r } { \mathcal { L } = \mathcal { L } _ { t e x t } + \lambda \mathcal { L } _ { f a i r } , } \end{array}
$$

where $\lambda$ is a hyperparameter that controls the trade-off between semantic consistency and fairness. By minimizing this combined loss function, we aim to simultaneously ensure semantic consistency and reduce bias in the language representation. We optimize the parameters of Fair Mapping while keeping the parameters of the diffusion model fixed.

# Inference

In the inference stage, as Figure 3 demonstrated, the detector is introduced before Fair Mapping. To build a robust system for users, the main goal of the detector is to decide whether the text should pass or skip Fair Mapping. In detail, it aims to match the target input with implicit sensitive attributes and avoid solving with explicit sensitive attributes. Due to the space limit, details for the detector can be found in Appendix.

![](images/9714e2af01d85dcce372659401e2a87270931663faed770573230b1f9dd50f73.jpg)  
Figure 3: Left: In the training stage, the parameters of the text encoder are frozen, and we apply $\mathcal { L } _ { t e x t }$ and $\mathscr { L } _ { f a i r }$ to update Fair Mapping. $d _ { a }$ denotes the distance between $\boldsymbol { v } _ { a }$ and $\boldsymbol { v }$ . Right: In the inference stage, the detector after the text encoder determines whether the text should pass or skip the Fair Mapping linear network.

Discussions. We can easily see that there are several strengths of Fair Mapping. Firstly, Fair Mapping is modelagnostic, i.e., as it only introduces a linear mapping network and a detector after the text encoder, it can easily be integrated into any text-to-image diffusion model as well as be a plug-in text-encoder with the same parameters. Secondly, Fair Mapping is lightweight. As a post-processing approach, Fair Mapping only introduces an additional linear map to be optimized while keeping the parameters of the diffusion model fixed. Moreover, as we will mention in the experiments, an eight-layer linear network is sufficient to achieve good performance on both utility and fairness with little additional time cost. Finally, our method is quite flexible. Due to the simplicity of our loss for each keyword, our linear network can be customized for any other prompts, loss of semantic consistency, and loss of fairness.

# Experiments

We mainly compared our Fair Mapping (FairM) with three text-guided diffusion models: Stable Diffusion (SD) [Rombach, Blattmann, and Dominik Lorenz 2022], Structured Diffusion (StruD) [Feng, He, and Tsu-Jui 2023] and Composable Diffusion (ComD) [Tang, Yang, and Chenguang 2023], as well as three state-of-the-art methods for fair diffusion generation: Fair Diffusion (FairD) [Friedrich, Schramowski, and Manuel Brack 2023], Unified Concept Editing (UCE) [Gandikota, Orgad, and Belinkov 2024] and Debias Visual Language (debiasVL) [Chuang, Jampani, and Li 2023]. We report the performance of our models from three aspects: Our Fair Mapping 1) outperforms baselines in fairness evaluation and computation cost, 2) showcases alignment and diversity in human-related descriptions by quantitative analysis, 3) is more lightweight and introduces acceptable time overhead 4) matches human preferences in image quality and text alignment of state-of-the-art text-to-image diffusion methods.

# Experimental Setup

Datasets. We select a total of 150 occupations and 20 emotions for the fair human face image generation following [Ning, Li, and Jianlin $\mathrm { S u } ~ 2 0 2 3 ]$ . For sensitive groups, we choose gender groups (male and female), racial groups (Black, Asian, White, Indian) and Age (young, middle age, old) provided by [Kärkkäinen and Joo 2021]. We provide a comprehensive list of keywords in Appendix.

Implementation details. In our experiments, we use stable Diffusion v1.5 [Rombach, Blattmann, and Dominik Lorenz 2022] as the base model and implement 50 DDIM denoising steps for generation. Standardized prompts are used for occupations and emotions. More details are in Appendix.

Evaluation metrics. Besides the two evaluation metrics in the above Section , language bias and diffusion bias (we denote as Bias), below we introduce some metrics for the utility and the quality of the generated images.

1) Alignment: To measure the alignment between generated images and human-related content, we adopt the CLIPScore [Hessel, Holtzman, and Maxwell Forbes 2021], which measures the distance between input textual features and generated image features. Due to the limitation [Otani, Togashi, and Yu Sawai 2023] of capturing specific requirements of human-related textual generation, we introduce the HumanCLIP metric (see Appendix for details) focusing on evaluating the CLIP-Score related to human appearance.

2) Diversity: We use intra-class average distance (ICAD) [Le and Odobez 2018] to evaluate the diversity of generative results. For each keyword, we measure the average distance between all generated images and the center of images by using a distance metric by squared Euclidean distance (see Appendix for details). A lower intra-class average distance suggests that the generative results are more similar and excessively focus on a few specific samples.

# Experimental Results

Figure 4 demonstrates the effectiveness of our method for debiasing. When compared to existing text-guided diffusion approaches, Fair Mapping demonstrates an enhanced capacity for generating diverse sensitive groups while upholding the stability of the generated results.

Fair Mapping can mitigate bias in diffusion models. The comparative analysis depicted in Figure 1 in our Appendix underscores the significant strides made by our method in mitigating language bias and diffusion bias. Besides a general illustration of language bias and diffusion bias. Table 1 showcases the fair evaluation results for sensitive attributes: gender and race. Fair Mapping demonstrates significant improvements in fairness compared to other baseline approaches. It achieves lower Diffusion Bias compared to Stable Diffusion for all sensitive attribute groups, especially on gender $18 \%$ for Occupation and $54 \%$ for Emotion) and race ( $14 \%$ for Occupation and $38 \%$ for Emotion). Besides, compared with other debiasing methods, Our method outperforms in enhancing the representation of minority groups. We did not test for Fair Diffusion primarily because the generated results heavily depend on subjective post-processing by humans.

![](images/b8d8ff2c79bdce0d8adb7994e71340d431881d6fb846404c6fcc357574cfbdcb.jpg)  
Engineer   
Figure 4: Comparison with original SD and different debiasing methods in prompt "an image of an engineer". Our method makes generated images equally represent genders and races. More visual results are in Appendix.

Table 1: Fair evaluation results of sensitive group gender and race. G., R., and A. denote Gender, Race and Age, respectively. The bold value denotes the best performance.

Alignment and diversity. In Table 2, when comparing with baselines for text-to-image diffusion generation, debiasing methods restrict the model’s ability to generate images as a trade-off for fair generation. Although both UCE methods maintain the quality of generation, they perform poorly in Human-CLIP evaluation. Furthermore, our method showcases a strong alignment with human-related prompts, with $13 \%$ improvements in the Human-CLIP metric for occupation. our method successfully captures human-related characteristics in generated images, despite having a slight loss in CLIP-Score.

Table 2: Evaluation results of image alignment and diversity. CLIP denotes as CLIP-Score and CLIP-H denotes as CLIPHuman. The bold value denotes the best performance.   

<html><body><table><tr><td>Dataset</td><td>Models</td><td>Bias (G.)</td><td>Bias (R.)</td><td>Bias (A.)</td></tr><tr><td rowspan="7">Occupation</td><td>SD</td><td>0.4466</td><td>0.4652</td><td>0.3173</td></tr><tr><td>StruD</td><td>0.4141</td><td>0.4100</td><td>0.2911</td></tr><tr><td>ComD</td><td>0.4027</td><td>0.4203</td><td>0.2884</td></tr><tr><td>UCE</td><td>0.3802</td><td>0.3266</td><td>0.2688</td></tr><tr><td>debiasVL</td><td>0.4573</td><td>0.4178</td><td>0.2982</td></tr><tr><td>FairM(Ours)</td><td>0.3625</td><td>0.2113</td><td>0.2547</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td rowspan="6">Emotion</td><td>SD</td><td>0.2599</td><td>0.1893</td><td>0.2142</td></tr><tr><td>StruD</td><td>0.2368</td><td>0.1824</td><td>0.2529</td></tr><tr><td>ComD</td><td>0.2344</td><td>0.1489</td><td>0.2318</td></tr><tr><td>UCE</td><td>0.2314</td><td>0.1505</td><td>0.1993</td></tr><tr><td>debiasVL</td><td>0.2992</td><td>0.1592</td><td>0.2231</td></tr><tr><td>FairM(Ours)</td><td>0.2231</td><td>0.1178</td><td>0.1561</td></tr></table></body></html>

<html><body><table><tr><td rowspan="2">Models</td><td colspan="3">Occupation</td><td colspan="3">Emotion</td></tr><tr><td>CLIP</td><td>CLIP-H</td><td>ICAD</td><td>CLIP</td><td>CLIP-H</td><td>ICAD</td></tr><tr><td>SD</td><td>0.2320</td><td>0.1339</td><td>3.64</td><td>0.2026</td><td>0.1399</td><td>3.84</td></tr><tr><td>StruD</td><td>0.2339</td><td>0.1284</td><td>3.61</td><td>0.1930</td><td>0.1103</td><td>3.82</td></tr><tr><td>ComD</td><td>0.2299</td><td>0.1367</td><td>3.60</td><td>0.1901</td><td>0.1155</td><td>3.82</td></tr><tr><td>FairD-Gender</td><td>0.2274</td><td>0.1348</td><td>3.62</td><td>0.1894</td><td>0.1298</td><td>3.91</td></tr><tr><td>UCE-Gender</td><td>0.2280</td><td>0.1133</td><td>3.61</td><td>0.1848</td><td>1.1198</td><td>3.76</td></tr><tr><td>dibiasVL-Gender</td><td>0.2011</td><td>0.1223</td><td>3.47</td><td>0.1806</td><td>0.0890</td><td>3.79</td></tr><tr><td>FairM-Gender</td><td>0.2021</td><td>0.1494</td><td>3.69</td><td>0.1809</td><td>0.1366</td><td>3.91</td></tr><tr><td>FairD-Race</td><td>0.2239</td><td>0.1292</td><td>3.64</td><td>0.1882</td><td>0.1266</td><td>3.89</td></tr><tr><td>UCE-Race</td><td>0.2327</td><td>0.1045</td><td>3.58</td><td>0.1914</td><td>0.0968</td><td>3.77</td></tr><tr><td>dibiasVL-Race</td><td>0.2187</td><td>0.1022</td><td>3.68</td><td>0.1795</td><td>0.1083</td><td>3.77</td></tr><tr><td>FairM-Race</td><td>0.2197</td><td>0.1522</td><td>3.68</td><td>0.1848</td><td>0.1324</td><td>3.95</td></tr><tr><td>FairD-Age</td><td>0.2171</td><td>0.1084</td><td>3.56</td><td>0.1846</td><td>0.0923</td><td>3.55</td></tr><tr><td>UCE-Age</td><td>0.2327</td><td>0.1045</td><td>3.58</td><td>0.1914</td><td>0.0968</td><td>3.77</td></tr><tr><td>dibiasVL-Age</td><td>0.2187</td><td>0.0942</td><td>3.62</td><td>0.1758</td><td>0.0949</td><td>3.88</td></tr><tr><td>FairM-Age</td><td>0.2141</td><td>0.1520</td><td>3.62</td><td>0.1743</td><td>0.1157</td><td>3.95</td></tr></table></body></html>

Regarding diversity, for the Occupation dataset in Table 2, our method demonstrates a significant improvement in ICAD of generated results when compared to Stable Diffusion. Specifically, the metrics show an increase from 3.64 to 3.68 and 3.69 for gender and race groups, respectively. For the Emotion dataset, the ICAD of our method is still better than other debiasing text-to-image methods. These results demonstrate our method’s ability to excel in generating varied results in diverse environments.

Computation cost. On an Nvidia V100, our debiasing method trains on 150 occupations in just 50 minutes, demonstrating impressive efficiency. As shown in Table 3, our method generates 100 images for a single occupation in 434 seconds, comparable to SD. While UCE increases the minimal inference time by adjusting origin parameters, it requires significantly more training time.

Table 3: Evaluation results in wall-clock time consumption on generation of 100 images.   

<html><body><table><tr><td>Models</td><td>Wall-clock Time (seconds)</td></tr><tr><td>SD</td><td>424</td></tr><tr><td>FairD</td><td>1463</td></tr><tr><td>UCE</td><td>426</td></tr><tr><td>debiasVL</td><td>764</td></tr><tr><td>FairM(ours)</td><td>434</td></tr></table></body></html>

Human preference. We conduct a human study about the fidelity and alignment of our method in Table 4. For fidelity, human preference scores reveal our method consistently outperforms other generative methods both for occupation and emotion description. As our method introduces a trade-off between prioritizing fairness and maintaining the alignment of facial expressions and textual descriptions, some participants expressed dissatisfaction to our method’s performance in achieving consistency between facial expressions and textual descriptions. Future research may focus on enhancing the consistency of text prompts while balancing the bias.

<html><body><table><tr><td rowspan="2">Models</td><td colspan="2">Occupation</td><td colspan="2">Emotion</td></tr><tr><td>Fidelity</td><td>Alignment</td><td>Fidelity</td><td>Alignment</td></tr><tr><td>SD</td><td>2.7558</td><td>3.6760</td><td>2.7230</td><td>3.4929</td></tr><tr><td>StruD</td><td>2.5399</td><td>2.9953</td><td>3.3427</td><td>3.3615</td></tr><tr><td>ComD</td><td>2.6667</td><td>3.0375</td><td>1.9718</td><td>3.6854</td></tr><tr><td>FairM-Gender</td><td>3.0140</td><td>3.0760</td><td>3.4883</td><td>3.2431</td></tr><tr><td>FairM-Race</td><td>3.0798</td><td>3.3661</td><td>3.0140</td><td>3.3475</td></tr><tr><td>Real Image</td><td>3.4694</td><td></td><td>3.5576</td><td></td></tr></table></body></html>

Table 4: Evaluation results for Human Preference. The higher the score, the more it aligns with human preferences. Please refer to the Appendix for details.

# Ablation Study

Finally, we conduct an ablation study on the necessities of our two components $\mathcal { L } _ { t e x t }$ in Eq.5 and $\mathscr { L } _ { f a i r }$ in loss (6). Table 5 shows the results of an ablation study examining the influence of different factors in the loss terms on model performance. We implement our experiments on sensitive attribute groups Gender and dataset Occupation. First, we can see that $\mathcal { L } _ { t e x t }$ can function independently, as indicated by individual rows representing the method’s performance when only one of these criteria is considered. However, it is evident that $\mathscr { L } _ { f a i r }$ alone is not effective, indicating the necessity of $\mathcal { L } _ { t e x t }$ to establish an effective semantic space.

Secondly, we can observe the combination of generating diverse sensitive attributes $( \mathcal { L } _ { t e x t } )$ and maintaining fairness in representation $( \mathcal { L } _ { f a i r } )$ achieves the lowest diffusion bias, indicating the superior performance of fairness.

Table 5: An ablation study on $\mathcal { L } _ { f a i r }$ and $\mathcal { L } _ { t e x t }$ in the loss function. O. denotes the Occupation dataset and E. denotes the Emotion dataset.   

<html><body><table><tr><td>Ltext</td><td>Lfair</td><td>Bias (O.)</td><td>Bias (E.)</td></tr><tr><td></td><td>1</td><td>0.4466</td><td>0.4622</td></tr><tr><td>1</td><td>√</td><td>1</td><td>1</td></tr><tr><td>√</td><td>1</td><td>0.4030</td><td>0.3862</td></tr><tr><td>√</td><td>√</td><td>0.3624</td><td>0.2113</td></tr></table></body></html>

In Figure 5, we study the effect of the fairness penalty regularization parameter $\lambda$ . Firstly, we can see that when $\lambda$ becomes larger, both CLIP-Score and Huma-CLIP decrease. This is due to a larger $\lambda$ implying that we will more focus on fairness rather than the quality of the generated images. Moreover, these two metrics only slightly decrease when $\lambda$ is less than 0.1, which further supports our previous conclusion that our method has almost the same quality of generated images. Secondly, we can observe that when $\lambda$ is larger and smaller than 0.1, the BiasScore will decrease. However, when $\lambda$ is greater than 0.1, the BiasScore will increase. We think in this case, the generated images experience severe distortion, resulting in a reduced amount of semantic information and consequently leading to a decline in fairness as well.

![](images/5040bb5e35cbbd94196e420b538880819a8e0f6a686b7eb03e29d9c8336aa315.jpg)  
Figure 5: The influence of different regularization parameter $\lambda$ . (O) and (E) denote Occupation dataset and Emotion dataset, respectively.

# Conclusion

In this paper, we advocate that the implicit biases from input text prompts contribute to significant observed bias in current text-to-image diffusion models. We develop Fair Mapping, a model-agnostic debiasing mapping network, to effectively mitigate bias with few additional parameters for training. Meanwhile, it is flexible for Fair Mapping to adapt different customized data. Furthermore, in fairness evaluation metrics, experiments demonstrate substantial efficiency compared to text-guided diffusion models and other debiasing methods.