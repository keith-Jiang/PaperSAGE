# Testing Causal Models with Hidden Variables in Polynomial Delay via Conditional Independencies

Hyunchai Jeong\*1, Adiba Ejaz\*2, Jin Tian3, Elias Bareinboim2

1Purdue University 2Columbia University 3Mohamed bin Zayed University of Artificial Intelligence jeong3 $@$ purdue.edu, adiba.ejaz@cs.columbia.edu, jin.tian@mbzuai.ac.ae, eb@cs.columbia.edu

# Abstract

Testing a hypothesized causal model against observational data is a key prerequisite for many causal inference tasks. A natural approach is to test whether the conditional independence relations (CIs) assumed in the model hold in the data. While a model can assume exponentially many CIs (with respect to the number of variables), testing all of them is both impractical and unnecessary. Causal graphs, which encode these CIs in polynomial space, give rise to local Markov properties that enable model testing with a significantly smaller subset of CIs. Model testing based on local properties requires an algorithm to list the relevant CIs. However, existing algorithms for realistic settings with hidden variables and non-parametric distributions can take exponential time to produce even a single CI constraint. In this paper, we introduce the c-component local Markov property (C-LMP) for causal graphs with hidden variables. Since C-LMP can still invoke an exponential number of CIs, we develop a polynomial delay algorithm to list these CIs in poly-time intervals. To our knowledge, this is the first algorithm that enables poly-delay testing of CIs in causal graphs with hidden variables against arbitrary data distributions. Experiments on real-world and synthetic data demonstrate the practicality of our algorithm.

# Code — https://github.com/CausalAILab/ ListConditionalIndependencies

# 1 Introduction

Causal models are the daily bread of many fields of research (Pearl 2000; Spirtes, Glymour, and Scheines 2001), but tools for testing them are lacking. In various studies, researchers posit a causal model and use it to compute causal effects from data (Tennant et al. 2020; Hoover 1990; King et al. 2004; Sverchkov and Craven 2017; Robins, Hernan, and Brumback 2000; Rotmensch et al. 2017). The model imposes testable constraints on the statistics of the data collected. Before using the model for causal inference, it’s crucial to test if these constraints are met, and adjust the model as needed (Pearl 1995, 2000; Bareinboim and Pearl 2016; Malinsky 2024; Ankan and Textor 2022).

Causal directed acyclic graphs (DAGs) are one popular model for causal assumptions (Pearl 2000; Spirtes, Glymour, and Scheines 2001). Conditional independencies (CIs) are the most basic constraint that a causal DAG imposes on observational data. The study of CIs in the context of graphical models dates back to at least the 1980’s (Pearl 1988; Dawid 1979; Spirtes et al. 1998; Pearl 1998; Pearl and Meshkat 1999; Pearl 2000). A classic problem in this line of research is: given observational data and a hypothesized causal graph, do all the CIs implied by this graph hold in the data? If the answer is no, the DAG may be revised.

A key idea in the early literature of graphical models was to use a DAG to represent the constraints of probability distributions. A multivariate probability distribution may encode exponentially many CIs with respect to the number of variables. A DAG can encode these CIs in polynomial space. The $d$ -separation criterion allows us to derive the CIs encoded in a DAG (Pearl 1988). The global Markov property of a DAG is the set of all CIs encoded in it (Pearl 1988). There is also a well-known local Markov property for DAGs (Pearl 1988; Lauritzen et al. 1990), which states that each variable must be conditionally independent of its non-descendants given its parents. Since the CI relation is a semi-graphoid, the linearly many CIs of the local Markov property together imply the exponentially many CIs of the global Markov property. This means that to test a DAG against observational data, it suffices to perform a linear number of CI tests as given by the local Markov property. For concreteness, consider the DAG $\mathcal { G } ^ { 1 }$ in Fig. 1a and assume all variables $\{ A , B , \dots , H , U _ { 1 } , U _ { 2 } , U _ { 3 } \}$ are observed. Though $\mathcal { G } ^ { 1 }$ encodes 35787 CIs, only 11 need testing by the local Markov property. For example, if we test that ${ \dot { F } } \perp { \mathsf { i } } \ \{ A , B \} \ { \mathsf { i } } \ \ \{ C \} $ , we do not need to test that $F \bot \bot \{ A \} \mid \{ B , { \dot { C } } \}$ , since the former implies the latter by the weak union axiom.

Unobserved confounding is a widespread phenomenon in real-world settings (Fisher 1936). It occurs when a hidden variable causally affects two or more observed variables. The local Markov property can be used to test Markovian causal DAGs, which represent models without unobserved confounding. However, it cannot be used to test non-Markovian DAGs, which represent models with unobserved confounding. This is because if the parents of a variable are partially unobserved, we cannot test CIs that require conditioning on these parents (Fig. 1a). Since the assumption of no unobserved confounding rarely holds in practice, alternative ways to test non-Markovian DAGs have been developed (Tian and

Pearl 2002b; Kang and Tian 2009; Geiger and Meek 1998, 1999; Richardson 2003, 2009; Hu and Evans 2023). Despite their power, these works either (a) make strong assumptions on the DAG or probability distribution, or (b) do not provide an algorithm to query their required CI tests in poly-time intervals, with naive algorithms taking exponential time to output a single CI constraint.

Summary of contributions. We give the first efficient algorithm for testing causal DAGs with hidden variables via conditional independencies. This enables researchers to test their causal assumptions using observational data prior to inference. Importantly, our approach extends to arbitrary data distributions and networks of unobserved confounding.

This result builds on a newer, fine-grained characterization of CIs in graphs based on a new construct called ancestral c-components (i.e., connected components in the bidirected skeleton). In particular, we show that $O ( n 2 ^ { s } )$ CI tests (Prop. 1) are required to test a DAG on $n$ variables whose largest c-component has size $s$ . This is an exponential improvement over naively testing all $\Theta ( 4 ^ { n } )$ CI constraints encoded in the DAG. The upshot is largest for DAGs with many variables but small c-components. For instance, the DAG $\mathcal { G } ^ { \bar { 2 } }$ in Fig. 1b implies 753 CIs, but only 5 really need testing. More specifically, our contributions are as follows:

1. We introduce the c-component local Markov property, or C-LMP (Def. 5). We show that C-LMP and the global Markov property are equivalent, admitting the same set of probability distributions for a given DAG. We then show an important property of C-LMP: a one-to-one mapping between the CI constraints it invokes and ancestral $c$ - components (Thm. 2). 2. Building on this characterization, we develop the first algorithm (LISTCI) capable of listing all testable CI constraints of C-LMP in polynomial delay (Thm. 3). On a DAG with $n$ nodes and $m$ edges, LISTCI takes $O ( n ^ { 2 } ( n + m ) )$ time to return each new CI constraint, if one exists, or exit when it has exhausted all CI constraints.

Experiments with synthetic data and a real-world protein signaling dataset (Sachs et al. 2005) corroborate the theoretical findings. For the sake of space, proofs are provided in Appendix C.

# 2 Preliminaries

Notation. We use capital letters to denote variables $( X )$ , small letters for their values $( x )$ , and bold letters for sets of variables $( \mathbf { X } )$ and their values $\mathbf { \tau } ( \mathbf { x } )$ . The probability distribution over a set of variables $\mathbf { X }$ is denoted by $P ( \mathbf { X } )$ . We consistently use $P ( \mathbf { x } )$ as abbreviations for probabilities $P ( \mathbf { X } = \mathbf { x } )$ . For disjoint sets of variables $\mathbf { X } , \mathbf { Y } , \mathbf { Z }$ , we use $\mathbf { X } \perp \perp \mathbf { Y } \mid \mathbf { Z }$ to denote that $\mathbf { X }$ and $\mathbf { Y }$ are conditionally independent given $\mathbf { Z }$ .

Structural causal models. The basic framework of our analysis rests on structural causal models (SCMs) (Pearl 2000, Def. 7.1.1). An SCM $\mathcal { M }$ is a quadruple $\begin{array} { l } { \mathcal { M } \ = } \end{array}$ $\langle \mathbf { V } , \mathbf { U } , \mathcal { F } , P ( \mathbf { u } ) \rangle$ where $\mathbf { V }$ and $\mathbf { U }$ are sets of endogeneous and exogeneous variables, respectively. $\mathcal { F }$ is a set of functions: each $V \in \mathbf { V }$ is a function $f _ { V } ( \mathbf { P A } _ { \mathbf { V } } , \mathbf { U } _ { \mathbf { V } } )$ of its endogeneous and exogeneous parents, $\mathbf { P A } \mathbf { v } \subseteq \mathbf { V }$ and $\mathbf { U } _ { \mathbf { V } } \subseteq \mathbf { U }$ respectively. $P ( \mathbf { u } )$ is a joint distribution over U. Each SCM $\mathcal { M }$ induces an observed distribution $P ( \mathbf { v } )$ over V. An SCM is said to be Markovian if $\mathbf { U } _ { V } , \mathbf { U } _ { W }$ are independent for every distinct V ${ \boldsymbol { \mathscr { r } } } , W \in \mathbf { V }$ , and non-Markovian otherwise. For a more detailed survey on SCMs, we refer to (Pearl 2000; Bareinboim et al. 2022).

![](images/612639df166d7e6101fffc46f1ac8584701160f994cfe83d88d2d632636fde27.jpg)  
Figure 1: (a) A causal DAG $\mathcal { G } ^ { 1 }$ in which the local Markov property implies the CI: $H \bot \{ A , B , C , E , F \} \mid \{ D , U _ { 1 } , U _ { 3 } \}$ . If $U _ { 1 }$ and $U _ { 3 }$ are unobserved, we cannot test this CI. (b) We project $\mathcal { G } ^ { 1 }$ onto its observed variables to get $\mathcal { G } ^ { 2 }$ . In $\mathcal { G } ^ { 2 }$ , the c-component local Markov property invokes the testable CI: $H \perp \perp \{ A , E , F \} \mid \{ B , C , D \}$ .

Causal graphs. The causal graph $\mathcal { G }$ for an SCM $\mathcal { M } =$ $\langle { \bf V } , { \bf U } , \mathcal { F } , \bar { P } ( \bar { \bf u } ) \rangle$ is constructed as follows: (1) add a vertex for every $V \in \mathbf { V }$ (2) add an edge $V _ { i } \to V _ { j }$ for every $V _ { i } , V _ { j } \in$ $\mathbf { V }$ if $V _ { i } \in \mathbf { P A } _ { \mathbf { V } _ { j } }$ (3) add a dashed bidirected edge between $V _ { i } , V _ { j }$ if $\mathbf { U } _ { i } , \mathbf { U } _ { j }$ are correlated or $\mathbf { U } _ { i } \cap \mathbf { U } _ { j } \neq \emptyset$ . $\mathcal { G }$ is said to be Markovian if it contains only directed edges, and semiMarkovian otherwise.

We denote the sets of parents, ancestors, and descendants of $\mathbf { X }$ (including $\mathbf { X }$ itself) in $\mathcal { G }$ as $P a ( \mathbf { X } ) , A n ( \mathbf { X } )$ , and $D e ( \mathbf { X } )$ , respectively. The set of non-descendants of $\mathbf { X }$ in $\mathcal { G }$ is denoted $N d ( \mathbf { X } ) = \mathbf { V } \backslash D e ( \mathbf { X } )$ , which does not include $\mathbf { X }$ itself. The set of spouses of $\mathbf { X }$ in $\mathcal { G }$ is $S p ( \mathbf { X } ) = \bigcup _ { X \in \mathbf { X } } \{ Y \mid$ $Y  X \}$ . X is said to be an ancestral set if it contains its own ancestors, i.e., $\mathbf { X } = A n ( \mathbf { X } )$ . We use $\mathcal { G } _ { \mathbf { X } }$ to denote the induced subgraph of $\mathcal { G }$ on $\mathbf { X } \subseteq \mathbf { V }$ . A subscript $\mathcal { G } ^ { \prime }$ , e.g., $A n ( \mathbf { X } ) _ { \mathcal { G } ^ { \prime } }$ indicates that the set is computed from the subgraph $\mathcal { G } ^ { \prime }$ . We omit the subscript when clear from context. An ordering $\mathbf { V } ^ { \prec }$ on variables $\mathbf { V }$ is said to be consistent with $\mathcal { G }$ (i.e., a topological ordering) if for any $X$ , $Y \in \mathbf { V }$ , $X \prec Y$ implies $Y \not \in A \bar { n } ( X ) _ { \mathcal { G } }$ . Let $\bar { \mathbf { V } } ^ { \leq X } = \{ \dot { Y } | Y  \prec X$ or $Y = X \}$ .

Semi-Markovianity vs Non-Markovianity. A nonMarkovian causal DAG $\mathcal { G }$ can be constructed for a non-Markovian SCM by making the exogenous variables U explicit. A non-Markovian DAG with arbitrary hidden variables can be ‘projected’ onto a semi-Markovian causal DAG $\mathcal { G } ^ { \prime }$ which imposes exactly the same CI constraints over the observed variables (Tian and Pearl 2002b). In $\mathcal { G } ^ { \prime }$ , each unobserved variable is (i) a parent of at most two observed variables and (ii) made implicit by adding a dashed bidirected edge between its two children. The complexity of the latent structure is irrelevant to the CIs over observed variables. Therefore, we work with semi-Markovian graphs for model testing.

$d$ -separation. A node $W$ on a path $\pi$ is said to be a collider on $\pi$ if $W$ has converging arrows into $W$ in $\pi$ , e.g., $\right. W \left.$ o $\mathbf { r }  W  \pi$ is said to be blocked by a set $\mathbf { Z }$ if there exists a node $W$ on $\pi$ satisfying one of the following two conditions: 1) $W$ is a collider, and neither $W$ nor any of its descendants are in $\mathbf { Z }$ , or 2) $W$ is not a collider, and $W$ is in $\mathbf { Z }$ (Pearl 1988). Given disjoint sets $\mathbf { X } , \mathbf { Y }$ , and $\mathbf { Z }$ in $\mathcal { G }$ , $\mathbf { Z }$ is said to $d$ -separate $\mathbf { X }$ from $\mathbf { Y }$ in $\mathcal { G }$ if and only if $\mathbf { Z }$ blocks every path from a node in $\mathbf { X }$ to a node in $\mathbf { Y }$ according to the $d$ -separation criterion (Pearl 1988). If $\mathbf { Z }$ $d$ -separates $\mathbf { X }$ from $\mathbf { Y }$ in $\mathcal { G }$ (written $\mathbf { X } \perp _ { d } \mathbf { Y } \mid \mathbf { Z } )$ , then $\mathbf { X }$ is conditionally independent of $\mathbf { Y }$ given $\mathbf { Z }$ in any observational distribution consistent with $\mathcal { G }$ (Pearl 1988; Richardson 2003).

Definition 1. (C-component) (Tian and Pearl 2002a) A set of variables $\mathbf { C } \subseteq \mathbf { V }$ in a causal graph $\mathcal { G }$ is said to be a confounded component (c-component, for short) if there is a path of only bidirected edges connecting any $V _ { i } , V _ { j } \in { \bf C }$ , and $\mathbf { C }$ is maximal.

For a variable $X \in \mathbf { V }$ , $\mathcal { C } ( X ) _ { \mathcal { G } }$ denotes the c-component containing $X$ in $\mathcal { G }$ .

Previously, we have referred to the set of all CIs encoded in a DAG. We define this formally.

Definition 2. (Global Markov Property (GMP)) (Pearl 1988; Geiger, Verma, and Pearl 1989) A probability distribution $P ( \mathbf { v } )$ over a set of variables $\mathbf { V }$ is said to satisfy the global Markov property for a causal graph $\mathcal { G }$ if, for arbitrary disjoint sets $\mathbf { X } , \mathbf { Y } , \mathbf { Z } \subset \mathbf { V }$ with $\mathbf { X } , \mathbf { Y } \neq \varnothing$ ,

$$
{ \bf X } \perp _ { d } { \bf Y } | { \bf Z } \implies { \bf X } \perp { \bf Y } | { \bf Z } \mathrm { i n } P ( { \bf v } ) .
$$

Various local Markov properties have been developed which identify a subset of the CIs invoked by GMP that imply all others. A prominent example is the local Markov property for Markovian DAGs.

Definition 3 (The Local Markov Property (LMP) (Pearl 1988; Lauritzen et al. 1990; Lauritzen 1996)1). A probability distribution $P ( \mathbf { v } )$ over a set of variables $\mathbf { V }$ is said to satisfy the local Markov property for a given Markovian DAG $\mathcal { G }$ if, for any variable $X \in \mathbf { V }$ ,

$$
X \perp \perp N d ( \{ X \} ) \setminus P a ( \{ X \} ) \mid P a ( \{ X \} ) \setminus \{ X \} \mathrm { i n } P ( { \bf v } ) .
$$

Example 1. Consider Fig. 1b. $\{ C , D , H \}$ is a c-component, and $\mathcal { C } ( H ) _ { \mathcal { G } ^ { 2 } } = \{ C , D , H \}$ . Since $\{ B , C , D \}$ $d$ -separates $H$ from $\{ A , E , F \}$ in $\mathcal { G } ^ { 2 }$ , $\mathcal { G } ^ { 2 }$ implies the CI: $H$ $\vec { \left. \right|} \perp \left\{ \bar { A } , E , F \right\} $ $\{ B , C , D \}$ .

# 3 The C-component Local Markov Property

In this section, we motivate and introduce the c-component local Markov property for causal DAGs with unobserved confounders. In Sec. 3.1, we demonstrate the limitations of the traditional local Markov property (LMP) when applied to non-Markovian DAGs. In Sec. 3.2, to solve this problem, we present the c-component local Markov property (C-LMP) for semi-Markovian DAGs and establish its equivalence with GMP. In Sec. 3.3, we provide a useful property of C-LMP that makes its CIs amenable to listing.

# 3.1 A Naive Approach to Testing Non-Markovian Graphs

First, we show the limitations of the well-known LMP (Def. 3) in testing non-Markovian DAGs. For each variable $X$ in a given graph, LMP states that $X$ is independent of its non-descendants conditioning on its parents. Intuitively, the parents of $X$ form a minimal set separating $X$ from its non-descendants.

Example 2. Consider Fig. 1a. The DAG $\mathcal { G } ^ { 1 }$ contains only directed edges; assuming all variables are observed, $\mathcal { G } ^ { 1 }$ is Markovian. LMP invokes 11 CIs for $\mathcal { G } ^ { 1 }$ : $A$ 片 $\{ U _ { 1 } , U _ { 2 } , U _ { 3 } \}$ , $B ~ \bot \bot ~ \{ U _ { 1 } , U _ { 2 } , U _ { 3 } \} ~ \mid ~ \{ A \}$ , $C$ 上 $\{ A , E , \bar { U } _ { 1 } , U _ { 2 } \} \mid \{ \bar { B } , U _ { 3 } \} , D \perp \{ A , B , E , F , \bar { U } _ { 1 } , \bar { U _ { 3 } } \}$ $\{ C , U _ { 2 } \}$ , $E$ $\{ A , C , D , F , H , U _ { 1 } , U _ { 2 } , U _ { 3 } \}$ $\{ B \}$ , $F \quad \bot \quad \left\{ A , B , E , D , H , U _ { 1 } , U _ { 2 } , U _ { 3 } \right\} \qquad \vert$ $\{ C \}$ , $H$ 片 $\{ A , B , C , E , F , U _ { 2 } \} \qquad | \quad \{ D , U _ { 1 } , U _ { 3 } \}$ , $U _ { 1 }$ 止 $\{ A , B , C , D , E , F , U _ { 3 } \} \mid \{ U _ { 2 } \} , U _ { 2 }$ 1 $\{ A , B , C , E , F , U _ { 3 } \}$ , $U _ { 3 }$ F $\{ A , B , E , U _ { 1 } , U _ { 2 } \}$ . All 11 CIs are testable using samples from the distribution $P ( a , b , c , d , e , f , h , u _ { 1 } , u _ { 2 } , u _ { 3 } )$

LMP fails trivially for semi-Markovian DAGs since, for example, a variable may be connected to a non-descendant by a bidirected edge. One could think to instead apply LMP to the ‘unprojected’ non-Markovian DAG underlying the given semi-Markovian DAG. The non-Markovian DAG would contain no bidireced edges since the unobserved parents are made explicit. However, LMP does not extend to non-Markovian DAGs either, as we show in the following example.

Example 3. Continuing Ex. 2. Assume we are given the non-Markovian DAG $\mathcal { G } ^ { 1 }$ shown in Fig. 1a. If $U _ { 1 } , U _ { 2 }$ and $U _ { 3 }$ are unobserved, only samples from $\begin{array} { r l } { P ( \mathbf { v } ) } & { { } = } \end{array}$ $\begin{array} { r } { \int _ { u _ { 1 } , u _ { 2 } , u _ { 3 } } P ( a , b , c , d , e , f , h , u _ { 1 } , u _ { 2 } , u _ { 3 } ) } \end{array}$ $d u _ { 1 } d u _ { 2 } d u _ { 3 }$ are available, where $\mathbf { V } = \{ A , B , C , D , E , F , H \}$ denotes the observed variables. All 11 CIs invoked by LMP for $\mathcal { G } ^ { 1 }$ , listed in Ex. 2, require samples from $P ( a , b , c , d , e , f , h , u _ { 1 } , u _ { 2 } , u _ { 3 } )$ . Hence, none of these CIs can be tested using $P ( \mathbf { v } )$ .

One approach to try salvaging these 11 CIs is to consider only those CIs in which $\{ U _ { 1 } , U _ { 2 } , U _ { 3 } \}$ appear before the conditioning bar. In such CIs, $\{ U _ { 1 } , U _ { 2 } , U _ { 3 } \}$ can be removed using the decomposition axiom. However, only two of the 11 CIs can be modified in this way, i.e.,

$$
{ \begin{array} { r } { E \bot \bot \{ A , C , D , F , H \} \mid \{ B \} , } \\ { F \bot \{ A , B , E , D , H \} \mid \{ C \} . } \end{array} }
$$

These two CIs do not suffice to derive the GMP for $\mathcal { G } ^ { 1 }$ . To witness, consider a graph $\mathcal { G } ^ { \prime }$ over the same variables as $\mathcal { G } ^ { 1 }$ but with only one edge $H  A$ . Say we have an observational distribution $P ( \mathbf { v } )$ faithfully induced by $\mathcal { G } ^ { \prime }$ . Then, the CIs in

Eqs. (1,2) both hold in $P ( \mathbf { v } )$ . However, $\mathcal { G } ^ { 1 }$ implies that

$$
H \perp \perp \{ A , E , F \} \mid \{ B , C , D \}
$$

which does not hold in $P ( \mathbf { v } )$ since $\mathcal { G } ^ { \prime }$ contains an edge $H  A$ . Only testing the two CIs in Eqs. (1,2) would lead to the false conclusion that $P ( \mathbf { v } )$ is consistent with $\mathcal { G } ^ { 1 }$ . As a result, it is insufficient to use only those CIs which invoke $\{ U _ { 1 } , U _ { 2 } , U _ { 3 } \}$ outside the conditioning set.

As in the example, to test a non-Markovian DAG, one can not simply ‘filter out’ CIs that require conditioning on unobserved variables. This is because such CIs can entail testable CIs over the observed variables. The remaining option is to derive all these entailed CIs using the semi-graphoid axioms, and test those which invoke only observed variables. This is the GMP (Def. 2) of the non-Markovian DAG, which can invoke $\Theta ( 4 ^ { n } )$ CIs for a DAG with $n$ observed variables (Prop. C.3.1). This approach fails to exploit any locality in the graph, and requires a prohibitive number of CI tests, many of which are redundant. This suggests the need for alternative compatibility properties for semi-Markovian (equivalently, non-Markovian) DAGs. We next introduce our contribution, the c-component local Markov property.

# 3.2 C-LMP: A Local Markov Property for Semi-Markovian DAGs

In a semi-Markovian graph, the observed parents of a variable do not suffice to separate it from its non-descendants. Therefore, a surrogate of the parents is needed to restore locality. The construct of a c-component (Def. 1) was introduced for this purpose (Bareinboim et al. 2022), which we explain via an example.

Example 4. Continuing Ex. 2, assume $\{ U _ { 1 } , U _ { 2 } , U _ { 3 } \}$ are unobserved in $\mathcal { G } ^ { 1 }$ (Fig. 1a). The second graph $\mathcal { G } ^ { 2 }$ (Fig. 1b) is the semi-Markovian projection of $\mathcal { G } _ { 1 }$ . Note that the conditional independence $H \downarrow \downarrow \{ A , B , C , E , F \} \mid \{ D , U _ { 1 } , U _ { 3 } \}$ cannot be tested from the data since $\{ U _ { 1 } , U _ { 3 } \}$ are not observed. This means that a different conditioning set is needed to make $H$ independent of its observed non-descendants.

One might condition on the observed descendants of $\{ U _ { 1 } , U _ { 3 } \}$ that are closest to $U _ { 1 } , U _ { 3 }$ , i.e., $\{ C , D \}$ . These variables are not separable from $H$ without conditioning on $U _ { 1 } , U _ { 2 }$ or $U _ { 3 }$ , which is not an option. $\{ C , D \}$ have bidirected edges to $H$ in $\mathcal { G } ^ { 2 }$ , the semi-Markovian projection of $\mathcal { G } ^ { 1 }$ . $\{ C , \bar { D } \}$ are now active on any paths on which $\{ C , D \}$ they are colliders: for instance, on the paths $E \left. B \right.$ $C \left. U _ { 3 } \right. H$ and $A  B  C  U _ { 3 }  H .$ . To block some of these paths, we also condition on the (remaining) observed parents of $\{ C , D \}$ , i.e., $\{ B \}$ . Firstly, conditioning on $\{ C , D \}$ already makes $B$ and its ancestors active on any paths where they are colliders; secondly, $B$ is connected to $H$ when conditioning on $\{ C , D \}$ . Therefore, conditioning on $\{ B \}$ does not introduce any new active paths to $X$ . Conditioning on $\{ B \}$ additionally blocks paths to $H$ containing $B$ on which $B$ is not a collider. Therefore, we have the conditioning set $P a ( \mathbf { C } ) \backslash \{ H \} = \{ B , C , D \}$ . The $\mathrm { C I }$ over observables $H$ $I \perp \perp \{ A , E , F \} \mid \{ B , C , D \}$ is thus derived. □

Ex. 4 is relatively simple since the c-component of $H$ is used to generate the given CI. However, the c-components of a variable do not always give rise to CIs.

Example 5. Consider, as an example DAG, a bidirected path of the form $V _ { 1 }  V _ { 2 } \cdots  V _ { n }$ on variables $\mathbf { V }$ . For each $V _ { i }$ , the c-component including $V _ { i }$ is the entire graph. Therefore, conditioning on the c-component results in the ‘vacuous’ CI: $V _ { i } \perp \perp \emptyset \mid \mathbf { V } \setminus V _ { i }$ . Clearly, from this set of vacuous CIs, we cannot derive non-vacuous CIs encoded the graph, such as those of the form $V _ { i } \perp \perp \{ V _ { j } \} , \forall i , j$ s.t. $| i - j | > 1$ (e.g., $V _ { 1 } \perp \perp \{ V _ { 3 } \}$ ). □

A useful insight due to (Richardson 2003) is that subsets of a variable’s c-component can give rise to distinct ‘surrogates’ for its parents and hence distinct CIs. This is because conditioning on a certain variable in a c-component closes some paths while opening others. We generalize c-components to ancestral c-components to define these ‘surrogates.’2

Definition 4. (Ancestral C-component (AC)) Given a causal graph $\mathcal { G }$ and a consistent ordering $\mathbf { V } ^ { \prec }$ , let $X$ be a variable in $\mathbf { V } ^ { \prec }$ . A set of variables $\mathbf { C }$ is said to be an ancestral ccomponent relative to $X$ if there exists an ancestral set $\mathbf { S } \subseteq$ $\mathbf { V } { \leq } \dot { X }$ containing $X$ such that ${ \mathcal { C } } ( X ) _ { { \mathcal { G } } _ { \mathbf { S } } } = \mathbf { C }$ . The collection of all such $\mathbf { C }$ is denoted:

$\mathcal { A } \mathcal { C } _ { X } = \left\{ \mathbf { C } \ | \ \mathbf { C } \right.$ is an ancestral c-component relative to $X \}$ .

Unlike c-components, there may be many ancestral ccomponents with respect to a given variable.

Example 6. Consider the graph $\mathcal { G }$ in Fig. 2 with ordering $A \prec B \prec \cdots \prec X \prec J \prec K$ . For the variable $X$ , $\{ X \}$ is an AC relative to $X$ induced by the ancestral set $\mathbf { S } = \{ X \}$ ; $\{ B , X \}$ is an AC relative to $X$ induced by the ancestral set $\dot { \bf S } = \{ \boldsymbol { B } , { \cal C } , { \cal D } , { \cal E } , { \cal X } \}$ . $\{ X , A , D , E \}$ is not an AC relative to $X$ since the exclusion of $B$ and/or $H$ disconnects the variables in question. For the variable $J$ , $\{ J \}$ is not an AC relative to $J$ since it excludes the ancestor $X$ to which $J$ is connected by a bidirected edge; $\{ X , J \}$ is an AC induced by the ancestral set $\{ X , J \}$ . □

We use ACs to define the c-component local Markov property, which generalizes LMP to semi-Markovian DAGs using this new notion of local independence.

Definition 5. (The C-component Local Markov Property (CLMP)) A probability distribution $P ( \mathbf { v } )$ over a set of variables $\mathbf { V }$ is said to satisfy the c-component local Markov property for a causal graph $\mathcal { G }$ with respect to the consistent ordering $\mathbf { V } ^ { \prec }$ , if, for any variable $X \in \mathbf { V } ^ { \prec }$ and ancestral c-component $\mathbf { C } \in \mathcal { A C } _ { X }$ relative to $X$ ,

$$
\begin{array} { r } { X \perp \perp \mathbf { S } ^ { + } \setminus P a ( \mathbf { C } ) \mid ( P a ( \mathbf { C } ) \setminus \{ X \} ) \mathrm { i n } P ( \mathbf { v } } \\ { \mathbf { S } ^ { + } = \mathbf { V } ^ { \le X } \setminus D e ( S p ( \mathbf { C } ) \setminus P a ( \mathbf { C } ) ) . \qquad } \end{array}
$$

Example 7. Continuing Ex. 6. We give a few examples of CIs invoked by C-LMP for the variable $X$ .

![](images/05ea3799f60473057842559810616affcbf8671d95138f00bde1d5c5001ed1eb.jpg)

![](images/9e7e57eb0bd72f5626af783b1042c31a84781c40217f097cddfec9f939fe2431.jpg)  
(b) $X$ is separated from $A$ but not $C$ when not conditioning on $B$ .

(a) $X$ is separated from $C$ but not $A$ when conditioning on $B$ .

![](images/6a050b2a976ef494b1d57086bebf4388ac165084844d96d35bba771b5a6e3bb9.jpg)  
(c) $X$ is separated from $F , I$ but not $D$ when conditioning on $\{ H , E \}$ .

Figure 2: Three ACs relative to the variable $X$ in the (same) causal DAG $\mathcal { G }$ . Assume an ordering $A \prec B \prec \cdot \cdot \cdot \prec X \prec J \prec K$ The ACs relative to $X$ (excluding $\{ X \}$ itself), shown in blue, separate it from the variables shown in green.

1. The AC $\mathbf { C } = \{ X , B \}$ gives the CI $X$ 上 $\{ C , D , E , F \} \mid$ $\{ B \}$ (Fig. 2a), since

$$
P a ( \mathbf { C } ) = P a ( \{ X , B \} ) = \{ X , B \}
$$

$$
\begin{array} { r l } & { \mathbf { S } ^ { + } = \mathbf { V } ^ { \leq X } \setminus D e ( S p ( \{ X , B \} \setminus P a ( \{ X , B \} ) ) ) } \\ & { \qquad = \{ A , B , C , D , E , F , H , I , X \} \setminus D e ( \{ A , H \} ) } \\ & { \qquad = \{ A , B , C , D , E , F , H , I , X \} \setminus \{ A , H , I \} } \\ & { \qquad = \{ B , C , D , E , F , X \} } \end{array}
$$

2. The AC $\mathbf { C } = \{ X , H \}$ gives the C $[ X \bot \bot$ $\{ A , D , I \} \mid \{ H \}$ (Fig. 2b), since

$$
P a ( \mathbf { C } ) = P a ( \{ X , H \} ) = \{ X , H \}
$$

$$
\begin{array} { r l } & { \mathbf { S } ^ { + } = \mathbf { V } ^ { \leq X } \setminus D e ( S p ( \{ X , H \} \setminus P a ( \{ X , H \} ) ) ) } \\ & { \qquad = \{ A , B , C , D , E , F , H , I , X \} \setminus D e ( \{ B , E \} ) } \\ & { \qquad = \{ A , B , C , D , E , F , H , I , X \} \setminus \{ B , C , E , F \} } \\ & { \qquad = \{ A , D , H , I , X \} . } \end{array}
$$

3. The AC $\mathbf { C } = \{ X , H , E \}$ gives the CI $X \perp \perp \{ A , F , I \} \mid$ $\{ H , E \}$ (Fig. 2c), since

$$
P a ( \mathbf { C } ) = P a ( \{ X , H , E \} ) = \{ X , H , E \}
$$

$$
\begin{array} { r l } & { \mathbf { S } ^ { + } = \mathbf { V } ^ { \leq X } \setminus D e ( S p ( \{ X , H , E \} \setminus P a ( \{ X , H , E \} ) ) ) } \\ & { \qquad = \{ A , B , C , D , E , F , H , I , X \} \setminus D e ( \{ B , D \} ) } \\ & { \qquad = \{ A , B , C , D , E , F , H , I , X \} \setminus \{ B , C , D , E \} } \\ & { \qquad = \{ A , E , F , H , I , X \} } \end{array}
$$

As a sanity check, let us examine the CIs C-LMP implies for a Markovian DAG $\mathcal { G }$ , where all $\mathrm { ~ c ~ }$ -components are singletons. There is exactly one AC $\mathbf { C } = \{ X \}$ relative to a given variable $X$ . Moreover, $P a ( \mathbf { C } ) \mathop { = } \tilde { P a } \bigl ( \{ X \} \bigr ) , S p ( \{ X \} ) = \emptyset$ and $\mathbf { S } ^ { + } = \mathbf { V } ^ { \le X } \setminus D e ( \varnothing ) = \mathbf { \dot { V } } ^ { \le X }$ . Therefore, the CI invoked by C-LMP for $X$ is

$$
X \perp \perp \mathbf { V } ^ { \le X } \setminus \left. { P a ( \{ X \} ) } \right| P a ( \{ X \} ) \setminus \{ X \}
$$

Thus, C-LMP reduces to the local well-numbering Markov property (Lauritzen et al. 1990) for a Markovian DAG $\mathcal { G }$ .3

In semi-Markovian DAGs, c-components are not necessarily singletons. Comparing the CIs invoked by LMP and C-LMP for a given variable $X$ , we see that C-LMP generalizes two concepts:

1. The conditioning set $P a ( \{ X \} ) \setminus \{ X \}$ stated in LMP is replaced with $P a ( \mathbf { C } ) \setminus \{ \dot { X } \}$ in C-LMP, using an AC C relative to $X$ .   
2. The conditioning set $P a ( \mathbf { C } ) \backslash \{ X \}$ renders $X$ independent of $\mathbf { S } ^ { + } \backslash P a ( \mathbf { C } )$ where $\mathbf { S } ^ { + } = \dot { \mathbf { V } } ^ { \le X } \backslash D e ( S p ( \mathbf { C } ) \backslash \bar { P } a ( \mathbf { C } ) )$ , as stated by C-LMP. The set $\mathbf { S } ^ { + } \setminus \left( \left( \mathbf { C } \right) \right.$ replaces the set $N d ( \{ X \} ) \setminus P a ( \{ X \} )$ in LMP.

In Ex. 4, we gave intuition for the generalised conditioning set $P a ( \mathbf { C } ) \backslash \{ X \}$ (Case 1). Next, we explain the construction of ${ \bf S } ^ { + }$ (Case 2) used to compute the maximal set of variables in $\mathbf { V } ^ { \leq X }$ that are independent of $X$ given $P a ( \mathbf { C } ) \setminus \{ X \}$ . Consider what happens to a variable $Y \in { \bf V } ^ { \le X } \setminus P \bar { a } ( \bar { \bf C } )$ when conditioning on $P a ( \mathbf { C } ) \setminus \{ X \}$ .

• If $Y$ is a descendant (or an ancestor) of some node $W \in$ $P a ( \mathbf { C } )$ , we have a directed path $\pi$ from $W$ to $Y$ (or viceversa). Conditioning on $\mathinner { P \bar { a } \mathopen { \left( \mathbf { C } \right) } } \setminus \{ X \}$ blocks $\pi$ (since $Y \not \in P a ( \mathbf { C } ) )$ , and hence any path from $X$ to $Y$ which contains $\pi$ as a sub-path. For example, in Fig. 2a, taking $\mathbf { C } = \{ X , B \}$ , $W = B$ and $Y = C$ , conditioning on $\{ B \}$ blocks the path $X \left. B \right. C$ .   
• If $Y$ is connected by a bidirected path to some node in $\mathbf { C }$ , but $Y$ is not in $S p ( \mathbf { C } )$ , then some node $V \in S p ( \mathbf { C } ) \ \backslash$ $P a ( \mathbf { C } )$ ‘intercepts’ this path, i.e., $V$ is a closed collider and thus blocks the path from $X$ to $Y$ . For example, in Fig. 2a, taking $\mathbf { C } = \{ X , B \}$ , $V = H$ and $Y = E$ , $H$ blocks the path $X  H  E$ .   
• If $Y$ is in $S p ( \mathbf { C } ) \setminus P a ( \mathbf { C } )$ , is an active bidirected path from $X$ to $Y$ when conditioning on $\mathbf { C } \backslash \{ X \}$ . For example, in Fig. 2a, taking $\mathbf { C } = \{ X , B \}$ and $Y = A$ , conditioning on $\{ B \}$ opens the path $X  B  A$ .

Analogous to how, for a given variable $X$ , different conditioning sets give rise to different CIs from $X$ , different ACs also give rise to different CIs from $X$ . The upshot of defining ACs is that they carve out a relatively small set of CIs (commonly known as a ‘basis’ (Bareinboim et al. 2022)) from which all CIs encoded in the given graph can be derived.

The main result of this section, given below, establishes that GMP and C-LMP are equivalent.

Theorem 1 (Equivalence of C-LMP and GMP). Let be a causal graph and $\mathbf { V } ^ { \prec }$ a consistent ordering. A probability distribution over V satisfies the global Markov property for $\mathcal { G }$ if and only if it satisfies the $c$ -component local Markov property for $\mathcal { G }$ with respect to $\mathbf { V } ^ { \prec }$ .

As a corollary of Thm. 1, we can conclude that C-LMP is equivalent to Richardson’s ordered local Markov property (Richardson 2003), since the latter is equivalent to GMP (Richardson 2003, Thm. 2, Section 3.1).

Corollary 1 (Equivalence of C-LMP and the Ordered Local Markov Property). Let $\mathcal { G }$ be a causal graph and $\mathbf { V } ^ { \prec }$ a consistent ordering. A probability distribution over $\mathbf { V }$ satisfies the ordered local Markov property (Richardson 2003) for $\mathcal { G }$ with respect to $\mathbf { V } ^ { \prec }$ if and only if it satisfies the $c$ -component local Markov property for $\mathcal { G }$ with respect to $\mathbf { V } ^ { \prec }$ .

In Appendix B, we further develop the connection between C-LMP and the ordered local Markov property. In fact, in Thm. B.2.1, we show that these two properties induce the exact same set of CIs for a given DAG and a consistent ordering. Thm. B.2.1 thus provides another way to obtain Thm. 1 as a corollary.

The equivalence of C-LMP and GMP means that the CIs invoked by C-LMP for a given causal DAG can be used to test the DAG against observational data.

# 3.3 Uniqueness Property of C-LMP

By definition, each CI invoked by C-LMP is generated from an AC. We further show that each CI can be generated from exactly one AC.

Theorem 2 (Unique AC for each CI Invoked by C-LMP). Let $\mathcal { G }$ be a causal graph, $\mathbf { V } ^ { \prec }$ a consistent ordering, and $X$ a variable in $\mathbf { V } ^ { \prec }$ . For every conditional independence relation invoked by the $c$ -component local Markov property of the form $X \bot \bot \mathbf { W } \mid \mathbf { Z } ,$ , there is exactly one ancestral $c$ -component $\mathbf { C } \in \mathcal { A C } _ { X }$ such that ${ \bf W } = { \bf V } ^ { \le X ^ { ' } } \backslash ( ( D e ( S p ( { \bf C } ) \backslash P a ( { \bf \dot { C } } ) ) ) \cup$ $P a ( \mathbf { C } ) ,$ ) and $\mathbf { Z } = P a ( \mathbf { C } ) \setminus \{ X \}$ .

The one-to-one correspondence between ACs and CIs invoked by C-LMP allows us to give bounds on the latter number that are tight in the exponent.

Proposition 1 (Number of CIs Invoked by C-LMP). Given a causal graph $\mathcal { G }$ and a consistent ordering $\mathbf { V } ^ { \prec }$ , let $n$ and $s \leq n$ denote the number of variables and the size of the largest c-component in $\mathcal { G }$ respectively. Then, the c-component local Markov property for $\mathcal { G }$ with respect to $\mathbf { V } ^ { \prec }$ invokes $O ( n 2 ^ { s } )$ conditional independencies implied by $\mathcal { G }$ over $\mathbf { V }$ . Moreover, there exists a graph $\mathcal { G }$ and a consistent ordering $\mathbf { V } ^ { \prec }$ for which the property induces $\Omega ( 2 ^ { n } )$ conditional independencies.

This result shows that C-LMP offers an exponential improvement on the $\Theta ( 4 ^ { n } )$ CIs invoked by GMP. However, C-LMP can still invoke an exponential number of CIs. For example, in $\mathcal G ^ { e x }$ (Fig. 3a) with $2 n$ nodes, there are $2 ^ { n } + ( n - 3 )$ CIs invoked by C-LMP.

The main upshot of the one-to-one correspondence between ACs and CIs invoked by C-LMP is that to list such

![](images/3b08f8a40a550e3cb64706f59b79d0836c60f628c29aba783d2c9a555d2e15f2.jpg)  
Figure 3: (a) An example showing that C-LMP may invoke an exponential number of CIs. (b) A causal graph used to show the execution of LISTCI in Ex. 9.

CIs, it suffices to enumerate ACs. We study the problem of listing CIs in the next section.

# 4 Listing CIs

Our goal in this section is to develop an algorithm that lists CIs invoked by C-LMP. In the worst case, there may exist exponentially many such CIs, requiring exponential time to list them all. In such cases, we look for algorithms that run in polynomial delay (Johnson, Yannakakis, and Papadimitriou 1988). Poly-delay algorithms output the first solution (or indicate none is available) in poly-time, and take poly-time to output each consecutive solution.

However, not all CIs invoked by C-LMP are useful for model testing. C-LMP invokes some ‘vacuous’ CIs of the form $X \perp \perp \varnothing \mid \mathbf { Z }$ , which do not need testing. Therefore, we constrain the problem by requiring that we list only nonvacuous CIs, as defined below.

Definition 6 (Vacuous CI and Admissible AC (AAC)). Given a conditional independence relation invoked by C-LMP of the form $X \perp \mathbf { W } \mid P a ( \mathbf { C } ) \setminus \{ X \}$ , where $\mathbf { W } = \mathbf { \bar { S } } ^ { + } \setminus P a ( \mathbf { C } )$ (by Def. 5), if $\mathbf { W } \neq \varnothing$ , the conditional independence relation is said to be non-vacuous and $\mathbf { C }$ is said to be an admissible ancestral c-component relative to $X$ .

Example 8. Consider the causal graph $\mathcal { G } ^ { 3 }$ (Fig. 3b). The AC $\{ J \}$ relative to $J$ is admissible. Given $\mathbf { S } ^ { + } = \mathbf { V } \backslash \{ F , H \}$ , we have $\mathbf { W } = \mathbf { S } ^ { + } \setminus \{ J \} = \{ A , B , C , D , E \}$ . However, the AC $\{ F , J \}$ relative to $J$ is not admissible. Since ${ \bf S } ^ { + } = \{ { \cal F } , { \cal J } \}$ , $\mathbf { \dot { W } } = \mathbf { \dot { S } } ^ { + } \setminus \{ F , J \} = \emptyset$ . L

Listing only non-vacuous CIs is important since C-LMP may invoke exponentially many vacuous CIs. To witness, consider a bidirected clique on $n$ nodes such that no two variables are independent of each other given any conditioning set. Every set ${ \bar { \mathbf { Z } } } \subseteq \mathbf { V } \setminus \{ X \}$ forms an AC, resulting in $\Omega ( 2 ^ { n } )$ vacuous CIs (see Ex. D.3.1 in Appendix D.3 for details).

Our bounds on the number of CIs invoked by C-LMP are also tight for the number of non-vacuous CIs (Prop. 1). We develop the algorithm LISTCI (Alg. 1) to list all non-vacuous CIs invoked by C-LMP in poly-delay.

Example 9. Consider the causal graph $\mathcal { G } ^ { 3 }$ (Fig. 3b) with $\mathbf { V } ^ { \prec } = \{ A , B , C , D , E , F , H , J \}$ . LISTCI $( \mathcal { G } ^ { 3 } , \mathbf { V } ^ { \prec } )$ lists 11 non-vacuous CIs invoked by C-LMP: $C$ $\perp \perp \left\{ A \right\} \mid \left\{ B \right\}$ , $D$ 上 $\{ A \} \ | \ \{ B , C \} , E \perp \{ A , B , C \} \ | \ \{ D \} , I$ $F$ 1 $\{ B \} \ | \ \{ A  \}$ , $F$ 1 $\{ E \} \mid \{ A , B , C , { \dot { D } } \}$ , $H$ 1 $\{ A , B , C , D , E \} \mid \{ F \} ,$ $J$ 1 $\{ A , B , C , D , E \} , J \perp \{ B \} \mid \{ A , F \} , J \perp \{ B \} \mid \{ A , F , H \} .$ , $J \perp \{ E \} \mid \{ A , B , C , D , F \}$ , $J \perp \{ E \} \mid \{ A , B , C , D , F , H \}$ . After, LISTCI terminates as there are no more non-vacuous CIs. □

Algorithm 1: LISTCI $( \mathscr { G } , \mathbf { V } ^ { \prec } )$   

<html><body><table><tr><td>with g.</td><td>1: Input: G a causal diagram; V  an ordering consistent</td></tr><tr><td></td><td>2:Output: Listing non-vacuous CIs invoked by C-LMP for 9 with respect to V<.</td></tr><tr><td></td><td>3:for each X ∈Vdo</td></tr><tr><td>4:</td><td>I ←C(X)gAn(x),R←C(X)gv≤x</td></tr><tr><td>5:</td><td>LISTCIX(9v≤x,X,V≤X,I,R)</td></tr></table></body></html>

# 4.1 Listing CIs for a Given Variable

The algorithm LISTCI iterates over each variable $X \in \mathbf { V } ^ { \prec }$ and lists all non-vacuous CIs invoked by C-LMP for $X$ . By Defs. 5, 6 and Thm. 2, listing non-vacuous CIs reduces to enumerating AACs. In this section, we show how to enumerate AACs relative to a given variable $X \in \mathbf { V } ^ { \prec }$ using the procedure LISTCIX (Alg. 2).

LISTCIX adopts a divide-and-conquer strategy similar to the algorithm of (Takata 2010). LISTCIX implicitly constructs a binary search tree for $X$ using a depth-first approach. Tree nodes of the form $\mathcal { N } ( \mathbf { I } ^ { \prime } , \mathbf { R } ^ { \prime } )$ represents the collection of all AACs $\mathbf { C }$ with $\mathbf { I } ^ { \prime } \subseteq \mathbf { C } \subseteq \mathbf { R } ^ { \prime }$ . The top-level call of LISTCIX, at the root node $\mathcal { N } ( \mathbf { I } , \mathbf { R } )$ , represents all AACs $\mathbf { C }$ relative to $X$ . This is due to the construction on line 4 of LISTCI so that I is contained in and $\mathbf { R }$ contains all possible AACs relative to $X$ . Thus, the top-level call can generate all CIs for $X$ .

Subsequent recursive calls expand this tree by shrinking the range $\mathbf { I } ^ { \prime } \subseteq \mathbf { R } ^ { \prime }$ one variable at a time. One requirement of the poly-delay property is that each AAC should appear exactly once in the enumeration. To expand the tree from $\mathcal { N } ( \mathbf { I } ^ { \prime } , \mathbf { R } ^ { \prime } )$ , LISTCIX constructs two ‘disjoint’ children (lines 10-11: a chosen variable $S \in \mathbf { V } ^ { \prec }$ cannot be in any AAC from the left child but must be in every AAC from the right child.

As another requirement of the poly-delay property, we expand the tree from a node $\mathcal { N } ( \mathbf { I } ^ { \prime } , \bar { \mathbf { R } } ^ { \prime } )$ if and only if the expansion is guaranteed to produce a non-vacuous CI. Equivalently, there must exist at least one AAC $\mathbf { C }$ such that $\mathbf { I } ^ { \prime } \subseteq \mathbf { C } \subseteq \mathbf { R } ^ { \prime }$ . If there is no such C, we prune the tree and back-track to the previous tree node. To perform this check for the existence of an AAC in poly-time, LISTCIX calls the function FINDAAC (Alg. 3). We explain FINDAAC in the next subsection.

Finally, a leaf node is reached when $\mathbf { I } = \mathbf { R } $ . LISTCIX outputs a non-vacuous CI generated from the AAC $\mathbf { C } = \mathbf { I }$ using Def. 5.

Example 10. Expanding Ex. 9 to demonstrate the construction of the search tree $\tau ^ { 3 }$ (Fig. 4) generated by running LISTC $( \mathcal { G } ^ { 3 } , \mathbf { V } ^ { \prec } )$ for $X ~ = ~ J$ . With $\textbf { I } = ~ \{ J \}$ and $\mathbf { R } = \{ A , C , D , F , H , J \}$ constructed on line 4, the initial search starts from the root node $\mathcal { N } ( \mathbf { I } , \mathbf { R } )$ on line 5. On line 3 of LISTCIX, FINDAAC returns $\{ J \}$ . With $S \ = \ F$ and ${ \bf R } ^ { \prime } = \{ J \}$ , the recursive call LISTCIX $( \mathcal { G } ^ { 3 } , J , \mathbf { V } ^ { \prec } , \mathbf { I } , \mathbf { R } ^ { \prime } )$ is made at line 10, spawning a child $\mathcal { N } _ { 1 } ( \{ J \} , \{ J \} )$ . The search continues from $\mathcal { N } _ { 1 }$ . FINDAAC returns $\{ J \}$ . $\mathcal { N } _ { 1 }$ is a leaf node, and LISTCIX outputs a CI: $J \perp \perp \left\{ A , B , C , D , E \right\}$ on line 6. The rest of the search tree for $J$ is shown in $\mathcal { T } ^ { 3 }$ . The full set of search trees is shown in Fig. D.3.1 in Appendix D.3. □

![](images/77d593c88ae1ebc241225e2d677cbc9d79a7ad333fea21b99c7d57a43572ff53.jpg)  
Figure 4: $\tau ^ { 3 }$ a search tree illustrating the running of LISTCI in Ex. 9 for $X = J$ .

# 4.2 Finding an AAC

In this section, we address the following subproblem, needed for LISTCIX to run in poly-delay: given a variable $X \in \mathbf { V } ^ { \prec }$ , and two ACs I, R relative to $X$ , how do we find an AAC C such that $\mathbf { I } \subseteq \mathbf { C } \subseteq \mathbf { R }$ (or indicate that there is none) in poly-time?

The poly-time constraint on solving this subproblem rules out the brute-force approach: namely, iterating over all subsets $\mathbf { C }$ such that $\mathbf { I } \subseteq \mathbf { C } \subseteq \mathbf { R }$ until we find some $\mathbf { C }$ that is an AAC (or conclude that there is none). We find that this exponential search is not necessary. The key idea behind our solution is to reduce this search to a verification of whether a particular AAC is admissible.

To elaborate, assume there exists an AAC $\mathbf { C }$ such that ${ \mathbf { I } } \subseteq$ $\mathbf { C } \subseteq \mathbf { R }$ . It is possible that $\mathbf { C } = \mathbf { I }$ , in which case we are done. Otherwise, consider what it means for I not to be admissible. When conditioning on $P a ( \mathbf { I } ) \setminus \{ X \}$ , every variable in $\mathbf { V } { \leq } X$ has an active path to $X$ . This means that every such variable (outside the conditioning set) is a descendant of $S p ( \mathbf { I } ) \backslash P a ( \mathbf { I } )$ (Def. 5). Since $\mathbf { C }$ is admissible, then there must be some descendant of $S p ( \mathbf { I } ) \setminus P a ( \mathbf { I } )$ , say $D$ , which has an active path to $X$ when conditioning on $P a ( \mathbf { I } ) \setminus \{ X \}$ but not when conditioning on $P a ( \mathbf { C } ) \backslash \{ X \}$ . We show that the AAC $\mathbf { C }$ can exist if and only if there exists any set $\mathbf { Z }$ separating $X$ from some such $D$ , with the restriction that $P a ( \mathbf { I } ) \setminus \{ \mathbf { \bar {  { X } } } , D \} \subseteq$ ${ \mathbf Z } \subseteq P a ( { \mathbf R } ) \backslash \{ X , D \}$ . $\mathbf { Z }$ need not be an AAC. We can check if such $\mathbf { z }$ exists (line 6) in poly-time using the function FINDSEPARATOR (Fig. C.2.2 in Appendix C.2).

Algorithm 2: LISTCIX $( \mathcal G _ { \mathbf V \leq x } , X , \mathbf V \leq x ^ { } , \mathbf I , \mathbf R )$   
Algorithm 3: FINDAAC $( \mathcal G _ { \mathbf V \leq x } , X , \mathbf V ^ { \leq X } , \mathbf I , \mathbf R )$   

<html><body><table><tr><td>1: Input: Sv≤x a causal diagram; X a variable; V≤X an ordering consistent with G;Iand R ACs relative to X. 2:Output:Listing non-vacuous CIs invoked by C-LMP associated with X and AACs C under the constraint</td></tr><tr><td>InCCR. 3:ifFINDAAC(Sv≤x,X,V≤X,I,R) ≠⊥ then</td></tr><tr><td>4: ifI=Rthen</td></tr><tr><td>5: S+ ←V≤X\De(Sp(I)\Pa(I)) 6: Output X ↓ S+\Pa(I)|Pa(I)\{X}</td></tr><tr><td>7: return</td></tr><tr><td>8: T ←R∩(Sp(I)\I),S ← Any node in T 9: ' ←C(X)gAn(IU{s),R' ←C(X)9R\De(s))</td></tr><tr><td>10: LISTCIX(9v≤x,X,V≤X,I,R') 11: LISTCIX(Sv≤x,X,V≤X,I',R)</td></tr></table></body></html>

Example 11. Expanding Ex. 10 to illustrate the usage of FINDAAC. Let $X ~ = ~ J$ , $\mathbf { V } ^ { \prec } \ = \ \mathbf { V } ^ { \leq J }$ , $\textbf { I } = \{ J \}$ , and $\mathbf { R } = \{ A , C , D , F , H , J \}$ . $\vec { \mathrm { 4 } } \mathrm { I N D A A C } ( \mathcal { G } ^ { 3 } , J , { \bf V } ^ { \le J } , { \bf I } , { \bf R } )$ returns ${ \bf C } = \{ { J } \}$ since there exists an AAC $\mathbf { C }$ relative to $J$ with $\mathbf { I } \subseteq \mathbf { C } \subseteq \mathbf { R }$ . With $\boldsymbol { \textbf { I } } = \{ \boldsymbol { F } , \boldsymbol { J } \}$ and $\mathbf { R } = \{ F , H , J \}$ , $\exists \mathbf { I N D A A C } ( \mathcal { G } ^ { 3 } , J , \mathbf { V } ^ { \leq J } , \mathbf { I } , \mathbf { R } )$ returns $\perp$ since none of the ACs $\mathbf { C }$ relative to $J$ with $\mathbf { I } \subseteq \mathbf { C } \subseteq \mathbf { R }$ are admissible. □

Lemma 1 (Correctness of FINDAAC). Given a causal graph $\mathcal { G }$ , a consistent ordering $\mathbf { V } ^ { \prec }$ , and a variable $X \in \mathbf { V } ^ { \prec }$ , let I, R be ancestral $c$ -components relative to $X$ such that ${ \mathbf { I } } \subseteq$ R. FINDAAC $( \mathcal G _ { \mathbf { V } \leq x } , \dot { X } , \mathbf { V } ^ { \leq X } , \mathbf { I } , \mathbf { R } )$ outputs an admissible ancestral $c$ -component $\mathbf { C }$ relative to $X$ such that $\mathbf { I } \subseteq \mathbf { C } \subseteq \mathbf { R }$ if such a $\mathbf { C }$ exists, and $\perp$ otherwise.

Lemma 2 (Correctness of LISTCIX). LISTCIX $( \mathcal G _ { \mathbf { V } \leq x } , X , \mathbf { V } ^ { \leq X } , \mathbf { I } , \mathbf { R } )$ enumerates all and only all non-vacuous conditional independence relations invoked by the $c$ -component local Markov property associated with $X$ and admissible ancestral $c$ -components $\mathbf { C }$ relative to $X$ where $\mathbf { I } \subseteq \mathbf { C } \subseteq \mathbf { R }$ . Further, LISTCIX runs in $O ( n ^ { 2 } ( n + m ) )$ delay where n and m represent the number of nodes and edges in $\mathcal { G }$ , respectively.

Our results are summarized in the following theorem, which provides the soundness, completeness, and poly-delay complexity of the proposed algorithm.

Theorem 3 (Correctness of LISTCI). Let $\mathcal { G }$ be a causal graph and $\mathbf { V } ^ { \prec }$ a consistent ordering. LISTC $( \mathscr { G } , \mathbf { V } ^ { \prec } )$ enumerates all and only all non-vacuous conditional independence relations invoked by the $c$ -component local Markov property in $O ( n ^ { 2 } ( n + m ) )$ delay where n and m represent the number of nodes and edges in $\mathcal { G }$ , respectively.

# 5 Experiments

In this section, we first demonstrate the runtime of LISTCI on benchmark DAGs of up to 100 nodes from the bnlearn repository (Scutari 2010). Next, we apply LISTCI to model testing on a real-world protein signaling dataset with an expert-provided graph (Sachs et al. 2005). Third, we provide analysis of the total number of non-vacuous CIs invoked

1: Input: $\mathcal G _ { \mathbf { V } \leq X }$ a causal diagram; $X$ a variable; $\mathbf { V } { \leq } X$ an   
ordering consistent with $\mathcal { G }$ ; I and $\mathbf { R }$ ACs relative to $X$ .   
2: Output: An AAC $\mathbf { C }$ relative to $X$ under the constraint   
$\mathbf { I } \subseteq \mathbf { C } \subseteq \mathbf { R }$ , if such $\mathbf { C }$ exists; $\perp$ otherwise.   
3: if ISADMISSIBLE $\mathcal { G } _ { \mathbf { V } \leq x , X , \mathbf { V } \leq X , \mathbf { I } ) }$ then   
4: return I   
5: for each $D \in D e ( S p ( \mathbf { I } ) \backslash P a ( \mathbf { I } ) )$ do   
6: $\begin{array} { r } { \mathbf { Z }  \mathrm { F I N D S E P A R A T O R } ( \mathcal G _ { \mathbf { V } \leq x } , \{ X \} , \{ D \} , } \\ { P a ( \mathbf { I } ) , P a ( \mathbf { R } ) ) \quad \quad } \end{array}$   
7: if $\mathbf { Z } \neq \perp$ then   
8: return C(X) An(I Z)   
9: return

by C-LMP, using LISTCI for the analysis. The details of the three experiments are shown in Appendix F.

Experiment 1 (Comparison of LISTCI with other algorithms). We compare the runtime of LISTCI with two baselines: LISTGMP (Fig. E.0.1 in Appendix E) and LISTCIBF (Alg. B.1.1 in Appendix B.1)4. LISTGMP lists all CIs invoked by GMP (Def. 2); LISTCIBF iterates over ancestral sets to list CIs invoked by the ordered local Markov property (Richardson 2003). The algorithms were run on DAGs that describe real-world scenarios from the bnlearn repository. Since the graphs are Markovian, non-Markovian graphs were generated by randomly assigning $U \%$ of nodes to be unobserved for $U \in \{ 0 , 1 0 , 2 0 , \ldots , 9 0 \}$ . For each $U$ , we generated 10 random samples. For a given graph, algorithm, and $U$ , if any one sample timed out $\mathit { \Theta } > 1$ hour), no further samples are tested. Fig. 5 shows the average runtime of the algorithms, with further details in Fig. F.1.1.

The results corroborate our theoretical conclusion that LISTCI outperforms the other algorithms. For LISTGMP, the algorithm did not timeout on graphs with $n < 1 0$ nodes. For LISTCIBF, we have mixed results. The algorithm did not time out for some graphs with up to $n = 3 5$ nodes, but there were other graphs with $n = 2 5$ where the algorithm did time out. For LISTCI, the algorithm did not timeout for many graphs up to $n = 8 0$ , but did time out for some graphs with $n = 7 0$ .

Experiment 2 (Application to model testing). A realworld protein signaling dataset (Sachs et al. 2005) has been used to benchmark causal discovery methods (Cundy, Grover, and Ermon 2021; Zantedeschi et al. 2023). The dataset (853 samples) comes with an expert-provided ground-truth DAG (11 nodes, 16 edges). Using LISTCI, we test to what extent this graph is compatible with the available data. We use a kernel-based CI test from the causal-learn package (Zheng et al. 2024) with p-value $p = 0 . 0 5$ (for the null hypothesis of dependence).

![](images/23880f19dd3fa0536689bce085e608d24a5deec661735629f4cbe707bbc6fb09.jpg)  
Figure 5: Plot of runtimes of the algorithms LISTGMP, LISTCIBF, and LISTCI on graphs of various sizes. A colored box indicates the interval of $n$ on which the relevant algorithm has timed out on some graphs with $n$ nodes. The y-axis uses a logarithmic scale.

For our chosen topological order, seven out of ten CIs invoked by C-LMP resulted in $p > 0 . 0 5$ . This suggests the ground-truth DAG may need revision before use as a benchmark for structure learning. The exact local CIs that are violated may guide experts in this revision process.

Experiment 3 (Analysis of C-LMP). We use LISTCI to understand the total number of non-vacuous CIs invoked by C-LMP. Let CI denote this number. CI is also the number of CIs that need to be tested from a given semi-Markovian causal DAG. Based on experiments with random graphs shown in Appendix F.3, we conclude that the graph topology associated with c-components plays a major role in CI. More specifically, two factors related to c-components are of primary interest:

1. $s \leq n$ : the size of the largest c-component, and 2. The sparsity of c-components, a proxy for which is the number of bidirected edges.

As we add bidirected edges, while c-components are sparse, CI increases exponentially with $s$ , as given by the bound $O ( n 2 ^ { s } )$ . As c-components become more dense, CI decays exponentially with the number of bidirected edges. As an illustrative example, please refer to Fig. F.3.1 and the discussion on Case 1 in Appendix F.3.

# 6 Conclusions

In this paper, we introduced a new conditional independence property for causal models with unobserved confounders, namely, the $c$ -component local Markov property (C-LMP, Def. 5). Given a DAG $\mathcal { G }$ , C-LMP identifies a small subset of conditional independence constraints (CIs) that together imply all other CIs encoded in $\mathcal { G }$ . We showed that C-LMP is equivalent to the global Markov property (Thm. 1), and that each CI that C-LMP invokes can be generated from a unique ancestral c-component (Thm. 2). Building on this foundation, we developed the first algorithm LISTCI (Alg. 1) capable of listing all CIs invoked by C-LMP in polynomial delay (Thm. 3). Reducing the number of CI tests needed to evaluate a causal model is important as it improves runtime performance and helps mitigate concerns about statistical power and multiple hypothesis testing. We hope our work will help researchers test their causal assumptions using observational data prior to inference.