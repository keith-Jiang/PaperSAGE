# Measuring Human and AI Values Based on Generative Psychometrics with Large Language Models

Haoran $\mathbf { Y } \mathbf { e } ^ { * \mathrm { ~ 1 ~ } }$ , Yuhang Xie\* 2, Yuanyi Ren\* 1, Hanjun Fang3, Xin Zhang4, Guojie Song† 1 5

1State Key Laboratory of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University 2School of Software and Microelectronics, Peking University 3Department of Sociology, Peking University 4School of Psychological and Cognitive Sciences, Peking University 5PKU-Wuhan Institute for Artificial Intelligence {hrye, yuhangxie}@stu.pku.edu.cn {yyren, hjfang, zhang.x, gjsong}@pku.edu.cn

# Abstract

Human values and their measurement are long-standing interdisciplinary inquiry. Recent advances in AI have sparked renewed interest in this area, with large language models (LLMs) emerging as both tools and subjects of value measurement. This work introduces Generative Psychometrics for Values (GPV), an LLM-based, data-driven value measurement paradigm, theoretically grounded in text-revealed selective perceptions. The core idea is to dynamically parse unstructured texts into perceptions akin to static stimuli in traditional psychometrics, measure the value orientations they reveal, and aggregate the results. Applying GPV to humanauthored blogs, we demonstrate its stability, validity, and superiority over prior psychological tools. Then, extending GPV to LLM value measurement, we advance the current art with 1) a psychometric methodology that measures LLM values based on their scalable and free-form outputs, enabling context-specific measurement; 2) a comparative analysis of measurement paradigms, indicating response biases of prior methods; and 3) an attempt to bridge LLM values and their safety, revealing the predictive power of different value systems and the impacts of various values on LLM safety. Through interdisciplinary efforts, we aim to leverage AI for next-generation psychometrics and psychometrics for valuealigned AI.

# Code — https://github.com/Value4AI/gpv Extended version — https://arxiv.org/abs/2409.12106

# 1 Introduction

Human values, a cornerstone of philosophical inquiry, are the fundamental guiding principles behind individual and collective decision-making (Rokeach 1973; Sagiv et al. 2017). Value measurement is a long-standing interdisciplinary endeavor for elucidating how specific values underpin and justify the worth of actions, objects, and concepts (Schwartz 1992; Klingefjord, Lowe, and Edelman 2024).

Traditional psychometrics often measure human values through self-report questionnaires, where participants rate the importance of various values in their lives. However, these tools are limited by response biases, resource demands, inaccuracies in capturing authentic behaviors, and inability to handle historical or subjective data (Ponizovskiy et al. 2020). Therefore, data-driven tools have been developed to infer values from textual data, such as social media posts (Shen, Wilson, and Mihalcea 2019). These tools can reveal personal values without relying on explicit selfreporting, but they are mostly dictionary-based, matching text to predefined value lexicons. Consequently, they often fail to grasp the nuanced semantics and context-dependent value expressions. Additionally, these tools are inherently static and inflexible, relying on expert-defined lexicons that are not easily adaptable to new or evolving values.

The rise of large language models (LLMs), with their remarkable ability to understand semantic nuances, presents new possibilities for data-driven value measurement. Recent studies have demonstrated that LLMs can effectively approximate annotators’ and even psychologists’ judgments on value-related tasks (Sorensen et al. 2024; Ren et al. 2024). Building on these advancements, this work introduces Generative Psychometrics for Values (GPV), an LLMbased, data-driven value measurement paradigm grounded in the theory of text-revealed selective perceptions (Postman, Bruner, and McGinnies 1948; Shen, Wilson, and Mihalcea 2019). Perceptions are the way individuals interpret and evaluate the world around them, and are servants of interests, needs, and, values (Postman, Bruner, and McGinnies 1948). Such perceptions are revealed in self-expressing texts, such as blog posts, and are utilized as atomic value measurement units in GPV. The core idea of GPV is to extract contextualized and value-laden perceptions (e.g., ”I believe that everyone deserves equal rights and opportunities.”) from unstructured texts, decode underlying values (e.g., Universalism) for arbitrary value systems, and aggregate the results to measure individual values.

The perceptions in GPV function similarly to the static psychometric items (stimuli) in self-report questionnaires, which support or oppose specific values (Schwartz 1992).

Notably, GPV enables the automatic generation of such items and their adaptation to any given data, overcoming the limitations of traditional tools (Fig. 1). By applying GPV to a large collection of human-authored blogs, we evaluate GPV against psychometric standards. GPV demonstrates its stability and validity in measuring individual values, and its superiority over prior psychological tools.

Meanwhile, the rapid evolution of LLMs raises significant concerns about their potential misalignment with human values. Recent literature treats LLMs as subjects of value measurement (Ma et al. 2024), employing self-report questionnaires (Huang et al. 2024) or their variants (Ren et al. 2024). However, these tools are inherently static, inflexible, and unscalable, as they rely on closed-ended questions derived from limited psychometric inventories.

To address these limitations, we extend the GPV paradigm to LLMs. Experimenting across 17 LLMs and 4 value theories, we advance the current art of LLM value measurement in several aspects. Firstly, GPV constitutes a novel evaluation methodology that does not rely on static psychometric inventories but measures LLM values based on their scalable and free-form outputs. In this manner, we mitigate response bias demonstrated in prior tools and enable contextspecific value measurements. Secondly, we conduct the first comparative analysis of different measurement paradigms, where GPV yields better measurement results regarding validity and utility. Lastly, we present novel findings regarding value systems and LLM values. Despite the popularity of Schwartz’s value theory within the AI community, alternative value systems like VSM (Hofstede 2011) indicate better predictive power. In addition, values like Long Term Orientation positively contribute to the predicted safety scores, while values like Masculinity negatively contribute.

Below we summarize our contributions:

• We introduce Generative Psychometrics for Values (GPV), a novel LLM-based value measurement paradigm grounded in text-revealed selective perceptions $( \ S \ 3 )$ . • Applying GPV to human-authored blogs, we demonstrate its stability, validity, and superiority over prior psychological tools $( \ S 4 )$ . • Applying GPV to LLMs, we enable LLM value measurements based on their scalable, free-form, and contextspecific outputs. With extensive evaluations across 17 LLMs, 4 value theories, and 3 measurement tools, we illustrate the superiority of GPV and uncover novel insights regarding value systems and LLM values $( \ S \ S )$ .

# 2 Related Work

For the complete reference list, please refer to the extended version of this paper.

# 2.1 Value Measurements for Human

The measurement of individual values is pivotal in elucidating the driving forces and mechanisms underlying human behavior (Schwartz 1992; Rokeach 1973). Due to the intricate relationship between behavior and values, researchers have developed different measurement methods, including self-report questionnaires (Maio 2010), behavioral observation (Fischer and Schwartz 2011), and experimental techniques (Murphy and Ackermann 2014). Self-report methods involve participants themselves assessing their agreement with descriptions (Sagiv, Sverdlik, and Schwarz 2011) or ranking the importance of items (Rokeach 1973). Behavioral observation methods require experts to analyze how personal values manifest in real-life actions (Bardi and Schwartz 2003; Schwartz and Butenko 2014). Furthermore, experimental methods employ structured scenarios to isolate and analyze variables affecting human behavior (Bekkers 2007). However, these methods are hindered by response biases, resource demands, inaccuracies in capturing authentic behaviors, and inability to handle historical or subjective data (Ponizovskiy et al. 2020).

On the other hand, data-driven tools partially address the adverse effects of resource costs, external interference, and response biases. Among them, dictionary-based tools such as LIWC dictionary (Graham, Haidt, and Nosek 2009) and personal values dictionary (PVD) (Ponizovskiy et al. 2020) analyze the frequency of value-related lexicons, flawed for overlooking nuanced semantics and contexts. Recent efforts to train deep learning models for value identification have largely focused on Schwartz’s values and are not validated for individual-level measurements (Sorensen et al. 2024). Other works transform self-report inventories into interactive assessments based on LLMs (Li et al. 2024b), yet inherit many of the limitations of self-reports, such as the inability to handle historical or subjective data.

# 2.2 Value Measurements for LLMs

The growing integration of LLMs into public-facing applications necessitates their comprehensive and reliable value measurements (Ma et al. 2024). Recently, applying psychometrics—originally designed for humans—to LLMs has gained significant interest (Pellert et al. 2023; Jiang et al. 2024). Related works involve psychometric tests such as the “dark triad” traits (Li et al. 2024c; Huang et al. 2024), the Big Five Inventory (BFI) (Safdari et al. 2023), Myers–Briggs Type Indicator (MBTI) (Pan and Zeng 2023), and morality inventories (Scherrer et al. 2023b). The test results are utilized to investigate the attributes of LLMs concerning political positions (Santurkar et al. 2023), cultural differences (Cao et al. 2023), and belief systems (Scherrer et al. 2023a).

However, researchers have observed discrepancies between constrained and free-form LLM responses, and the latter is considered more practically relevant (Ro¨ ttger et al. 2024; Ren et al. 2024). The variability in LLM responses to subtle contextual changes also necessitates scalable and context-specific evaluation methods (Ro¨ttger et al. 2024), which this work aims to address.

# 3 Generative Psychometrics for Values (GPV) 3.1 Value Measurement Based on Selective Perceptions

Values are broad motivational goals and guiding principles in life (Schwartz 1992). Value measurement quantitatively evaluates the significance attributed to various values through individuals’ behavioral and linguistic data (Adkins, Russell, and WERBEL 1994; Meglino and Ravlin 1998; Rokeach 1973). Given any pluralistic value system as a reference frame, we formalize the value measurement task as follows.

![](images/18bdfffba21cd3721815ce7e0d919bfd5057f856dc8fd7428bb20ce8d6adc758.jpg)  
Figure 1: Illustrations of the three measurement paradigms. (a) Self-reports require individuals to rate their agreement with expert-defined perceptions. (b) Dictionary-based methods count expert-defined and value-related lexicons given text data. (c) GPV automatically and dynamically extracts perceptions from text data and learns to measure open-vocabulary values.

Definition 1 (Value Measurement). Value measurement is a function $f$ :

$$
f : ( V , D ) \to \mathbf { w } \in \mathbb { R } ^ { n } .
$$

Here, $V = \{ v _ { 1 } , v _ { 2 } , \ldots , v _ { n } \}$ denotes a value system, where each $v _ { i }$ represents a particular value dimension; $D$ denotes the individuals’ behavioral and linguistic data; and ${ \textbf { w } } =$ $( w _ { 1 } , w _ { 2 } , \ldots , w _ { n } )$ is a value vector with $w _ { i }$ indicating the relative importance of $v _ { i }$ .

Extensive research explores the underlying mechanisms of $f$ , by which human values drive behaviors and behaviors reflect values (Adkins, Russell, and WERBEL 1994; Meglino and Ravlin 1998; Schwartz 1992; Rokeach 1973). Most related to this work, self-reports (Fig. 1(a)) instantiate $f$ by self-rating the agreement with expert-defined items; dictionary-based methods (Fig. 1(b)) instantiate $f$ by counting expert-defined and value-related lexicons. Both tools conduct value measurement in a limited value space (e.g. 10 Schwartz’s values define a limited 10-dimensional value space) and are inherently static and inflexible.

GPV Overview. In contrast, GPV (Fig. 1(c)) instantiates $f$ through selective perceptions, a process of selecting stimuli from the environment based on an individual’s interests, needs, and values (Postman, Bruner, and McGinnies 1948;

Anderson 2019). For example, when considering a construction project of a new park, individuals who value Hedonism will emphasize the recreational benefits, while those who prioritize Economic Efficiency will focus on the project’s cost. These differing perceptions encode value orientations. GPV leverages LLMs to automatically parse self-expressing texts into such perceptions, trains an LLM for perceptionlevel and open-vocabulary value measurement, and aggregates the results as individual values. We elaborate on the perception-level value measurement in $\ S \ 3 . 2$ , then parsing and aggregation in $\ S 3 . 3$ .

# 3.2 Perception-Level Value Measurement

Perception. Perceptions are selective stimuli from the environment that reflect an individual’s interests, needs, and values (Postman, Bruner, and McGinnies 1948). Here, perceptions are utilized as atomic measurement units, ideally capturing the following properties (Gibson 1960): (1) A perception should be value-laden and accurately describe the measurement subject, ensuring meaningful measurement. (2) A perception is an atomic measurement unit, ensuring unambiguous measurement. (3) A perception is wellcontextualized and self-contained, ensuring that it alone is sufficient for value measurement. (4) All perceptions comprehensively cover all value-laden aspects of the measured subject, ensuring that no related content in the data is left unmeasured.

Training. We fine-tune Llama-3-8B (Dubey et al. 2024) for perception-level and open-vocabulary value measurement. Its fine-tuning involves the following two tasks (Sorensen et al. 2024) using datasets of ValueBench (Ren et al. 2024) and ValuePrism (Sorensen et al. 2024): (1) Relevance classification determines whether a perception is relevant to a value. (2) Valence classification determines whether a perception supports, opposes, or remains neutral (context-dependent) towards a value. Both tasks are formulated as generating a label given a value and a perception. We present further training details in Appendix A.

Table 1: Accuracy on relevance and valence classification.   

<html><body><table><tr><td>Model</td><td>Relevance</td><td>Valence</td></tr><tr><td>Kaleido</td><td>83.5%</td><td>82.5%</td></tr><tr><td>GPT-4 Turbo</td><td>79.8%</td><td>87.5%</td></tr><tr><td>ValueLlama (ours)</td><td>90.0%</td><td>91.5%</td></tr></table></body></html>

Inference. We refer to the fine-tuned Llama-3-8B as ValueLlama. Given a value system $V = \{ v _ { 1 } , v _ { 2 } , \ldots , v _ { n } \}$ and a sentence of perception $s$ , we employ ValueLlama to calculate the relevance and valence probability distribution of each value $\boldsymbol { v } _ { i }$ to $s$ , respectively denoted as ${ p _ { r e l } ( \cdot | v _ { i } , s ) }$ and $p _ { v a l } ( \cdot | v _ { i } , s )$ . Then, we define $w _ { i }$ as $p _ { v a l } ( \mathrm { s u p p o r t } | v _ { i } , s ) \ -$ $p _ { v a l } ( \mathrm { o p p o s e } | v _ { i } , s )$ if the value is relevant $( p _ { r e l } ( \cdot | v _ { i } , s ) \ >$ 0.5) and its valence is classified as ”support” or ”oppose”. Otherwise, $w _ { i }$ is considered unmeasured. The prompts for inference are listed in Appendix A.

Evaluating Perception-level Value Measurements. To evaluate the accuracy of perception-level value measurements, we hold out 50 values and 200 associated items (146 with ”Supports” valence and 54 with ”Opposes” valence) from ValueBench as a test dataset, also ensuring the test values are not included in ValuePrism. Using the same zeroshot prompt, we measure the relevance and valence of the test items with Kaleido (Sorensen et al. 2024), GPT-4 Turbo (Achiam et al. 2023), and ValueLlama. Table 1 presents the comparison results, indicating that ValueLlama outperforms state-of-the-art general and task-specific LLMs in zero-shot perception-level value measurement.

# 3.3 Parsing and Aggregation

To measure values at the individual level, GPV chunks long texts (e.g., blog posts) into segments and prompts an LLM (this work used GPT-3.5 Turbo) to parse each segment into perceptions. Parsing is guided by the background on human values, definitions of perceptions, and few-shot examples (Appendix B.1.) Then, GPV performs perception-level value measurement $( \ S 3 . 2 )$ for the parsing results. Individual-level measurements are calculated by averaging the perceptionlevel measurements for each value (Schwartz et al. 2007).

Evaluating LLM Parsing. The parsing results are considered high-quality by trained human annotators. On average, the annotators agree that the parsing results meet the defined four criteria in over $8 5 \%$ of cases, deeming them suitable for further value measurement. The evaluation is detailed in Appendix B.2.

# 3.4 Discussion

Relation to Self-Reports. The items organized in selfreport inventories are essentially perceptions that support or oppose specific values (Schwartz 1992). Compared to GPV, these traditional psychometric inventories compile static and unscalable perceptions, covering a limited measurement range. They also necessitate an additional self-report process to assess the individual’s agreement with the items.

Relation to Dictionary-Based Methods. Both GPV and dictionary-based methods share the fundamental principle that values are embedded in language (Shen, Wilson, and Mihalcea 2019), and they each measure values through text data. However, dictionary-based methods depend on predefined lexicons for closed-vocabulary values and are far less expressive than GPV in capturing semantic nuances. Further analysis is presented in $\ S 4 . 2$ .

Advantages of GPV. Compared with traditional tools, GPV 1) effectively mitigates response bias and resource demands by dispensing with self-reports; 2) captures authentic behaviors instead of relying on forced ratings; 3) can handle historical or subjective data; 4) measures values in openended value spaces and easily adapts to new or evolving values without expert effort; and 5) enables more scalable and flexible value measurement.

# 4 GPV for Humans

This section measures human values using 791 blogs from the Blog Authorship Corpus (Schler et al. 2006), selected after filtering out low-quality entries (Appendix C.1). We evaluate GPV using standard psychological metrics including stability, construct validity, concurrent validity, and predictive validity, and demonstrate its superiority over established psychological tools.

# 4.1 Validation

Stability. As values are relatively stable psychological constructs for humans (Sagiv and Roccas 2017; Sagiv et al. 2017; Kimura 2023), we expect that the same individual should exhibit consistent value tendencies across different scenarios. Across 48,888 perception-value pairs, $8 6 . 6 \%$ of the perception-level measurement results are consistent with the individual-level aggregated results, indicating desirable stability. Detailed results and extended discussions are shown in Appendix C.2.

Construct Validity. Construct validity is the extent to which a test measures what it claims to measure. In Schwartz’s value system, some values are theoretically positively correlated, such as Self-Direction and Stimulation, while others are negatively correlated, such as Power and Benevolence. Altogether, the 10 Schwartz values form a circumplex structure (Schwartz and Bilsky 1990), where values that are closer together are more compatible, while those that are farther apart are more conflicting (Fig. 2a). We employ multidimensional scaling (MDS) (Cieciuch and Schwartz 2012; Bilsky, Janik, and Schwartz 2011) on the value correlations obtained by GPV, and project both the 10 basic values and the 4 higher-order values onto two-dimensional

Table 2: Correlations between the measurement results of PVD and GPV for four high-level values: Self-transcendence (Stran), Conservation (Cons), Openness to Change (Open), and Self-enhancement (Senh).   

<html><body><table><tr><td colspan="2"></td><td colspan="4">GPV</td></tr><tr><td colspan="2"></td><td>Stran</td><td>Cons</td><td>Open</td><td>Senh</td></tr><tr><td rowspan="3">PVD</td><td>Stran</td><td>0.0421</td><td>0.0077</td><td>-0.0318</td><td>-0.0579</td></tr><tr><td>Cons</td><td>-0.0530</td><td>0.0687</td><td>0.0290</td><td>-0.0321</td></tr><tr><td>Open</td><td>-0.0345</td><td>-0.1376</td><td>0.0369</td><td>-0.0615</td></tr><tr><td></td><td>Senh</td><td>-0.0693</td><td>-0.0345</td><td>0.0540</td><td>0.0880</td></tr></table></body></html>

MDS plots. Then, we assess whether their relative positions align with the theoretical structure. As illustrated in Fig. 2, basic values of the same category (represented by the same color) generally cluster together. Higher-order opposing values are positioned farther apart. The relative positions of a few values do not strictly follow the theoretical structure. For example, Conservation is relatively distant from the other three higher-order values. Such deviations may reflect a gap between the values manifested by self-report and objective data (Ponizovskiy et al. 2020). Overall, the relative positioning of most values resembles the theoretically expected pattern in Fig. 2a, indicating desirable construct validity. More experimental details are provided in Appendix C.3.

Concurrent Validity. Concurrent validity is the extent to which a test correlates with other measures of the same construct administered simultaneously. Theoretically expected correlations can validate newly developed instruments (Lin and Yao 2024). We evaluate the concurrent validity of GPV by comparing it with the personal values dictionary (PVD) (Ponizovskiy et al. 2020), a well-established measurement tool with proven reliability and validity. We analyze the correlations between GPV and PVD measurements, with the results of low-level values presented in Appendix C.4 and high-level aggregated values in Table 2. The results indicate that among the 10 basic values, both identical values (e.g., SE-SE) and most compatible values (e.g., CO-SE) show positive correlations; most opposing values (e.g., BE-AC) exhibit negative correlations. Similarly, within the 4 higherorder values, positive correlations are observed when measuring identical values, whereas most opposing values display negative correlations. These correlations, though not strong, are theoretically expected, which supports the concurrent validity of GPV. $\ S \ 4 . 2$ exemplifies the cases where GPV misaligns with PVD.

Predictive Validity. Predictive validity is the extent to which a test predicts future behavior or outcomes. We assess predictive validity by examining if our measurement results align with the blog authors’ gender-related sociodemographic traits. Previous research indicates that, in a statistical sense, men prioritize power, stimulation, hedonism, achievement, and self-direction, while women emphasize benevolence and universalism (Schwartz and Rubel 2005). Our measurement results, presented in Table 3, reveal that men and women score higher on the values they typically prioritize, confirming the consistency of our measurements with established psychological findings.

# 4.2 Case Study

We exemplify the advantage of GPV over prior data-driven tools such as PVD in Fig. 3. Some values, while not explicitly mentioned in PVD-designed lexicons, are implied within the text. For example, in Schwartz’s theory, Achievement is defined as “the personal pursuit of success, demonstrating competence according to social standards.” In this context, “the teacher’s praise” and “performing well in an exam” both embody the “success” element of achievement. Although the text does not directly reference Achievement or Achievement-related lexicons, the author’s expression of joy and aspiration for these outcomes reflects this value. While GPV effectively captures this aspect, PVD does not.

Some PVD-designed lexicons fail to align with the measurement subject or reflect their intended values. For instance, “friendly” and “goal” target the author’s deskmate; picking up “money” does not indicate the author’s own values of Power. GPV effectively avoids such misinterpretation.

# 5 GPV for Large Language Models

We evaluate 17 LLMs across 4 value systems using 3 measurement tools: self-report questionnaires (Huang et al. 2024), ValueBench (Ren et al. 2024), and GPV. Unless otherwise specified, we use LLM-generated value-eliciting questions for GPV to ensure a comprehensive and thorough measurement of each value. The detailed experimental setup is described in Appendix D.1.

Across 19910 perception-value pairs, $8 6 . 8 \%$ perceptionlevel measurement results are consistent with the LLM-level aggregated results, indicating desirable stability; we present the detailed results in Appendix D.2.

This section focuses on comparing GPV against prior measurement tools. We defer the value measurement results of all LLMs to Appendix D.4.

# 5.1 Comparative Analysis of Construct Validity

Using the measurement results from 17 LLMs as data points, we compute correlations between Schwartz’s values. The results are visualized in a heatmap for each measurement tool in Fig. 4. The heatmap reveals the superior construct validity of GPV, as its measurement results align more closely with the theoretical structure (Fig. 2a). Specifically, values that are adjacent in the theoretical circumplex structure exhibit positive correlations, while those that are theoretically distant show negative correlations.

In contrast, prior tools obtain almost all-positive correlations, contrary to theoretical expectations. This discrepancy indicates their strong susceptibility to response biases, wherein certain LLMs generally tend to assign higher scores in self-report or respond more supportively in ValueBench. Such biases obscure the genuine value orientations of the LLMs. Even when centering the measurement results of prior tools (Appendix D.3), the correlation results remain inconsistent with the theoretical structure. This finding aligns with recent studies revealing the unreliability of

![](images/e912de06229bb2ea570bbaa7cbdd6897d93f010a2cf2076b02a42e19c1fc0eeb.jpg)

![](images/c0dc118d1b3b71a2fafca370bac5b80843155aa4006ec83452c80b5f83ca244b.jpg)

(a) Theoretical structure.

(b) MDS of 10 basic values.

(c) MDS of 4 high-level values.

Figure 2: Two-dimensional MDS of individual values measured by GPV.   

<html><body><table><tr><td>Gender</td><td>SE</td><td>CO</td><td>TR</td><td>BE</td><td>UN</td><td>SD</td><td>ST</td><td>HE</td><td>AC</td><td>PO</td></tr><tr><td>Male</td><td>0.478</td><td>-0.424</td><td>0.261</td><td>0.691</td><td>0.593</td><td>0.777</td><td>0.797</td><td>0.745</td><td>0.757</td><td>0.626</td></tr><tr><td>Female</td><td>0.459</td><td>-0.414</td><td>0.214</td><td>0.751</td><td>0.649</td><td>0.748</td><td>0.761</td><td>0.736</td><td>0.725</td><td>0.587</td></tr></table></body></html>

Table 3: GPV measurement results on Schwartz values for male and female groups.   

<html><body><table><tr><td>Value Pair</td><td>Self-report</td><td>ValueBench</td><td>GPV</td></tr><tr><td>UA&DA</td><td>0.09</td><td>-0.17</td><td>0.65</td></tr><tr><td>Indv&SD</td><td>0.21</td><td>-0.38</td><td>0.61</td></tr><tr><td>Indu&He</td><td>0.30</td><td>-0.30</td><td>0.65</td></tr><tr><td>CO&Be</td><td>0.22</td><td>0.79</td><td>0.38</td></tr><tr><td>Avg.</td><td>0.21</td><td>-0.01</td><td>0.57</td></tr></table></body></html>

LLMs as survey respondents (Dominguez-Olmedo, Hardt, and Mendler-Du¨nner 2023; Ro¨ttger et al. 2024).

Besides Schwartz’s value system, we also evaluate the construct validity by relating the values of different value theories that are theoretically positively correlated. Results in Table 4 indicate the superior construct validity of GPV; i.e., for the theoretically positively correlated values, measuring with GPV also yields higher correlations.

In summary, evaluations within and across value theories indicate superior construct validity of GPV over prior tools that are prone to response bias.

# 5.2 Comparative Analysis of Value Representation Utility

The utility of human value measurements lies in their predictive power for human behavior (Schwartz et al. 2007). In the context of LLMs, many related studies are motivated by value alignment for safe LLM deployment (Ji et al. 2023). However, few studies have connected LLM values with their safety. In this section, we evaluate the value representation utility of different measurement tools in terms of their predictive power for LLM safety scores.

Table 4: Correlation between theoretically positively correlated values when using different tools, including Uncertainty Avoidance (UA) & Discomfort with Ambiguity (DA), Individualism (Indv) & Self-Direction (SD), Indulgence (Indu) & Hedonism $\mathrm { ( H e ) }$ , and Concern for Others (CO) & Benevolence (Be).   
Table 5: Classification accuracy when using linear probing for value measurement results.   

<html><body><table><tr><td>Tools</td><td>Acc. (%)</td></tr><tr><td>Self-report</td><td>56.7 ± 26.0</td></tr><tr><td>ValueBench</td><td>67.8 ± 20.6</td></tr><tr><td>GPV</td><td>85.6 ± 14.1</td></tr></table></body></html>

Here, we use the safety scores of 17 LLMs from SALADBench (Li et al. 2024a) as ground truth and randomly sample 100 prompts from Salad-Data (Li et al. 2024a) for GPV measurement. We follow the standard linear probing protocol and train a linear classifier to predict the relative safety of LLMs, using the value measurement results as features. We perform its training 30 times for each measurement tool with randomly sampled data splits to ensure statistically meaningful results. Full experimental details are given in Appendix D.4.

Using values from different value theories as features leads to different results. We present the best classification accuracy of different measurement tools in Table 5. The results indicate that GPV is more predictive of LLM safety scores than prior tools. It suggests that GPV values can be an interpretable and actionable proxy for LLM safety under specific context (R¨ottger et al. 2024).

In addition, as detailed in Appendix D.4, we examine the predictive power of various value systems for LLM safety scores, as well as the impact of different values on LLM safety. We find that, despite the popularity of Schwartz’s value system within the AI community, VSM (Hofstede

Benevolence: { 'friendly' } User Self-direction: { 'goal' }   
Today is a wonderful day! In the morning, \*···\*   
found some money and gave it to a police officer, PVD Power: { 'money' }   
who gave me a thumbs-up. At noon, my desk   
mate told me that his goal is to become a pilot. perception 1: Acting generously by giving found money to a police   
He is very friendly, and we get along very well. officer.  { 'Benevolence':1 }   
iIn tmhaethafctlearsnso.oTnh,eI taenaschwer esdaida Idiwffaiscualtgqouoedstkiiodn, 好 pmeartce pwtihona2s:p iArepsptreocbiaetiaopnilfotr. t h{e'Ffriend'l:y1 r}elationship with my desk   
and I was very pleased. I want to do well on my GPV   
next exam. perception N: Desire to perform well on the next exam. 64 24M { 'Self-direction':1, 'Achievement':1 }

Figure 3: Comparative analysis of PVD (Ponizovskiy et al. 2020) and GPV: a case study.

![](images/c6aeffdcfaa584d2b738544bb876c29d4294801928c7887d39ab3106d14a97ba.jpg)  
Figure 4: Correlations between Schwartz values when using different measurement tools. From dark blue to dark red, the correlation ranges from -1 to 1. The Schwartz values are ordered along the $\mathbf { X }$ -axis and y-axis according to their positions in the theoretical circumplex structure. See the extended version for more details.

2011) is more predictive of LLM safety. Within VSM, values like Long-term Orientation positively contribute to LLM safety while values like Masculinity negatively contribute.

In summary, GPV is more predictive of LLM safety. The proposed Value Representation Utility also enables us to evaluate both the predictive power of a value system and the relationship between each encoded value and LLM safety.

# 5.3 Discussion

Superiority of GPV. We discuss that the superior construct validity may be attributed to the encoded knowledge. ValueLlama learns the correlations between different values and exploits them to generate coherent and valid measurements. In addition, measuring the free-form LLM responses is more reliable than prompting with forcedchoice questions (Dominguez-Olmedo, Hardt, and MendlerDu¨nner 2023). The superior value representation utility of GPV may be attributed to the context-specific value measurements. Unlike humans, who exhibit stable values, LLMs may not be treated as monolithic entities, highlighting the importance of context-specific measurement (Ro¨ttger et al. 2024). GPV, for the first time, enables reliable contextspecific measurements. Overall, compared to prior tools, GPV for LLM value measurements 1) mitigates response bias and yields theoretically valid results; 2) is more practically relevant due to measuring scalable and free-form LLM responses; and 3) enables context-specific measurements.

Limitations and Future Work. The current studies are limited to evaluating LLMs in English. Since the used languages are shown to affect LLM values (Cahyawijaya et al. 2024), future research should consider multi-lingual measurements. Additionally, future investigations should explore the spectrum of values an LLM can exhibit, examining the effects of different profiling prompts. Though LLM values may be steerable, current alignment algorithms establish default model positions and behaviors, making it still meaningful to evaluate the values and opinions reflected in these defaults (Ro¨ ttger et al. 2024).

# 6 Conclusion

This paper introduces GPV, an LLM-based tool designed for value measurement, theoretically based on text-revealed selective perceptions. Experiments conducted through diverse lenses demonstrate the superiority of GPV in measuring both human and AI values. GPV offers promising opportunities for both sociological and technical research. In sociological research, GPV enables scalable, automated, and cost-effective value measurements that reduce response bias compared to self-reports and provide more semantic nuance than prior data-driven tools. It is highly flexible and can be used independently of specific value systems or measurement contexts. For technical research, GPV presents a new perspective on value alignment by offering interpretable and actionable value representations for LLMs.

# Ethical Statement

Measuring values with GPV may involve biases encoded in LLMs, during perception-level measurement and perception parsing. Currently, GPV is intended for research purposes only, and researchers should exercise caution when applying it to content with subjective or controversial interpretations.

For the perception-level measurement, we fine-tuned our model using established psychological inventories and synthetic data validated across cultures, aiming to reduce measurement bias. In the three-class valence classification task, the model is trained to provide neutral predictions when additional context is needed, thereby minimizing the risk of bias. Nevertheless, achieving unbiased measurement requires further investigation.

The parsing results in this study are considered highquality by our annotators. However, since the annotators share a similar demographic background, their evaluations may lack a comprehensive and diverse perspective. Additionally, the blog data analyzed in this work primarily focuses on general, everyday topics and rarely involves controversial issues. Addressing potential biases in parsing remains an open area for future research.