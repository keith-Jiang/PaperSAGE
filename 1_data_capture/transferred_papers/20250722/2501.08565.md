# DualOpt: A Dual Divide-and-Optimize Algorithm for the Large-scale Traveling Salesman Problem

Shipei Zhou1\*, Yuandong $\mathbf { D i n g ^ { 1 * } }$ , Chi Zhang1, Zhiguang $\mathbf { C a o } ^ { 2 }$ , Yan ${ \bf { J i n } } ^ { 1 \dagger }$

1School of Computer Science, Huazhong University of Science and Technology, China 2School of Computing and Information Systems, Singapore Management University, Singapore m202273616@hust.edu.cn, yuandong@hust.edu.cn, chizhang $0 2 \textcircled { a }$ hust.edu.cn, zgcao@smu.edu.sg,jinyan $@$ mail.hust.edu.cn

# Abstract

This paper proposes a dual divide-and-optimize algorithm (DualOpt) for solving the large-scale traveling salesman problem (TSP). DualOpt combines two complementary strategies to improve both solution quality and computational efficiency. The first strategy is a grid-based divide-andconquer procedure that partitions the TSP into smaller subproblems, solving them in parallel and iteratively refining the solution by merging nodes and partial routes. The process continues until only one grid remains, yielding a high-quality initial solution. The second strategy involves a path-based divide-and-optimize procedure that further optimizes the solution by dividing it into sub-paths, optimizing each using a neural solver, and merging them back to progressively improve the overall solution. Extensive experiments conducted on two groups of TSP benchmark instances, including randomly generated instances with up to 100,000 nodes and realworld datasets from TSPLIB, demonstrate the effectiveness of DualOpt. The proposed DualOpt achieves highly competitive results compared to 10 state-of-the-art algorithms in the literature. In particular, DualOpt achieves an improvement gap up to $1 . 4 0 \%$ for the largest instance TSP100K with a remarkable $1 0 4 \mathrm { x }$ speed-up over the leading heuristic solver LKH3. Additionally, DualOpt demonstrates strong generalization on TSPLIB benchmarks, confirming its capability to tackle diverse real-world TSP applications.

# Code — https://github.com/Learning4OptimizationHUST/DualOpt

# Introduction

The Traveling Salesman Problem (TSP) is an NP-hard combinatorial optimization problem, which has numerous realworld applications (Madani, Batta, and Karwan 2021; Hacizade and Kaya 2018; Matai, Singh, and Mittal 2010). Let $\textit { G } = \ : ( V , E )$ represent an undirected graph, where $V ~ = ~ \{ v _ { i } ~ | ~ 1 ~ \leq ~ i ~ \leq ~ N \}$ is the set of nodes and $E ~ = ~ \{ e _ { i j } ~ \mid ~ 1 ~ \leq ~ i , j ~ \leq ~ N \}$ is the set of edges, with $N$ being the total number of nodes. For each edge $e _ { i j }$ , the travel cost $\it { c o s t } ( i , j )$ is defined as the Euclidean distance between the nodes $v _ { i }$ and $v _ { j }$ . A special node $v _ { d } \in V$ serves as the depot, from which the salesman starts and ends the trip. A feasible solution of TSP is defined as a Hamiltonian cycle that starts from the depot, visits each node exactly once, and ends at the depot. The objective is to minimize the total travel cost of the solution route $\tau$ , denoted as $L ( \tau ) = \sum _ { i = 1 } ^ { N - 1 } c o s t ( \tau _ { i } , \tau _ { i + 1 } ) + c o s t ( \tau _ { N } , \tau _ { 1 } )$ , where $\tau _ { i }$ represents the $i - t h$ node in the route.

Due to its theoretical and practical interest, various traditional exact/heuristic and machine learning-based algorithms have been proposed in the literature. While exact algorithms are generally computationally infeasible for largescale instances, heuristics can provide near-optimal solutions for TSP with millions of cities, though they cannot guarantee optimality. They often involve time-consuming iterative searches, making them less suitable for timesensitive tasks. Machine learning based algorithms, on the other hand, offer high computational efficiency and can achieve solution quality comparable to traditional methods for small-scale TSP instances. However, applying them to large-scale TSP, especially those with over 1,000 cities, remains a challenge. One approach is to leverage pretrained models from small-scale instances for larger ones, but this often results in poor performance due to distribution shift (Joshi et al. 2022). Training models specifically for large-scale TSP is also impractical due to computational resource limitations.

A basic approach to deal with the large-scale routing problem is to apply the general principle of “divide-andconquer” (Fu, Qiu, and Zha 2021; Pan et al. 2023; Hou et al. 2023; Chen et al. 2023; Ye et al. 2024b; Zheng et al. 2024; Xia et al. 2024). It involves decomposing TSP into a subset of small sub-problems, which can be efficiently solved using traditional or machine learning algorithms to attain sub-solutions. The final solution is then obtained by combining these sub-solutions. This approach can significantly reduce the computational complexity of the large-scale TSP, and yield high-quality solutions in a relatively short time.

In this work, we introduce a novel dual divide-andoptimize algorithm DualOpt based on the divide-andconquer framework. The first divide-and-optimize procedure partitions the nodes into $M \times M$ equal-sized grids based on their coordinates. The nodes within each grid are initially solved using the well-known LKH3 solver (Helsgaun 2017). Next, an edge-breaking strategy is proposed to decompose the route into partial routes and nodes, significantly reducing the number of nodes. Subsequently, every four adjacent grids are merged into one larger grid. This process is repeated until all nodes are contained within a single grid, at which point the LKH3 solver is applied to obtain a complete route based on the partial routes and nodes. The second divide-and-optimize procedure further refines the obtained route by partitioning it into non-overlapping subpaths. These subpaths are then optimized in parallel using a neural solver (Kim, Park et al. 2021; Pan et al. 2023; Ye et al. 2024b), ultimately leading to an optimized route. The main contributions of this work are summarized as follows:

• We propose the DualOpt algorithm, which combines two complementary divide-and-conquer frameworks to address the large-scale TSP. The first framework employs grid-based partitioning, while the second applies pathbased optimization, significantly improving both solution quality and computational efficiency of large-scale TSP.   
• We introduce a novel edge-breaking strategy that decomposes routes into partial routes and nodes. By representing these partial routes solely by their start and end nodes, this strategy considerably reduces the number of nodes, thereby decreasing computational complexity and improving search efficiency.   
• We conduct extensive experiments on both randomly generated and real-world large-scale TSP instances. To the best of our knowledge, DualOpt achieves state-ofthe-art performance compared to other machine learningbased approaches, particularly excelling in large-scale instances with up to 100,000 nodes.

# Related Work

Here we present representative traditional and machine learning-based algorithms to solve TSP, and then focus on the divide-and-conquer algorithms that are more effective for solving large-scale TSP over 1000 nodes.

Traditional Algorithms Traditional algorithms can be roughly classified into two categories: exact and heuristic algorithms (Gutin and Punnen 2006; Accorsi and Vigo 2021). Concorde (Applegate et al. 2009) is one of the best exact solvers, which models TSP as a mixed-integer programming problem and solves it using a branch-and-cut (Toth and Vigo 2002). Exact algorithms can theoretically guarantee optimal solutions for instances of limited size, but are impractical for solving large instances due to their inherent exponential complexity. LKH3 (Helsgaun 2017) is one of the state-ofthe-art heuristics that uses the $k$ -opt operators to find neighboring solutions, which is guided by a $\alpha$ -nearness measure based on the minimum spanning tree. The heuristics are the most widely used algorithms in practice, yet they are still time consuming to obtain high-quality solutions when solving problems with tens of thousands of nodes.

Machine Learning-based Algorithms In recent years, machine learning-based algorithms have attracted more interest in solving the TSP. Depending on how solutions are constructed, they can be broadly categorized into end-to-end approaches and search-based approaches.

End-to-end approaches generate solutions from scratch. The Attention Model (Kool, van Hoof, and Welling 2019) utilizes the Transformer architecture (Vaswani et al. 2017) and is trained with REINFORCE (Wiering and Van Otterlo 2012) using a greedy rollout baseline. POMO (Kwon et al. 2020) improves on this by selecting multiple nodes as starting points, using symmetries in solution representation, and employing a shared baseline to enhance REINFORCE training. DIFUSCO (Sun and Yang 2023) uses graph-based denoising diffusion models to generate solutions, which are further optimized by local search with $k$ -opt operators. Although DIFUSCO can handle problems with up to 10,000 nodes, it is less suitable for time-sensitive scenarios. Pointerformer (Jin et al. 2023) incorporates a reversible residual network in the encoder and a multi-pointer network in the decoder, allowing it to solve problems with up to 1000 nodes. Search-based approaches start with a feasible solution and iteratively apply predefined rules to improve it. NeuRewriter (Chen and Tian 2019) iteratively rewrites local components through a region-picking and rule-picking process, with the model trained using Advantage Actor-Critic, and the reduced cost per iteration serves as the reward. NeuroLKH (Xin et al. 2021) enhances the traditional LKH3 solver by employing a sparse graph network trained through supervised learning to simultaneously generate edge scores and node penalties, which guide the improvement process. DeepACO (Ye et al. 2024a) integrates neural enhancements into Ant Colony Optimization (ACO) algorithms, further improving its performance.

Machine learning-based algorithms perform well on TSP instances with up to 1,000 nodes, but struggle with larger instances due to the exponential increase in memory requirements and computation time as the number of nodes grows. This leads to memory and time constraints during training, making it difficult to converge to near-optimal solutions. To address this challenge in solving large-scale problems with thousands or more nodes, the divide-and-conquer strategy is often employed. It is always combined with traditional or learning-based algorithms to generate high-quality solutions in a relatively short time. GCN-MCTS (Fu, Qiu, and Zha 2021) applies graph sampling to construct fixedsize sub-problems, solved by graph convolutional networks, with heatmaps guiding Monte-Carlo Tree Search (MCTS). H-TSP (Pan et al. 2023) hierarchically builds TSP solutions using a two-level policy: the upper-level selects subproblems, while the lower-level generates and merges openloop routes. ExtNCO (Chen et al. 2023) uses LocKMeans with $o ( n )$ complexity to divide nodes into sub-problems, solving them with neural combinatorial optimization and merging solutions via a minimum spanning tree. GLOP (Ye et al. 2024b) learns global partition heatmaps to decompose large-scale problems and introduces a scalable real-time solver for small Shortest Hamiltonian Path problems. UDC (Zheng et al. 2024) proposes a Divide-ConquerReunion framework using efficient Graph Neural Networks for division and fixed-length solvers for sub-problems. Soft

Dist (Xia et al. 2024) demonstrates that a simple baseline method outperforms complex machine learning approaches in heatmap generation, and the heatmap-guided MCTS paradigm is inferior to the LKH3 heuristic despite leveraging hand-crafted strategies.

# The Proposed DualOpt Algorithm

The proposed DualOpt algorithm is based on and extends the basic divide-and-conquer method by incorporating a gridbased procedure and a path-based procedure to improve efficiency. Each procedure operates on two levels: the first level is responsible for generating sub-problems, while the second level focuses on solving these sub-problems.

Algorithm 1: The Proposed DualOpt Algorithm for   

<html><body><table><tr><td>the Large-scale TSP</td></tr><tr><td>Input: TSP instance V = {U1, U2,.· , UN} Output: Solution route T 1 PartialRoutesSetY←@; 2 NodesSetN ←V ; 3 repeat 4 Grids←Partition(Y,) ;</td></tr></table></body></html>

The whole procedure of DualOpt is summarized in Algorithm 1. It starts by partitioning the TSP instance into smaller grids, which reduces the problem size and allows for parallel solving. In each iteration, the partial routes and nodes within each grid are solved in parallel to generate a solution for that portion of the TSP. Next, an edge-breaking procedure is applied to divide each route into smaller partial routes and nodes. The grids are then merged into larger ones using a grid merging procedure. This grid-based iteration continues until only one grid remains, forming a reduced problem that consists of a number of partial routes and nodes. From this reduced problem, a high-quality initial solution of is constructed. To further refine the solution, it is divided into subpaths, which are optimized in batches and then merged. This iterative refinement, performed with varying partition sizes, progressively improves the solution quality.

# A Grid-based Divide-and-Conquer Procedure

To decompose the large-scale TSP for efficient solving without significantly downgrading solution quality, we employ an iterative grid decomposition strategy, denoted as the Gridbased Divide-and-Conquer Procedure, as depicted in Algorithm 2. Initially, the node set $\aleph$ contains all the nodes of the TSP instance, while the partial route set $\Upsilon$ is initialized as empty. In each iteration, the 2D space is discretized into an evenly spaced grid of size $K = 2 ^ { \hat { N } _ { i t e r } - i t e r } \times 2 ^ { N _ { i t e r } - i t e r }$ grids. The node set $\aleph$ and partial route set $\Upsilon$ are then divided into $K$ subsets $\{ ( \Upsilon _ { 1 } , \aleph _ { 1 } ) , \dots , ( \Upsilon _ { K } , \aleph _ { K } ) \}$ based on their positions within the grid. Each of these subsets is solved in parallel using the well-known TSP solver LKH3 (Helsgaun 2017), resulting in $K$ small routes $\{ R _ { 1 } , \ldots , R _ { K } \}$ . If the current iteration (iter) is not the last one, the algorithm proceeds by applying a proposed edge-breaking strategy in parallel, which breaks a subset of the edges of the routes and updates $\Upsilon$ and $\aleph$ with new sets of partial routes and nodes $\{ ( \Upsilon _ { 1 } ^ { \prime } , \aleph _ { 1 } ^ { \prime } ) , . . . , ( \Upsilon _ { K } ^ { \prime } , \aleph _ { K } ^ { \prime } ) \}$ . As the iterations progress, $K$ decreases and the grid size increases correspondingly until only one grid remains. At this final stage, a reduced problem consisting of partial routes and nodes is formed, from which an initial high-quality TSP solution $\tau$ is obtained.

Algorithm 2: Grid-based Divide-and-Conquer   

<html><body><table><tr><td></td><td>Input: TSP instance V = {U1,U2,..,UN}, number of iterations Niter Output: Solution route T</td></tr><tr><td></td><td>1 iter←1; 2 NodesSetN←V ;</td></tr><tr><td></td><td>3 PartialRoutesSet Y←D;</td></tr><tr><td></td><td>4 while iter≤Niter do</td></tr><tr><td>5</td><td>K ← 2Niter-iter × 2Niter-iter; /*Calculate the number of grids*/</td></tr><tr><td>6</td><td>{(£1,1),...,(£K,NK)}← Partition(M,,K);/*Partition nodes and</td></tr><tr><td>7</td><td>partial routes into grids*/ {R1,...,RK}←</td></tr><tr><td></td><td>SolveGridsParallel({(Mi,Ni),...,(Mk,Nk)}); /*Solve the TSP for each grid in parallel*/</td></tr><tr><td>8 9</td><td>if iter≠Niter then {(r',1),...,(2'κ,Nκ)}←</td></tr><tr><td></td><td>BreakEdgesParallel({R1,...,Rk}; /*Break edges into partial routes and nodes inparallel*/</td></tr><tr><td>10 11</td><td>Y is updated by {Y',.., Y'k} ; N is updated by {N1,...,N}；</td></tr><tr><td>12</td><td>else</td></tr><tr><td>13</td><td>T ←SolveReducedProb(M,N) ;/*Solve the</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>reduced problem to get a complete route*/</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>14</td><td>iter←iter+1;</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>15return T</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></table></body></html>

![](images/f50a6887f92cc2a4e7b241073da148e7dc7ea9c71295bb14a9513dd975392c38.jpg)  
Figure 1: An illustration of the Grid-based Divide-andConquer Procedure with $N _ { i t e r } = 3$ .

routes per grid, which are then decomposed into partial routes and separate nodes. In the second iteration, $K$ is reduced to 4, resulting in 4 larger grids where the LKH3 processes partial routes with fixed nodes. In the final iteration, $K$ is reduced to 1, merging the grids into a single one. As a result, the number of nodes decreases while the number of partial routes increases. This reduction in problem scale enables the LKH3 solver to efficiently find a TSP solution with high quality.

The LKH3 Solver The LKH3 solver, a state-of-the-art heuristic for the TSP, is built upon the foundational LinKernighan heuristic, incorporating several advanced components. Among these, the use of a 1-tree structure is particularly notable, as it provides a lower bound that guides the search process effectively. The algorithm dynamically employs $K$ -opt neighborhood operators, where multiple edges are iteratively removed and reconnected to explore potential routes within the solution space. The efficiency of LKH3 is further enhanced by using candidate sets, which strategically limits the number of neighborhood operators, thereby reducing search overhead.

The number of nodes within each grid, typically ranging from a few hundreds to approximately one thousand, is determined by both the instance scale and the number of grids $K$ . The input to the solver includes not only the coordinates of nodes but also the fixed partial routes. LKH3 excels in handling node scales within this range, delivering high-quality solutions with remarkable speed, particularly for routing problems involving fixed edges. Therefore, LKH3 was selected as the subsolver for this study.

Edge-breaking Strategy Recall that multiple routes are generated, one for each grid within the 2D space. When merging and optimizing these routes, it is essential to reschedule the surrounding nodes of each grid in conjunction with the nodes of adjacent grids, while excluding the internal nodes from this rescheduling process. To achieve this, we propose an edge-breaking strategy that effectively reduces the problem’s scale. Specifically, for each grid, we define an internal grid that maintains a fixed spacing from the outer grid. This spacing is calculated as spacing $\begin{array} { r } { = \frac { x _ { \mathrm { m a x } } - x _ { \mathrm { m i n } } } { 2 ^ { N _ { \mathrm { i t r } } + 2 } } } \end{array}$ where $x _ { \mathrm { m a x } }$ and $x _ { \mathrm { m i n } }$ denote the maximum and minimum $x$ -coordinates of the grid, respectively. We then remove all edges that lie outside the internal grid, as well as those crossing into it. As a result, the nodes and edges within the internal grid form several partial routes that each may consist of more than two connected nodes, along with isolated nodes.

# A Path-based Divide-and-Optimize Procedure

Based on the solution derived from the grid-based divideand-conquer procedure, further optimization is achieved through the path-based divide-and-optimize procedure, as detailed in Algorithm 3. Following previous studies (Kim, Park et al. 2021; Ye et al. 2024b; Zheng et al. 2024), this iterative algorithm aims to refine an initial solution $\tau$ by partitioning it into smaller sub-paths, optimizing these sub-paths individually, and then merging them to generate an improved solution $\tau ^ { * }$ .

The procedure begins by iterating over a specified set of sub-path lengths $l e n _ { 1 } , \ldots , l e n _ { m }$ and iterations $i t e r _ { 1 } , \ldots , i t e r _ { m }$ . For each sub-path length len, the initial solution $\tau$ is divided into sub-paths of the designated length. If the length of the last sub-path is shorter than the designated length, it remains unchanged. Each sub-path can be considered as an open-loop TSP with two fixed endpoints. These sub-paths undergo a distribution normalization of vertex coordinates to maintain consistency with the original TSP instance, resulting in normalized sub-paths denoted as $S u b P a t h S e t ^ { ' }$ . Following normalization, a neural solver is employed in batch mode to optimize the normalized subpaths, and sub-paths are updated by the policy when they are better than the current ones, producing a set of subpaths $S u b P a t h S e t ^ { * }$ . These sub-paths are then merged to form an improved solution $\tau ^ { * }$ by connecting their fixed endpoints in their original order. To ensure continuous refinement and overlap during the iterations, the starting point $\kappa$ for sub-path division is incremented by $\begin{array} { r } { \operatorname* { m a x } ( 1 , \frac { l ^ { \smile } } { i t e r } ) } \end{array}$ . This of sub-path length and iteration count, progressively refining the manageable sub-paths of the initial solution $\tau$ to yield a highly optimized solution $\tau ^ { * }$ .

Neural Solver The underlying solver of the path-based divide-and-optimize procedure is an attention-based neural network, which can efficiently solve the obtained sub-paths through batch parallelism.

The neural network uses an encoder-decoder architecture. The encoder employs self-attention layers to embed the input node sequence, while the decoder generates the node sequence auto-regressively. Each sub-path $\pi$ can be regarded as an open-loop TSP with specified start and end nodes. We Park et al. 2021), defined as $h _ { ( c ) } ^ { ( L ) } \ = \ \overline { { { [ \bar { h } } ^ { ( L ) } , h _ { \pi _ { p r e } } ^ { ( L ) } , \bar { h } _ { \pi _ { d e s } } ^ { ( L ) } ] } }$ Here, $h$ denotes a high-dimensional embedding vector from the encoder, and $L$ represents the number of multi-head attention layers. $\overline { { h } } ^ { ( L ) }$ is the mean of the final node embed

# Algorithm 3: Path-based Divide-and-Optimize

Input: Solution $\tau = \{ \tau _ { 1 } , \tau _ { 2 } , . . . , \tau _ { N } \}$ , a set of sub-path lengths $\{ l e n _ { 1 } , \ldots , l e n _ { m } \}$ , and a set of iterations $\{ i t e r _ { 1 } , \ldots , i t e r _ { m } \}$ Output: Optimized Solution $\tau ^ { * }$ 1 for $l e n \gets l e n _ { 1 }$ to $\boldsymbol { l e n } _ { m }$ do 2 $\kappa  1$ ; 3 for $i t e r \gets i t e r _ { 1 }$ to iter $\cdot _ { m }$ do 4 SubP athSet $\begin{array} { c } { { \left\{ \tau _ { \kappa : \kappa + l e n } , \tau _ { \kappa + l e n : \kappa + 2 l e n } , . . . \right\} ; } } \\ { { \left. \varsigma _ { a , b } D _ { a } + b \varsigma _ { o } t ^ { \prime } \begin{array} { r l } \end{array} \right. } } \end{array}$ 5 NormalizeV ertexCoordinate(SubP athSet); 6 SubP athSet∗ $N e u r a l S o l v e r B a t c h ( S u b P a t h S e t ^ { ' } )$ ; 7 $\tau ^ { * } \gets M e r g e S u b P a t h ( S u b P a t h S e t ^ { * } )$ ; 8 $\begin{array} { r } { \kappa  \kappa + m a x ( 1 , \frac { l e n } { i t e r } ) } \end{array}$ ; 9 τ τ ∗

Node Coordinate Normalization To improve the robustness of the model with respect to the sub-paths, we normalize the node coordinates to be uniformly distributed within the range $[ 0 , 1 ]$ $\mathrm { F u }$ , Qiu, and Zha 2021). We define $x _ { m a x } ~ = ~ \operatorname* { m a x } _ { i \in G } x _ { i }$ , $x _ { m i n } ~ = ~ \mathrm { m i n } _ { i \in G } x _ { i }$ , $y _ { m a x } =$ $\operatorname* { m a x } _ { i \in G } y _ { i }$ , $y _ { m i n } = \operatorname* { m i n } _ { i \in G } y _ { i }$ as the maximum and minimum values of the horizontal and vertical coordinates of all $N$ nodes in the TSP instance. For each node in the sub-path, we convert its coordinate $( x _ { i } , y _ { i } )$ to $( x _ { i } ^ { ' } , y _ { i } ^ { ' } )$ as shown in Eq. (3), ensuring that all coordinates fall within the range $[ 0 , 1 ]$ .

$$
\begin{array} { r } { x _ { i } ^ { ' } = \frac { x _ { i } - x _ { \mathrm { m i n } } } { \operatorname* { m a x } ( x _ { \mathrm { m a x } } - x _ { \mathrm { m i n } } , y _ { \mathrm { m a x } } - y _ { \mathrm { m i n } } ) } , } \\ { y _ { i } ^ { ' } = \frac { y _ { i } - y _ { \mathrm { m i n } } } { \operatorname* { m a x } ( x _ { \mathrm { m a x } } - x _ { \mathrm { m i n } } , y _ { \mathrm { m a x } } - y _ { \mathrm { m i n } } ) } . } \end{array}
$$

# Experiments

10 return τ ∗

dings, $h _ { \pi _ { p r e } } ^ { ( L ) }$ is the embedding of the previously selected node, and $h _ { \pi _ { d e s } } ^ { ( L ) }$ is embedding of the end node of the subpath. Moreover, following insights from previous studies (Kim, Park et al. 2021; Cheng et al. 2023; Ye et al. 2024b), we leverage the symmetry of sub-paths. Specifically, reversing the traversal direction of a sub-path (moving backward versus forward) yields an equivalent sequence, which helps improve the network’s robustness and efficiency.

The network employs a single-step constructive policy aimed at optimizing the solution by generating sequences in both forward and backward directions. The policy is defined as follows, where $s$ represents a sub-path with a start node $\pi _ { 1 }$ and an end node $\pi _ { n }$ . The policy $p _ { \theta } ( \pi _ { f } , \pi _ { b } | s )$ is parameterized by $\theta$ and specifies a stochastic policy for constructing the forward solution $\pi _ { f }$ and the backward solution $\pi _ { b }$ :

$$
\begin{array} { l } { { \displaystyle p _ { \theta } ( \pi _ { f } , \pi _ { b } | s ) = p _ { \theta } ( \pi _ { f } | s ) p _ { \theta } ( \pi _ { b } | s ) = } \ ~ } \\ { { \displaystyle \prod _ { t = 1 } ^ { n - 1 } p _ { \theta } ( \pi _ { 1 + t } | s , \pi _ { 1 : t } , \pi _ { n } ) \times \prod _ { t = 1 } ^ { n - 2 } p _ { \theta } ( \pi _ { n - t } | s , \pi _ { n : n - t + 1 } , \pi _ { 1 } ) . } } \end{array}
$$

The model is trained using REINFORCE with a shared baseline. The objective is to minimize the expected length of the solutions to the sub-problems. To achieve this, the loss function is defined as the expected length of these solutions. The policy gradient is then computed as follows:

$$
\begin{array} { r l } & { \nabla \mathcal { L } ( \theta | s ) = \mathrm { E } _ { p _ { \theta } ( \pi _ { f } | s ) } [ R ( \pi _ { f } ) - b ( s ) ] \nabla \log p _ { \theta } ( \pi _ { f } | s ) } \\ & { \quad \quad \quad + \mathrm { E } _ { p _ { \theta } ( \pi _ { b } | s ) } [ R ( \pi _ { b } ) - b ( s ) ] \nabla \log p _ { \theta } ( \pi _ { b } | s ) . } \end{array}
$$

The shared baseline $b ( s )$ is employed to reduce variance and enhance training stability. This baseline is computed by averaging the path lengths obtained from two greedy rollouts. Note that during model training, both forward and backward solutions are utilized. However, during inference, only the superior trajectory between the two solutions is selected to ensure the best possible outcome.

To evaluate the performance of our proposed DualOpt, we conducted extensive experiments on the large-scale TSP instances, with up to 100,000 nodes. We compared the performance of DualOpt with that of state-of-the-art algorithms from the literature. All experiments, including rerunning the baseline algorithms, were executed on a machine with an RTX 3080(10GB) GPU and a 12-core Intel(R) Xeon(R) Platinum 8255C CPU.

Our evaluation focused on two groups of the largescale instances: randomly generated instances TSP random and the well-known real-world benchmark TSPLIB. The TSP random represents random Euclidean instances, with node coordinates uniformly sampled from a unit square $[ 0 , 1 ] ^ { 2 }$ . This group includes seven datasets, each corresponding to a different number of nodes, denoted as $\tt T S P { \cal N }$ , i.e., TSP1K (1,000 nodes), TSP2K, TSP5K, TSP10K, TSP20K, TSP50K, and TSP100K (100,000 nodes). For a fair comparison, we used the same test instances provided by Fu et al. (Fu, Qiu, and Zha 2021) and Pan et al. (Pan et al. 2023). Each dataset contains 16 instances, except TSP1K, which includes 128 instances, and TSP100K, which contains one instance. The TSPLIB is a well-known real-world benchmark (Reinelt 1991) that consists of 100 instances with diverse node distributions derived from practical applications, with sizes ranging from 14 to 85,900 nodes. For our experiments, we select the ten largest TSPLIB instances, each containing more than 5,000 nodes.

Baselines We compare DualOpt against ten leading TSP algorithms in the literature. Traditional Algorithms: 1) Concorde (Cook et al. 2011): One of the best exact solvers. 2) LKH3 (Helsgaun 2017): One of the highly optimized heuristic solvers. Machine Learning Based Algorithms: 1) POMO (Kwon et al. 2020): Feature an end-to-end model based on Attention Model, comparable to LKH3 for smallscale TSP instances. 2) DIMES (Qiu, Sun, and Yang 2022): Introduce a compact continuous space for parameterizing the underlying distribution of candidate solutions for solving large-scale TSP with up to 10K nodes. 3) DIFUSO (?): Leverage a graph-based denoising diffusion model to solve large-scale TSP with up to 10K nodes. 4) SIL (Luo et al. 2024): Develop an efficient self-improved mechanism that enables direct model training on large-scale problem instances without labeled data, handling TSP instances with up to 100K nodes. Divide-and-Conquer Algorithms: 1) $\mathbf { G C N + M C T S }$ (Fu, Qiu, and Zha 2021): Combine a GCN model trained with supervised learning and MCTS to solve large-scale TSP involving up to 10K nodes with a long searching time. 2) SoftDist (Xia et al. 2024): Critically evaluate machine learning guided heatmap generation, the heatmap-guided MCTS paradigm for large-scale TSP. 3) HTSP (Pan et al. 2023): Hierarchically construct a solution of a TSP instance with up to 10K nodes based on two-level policies. 4) GLOP (Ye et al. 2024b): Decompose large routing problems into Shortest Hamiltonian Path Problems, further improved by local policy. It is the first neural solver to effectively scale to TSP with up to 100K nodes.

Parameter Settings For the grid-based divide-andconquer procedure, we set the value of $N _ { i t e r }$ for different TSP problem scales, as shown in Table 1. The LKH3 solver is used with its default parameters as specified in the literature (Helsgaun 2017) both in DualOpt and baseline. LKH3 baseline runs instance-by-instance. In the grid-based divide-and-conquer stage, LKH3 sub-solver just run parallel for grids in a single instance. For the path-based divideand-optimize procedure, we train different neural solvers with graph size $l e n = \{ 5 0 , 2 0 , 1 0 \}$ , with node coordinates randomly generated from a uniform distribution within a unit square. During the training process, we use the same hyper-parameter settings as those used by Kool et al (Kool, van Hoof, and Welling 2019). And during test time, we set $i t e r = \{ 2 5 , 1 0 , 5 \}$ .

Table 1: Parameter setting of $N _ { i t e r }$ .   

<html><body><table><tr><td>Number of Iterations</td><td>Problem Scale of TSP</td></tr><tr><td>Niter=2</td><td>N<5,000</td></tr><tr><td>Niter =3</td><td>5,000≤N<20,000</td></tr><tr><td>Niter =4</td><td>20,000≤N<100,000</td></tr><tr><td>Niter =5</td><td>N ≥100,000</td></tr></table></body></html>

Comparative Studies Table 2 presents a comparison of 10 leading algorithms and our DualOpt algorithm on randomly distributed datasets TSP random. The “Obj.” column shows the average objective lengths of the routes obtained by each algorithm for each instance, while the “Gap” column measures the percentage difference between the average route lengths attained by each algorithm and the LKH3, considered as the ground truth, i.e., $G a p \ =$ Obj. of oAbljg.o.f-LOKbjH3of LKH3 × 100%. The “Time” column indicates the average time required to solve each instance. The $\dagger$ following the algorithm indicates that the results of this algorithm are drawn from the literature directly and “-” means the results are not provided in the literature. The “OOM” stands for out of CUDA memory with our platform.

From Table 2, we can make the following comments about the TSP random instances. First, the heuristic solver

LKH3 provides the highest quality solutions for all instances except TSP1K and TSP2K, though it requires long time searching. DualOpt matches or improves upon LKH3’s results for all instances except TSP5K, and achieves an improved result with an improvement gap up to $1 . 4 0 \%$ for the largest instance TSP100K, with a remarkable $1 0 4 \mathrm { x }$ speedup. Second, DualOpt clearly outperforms machine learning based algorithms in both solution quality and running time. POMO underperforms across all instances, primarily because it cannot be trained directly on large-scale instances, and models trained on small-scale instances fail to generalize effectively to large-scale instances. Compared to the current state-of-the-art SIL, our DualOpt surpasses SIL in all instances except for a tie on TSP1K. Third, DualOpt consistently outperforms all four divide-and-conquer algorithms in terms of the best objective result, achieving better results than GLOP for all instances. These observations highlight the superiority of DualOpt in both solution quality and computational efficiency.

To demonstrate the solver’s generalization capabilities, we then directly applied the trained neural solver to the real-world dataset TSPLIB with varied distributions. Table 3 presents the results for the 10 largest instances from TSPLIB. Despite the different node distributions in TSPLIB compared to those in TSP random, DualOpt demonstrates impressive generalization capabilities. It achieves highly competitive results compared to three leading large-scale TSP algorithms from the literature. This performance highlights DualOpt’s effectiveness in handling various types of TSP instances and further underscores its versatility in realworld applications.

An Ablation Study Our DualOpt integrates two important divide-and-conquer procedures to achieve superior performance. To evaluate the impact of each procedure, we conducted an ablation study with two distinct variants of DualOpt: $\mathrm { D u a l O p t } _ { w / o P a t h }$ and $\mathrm { D u a l O p t } _ { w / o G r i d }$ . $\mathrm { D u a l O p t } _ { w / o P a t h }$ employs only the grid-based divide-andconquer procedure, while $\mathrm { D u a l O p t } _ { w / o G r i d }$ generates the initial solution using a simple yet effective approach called random insertion, which is then improved using the pathbased divide-and-optimize procedure. The results of this ablation study are summarized in Table 4. The comparative results clearly demonstrate the significance of both procedures within DualOpt. Specifically, the original DualOpt consistently outperforms the variants, underscoring the importance of combining both the grid-based and path-based procedures for achieving superior performance.

# Conclusion

In this paper, we introduced a novel dual divide-andoptimize algorithm DualOpt to solve large-scale traveling salesman problem, which integrates two complementary strategies: a grid-based divide-and-conquer procedure and a path-based divide-and-optimize procedure. Through extensive experiments on randomly generated instances and realworld datasets, DualOpt consistently achieves highly competitive results with state-of-the-art TSP algorithms in terms of both solution quality and computational efficiency. Moreover, DualOpt is able to generalize across different TSP distributions, making it a versatile solver for tackling diverse real-world TSP applications. The success of DualOpt paves the way for future research into more sophisticated divide-and-conquer strategies and their application to different types of other combinatorial optimization problems.

Table 2: Comparative results with 10 leading algorithms on large-scale TSP random with up to 100K nodes.   

<html><body><table><tr><td>Algorithm</td><td>Obj.</td><td>TSP1K Gap(%)</td><td>Time</td><td>Obj.</td><td colspan="2">TSP2K Gap(%)</td><td>Time</td><td>Obj.</td><td>TSP5K Gap(%)</td><td></td><td>Time</td></tr><tr><td>LKH3 Concorde</td><td>23.31 23.12</td><td>0.00 -0.82</td><td>2.4s 7m</td><td>32.89 32.44</td><td colspan="2"></td><td>9.3s 2.8h</td><td>51.52 53.45</td><td colspan="2">0.00 3.75</td><td>1.3m 3.2h</td></tr><tr><td>POMOt DIMESt DIFUSCOt</td><td>30.52 23.69 23.39</td><td>30.93 1.63 0.34</td><td>4.3s 2.2m 11.5s</td><td>46.49</td><td></td><td>41.35</td><td>35.9s</td><td>80.79</td><td>56.81</td><td></td><td>9.6m</td></tr><tr><td>SILt GCN-MCTSt</td><td>23.31 23.86</td><td>0.00 2.36</td><td>0.6s 5.8s</td><td>-- 33.42</td><td></td><td>1.61</td><td>1 3.3m</td><td>51.92 52.83</td><td>0.78 2.54</td><td></td><td>28.5s 6.3m</td></tr><tr><td>SOFTDIST† H-TSP GLOP</td><td>23.63 24.66</td><td>1.37 5.79</td><td>1.57 0.7s</td><td></td><td>35.22</td><td>7.08</td><td>= 1.5s</td><td>55.72</td><td>8.15</td><td></td><td>2.3s</td></tr><tr><td>DualOpt</td><td>24.01 23.31</td><td>3.00 0.00</td><td>0.4s 5.6s</td><td></td><td>33.90 32.72</td><td>3.07 -0.52</td><td>0.9s 7.6s</td><td>53.49 51.56</td><td></td><td>3.82 0.08</td><td>1.8s 13.9s</td></tr><tr><td></td><td></td><td>TSP10K</td><td></td><td></td><td>TSP20K</td><td></td><td></td><td>TSP50K</td><td></td><td>TSP100K</td><td></td></tr><tr><td>Method</td><td>Obj.</td><td>Gap(%)</td><td>Time</td><td>Obj.</td><td>Gap(%)</td><td>Time</td><td>Obj.</td><td>Gap(%) Time</td><td>Obj.</td><td>Gap(%)</td><td>Time</td></tr><tr><td>LKH3</td><td>72.96</td><td>0.00</td><td>6.3m</td><td>103.28</td><td>0.00</td><td>27.4m</td><td>164.08</td><td>0.00 3.0h</td><td></td><td>234.098 0.00</td><td>14.9h</td></tr><tr><td>DIMESt</td><td>74.06</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DIFUSCOt</td><td></td><td>1.50</td><td>13m</td><td></td><td></td><td></td><td></td><td>-</td><td>1</td><td>1</td><td>1</td></tr><tr><td>SIL+</td><td>73.62</td><td>0.90</td><td>3m</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>73.32</td><td>0.49</td><td>51s</td><td></td><td></td><td></td><td>164.53 0.27</td><td>3.8m</td><td>232.66</td><td>-0.61</td><td>7.5m</td></tr><tr><td>GCN-</td><td>74.93</td><td>2.70</td><td>6.5m</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MCTS†</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>SOFTDIST+</td><td>74.03</td><td>1.40</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>H-TSP</td><td>78.45</td><td>7.52</td><td>1m</td><td>110.7</td><td>7.18</td><td>10.4s</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GLOP</td><td>75.42</td><td></td><td>4.8s</td><td>106.7</td><td>3.31</td><td></td><td>OOM 168.2</td><td></td><td></td><td>OOM</td><td></td></tr><tr><td></td><td></td><td>3.37</td><td>3.5s</td><td></td><td></td><td>7.9s</td><td>2.51</td><td>12.1s</td><td>237.99</td><td>1.66</td><td>2.5m</td></tr><tr><td>DualOpt</td><td></td><td></td><td></td><td>102.9</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>72.62</td><td>-0.47</td><td>33.9s</td><td></td><td>-0.37</td><td></td><td>162.81 -0.77</td><td>6.5m</td><td>230.83</td><td>-1.40</td><td>8.6m</td></tr></table></body></html>

Table 3: Comparative results on 10 largest instances of TSPLIB.   

<html><body><table><tr><td rowspan="2">Instance</td><td colspan="2">LKH3</td><td colspan="3">H-TSP</td><td colspan="3">GLOP</td><td colspan="3">DualOpt</td></tr><tr><td>Obj.</td><td>Time</td><td>Obj.</td><td>Gap(%)</td><td>Time</td><td>Obj.</td><td>Gap(%)</td><td>Time</td><td>Obj.</td><td>Gap(%)</td><td>Time</td></tr><tr><td>rl5915</td><td>572085</td><td>9.9m</td><td>652336</td><td>14.03</td><td>9s</td><td>628928</td><td>9.94</td><td>16s</td><td>579152</td><td>1.24</td><td>14s</td></tr><tr><td>rl5934</td><td>559712</td><td>9.9m</td><td>642214</td><td>14.74</td><td>9s</td><td>616629</td><td>10.17</td><td>16s</td><td>565912</td><td>1.11</td><td>19s</td></tr><tr><td>pla7397</td><td>23382264</td><td>12.4m</td><td>25494130</td><td>9.03</td><td>12s</td><td>24990688</td><td>6.88</td><td>16s</td><td>23536550</td><td>0.66</td><td>41s</td></tr><tr><td>rl11849</td><td>929001</td><td>18.9m</td><td>1046963</td><td>11.2</td><td>17s</td><td>1006378</td><td>8.3</td><td>17s</td><td>935904</td><td>0.74</td><td>49s</td></tr><tr><td>usa13509</td><td>20133724</td><td>22.6m</td><td>21923532</td><td>8.89</td><td>20s</td><td>21023604</td><td>4.42</td><td>17s</td><td>20248476</td><td>0.57</td><td>1.3m</td></tr><tr><td>brd14051</td><td>474149</td><td>23.5m</td><td>506211</td><td>6.76</td><td>20s</td><td>491735</td><td>3.71</td><td>18s</td><td>474559</td><td>0.09</td><td>1m</td></tr><tr><td>d15112</td><td>1588550</td><td>25.2m</td><td>1696577</td><td>6.80</td><td>22s</td><td>1648777</td><td>3.79</td><td>18s</td><td>1589033</td><td>0.03</td><td>1.2m</td></tr><tr><td>d18512</td><td>652911</td><td>31m</td><td>694116</td><td>6.31</td><td>26s</td><td>676840</td><td>3.66</td><td>21s</td><td>652457</td><td>0.07</td><td>1.3m</td></tr><tr><td>pla33810</td><td>67084217</td><td>56.4m</td><td></td><td>00M</td><td></td><td>71934504</td><td>7.23</td><td>33s</td><td>68083048</td><td>1.49</td><td>1.2m</td></tr><tr><td>pla85900</td><td>144004484</td><td>6.5h</td><td></td><td>0OM</td><td></td><td>146039168</td><td>1.41</td><td>1.7m</td><td>145008800</td><td>0.70</td><td>3.5m</td></tr></table></body></html>

Table 4: An ablation study of DualOpt and its two variants on TSP random   

<html><body><table><tr><td></td><td></td><td colspan="2">DualOpt</td><td colspan="3">DualOptwloPath</td><td colspan="3">DualOptw /oGrid</td></tr><tr><td>Instance</td><td>Obj.</td><td>Gap(%)</td><td>Time</td><td>Obj.</td><td>Gap(%)</td><td>Time</td><td>Obj.</td><td>Gap(%)</td><td>Time</td></tr><tr><td>TSP1K</td><td>23.31</td><td>0.00</td><td>5.6s</td><td>23.34</td><td>0.13</td><td>4.5s</td><td>24.56</td><td>5.36</td><td>5.2s</td></tr><tr><td>TSP2K</td><td>32.72</td><td>-0.52</td><td>7.6s</td><td>32.75</td><td>-0.43</td><td>6.3s</td><td>34.3</td><td>4.29</td><td>5.3s</td></tr><tr><td>TSP5K</td><td>51.56</td><td>0.08</td><td>13.9s</td><td>51.65</td><td>0.25</td><td>12.1s</td><td>54.03</td><td>4.87</td><td>7.2s</td></tr><tr><td>TSP10K</td><td>72.62</td><td>-0.47</td><td>33.9s</td><td>72.85</td><td>-0.15</td><td>31.7s</td><td>76.25</td><td>4.51</td><td>8.2s</td></tr><tr><td>TSP20K</td><td>102.9</td><td>-0.37</td><td>1m</td><td>103.4</td><td>0.12</td><td>59s</td><td>107.76</td><td>4.34</td><td>8.7s</td></tr><tr><td>TSP50K</td><td>162.81</td><td>-0.77</td><td>6.5m</td><td>164</td><td>-0.05</td><td>6.4m</td><td>170.24</td><td>3.75</td><td>31.3s</td></tr><tr><td>TSP100K</td><td>230.83</td><td>-1.40</td><td>8.6m</td><td>234.2</td><td>0.04</td><td>8.5m</td><td>240.82</td><td>2.87</td><td>2.5m</td></tr></table></body></html>