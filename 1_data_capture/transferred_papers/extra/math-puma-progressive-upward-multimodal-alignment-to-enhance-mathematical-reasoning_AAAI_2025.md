# Math-PUMA: Progressive Upward Multimodal Alignment to Enhance Mathematical Reasoning

Wenwen Zhuang1\*, Xin Huang2\*, Xiantao Zhang3\*, Jin Zeng

1University of Chinese Academy of Sciences 2Beijing Institute of Technology 3Beijing University of Aeronautics and Astronautics zhuangwenwen23@mails.ucas.ac.cn, huangxin $@$ bit.edu.cn, zhangxiantao $@$ buaa.edu.cn, zengjing2she327@gmail.com

# Abstract

Multimodal Large Language Models (MLLMs) excel in solving text-based mathematical problems, but they struggle with mathematical diagrams since they are primarily trained on natural scene images. For humans, visual aids generally enhance problem-solving, but MLLMs perform worse as information shifts from textual to visual modality. This decline is mainly due to their shortcomings in aligning images and text. To tackle aforementioned challenges, we propose Math-PUMA, a methodology focused on Progressive Upward Multimodal Alignment. This approach is designed to improve the mathematical reasoning skills of MLLMs through a three-stage training process, with the second stage being the critical alignment stage. We first enhance the language model‚Äôs mathematical reasoning capabilities with extensive set of textual mathematical problems. We then construct a multimodal dataset with varying degrees of textual and visual information, creating data pairs by presenting each problem in at least two forms. By leveraging the Kullback-Leibler (KL) divergence of next-token prediction distributions to align visual and textual modalities, consistent problem-solving abilities are ensured. Finally, we utilize multimodal instruction tuning for MLLMs with high-quality multimodal data. Experimental results on multiple mathematical reasoning benchmarks demonstrate that the MLLMs trained with Math-PUMA surpass most open-source MLLMs. Our approach effectively narrows the performance gap for problems presented in different modalities.

Code ‚Äî https://github.com/wwzhuang01/Math-PUMA

# Introduction

Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities, particularly when tackling mathematical problems in textual form (Wei et al. 2022; Chen et al. 2022; Gou et al. 2023; Yu et al. 2023; Shao et al. 2024). However, Multimodal Large Language Models (MLLMs) face greater challenges when tackling problems that involve images. These models need to not only interpret textual information but also comprehend mathematical diagrams and identify details crucial for solving problems. Although MLLMs have exhibited notable efficacy in general visual question answering (Radford et al. 2021; Li et al.

![](images/9870c00857e3e89c33d794328963eb25a4ad1b073c44a6255d90c712e5cb9f41.jpg)  
Figure 1: (Top) Three examples of GPT-4o solving multimodal math problems. These examples represent different modalities of the same question. (Bottom) Results of several open-source MLLMs and human on five different tasks of MATHVERSE (Zhang et al. 2024a).

2022; Liu et al. 2023), their training predominantly relies on datasets comprising natural scene images. This reliance engenders a substantial domain discrepancy when these models are applied to mathematical diagrams, thereby resulting in inferior performance.

For humans, regardless of the modality in which information is presented, problems with equivalent amounts of information tend to have similar levels of difficulty. Furthermore, incorporating images into problem-solving tasks can enhance human comprehension and resolution abilities. As illustrated in Figure 1, an increase in visual data often correlates with a decline in the efficacy of most MLLMs. Additionally, there is a notable disparity in effectiveness between text-centric and exclusively visual problems. For example, GPT-4o (OpenAI 2024b) demonstrates strong proficiency in solving text-only mathematical problems, but its effectiveness diminishes progressively as the modality transitions from textual to visual. This reduction in capability primarily stems from the current models‚Äô inadequate alignment between visual and textual data, which impairs their overall functionality.

To address this issue, we propose Math-PUMA, a methodology centered around Progressive Upward Multimodal Alignment (PUMA), aimed at enhancing the mathematical reasoning capabilities of MLLMs. Our approach is structured into three distinct stages, with stage 2 serving as the pivotal alignment phase. (1) Stage 1: We train the LLM using a substantial dataset of text-based math problems to enhance its problem-solving capabilities. This phase capitalizes on the extensive availability of text-based math problem-solving data. (2) Stage 2: It is observed that the model‚Äôs mathematical problem-solving ability diminished progressively from text to vision, exhibiting an upward pyramidal structure. Consequently, the model‚Äôs capabilities are categorized into four hierarchical levels. We construct 692K data pairs, with each pair conveying identical information but differing in multimodal representation. By leveraging the KL divergence between next-token prediction distributions for text-rich and vision-rich problems, we achieve progressive bottom-up modal alignment across these hierarchical levels, thereby enhancing the model‚Äôs ability to tackle multimodal mathematical problems. (3) Stage 3: We select 996K high-quality multimodal problem-solving data to fine-tune the model, further enhancing its performance in multimodal mathematical problem-solving tasks.

The contributions of this paper are three-fold:

‚Ä¢ We curate a large-scale dataset, Math-PUMA-1M, which comprises 692K data pairs and 996K multimodal mathematical data. This dataset serves as a valuable resource for model training.   
‚Ä¢ We propose Math-PUMA, a methodology based on Progressive Upward Multimodal Alignment, which enhances mathematical reasoning in MLLMs through a three-stage process.   
‚Ä¢ Experimental results on three widely-used benchmarks demonstrate that the MLLMs trained with Math-PUMA outperform most open-source models. Notably, our approach effectively narrows the performance gap for problems that contain the same information but are presented in different modalities, as evidenced by results on MATHVERSE.

# Related Work

# Multimodal Large Language Models

The exploration of Multimodal Large Language Models (MLLMs) has been inspired by advancements in Large Language Models (LLMs), resulting in remarkable capabilities across a variety of tasks that require both visual and linguistic understanding. CLIP (Radford et al. 2021) is a breakthrough model that learns transferable visual representations from natural language supervision. LLaVA series (Liu et al. 2023, 2024a) pioneer visual instruction tuning for LLMs, employing a simple MLP as a projector to connect the vision encoder with the language model. Models such as Qwen-VL (Bai et al. 2023) and Deepseek-VL (Lu et al. 2024a) introduce a new visual receptor or a hybrid vision encoder, significantly enhancing their ability to perceive and understand visual inputs. However, despite these significant strides, MLLMs still face considerable challenges, particularly in multimodal mathematical reasoning. This is primarily due to the substantial domain gap between the natural scene image and the abstract mathematical graphics. There is a pressing need to enhance the understanding and reasoning abilities of MLLMs in relation to mathematical diagrams.

# Multimodal Mathematical Reasoning

The advancement of MLLMs has driven significant research into multimodal reasoning. Current efforts are primarily centered on data augmentation to improve models‚Äô performance. Significant efforts have been invested in augmenting text-only mathematical problem-solving data to enhance LLMs‚Äô reasoning capabilities (Saxton et al. 2019; Yu et al. 2023; Liu and Yao 2024). G-LLaVA (Gao et al. 2023a) and Math-LLaVA (Shi et al. 2024) improve multimodal mathematical reasoning by constructing the Geo170K and MathV360K datasets, respectively. These are created by generating additional questions for images sourced from public datasets. However, they only serve to expand the text, without increasing the diversity of images in the dataset. GeoGPT4V (Cai et al. 2024) leverages GPT-4V (OpenAI 2023b) to generate new problems and images based on existing datasets, creating a dataset of 4.9K geometric problems combined with 19K open-source data. Nevertheless, due to GPT-4V‚Äôs subpar capability in generating code from image descriptions, the quality of the generated data is comparatively inferior. By comparison, our work not only makes new advancements in data augmentation, including text rephrasing and the generation of high-quality images, but also introduces a novel alignment method used for training.

# Methodology

# Data Construction

In order to refine alignment between visual and textual modalities, we need to construct data pairs. We clarify a ‚Äúdata pair‚Äù as a set of two data components, which share an equivalent level of information within each problem context, and their solutions are identical. A ‚Äúdata pair‚Äù is defined as two data components that contain equivalent information within the same problem context, and their solutions are identical. However, the distribution of information across different modalities may vary within each pair. We use the term ‚Äúvision-rich‚Äù to describe the data component where the visual modality has a higher proportion of information, whereas ‚Äútext-rich‚Äù refers to the component with a higher proportion of textual information.

The methods we employ to construct data pairs include automatic data generation and data augmentation based on publicly available sources.

Automatic Data Generation We implement an automatic data generation pipeline for three categories of mathemat

Visual Modality Textual Modality Plotter Question Designer AÂÖ® Textual info. Visual info. Find ùêµùê∂. ‚àÜùê¥ùêµC, ùê¥ùêµ=2, ÂõΩA ùê¥ùê∂=1, ‚à†ùê¥=60¬∞ V   
oo 0o Calculate A cylinder the volume. with h=3, r=1. Solver Determine A linear function Question Rationale the function. through (0,1), re-solve (2,4). Answer Extracted verify answer   
Ë°Ä Pair Constructor Vision-rich Text-rich Vision-only Vision-dom. Text-dom. Text-only [NONE] Find area of Find the area Find the area Find area of ‚àÜABC. of isosceles of isosceles ‚àÜABC. triangle ABC. triangle ABC with altitude AD=4 and BD=2. [NONE] PARED Random construct a text-rich/vision-rich data pair

ical problems: plane geometry, solid geometry, and functions. The pipeline consists of four agents: (1) Question Designer, responsible for formulating problems and assigning information to visual and textual modalities; (2) Plotter, which generates diagrams; (3) Solver, which provides answers and explanations; and (4) Pair Constructor, which produces four types of data and randomly selects two to form a data pair. Figure 2 illustrates this automatic data generation process.

detailed explanation for each problem by calling GPT4o-mini (OpenAI 2024a) and verifying the explanations against the standard answer to ensure accuracy.

‚Ä¢ Pair Construction: The Pair Constructor combines the diagram from the Plotter and the text from the Solver to obtain up to four types of data, each comprising the same information but presented in a different modality: visiononly, vision-dominant, text-dominant, and text-only. Two of these are randomly selected to form a data pair, with the component containing more visual information classified as vision-rich and the other as text-rich.

‚Ä¢ Question Design: The Question Designer employs a random selection process to determine the specific type of mathematical problem to be generated. It also randomly selects the information carrier, deciding whether to present the information as text or an image. This choice dictates the visual information sent to the Plotter and the textual information sent to the Solver. ‚Ä¢ Plotting: In accordance with the visual information received, the Plotter uses the predefined fundamental tools to plot diagrams. ‚Ä¢ Problem Solving: The Solver calculates the answer using the text-only version of the problem, which contains complete information. As the calculation is performed programmatically, the answer is reliable. Considering that MLLMs can obtain stronger reasoning abilities from step-by-step solutions, the Solver generates a

We generated 40K data each for plane geometry, solid geometry, and functions, summing up to 120K.

Data Augmentation We initially collect 80K mathematical problem-solving data from online sources. By rephrasing the problems from multiple perspectives (Yu et al. 2023) and applying a series of traditional image processing techniques such as scaling, stretching, and gamma transformation, we expand the dataset to 310K. Additionally, we utilize the VisualWebInstruct dataset (TIGER-Lab 2024) containing 262K data. To automate the construction of data pairs, we employ a straightforward text-to-image rendering process to convert the content from textual to visual form. The original data serve as the text-rich component, while the generated data form the vision-rich component. In total, we obtain 572K data pairs.

# Training Stages

We employ a three-stage pipeline to train our models, with specific details shown in Figure 3.

Stage 1: Enhancing the Language Model‚Äôs Mathematical Reasoning Abilities Given the abundance of unsupervised text-based mathematical training corpora and problem-solving data (Shao et al. 2024), in comparison to the scarcity of high-quality multimodal mathematical problem-solving data, we initially train the LLM on a large corpus of text-based math problems to bolster its mathematical reasoning capabilities. To leverage the strengths of existing LLMs that have demonstrated superior performance in mathematical problem-solving (Shao et al. 2024; Yang et al. 2024), we use them to initialize our MLLMs. Subsequently, we fine-tune the model using 200K data extracted from various datasets (Yue et al. 2023; Tong et al. 2024; Mitra et al. 2024; LI et al. 2024). This phase significantly enhances the LLM‚Äôs mathematical reasoning abilities.

Stage 2: Progressive Upward Multimodal Alignment We observe that the multimodal mathematical reasoning ability of MLLMs resembles a pyramid, with performance declining from bottom to top as the information shifts from text to visual modalities. In order to address the discrepancy in performance between text-rich and vision-rich mathematical reasoning, we propose PUMA (Progressive Upward Multimodal Alignment). The objective of PUMA is to facilitate the effective resolution of vision-rich mathematical problems by aligning the outputs of MLLMs with text-rich data, thereby enhancing their reasoning capabilities across different modalities.

![](images/ced886ac173ecbe89a409083ddfd09db93e9ddca5f8a3d409d4fcd65347881b0.jpg)  
Figure 3: Overview of the Math-PUMA approach. (Left) The three stage training process of Math-PUMA. (Right) The details for aligning data pair. The input data pair includes text-rich data at the strong level and vision-rich data at the weak level, simultaneously processed by the MLLM. The strong logits and labels are used to supervise the weak logits.

Let $i = { 0 , 1 , 2 , 3 }$ represents the levels of capability for MLLMs, ranging from weak to strong (top-down). For a visual mathematical problem, the inference results of MLLMs are progressively inferior on the $i$ -th level compared to the $( i + 1 )$ -th level. We denote the response distribution (logits) obtained by MLLMs when processing the input of $i$ -th level as $p _ { i }$ , while the response distribution (logits) obtained on the input of $( i + 1 )$ -th level is denoted as $p _ { i + 1 }$ . The forward KL (FKL) divergence and reverse KL (RKL) divergence between these distributions are calculated, since they converge to the same objective after a sufficient number of epochs for MLLMs (Wu et al. 2024).

Let $\begin{array} { r c l } { \mathbf { y } ^ { ( i ) } } & { = } & { \{ y _ { t } ^ { ( i ) } \} _ { t = 1 } ^ { T } } \end{array}$ represent the response generated by MLLMs based on input $\mathbf { x } ^ { ( i ) }$ . Here, $y _ { t } ^ { ( i ) } \in$ $\{ Y _ { 1 } ^ { ( i ) } , Y _ { 2 } ^ { ( i ) } , . . . , Y _ { V } ^ { ( i ) } \}$ , with $V$ representing the vocabulary size. $p _ { i }$ and $p _ { i + 1 }$ represent the distributions of weak and strong levels, z(i) $\begin{array} { r c l } { \mathbf { z } ^ { ( i ) } } & { = } & { ( z _ { 1 } ^ { ( i ) } , z _ { 2 } ^ { ( i ) } , . . . , z _ { V } ^ { ( i ) } ) } \end{array}$ and $\begin{array} { r l } { \mathbf { z } ^ { ( i + 1 ) } } & { { } = } \end{array}$ $( z _ { 1 } ^ { ( i + 1 ) } , z _ { 2 } ^ { ( i + 1 ) } , . . . , z _ { V } ^ { ( i + 1 ) } )$   
strong levels, respectively. The FKL divergence and RKL divergence are computed as follows:

$$
\begin{array} { r l } & { \mathcal { L } _ { \mathrm { F K L } } = \displaystyle \frac { 1 } { T V } \sum _ { t = 1 } ^ { T } \mathrm { K L } \Big ( p _ { i } ( \boldsymbol { y } _ { t } ^ { ( i ) } | \mathbf { y } _ { < t } ^ { ( i ) } ) | | p _ { i + 1 } ( \boldsymbol { y } _ { t } ^ { ( i + 1 ) } | \mathbf { y } _ { < t } ^ { ( i + 1 ) } ) \Big ) } \\ & { \qquad = \displaystyle \frac { 1 } { T V } \sum _ { t = 1 } ^ { T } \sum _ { j = 1 } ^ { V } p _ { i } ( Y _ { j } ^ { ( i ) } | \mathbf { y } _ { < t } ^ { ( i ) } ) \log \frac { p _ { i } ( Y _ { j } ^ { ( i ) } | \mathbf { y } _ { < t } ^ { ( i ) } ) } { p _ { i + 1 } ( Y _ { j } ^ { ( i + 1 ) } | \mathbf { y } _ { < t } ^ { ( i + 1 ) } ) } , } \end{array}
$$

$$
\begin{array} { r l } & { \mathcal { L } _ { \mathrm { R K L } } = \displaystyle \frac { 1 } { T V } \sum _ { t = 1 } ^ { T } \mathrm { K L } \Big ( p _ { i + 1 } ( y _ { t } ^ { ( i + 1 ) } | \mathbf { y } _ { < t } ^ { ( i + 1 ) } ) | | p _ { i } ( y _ { t } ^ { ( i ) } | \mathbf { y } _ { < t } ^ { ( i ) } ) \Big ) } \\ & { \quad \quad \quad = \displaystyle \frac { 1 } { T V } \sum _ { t = 1 } ^ { T } \sum _ { j = 1 } ^ { V } p _ { i + 1 } ( Y _ { j } ^ { ( i + 1 ) } | \mathbf { y } _ { < t } ^ { ( i + 1 ) } ) \log \frac { p _ { i + 1 } ( Y _ { j } ^ { ( i + 1 ) } | \mathbf { y } _ { < t } ^ { ( i + 1 ) } ) } { p _ { i } ( Y _ { j } ^ { ( i ) } | \mathbf { y } _ { < t } ^ { ( i ) } ) } , } \end{array}
$$

with

$$
p _ { i } ( Y _ { j } ^ { ( i ) } | \mathbf { y } _ { < t } ^ { ( i ) } ) = \frac { \exp { ( z _ { j } ^ { ( i ) } / \tau ) } } { \sum _ { k = 1 } ^ { V } \exp ( z _ { k } ^ { ( i ) } / \tau ) } ,
$$

where $\tau$ represents the temperature hyperparameter.

Furthermore, to maintain training stability, we calculate a hard loss by utilizing the solutions of mathematical problems as the ground truth labels, i.e.,

$$
\begin{array} { l } { { \displaystyle { \mathcal { L } } _ { \mathrm { h a r d } } = - \frac { 1 } { T V } \sum _ { t = 1 } ^ { T } \log p _ { i } ( y _ { t } ^ { ( i ) } | \mathbf { x } ^ { ( i ) } , \mathbf { y } _ { < t } ^ { ( i ) } ) } \ ~ } \\ { { \displaystyle ~ = - \frac { 1 } { T V } \sum _ { t = 1 } ^ { T } \sum _ { j = 1 } ^ { V } \log p _ { i } ( Y _ { j } ^ { ( i ) } | \mathbf { x } ^ { ( i ) } , \mathbf { y } _ { < t } ^ { ( i ) } ) } . } \end{array}
$$

Finally, the total loss is computed as

$$
\begin{array} { r } { \mathcal { L } = \lambda _ { \mathrm { K L } } ( \alpha _ { \mathrm { K L } } \mathcal { L } _ { \mathrm { F K L } } + ( 1 - \alpha _ { \mathrm { K L } } ) \mathcal { L } _ { \mathrm { R K L } } ) \tau ^ { 2 } + ( 1 - \lambda _ { \mathrm { K L } } ) \mathcal { L } _ { \mathrm { h a r d } } , } \end{array}
$$

where $\lambda _ { \mathrm { K L } }$ is a hyperparameter that balances the weight between the combined FKL and RKL divergences and the hard loss term, $\alpha _ { \mathrm { K L } }$ is a weight hyperparameter that balances the contribution between $\mathcal { L } _ { \mathrm { F K L } }$ and $\mathcal { L } _ { \mathrm { R K L } }$ . The purpose of multiplying $\mathrm { K L }$ by $\tau ^ { 2 }$ is to equalize the gradients of the two losses.

At this stage, we use a total of 692K data pairs for training, which includes 120K data pairs automatically generated and 572K data pairs obtained through data augmentation based on publicly available data as described in Data Construction.

Stage 3: Multimodal Instruction Tuning In the final phase, we enhance the model‚Äôs reasoning capabilities by incorporating multimodal problem-solving data. Initially, we retain the majority of the high-quality data used in Stage 2 and augment our dataset with the MathV360K dataset (Shi et al. 2024). Specifically, we focus on enriching the geometric problem subset within MathV360K, expanding it from 40K to 120K in order to address the scarcity of geometric data. Furthermore, as referenced in (Lu et al. 2024a), we incorporate a balanced amount of textual data to mitigate potential modality imbalances and enhance the model‚Äôs overall performance. All data include detailed reasoning processes to guide the model‚Äôs understanding and learning.

Ultimately, we compile a large-scale instruction tuning dataset, comprising a total of 996K data. This multimodal instruction tuning not only bolsters the model‚Äôs reasoning and problem-solving abilities but also ensures that it can effectively leverage both textual and visual information for improved performance in mathematical problem-solving.

# Experiments

# Experimental Setup

Models We validate the effectiveness of our method across various base models and scales, including DeepSeek-Math7B (Shao et al. 2024), Qwen2-1.5B and Qwen2-7B (Yang et al. 2024), chosen as the LLM for Math-PUMA. To ensure the compatibility with DeepSeek-Math and DeepSeekVL (Lu et al. 2024a), we adhere to the architecture of DeepSeek-VL. For Qwen2, we adopt a similar architecture to LLaVA, with the visual encoder designated as SigLIP$\mathrm { s o 4 0 0 m }$ -patch14-384 (Zhai et al. 2023).

Benchmarks We conduct extensive experiments on three popular multimodal mathematical problem-solving benchmarks: MATHVERSE (Zhang et al. 2024a), MATHVISTA (Lu et al. 2024b), and WE-MATH (Qiao et al. 2024). MATHVERSE evaluates the multimodal mathematical reasoning abilities of MLLMs under five different conditions. MATHVISTA comprises samples that require fine-grained, in-depth visual understanding and compositional reasoning, posing a challenge for all baseline models on this benchmark. WEMATH is the first benchmark specifically designed to explore the problem-solving principles beyond the end-to-end performance.

Evaluation and Metrics We refer to the leaderboards and adopt the official implementations of MATHVERSE, MATHVISTA, and WE-MATH. For MATHVERSE and MATHVISTA, initially, we use GPT-4o-mini (OpenAI 2024a) to extract answers from the responses generated by MLLMs. Subsequently, we employ GPT-4o-mini once more to verify the correctness of the extracted answers. The prompts used for answer extraction and correctness assessment are kept consistent with the official implementation. Ultimately, we calculate the accuracy scores as the evaluation metric. For WE-MATH, we select the average and Rote Memorization (RM) scores as evaluation metrics.

Implementation Details Our experiments are conducted using PyTorch version 2.1.0 and CUDA 12.1, utilizing 32 NVIDIA A100 GPUs with 80GB memory each. The training process is divided into three stages, each with specific hyperparameters and configurations. We employ the AdamW optimizer (Kingma and Ba 2014), configured with $\beta _ { 1 } = 0 . 9$ and $\beta _ { 2 } = 0 . 9 9 9$ . The learning rate is adjusted across three stages: $3 \times 1 0 ^ { - 5 }$ for stage 1, $5 \times 1 0 ^ { - 5 }$ for stage 2, and $3 \bar { \times } 1 0 ^ { - 5 }$ for stage 3. A cosine learning rate schedule is implemented with a warm-up phase covering $2 \%$ of the total training steps. Additionally, a decay rate of 0.1 is applied. The KL divergence is controlled using specific hyperparameters: $\alpha _ { \mathrm { K L } }$ is set to 0.2, $\tau$ to 1.0, and $\lambda _ { \mathrm { K L } }$ to 0.1. The training is conducted over 1 epoch. The batch sizes for three stages are 256, 512, and 256, respectively.

# Performance Comparison

Comparison on MATHVERSE MATHVERSE is capable of clearly demonstrating the gap between visual and textual modalities. From Table 1, it can be observed that the MLLMs trained by Math-PUMA achieve the state-of-theart (SOTA) among open-source MLLMs. Compared to the previous SOTA method, MAVIS, the MLLMs trained by Math-PUMA exhibit accuracy scores improvement about $6 \%$ . When compared to the closed-source GPT-4V (OpenAI 2023b), Math-PUMA-Qwen2-7B performs competitively with only a gap of $4 . 7 \%$ , demonstrating the effectiveness of Math-PUMA.

Comparison on MATHVISTA MATHVISTA is a comprehensive benchmark designed to evaluate mathematical reasoning. According to the results presented in Table 1, Math-PUMA-Qwen2-7B demonstrates SOTA performance in GPS, ALG, GEO and SCI domains among opensource MLLMs of the same scale. It outperforms InternLMXComposer2-VL (Dong et al. 2024) by significant margins, with accuracy improvements of $1 6 . 4 \%$ , $1 5 . 7 \%$ , $1 6 . 8 \%$ , and $4 . 9 \%$ in these respective domains.

Comparison on WE-MATH WE-MATH places strong emphasis on the importance of the mathematical reasoning process. Table 2 demonstrates that Math-PUMAQwen2-7B achieves SOTA performance in average scores among open-source MLLMs with approximate 10B parameters, surpassing InternLM-XComposer2-VL. Notably, even among open-source MLLMs with parameters exceeding 20B, Math-PUMA-Qwen2-7B outperforms LLaVA-NeXT (Liu et al. 2024b) 72B model, reaching the performance of LLaVA-NeXT 110B model. While Math-PUMA-Qwen2- 7B surpasses Qwen-VL-Max (Bai et al. 2023) among closed-source models, there remains a significant gap compared to GPT-4V and GPT-4o.

# Ablation Study

Ablation studies are performed on MATHVERSE to highlight the contribution of each training stage and to assess the impact of their sequential order on Math-PUMA.

Table 1: Mathematical evaluation on MATHVERSE and MATHVISTA testmini sets. For MATHVERSE, we calculate the ‚ÄúALL‚Äù score without averaging the ‚ÄúText-only‚Äù version. For MATHVISTA, we select 4 mathematical categories from the original 12 categories. ALL: overall accuracy across original categories; GPS: geometry problem solving; ALG: algebraic reasoning; GEO: geometry reasoning; SCI: scientific reasoning. For closed-source and open-source MLLMs, the best accuracy scores are marked in bold fonts, while the second best accuracy scores are marked in underline fonts, respectively.   

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2"># Params.</td><td colspan="6">MATHVERSE</td><td colspan="5">MATHVISTA</td></tr><tr><td></td><td>dm</td><td>Tex- Visi.on-</td><td></td><td>Visio-</td><td>Vision-</td><td>ALL‚Üë</td><td>GPS‚Üë</td><td>ALG ‚Üë</td><td>GEO ‚Üë</td><td>SCI ‚Üë</td></tr><tr><td colspan="10">Baselines</td><td></td><td></td><td></td></tr><tr><td>Random chaormance</td><td></td><td>124</td><td></td><td>12.4</td><td></td><td></td><td>12.4</td><td></td><td></td><td>217</td><td>204</td><td>17.2</td></tr><tr><td></td><td>--</td><td></td><td>12.4</td><td></td><td>124</td><td>124</td><td></td><td>17</td><td>21.6</td><td></td><td></td><td></td></tr><tr><td colspan="10">Closed-source LLMs</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td>333</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>CPT-4PT (enA202tal. 2022)</td><td></td><td></td><td></td><td>18.9</td><td></td><td></td><td></td><td>332</td><td>39.3</td><td>310</td><td>310</td><td>50.8</td></tr><tr><td colspan="10">Closed-source MLLMs</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td>23.17</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Qwin-L-Promialm2)</td><td></td><td>211.8</td><td>15.7</td><td></td><td>94</td><td>130</td><td>10.0</td><td>43.</td><td>385</td><td>3.1</td><td>40</td><td>590</td></tr><tr><td>Qwen-VL-Max (Bai et al.2023)</td><td></td><td>24.8</td><td>30.3</td><td>24.8</td><td>20.6</td><td>23.3</td><td>25.1</td><td></td><td>1</td><td>-</td><td></td><td></td></tr><tr><td>GPT-4V (OpenAI 2023b)</td><td></td><td>38.3</td><td>52.1</td><td>40.9</td><td>34.9</td><td>33.6</td><td>29.8</td><td>49.9</td><td>50.5</td><td>53.0</td><td>51.0</td><td>63.1</td></tr><tr><td colspan="10">Open-source MLLMs</td><td colspan="3"></td></tr><tr><td>mPLUG-Owl2 (Ye et al. 2024) LLaMA-Adapter-V2 (Gao etal.2023b)</td><td>7B</td><td>4.6</td><td>6.6</td><td>6.3</td><td>6.3</td><td>5.6</td><td>4.9</td><td>22.2</td><td>23.6</td><td>23.6</td><td>23.9</td><td>26.3</td></tr><tr><td>LLaVA-1.5 (Liu et al. 2024a)</td><td>7B 13B</td><td>5.7</td><td>6.2</td><td>5.9</td><td>6.1</td><td>4.2</td><td>6.1</td><td>23.9</td><td>25.5</td><td>26.3</td><td>24.3</td><td>29.5</td></tr><tr><td></td><td>8B</td><td>7.6</td><td>8.8</td><td>7.6</td><td>7.4</td><td>7.4</td><td>6.9</td><td>25.7</td><td>18.3</td><td>19.6</td><td>17.6</td><td>42.6</td></tr><tr><td>LLaVA-NeXT (Liu et al. 2024b)</td><td>7B</td><td>10.3</td><td>12.8</td><td>12.0</td><td>10.7</td><td>9.7</td><td>6.3</td><td>34.6</td><td></td><td>1</td><td>-</td><td></td></tr><tr><td>MiniGPT-v2 (Chen et al.2023a)</td><td>13B</td><td>11.0</td><td>12.1</td><td>12.0</td><td>13.1</td><td>10.3</td><td>7.4</td><td>23.1</td><td>26.0</td><td>28.1</td><td>24.7</td><td>25.4</td></tr><tr><td>SPHINX-Plus (Gao et al.2024) ShareGPT4V (Chen et al.2023b)</td><td>13B</td><td>12.2</td><td>13.9</td><td>11.6</td><td>11.6</td><td>13.5</td><td>10.4</td><td>36.8</td><td></td><td></td><td></td><td></td></tr><tr><td>InternLM-XC2. (Dong et al. 2024)</td><td>7B</td><td>13.1 16.3</td><td>16.2</td><td>16.2</td><td>15.5</td><td>13.8</td><td>3.7</td><td>27.5</td><td>27.4</td><td>1</td><td>27.6</td><td></td></tr><tr><td>G-LLaVA (Gao et al. 2023a)</td><td>7B</td><td></td><td>20.2</td><td>14.3</td><td>14.2</td><td>17.5</td><td>15.2</td><td>47.8</td><td>31.7</td><td>32.0</td><td>30.5</td><td>37.7</td></tr><tr><td>SPHINX-MoE (Gao et al.2024)</td><td>8√ó7B</td><td>16.6</td><td>20.9</td><td>20.7</td><td>17.2</td><td>14.6</td><td>9.4</td><td>23.8</td><td>38.9</td><td>36.3</td><td>35.6</td><td>20.5</td></tr><tr><td>DeepSeek-VL (Lu et al.2024a)</td><td></td><td>16.8</td><td>26.2</td><td>17.4</td><td>16.7</td><td>12.5</td><td>11.1</td><td>42.3</td><td>31.2</td><td>31.7</td><td>30.5</td><td>50.8</td></tr><tr><td></td><td>7B</td><td>19.3</td><td>23.0</td><td>23.2</td><td>20.2</td><td>18.4</td><td>11.8</td><td>34.9</td><td>28.4</td><td>29.2</td><td>27.2</td><td>35.3</td></tr><tr><td>Math-LLaVA (Shi et al.2024) MAVIS (Zhang et al. 2024b)</td><td>13B</td><td>22.9</td><td>27.3</td><td>24.9</td><td>24.5</td><td>21.7</td><td>16.1</td><td>38.3</td><td>29.3</td><td>28.5</td><td>30.5</td><td>42.6</td></tr><tr><td></td><td>7B</td><td>27.5</td><td>41.4</td><td>29.1</td><td>27.4</td><td>24.9</td><td>14.6</td><td></td><td></td><td>1</td><td>-</td><td>-</td></tr><tr><td>Math-PUMA-Qwen2-1.5B</td><td>1.5B</td><td>29.6</td><td>35.8</td><td>32.2</td><td>31.3</td><td>30.4</td><td>18.5</td><td>44.5</td><td>47.6</td><td>434</td><td>47.3</td><td>41.0</td></tr><tr><td>Math-PUMA-Qwen2-7B</td><td>7B</td><td>33.6</td><td>42.1</td><td>35.0</td><td>33.4</td><td>31.6</td><td>26.0</td><td>47.9</td><td>48.1</td><td>47.7</td><td>47.3</td><td>42.6</td></tr><tr><td>Math-PUMA-DeepSeek-Math-7B</td><td>7B</td><td>31.8</td><td>43.4</td><td>35.4</td><td>33.6</td><td>31.6</td><td>14.7</td><td>44.7</td><td>39.9</td><td>39.2</td><td>41.4</td><td>48.4</td></tr></table></body></html>

The Role of Each Stage To evaluate the significance of each stage, we conduct three ablation experiments by individually removing stages 1, 2, and 3. We then assess the accuracy across overall, text-dominant, and vision-only scenarios, as well as the gaps between them. The results of these experiments are summarized in Table 3.

Removing Stage 1: Stage 1 aims to enhance the mathematical reasoning capabilities of the LLMs. As observed in Table 3, upon removing stage 1, there is a slight decrease in the accuracy compared to the corresponding model trained with all three stages. This reduction occurs because stage 1 serves as the foundation for stage 2. When the LLM lacks strong mathematical reasoning capabilities, strong logits are not reliable to supervise weak logits, resulting in lower performance. However, due to the presence of complete stage 2 and 3, the gap remains close to that of the complete threestage training model and relatively low.

Removing Stage 2: Stage 2 embodies our devised PUMA, facilitating a close alignment between visual and textual modalities. As depicted in Table 3, the absence of stage 2 results in a wider gap in reasoning performance between textual and visual modalities when compared to the three-stage approach. Nonetheless, with the enhancement of mathematical reasoning capabilities by stage 1 and multimodal instruction tuning with high-quality data through stage 3, the overall performance persists at a high level.

Removing Stage 3: Stage 3 is multimodal instruction tuning. We observe that if only stage 1 and 2 are performed without subsequent multimodal instruction tuning, MLLMs tend to lose instruction-following (IF) capabilities to some extent. As seen in Table 3, the performance of MLLMs drastically declines when stage 3 is excluded, primarily due to the loss of IF capabilities. Since we have conducted stage 2, the gap between textual and visual modalities remains relatively small.

Sequential Order of Stages We swap stage 2 and 3 to assess their impact on MLLMs. As shown in Table 3, exchanging stage 2 and 3 leads to a significant performance drop. Our analysis of each stage reveals the critical role of stage 3 in maintaining the IF capabilities of MLLMs. Consequently, rearranging the stage 2 and 3 results in the loss of IF capabilities of MLLMs, thereby influencing their overall performance. Nonetheless, the eventual implementation of stage 2 ensures that the gap between textual and visual modalities remains relatively small.

Table 2: Evaluation results on WE-MATH testmini set. AVG: average score (strict); RM: rote memorization (strict). The best scores of each category are marked in bold fonts.   

<html><body><table><tr><td>Model</td><td>#Params.</td><td>AVG‚Üë</td><td>RM‚Üì</td></tr><tr><td colspan="4">Closed-sourceMLLMs</td></tr><tr><td>Qwen-VL-Max (Bai etal.2023) Gemini-1.5-Pro (Reid etal.2024) GPT-4V (OpenAI 2023b) GPT-4o (OpenAI 2024b)</td><td></td><td>10.5 26.4 31.1 42.9</td><td>75.5 54.8 47.9 34.2</td></tr><tr><td colspan="4">Open-sourceMLLMs(‚â•20B)</td></tr><tr><td>InternVL-Chat-V1.5 (Chen etal.2024) LLaVA-NeXT(Liuetal.2024b) LLaVA-NeXT(Liu etal.2024b)</td><td>26B 72B 110B</td><td>15.0 13.4 19.2</td><td>73.3 71.0 66.0</td></tr><tr><td colspan="4">Open-source MLLMs(‚âà10B)</td></tr><tr><td>LLaVA-1.5 (Liu et al. 2024a) LLaVA-1.5 (Liu et al. 2024a) LLaVA-1.6 (Liu et al.2024b) LLaVA-1.6 (Liu et al. 2024b) DeepSeek-VL (Lu etal.2024a) G-LLaVA (Gao et al. 2023a) Math-LLaVA(Shietal.2024)</td><td>7B 13B 7B 13B 7B 13B 13B</td><td>6.5 8.4 3.3 5.2 6.3 6.5 11.1 12.7</td><td>85.6 78.1 89.1 86.9 84.8 86.6 72.8</td></tr><tr><td>InternLM-XC2.(Dong et al.2024) Math-PUMA-Qwen2-1.5B</td><td>7B 1.5B</td><td>10.4</td><td>77.6 75.5</td></tr><tr><td>Math-PUMA-Qwen2-7B Math-PUMA-DeepSeek-Math</td><td>7B 7B</td><td>19.2 15.6</td><td>67.8 67.4</td></tr></table></body></html>

<html><body><table><tr><td>Order</td><td>LLM</td><td>ALL‚Üë</td><td>Text- dom. ‚Üë</td><td>Vision- only ‚Üë</td><td>Gap‚Üì</td></tr><tr><td colspan="6">Standardpipeline</td></tr><tr><td rowspan="2">1‚Üí2‚Üí3</td><td>Qwen2-1.5B</td><td>29.6</td><td>35.8</td><td>18.5</td><td>93.5</td></tr><tr><td>Qwen2-7B DeepSeek-Math</td><td>33.6 31.8</td><td>42.1 43.4</td><td>26.0 14.7</td><td>61.9 195.2</td></tr><tr><td colspan="6">Effectivenessof Stage1(Enhancing LLM)</td></tr><tr><td rowspan="3">2‚Üí3</td><td>Qwen2-1.5B</td><td>17.0</td><td>19.9</td><td>12.1</td><td>64.5</td></tr><tr><td>Qwen2-7B</td><td>19.6</td><td>27.3</td><td>11.9</td><td>129.4</td></tr><tr><td>DeepSeek-Math</td><td>23.9</td><td>30.7</td><td>11.2</td><td>174.1</td></tr><tr><td colspan="6">EffectivenessofStage2(Math-PUMA)</td></tr><tr><td rowspan="3">1‚Üí3</td><td>Qwen2-1.5B</td><td>24.6</td><td>40.3</td><td>9.8</td><td>311.2</td></tr><tr><td>Qwen2-7B</td><td>27.2</td><td>44.1</td><td>11.0</td><td>300.9</td></tr><tr><td>DeepSeek-Math</td><td>29.3</td><td>43.4</td><td>9.1</td><td>376.9</td></tr><tr><td colspan="6">Effectiveness ofStage3(Multimodal instruction tuning)</td></tr><tr><td rowspan="2">1‚Üí2</td><td>Qwen2-1.5B</td><td>11.7</td><td>15.5</td><td>8.1</td><td>91.4</td></tr><tr><td>Qwen2-7B DeepSeek-Math</td><td>21.2 22.2</td><td>28.9 36.2</td><td>12.2 14.8</td><td>136.9 144.6</td></tr><tr><td colspan="6">Sequential Order of Stages</td></tr><tr><td rowspan="2">1‚Üí3‚Üí2</td><td>Qwen2-1.5B</td><td>24.5</td><td>38.2</td><td>12.1</td><td>215.7</td></tr><tr><td>Qwen2-7B DeepSeek-Math</td><td>26.7 23.4</td><td>34.4 34.3</td><td>18.7 4.3</td><td>84.0 697.7</td></tr></table></body></html>

Table 3: Results of ablation study. Order: the sequential order of Stage 1, 2, and 3; ALL: overall accuracy; Text-dom.: accuracy of text-dominant data; Vision-only: accuracy of vision-only data; Gap: (Text-dom. ‚àí Vision-only) / Visiononly. The best scores of each LLM are marked in bold fonts.

Have the modality gaps truly narrowed? Through the aforementioned analysis, we have demonstrated the effectiveness of our method. However, we still seek to provide a definitive conclusion to address the initial query: Has the performance gap between different modalities truly narrowed? To this end, we base our exploration on the evaluation metrics provided by MATHVERSE, calculating the average scores of the model on textual and visual questions to intuitively assess the model‚Äôs performance across these two distinct modalities. Additionally, we compute the skewness and coefficient of variation of the scores on different types of questions in MATHVERSE to corroborate our observations regarding the modal balance.

![](images/ece273ec5d3b0bf0806ae83d8a703a1cc42dd01f1794ba17a2544529a27cc430.jpg)  
Figure 4: Visualizing MLLMs‚Äô performance on MATHVERSE. ‚ÄúText‚Äù shows average scores for text-dominant and text-lite categories, while ‚ÄúVision‚Äù represents average scores for vision-intensive, vision-dominant, and vision-only categories. ‚ÄúAbsolute Skewness‚Äù and ‚ÄúCoefficient of Variance‚Äù denote the statistical measures of score distribution across the five categories, with skewness taken as an absolute value.

As illustrated in Figure 4, in terms of overall performance, our model achieves high average scores on both textual and visual questions, outperforming closed-source MLLMs such as Gemini-1.0-Pro and Qwen-VL-Max. We analyze the performance gap between textual and visual modalities. Our model maintains a high level of performance while exhibiting a relatively smaller gap, which is even smaller than that of GPT-4V. Additionally, regarding score distribution, a model that performs consistently across modalities should demonstrate similar scores across various types of questions in MATHVERSE. This consistency is indicated by lower absolute skewness and coefficient of variation. By visualizing the score distributions of several models, it is evident that our model exhibits low levels of both skewness and coefficient of variation, indicating a well-balanced performance across different types. In summary, our method mitigates the performance disparity between different modalities.

# Conclusion

In this paper, we present Math-PUMA, a progressive upward multimodal alignment approach aimed at enhancing the mathematical reasoning capabilities of MLLMs. Experimental results indicate that Math-PUMA MLLMs not only achieve state-of-the-art performance among open-source models on multiple mathematical benchmarks but also significantly reduce the performance gap between textual and visual modalities.