# RaDIO: Real-Time Hallucination Detection with Contextual Index Optimized Query Formulation for Dynamic Retrieval Augmented Generation

Jia Zhu1, Hanghui $\mathbf { G u o } ^ { 1 }$ , Weijie $\mathbf { S h i } ^ { 2 * }$ , Zhangze Chen1, Pasquale De Meo3

1The Zhejiang Key Laboratory of Intelligent Education Technology and Application, Zhejiang Normal University, China 2Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, China 3Department of Computer Science, University of Messina, Italy {jiazhu, ghh1125}@zjnu.edu.cn, wshiah $@$ connect.ust.hk, zjnuczz $@$ zjnu.edu.cn, pdemeo@unime.it

# Abstract

The Dynamic Retrieval Augmented Generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs). However, current dynamic RAG methods fall short in both aspects: identifying the optimal moment to activate the retrieval module and crafting the appropriate query once retrieval is triggered. To overcome these limitations, we introduce an approach, namely, RaDIO, Real-Time Hallucination Detection with Contextual Index Optimized query formulation for dynamic RAG. The approach is specifically designed to make decisions on when and what to retrieve based on the LLM’s real-time information needs during the text generation process. We evaluate RaDIO along with existing methods comprehensively over several knowledge-intensive generation datasets. Experimental results show that RaDIO achieves superior performance on all tasks, demonstrating the effectiveness of our work.

# Code — https://github.com/ghh1125/RaDIO

# Introduction

Large Language Models (LLMs) have advanced significantly in a range of natural language processing (NLP) tasks and are increasingly being integrated into various AI applications (Chowdhery et al. 2022; Touvron et al. 2023a). Despite their impressive capabilities, LLMs often generate text that appears coherent but is factually incorrect, a problem known as LLM hallucination (Maynez et al. 2020; Ji et al. 2023a,b; Su et al. 2024c).

Retrieval-Augmented Generation (RAG) has become a key approach to addressing hallucinations in LLMs. As illustrated in Figure 1, RAG improves LLMs by retrieving relevant information from external databases and adding it to the model’s inputs (Jiang et al. 2022). In many NLP challenges, the use of RAG has led to superior effectiveness (Borgeaud et al. 2022; Shi et al. 2023b).

Traditional RAG usually relies on single-round retrieval, that is they use the LLM generated text as input to retrieve relevant information from external sources. This method performs well for simpler tasks but struggles with complex, multi-step, and long-form generation tasks.

三   
自自 区   
Documents Embedding Model Semantic Vector Inadequate for 1 complex, ， Q mutistp   
Users Query Retrieval Vector Database generation tasks. 4 1 Output LLM Traditional Limitations

A more advanced solution is the dynamic RAG, i.e., a RAG that performs multiple retrievals during the LLM’s generation process (Borgeaud et al. 2022). Dynamic RAG involves two key steps: identifying the optimal time instant to activate the retrieval module and crafting the appropriate query once retrieval is triggered (Jiang et al. 2022).

Several kinds of dynamic RAG methods are currently available and each of them implements ad-hoc strategies to carry out the two steps above. For instance, IRCoT (Trivedi et al. 2022) employs a global augmentation method, and it activates the retrieval module for each generated sentence, and the query is formed using the latest sentence. IC-RALM (Ram et al. 2023) defines a sliding window and triggers the retrieval module after processing a preset number of tokens, using the last $n$ tokens to form the query.

Designing a dynamic Retrieval-Augmented Generation technique raises two crucial questions: When is the optimal time to initiate the retrieval phase? How can we formulate effective queries to external sources that address the immediate information needs of a Large Language Model?

Current dynamic RAG strategies often rely on static rules to determine when to retrieve information, without considering whether retrieval is truly necessary. This approach can lead to unnecessary retrievals, introducing irrelevant or noisy data that compromises the quality of the LLM output.

Furthermore, implementing retrieval augmentation increases the time and computational cost of LLM inference. This added cost is unwarranted if the LLM can produce accurate output independently or if the integrated content does not meet acceptable standards.

Moreover, current RAG strategies for determining what to retrieve typically focus on the LLM’s most recent sentence or last few tokens. However, this narrow scope may fail to capture the model’s real-time information needs, which could span the entire context. Consequently, retrieving documents based solely on recent sentences or tokens is likely to yield suboptimal results in LLM generation performance.

To address these limitations, we propose a new framework called RaDIO (Real-Time Hallucination Detection with Contextual Index Optimized query formulation for dynamic RAG), which is specifically designed to make informed decisions about when and what to retrieve, ensuring that the generated content is accurate and relevant.

A crucial component of RaDIO is our novel hallucination detection method, which determines when to initiate retrieval. Unlike existing approaches that rely on static rules, our method uses a data-driven approach to select the optimal time for retrieval. Our approach considers three key factors: the uncertainty of the Large Language Model about its generated content, the importance of choosing an entity, and the semantic significance of each entity. By analyzing these factors, we can identify instances where the model may be producing inaccurate information or “hallucinating”.

To achieve this, our hallucination detection module first randomly selects entities from Wikipedia documents on a given topic and truncates the documents at those entities. From the truncation point, the LLM then generates uncertain content (entities) that might have hallucinations. We compare the entities at the beginning of the generated content with the original entities from the truncated Wikipedia text to detect semantic discrepancies, as hallucinations often involve changes in key entities (e.g., incorrect dates, events, or names). Using this approach, we can construct a large training set of hallucination examples and train a multi-layer perceptron (MLP)-based hallucination detector. This detector takes the contextual embeddings of tokens from the final layer of the LLM’s Transformer architecture as input and provides real-time feedback to the RAG system.

Once we have determined the best possible time to trigger the RAG, we need to formulate a query that retrieves the most relevant documents.To achieve this, we propose a novel query formulation approach that leverages multi-headed attention mechanisms. Our method benefits from the information associated with different representational subspaces, allowing it to accurately distinguish between important and less important tokens. Our approach considers the content and relative position of each token within the text, providing a deeper understanding of the contextual relevance and importance of each token. We further refine our query by analyzing the semantic similarity between embedding vectors, selecting the most crucial tokens that convey the essential meaning and context. By integrating these components, our query formulation process generates a high-quality query that effectively captures the nuances of the input text, leading to improved performance in downstream tasks.

We evaluate RaDIO against other well-known dynamic RAG frameworks across four knowledge-intensive generation benchmarks. The results demonstrate that RaDIO outperforms all other methods on these datasets, demonstrating the effectiveness of our method. Additionally, the results of the ablation study indicate that our strategies for when to retrieve and what to retrieve consistently outperform other strategies, regardless of the retrieval model used.

The main contributions of our work are as follows:

• Introduction of RaDIO Framework: We propose a novel dynamic Retrieval-Augmented Generation (RAG) framework named RaDIO, which stands for Real-Time Hallucination Detection with Optimized Query Formulation. Unlike previous approaches, RaDIO dynamically optimizes both the timing and content of retrievals based on the Large Language Model’s (LLM) real-time information needs during text generation. • Unsupervised Real-Time Hallucination Detection: We develop an unsupervised training method for real-time hallucination detection, leveraging the internal states of the LLM throughout the text generation process. This approach allows the system to identify and address hallucinations as they occur, ensuring more accurate and reliable outputs while reducing the LLM’s content uncertainty during generation. • Comprehensive Evaluation on Knowledge-Intensive Datasets: We conduct an extensive evaluation of existing dynamic RAG methods alongside RaDIO across four knowledge-intensive datasets using LLMs. Our experimental results demonstrate that RaDIO achieves state-ofthe-art (SOTA) performance across all tasks, showcasing the effectiveness and superiority of our approach.

# Related Works

LLMs have shown impressive effectiveness across various tasks, but their built-in knowledge often falls short for knowledge-intensive applications. To address this, RAG strategies are widely used to enhance LLM performance.

A common method is single-round retrieval augmentation (Khandelwal et al. 2019; Borgeaud et al. 2022; Jiang et al. 2022; Shi et al. 2023a), which uses the initial input to query an external corpus and integrates the retrieved knowledge into the model (Mondal et al. 2024). For instance, REPLUG (Shi et al. 2023a) uses LLMs to generate training data for retrieval models, while UniWeb (Li et al. 2023) employs an adaptive search engine to determine when retrieval is needed. While effective for straightforward tasks, singleround retrieval struggles with complex tasks involving longform text, such as open-domain summarization and chainof-thought reasoning. In these cases, relying solely on the initial input often lacks sufficient external knowledge (Jiang et al. 2023a; Salemi and Zamani 2024).

To address this, multi-round retrieval augmentation strategies have been explored. RETRO (Borgeaud et al. 2022) and ICRALM (Ram et al. 2023) perform retrieval every few tokens, while IRCot (Trivedi et al. 2022) retrieves information for each sentence. Fixed intervals can be inefficient, so FLARE (Jiang et al. 2023a) triggers retrieval when encountering uncertain tokens. Recently, Su et al. (Su et al. 2024a) introduced DRAGIN, a dynamic RAG framework that makes real-time retrieval decisions based on LLM needs. However, DRAGIN’s reliance on attention entropy and token relevance may be insufficient.

Despite advancements, current retrieval-augmented methods still have limitations. Single-round retrieval may not cover all necessary external knowledge for complex tasks, while multi-round techniques with fixed intervals or simplistic triggers may not fully align with LLMs’ dynamic needs. DRAGIN, despite its innovation, relies on potentially inadequate metrics for retrieval triggering.

# Proposed Approach

In this section, we introduce RaDIO, our approach to Dynamic Retrieval-Augmented Generation. As shown in Figure 2, RaDIO includes two main components: Real-time Hallucination Detection and Contextual Index Optimized Query Formulation, described in detail below.

# Real-time Hallucination Detection

Most existing dynamic RAG frameworks rely on static, predefined rules to trigger the retrieval module. An example of a static rule to trigger the retrieval module can be found in the FLARE system (Jiang et al. 2023a), which dynamically initiates retrieval when the LLM’s next-token generation probability falls below a certain threshold. However, the activation of the retrieval module should depend not only on the generation probability but also consider the token’s importance within the global generation context. Meanwhile, hallucinations often involve changes in entities (e.g., incorrect dates or names) (Chang et al. 2024). Therefore, how to effectively judge the semantic differences of entities is one of the urgent problems to be solved in judging hallucinations.

To address these limitations, we propose an enhanced real-time hallucination detection approach for triggering retrieval within dynamic RAG frameworks which takes into account the importance of tokens of entities.

We leverage unsupervised data generation and automatic annotation of LLM-generated content to detect hallucinations. The process of hallucination detection is graphically shown in Figure 3 and, initially, it selects a Wikipedia dataset $\boldsymbol { \mathcal { W } }$ represented as ${ \mathcal { W } } = \{ \mathbf { w } _ { 1 } , \mathbf { w } _ { 2 } , . . . , \mathbf { w } _ { n } \}$ , where each $\mathbf { w } _ { i }$ represents an individual article. Subsequently, we select key entities (eg. name, year, number, and other significant terms) from each article $\mathbf { w } _ { i }$ and truncate each document $\mathbf { w } _ { i }$ according to the locations of the extracted entities (Except for the beginning of each sentence). Truncated documents are then sent as input to the LLM with the prompt: “Below is the opening sentence from a Wikipedia article titled [Title]. Please continue the passage from where the sentence ends. [First sentence of that article].” The LLM then generates a piece of text, say $\mathbf { T } _ { i }$ , for each truncated segment.

To detect hallucinations, we perform entity extraction on the generated text $\mathbf { T } _ { i }$ at each breakpoint of the original article. However, entities often exist in multiple forms of expression, such as abbreviations, variations in naming conventions, or differences in word order. Therefore, simple direct matching may not capture all entity-level errors.

To address this, we combine manual evaluations with automated comparisons between the extracted entities and the originally selected entities. For example, we leverage external linguistic resources such as WordNet (Miller 1995)

， Input:Please give me a brief introduction to Marie Curie.   
1 Marie Curiewasborn inWarsaw,Poland in1867.Shemoved to   
8 Parisi9eeeon for her research on radioactivity.. 1 Real-time Hallucination Detection Nobel Prize in Chemistry for + ↓ + + ★ Score 0.13 0.13 0 0.42 回 Score<μ μ<Score<0.5 Contextual Index Optimized Query Formulation Please give mea brief introduction to Marie Curie <SEP>Marie Curie was born in Warsaw, Poland in 1867.In 1903, she was awarded the Nobel Prize in ... MarieCurie Poland MarieCurie Warsaw18671903 Eq.(5) the Nobel Prize the Nobel Prize 1903 ↓ Retrieval Query: Marie Curie, Module the Nobel Prize，1903 Retrieved Externalknowledge LLMContinual Generation based on External Knowledge: Marie Curie was born in Warsaw, Poland in 1867.She moved to Paris in 1891 to attend the University of Paris. In 1903, she was awarded the Nobel Prize in Physics for her....

and utilize Word2Vec (Rong 2014) to calculate the cosine similarity between the vectors of different entity representations. This allows us to assess whether two entities are semantically equivalent. If the entities are semantically similar (e.g., “NBA” and “National Basketball Association” are the same), no hallucination is detected. Otherwise, we label the LLM-generated text as hallucinatory.

More formally, for each article $\mathbf { w } _ { i }$ , we construct a tuple $\mathbf { D } _ { i } { = } \{ \mathbf { w } _ { i } , \mathbf { T } _ { i } , \mathbf { e } _ { i } , \mathbf { n } _ { i } , H _ { i } \}$ , where $\mathbf { T } _ { i }$ is the LLM-generated article, $\mathbf { e } _ { i }$ is the originally selected entity, $\mathbf { n } _ { i }$ is the entity extracted from the LLM-generated text and $H _ { i }$ is the hallucination label, i.e., 1 for hallucinatory, 0 for non-hallucinatory. This process enables us to automatically annotate a large dataset with hallucination labels, which can be used as a training set to detect hallucinations.

We train an MLP (multi-layer perceptron) using input matrices derived from the processed Wikipedia hallucination dataset $\mathbf { D } _ { i }$ , which includes LLM-generated text and associated features. More specifically, the classifier outputs a probability $P$ , indicating the likelihood of hallucination occurrence when generating a specific text segment:

$$
P = \mathrm { M L P } \left( \mathrm { V e c } \left( \mathrm { M x } \left( \mathbf D _ { i } \left\{ \mathbf w _ { i } , \mathbf T _ { i } , \mathbf e _ { i } , \mathbf n _ { i } : \mathbf H _ { i } \right\} \right) \right) + b \right) ,
$$

where $\mathrm { V e c } ( \mathbf { M x } ( \mathbf { D } _ { i } ) )$ represents the process of converting the input features $\mathbf { D } _ { i }$ from the hallucination dataset into a matrix and then into vector form, and $b$ is the bias vector.

Meanwhile, we utilize the binary cross-entropy (BCE) loss to train the classifier:

$$
L _ { \mathrm { B C E } } ( y _ { i } , p _ { i } ) = y _ { i } \log ( p _ { i } ) + ( 1 - y _ { i } ) \log ( 1 - p _ { i } ) ,
$$

where $y _ { i }$ is a true label of the $i$ -th example, and $p _ { i }$ indicates the predicted probability of belonging to the positive class.

Wikipedia Content Wikipedia Content Alexander Fleming was a Marie Curie was a physicist and Scottish bacteriologist who chemist who conducted pioneering discoveredPenicillinin 1928. research on radioactivity.In 1903, This discovery led to the she won theNobel Prize in Physics,   
WIKIPEDIA development of antibiotics and WIKIPEDIA becoming the first woman to   
T Cw has saved countless lives. W Tbe Free Encyclo receivea Nobel Prize. W   
Random SelectanEntity After the First Sentence Random Selectan Entity After the FirstSentence ↓ ↓ Alexander Fleming was a Scottish .. .who conducted pioneering bacteriologistwho discovered > research on radioactivity. In 1903 Wi she won the> Wi The text preceding the selected entity isi The text preceding the selected entity is used as input context for the LLM used as input context forthe LLM LLM Continue Generation Nobel LLM Continue Generation based on tthe Context based on tthe Context   
Penicillin Prize in niPenicillinin 1928. His discovery Physics niNobel Prize in Chemistryin 1911 eii was crucialin the.…. T eil and is the only woman.….T Matched Mismatched tupleDi Multi-Layer Perceptron tupleD Multi-Layer Perceptron OF 1 (MLP) Eq.(1) (MLP)Eq.(1) Train Eq.(2) Train Eq.(2) HallucinationClassifier HallucinationClassifier

Therefore, we have trained an MLP-based hallucination detector using a large corpus of Wikipedia data to ensure its effectiveness in identifying hallucinations.

To further enhance the accuracy and efficiency of hallucination detection, we employ a token-level scoring mechanism. Specifically, for each token generated by the LLM’s last layer, we compute its probability score using an MLP that has been previously trained on Wikipedia data. If this score exceeds the average score of all tokens and the probability output is below 0.5, we indicate a potential hallucination and trigger the RaDIO alarm. This approach allows us to achieve dynamic (real-time) hallucination detection for LLM outputs, improving judgment efficiency and accuracy while reducing resource consumption.

# Contextual Index Optimized Query Formulation

Once the retrieval augmentation position is identified (i.e., the hallucination monitoring token’s location), the following step in the RAG framework is to formulate a query to retrieve necessary information from an external database, enabling the LLM to continue generating text. However, existing dynamic RAG frameworks rely solely on attention weights of the LLM’s latest sentence or the last few tokens but such a solution is not entirely satisfactory because it may not be able to accurately represent the relationships between words or indicate the importance of specific tokens.

To mitigate this limitation, RaDIO leverages the inherent framework of Transformer-based LLMs and introduces a multi-head attention mechanism to capture information from multiple representation subspaces.

Our approach has two significant advantages: on the one hand, it can employ contextual features and similarity information of the tokens used to detect the hallucination, and on the other hand, it provides a more comprehensive understanding of which tokens to prioritize as retrieval objects.

The multi-head attention mechanism enables the model to compute attention weights through multiple heads and subsequently combine them to generate more precise weight judgments (Vaswani 2017). By representing attention across different heads, the model can focus on various semantic and syntactic features of the text, offering a more comprehensive understanding of which parts of the text are most important for retrieving external information. This mechanism involves several steps: linear transformations of the input, concatenation of the results, and transformation of the attention weights to produce the final attention representation. Specifically, the multi-head attention result is calculated using the following formula:

$$
\operatorname { A t t } _ { i } ( Q , K , V ) = C ( \operatorname { h e a d } _ { 1 } , \operatorname { h e a d } _ { 2 } , \dots , \operatorname { h e a d } _ { h } ) W ^ { O } ,
$$

where $C$ denotes the concatenation operation and head $\mathsf { l } _ { i }$ represents the attention result for the $i ^ { t h }$ head, calculated as:

$$
\mathrm { h e a d } _ { i } = \mathrm { s o f t m a x } \left( Q W _ { i } ^ { Q } \cdot ( K W _ { i } ^ { K } ) ^ { T } \cdot d _ { k } ^ { - \frac { 1 } { 2 } } \right) V W _ { i } ^ { V } .
$$

Here, $Q , K$ , and $V$ represent the query, key, and value matrices, respectively. $W _ { i } ^ { Q } , W _ { i } ^ { K }$ , and $W _ { i } ^ { V }$ are linear transformation matrices for the $i ^ { t h }$ attention head, and $W ^ { O }$ is the output linear transformation matrix (Eq.3).

In addition to attention scores, we also need to consider the actual content and relative position of tokens in the text context to evaluate their importance. To this end, RaDIO calculates the TF-IDF score (TD) of each word in the text to identify words with high information density enabling the system to prioritize content-rich tokens over less informative ones, thereby better capturing key parts of the information.

Meanwhile, we extract the corresponding words from the vocabulary table for these token markers, arrange them in their original order in the text, and normalize their positions using the token position index to obtain a relative position score Pi. The position score Pi is calculated as Pi = poNs(i) , where $\mathrm { p o s } ( i )$ is the position of the token in the text, and $N$ is the total number of tokens in the text.

By incorporating TF-IDF score and position score evaluation into our framework, we enhance our ability to capture the significance of each word within the context of the entire text, rather than relying solely on attention scores. This approach complements the attention mechanism by assigning greater weight to rare but contextually important tokens, ensuring that retrieval focuses on the most relevant parts of the generated text (Ramos et al. 2003; Radford et al. 2019). As a result, we can perform more precise importance evaluations for tokens, identifying those that are most pertinent for subsequent retrieval tasks.

Simultaneously, RaDIO also leverages the similarity between word embedding vectors to measure the semantic relevance between all words in the context and query words. This allows us to more accurately identify which words have semantic relevance to the query content. Specifically, we employ a pre-trained word embedding model to convert each word in the text into a vector representation. We then compute the cosine similarity between these vectors and the vector representation of the query words. Cosine similarity effectively measures the directional similarity between two vectors, which assists in identifying the words most semantically relevant to the query content. Through this process, we can extract the most relevant words from the text, optimize retrieval results, and ensure that the retrieved information better matches the query requirements, thereby reducing noise at the word level.

To assess the significance of each token, we use a multifaceted evaluation framework integrating attention scores, TF-IDF values, and cosine similarity measures. We normalize these metrics to a common scale for fair comparison and combine the normalized values using linear addition to produce a comprehensive token importance score:

$$
\mathrm { T o k } _ { i } = \sigma ( \mathrm { A t t } _ { i } ) + \sigma ( \mathrm { T D } _ { i } ) + \sigma ( \mathrm { C o s } _ { i } ) + \mathrm { P } _ { i } ,
$$

where $\sigma$ denotes the normalization function and $+$ represents the addition of the four token computation results.

To enhance the effectiveness of the retrieval process, we incorporate token-level contextual information and relevance scores to refine the selection of documents, ensuring that the retrieved information is highly pertinent. Specifically, RaDIO leverages the computed token importance scores to rank tokens in descending order of their significance. This ranking helps identify the most critical tokens likely to be relevant for the retrieval task and obtain external knowledge support. Subsequently, RaDIO employs a stateof-the-art retrieval model (e.g., SGPT) to fetch relevant documents from an external knowledge base, based on the topranked tokens. The retrieved documents are seamlessly integrated into the LLM’s prompt template:

External Knowledge: [1] $K _ { i 1 }$ [2] $K _ { i 2 }$ [3] $K _ { i 3 }$ Using the external knowledge provided above, please answer the following question: Question: [Qes.] Answer: Insert truncated output $\boldsymbol { \mathrm { ~ \textmu ~ } }$ and additional relevant details here

This integration effectively bridges the knowledge gaps in the LLM’s generation process. Specifically, at the truncation point where the LLM-generated content exhibits hallucinations, we integrate external retrieval knowledge to enable the LLM to continue generating content from this point based on external knowledge.

This synergistic integration of token importance evaluation, ranking-based token selection, and external knowledge retrieval allows RaDIO to bridge the knowledge gaps in the LLM’s generation process, ensuring that the generated content is more accurate and complete, with a noticeable improvement in the quality and reliability of the output, while also alleviating the uncertainty (Hallucination) of the Large Language Model about its generated content.

# Experiments Experimental Setups

Datasets. We consider four benchmark datasets for experimental analysis, namely: a) 2WikiMultihopQA (Ho et al. 2020): This dataset tests the model’s ability to perform chain-of-thought (CoT) reasoning and generate answers by integrating information from multiple Wikipedia articles. $\textbf { \textit { b } }$ ) HotpotQA (Yang et al. 2018): This dataset, similar to 2WikiMultihopQA, focuses on questions requiring information from multiple documents and generates CoT reasoning processes and final answers to evaluate the model’s ability to connect different pieces of evidence. c) StrategyQA (Geva et al. 2021): This dataset assesses commonsense reasoning by posing questions that require implicit strategies, generating CoT processes and final answers to evaluate abstract thinking and commonsense knowledge. d) IIRC (Abdelsalam et al. 2021): This dataset tests reading comprehension by requiring the integration of information from multiple documents, with questions often needing answers that span several passages and demand advanced synthesis.

Evaluation Metrics. We employed a comprehensive set of metrics to evaluate the performance of RaDIO, and other RAG methods across different datasets and retrieval methods. These metrics can be broadly categorized into two groups: effectiveness metrics and efficiency metrics.

Effectiveness metrics measure the accuracy and quality of generated answers. We used Exact Match (EM), F1 Score, Precision (Pre.), and Recall (Rec.). EM gauges the proportion of answers that exactly match the reference, offering a strict accuracy assessment. The F1 Score, the harmonic mean of precision and recall, provides a balanced evaluation of answer quality. Precision and recall were assessed to evaluate response accuracy.

Efficiency metrics evaluate computational resources and time complexity. We measured the Retrieve Count (Rc) for the number of documents retrieved, indicating the model’s efficiency in gathering relevant information. Generate Count (Gc) reflects the number of times answers are produced, assessing response generation capacity. Hallucinated Count (Hc) tracks occurrences of content not grounded in the input or retrieved documents. Finally, Token Count (Tc) and Sentence Count (Sc) evaluate verbosity and response length.

Implementation. We choose six text generation baselines for comparison (Su et al. 2024b) such as DRAGIN, woRAG, SR-RAG, FL-RAG, FS-RAG, and FLARE. The woRAG means the LLM does not apply any RAG. SR-RAG, according to the initial query, meaningful passages are retrieved from an external corpus and integrated into the input of the LLM. FL-RAG, the retrieval module is triggered every $n$ tokens, FS-RAG, the retrieval module is triggered every sentence (Trivedi et al. 2022), FLARE, the retrieval model is triggered every time an uncertain token is encountered (Jiang et al. 2023b), and finally DRAGIN (Su et al. 2024a), which has been already presented in the related work section. Note that wo-RAG and SR-RAG are single-round RAG, while the remaining methods implement a multi-round retrieval augmentation strategy.

In our experiments, DRAGIN serves as the Base, as referenced in other tables throughout this work. We choose LLaMA2-7B CHAT (Touvron et al. 2023b), a fine-tuned LLM specifically designed for dialogue-based applications, as the underlying LLM. We also employed three retrieval strategies, namely: BM25 (Lv and Zhai 2011), SBERT (Wang and Kuo 2020) and SGPT (Muennighoff 2022).

Experiments are conducted on two NVIDIA H800 GPUs, taking approximately 18 hours. As for the hyperparameter settings, we set the maximum generated sequence length to 256 tokens, with a retrieval top- $k$ value of 3; we selected the top 25 passages to ensure the quality of the generated responses. We ran our procedure on datasets containing 1000 data points each to obtain robust and reliable results.

Table 1: Comparison of EM, F1, Precision, and Recall scores between the Base method and RaDIO across various datasets and retrieval methods. The highest scores for each metric in each dataset are highlighted in bold.   

<html><body><table><tr><td rowspan="2">Matrix</td><td colspan="2">2WikiMultihopQA</td><td colspan="2">HotpotQA</td><td colspan="2">IRC</td><td colspan="2">StrategyQA</td></tr><tr><td>Base</td><td>RaDIO</td><td>Base</td><td>RaDIO</td><td>Base</td><td>RaDIO</td><td>Base</td><td>RaDIO</td></tr><tr><td>EM(BM25)</td><td>0.214</td><td>0.228</td><td>0.219</td><td>0.246</td><td>0.156</td><td>0.196</td><td>0.639</td><td>0.654</td></tr><tr><td>EM (SGPT)</td><td>0.209</td><td>0.214</td><td>0.202</td><td>0.211</td><td>0.125</td><td>0.173</td><td>0.604</td><td>0.610</td></tr><tr><td>EM(SBERT)</td><td>0.231</td><td>0.254</td><td>0.165</td><td>0.181</td><td>0.142</td><td>0.148</td><td>0.645</td><td>0.651</td></tr><tr><td>F1 (BM25)</td><td>0.282</td><td>0.303</td><td>0.314</td><td>0.351</td><td>0.188</td><td>0.239</td><td>0.639</td><td>0.654</td></tr><tr><td>F1 (SGPT)</td><td>0.278</td><td>0.282</td><td>0.301</td><td>0.306</td><td>0.153</td><td>0.216</td><td>0.604</td><td>0.610</td></tr><tr><td>F1 (SBERT)</td><td>0.294</td><td>0.317</td><td>0.244</td><td>0.262</td><td>0.172</td><td>0.175</td><td>0.645</td><td>0.651</td></tr><tr><td>Pre.(BM25)</td><td>0.288</td><td>0.301</td><td>0.331</td><td>0.350</td><td>0.195</td><td>0.251</td><td>0.639</td><td>0.654</td></tr><tr><td>Pre. (SGPT)</td><td>0.284</td><td>0.288</td><td>0.319</td><td>0.322</td><td>0.159</td><td>0.224</td><td>0.604</td><td>0.610</td></tr><tr><td>Pre.(SBERT)</td><td>0.298</td><td>0.322</td><td>0.256</td><td>0.273</td><td>0.176</td><td>0.179</td><td>0.645</td><td>0.651</td></tr><tr><td>Rec.(BM25)</td><td>0.285</td><td>0.295</td><td>0.316</td><td>0.345</td><td>0.196</td><td>0.239</td><td>0.639</td><td>0.654</td></tr><tr><td>Rec. (SGPT)</td><td>0.281</td><td>0.286</td><td>0.305</td><td>0.317</td><td>0.156</td><td>0.220</td><td>0.604</td><td>0.610</td></tr><tr><td>Rec. (SBERT)</td><td>0.299</td><td>0.326</td><td>0.255</td><td>0.278</td><td>0.180</td><td>0.183</td><td>0.645</td><td>0.651</td></tr></table></body></html>

Table 2: Comparison of RaDIO and other RAG methods for LLaMA2-7B-CHAT across four different datasets   

<html><body><table><tr><td>Dataset</td><td>Metric</td><td>RaDIO</td><td>Base</td><td>wo-RAG</td><td>SR-RAG</td><td>FL-RAG</td><td>FS-RAG</td><td>FLARE</td></tr><tr><td rowspan="2">2WikiMultihopQA</td><td>EM</td><td>0.254</td><td>0.231</td><td>0.146</td><td>0.169</td><td>0.112</td><td>0.189</td><td>0.143</td></tr><tr><td>F1</td><td>0.317</td><td>0.294</td><td>0.223</td><td>0.255</td><td>0.192</td><td>0.265</td><td>0.213</td></tr><tr><td rowspan="2">HotpotQA</td><td>EM</td><td>0.246</td><td>0.219</td><td>0.184</td><td>0.164</td><td>0.146</td><td>0.214</td><td>0.149</td></tr><tr><td>F1</td><td>0.351</td><td>0.314</td><td>0.275</td><td>0.150</td><td>0.211</td><td>0.304</td><td>0.221</td></tr><tr><td>StrategyQA</td><td>Pre.</td><td>0.654</td><td>0.645</td><td>0.659</td><td>0.645</td><td>0.634</td><td>0.629</td><td>0.627</td></tr><tr><td rowspan="2">IIRC</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>EM</td><td>0.196</td><td>0.156</td><td>0.139</td><td>0.187</td><td>0.172</td><td>0.178</td><td>0.134</td></tr></table></body></html>

# Experimental Results

We discuss our experimental results, which aim to comprehensively evaluate the performance of RaDIO against various competitors across four datasets: 2WikiMultihopQA, HotpotQA, StrategyQA, and IIRC. For the case study, please refer to the code link provided in the Abstract.

Overall Results. We first focused on the impact of the retrieval method. As mentioned above, the methods we included in our experimental evaluation were a BM25, SGPT, and SBERT; we measured the EM, F1, Pre. and Rec. obtained by RaDIO with DRAGIN, which is currently the state of the art in dynamic RAG. The results obtained are reported in Table 1. The main lessons learned from our experiments are a) RaDIO, regardless of the retrieval module we consider, always performs better than DRAGIN on all the datasets examined and for all the evaluation measures. In some cases, the percentage improvement of RaDIO against DRAGIN exceeds $2 5 \%$ . b) The chosen retrieval module affects the performance of both RaDIO and DRAGIN: generally, the BM25 method achieves the best results, but on the 2WikiMultihopQA dataset it is outperformed by SBERT. SGPT is also less effective than both BM25 and SBERT.

In the 2WikiMultihopQA dataset, RaDIO in conjunction with BM25 achieves an EM score of 0.228 while DRAGIN achieves a score equal to 0.214 in the same experimental configuration. F1 score rises from 0.282 to 0.303, Pre. improves from 0.288 to 0.301, and Rec. enhances to 0.295. As already pointed out, the SBERT retrieval model achieves the most convincing results on the 2WikiMultihopQA dataset, and, more precisely, we report a further improvement in the EM score (which increases to 0.254), while F1 reaches 0.317, Pre. rises to 0.322, and Rec. rises to 0.326.

The results on the HotpotQA dataset are also noteworthy. With BM25 retrieval, RaDIO’s EM score jumps from 0.219 to 0.246, F1 score increases from 0.314 to 0.351, Precision improves from 0.331 to 0.350, and Recall grows to 0.345. Using SBERT retrieval, RaDIO’s EM score rises to 0.181, F1 score reaches 0.262, Precision climbs to 0.273, and Recall enhances to 0.278. Similar improvements also occur on IIRC and StrategyQA datasets.

Comparison with other RAG methods. We compared RaDIO with six competitors using EM and F1 scores for the 2WikiMultihopQA, HotPotQA, and IIRC datasets, and precision for the StrategyQA dataset. The results, summarized in Table 2, highlight several key points: a) RaDIO always performs best, regardless of the dataset we consider, and often achieves a significant improvement over DRAGIN. b) Methods based on static rules (i.e. SR-RAG and FL-RAG) show worse performance than methods that do not use RAG (e.g. wo-RAG on the HotpotQA dataset has a higher EM and F1 score than SR-RAG or FL-RAG). Such a result highlights the importance of choosing the right time to perform the retrieval: an imprecise choice not only wastes computational resources but also does not lead to any quality improvement. c) FLARE often shows less brilliant results than all the other methods; we recall that FLARE calculates the probability of generating a token and, if this probability is below a certain threshold, it starts the retrieval. This rule seems to be ineffective in many datasets; conversely, the decision model implemented by RaDIO is richer and more sophisticated, taking into account several factors to detect important tokens.

Table 3: Compare the efficiency of various retrieval and generation methods in base and RaDIO across four different datasets.   

<html><body><table><tr><td rowspan="2">Matrix</td><td colspan="2">2WikiMultihopQA</td><td colspan="2">HotpotQA</td><td colspan="2">IIRC</td><td colspan="2">StrategyQA</td></tr><tr><td>Base</td><td>RaDIO</td><td>Base</td><td>RaDIO</td><td>Base</td><td>RaDIO</td><td>Base</td><td>RaDIO</td></tr><tr><td>Rc (BM25)</td><td>1.018</td><td>1.015</td><td>2.832</td><td>1.041</td><td>3.696</td><td>1.209</td><td>4.422</td><td>1.131</td></tr><tr><td>Rc (SGPT)</td><td>3.602</td><td>1.016</td><td>3.002</td><td>1.304</td><td>3.002</td><td>1.242</td><td>4.305</td><td>1.122</td></tr><tr><td>Rc (SBERT)</td><td>1.804</td><td>1.035</td><td>0.974</td><td>1.078</td><td>1.534</td><td>1.014</td><td>2.390</td><td>1.268</td></tr><tr><td>Gc (BM25)</td><td>2.038</td><td>2.031</td><td>6.372</td><td>2.082</td><td>7.431</td><td>2.417</td><td>8.849</td><td>2.262</td></tr><tr><td>Gc (SGPT)</td><td>7.208</td><td>2.033</td><td>6.007</td><td>2.068</td><td>6.009</td><td>2.563</td><td>8.613</td><td>2.244</td></tr><tr><td>Gc (SBERT)</td><td>3.982</td><td>2.087</td><td>2.725</td><td>2.542</td><td>3.206</td><td>2.040</td><td>5.601</td><td>3.315</td></tr><tr><td>Hc (BM25)</td><td>1.018</td><td>1.015</td><td>2.832</td><td>1.041</td><td>3.696</td><td>1.209</td><td>4.422</td><td>1.131</td></tr><tr><td>Hc (SGPT)</td><td>3.602</td><td>1.016</td><td>3.002</td><td>1.304</td><td>3.002</td><td>1.242</td><td>4.305</td><td>1.122</td></tr><tr><td>Hc (SBERT)</td><td>1.804</td><td>1.035</td><td>0.974</td><td>1.078</td><td>1.534</td><td>1.014</td><td>2.390</td><td>1.268</td></tr><tr><td>Tc (BM25)</td><td>523.763</td><td>521.979</td><td>694.887</td><td>268.811</td><td>959.080</td><td>621.381</td><td>894.252</td><td>228.675</td></tr><tr><td>Tc (SGPT)</td><td>1852.543</td><td>522.494</td><td>775.129</td><td>267.144</td><td>775.328</td><td>668.590</td><td>870.086</td><td>226.875</td></tr><tr><td>Tc (SBERT)</td><td>537.053</td><td>386.106</td><td>216.537</td><td>200.988</td><td>497.527</td><td>471.739</td><td>325.277</td><td>123.398</td></tr><tr><td>Sc (BM25)</td><td>36.530</td><td>36.474</td><td>34.372</td><td>13.141</td><td>47.099</td><td>28.235</td><td>59.893</td><td>15.580</td></tr><tr><td>Sc (SGPT)</td><td>123.930</td><td>36.275</td><td>40.494</td><td>13.087</td><td>36.826</td><td>29.106</td><td>60.191</td><td>14.923</td></tr><tr><td>Sc (SBERT)</td><td>30.908</td><td>22.932</td><td>10.629</td><td>10.334</td><td>25.094</td><td>26.603</td><td>23.592</td><td>9.181</td></tr></table></body></html>

Hence, RaDIO can effectively decide when to start retrieval operations and what to retrieve. d) Generally, singleround RAG perform worse than multi-round RAG methods. However, it should be noted that the performance of a RAG depends crucially on datasets and the evaluation metrics: for example, in the StrategyQA dataset, the differences in precision between the different models are negligible.

On the 2WikiMultihopQA dataset, RaDIO achieved stateof-the-art results with an EM of 0.254 and an F1 of 0.317, outperforming DRAGIN and wo-RAG (0.146). RaDIO also led on the HotpotQA and IIRC datasets, with an F1 of 0.351, surpassing DRAGIN and wo-RAG (0.275), demonstrating its effectiveness in handling complex queries.

However, on the StrategyQA dataset, wo-RAG (0.659) outperformed RaDIO and other RAG methods due to its simpler approach, which is more effective for binary classification. Additionally, Llama2-7B model’s limited capacity might also restrict RaDIO’s advanced features, making woRAG’s straightforward method more efficient in this case.

Efficiency Comparison Experiment. Finally, We compared the efficiency of the Base method and RaDIO, and the results show that RaDIO consistently outperforms the Base method across multiple datasets and evaluation metrics.

On the 2WikiMultihopQA dataset, RaDIO achieved an Rc of 1.015 with BM25 retrieval, slightly lower than the Base method’s Rc of 1.018. With SGPT retrieval, RaDIO’s Rc decreased from 3.602 to 1.016, and with SBERT retrieval, it improved from 1.804 to 1.035. In the HotpotQA dataset, RaDIO’s Rc was 1.041 with BM25 retrieval, significantly outperforming the Base method’s Rc of 2.832. With SGPT retrieval, RaDIO’s Rc decreased from 3.002 to 1.304, and with SBERT retrieval, it improved from 0.974 to 1.078.

In the IIRC dataset, RaDIO’s Gc was 2.417 with BM25 retrieval, a notable improvement over the Base method’s Gc of 7.431. With SGPT retrieval, RaDIO’s Gc decreased from 6.009 to 2.563, and with SBERT retrieval, it decreased from 3.206 to 2.040. In the StrategyQA dataset, RaDIO’s Tc was 228.675 with BM25 retrieval, significantly lower than the Base method’s Tc of 894.252. With SGPT retrieval, RaDIO’s Tc decreased from 870.086 to 226.875, and with SBERT retrieval, it decreased from 325.277 to 123.398.

# Conclusions and Future Works

In this work, we introduce RaDIO, an innovative dynamic Retrieval-Augmented Generation (RAG) framework designed to address the real-time information needs of LLMs during text generation. RaDIO integrates two key components, namely, timely retrieval activation and precise query formulation, to achieve state-of-the-art performance across various knowledge-intensive benchmarks, significantly outperforming existing dynamic RAG methods.

Our experimental results highlight RaDIO’s effectiveness in overcoming the limitations of existing dynamic RAG approaches, which often rely on static or pre-defined retrieval strategies. The reasons for RaDIO’s success lie in its ability to make optimal decisions about when and what to retrieve, and these decisions fit well with the real-time information needs of the LLM.

Looking ahead, we envisage that future work can build upon our research by exploring new directions. One potential area is to extend RaDIO to multi-task learning scenarios, where a single model is trained on multiple tasks simultaneously. This could lead to even more robust and versatile models that can handle different NLP tasks. Additionally, integrating RaDIO with other NLP tasks, such as question answering, text summarization, and machine translation, could further demonstrate its effectiveness in various applications.