# Beyond Single Emotion: Multi-label Approach to Conversational Emotion Recognition

Yujin Kang, Yoon-Sik Cho

Department of Artificial Intelligence, Chung-Ang University, Republic of Korea zinzin32, yoonsik @cau.ac.kr

# Abstract

Emotion recognition in conversation (ERC) has been promoted with diverse approaches in the recent years. However, many studies have pointed out that emotion shift and confusing labels make it difficult for models to distinguish between different emotions. Existing ERC models suffer from these problems when the emotions are forced to be mapped into single label. In this paper, we utilize our strategies for extending single label to multi-labels. We then propose a multi-label classification framework for emotion recognition in conversation (ML-ERC). Specifically, we introduce weighted supervised contrastive learning tailored for multi-label, which can easily applied to previous ERC models. The empirical results on existing task with single label support the efficacy of our approach, which is more effective in the most challenging settings: emotion shift or confusing labels. We also evaluate ML-ERC with the multi-labels we produced to support our contrastive learning scheme.

Conversation Label Multiple emotions Happiness I've been looking forward to this party for so long. I'm so excited! Excited Excited Emotion Shift ùë†! Me too! Excited ENetea Let‚Äôs go into the party now! ùë†" pWaortwy! ‚ÄôTvheisaliswaeyxsacdtrlyealikme tdhoef. Hnaepspsi- Hap Exc ùë†!

# Introduction

Motivated by the introduction of AI conversational systems, such as chatbots in healthcare, recommendations, and customer services, emotion recognition in conversation (ERC) has attracted increasing attention in recent years. The ERC task aims to identify the emotion at each utterance in a conversation. While consistent improvement is being shown, there still remain challenges for improvement. Previous studies have pointed out that emotion shift (Hazarika et al. 2018; Song et al. 2022b; Tu et al. 2023) and confusing labels (Ghosal et al. 2019; Ishiwatari et al. 2020; Lee 2022) are the causes that make the ERC task difficult (Majumder et al. 2019; Li et al. 2021; Shen et al. 2021; Yang et al. 2022; Qin et al. 2023). An emotion shift in ERC occurs when the emotions of the same speaker change during the speaker‚Äôs consecutive utterances. Confusing emotion, where two similar emotions cannot be distinguished within an utterance, is another challenge in ERC. Recent studies have proposed models to address these issues by introducing emotion shift detection module (Gao et al. 2022), using curriculum learning (Yang et al. 2022) to better handle emotion shift, and constructing grayscale labels (Lee 2022).

These two problems, emotion shift and confusing labels, arise from the practice of annotating a single emotion label to an utterance overlooking that an utterance can encompass multiple emotions. As illustrated in Figure 1, each utterance within a conversation is assigned to a single label. Although an utterance can encompass multiple emotions (right), only the most intense emotion is retained and other emotions are discarded. In fact, organizing emotions in 2D or circular arrangements that originated from a study in the 1950s (Schlosberg 1952) has become more common. The studies from psychology (Russell 1980; Mikels et al. 2005) use valence-arousal 2D emotion space to describe emotions. From these perspectives, restricting each utterance to one emotion label is oversimplified. Mikels et al. (2005) pointed out that emotions are often blended and expressed through behaviors or utterances, which has motivated a new approach to capturing mixed-emotions. Regardless of these studies, however, the emotion recognition task in ERC still remains in predicting the single label.

To address the aforementioned challenges, we propose a model called Multi-Label classification for Emotion Recognition in Conversation (ML-ERC). Although all previous ERC models attempted to predict a single-label for each utterance, we switch this same ERC task to multi-label (emotion) prediction. Since multi-label annotation requires significant time and efforts, we thus propose a pseudo multilabel assignment strategy without additional cost for multiemotion labels. The self-annotation scheme is devised based on the studies in human emotions (Kuppens, Allen, and Sheeber 2010; Koval et al. 2015) and inductive reasoning. Specifically, when assigning the additional emotion labels, we conversely make use of the emotion shift, which previously had a negative impact.

We employ the supervised contrastive learning (SupCon) (Khosla et al. 2020) scheme for our multi-label ERC task. However, given that SupCon is originally designed for single supervisory signals, it cannot be directly extended to multi-label settings. To bridge this gap, we introduce a novel multi-label weighted supervised contrastive loss, MulWCL, designed to better account for multi-label tasks. This objective makes multi-label classification more effective, and surprisingly it also contributes to performance improvements in single-label classification by mitigating the challenges from emotion shift and confusing labels prevalent in ERC field. Our contribution is three-fold.

‚Ä¢ We approach ERC by utilizing a multi-label to address the two problems: emotion shift and confusing labels. For this new approach, pseudo multi-labeling scheme for multi-label is introduced.   
‚Ä¢ Our ML-ERC incorporates weighted supervised contrastive loss to consider the characteristics of multi-label classification, and employs a soft multi-labeling method within the module to facilitate the training process.   
‚Ä¢ We conduct extensive experiments to verify the effectiveness of our proposed model. We integrate our multi-label scheme into existing single label ERC models, and show how our objective improves all of the existing baseline models.

# Related Work

Emotion recognition in conversation Previous works on emotion recognition in textual conversation can be summarized into three methods: Recurrence-based, Graphbased, and Knowledge-enhanced methods. Recurrencebased works (Hazarika et al. 2018; Majumder et al. 2019; Hu et al. 2023) consider utterances as sequential data. Graphbased models (Ghosal et al. 2019; Ishiwatari et al. 2020; Shen et al. 2021) represent the relationship of an utterance using a graph. The knowledge-enhanced models improve the performance of ERC by associating external knowledge (Ghosal et al. 2020; Zhu et al. 2021; Lee and Lee 2022). There are also methods other than these three. Yang et al. (2022) improve performance by applying hybrid curriculum learning. Gao et al. (2022) propose a multi-task learning framework that employs emotion shift detection as an auxiliary task and ERC as the main task. Lee (2022) attempts to understand emotion using the grayscale label. Zhang et al. (2023) mimic human thinking through the use of prompts and paraphrasing. Recently, several works (Song et al. 2022a; Yang et al. 2023; Hu et al. 2023; Kang and Cho 2024) utilize contrastive learning to effectively learn emotion.

Multi-label classification Multi-label classification has gained continuous attention in the field of NLP due to its

<html><body><table><tr><td>Model</td><td>original F1</td><td>w/o ES</td><td>only ES</td></tr><tr><td>DialogueRNN</td><td>62.75</td><td>69.2(+6.45)</td><td>47.5(-15.25)</td></tr><tr><td>GloVe bcLSTM</td><td>61.90</td><td></td><td>52.37(-9.53)</td></tr><tr><td>TODKAT</td><td>62.60</td><td>64.62(+2.02)</td><td>56.24(-6.36)</td></tr><tr><td>TODKAT+HCL</td><td>63.03</td><td>67.01(+3.98)</td><td>56.91(-6.12)</td></tr></table></body></html>

Table 1: Impact of emotion shift on the performance of ERC. IEMOCAP dataset was used for the evaluation. The ‚Äòonly ES‚Äô, ‚Äòw/o ES‚Äô represent utterances with emotion-shift and without emotion-shift, respectively.

practical applications (Nam et al. 2017; Zhang et al. 2021). Particularly, multi-label emotion classification (ER) has seen active research progress (Alhuzali and Ananiadou 2021; Lin et al. 2023) with the release of datasets, which provide multilabel emotion annotations for tweets (Mohammad et al. 2018) and multimodal language (Zadeh et al. 2018). However, they are quite different from ERC.

Since dialogue involves many complex factors, multilabel annotation on each utterance in ERC is challenging and requires human-intensive resources. MEISD dataset (Firdaus et al. 2020) marks a pioneering effort to introduce multi-label annotations in ERC. Zhao et al. (2022) propose multi-label emotional dialogue in chinese. Despite their efforts, studies in ERC still remain in single classifications with single-labeled datasets. To the best of our knowledge, yet there are no models in the literature for multilabel classification in ERC. In this study, we introduce a pseudo-label strategy that can build multi-labels from existing datasets. We further introduce a multi-label classification model which well reflects the nature of emotions in ERC.

Multi-label contrastive learning Supervised contrastive learning (SupCon) (Khosla et al. 2020) has contributed to performance improvements in NLP (Gunel et al. 2020; Lin et al. 2022). However, it is not readily applicable to multilabel instances since this learning approach assumes that the sample has single label. Consequently, there are few investigations for contrastive learning for multi-label in the NLP field. Su, Wang, and Dai (2022) introduce multi-label contrastive learning based on the label similarity of multi-label instances. Lin et al. (2023) propose five contrastive losses designed for multi-label text classification. These works primarily focus on using the label correlations across instances. In this work, we consider a multi-label contrastive learning approach from two perspectives: the class-level and the instance-level.

# Proposed Approach Motivation for Multi-label ERC

Many ERC studies have discussed emotion shift and confusing emotion. Here we investigate how these phenomena affect classification performance in ERC. The first is the issue of emotion shift, which refers to the transition of emotions. Table 1 summarizes the experimental results from several studies (Majumder et al. 2019; Ghosal et al. 2021; Yang et al. 2022), revealing the influence of emotion shift on performance. According to Table 1, F1 score always increases when emotion shifts are taken out from the test data. When performing evaluations on emotion shifts only, we observe considerable performance degradation, which is up to $1 5 \%$ . The second key challenge is the confusing labels. Previous methods (Majumder et al. 2019; Xie et al. 2021; Shen et al. 2021; Li et al. 2021) often failed to distinguish subtle differences between certain emotions, such as excited-happy and anger-frustrated. Due to the vague boundaries of confusing emotions, many models struggle to classify these emotions.

![](images/99b6af4a7ba3e64c34d9ffd96b48c1e6ca0a2d055d1fd34d06696d2c8abbebaa.jpg)  
Figure 2: The t-SNE visualization on test set of IEMOCAP dataset. Data points marked with a black border indicates samples where emotion shift occured. The marking denotes that each label dominates this space.

Generating pseudo multi-emotion labels In this study, we tackle the challenges in ERC with multi-label classification. In multi-label classification, each instance can be associated with multiple labels, which means, in ERC, each utterance can be annotated with multiple emotion labels. However, the current benchmark datasets are labeled with single emotions. To fill this gap, we propose a self annotation scheme for generating pseudo multi-emotion labels without incurring additional costs.

For the automatic annotation strategy, we specifically employ emotion inertia (Kuppens, Allen, and Sheeber 2010; Koval et al. 2015), a concept from psychological theory describing the resistance to changes in emotions. When an emotional change occurs, the preceding emotion‚Äôs influence results in the persistence of that emotional state. Based on emotion inertia, the pseudo labels are aligned to emotion shift allowing two emotions (old and new) to coexist at every emotional shift. To support our approach, we visualize the embeddings of utterances from IEMOCAP, a benchmark dataset for ERC, using RoBERTa-large as the embedding module. As illustrated in Figure 2, data points with emotion shifts are primarily located within or near the overlapping regions of each emotion. These observations suggest that emotion-shift data may simultaneously encompass multiple emotions.

Our pseudo multi-label scheme can generate multiple labels for each utterance from the existing dataset with single labels without any human effort for labeling. If the same speaker‚Äôs present utterance and the previous utterance have different labels in conversation, and none of which are neutral, we target the current utterance for multi-label annotation. When the specified conditions are satisfied, we generate pseudo multi-labels by aggregating two emotions from the previous and current utterances of the same speaker; otherwise, we keep the annotation as single. We also provides additional examples applying our scheme to the existing dataset in Appendix B (see Figure S1 and Table S1).

# Multi-label ERC Model

Problem definition Each ERC dataset consists of multiple independent conversations, where each conversation is a sequence of utterances attached to speaker and emotion: $C = \bar { \{ ( u _ { i } , s _ { i } , y _ { i } ) \} } _ { i = 1 } ^ { N }$ . Here $s _ { i } , y _ { i }$ represent the speaker and label of $u _ { i }$ and $N$ denotes the number of utterances in a conversation. When target utterance $( u _ { t } , s _ { t } )$ and its context $\{ ( u _ { i } , s _ { i } ) \} _ { i = 1 } ^ { t - 1 }$ are given, the goal of single-label classification of ERC is to predict the emotion label $( y _ { t } )$ of target $\boldsymbol { u } _ { t }$ in the predefined label set $K = \{ k _ { 1 } , k _ { 2 } , \ldots , k _ { | K | } \}$ .

Motivated by our findings, we approach the problem from the point of view of multi-label classification task by associating an utterance to multiple labels. We restructure the dataset by adding multi-hot label $\mathbf { y } ^ { \mathrm { p s e u d o } }$ to the existing data and expand $C$ to $C = \{ ( u _ { i } , s _ { i } , y _ { i } , \mathbf { y } _ { i } ^ { \mathrm { p s e u d o } } ) \} _ { i = 1 } ^ { N }$ . We define $\mathbf { y } _ { i } ^ { \mathrm { p s e u d o } } = \{ k _ { i } ^ { 1 } , k _ { i } ^ { 2 } , \dotsc , k _ { i } ^ { | K | } \} \in \mathbb { Z } _ { 2 } ^ { | K | }$ , where each emotion $k ^ { j } \in \{ 0 , 1 \}$ . The value $k _ { i } ^ { j } = 1$ indicates that the $i$ -th utterance has emotion $k _ { j }$ . Throughout this paper, we rename the current emotion label as the main emotion and define the additional emotion labels from the multi-label settings as the sub emotions.

Embedding module We bring RoBERTa-Large (Liu et al. 2019), a pre-trained language model (PLM), as an embedding module. For each utterance, we prepend its respective speaker and concatenate it with the context of the current utterance. A special token [CLS], which reflects context information, is placed at the beginning of this sequence. In embedding stage, the input and output of $u _ { i }$ are as follows:

$$
\mathrm { R o B E R T a } ( [ \mathrm { C L S } ] , s _ { 1 } , u _ { 1 } , \dots , s _ { i } , u _ { i } ) = \mathbf { h } _ { i }
$$

,where $\mathbf { h } _ { i } \in \mathbb { R } ^ { 1 \times d _ { h } }$ is the embedding of [CLS] token of $u _ { i }$ in the last hidden layer.

Multi-label prediction module In multi-label setting, more than one labels can coexist in one sample. Therefore, given the $\mathbf { h } _ { i }$ , embedding of $u _ { i }$ , our model predicts the multilabel of $u _ { i }$ following Equation 2 ‚Äì 4.

$$
\begin{array} { c } { \mathbf { z } _ { i } = \operatorname { L i n e a r } ( \mathbf { h } _ { i } ) , } \\ { \tilde { \mathbf { z } } _ { i } = \operatorname { t a n h } ( \mathbf { z } _ { i } ) , } \end{array}
$$

$$
{ \bf o } _ { i j } = \left\{ \begin{array} { l l } { { 1 } } & { { \mathrm { i f } \tilde { z } _ { i j } > \mathrm { m e a n } ( \tilde { \bf z } _ { i } ) } } \\ { { 0 } } & { { \mathrm { o t h e r w i s e } } } \end{array} , j \in \{ 1 , . . . , | K | \} \right.
$$

, where $\mathbf { z } _ { i } ~ \in ~ \mathbb { R } ^ { 1 \times | K | }$ and $\mathbf { o } _ { i j }$ represent the prediction for label $k _ { j }$ of data $u _ { i }$ . Here we use calibrated threshold (Hou et al. 2021) by taking the mean of each component in $\tilde { \mathbf { z } }$ .

ML-ERC learning objectives Supervised contrastive learning (SupCon) (Khosla et al. 2020) pulls data points with same label closer to the anchor while repelling negative samples from the anchor. However, SupCon cannot be directly used in the multi-label setting for two reasons. First, finding the positive and negatives pairs with multi-label vector is too complex as the size of label set increases. Second, as the multi-label can coexist within a sample, it causes blurring effect (Lin et al. 2023), where the multiple emotions cause overlapping positive and negative pairs. To address this, we introduce a multi-label weighted supervised contrastive loss, MulWCL, designed specifically for a task with multi-labels.

![](images/dd371a5b26c819d00292b0fdfaa198c32b8bb63160e16c1dba2fb33376981670.jpg)  
Figure 3: Framework of multi-label weighted supervised contrastive loss (MulWCL). The thickness of the lines indicating the intensity of the repelling force and different colors of the sample represent different labels.

For the first challenge, we redefine the positive and negative sets in our settings. Given the extensive range of possible label combinations in multi-label settings, finding a positive pair with fully matching labels is challenging. Therefore, we define the positive set $P ( i )$ as instances sharing a main emotion with the anchor. For negative set of sample $i$ , we introduce two distinct sets: $A _ { m } ( i )$ and $V _ { m } ( i )$ .

$$
\begin{array} { r } { P _ { m } ( i ) = \{ p | y _ { p } = y _ { i } , p \neq i \} . \qquad } \\ { \mathbf { m } _ { i } = \{ k _ { j } | k _ { i } ^ { j } \neq 0 , k _ { i } ^ { j } \in \mathbf { y } _ { i } ^ { \mathrm { p s e u d o } } \} , \qquad } \\ { A _ { m } ( i ) = \{ a | y _ { a } \notin \mathbf { m } _ { i } , a \neq i \} , \qquad } \\ { V _ { m } ( i ) = \{ v | y _ { v } \in ( \mathbf { m } _ { i } - y _ { i } ) , v \neq i \} . \qquad } \end{array}
$$

$A _ { m } ( i )$ is the true negative set comprised of samples whose labels do not belong to the multi-label of sample $i$ . $V _ { m } ( i )$ is a set comprising samples not matching the main label of sample $i$ but aligning with one of sub emotions of sample $i$ . While the vanilla SupCon pushes $V _ { m }$ away with the same force as it does samples in true negative set $A _ { m }$ , we reduce the weight of repulsion in $V _ { m }$ . The intuition behind this is that any pair of samples from multi-label set $V _ { m }$ can have looser repulsion than the samples drawn from the true negative set $A _ { m }$ in the latent space. When a sample takes multiple emotions, an emotion can be both positive and negative to a given sample, which causes the second challenge, blurring effect. Since we categorize the samples into a positive set and two negative sets, our MulWCL ensures that there is no overlap between the three sets.

We design two weights from different perspectives: classlevel and instance-level. The samples exhibiting multiple emotions pose challenges in representing each emotion clearly, compared to samples characterized by a single label. Thus, we apply weights tailored to the multi-label context to achieve better distinctions between emotions. We design the class-level weighted score for the multi-label set $V _ { m }$ by reflecting the similarity between the anchor‚Äôs main emotion and sub emotion in $V _ { m }$ . We adjust the repelling force, reducing it as the similarity between labels increases. The calculations for both positive and negative scores are presented in Equations 8 and 9.

$$
\begin{array} { r l } & { \mathcal { P } _ { \mathrm { m u l t i } } ( i ) = \displaystyle \sum _ { p \in P _ { m } ( i ) } \exp ( \sin ( \mathbf { h } _ { i } , \mathbf { h } _ { p } ) / \tau ) , } \\ & { \mathcal { N } _ { \mathrm { m u l t i } } ( i ) = \displaystyle \sum _ { a \in A _ { m } ( i ) } \exp ( \sin ( \mathbf { h } _ { i } , \mathbf { h } _ { a } ) / \tau ) } \\ & { ~ + \displaystyle \sum _ { v \in V _ { m } ( i ) } \underbrace { ( 1 - \frac { ( \sin ( r _ { i } , r _ { v } ) + 1 ) } { 2 } ) } _ { \mathrm { c l a s s w e i g h t } } \cdot \exp ( \sin ( \mathbf { h } _ { i } , \mathbf { h } _ { v } ) / \tau ) , } \end{array}
$$

where the $\mathrm { s i m } ( \cdot )$ calculates similarity between two samples and we use a cosine similarity. $\tau$ is a temperature parameter. $r$ indicates label representation in Valence-Arousal dimension obtained from previous studies (Yang et al. 2022, 2023). Further details regarding the relation between emotions can be found in the Appendix B.

Furthermore, we calculate the instance-level weighted score, leveraging entropy measure. We try to capture patterns more from the samples with distinct emotions while paying less attention to the samples with mixed emotions. Thus, we apply small weights to the samples with high entropy.

$$
e _ { i } = \frac { - \sum _ { j = 1 } ^ { | K | } \sigma ( \mathbf { z } _ { i } ) _ { j } \log \sigma ( \mathbf { z } _ { i } ) _ { j } } { \log | K | } .
$$

$$
\mathcal { L } _ { \mathrm { M u l W C L } } ( i ) = \underbrace { ( 1 - e _ { i } ) } _ { : \mathrm { m e t a t i } } \left( \frac { - 1 } { | P _ { m } ( i ) | } \log \frac { \mathcal { P } _ { \mathrm { m u l t i } } ( i ) } { \mathcal { N } _ { \mathrm { m u l t i } } ( i ) } \right) .
$$

in|st c{ezweig}ht

Entropy is calculated in Equation 10 and $\sigma$ is softmax function. The entropy inverted to serve as weights. These instance-wise weights adjust the attraction within positive pairs and the repulsion within negative pairs. We highlight that our loss function, $\mathcal { L } _ { \mathrm { M u l W C L } }$ , applies both class-wise and instance-wise weights through label similarity in Equation 9 and entropy in Equation 11. Finally, the overall loss we optimize is presented below.

$$
\mathcal { L } _ { \mathrm { M L - E R C } } = \mathcal { L } _ { b c e } + \alpha \mathcal { L } _ { \mathrm { M u l W C L } } ,
$$

where $\mathcal { L } _ { b c e }$ is the binary cross-entropy loss and $\alpha$ is hyperparameter that controls the effect of our weighted multi-label loss.

Soft multi-labeling As multi-label annotation is timeconsuming and expensive, we assign pseudo multi-label on the data where emotion shifts occur. However, we speculate that there still could be more utterances with multiple emotions that our pseudo labeling scheme has missed. We additionally introduce soft multi-labels annotated to the potential utterances using the embeddings learned from the model. To enhance the efficacy of soft-labeling, we employ the entropy calculated in Equation 10 as criterion for soft-label generation. This approach aims to avoid incorrectly applying multilabeling to data that is unlikely to exhibit multiple emotions.

<html><body><table><tr><td>Algorithm1: Learning procedure of ML-ERC for each batch B at each epoch. Once the model runs several iterations, we conduct soft-labeling.</td></tr><tr><td>Input: B={(ui,Si,yi,yPseudoÔºåpsof =yPseudo)}; K = single label set; Output: LML-ERC; Bnew = batch updated with soft-labeling Bnew = [] fori=1,.,Nb do 0i={0ij =0Ôºâ1 hi=RoBERTa(ui,Si,context) Zi = Normalize(hi) ; //Eq 2Ôºå3 forj=1,...,Kdo if mean(zi) < zij then Oij=1 end end calculate entropy ei ; // Eq 10 Pnew =yps if count({x ‚â† 0 |x ‚àà ypseudo}) == 1 then ifŒ≥<eithen ‰∏®Pnew=Oi pseudo</td></tr></table></body></html>

$$
\mathbf { p } _ { i } ^ { \mathrm { s o f t } } = \left\{ \begin{array} { l l } { \mathbf { o } _ { i } } & { \mathrm { i f ~ } \left( \sum _ { j = 1 } ^ { | K | } \mathbf { 1 } ( y _ { j } ^ { \mathrm { p s e u d o } } \neq 0 ) = 1 \right) } \\ { \mathbf { y } _ { i } ^ { \mathrm { p s e u d o } } } & { \mathrm { o t h e r w i s e } } \end{array} \right.
$$

, where ${ \bf p } _ { i } ^ { \mathrm { s o f t } }$ and $\mathbf { o } _ { i }$ represent the soft multi-label and the multi-label prediction of sample $i$ , respectively. The $\mathbf { 1 } ( \cdot )$ is an indicator function, and $\gamma$ is a hyperparameter that controls the threshold for soft-label assignment.

If the entropy of the data exceeds a pre-defined value $( \gamma )$ , we additionally assign the multi-labels $\mathbf { o } _ { i }$ predicted for $u _ { i }$ in Equation 4 as its soft-labels. These soft-labels are then used as pseudo multi-labels for the corresponding data in the next training epoch. It is worth noting that soft-labeling is applied exclusively during the training phase and is not used during the inference stage. To ensure the quality of the softlabels, we initially train our model for a sufficient number of iterations until the model becomes reliable, and integrate the soft-label strategy into the training process. This kind of strategy can be seen in other works (Wang et al. 2020). Our ML-ERC is outlined in the Algorithm 1.

# Experimental Settings

Data We conduct experiments on three benchmark ERC datasets annotated with single labels. The statistics for each dataset are provided in Table S4 in Appendix F. For the ERC task, only text scripts are being used. EmoryNLP (Zahiri and Choi 2018) is labeled with joyful, mad, neutral, peaceful, powerful, scared, and sad from the Feeling wheel (Willcox 1982). MELD (Poria et al. 2019) is a multi-modal dataset with a label set that includes anger, disgust, fear, joy, neutral, surprise, and sadness. IEMOCAP (Busso et al. 2008) is a dyadic multimodal dataset with labels including excited, neutral, frustrated, sadness, happiness, and anger.

Baselines We bring representative methods in ERC : recurrence-based DialogueRNN (Majumder et al. 2019), graph-based DAG-ERC (Shen et al. 2021), knowledgebased CoMPM (Lee and Lee 2022), and PLM based MPLP (Zhang et al. 2023). Furthermore, we implement the RoBERTa-large model (Liu et al. 2019) by adding a classification layer to the top of the embedding.

Training setup To train our ML-ERC method, we set the learning rate, the number of batch sizes and epochs are 1e6, 16 and 30, respectively. We fix $\tau$ in $\operatorname { E q } 8 , 9$ to 0.05. For $\alpha$ in Eq 12, we search the parameter using the validation set. We set $\alpha$ to 0.7 for Emorynlp, 0.1 for MELD, and 0.4 for IEMOCAP. All experiments are performed on an Nvidia RTX A6000 GPU. Further details on parameters are provided in Appendix E.

# Experimental Results and Analysis

Our approach using multi-label to ERC benchmark dataset is new in the ERC literature. We therefore evaluate our model in two ways to verify the effectiveness of approach: singlelabel classification and multi-label classification.

# Results of Single-label Classification

Previous ERC methods have suffer from the problems of emotion shift and confusing labels, which are the motivation of our work. Here, we verify how our multi-label scheme mitigates the aforementioned challenges.

Implementation and evaluation details We capture the final representation of the data and the loss values directly from baseline ERC models. The embeddings generated by the ERC models are then leveraged to compute the multilabel loss in Equation 12. We integrate the multi-label loss values $( \mathcal { L } _ { \mathrm { M L - E R C } } )$ with the original loss from ERC model $( \mathcal { L } _ { \mathrm { E R C } } )$ through the hyperparameter $\beta$ , which is set to 0.5, and optimize a given model. Thus, the final loss for singlelabel classification is expressed as $\mathcal { L } ~ = ~ \beta \mathcal { L } _ { \mathrm { E R C } } + ( \bar { 1 } ~ - ~$ $\beta ) \mathcal { L } _ { \mathrm { M L - E R C } }$ . We maintain the hyperparameter settings as defined by the original models without conducting any additional tuning when integrating our objective for fair comparison. Following previous ERC studies, we use the weighted F1-score as our evaluation metric.

Effect of multi-label objectives Table 2 shows the efficacy of multi-label objective. All of the results are reproduced using the original code. The ERC models attain a performance boost through our loss. We consistently achieve performance improvements on all the baselines across three datasets. It is worth noting that we inject a multi-label perspective to ERC models without additional training data. We speculate that our method takes advantage of the rich information overlooked by previous single-label approaches.

Table 2: Experiment results in single-label classification.   

<html><body><table><tr><td rowspan="2">Made1</td><td rowspan="2">Loss</td><td colspan="3"></td></tr><tr><td>EMORY</td><td>DELDet</td><td>IEMOCAP</td></tr><tr><td rowspan="2">RoBERTa</td><td>LERC</td><td>33.08</td><td>64.27</td><td>63.58</td></tr><tr><td>+LML-ERC</td><td>35.74</td><td>65.15</td><td>63.96</td></tr><tr><td rowspan="2">DialogueRNN</td><td>LERC</td><td>37.44</td><td>57.03</td><td>62.75</td></tr><tr><td>+ LML-ERC</td><td>38.18</td><td>57.51</td><td>63.56</td></tr><tr><td rowspan="2">DAG-ERC</td><td>LERC</td><td>38.85</td><td>63.38</td><td>67.07</td></tr><tr><td>+ LML-ERC</td><td>39.02</td><td>63.58</td><td>68.12</td></tr><tr><td rowspan="2">CoMPM</td><td>LERC</td><td>36.20</td><td>64.87</td><td>66.47</td></tr><tr><td>+ LML-ERC</td><td>37.35</td><td>65.90</td><td>68.00</td></tr><tr><td rowspan="2">MPLP</td><td>LERC</td><td></td><td>65.45</td><td></td></tr><tr><td>+ LML-ERC</td><td></td><td></td><td>65.03</td></tr></table></body></html>

<html><body><table><tr><td rowspan="2">Madel</td><td rowspan="2">Loss</td><td colspan="3"></td></tr><tr><td>EMORY</td><td>DELDet</td><td>IEMOCAP</td></tr><tr><td rowspan="2">RoBERTa</td><td>LERC</td><td>29.17</td><td>51.37</td><td>45.01</td></tr><tr><td>+LML-ERC</td><td>31.37</td><td>55.15</td><td>48.20</td></tr><tr><td rowspan="2">DialogueRNN</td><td>LERC</td><td>36.81</td><td>41.87</td><td>46.86</td></tr><tr><td>+ LML-ERC</td><td>37.97</td><td>41.82</td><td>53.41</td></tr><tr><td rowspan="2">DAG-ERC</td><td>LERC</td><td>35.81</td><td>50.88</td><td>63.25</td></tr><tr><td>+ LML-ERC</td><td>35.01</td><td>50.95</td><td>63.78</td></tr><tr><td rowspan="2">CoMPM</td><td>LERC</td><td>22.50</td><td>50.03</td><td>46.14</td></tr><tr><td>+ LML-ERC</td><td>29.79</td><td>53.45</td><td>45.60</td></tr><tr><td rowspan="2">MPLP</td><td>LERC</td><td></td><td>52.45</td><td>49.34</td></tr><tr><td>+ LML-ERC</td><td></td><td>55.28</td><td>53.78</td></tr></table></body></html>

Table 3: Experiment results in emotion shift data.   

<html><body><table><tr><td rowspan="2">Base Model</td><td rowspan="2">Objective</td><td colspan="5">Confusing labels(‚Üì)</td></tr><tr><td>Peaceful - Happy</td><td>Powerful - Happy</td><td>Sad-Fear</td><td>Angry - Frustrated</td><td>Excited - Happy</td></tr><tr><td rowspan="2">RoBERTa</td><td>LERC</td><td>34.14</td><td>38.96</td><td>28.67</td><td>26.39</td><td>40.77</td></tr><tr><td>+LML-ERC</td><td>33.60 (-0.54)</td><td>32.37 (-6.59)</td><td>24.81 (-3.86)</td><td>23.24 ( -3.15)</td><td>24.44 ( -16.33)</td></tr><tr><td rowspan="2">CoMPM</td><td>LERC</td><td>33.25</td><td>34.32</td><td>30.89</td><td>28.20</td><td>29.10</td></tr><tr><td>+LML-ERC</td><td>26.94 (-6.31)</td><td>33.91 (-0.41)</td><td>24.97 (-5.92)</td><td>22.16 (-6.04)</td><td>25.66 ( -3.44)</td></tr><tr><td>DAG-ERC</td><td>LERC +LML-ERC</td><td>24.07 ( +0.69)</td><td>32.15 ( -3.79)</td><td>25.52 (-0.10)</td><td>19.95 ( -0.21)</td><td>33.01(-0.77)</td></tr></table></body></html>

Table 4: The misclassified rate (lower the better) as confusing labels on ERC datasets. We select confusing labels, which have a similarity of 0.6 or higher between two emotions in Figure S2 in Appendix.

Performance on emotion shift We perform single-label classification on selected test data which involves emotion shift, which is presented in Table 3. Compared to the performance of the full dataset in Table 2, Table 3 shows a significant performance drop ranging from $3 \%$ to $20 \%$ on the emotion shift data. These results show that traditional ERC methods are vulnerable to classify samples with emotion shift. However, incorporating our multi-label objective into the training of ERC models enhances performance across most evaluations, while some results slightly underperform the baselines.

Performance on confusing labels Table 4 shows the proportion which incorrectly classified as confusing emotions compared with the true label. Confusing emotions are closely positioned in embedding space, highly co-occurring. Training with single labels forces multiple emotions into one emotion, bringing similar emotions closer in the embedding space. Our MulWCL effectively reduces the overlapping areas between these confusing emotions. Thus, models trained with our method classify emotions in similar spaces better (less confused) than original model.

# Results of Multi-label Classification

In previous section, we demonstrated that our multi-label approach allviates the issues in ERC and achieves performance improvements. In this section, we evaluate the multi-label classification performance of our proposed framework.

Implementation and evaluation details In ERC, research on multi-label classification is rudimentary. Thus, we apply our multi-label prediction module to single-label baseline models. We calculate the embedding values for each utterance through the ERC models, and then predict the multilabels for each embedding using a calibrated threshold in equation 4. 1 As evaluation metrics, we choose weighted F1 scores and macro F1 score. We additionally use metrics used in other multi-label tasks, such as AUC (Area Under the Curve) and hamming loss.

Multi-label classification performance The results are reported in Table 5, where we compare ML-ERC against other multi-label performance extended from baselines. Our ML-ERC outperforms the multi-label performances of baseline models. The ERC models trained on single-label datasets tend to overfocus on tracking the strongest signal while disregarding the probabilities of other emotional signals. Thus, extending the single-label ERC to multi-label by using a calibrated threshold still exhibits limited performance on multi-label classification.

Ablation study We first strip independently two components, MulWCL and soft multi-labeling in Table 6. The results demonstrate that omitting MulWCL consistently leads to performance degradation, highlighting its vital importance in ML-ERC. The absence of soft multi-labeling leads to reduced performance on both the EmoryNLP and IEMOCAP datasets. These results reveal that the two modules designed for multi-label classification effectively enhance performance in both multi-label and single-label classification. For extended experiments on MulWCL and soft multilabeling, see Appendix C.

Table 5: Experiment results in multi-label classification. M-F1, W-F1, and HL represent macro-F1 $( \% )$ , weighted-F1 $( \% )$ and hamming loss, respectively. Higher macro-F1, weighted-F1, and AUC, along with lower hamming loss, indicate better performance. We highlight the best performance among the main results in bold. All results are reproduced using their respective official codebase.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="4">EmoryNLP</td><td colspan="4">MELD</td><td colspan="4">IEMOCAP</td></tr><tr><td>M-F1(‚Üë)</td><td>W-F1(‚Üë)</td><td>AUC(‚Üë)</td><td>HL()</td><td>M-F1(‚Üë)</td><td>W-F1(‚Üë)</td><td>AUC(‚Üë)</td><td>HL()</td><td>M-F1(‚Üë)</td><td>W-F1(‚Üë)</td><td>AUC(‚Üë)</td><td>HL(‚Üì)</td></tr><tr><td>DialogueRNN</td><td>32.80</td><td>40.62</td><td>0.583</td><td>0.2388</td><td>34.37</td><td>56.61</td><td>0.612</td><td>0.1622</td><td>61.77</td><td>62.95</td><td>0.758</td><td>0.1440</td></tr><tr><td>DAG-ERC</td><td>36.47</td><td>41.33</td><td>0.586</td><td>0.3647</td><td>39.13</td><td>55.91</td><td>0.649</td><td>0.3038</td><td>58.34</td><td>57.97</td><td>0.772</td><td>0.2814</td></tr><tr><td>CoMPM</td><td>37.76</td><td>39.74</td><td>0.617</td><td>0.4018</td><td>39.80</td><td>55.12</td><td>0.714</td><td>0.2720</td><td>57.06</td><td>58.19</td><td>0.808</td><td>0.2803</td></tr><tr><td>MPLP</td><td></td><td>1</td><td></td><td>1</td><td>41.71</td><td>55.49</td><td>0.691</td><td>0.2466</td><td>59.39</td><td>59.99</td><td>0.811</td><td>0.2421</td></tr><tr><td>ML-ERC</td><td>38.69</td><td>41.56</td><td>0.630</td><td>0.2535</td><td>50.03</td><td>63.01</td><td>0.718</td><td>0.1250</td><td>68.58</td><td>69.17</td><td>0.815</td><td>0.1312</td></tr></table></body></html>

Table 6: Ablation study. S-F1 is weighted F1-score. S-F1 is the result of single-label classification, whereas M-F1, W-F1, AUC and HL are the outcomes of multi-label classification.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="5">EmoryNLP</td><td colspan="5">MELD</td><td colspan="5">IEMOCAP</td></tr><tr><td>S-F1(‚Üë)M-F1(‚Üë)W-F1(‚Üë)AUC(‚Üë)HL(‚ÜìÔºâ S-F1(‚Üë) M-F1(‚Üë)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>W-F1(‚Üë)AUC(‚Üë)HL(‚Üì) S-F1(‚Üë)M-F1(‚Üë)W-F1(‚Üë) AUC(‚Üë) HL(‚Üì)</td><td></td><td></td><td></td><td></td></tr><tr><td>ML-ERC</td><td>35.74</td><td>38.69</td><td>41.56</td><td>0.630</td><td>0.253</td><td>65.15</td><td>50.03</td><td>63.01</td><td>0.718</td><td>0.125</td><td>63.96</td><td>68.58</td><td>69.17</td><td>0.815</td><td>0.131</td></tr><tr><td>w/o MulWCL</td><td>34.85</td><td>37.72</td><td>40.70</td><td>0.625</td><td>0.253</td><td>64.21</td><td>48.64</td><td>62.87</td><td>0.715</td><td>0.130</td><td>62.14</td><td>66.81</td><td>67.97</td><td>0.807</td><td>0.131</td></tr><tr><td>w/o Soft-label 34.75</td><td></td><td>38.05</td><td>40.88</td><td>0.624</td><td>0.256</td><td>64.16</td><td>50.16</td><td>63.28</td><td>0.721</td><td>0.126</td><td>63.29</td><td>68.42</td><td>68.96</td><td>0.8170.128</td><td></td></tr><tr><td>w/o Both</td><td>33.12</td><td>36.63</td><td>39.38</td><td>0.620</td><td>0.267</td><td>63.65</td><td>48.64</td><td>62.40</td><td>0.717</td><td>0.125</td><td>63.13</td><td>65.35</td><td>67.23</td><td>0.7960.137</td><td></td></tr></table></body></html>

<html><body><table><tr><td rowspan="2">Loss</td><td colspan="4">EmoryNLP</td><td colspan="4">MELD</td><td colspan="4">IEMOCAP</td></tr><tr><td>M-F1(‚Üë)</td><td>W-F1(‚Üë)</td><td>AUC(‚Üë)</td><td>HL(‚Üì)</td><td>M-F1(‚Üë)</td><td>W-F1(‚Üë)</td><td>AUC(‚Üë)</td><td>HL(‚Üì)</td><td>M-F1(‚Üë)</td><td>W-F1(‚Üë)</td><td>AUC(‚Üë)</td><td>HL(‚Üì)</td></tr><tr><td>BCE</td><td>34.57</td><td>37.12</td><td>0.597</td><td>0.2888</td><td>48.60</td><td>62.52</td><td>0.716</td><td>0.1256</td><td>65.41</td><td>66.28</td><td>0.816</td><td>0.1339</td></tr><tr><td>+ SupCon</td><td>36.63</td><td>39.38</td><td>0.620</td><td>0.2675</td><td>48.64</td><td>62.40</td><td>0.717</td><td>0.1258</td><td>65.35</td><td>67.23</td><td>0.796</td><td>0.1373</td></tr><tr><td>+ SCL</td><td>37.03</td><td>39.95</td><td>0.617</td><td>0.2632</td><td>48.97</td><td>62.76</td><td>0.712</td><td>0.1251</td><td>67.24</td><td>68.52</td><td>0.807</td><td>0.1325</td></tr><tr><td>+ JSCL</td><td>37.86</td><td>40.50</td><td>0.619</td><td>0.2674</td><td>49.88</td><td>62.28</td><td>0.714</td><td>0.1248</td><td>67.35</td><td>68.16</td><td>0.820</td><td>0.1339</td></tr><tr><td>+ JSPCL</td><td>33.46</td><td>35.77</td><td>0.589</td><td>0.2949</td><td>49.07</td><td>62.56</td><td>0.716</td><td>0.1261</td><td>66.18</td><td>66.98</td><td>0.821</td><td>0.1478</td></tr><tr><td>+ SLCL</td><td>17.27</td><td>23.23</td><td>0.503</td><td>0.3134</td><td>39.19</td><td>58.63</td><td>0.668</td><td>0.1496</td><td>61.12</td><td>62.11</td><td>0.792</td><td>0.1869</td></tr><tr><td>+ ICL</td><td>34.24</td><td>36.56</td><td>0.592</td><td>0.3058</td><td>49.02</td><td>63.10</td><td>0.720</td><td>0.1313</td><td>62.34</td><td>63.34</td><td>0.794</td><td>0.1698</td></tr><tr><td>+ MulWCL</td><td>38.05</td><td>40.88</td><td>0.624</td><td>0.2566</td><td>50.16</td><td>63.28</td><td>0.721</td><td>0.1260</td><td>68.42</td><td>68.96</td><td>0.817</td><td>0.1288</td></tr></table></body></html>

Table 7: Comparisons against multi-label contrastive losses. Bold score indicates the best performance, and underlined score indicates the second-best performance in each evaluation setting. All of the results in baselines are implemented with method outlined in the original paper.

Comparisons against multi-label contrastive losses To better demonstrate the effectiveness of our multi-label contrastive loss, we replace it with other learning objectives and compare the performances. Lin et al. (2023) has introduced five contrastive losses, SCL, JSCL, JSPCL, SLCL, and ICL tailored for multi-label contexts (details provided in Appendix D). We exclude pseudo labeling from our model to strictly verify the effect of multi-label contrastive loss of ours. In Table 7, we find that MulWCL outperforms other multi-label contrastive losses in most metrics.

# Conclusion

In this paper, we propose a novel ERC model for MultiLabel classification for Emotion Recognition in Conversation (ML-ERC) to tackle problems when the emotions are constrained to a single label. ML-ERC employs a novel weighted supervised contrastive learning (MulWCL) to obtain better representative embedding and a soft-labeling method to facilitate multi-label classification. The experimental results show that ML-ERC not only exhibits superior performance in multi-label classification but also achieves a performance boost for all ERC baselines by effectively mitigating the ERC challenges.