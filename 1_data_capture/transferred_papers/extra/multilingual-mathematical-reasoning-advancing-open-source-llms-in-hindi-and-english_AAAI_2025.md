# Multilingual Mathematical Reasoning: Advancing Open-Source LLMs in Hindi and English

Avinash Anand, Kritarth Prasad, Chhavi Kirtani, Ashwin R Nair, Manvendra Kumar Nema, Raj Jaiswal, Rajiv Ratn Shah

Indraprastha Institute of Information Technology, Delhi avinasha, kritarth20384, chhavi18229, ashwin20037, manvendra22038, Jaiswalp, rajivratn @iiitd.ac.in

# Abstract

Large Language Models (LLMs) excel in linguistic tasks but struggle with mathematical reasoning, particularly in nonEnglish languages like Hindi. This research aims to enhance the mathematical reasoning skills of smaller, resourceefficient open-source LLMs in both Hindi and English. We evaluate models like OpenHathi 7B, LLaMA-2 7B, WizardMath 7B, Mistral 7B, LLeMMa 7B, MAmmoTH 7B, Gemini Pro, and GPT-4 using zero-shot, few-shot chain-of-thought (CoT) methods, and supervised fine-tuning. Our approach incorporates curriculum learning, progressively training models on increasingly difficult problems, a novel Decomposition Strategy to simplify complex arithmetic operations, and a Structured Solution Design that divides solutions into phases. Our experiments result in notable performance enhancements. WizardMath 7B exceeds Gemini’s accuracy on English datasets by $+ 6 \%$ and matches Gemini’s performance on Hindi datasets. Adopting a bilingual approach that combines English and Hindi samples achieves results comparable to individual language models, demonstrating the capability to learn mathematical reasoning in both languages. This research highlights the potential for improving mathematical reasoning in open-source LLMs.

Code and Dataset — https://github.com/midasresearch/Multilingual-Mathematical-Reasoning.git Extended version — https://arxiv.org/abs/2412.18415

# Introduction

Enhancing AI systems to solve complex problems has become a crucial objective within the AI research community, particularly in the realm of mathematical question answering. While models like GPT-4 and Gemini have demonstrated their strengths in arithmetic (Zhang et al. 2024), algebra(Kao, Wang, and Hsieh 2024), scientific text generation(Anand et al. 2024d, 2023a), and symbolic manipulation(Dave et al. 2024), they are not without limitations. Our evaluations on the GSM8K (Cobbe et al. 2021) and MATH (Hendrycks et al. 2021) datasets reveal a stark contrast in their capabilities. These models perform well on the relatively straightforward GSM8K dataset, but their effectiveness significantly diminishes when tasked with the more challenging MATH dataset. This dataset includes highschool competition-level questions that require a deeper level of contextual understanding and more advanced reasoning skills. The discrepancies in performance highlight the current limitations of these models in handling complex mathematical problem-solving.

In addition to these challenges, there is a noticeable gap in the performance of large language models (LLMs) when applied to English versus non-English languages, particularly in natural language processing tasks such as question answering and classification. This gap is particularly evident in Hindi, India’s predominant language, which is used by over 105 million students according to ${ \mathrm { U D I S E } } +$ reports for 2019- $2 0 ^ { 1 }$ . Enhancing the capabilities of LLMs in Hindi is essential to make these tools more accessible and effective in subjectspecific learning contexts. While research efforts such as OpenHathi-7B (AI 2023), Hi-NOLIN (Research 2023), and Airavata (Gala et al. 2024) have made strides in adapting LLMs to the Hindi language, these models were not originally optimized for domain-specific tasks like mathematical problem-solving.

Recent advancements in open-source LLMs have shown promise in improving mathematical and physics problemsolving abilities (Anand et al. 2024a,c,b, 2023b), as evidenced by prominent models like WizardMath (Luo et al. 2023), Mistral (Jiang et al. 2023), LLeMMA (Azerbayev et al. 2023), and MAmmoTH (Yue et al. 2023). However, these advancements have largely focused on the English language, with limited performance gains observed in Hindi math datasets. Additionally, closed-source models such as GPT-4 and Gemini-Pro continue to outperform open-source models on established benchmarks like GSM8K and MATH, as well as on newly defined Hindi datasets. The significant performance disparity can be attributed to the vast difference in the number of parameters these models are trained on. While the open-source LLMs explored in this research have fewer than 10 billion parameters, well-known closed-source LLMs are trained on considerably large parameter counts. Given the constraints on computational resources, this research focuses on enhancing the performance of smaller open-source LLMs (SLLMs), acknowledging the limitations while seeking to optimize within these parameters.

![](images/6e004a6ff513697d89fc1150140824924ea0bfd943c8959a9ff654228dd1c0ae.jpg)  
Figure 1: Curriculum Learning with Structured Solutions: A Comprehensive Framework to Gradually Guide Models Through Complex Mathematical Challenges.

This research introduces several key contributions aimed at improving the mathematical capabilities of SLLMs, particularly in Hindi:

1. Introduction of the Decomposition Strategy: A novel approach designed to enhance SLLMs’ ability to solve complex mathematical operations by breaking them down into smaller, more manageable components in the enhanced HAWP dataset (see Figure 1, 2).   
2. Structured Solution Approach with Curriculum Learning: A combined methodology that integrates a structured solution framework with Curriculum Learning, as illustrated in Figure 1, 2. This approach progressively guides models through increasingly complex mathematical problems, enhancing their problemsolving abilities.   
3. Bilingual Combined Training: We propose the methodology of Bilingual Combined Training, where the Structured Solution Approach with Curriculum Learning is applied on a dataset containing both English and Hindi versions of Mathematical questions-answers.   
4. IndiMathQA Dataset Creation: We developed the IndiMathQA dataset by curating 598 math problems from NCERT 2 textbooks for grades 10-12, spanning 14 mathematical domains. Expert annotations categorized these into easy, medium, and hard problems. The dataset was expanded to 7,823 questions.

5. Performance Analysis of Multilingual LLMs: A comprehensive analysis of several LLMs, including five open-source English LLMs, three open-source Hindi LLMs, and two closed-source models, was conducted using both English and Hindi mathematical datasets to assess their capabilities and limitations.

# Related Work

Recent advances in large language models (LLMs) have significantly improved their ability to perform complex tasks, particularly in the areas of natural language processing and mathematical reasoning. However, one area that remains underexplored is the application of Curriculum Learning to these models. Originally proposed by Bengio et al. (Bengio et al. 2009), Curriculum Learning is a training strategy that mimics the way humans learn by gradually increasing the complexity of tasks presented to the model. Although widely used in deep learning, its application to LLMs has been limited, particularly in enhancing the models’ capabilities in complex, multistep reasoning tasks (Soviany et al. 2022). This section discusses various open-source and bilingual LLMs, their architectures, and the benchmark datasets used to evaluate their performance.

# Open-Source Large Language Models

Recent advancements in open-source large language models (LLMs) have demonstrated significant progress in both general and bilingual applications. Foundational models like LLaMA (Touvron et al. 2023) offer efficient fine-tuning capabilities on vast unlabeled data, while specialized models such as WizardMath (Luo et al. 2023) and MAmmoTH (Yue et al. 2023) enhance mathematical reasoning through innovative training methods and comprehensive datasets. Additionally, models like LLeMMA (Azerbayev et al. 2023)

![](images/3c7ccf318085df2a2e77f4ad85a73e18b86164a40bdf0777284f69bd2ee85535.jpg)  
Figure 2: Overall Methodology: The top section illustrates our primary approach, which combines Curriculum Learning and Bilingual Integrated Training. The bottom section depicts the process of applying the decomposition strategy to the HAWP dataset.

and Mistral (Jiang et al. 2023) leverage scientific literature and optimized architectures to achieve advanced mathematical competencies without extensive fine-tuning. In the bilingual domain, models such as OpenHathi-7B Sarvam AI (AI 2023), Hi-NOLIN (Research 2023) from the Pythia, and AIRAVATA (Gala et al. 2024) extend capabilities to English and Hindi by incorporating custom tokenizers, multilingual training datasets, and instruction-tuning techniques. These developments collectively enhance the versatility and applicability of open-source LLMs across diverse linguistic and specialized tasks.

# Benchmark Datasets

(Cobbe et al. 2021) introduced the GSM8K dataset, which consists of 8,500 grade school math problems that require basic arithmetic operations. These problems are designed to be solvable by proficient middle school students. Similarly, (Hendrycks et al. 2021) released the MATH dataset, containing 12,500 complex problems from high school competitions such as AMC 10 and AMC 12, intended for high school students. It covers topics: Algebra, Counting and Probability, Geometry, Intermediate Algebra, Number Theory, Prealgebra, and Precalculus. (Lightman et al. 2023) introduced PRM800K, a dataset with 800,000 step-level feedback labels for solutions to MATH (Hendrycks et al. 2021) problems, providing annotations (”Positive,” ”Negative,” or ”Neutral”) to each solution step. For Hindi-speaking students, (Sharma, Mishra, and Sharma 2022) released HAWP (Hindi Arithmetic Word Problems), which is the only publicly available dataset of Hindi mathematical questions. It is for grades 1 to 6, and comprises 2,336 basic math word problems requiring a single operator solution. In the next section, we will discuss more on dataset augmentation to create training dataset and their integration in overall process.

# Methodology

# Decomposition Strategy on HAWP Dataset

To improve the computational accuracy of large language models (LLMs) in arithmetic operations involving large numbers, we propose a Decomposition Strategy for multiplication and division tasks. For multiplication, this involves breaking down the multiplicand into place value components—such as hundreds, tens, and ones—and multiplying each by the other multiplicand. The products are then aggregated to obtain the final result. For division, the dividend is similarly decomposed into segments, each divided by the divisor, with the quotients summed to produce the final answer. This has been proposed to combat the poor calculation skills of open-source language models. In this paper, we focus on introducing and validating the Decomposition Strategy using the HAWP dataset, which contains basic mathematical word problems requiring single-operation calculations. This allows us to clearly demonstrate the strategy’s effectiveness in a controlled, straightforward context. While exploring its application to more complex datasets is an exciting future direction, we have chosen to concentrate on HAWP for now to ensure a thorough and focused evaluation of this novel approach.

We utilized 2,336 Hindi arithmetic problems from the HAWP dataset, covering basic operations like addition, subtraction, multiplication, and division. Since the original dataset lacked solutions, we enhanced it by generating question-answer pairs using GPT-4, which were then carefully reviewed and corrected by five human experts, resulting in the Enhanced HAWP dataset. To evaluate the Decomposition Strategy’s effectiveness, we applied it to the Enhanced HAWP dataset (see Figure 2). Manually solved examples using this strategy were used in few-shot learning with GPT-4 to modify the remaining solutions in Enhanced HAWP. These refined solutions, with $70 \%$ and $30 \%$ training/testing split, were then used to fine-tune the models OpenHathi 7B, WizardMath-v1.1 7B, and LLeMMa 7B. The resulting accuracy improvements are shown in Table 1. In our final phase, we focused on exploring the benefits of fine-tuning using an augmented version of the dataset that we previously prepared Decomposition Strategy-enhanced dataset. We expanded the original 2,336 problems to 10,000 using a one-shot prompting technique with GPT-4. These newly generated samples were carefully reviewed by five human experts for accuracy, resulting in the HMQA (Hindi Math Questions-Answers) dataset. We then used this augmented dataset to fine-tune the models OpenHathi 7B, WizardMath-v1.1 7B, and LLeMMa 7B, with the resulting accuracy compared to previous settings in Table 1.

Table 1: Performance of LLMs in Hindi Math questions using decomposition strategy. Bold values indicate improvements from the previous step. Underlined values show the highest performance of SLLMs for each operation.   

<html><body><table><tr><td>Model</td><td>Add</td><td>Sub</td><td>Mul</td><td>Div</td></tr><tr><td colspan="5">Zero-shot Prompting</td></tr><tr><td>OpenHathi-7B LLaMA-2-7B LLeMMA-7B</td><td>0.35 0.39 0.49</td><td>0.53 0.55 0.63</td><td>0.44 0.33 0.22</td><td>0.33 0.5 0.17</td></tr><tr><td>Mistral-7B WizardMath-7B Gemini-pro GPT-4</td><td>0.49 0.63 0.78 0.98</td><td>0.55 0.67 0.80 0.93</td><td>0.22 0.22 1.0 0.88</td><td>0.25 0.67 0.75 0.91</td></tr><tr><td colspan="5">Few-shot Prompting</td></tr><tr><td>OpenHathi-7B LLaMA-2-7B</td><td>0.49 0.53</td><td>0.63 0.72</td><td>0.34 0.44</td><td>0.58 0.5</td></tr><tr><td>LLeMMA-7B</td><td>0.82</td><td>0.9</td><td>1.0</td><td>0.67</td></tr><tr><td>Mistal-7B WizardMath-7B</td><td>0.78 0.72</td><td>0.77 0.73</td><td>0.56 0.56</td><td>0.83 0.67</td></tr><tr><td colspan="5">Instruction-Tuning (Enhanced HAWP)</td></tr><tr><td>OpenHathi-7B</td><td>0.78</td><td>0.85</td><td>0.22</td><td>0.50</td></tr><tr><td>LLeMMA-7B WizardMath-7B</td><td>0.78 0.96</td><td>0.83 1.0</td><td>0.67 0.78</td><td>0.67 0.75</td></tr><tr><td colspan="5">Instruction-Tuning (HAWP+Decomposition Strategy)</td></tr><tr><td>OpenHathi-7B</td><td>0.78</td><td>0.85</td><td>0.22</td><td>0.67</td></tr><tr><td>LLeMMA-7B</td><td>0.80</td><td>0.925</td><td>0.67</td><td>0.83</td></tr><tr><td>WizardMath-7B</td><td>0.95</td><td>1.0</td><td>0.78</td><td>0.83</td></tr><tr><td colspan="5">Instruction-Tuning (HMQA)</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td>0.44</td><td>0.75</td></tr><tr><td>OpenHathi-7B LLeMMA-7B</td><td>0.82 0.86</td><td>0.85 0.97</td><td>0.67</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></table></body></html>

# IndiMathQA

We have meticulously curated our own comprehensive math problem dataset, referred to as IndiMathQA, sourcing problems from the official NCERT textbooks3 used in Indian schools. This dataset contains 598 manually curated math problems and their corresponding solutions. These problems are suited for students in grades 10, 11, and 12, and it encompasses a wide range of problems that vary in complexity and span 14 major mathematical domains, including sets, trigonometry, and the binomial theorem, among others. Appendix provides more details on topic distribution.

# LLM Enhancement in Bilingual Mathematics

In this section we demonstrate the strategies used in improving mathematical reasoning skills in Bilingual settings. Our proposed strategies are namely Structured Solution Creation, Curriculum Learning, and Bilingual Training in Hindi and English. We explain the Bilingual Training Dataset Creation in two phases: (i) Classification based on Complexity (required for Curriculum Learning), (ii) Structured Solution Creation and Bilingual Translations. Finally, we demonstrate how we conducted curriculum learning based bilingual fine-tuning on our training datasets.

Classification based on Complexity We have carefully curated a collection of mathematical problems categorized into easy, medium, and hard levels. This collection includes problems from our own dataset as well as from benchmark datasets, such as GSM8K and MATH. Below, we outline the methods we used to classify each problem by its complexity. We utilize additional datasets (GSM8K and MATH) for the sole reason of having more diversity of mathematical topics in our dataset.

IndiMathQA: The IndiMathQA dataset was carefully annotated by a team of five human experts, resulting in 136 easy, 218 medium, and 244 hard questions. To ensure the reliability of these annotations, we calculated the Average Fleiss’ Kappa score, which came out to 0.58, indicating low bias and substantial agreement among the annotators. Further details on the annotation process can be found in the Appendix. This dataset was then augmented to a total of 7823 questions with similar concepts (820 easy, 2,470 medium, and 4,533 hard) using the GPT-4 API, which were then reviewed by a team of 5 human experts to correct any errors to ensure accuracy and consistency (See appendix for augmentation prompt details).

GSM8K: GSM8K is a grade-school level mathematics dataset, where all questions are generally low in complexity. However, to ensure precision in our classifications, we used LLama 3 (405B) with prompt engineering to assess and rank the questions according to their complexity, based on various criteria, including Language Understanding, Mathematical Complexity, Reasoning Complexity, Number of Variables, and Conceptual Complexity (details in the appendix). For our experiments, we selected the 700 questions with the lowest complexity as the Easy level questions.

MATH: The MATH dataset features competition-level questions for students in grades 8 through 12, with each question annotated by complexity, ranging from Level 1 (easiest) to Level 5 (hardest). For our experiments, we categorized the Level 1 questions in the set as Easy, Levels 2 and 3 as Medium, and the remaining levels as Hard. This classification resulted in 664 Easy, 3,140 Medium, and 3,994 Hard questions.

Structured Solution Generation and Language Translations LLMs often encounter challenges with hallucinations when solving reasoning tasks. In our manual inspection of base model solutions, we noticed that LLMs sometimes became so focused on solving the problem that they overlooked the underlying theoretical principles required for an accurate solution. This observation aligns with findings from (Zheng et al. 2023), which introduces a novel prompting technique that encourages LLMs to step back and ask questions to better understand the background of a problem. Inspired by this, we hypothesized that by fine-tuning our LLMs on solutions that first pause to consider the theoretical framework, we could guide them to produce more accurate responses. Building on this idea, we didn’t stop at just providing the theoretical framework; we went a step further. We designed a comprehensive, step-by-step structured solution format for fine-tuning, which we believe will train the LLM to approach reasoning tasks more methodically and with greater accuracy. To achieve this, we transformed the existing solutions in our datasets into a clear, organized format under the following headings: (i) Data Identification, (ii) Problem Analysis, (iii) Theoretical Framework, (iv) Methodology Development, (v) Computation, and (vi) Answer. (see Figure 1)

To guide this process, our team created few-shot examples that illustrate how sample answers should be divided into this structured format. These examples, along with a detailed prompt, were provided to GPT-4, which then generated structured solutions for all problems in our training and testing datasets. A team of 5 human experts then identified and corrected any mistakes in the structured solutions.

Originally, our datasets were in English. After structuring the solutions, we used LLAMA 3 (405B) to translate both the questions and their structured solutions into Hindi. Finally, the English and Hindi versions of GSM8K, MATH, and IndiMathQA are combined on the basis of Easy, Medium, and Hard. This results in a total of 2184 Easy, 5470 Medium, and 8527 Hard problems in our dataset.

Curriculum Learning based Fine-Tuning We apply the technique of Curriculum Learning (Wang, Chen, and Zhu 2021) to SLLMs, hypothesizing that by incrementally increasing the complexity of problems during fine-tuning, we can simulate the natural process of human learning—where mastering simpler tasks paves the way for tackling more challenging ones. Our approach utilizes the Easy and Medium datasets, carefully constructed to cover a diverse range of mathematical topics. Each dataset was divided into $70 \%$ for training and $30 \%$ for testing, ensuring this split was consistently applied across all problem categories: easy, medium, and hard.

To implement Curriculum Learning, we first train our SLLMs on the Easy dataset, producing a model checkpoint we refer to as SFT Easy. This checkpoint is then further fine-tuned using the Medium dataset, resulting in the final checkpoint, SFT Easy+Medium. We evaluate the performance difference between these checkpoints using testing sets from both benchmark datasets and our curated dataset.

We propose a hypothesis that fine-tuning LLMs on a dataset combining identical question-answer pairs in both English and Hindi could enhance the model’s ability to understand and reason through math problems in Hindi—a language where the LLM might not be as proficient. Our reasoning is grounded in the idea that by exposing the LLM to parallel data in English, a language it excels in, the model can leverage its strengths in English to build stronger associations and improve its performance in Hindi. To test this hypothesis, our Curriculum Learning-based fine-tuning is conducted in two distinct ways:

1. Training the SLLMs separately on English and Hindi datasets, with results presented in Table 2   
2. Employing Bilingual Combined Training, where the model is trained on a combined dataset of both English and Hindi question-answer pairs. The outcomes of this bilingual training are detailed in Table 3 and 4.

For our evaluation of SLLMs in Hindi Math reasoning, we only evaluate performance on the Hindi version of IndiMathQA and the HAWP dataset. For the purpose of clarity, we refer the Hindi version of IndiMathQA as HMKB and the English version as EMKB. (Table 2)

# Ablation Study

To comprehensively understand the results and significance of each novel methodology employed in our study, we evaluated performance at every stage. In our experiments with the Hindi dataset, Table 1 shows accuracy metrics attained by base models employing zero-shot and few-shot prompting. The findings underscore a substantial performance enhancement with few-shot prompting compared to zero-shot, demonstrating a notable increase of $20 \mathrm { - } 5 0 \%$ across all operations. This highlights the effectiveness of providing task examples to LLMs. Further, fine-tuning on an enhanced HAWP dataset led to substantial improvements $20 \text{‰}$ in general) in OpenHathi’s performance in addition and subtraction tasks, and in WizardMath’s performance across all operations. However, LLeMMA-7B’s performance declined after fine-tuning. After manual assessment of its responses, we found that it is exhibiting hallucinations in its solutions. This aligns with recent findings that fine-tuning on new knowledge can increase hallucinations (Gekhman et al. 2024). LLeMMA, primarily pre-trained on English mathematical data, showed hallucinations when provided with new Hindi mathematical knowledge. Our novel Decomposition Strategy significantly enhanced LLeMMA’s performance, demonstrating that breaking down complex calculations can reduce hallucinations and enhance reasoning skills. Additionally, addressing hallucinations through augmentation of samples proved effective, as shown by the improvements from instruction-tuning on HMQA for both OpenHathi and LLeMMA. A detailed analysis of the benefits of curriculum learning on both Hindi and English datasets is also provided in the following Results & Analysis section.

<html><body><table><tr><td rowspan="3">Models</td><td rowspan="3">Setings</td><td colspan="6">English Benchmarks</td><td colspan="4">Hindi Benchmarks</td></tr><tr><td rowspan="2">GSM8K</td><td rowspan="2">MATH</td><td rowspan="2">PRM800K</td><td colspan="3">EMKB</td><td>Enhanced</td><td colspan="3">HMKB</td></tr><tr><td>Easy</td><td>Medium</td><td>Hard</td><td>HAWP</td><td>Easy Medium</td><td></td><td>Hard</td></tr><tr><td></td><td>Base</td><td>33%</td><td>22%</td><td>27%</td><td>36%</td><td>28%</td><td>21%</td><td>19%</td><td>17%</td><td>11%</td><td>8%</td></tr><tr><td>LLaMA-7B LLeMMA-7B</td><td>Base</td><td>14%</td><td>10%</td><td>12%</td><td>14%</td><td>12%</td><td>9%</td><td>12%</td><td>11%</td><td>8%</td><td>5%</td></tr><tr><td>Mistral-7B</td><td>Base</td><td>37%</td><td>23%</td><td>29%</td><td>39%</td><td>30%</td><td>24%</td><td>25%</td><td>22%</td><td>14%</td><td>10%</td></tr><tr><td>MAmmoTH-7B</td><td>Base</td><td>24%</td><td>14%</td><td>19%</td><td>27%</td><td>13%</td><td>11%</td><td>30%</td><td>27%</td><td>22%</td><td>18%</td></tr><tr><td>WizardMath-7B</td><td>Base</td><td>71%</td><td>36%</td><td>40%</td><td>64%</td><td>48%</td><td>44%</td><td>68%</td><td>61%</td><td>46%</td><td>36%</td></tr><tr><td>LLaMA-7B</td><td>[SFT_easy]</td><td>39%</td><td>25%</td><td>28%</td><td>40%</td><td>29%</td><td>21%</td><td>24%</td><td>22%</td><td>12%</td><td>8%</td></tr><tr><td>LLeMMA-7B</td><td>[SFT_easy]</td><td>21%</td><td>11%</td><td>12%</td><td>21%</td><td>13%</td><td>9%</td><td>15%</td><td>14%</td><td>9%</td><td>5%</td></tr><tr><td>Mistral-7B</td><td>[SFT_easy]</td><td>43%</td><td>25%</td><td>31%</td><td>45%</td><td>32%</td><td>24%</td><td>30%</td><td>27%</td><td>15%</td><td>10%</td></tr><tr><td>MAmmoTH-7B</td><td>[SFT_easy]</td><td>30%</td><td>16%</td><td>22%</td><td>33%</td><td>14%</td><td>13%</td><td>41%</td><td>37%</td><td>23%</td><td>18%</td></tr><tr><td>WizardMath-7B</td><td>[SFT_easy]</td><td>79%</td><td>37%</td><td>42%</td><td>70%</td><td>52%</td><td>44%</td><td>73%</td><td>66%</td><td>47%</td><td>37%</td></tr><tr><td>LLaMA-7B</td><td>[SFT_easy+medium]</td><td>42%</td><td>35%</td><td>34%</td><td>41%</td><td>36%</td><td>24%</td><td>25%</td><td>25%</td><td>20%</td><td>16%</td></tr><tr><td>LLeMMA-7B</td><td>[SFT_easy+medium]</td><td>21%</td><td>18%</td><td>19%</td><td>21%</td><td>18%</td><td>12%</td><td>15%</td><td>15%</td><td>11%</td><td>10%</td></tr><tr><td>Mistral-7B</td><td>[SFT_easy+medium]</td><td>45%</td><td>37%</td><td>34%</td><td>46%</td><td>39%</td><td>26%</td><td>31%</td><td>29%</td><td>28%</td><td>22%</td></tr><tr><td>MAmmoTH-7B</td><td>[SFT_easy+medium]</td><td>33%</td><td>25%</td><td>30%</td><td>34%</td><td>21%</td><td>15%</td><td>42%</td><td>40%</td><td>32%</td><td>26%</td></tr><tr><td>WizardMath-7B</td><td>[SFT_easy+medium]</td><td>80%</td><td>45%</td><td>44%</td><td>73%</td><td>64%</td><td>46%</td><td>77%</td><td>69%</td><td>52%</td><td>42%</td></tr><tr><td colspan="10">BilingualModel Evaluation</td></tr><tr><td>OpenHathi-7B</td><td>Base</td><td>33%</td><td>19%</td><td>24%</td><td>36%</td><td>26%</td><td>20%</td><td>50%</td><td>32%</td><td>28%</td><td>24%</td></tr><tr><td>Airavata-7B</td><td>Base</td><td>22%</td><td>11%</td><td>15%</td><td>21%</td><td>12%</td><td>9%</td><td>12%</td><td>14%</td><td>10%</td><td>6%</td></tr><tr><td>Hi-NOLIN-9B</td><td>Base</td><td>31%</td><td>16%</td><td>22%</td><td>30%</td><td>21%</td><td>16%</td><td>45%</td><td>30%</td><td>26%</td><td>24%</td></tr><tr><td colspan="10">Closed Source Models</td></tr><tr><td>Gemini 1.0 Pro</td><td>Base</td><td>75%</td><td>39%</td><td>38%</td><td>68%</td><td>60%</td><td>43%</td><td>81%</td><td>72%</td><td>60%</td><td>48%</td></tr><tr><td>GPT-4</td><td>Base</td><td>91%</td><td>57%</td><td>70%</td><td>92%</td><td>90%</td><td>81%</td><td>93%</td><td>91%</td><td>83%</td><td>70%</td></tr></table></body></html>

Table 2: Performance Comparison of Open-Source and Closed-Source Models on English and Hindi Mathematical Benchmarks

# Result & Analysis

In this section, we first examine the impact of Curriculum Learning based fine-tuning in Hindi and English separately. The analysis then explores the results from bilingual combined training. Lastly, we compare the problem-solving capabilities of lightweight open-source models (SLLMs) against closed-source models (LLMs) across different languages and difficulty levels.

# Curriculum Learning - English Training

We explore the impact of Curriculum Learning on English Dataset on the performance of SLLMs. In the base setting, the models were fine-tuned on the entire English dataset without distinguishing problem complexity. In this setting, WizardMath-7B demonstrated the highest performance, while LLeMMA-7B exhibited the lowest performance across all benchmarks and our English dataset, EMKB, as shown in Table 2. Following this, the models underwent fine-tuning on a subset of easy problems (SFT easy), leading to a $4 \%$ improvement on easy problems and a $6 \%$ increase on the GSM8K benchmark, indicating effective learning of simpler questions during this phase. However, the improvements on more challenging benchmarks like MATH and PRM800K were modest, with only a $1 - 2 \%$ increase. In the next stage, models were fine-tuned on both easy and medium problems (SFT easy+medium). This approach yielded a consistent $6 \%$ performance increase on medium problems and a $3 \%$ improvement on hard problems. These findings (Table 2), suggest that systematically increasing the difficulty of problems enables models to surpass their base setting performance.

# Curriculum Learning $\mathbf { \nabla } \cdot \mathbf { \mu }$ Hindi Training

When applying Curriculum Learning to the Hindi datasets, initially, WizardMath-7B led, while LLeMMA-7B lagged on the Enhanced HAWP Benchmark. Fine-tuning on easy problems (SFT easy) improved performance by $3 - 5 \%$ , but gains on medium and hard problems were minimal. Introducing Curriculum Learning (SFT easy+medium) led to an additional $2 \%$ improvement on the benchmark and $3 - 5 \%$ on more difficult problems (Table 2). This stepwise training approach effectively enhanced the models’ ability to tackle increasingly complex tasks, demonstrating the value of a structured learning regimen in Hindi datasets.

# Curriculum Learning - Bilingual Combined Training

Finally, we tested the performance of SLLMs on full IndiMathQA dataset, covering both Hindi and English versions (Tables 3 and 4). SLLMs were fine-tuned using Curriculum Learning on a bilingual combined training set. As a general trend, all models that went through a combined bilingual training (Tables 3 and 4) performed better on Hindi Benchmarks in comparison to single language fine-tuning (Table 2). This is a remarkable enhacement achieved from our hypothesis that combined fine-tuning on English and Hindi can help improve model’s Hindi Mathematical Reasoning. Initially, WizardMath-7B achieved the highest performance, while Airavata-7B had the lowest results (Base Settings: Table 2). Fine-tuning on easy problems (SFT Easy: Table 3) in both languages led to a consistent $3 - 5 \%$ improvement on easy questions, enhancing the models’ ability to generalize across different linguistic contexts. However, improvements on medium and hard problems were minimal, highlighting the limitations of focusing solely on easy problems. When fine-tuned on both easy and medium problems in both languages (SFT Easy+Medium: Table 4), the models showed more significant gains, with medium problems improving by $1 1 - 1 8 \%$ and hard problems by around $2 \%$ . This demonstrates the effectiveness of Curriculum Learning in enhancing problem-solving abilities and leveraging bilingual training.

# Fine-Tuning Open-Source Models (SLLMs)

In our evaluation of the performance of open-source models such as LLaMA-7B, LLeMMA-7B, Mistral-7B, MAmmoTH-7B, and WizardMath-7B when fine-tuned on combined both Hindi and English versions of IndiMathQA (HMKB and EMKB) (Tables 3 and 4), we observed that fine-tuning on both languages combined improves the performance on both the languages significantly compared to the gains when fine-tuning on a single language. As shown in Table 3 and 4, fine-tuning on easy problems from both languages led to a marginal $2 - 3 \%$ performance increase on easy problems in both Hindi and English. This improvement is better than the pre-trained models but less substantial than the improvements seen with single-language finetuning, as indicated in Table 2. However, Table 3 and 4 further demonstrate that fine-tuning easy and medium problems from both languages resulted in a significant major improvement of $1 1 - 1 8 \%$ accuracy.

# SLLMs (Lightweight open-source) vs LLMs (closed-source)

WizardMath-7B is the best-performing SLLM in our research. Although GPT-4 performance exceeds even the enhanced performance of WizardMath (Table 2, 3 and 4), through Curriculum Learning (SFT easy+medium) and Bilingual Parallel Training, WizardMath-7B outperforms Gemini 1.0 Pro in English datasets by about $5 \%$ (Table 2, 3 and 4). This improvement highlights the effectiveness of our methodology in enhancing SLLM’s problemsolving abilities in English. However, in Hindi datasets, while WizardMath-7B performance is comparable to Gemini Pro, it still lags by approximately $3 \%$ across Medium and Hard difficulties, likely because WizardMath is more proficient in solving math problems in English than in Hindi.

# English Models vs Bilingual Models

Finally, in this comparative analysis of bilingual models and other open-source models (Tables 2, 3 and 4), we observe that bilingual models perform consistently better across English and Hindi, unlike most open-source models, except for WizardMath-7B. This consistency is likely due to the language-independent nature of mathematical reasoning. However, bilingual models like OpenHathi-7B, which are not pre-trained on mathematical tasks, show only slight improvement after fine-tuning, suggesting limited learning efficiency. The superior performance of WizardMath-7B highlights the importance of pre-training models on mathematical tasks for robust performance across languages.

Table 3: Performance of Bilingual Models on IndiMathQA Using SFT easy Training   

<html><body><table><tr><td rowspan="4">Models</td><td colspan="4">IndiMathQA</td></tr><tr><td colspan="4">EMKB HMKB</td></tr><tr><td colspan="4">Easy Medium Hard Easy Medium Hard</td></tr><tr><td>43% 31%</td><td>22% 25%</td><td>16%</td><td>13%</td></tr><tr><td>LLaMA-7B Llemma-7B</td><td colspan="4">9%</td></tr><tr><td>Mistral-7B</td><td colspan="4">20% 12% 48% 33%</td></tr><tr><td>Mammoth-7B</td><td colspan="4">24% 36% 15% 13%</td></tr><tr><td>WizardMath-7B73%</td><td colspan="4">64% 44%</td></tr><tr><td></td><td colspan="4">68% BilingualModels</td></tr><tr><td colspan="4"></td></tr><tr><td>OpenHathi-7B</td><td>41% 30%</td><td>23% 36%</td><td>34% 26%</td></tr><tr><td>Airavata</td><td>23% 14% 11%</td><td>16% 11%</td><td>9%</td></tr><tr><td>Hi-NOLIN 38%</td><td>27% 25%</td><td>33% 32%</td><td>25%</td></tr></table></body></html>

Table 4: Performance of Bilingual Models on IndiMathQA Using SFT easy+medium Training   

<html><body><table><tr><td rowspan="3">Models</td><td colspan="3">IndiMathQA</td></tr><tr><td colspan="2">EMKB</td><td colspan="2">HMKB</td></tr><tr><td>Easy Medium Hard Easy MediumHard</td><td></td><td></td><td></td></tr><tr><td>LLaMA-7B</td><td>44% 35%</td><td>23%</td><td>29% 24%</td><td>19%</td></tr><tr><td>Llemma-7B</td><td>21% 14%</td><td>8%</td><td>15%</td><td>10% 9%</td></tr><tr><td>Mistral-7B</td><td>50% 39%</td><td>29%</td><td>33%</td><td>26% 22%</td></tr><tr><td>Mammoth-7B</td><td>40% 20%</td><td>18%</td><td>44%</td><td>35% 27%</td></tr><tr><td>WizardMath-7B75%</td><td>66%</td><td>47%</td><td>72%</td><td>57% 45%</td></tr><tr><td colspan="5">Bilingual Models</td></tr><tr><td>OpenHathi-7B</td><td>43% 33%</td><td>24%</td><td>40%</td><td>37%</td><td>31%</td></tr><tr><td>Airavata</td><td>25% 16%</td><td>13%</td><td>18%</td><td>14%</td><td>11%</td></tr><tr><td>Hi-NOLIN</td><td>40%</td><td>30% 21%</td><td>39%</td><td>35%</td><td>28%</td></tr></table></body></html>

# Conclusion

This research developed a Bilingual Math Problem Solver using curriculum learning, query decomposition, and structured solution generation. The Decomposition Strategy improved reasoning by breaking down complex queries, Structured Solution addressed the problem of Hallucinations, while curriculum learning enhanced performance on medium and hard problems. WizardMath-7B consistently outperformed other SLLMs (Lightweight open-source) models and often surpassed closed-source models like Gemini 1.0 Pro with these strategies. Our findings demonstrate that integrating these methodologies significantly enhances the problem-solving capabilities of LLMs. Bilingual Parallel Training (Training in multiple languages) provided diverse problem-solving perspectives, proving more effective than single-language training. This study shows how these diverse methodologies can be used to address issues with LLMs in math problem-solving, and can effectively enhance their performance in Hindi.