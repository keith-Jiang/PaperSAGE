# Affordances-Oriented Planning Using Foundation Models for Continuous Vision-Language Navigation

Jiaqi Chen1, Bingqian Lin2, Xinmin Liu3, Lin Ma3, Xiaodan Liang2∗, Kwan-Yee K. Wong1

1The University of Hong Kong 2Shenzhen Campus of Sun Yat-sen University 3Meituan

# Abstract

LLM-based agents have demonstrated impressive zero-shot performance in vision-language navigation (VLN) task. However, existing LLM-based methods often focus only on solving high-level task planning by selecting nodes in predefined navigation graphs for movements, overlooking low-level control in navigation scenarios. To bridge this gap, we propose AOPlanner, a novel Affordances-Oriented Planner for continuous VLN task. Our AO-Planner integrates various foundation models to achieve affordances-oriented low-level motion planning and high-level decision-making, both performed in a zeroshot setting. Specifically, we employ a Visual Affordances Prompting (VAP) approach, where the visible ground is segmented by SAM to provide navigational affordances, based on which the LLM selects potential candidate waypoints and plans low-level paths towards selected waypoints. We further propose a high-level PathAgent which marks planned paths on the image input and reasons the most probable path by comprehending all environmental information. Finally, we convert the selected path into 3D coordinates using camera intrinsic parameters and depth information, avoiding challenging 3D predictions for LLMs. Experiments on the challenging R2R-CE and RxR-CE datasets show that AO-Planner achieves state-of-the-art zero-shot performance $8 . 8 \%$ improvement on SPL). Our method can also serve as a data annotator to obtain pseudo-labels, distilling its waypoint prediction ability into a learning-based predictor. This new predictor does not require any waypoint data from the simulator and achieves $47 \%$ SR competing with supervised methods. We establish an effective connection between LLM and 3D world, presenting novel prospects for employing foundation models in low-level motion control.

# Introduction

Large Language models (LLMs) (OpenAI 2023a; Team et al. 2023; Anil et al. 2023) have shown strong capabilities and potentials in embodied intelligence tasks. In particular, for vision-and-language navigation (VLN) task (Anderson et al. 2018; Qi et al. 2020b; Hong et al. 2021; Chen et al. 2022; Qiao et al. 2022; Wang et al. 2023b), where agents need to make decisions by comprehending language instructions and visual observations, LLMs have demonstrated impressive zero-shot navigation performance. Recent works (Zhou,

![](images/2b4dc6035adf46d4623f2960c368bcc58a257e2cc61a23f6e3db310d6ae67843.jpg)  
Figure 1: In discrete VLN, LLMs only need to perform highlevel planning by selecting a view as the forward direction (left). For continuous environments, previous agents rely on collecting simulator data to train low-level policies. In this paper, we utilize multimodal foundation models and propose visual affordances prompting to predict low-level candidate waypoints and paths in a zero-shot setting (right).

Hong, and Wu 2023; Long et al. 2023; Chen et al. 2024b) have effectively leveraged the high-level capabilities of LLMs for task planning, decision-making, instruction parsing, etc.

However, there exist several challenges and gaps which hinder the application of LLM-based agents to more practical navigation tasks. For instance, agents in the classical discrete VLN setting only need to navigate between predefined viewpoints with high-level decision-making capabilities. In contrast, for more practical VLN in continuous environments (VLN-CE), learning-based models (Krantz et al. 2020; Krantz and Lee 2022; Hong et al. 2022; Wang et al. 2023a; An et al. 2024; Wang et al. 2024) must predict low-level actions (e.g., turn left $1 5 ^ { \circ }$ ) or positions of subgoals in the environment that can drive agents in the simulator. Recently proposed zero-shot agents for navigation in continuous environments (Cai et al. 2023; Chen et al. 2023) only utilize LLMs for high-level task planning, and they still require a large amount of sampled environmental data from the simulator to train low-level policies. Obviously, the gaps between discrete and continuous environments have made current research inadequate in exploring and exploiting the practical navigation potentials of LLMs. Hence, we raise the following question: can LLMs not only handle high-level tasks but also serve as low-level motion planners?

In this paper, we propose a novel zero-shot affordancesoriented planning framework, named AO-Planner, which can navigate agents in continuous environments and is capable of handling both low-level path planning and high-level decision-making. Our key target is to bridge the gap between LLM’s RGB space predictions and 3D world navigation via pixel-level path planning in affordances. First, we utilize Grounded SAM (Ren et al. 2024) to segment navigable regions on the ground as navigational affordances and sample points for the LLM to choose from. We then propose a novel Visual Affordances Prompting (VAP) method to leverage LLM’s spatial understanding abilities and achieve low-level motion planning in a zero-shot manner. As shown in Figure 1, the LLM (Team et al. 2023; Reid et al. 2024) is tasked with selecting the most appropriate waypoints from affordances and connecting some of the points in affordances to plan navigable paths while avoiding obstacles. This process is conducted entirely in the RGB space, in which LLM excels, without the necessity to predict 3D spatial coordinates directly. These low-level candidates (including waypoints and the corresponding paths) are then visualized and fed to another zero-shot PathAgent in the second stage, which is responsible for selecting the path to follow. Finally, by combining depth information with the camera intrinsic parameters, pixel predictions in the RGB space are converted into a series of 3D coordinates that navigate the agent to the designated location.

Experiments on the R2R-CE (Krantz et al. 2020) and RxRCE (Ku et al. 2020) datasets demonstrate that our proposed framework achieves state-of-the-art zero-shot performance. The previous best-performing method, $A ^ { 2 } \mathrm { { N a } \bar { \nu } }$ (Chen et al. 2023), still relies on sampling a large amount of data from the simulator to train low-level policies. In contrast, our approach has shown an improvement of $2 . 9 \%$ in SR on R2RCE and $5 . 6 \%$ in SR on $\mathbf { R } \mathbf { x } \mathbf { R } \mathbf { - C E }$ , without using any training data. For the SPL metric on RxR-CE, our method can even achieve an $8 . 8 \%$ performance improvement. To simulate the challenges of motion planning in the continuous Habitat simulator (Savva et al. 2019), we also adopt a setting that avoids using any additional data, such as the navigation graph from discrete MP3D simulator (Chang et al. 2017), to train low-level policies. We implement a variant of the AOPlanner where we utilize waypoints predicted by LLMs as pseudo-labels, replacing the navigation graph obtained from the MP3D simulator as pseudo ground truths (p-GTs), to train a new learning-based waypoint predictor at the same data scale. We combine this distilled waypoint predictor with the VLN agent used in ETPNav (An et al. 2024) and achieve competitive performance, with an SR of $47 \%$ . The tremendous potentials of multimodal LLM in low-level motion planning are revealed through AO-Planner’s impressive performance.

Our main contributions can be summarized as follows.

• We propose AO-Planner for the VLN-CE task. It leverages foundation models for affordances-oriented planning and converts predictions in the RGB space to 3D

coordinates, bridging the gap between LLM’s high-level decision-making and 3D world navigation. • We present a novel visual affordances prompting (VAP) method to unleash spatial understanding and reasoning abilities of LLMs. This also uncovers previously unexplored motion planning abilities of LLMs. • Our AO-Planner achieves state-of-the-art zero-shot performance without using any simulator data for training lowlevel policies. It can also provide reliable pseudo-labels for training supervised models to achieve competitive performance.

# Related Work

Vision-and-Language Navigation (VLN) VLN (Anderson et al. 2018) is a representative task in the field of embodied AI, requiring an agent to combine instructions and visual observations to move through complex environments and reach a target location. In this task, the most classic scenario is the discrete VLN setting based on the MP3D simulator (Chang et al. 2017), where an agent only needs to select a node from a predefined navigation graph and teleport to it. Therefore, previous works (Wang et al. 2019; Ma et al. 2019; Deng, Narasimhan, and Russakovsky 2020; Hong et al. 2021; Chen et al. 2021, 2022; Qiao et al. 2022; An et al. 2022; Wang et al. 2023b) have focused on endowing agents with high-level decision-making abilities without considering low-level motion.

In recent years, VLN in continuous environment (VLNCE) built on the Habitat simulator (Savva et al. 2019) has received much attention. In this setting, the action space only consists of low-level actions, such as forward and rotate, instead of teleporting the agent directly. To overcome the challenge of directly predicting low-level actions, some works (Krantz et al. 2021; Hong et al. 2022; Krantz and Lee 2022; An et al. 2024) train a waypoint model to predict candidate waypoints in the surroundings as a replacement for the candidate waypoints provided by the MP3D simulator, bridging the gap between discrete and continuous VLN. However, such waypoint models rely on the simulator-specific navigation graph data from MP3D for training, and may suffer from generalization issues. Furthermore, the candidate positions predicted by these models are only some locations near the agent, which can be reached simply by moving forward without considering motion planning.

Navigation with Large Language Models In navigation tasks, there is often a wealth of visual and semantic information involved. Hence, many LLM-based navigation methods have emerged in recent years. Some methods (Zhou, Hong, and Wu 2023; Long et al. 2023; Chen et al. 2024b) directly utilize GPT as agents and achieve zero-shot navigation through appropriate prompting methods. Another group of methods (Pan et al. 2023; Lin et al. 2024; Zhang et al. 2024; Zheng et al. 2024), based on open-source LLMs such as Llama (Touvron et al. 2023), collect data and design tasks to fine-tune the LLMs, enabling them to have impressive navigation performance while maintaining interpretability and universality. There are also methods which incorporate LLMs as part of their frameworks (Cai et al. 2023; Chen et al.

Prompts: “Ground" Grounded SAM LLM Visual Affordances Prompting Exit the living room and [Task Description] [Instruction] "Waypoints": [6, 8, 3], turn right into the kitchen .. [Waypoint Definition] [Requirements] "Paths": [[1,4, 6],[2, 5, 8],[3]] 福 九 ® 'Waypoints'.... select some positions that need to be passed through.. 福 8 9 6789 曲 日 2 'Waypoints' while navigating around obstacles to avoid collisions.. Image Affordances Low-level Motion Planning

2023), leveraging LLMs’ high-level capabilities for handling subtasks such as instruction parsing, scene description, and decision-making. However, these approaches merely exploit the high-level capabilities of LLMs and still require collecting data to train low-level policies. The potentials of LLMs in low-level motion planning remain unexplored.

Visual Prompting Prompting is an important technique for activating various capabilities of LLMs. Recently, with the development of multimodal LLMs, some visual prompting methods (Yang et al. 2023a,b; Nasiriany et al. 2024; Lei et al. 2024) have emerged. For example, SoM (Yang et al. 2023a) extracts and labels certain objects through object detection or segmentation to enhance the visual grounding capability of LLMs. Yang et al. (2023b) systematically summarize the results and potentials achieved using different visual markers on the multimodal GPT-4V (OpenAI 2023b) model. These methods have demonstrated the importance of visual prompting in unleashing the multimodal understanding capabilities of LLMs. Nonetheless, they do not address motion planning problem in navigation tasks.

# Method

# VLN-CE Task Definition

The task of vision-language navigation can be formulated as follows. For each episode, the agent needs to follow a fixed instruction $I$ to move from the starting point to the target destination. At step $t$ , the agent can obtain an observation $O _ { t }$ , which includes views $\{ V _ { t } ^ { i } \bar  \} _ { i = 1 } ^ { N }$ from different directions. In our framework, we set $N = 4$ and collect non-overlapping views from the front, back, left, and right directions as observation, i.e., Ot = {Vti}i4=1. For the action space, the VLN-CE task defines four parameterized low-level actions, namely FORWARD $( 0 . 2 5 \mathrm { m } )$ , ROTATE LEFT/RIGHT $( 1 5 ^ { \circ } )$ , and STOP. In this work, the agent needs to select a waypoint location and the corresponding path as an action $a _ { t }$ , and then convert the relative position into a series of low-level actions.

# Framework Overview

As shown in Figure 2 and Figure 3, we follow some popular methods (Hong et al. 2022; Krantz and Lee 2022) in the VLNCE task and design a two-stage framework, with two LLMbased agents responsible for predicting candidate paths at the low level and making decisions based on all environmental information at the high level. This simplifies and reduces the difficulty of the task since there is no need to simultaneously address two tasks of completely different levels.

Specifically, we first utilize the Grounded SAM (Ren et al. 2024) model to obtain navigational affordances and sample some points from them, facilitating the selection by the LLM agent. We further design prompts for the low-level agent to search for potential waypoints and plan corresponding paths by connecting the sampled points within the affordances. These candidate results are visualized and fed to the highlevel agent in the second stage, where we refer to the previous zero-shot method (Chen et al. 2024b) that combines instructions and historical information to make the final decision on the location to move to. Additionally, points predicted in RGB space in the first stage can be transformed into 3D world coordinates based on depth information and camera intrinsics, and further be converted into low-level actions, thereby connecting the LLM with the 3D world.

# Visual Affordances Prompting (VAP)

Prompting technique has been widely used to unleash the powerful capabilities of LLMs. For multimodal LLMs such as GPT-4 (OpenAI 2023a,b) and Gemini (Team et al. 2023; Reid et al. 2024), the importance of visual prompting is gradually being recognized (Yang et al. 2023a,b; Nasiriany et al. 2024). In this work, we propose a novel Visual Affordances Prompting (VAP) approach for low-level planning in continuous VLN task. As shown in Figure 2, we mark a set of navigable points within affordances, from which the LLM selects candidate waypoints and connects some points to form candidate paths, enabling low-level planning. VAP effectively achieves zero-shot path planning in the VLN-CE task.

Navigational Affordances “Affordances” is a widely used concept in the field of robotics, referring to the regions in the environment where an agent’s actions can be performed (Gibson 2014). Previous works in navigation tasks (Qi et al. 2020a; Luddecke and Worgotter 2017) have explored how to learn from supervised data and ultimately predict safe and navigable affordances in the environment.

Fortunately, we have found that current foundation models have the capability to solve this problem in a zero-shot setting. The Grounded SAM (Ren et al. 2024) method we adopt combines an open-set detector Grounding Dino (Liu et al. 2023) with Segment Anything Model (SAM) (Kirillov et al. 2023). By inputting the prompt “Ground”, the open-set detector can detect the visible ground and provide the corresponding bounding boxes to SAM for segmentation, resulting in high-quality navigational affordances. In the environment, we set the FOV of the agent’s camera to 90 degrees and collect observations from four directions in counterclockwise order, namely front, left, back, and right views. For each view $V _ { t } ^ { i }$ in direction $i$ , the output of Grounded SAM is a set of $k$ masks $\{ m _ { 1 } , . . . m _ { k } \}$ , and we fuse these masks into one to represent the final affordances $A _ { t } ^ { i }$ .

Visual Prompting Inspired by previous Set-of-Mark (SoM) prompting (Yang et al. 2023a), we propose a novel visual affordances prompting method to assist LLM in unleashing its visual grounding and affordances-oriented spatial planning. Given a view $V _ { t } ^ { i }$ of size $h \times w$ , we uniformly distribute a set of points within its affordances $A _ { t } ^ { i }$ . We then label these points in order and visualize them on the image, resulting in an augmented view $V _ { t } ^ { i * }$ . This allows us to easily query LLM to select suitable points and provide their corresponding IDs without directly predicting their coordinates in RGB space or the 3D world.

Compared to the previous waypoint models (Hong et al. 2022; Krantz and Lee 2022; An et al. 2024), which only take RGB and/or depth information as inputs and directly predict reachable locations near the current position, we incorporate semantic information and require path planning of the firststage agent to navigate around obstacles. Specifically, we include the instructions $I$ of the current episode as part of the prompts, asking the agent to examine key information such as scene descriptions, landmarks, and objects, and to select corresponding waypoints and paths. Additionally, we provide some low-level task descriptions $D _ { L }$ , such as specific definitions for waypoints and paths, like requiring waypoint predictions to maintain a distance from obstacles and be located in crucial regions, and connecting points to form paths that navigate around obstacles. The prompting process can be formulated as

$$
W _ { t } ^ { i } , P _ { t } ^ { i } = L L M ( D _ { L } , I , V _ { t } ^ { i * } ) ,
$$

where $i$ represents one of the four view directions. We further merge the predicted waypoints $\mathbf { \nabla } W _ { t } ^ { i }$ and paths $\mathbf { \nabla } _ { P _ { t } ^ { i } } ^ { p _ { i } ^ { i } }$ from four directions into two candidate lists $W _ { t }$ and $P _ { t }$ for waypoints and paths at the current position. Our proposed prompts help LLM better understand the task, enabling it to predict waypoints and plan reasonable paths based on affordances.

![](images/9eccbddaab7f764a6d513d3013b14fe0d8aa35c7b407227551863d6dc597b65e.jpg)  
Figure 3: Our proposed high-level PathAgent. Different from previous zero-shot VLN agents, we utilize visual prompting by marking candidate waypoints and their corresponding paths (i.e., Path 0-5) in all four observation directions. This allows the PathAgent to make action decisions in the proficient RGB space and then map pixel-based paths to 3D coordinates using depth information and camera intrinsic parameters.

# High-level PathAgent

After obtaining potential low-level waypoint and path predictions, we introduce another agent, dubbed PathAgent, to perform high-level decision-making. As shown in Figure 3, we refer to MapGPT designed for discrete VLN tasks (Chen et al. 2024b) and make some improvements specifically for the VLN-CE task. Instead of directly providing some captions of the visual observations or providing originally observed images to a multimodal LLM, we also utilize the idea of visual prompting. For a given step $t$ , we visualize the candidate waypoints $\mathbf { } W _ { t }$ and paths $P _ { t }$ predicted in the first stage by rendering them on the visual observation $O _ { t }$ (containing four perspective directions) and marking them with IDs, resulting in an augmented observation $O _ { t } ^ { \prime }$ . Through visual prompting, we only need four images to retain all observational information, without relying on the simulator to provide images corresponding to candidate waypoints like MapGPT (Chen et al. 2024b), which may neglect some observations. Additionally, we add high-level task description $D _ { H }$ , instruction $I$ , and historical information $H _ { t }$ as inputs to the LLM. Following previous work, we require the LLM to output an interpretable thinking process $T _ { t }$ and select one path from $P _ { t }$ set as action $a _ { t }$ . The entire process is defined as

$$
T _ { t } , a _ { t } = L L M ( D _ { H } , I , H _ { t } , O _ { t } ^ { \prime } , W _ { t } , P _ { t } ) .
$$

# 3D Mapping and Motion Control

For our zero-shot framework, once the agent selects a waypoint as a subgoal, we need to convert the planned path into a sequence of actions to guide the agent along the path. During the low-level motion planning in the first stage, the LLM agent plans paths in the RGB space by connecting points. A pixel-based path is actually a series of line segments. We can utilize the camera intrinsic parameters and depth information to map each point in the line segments from pixel coordinates to 3D world coordinates. For each line segment, we follow the low-level controller in ETPNav (An et al. 2024), which designs a rotate-then-forward control flow. Based on the world coordinates of two points in a line segment, we calculate their relative orientation and distance, and convert them into a series of low-level ROTATE and FORWARD actions within the action space. Through this approach, we bridge the gap between the LLM’s predicted 2D results and the 3D world, successfully applying the spatial planning abilities of LLM to low-level motion.

# Waypoint Distillation

In this work, we not only use LLMs with VAP as zero-shot waypoint predictors, but also explore transferring this capability to a learning-based waypoint predictor to obtain similar waypoint predictions. Specifically, we utilize our LLM-based waypoint predictor as a zero-shot data annotator to predict waypoints as pseudo ground-truths (p-GTs). We then leverage these p-GTs to train the waypoint predictor in ETPNav (An et al. 2024) and combine it with the high-level VLN agent used in ETPNav. By distilling the waypoint prediction capability of LLMs to a learning-based waypoint predictor, we can achieve more competitive performance as it can be combined with a learning-based high-level agent for fine-tuning, without relying on additional MP3D simulator data. This is also a cost-effective paradigm since the scale of the pseudo labels we collect is the same as the scale of the MP3D data used by the previous waypoint predictor. We sample around 10K locations with 40K images for training, which only requires 40K calls to Gemini.

# Experiments

In this section, we present the main experimental results. For implementation details and more results, please refer to the appendices in our arXiv version paper (Chen et al. 2024a).

# Experimental Setting

Dataset We conduct experiments on the challenging R2RCE (Krantz et al. 2020) and RxR-CE (Ku et al. 2020) datasets. R2R-CE is derived from the discrete path annotations from the R2R dataset (Anderson et al. 2018) and is converted into continuous environments with the Habitat simulator (Savva et al. 2019). R2R-CE provides step-by-step high-level instructions which are corresponding to indoor navigation trajectories that should be followed by agents. RxR-CE is a larger and more challenging dataset, which has longer instructions, and contains multilingual descriptions. However, its scene splits are similar to R2R-CE, and experiments are also conducted using the Habitat simulator. We follow some previous zero-shot work (Zhou, Hong, and Wu 2023; Long et al. 2023; Chen et al. 2024b) in the discrete VLN task, evaluating AOPlanner on the entire validation unseen set of R2R-CE and a random sampling subset with 500 cases from the validation unseen set of RxR-CE. To save API costs, we also additionally sample a subset containing 100 cases from the validation unseen set of R2R-CE for ablation study.

Evaluation Metircs Following previous works (Krantz et al. 2020; Hong et al. 2022), we adopt some evaluation metrics widely used in the VLN tasks, namely navigation error (NE), oracle success rate (OSR), success rate (SR), success weighted by the inverse of the path length (SPL), and normalized dynamic time wrapping (nDTW), respectively.

# Experimental Results

Zero-Shot Performance As shown in Table 1, our method achieves state-of-the-art performance under zero-shot setting. It shows a $5 . 5 \%$ SPL improvement on R2R-CE and an $8 . 8 \%$ SPL improvement on RxR-CE over the previous best-performing method $A ^ { 2 } \mathrm { { N a v } }$ (Chen et al. 2023). Furthermore, while $A ^ { \mathrm { 2 } } \mathrm { \bar { N } a v }$ does not require any instruction-goal pair demonstrations, it still needs to sample a large number of image-path pairs from the Habitat simulator to train low-level policies, which may suffer from potential generalization issues. Some other methods that do not rely on simulator data show worse performance. For example, Cow (Gadre et al. 2022) only achieves a success rate of $7 . 8 \%$ and an SPL of $5 . 8 \%$ on R2R-CE. In contrast, our proposed method, while not requiring any data for training, significantly outperforms previous zero-shot methods.

Our zero-shot performance is comparable to three supervised methods, SASRA (Zubair Irshad et al. 2021), Seq2Seq (Krantz et al. 2020), and NaVid (Zhang et al. 2024) on R2RCE or RxR-CE. However, we also observe that the current zero-shot performance still has a significant gap compared to supervised performance, which aligns with the performance comparison of zero-shot and supervised methods in the discrete VLN task (Zhou, Hong, and Wu 2023). This may be due to LLMs’ unfamiliarity with this VLN-CE task, lacking direct learning from the Habitat simulator. Additionally, the image quality of continuous scenes reconstructed by the Habitat simulator is not as high as images directly captured in MP3D. These factors affect the performance of LLMs.

It is important to note that we do not modify any prompts when applying AO-Planner to both R2R-CE and RxR-CE, which also demonstrates that our zero-shot method is more general and does not require additional fine-tuning like dataset-specific supervised models.

Distilling Waypoint Predictor We aim to transfer the capability of LLM-based waypoint prediction to a learning-based waypoint predictor to avoid reliance on discrete MP3D data. For the waypoint model selection, we adopt the Chamfer distance metric used by ETPNav to evaluate the alignment between the model’s waypoint predictions and the ground truths. Our waypoint predictor achieves a result of 1.08, which is slightly worse than the 1.04 achieved in the original ETPNav. This further demonstrates that using predictions from LLM as pseudo GTs can achieve high-quality waypoint predictions.

As shown in Table 1, the experiments on R2R-CE and RxR-CE datasets demonstrate the potential of our proposed AO-Planner. By using the waypoint predictor to provide subgoals, AO-Planner significantly outperforms NaVid, which does not use any waypoint predictor, in both OSR and SR. We notice that the performance of AO-Planner is still behind ETPNav trained on MP3D ground truth. However, our method does not rely on any manual annotation or simulator data for training low-level policies. In this work, we point out the limitations of existing methods and re-demonstrate the effectiveness of waypoint prediction in a more practical setting. Our AO-Planner provides a meaningful reference by leveraging foundation model in not only serving as zeroshot predictors but also providing reliable waypoint data for training, free from the constraints of simulator data.

Table 1: Comparison with supervised and zero-shot methods on validation unseen split of R2R-CE and RxR-CE. Our approach achieves state-of-the-art zero-shot performance on two benchmarks (zero-shot AO-Planner) and also obtains competitive supervised performance via distilling LLM’s waypoint prediction ability into a learning-based predictor (supervised AO-Planner), without relying on simulator data for training low-level models. In contrast, $A ^ { 2 } \mathrm { { N a v } }$ requires sampling simulator data for training policies and ETPNav utilizes the navigation graph from discrete MP3D simulator for training a waypoint predictor.   

<html><body><table><tr><td rowspan="2">Settings</td><td rowspan="2">Methods</td><td rowspan="2">Simulator Data</td><td colspan="4">R2R-CE</td><td colspan="4">RxR-CE</td></tr><tr><td>NE↓</td><td>OSR↑</td><td>SR↑</td><td>SPL↑</td><td>NE↓</td><td>SR↑</td><td>SPL个</td><td>nDTW↑</td></tr><tr><td rowspan="7">Supervised</td><td>SASRA(Zubair Irshad et al.2021)</td><td>√</td><td>8.32</td><td></td><td>24</td><td>22</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Seq2Seq (Krantz et al. 2020)</td><td></td><td>7.77</td><td>37</td><td>25</td><td>22</td><td>12.1</td><td>13.9</td><td>11.9</td><td>30.8</td></tr><tr><td>LAW (Raychaudhuri et al. 2021)</td><td>√</td><td>1</td><td></td><td>35</td><td>31</td><td>10.9</td><td>8.0</td><td>8.0</td><td>38.0</td></tr><tr><td>DC-VLN (Hong et al. 2022)</td><td>√</td><td>6.20</td><td>52</td><td>41</td><td>36</td><td>8.98</td><td>27.1</td><td>22.7</td><td>46.7</td></tr><tr><td>NaVid (Zhang et al. 2024)</td><td></td><td>5.47</td><td>49</td><td>37</td><td>36</td><td>8.41</td><td>23.8</td><td>21.2</td><td>-</td></tr><tr><td>ETPNav (An et al. 2024)</td><td>√</td><td>4.71</td><td>65</td><td>57</td><td>49</td><td>5.64</td><td>54.8</td><td>44.9</td><td>61.9</td></tr><tr><td>AO-Planner (Ours)</td><td></td><td>5.55</td><td>59</td><td>47</td><td>33</td><td>7.06</td><td>43.3</td><td>30.5</td><td>50.1</td></tr><tr><td rowspan="6">Zero-Shot</td><td>CLIP-Nav (Dorbala et al. 2022)</td><td></td><td>-</td><td></td><td>5.6</td><td>2.9</td><td>-</td><td>9.8</td><td>3.2</td><td></td></tr><tr><td>Seq CLIP-Nav (Dorbala etal.2022)</td><td></td><td></td><td></td><td>7.1</td><td>3.7</td><td>1</td><td>9.1</td><td>3.3</td><td></td></tr><tr><td>Cow (Gadre et al. 2022)</td><td></td><td></td><td></td><td>7.8</td><td>5.8</td><td></td><td>7.9</td><td>6.1</td><td></td></tr><tr><td>ZSON (Majumdar et al. 2022)</td><td>√</td><td></td><td>-</td><td>19.3</td><td>9.3</td><td>-</td><td>14.2</td><td>4.8</td><td>-</td></tr><tr><td>A²Nav (Chen et al. 2023)</td><td>√</td><td></td><td></td><td>22.6</td><td>11.1</td><td></td><td>16.8</td><td>6.3</td><td></td></tr><tr><td>AO-Planner (Ours)</td><td></td><td>6.95</td><td>38.3</td><td>25.5</td><td>16.6</td><td>10.75</td><td>22.4</td><td>15.1</td><td>33.1</td></tr></table></body></html>

Table 2: Ablation study on different prompting settings.   

<html><body><table><tr><td>Prompting Methods</td><td>OSR↑</td><td>SR↑</td><td>SPL↑</td></tr><tr><td>Visual Affordances Prompting</td><td>39</td><td>27</td><td>16.9</td></tr><tr><td>w/o Instructions</td><td>36</td><td>24</td><td>14.6</td></tr><tr><td>w/o Waypoint Definitions</td><td>35</td><td>23</td><td>14.7</td></tr><tr><td>w/o Affordances</td><td>27</td><td>15</td><td>7.2</td></tr></table></body></html>

# Ablation Study

Different Prompting Methods As shown in Table 2, we design three representative prompting methods to compare with our proposed Visual Affordances Prompting (VAP). The waypoint model designed for low-level motion in previous work (Hong et al. 2022; Krantz and Lee 2022; An et al. 2024) typically only use RGB and/or depth information as input. Given LLM’s semantic understanding capabilities, we have incorporated instructions as part of the prompts in VAP to help LLMs check scene descriptions, landmarks, and objects information. Our method with instructions obtains a $3 \%$ improvement in SR and a $2 . 3 \%$ improvement in SPL.

In VAP, we have also made some simple definitions for waypoints, including obstacle avoidance and finding crucial regions. We conduct an additional ablation without these definitions, but instead using a data style similar to MP3D, selecting waypoints that are 2-3 meters away. This results in a $4 \%$ reduction in SR and a $2 . 2 \%$ reduction in SPL.

Table 3: Ablation study on different combinations of waypoint predictors and high-level VLN agents.   

<html><body><table><tr><td>Methods</td><td>OSR↑ SR↑</td><td>SPL↑</td></tr><tr><td>Zero-Shot Predictor + Baseline Agent</td><td>31</td><td>20 12.4</td></tr><tr><td>Supervised Predictor+Baseline Agent</td><td>34</td><td>23 13.1</td></tr><tr><td>Zero-Shot Predictor+PathAgent</td><td>39</td><td>27 16.9</td></tr><tr><td>Supervised Predictor+PathAgent</td><td>42</td><td>29 17.6</td></tr></table></body></html>

Another key factor in our VAP is the use of navigational affordances. We provide an experiment where we do not use SAM to segment the affordances, but directly scatter some points for the LLMs to select. The experiment shows that this greatly increases the difficulty for Gemini to perform low-level motion planning, as there are more options, and some points are located on obstacles, causing interference. This lowers SR from $27 \%$ to $1 5 \%$ .

Different Combinations of Low-Level Predictors and High-Level Agents Our AO-Planner defaults to using a zero-shot waypoint predictor in the first stage and our PathAgent in the second stage. In this experiment, we compare different implementations of the two stages. In Table 3, “Baseline Agent” refers to transferring MapGPT (Chen et al. 2024b) from a discrete environment to the VLN-CE task. It does not visualize the waypoint predictions in the observation or calculate the position of the waypoint via 3D mapping, but simply selects a view and moves a fixed distance each time. Our PathAgent shows a $6 \%$ performance improvement in SR compared to the “Baseline Agent”.

Besides, We explore replacing our zero-shot waypoint predictor with a supervised waypoint predictor. Our proposed

# Step O: Front View Step 1: Left View Step 2: Right View

荣

![](images/82f94330ba5fa94107bcda37493b64fc608c3c817f431f58f97820ae4bcc4f15.jpg)

![](images/ce6ac5eeb8c7f143a6538757a9868f4202db2ada2a185748daa9ae8fe41a10a9.jpg)  
Step 3: Left View

![](images/191ad83d815a5fdd4008b4b25c1e72fe475e6979eab0fdcd71df0686cfcccad2.jpg)

# Instruction:

Make a left out the room with the toilet and walk past the bathtub. The bathtub will be on your right as you pass. Go thought the first door on your right to enter the closet.Stopas soon as you enter the closet.

Thought: Starting from the current location, Ineed to   
identify the room with the   
toilet. Observing the images, I can see a bathtub in the   
view. According to the   
instructions,I should make a left out of the current room with the toilet, and the   
bathtub should be on my   
right as l pass.   
Action: 1

Thought: I need to turn left from the room with the toilet, walk past the bathtub on my right, and then enter the first door on my right to enter the closet. Based on the images, I should move to a location where the bathtub is on my right. Action: 6

![](images/90b7f40eb35a40635b4a9ffc8867a2428c7e1af7d06e2eba56d47d61db6f1a6e.jpg)  
Figure 4: An example of successful navigation in a continuous environment. We present visualizations of the motion planning results from the low-level agent (upper) and the thinking process of the high-level PathAgent based on visualized candidate paths (only the selected directions are shown in this figure) and the selection of a path ID as an action (bottom). The agent ultimately decides to stop after observing the target “closet”.

Thought: According to the instructions.after turning left from the room with the toilet, should walk past the bathtub on my right.In the current scene, I can see the bathtub on the left when facing   
backwards. This means l am currently past the bathtub, and thenext step is to enter the first door on my right to enter the closet.   
Action: 17

Thought: Ihave entered the closet as per the instructions. In my current step, I need to stop as soon as l enter the closet. I can observe that the view matchescloset-like features with clothing racks, which means l have successfullyentered the closet.

Action: Stop

Table 4: Ablation study on using different LLMs.   

<html><body><table><tr><td colspan="2">LLMs</td><td colspan="2">Metircs</td></tr><tr><td>Low-Level</td><td>High-Level</td><td>OSR↑</td><td>SR↑ SPL↑</td></tr><tr><td>GPT-40</td><td>Gemini-1.5-Pro</td><td>23 11</td><td>6.9</td></tr><tr><td>Gemini-1.5-Pro</td><td>Gemini-1.5-Pro</td><td>25 14</td><td>9.1</td></tr><tr><td>GPT-40</td><td>GPT-40</td><td>34</td><td>16 9.4</td></tr><tr><td>Gemini-1.5-Pro</td><td>GPT-40</td><td>39</td><td>27 16.9</td></tr></table></body></html>

zero-shot predictor does not rely on any ground-truth from the MP3D simulator for training and performs on par with $1 6 . 9 \%$ vs. $1 7 . 6 \%$ in SPL) the supervised waypoint predictor used by DC-VLN (Hong et al. 2022). This demonstrates that our visual affordances prompting is a promising new approach in zero-shot continuous VLN.

Agents using Different LLMs Our zero-shot AO-Planner is a two-stage framework. As shown in Table 4, we use different LLMs for low-level and high-level agents, respectively, to observe their impact on performance. Gemini-1.5 has demonstrated impressive performance on some embodied tasks (Reid et al. 2024), such as OpenEQA (Majumdar et al. 2024). In our low-level motion planning task, we find that Gemini-1.5-Pro outperforms OpenAI’s GPT-4o model. However, contrary to the results of low-level ablation, Gemini-1.5 performs poorly in high-level decision-making. This could be attributed to differences in the training data of the two models, which have resulted in distinct distributions of capabilities. Investigating this underlying cause further will be our future work. Based on the aforementioned experimental results, we default to using Gemini-1.5 as the LLM for the low-level agent and GPT-4o for the high-level agent.

# Qualitative Results

As shown in Figure 4, we present a comprehensive example of navigation in a continuous environment. It can be observed that the low-level motion planning agent provides some highquality candidates. For instance, in the left view at step 1, the bathtub blocks most of the forward direction, but the low-level agent finds three paths that navigate around the obstacle and reach the distant waypoints. The trajectory in the top-down map also demonstrates that the agent successfully avoids the bathtub obstacle and prevents collisions.

The high-level PathAgent consistently selects the correct directions at each step, and the output thoughts also demonstrate its understanding of the task. Upon encountering landmarks that exist in the instruction, the agent promptly assesses the progress of instruction execution and makes accurate decisions until reaching the goal.

# Conclusion

This paper proposes AO-Planner, where we explore the previously under-researched low-level motion planning capability of LLM for addressing the challenging VLN-CE task, effectively bridging the gap between LLM and 3D world. We introduce a novel visual affordances prompting strategy, which provides navigational affordances to enable LLM to plan low-level motions. A high-level PathAgent is further proposed to perform path selection in the RGB space by comprehending all environmental information. Finally, we map the selected pixel-based path to 3D coordinates for navigation. Experiments on R2R-CE and RxR-CE demonstrate the effectiveness of our approach that achieves state-of-the-art zero-shot and competitive supervised performance.