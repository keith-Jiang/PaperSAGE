# Harnessing Large Language Models for Knowledge Graph Question Answering via Adaptive Multi-Aspect Retrieval-Augmentation

Derong ${ \bf X } { \bf u } ^ { 1 2 }$ , Xinhang $\mathbf { L i } ^ { 1 }$ , Ziheng Zhang3, Zhenxi $\mathbf { L i n } ^ { 3 }$ , Zhihong $\mathbf { Z } \mathbf { h } \mathbf { u } ^ { 4 }$ , Zhi Zheng1, Xian $\mathbf { W _ { u } } ^ { 3 * }$ Xiangyu Zhao2\*, Tong $\mathbf { X } \mathbf { u } ^ { 1 * }$ , Enhong Chen1

1University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, 2City University of Hong Kong, 3Jarvis Research Center, Tencent YouTu Lab, 4Peking University {derongxu,xinhangli,zhengzhi97} $@$ mail.ustc.edu.cn, {zihengzhang, chalerislin, kevinxwu}@tencent.com, xianzhao@cityu.edu.hk, {tongxu, cheneh}@ustc.edu.cn, zhihongzhu $@$ stu.pku.edu.cn

# Abstract

Large Language Models (LLMs) demonstrate remarkable capabilities, yet struggle with hallucination and outdated knowledge when tasked with complex knowledge reasoning, resulting in factually incorrect outputs. Previous studies have attempted to mitigate it by retrieving factual knowledge from large-scale knowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of answers. However, this kind of approach often introduces noise and irrelevant data, especially in situations with extensive context from multiple knowledge aspects. In this way, LLM attention can be potentially mislead from question and relevant information. In our study, we introduce an Adaptive MultiAspect Retrieval-augmented over KGs (Amar) framework. This method retrieves knowledge including entities, relations, and subgraphs, and converts each piece of retrieved text into prompt embeddings. The Amar framework comprises two key sub-components: 1) a self-alignment module that aligns commonalities among entities, relations, and subgraphs to enhance retrieved text, thereby reducing noise interference; 2) a relevance gating module that employs a soft gate to learn the relevance score between question and multi-aspect retrieved data, to determine which information should be used to enhance LLMs’ output, or even filtered altogether. Our method has achieved state-of-the-art performance on two common datasets, WebQSP and CWQ, showing a $1 . 9 \%$ improvement in accuracy over its best competitor and a $6 . 6 \%$ improvement in logical form generation over a method that directly uses retrieved text as context prompts. These results demonstrate the effectiveness of Amar in improving the reasoning of LLMs.

Code — https://github.com/Applied-Machine-LearningLab/AMAR

# Introduction

Recently, large language models (LLMs) like GPT-4 (OpenAI 2024) and Llama (Touvron et al. 2023) have shown impressive performance improvements across a variety of natural language processing (NLP) tasks (WANG et al. 2024;

Question   
pwhat countries included in KG oceania ? Retrieved Entities Retrieved Relations   
pOceania Relevant ptopic.notable_types Relevant   
pRemote Oceania Partially plocation.contains Irrelevant Ground Truth Retrieval Recall   
(JOIN base. biblioness. Logical Form: (AND   
bibs_location.loc_type … WebQSP CWQ   
Answers: Australia, Fiji, 2 4 6 8 10   
New Zealand, Number of Retrieval Relation

WEN et al. 2023; XU et al. 2024; ZENG et al. 2023). However, when dealing with specialized knowledge not in training corpus and complex knowledge reasoning, LLMs still struggle with outdated knowledge and hallucination problems (Ji et al. 2023). They thus may produce factually incorrect outputs, limiting their usefulness in the areas requiring high reliability, such as healthcare (He et al. 2023) and safety (Dong et al. 2024). To solve complex reasoning with specialized knowledge, a line of research (Luo et al. 2023; Sun et al. 2024) have explored Knowledge Graph Question Answering (KGQA), a task that improves logical reasoning and prediction of answers by retrieving reliable information from large-scale knowledge graphs (KGs) like Freebase (Bollacker et al. 2008) and Wikidata (Vrandecˇic´ and Kr¨otzsch 2014). KGs store extensive factual knowledge in a structured format known as triplets, consisting of (head entity, relation, tail entity), which is seen as a potential solution for enhancing the interpretability of LLMs reasoning (Sun et al. 2024).

The recent advancements in KGQA can be broadly classified into two main categories. The first category involves using LLMs to transform input questions into structured logical forms (Luo et al. 2023), such as S-expressions, which can then be queried on KG using a graph database query language like SPARQL to obtain the final answers. These approaches leverage the structured nature of KG to extract precise information in response to complex queries. The second category of KGQA involves directly predicting answers using LLMs without extra logical forms. Some works in this category perform multi-hop reasoning on KG in a step-by-step manner by querying LLMs (Sun et al. 2024). Other works retrieve additional contextual information related to the question and use it as contextual input to enhance LLMs (He et al. 2024; LUO et al. 2024). These approaches aim to improve the accuracy of answers by incorporating more context into the reasoning process.

However, the retrieval process often yields a large amount of information, as shown in Figure 1, which may be irrelevant or only partially relevant to the question and ground truth. The retrieval recall curve demonstrates a positive correlation between the number of retrieved data and the recall score, indicating that the retrieved data contain valuable information. Therefore, it raises a question: How to identify which retrieved knowledge is valuable and which is not? This question becomes particularly crucial when a significant portion of retrieved knowledge does not provide substantial assistance. In terms of this, we observed that previous studies (Yu et al. 2022; Hu et al. 2022) failed to considering commonalities among different aspects of retrieved information, which can be beneficial in identifying crucial knowledge. And they also neglected adaptive learning to establish relevance between question and retrieved text.

To address these overlooked challenges, we propose an adaptive multi-aspect retrieval-augmented over KG (AMAR) framework. Instead of directly appending the retrieved information as context input, AMAR utilizes the retrieved information more flexibly, therefore enhancing LLMs reasoning. AMAR primarily includes two modules: 1) Self-alignment module, where multi-aspect retrieval data (including entities, relations, and subgraphs, all of which are linearized as text) is separately mapped into prompt embeddings from text, facilitating fine-grained tuning. Next, cross-attention and self-attention are applied to the multi-aspect embeddings to obtain consistency tokens. The aim is to align the commonalities among different pieces of information. For instance, if an entity and a subgraph both mention “Oceania”, this implies that they are probably consistent in conveying the same piece of information. In this way, we can enhance crucial knowledge, thereby reducing noise interference. 2) Relevance gating module, which measures the relevance between the embedding of question and consistency tokens. This module introduces siamese networks with shared weights to learn relevance score, which serves as a soft gate to adaptively decide which retrieved information is more useful for LLMs reasoning. Through the selfalignment and relevance gating modules, AMAR adaptively filters and selects multi-aspect retrieval knowledge, enabling a more rational utilization of context and avoiding interference from noise. We then fine-tune LLM in a parameterefficient manner to leverage the retrieved knowledge, assisting in generating reasonable logical forms. Finally, we refine the logical forms using the similarity of entities and relations and query KG to obtain final answers. Overall, this work makes three key contributions:

• To the best of our knowledge, this is the first study to enhance LLMs reasoning for KGQA tasks by utilizing multi-aspect KG information as prompt embeddings. • We propose novel self-alignment and relevance gating modules, which enable LLMs to adaptively filter and select multi-aspect retrieval knowledge, allowing for more rational utilization of context while effectively avoiding interference from noise. • The effectiveness of AMAR was extensively validated on two datasets across five metrics. AMAR demonstrated superior performance, achieving state-of-the-art (SOTA) results compared to 22 strong baselines.

# Preliminaries

We first introduce key concepts and notations for our task.

Knowledge Graph (KG). A KG is a collection of factual knowledge organized in form of triples: $\mathcal { G } = \{ ( h , r , t ) \} \subseteq$ $\mathcal { E } \times \mathcal { R } \times \mathcal { E }$ , where $\mathcal { E }$ represents the entity set and $\mathcal { R }$ represents relation set. Each triple consists of three elements: a head entity $h$ , a relation $r$ , and a tail entity or a literal $t$ .

Logical Form. The logical form is a structured language representation of a question. In this work, we adopt Sexpression $\mathcal { F }$ as our chosen logical expression, following (Luo et al. 2023; Yu et al. 2022). As shown in the examples provided in Figure 1, the S-expression utilizes functions (such as JOIN, AND) that operate on set-based semantics, which keep a balance between readability and compactness and thus is well-suited for KGQA (Gu et al. 2021).

KGQA with LLM. KGQA is a classical NLP task that has been further enhanced through the utilization of LLMs. Given a question $q$ , this task aims to retrieve knowledge related to $q$ from a KG $\mathcal { G }$ and generate an S-expression $\mathcal { F }$ . Since many KG storage engines support SPARQL, the generated S-expression $\mathcal { F }$ is converted into a SPARQL query, which is further executed against $\mathcal { G }$ to obtain the final answers. In our work, We retrieve multi-aspect knowledge (including entities $k _ { e }$ , relations $k _ { r }$ , and subgraphs $k _ { s }$ ) and design a model $f$ that utilizes LLMs to generate $\mathcal { F }$ based on question and retrieval knowledge as input, i.e., ${ \boldsymbol { \mathcal { F } } } \ =$ $f ( q , \overline { { k _ { e } } } , k _ { r } , k _ { s } )$ . By converting and executing the SPARQL query on the KG, we can obtain final answers denoted as $a = q u e r y ( c o n v e r t ( \mathcal { F } ) ) \in \mathcal { A } _ { q }$ , where $\mathcal { A } _ { \boldsymbol { q } } \subseteq \mathcal { E }$ .

# Methodology

In this section, as shown in Figure 2, we begin with multiaspect knowledge retrieval. We then delve into two specific modules, providing detailed explanations of their functionalities, and demonstrate the process of querying KG using logical form.

# Retrieval of Multi-Aspect Knowledge

Previous KGQA methods either focus on retrieving entities and relations (Hu et al. 2022) or retrieving multiple triplets to form subgraphs (Yu et al. 2022; He et al. 2024). In our

Logical Form: (AND (JOIN base. biblioness. Answer: ESliemileanrti-tywi&seSiPgromdouicdt bibs_location.loc_type Australia, Fiji, New CRaelefidnoeni&a Q…u…ery ↑ LLM LoRA 🔥 KG   
Siamese Siamese Siamese Australia   
Network Network Network ↑ ↑ ↑ ↑ ↑ → New Caledonia -↑----↑- +---+------- Pacific Ocean   
𝑒! 𝒒 𝑟! 𝒒 𝑠! 𝒒 → ↑ ↑ ↑ ▲ → East Timor Indonesia ↑ ↑ ↑ Relevance Gating🔥 𝒒: what countries Oceania Self Cross Self included in Retrieve   
Attention Attention Attention Self-Alignment🔥 ocean🔥iaT?unable Fiji Geographical region ↑ + base.aareas.schema …… ↑ Multi-Aspect location.location.contains Entity Relation Subgraph Entity Relation Subgraph Knowledge location.location.containedby

research, we propose to leverage three types of knowledge simultaneously. Each possesses varying aspects of information, which complement each other and also share commonalities. This enables us to effectively align crucial knowledge across three types. Due to the differences in their representations (e.g., length and format), we employ different retrieval methods for each type.

Entity Retrieval. One effective approach for retrieving candidate entities $k _ { e }$ is to conduct entity linking with question $q$ . Following Hu et al. (2022), we employ the ELQ (Li et al. 2020) for question entity linking, which utilizes a biencoder to simultaneously detect mentions and link them to entities end-to-end. Then FACC1 (Gabrilovich, Ringgaard, and Subramanya 2013) (a comprehensive Freebase annotation of corpora) is employed to identify entities that were not linked by ELQ, to enhance the range of candidate entities.

Relation Retrieval. In large-scale KG (e.g. Freebase), relations are typically organized hierarchically, such as the example base.biblioness.bibs location.loc type. Therefore, directly using question-based dense retrieval for similarity may not be effective. To address this, following Hu et al. (2022), we propose masking entity mentions detected during the candidate entity retrieval stage with a [BLANK] token for each question $q$ . Building on the work of Hu et al. (2022); Das et al. (2021), we train two separate BERT models that encode questions and relations into a shared dense space. The objective of optimization is to maximize the score of the relevant relation compared to randomly sampled relations. To retrieve the nearest relations, we employ FAISS (Douze et al. 2024), a highly efficient vector database, which allows us to speed up the search process and obtain the most relevant results.

Subgraph Retrieval. One crucial consideration is the wealth of structural and semantic information contained within KG. Since KG data is typically stored as triplets, we linearize triplets by combining head entity, relation, and tail entity for retrieval. Following (Yu et al. 2022), we propose grouping linearized sentences with the same head entity into a document. To save computing resources, we only focus on 1-hop subgraphs to capture structural information. Furthermore, concerning the potential information loss when converting long documents into vectors, we employ sparse retrieval approaches that rely on keyword dependencies. Specifically, we employ techniques like BM25, which calculates TF-IDF scores based on sparse word matches between input questions and KB-linearized passages.

# Self-Alignment

Although multi-aspect retrieval offers comprehensive auxiliary information, it can also introduce irrelevant knowledge and noise, resulting in negative impacts. To address this concern, we propose to align the commonalities among multiaspect information to improve informativeness.

We first utilize LLM’s embedding layer to convert the multi-aspect retrieval knowledge into text embeddings $\boldsymbol { X } _ { e } \in \mathbb { R } ^ { t _ { k } \times i \times e }$ , where the $t _ { k }$ represent top $k$ retrieval texts, $l$ denotes the maximum length of text, and $e$ indicate the embedding dim of the token. We then apply an averaging operation, followed by a projector network $\mathcal { M }$ , which transforms these embeddings into prompt embeddings $\boldsymbol { X } _ { t } ~ \in ~ \mathbb { R } ^ { t _ { k } \times e }$ (i.e., one piece of retrieval text is projected to one token embedding). For the sake of efficiency, $\mathcal { M }$ is designed to consist of down-projection and up-projection layers, with a nonlinear layer situated between them, as follows:

$$
X _ { e } = E m b e d d i n g s ( T ) ,
$$

$$
\boldsymbol { X } _ { t } = \mathcal { M } ( \hat { \boldsymbol { X } } _ { e } ) , \quad \hat { \boldsymbol { X } } _ { e } = \frac { 1 } { l } \sum _ { i = 1 } ^ { l } \boldsymbol { X } _ { e } [ : , i , : ] ,
$$

where $T$ represents the retrieval text of entities, relations, or subgraphs. Next, we apply self-attention separately to the prompt embeddings of entities and relations to obtain entityconsistency tokens $e _ { c }$ and relation-consistency tokens $\pmb { r } _ { c }$ :

$$
\begin{array} { r } { \pmb { e } _ { c } = S e l f \mathbf { - } A t t n ( \pmb { E } _ { t } ) \in \mathbb { R } ^ { t _ { k } \times e } , } \\ { \pmb { r } _ { c } = S e l f \mathbf { - } A t t n ( \pmb { R } _ { t } ) \in \mathbb { R } ^ { t _ { k } \times e } , } \end{array}
$$

Here, we use $\boldsymbol { S } _ { t }$ , $\pmb { E } _ { t }$ , and $\scriptstyle { R _ { t } }$ represent prompt embeddings $X _ { t }$ of subgraphs, entities, and relations, respectively. This attention is applied to individual retrieval text (e.g., one sentence) rather than individual tokens to learn the correlation and consistency between different retrieval information and their importance within the entire top $k$ retrieval data.

In addition, we further focus on leveraging subgraphs information. To determine the significance of retrieval text, we employ entities and relations as alignment factors. As shown in Figure 2, by aligning triples in a subgraph that is highly consistent to relation ‘location.location.contains’, we can weight important data through commonalities of multi-aspect knowledge. To obtain subgraph-consistency tokens $s _ { c }$ , we perform cross-attention to entity-subgraph and relation-subgraph pairs, respectively:

$$
\begin{array} { r } { \pmb { s } _ { c } ^ { e } = C r o s s \mathbf { - } A t t n ( \pmb { S } _ { t } , \pmb { E } _ { t } , \pmb { E } _ { t } ) \in \mathbb { R } ^ { t _ { k } \times e } , } \\ { \pmb { s } _ { c } ^ { r } = C r o s s \mathbf { - } A t t n ( \pmb { S } _ { t } , \pmb { R } _ { t } , \pmb { R } _ { t } ) \in \mathbb { R } ^ { t _ { k } \times e } , } \end{array}
$$

and sum the results to get $\pmb { s } _ { c } = \pmb { s } _ { c } ^ { e } + \pmb { s } _ { c } ^ { r } \in \mathbb { R } ^ { t _ { k } \times e }$ . The consistency tokens of entity $e _ { c }$ , relation $\pmb { r } _ { c }$ , and subgraph $\scriptstyle { \pmb { s } } _ { c }$ contain refined knowledge aligned between multi-aspect information, enhancing the utilization of the retrieved data.

# Relevance Gating

After obtaining the consistency tokens, we expect the model to learn the relevance between retrieval data and the question. The relevance is used to construct a soft gating mechanism, which will adaptively select the relevant retrieval information to be utilized. To achieve this, we design a siamese network for each type of consistency token to measure its relevance to the question embedding $Q _ { e } \in \mathbb { R } ^ { l \times e }$ . Each siamese network consists of a shared MLP $\mathcal { M } _ { s h a r e }$ networks that processes both question embedding and consistency tokens, the generated $\mathbf { \boldsymbol { q } } _ { m } \in \mathbb { R } ^ { l \times e }$ and $\bar { \mathbf { x } _ { c } } \in \mathbb { R } ^ { t _ { k } \times e }$ are then used to calculate similarity score $G _ { s i m } \ \in \ \mathbb { R } ^ { t _ { k } \times l }$ through a batch matrix multiplication. This score is subsequently averaged, and a sigmoid activation function is applied to produce the final relevance score $g$ , as described below:

$$
\begin{array} { l } { q _ { m } = \mathcal { M } _ { s h a r e } ( Q _ { e } ) , \quad x _ { c } = \mathcal { M } _ { s h a r e } ( X _ { c } ) , } \\ { \displaystyle g = S i g m o i d ( \frac { 1 } { l } \sum _ { i = 1 } ^ { l } G _ { s i m } [ : , i ] ) , \quad G _ { s i m } = x _ { c } \cdot q _ { m } ^ { T } . } \end{array}
$$

Here $\pmb { X } _ { c }$ can be denoted as entities, relations, or subgraphs consistency tokens. The relevance score of entity $\pmb { g } _ { e }$ , relations ${ \pmb g } _ { r }$ and subgraph $g _ { s }$ serve as soft gates, are used to model the influence of each consistency tokens by elementwise product, respectively:

$$
\pmb { e } _ { c } ^ { w } = \pmb { g } _ { e } \circ \pmb { e } _ { c } , \quad \pmb { r } _ { c } ^ { w } = \pmb { g } _ { r } \circ \pmb { r } _ { c } , \quad \pmb { s } _ { c } ^ { w } = \pmb { g } _ { s } \circ \pmb { s } _ { c } .
$$

To enhance generalization ability, we introduce randomly initialized soft tokens $\boldsymbol { p } \in \mathbb { R } ^ { l \times e }$ , which are concatenated with the weighted consistency tokens and the question embedding. These combined embeddings are then fed into LLMs to generate logical form $\mathcal { F }$ as follows:

$$
\mathcal { F } = f _ { \theta , \phi _ { 1 } , \phi _ { 2 } } ( [ p ; e _ { c } ^ { w } ; \pmb { r } _ { c } ^ { w } ; \pmb { s } _ { c } ^ { w } ; \pmb { Q } _ { e } ] ) ,
$$

where $\left[ ; \right]$ represents concatenation operation, the parameters $\theta$ of LLMs itself are frozen. The parameters that require back-propagation optimization include our model parameters $\phi _ { 1 }$ and LoRA parameters $\phi _ { 2 }$ .

# Query Execution

Due to the long-tail distribution of fine-tuned data and the lack of specific knowledge, LLM may not strictly adhere to the contextual information provided (Luo et al. 2023). As a result, the generated logical forms may contain non-existent entities or relations. For instance, when asked ‘where was rihanna born and raised?’, an LLM might generate the logical form ‘(JOIN (R people.person.place of birth) Rihana)’ instead of the correct spelling ‘Rihanna’. This discrepancy renders the logical form non-executable on KG, and the same issue can also arise with relations.

To further refine the quality of entity and relation, we employ a similarity-based approach with KG. Specifically, we utilize an unsupervised SimCSE model to measure the similarity between each entity in the generated logical form $\mathcal { F }$ and the labels of entities in the entity set $\mathcal { E }$ . By setting a threshold, we retain the most relevant entities $\mathcal { E } _ { s u b }$ . Additionally, we query the KG to identify relations $\mathcal { R } _ { 2 - h o p }$ that are within 2 hops of the obtained subset of entities $\dot { \mathcal { E } } _ { s u b }$ . Similarly, we calculate the similarity between all relations in $\mathcal { F }$ and the identified set of relations. This refinement process allows us to generate a new list of candidate logical forms $\mathcal { F } _ { n e w }$ that better align with the KG. After converting to SPARQL language, it can be used to query answers from KG: $a = q u e r y ( c o n v e r t ( \mathcal { F } _ { n e w } ) )$ .

# Experiments

# Experiment Settings

Datasets. Our experiments were conducted using two wellknown datasets: WebQuestionsSP (WebQSP) (Yih et al. 2016) and ComplexWebQuestions (CWQ) (Talmor and Berant 2018). Both datasets contain SPARQL queries that correspond to the questions and can be executed on Freebase to obtain answers.

Baselines. In this study, we evaluate performance with 22 baselines, which are categorized into four groups: embedding-based (EM-based), information retrieval-based (IR-based), semantic parsing-based (SP-based), and LLMbased methods. It is important to note that some methods, such as DecAF (Yu et al. 2022), can be classified as multiple groups, specifically IR-based and SP-based. To ensure fairness, we do not include the results of using the oracle entity linking annotations setting, such as RoG (LUO et al. 2024). Evaluation Metrics. We use Hits $\ @ 1$ , F1, and Acc as primary evaluation metrics following (Luo et al. 2023). Hits $@ 1$ assesses the accuracy of top-1 predicted answer, F1 considers the coverage of all possible answers, and Acc measures the strict exact-match accuracy. We further assess the quality of generated S-expressions by employing two metrics: the extract match ratio (EM) and the match after beam search ratio (BM) with ground-truth S-expressions, for analytical experiments.

Table 1: Performance comparison of different types of KGQA methods on WebQSP and CWQ datasets. We present the Mean scores and standard deviations (mean $\pm$ std) of five experiments with different random seeds. The best result is highlighted in bold, and the baseline results are taken from corresponding papers.   

<html><body><table><tr><td rowspan="2">Type</td><td rowspan="2">Model</td><td colspan="3">HebQOSP</td><td colspan="3">Hw@1</td></tr><tr><td>F1</td><td></td><td>Acc</td><td>F1</td><td></td><td>Acc</td></tr><tr><td rowspan="4">EM-based</td><td>KV-Mem (Miler etal. 2016)</td><td>34.5</td><td>46.7</td><td>-</td><td>15.7</td><td>18.4</td><td>1</td></tr><tr><td>TraMstrNet (tl2a121)</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>67.4</td><td>744</td><td>__</td><td>44.0</td><td>488</td><td>__</td></tr><tr><td>KGT5 (Saxena et al. 2022)</td><td>-</td><td>56.1</td><td></td><td>-</td><td>36.5</td><td></td></tr><tr><td rowspan="5">IR-based</td><td>GraftNet (Sun et al. 2018)</td><td>60.4</td><td>66.4</td><td>-</td><td>32.7</td><td>36.8</td><td>-</td></tr><tr><td>PullNet (Sun et al. 2019)</td><td></td><td>68.1</td><td>-</td><td></td><td>45.9</td><td>-</td></tr><tr><td>SR+NSM (Zhang et al. 2022)</td><td>64.1</td><td>68.9</td><td></td><td>47.1</td><td>50.2</td><td></td></tr><tr><td>SR+NSM+E2E (Zhang et al.2022)</td><td>64.1</td><td>69.5</td><td>-</td><td>46.3</td><td>49.3</td><td></td></tr><tr><td>UniKGQA (Jiang et al. 2023b)</td><td>71.0</td><td>77.0</td><td></td><td>49.4</td><td>50.9</td><td></td></tr><tr><td rowspan="6">SP-based</td><td>CBR-KBQA (Das et al. 2021)</td><td>72.8</td><td>-</td><td>69.9</td><td>70.0</td><td></td><td>67.1</td></tr><tr><td>GMT-KBQA (Hu et al. 2022)</td><td>76.6</td><td></td><td>73.1</td><td>77.0</td><td>-</td><td>72.2</td></tr><tr><td>Unif-G(e</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>73.9</td><td>__</td><td>__</td><td>68.8</td><td>--</td><td>__</td></tr><tr><td>DecAF (Yu et al. 2022)</td><td>78.8</td><td>82.1</td><td></td><td>-</td><td>70.4</td><td></td></tr><tr><td>FC-KBQA (Zhang et al. 2023)</td><td>76.9</td><td>1</td><td></td><td>56.4</td><td>-</td><td></td></tr><tr><td rowspan="7">LLM-based</td><td>KD-CoT (Wang et al. 2023)</td><td>52.5</td><td>68.6</td><td>-</td><td>-</td><td>55.7</td><td>-</td></tr><tr><td>Pangu (Gu,Deng,and Su 2023)</td><td>79.6</td><td></td><td>-</td><td>-</td><td></td><td>-</td></tr><tr><td>StructGPT (Jiang et al. 2023a)</td><td>=</td><td>72.6</td><td>1</td><td>-</td><td></td><td>1</td></tr><tr><td>ChatKBQA (Luo et al. 2023)</td><td>79.8</td><td>83.2</td><td>73.8</td><td>77.8</td><td>82.7</td><td>73.3</td></tr><tr><td>ToG-R (GPT-4) (Sun et al. 2024)</td><td>--</td><td>82.6</td><td>1</td><td>-</td><td>69.5</td><td></td></tr><tr><td>G-Retriever (He et al. 2024)</td><td></td><td>70.1</td><td></td><td></td><td></td><td>-</td></tr><tr><td>GNN-RAG (Mavromatis et al. 2024)</td><td>73.5</td><td>82.8</td><td></td><td>60.4</td><td>62.8</td><td>-</td></tr><tr><td></td><td>AMAR (Ours)</td><td>81.2±0.15</td><td>84.3±0.16</td><td>75.2±0.10</td><td>78.5±0.11</td><td>83.1±0.09</td><td>74.5±0.07</td></tr></table></body></html>

Implementation Details. Following Luo et al. (2023), we fine-tune LLaMA2-7B on WebQSP and LLaMA2-13B on CWQ using LoRA. We evaluate the impact of backbones and fine-tuning methods in our subsequent experiments. During inference, we utilize beam search to generate multiple logical forms. We select the executable logical form with the highest score to obtain answers. All experiments were done on NVIDIA A6000 GPUs. We only searched the number of retrievals $k$ with values of $\{ 4 , 8 , 1 6 , \dot { 3 } 2 , 6 4 , 1 0 0 \}$ .

# Main Results

As observed from Table 1, AMAR outperforms all baselines across all metrics on both datasets. Notably, on WebQSP dataset, accuracy has improved by $1 . 6 \%$ compared to the second-best baseline, ChatKBQA, marking new stateof-the-art performance. Specifically, AMAR surpasses subgraph retrieval techniques such as $\mathbf { S R + N S M }$ and DecAF with $2 6 \%$ and $2 . 4 \%$ F1 improvements on WebQSP, respectively, as well as entity and relation retrieval methods like GMT-KBQA with $5 . 3 \%$ F1 improvement on WebQSP. This can be attributed to our proposed self-alignment mechanism, which effectively aligns multi-aspect knowledge. On the other hand, AMAR also outperforms other LLM-based approaches, such as G-Retriever and ChatKBQA, suggesting that our approach of learning prompt embeddings for retrieval can more flexibly leverage the capabilities of LLMs to utilize retrieval knowledge.

Table 2: Ablation study of sub-modules on WebQSP dataset.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="5">WebQSP</td></tr><tr><td>F1</td><td>Hits@1</td><td>Acc</td><td>EM</td><td>BM</td></tr><tr><td>AMAR</td><td>81.2</td><td>84.3</td><td>75.2</td><td>63.9</td><td>76.4</td></tr><tr><td>w/o SN</td><td>80.1</td><td>83.3</td><td>74.3</td><td>63.6</td><td>77.0</td></tr><tr><td>w/o SA</td><td>79.5</td><td>82.6</td><td>73.8</td><td>63.0</td><td>75.7</td></tr><tr><td>W/o RG</td><td>79.4</td><td>82.2</td><td>74.3</td><td>63.4</td><td>76.7</td></tr><tr><td>w/o SA&RG</td><td>78.7</td><td>81.0</td><td>72.5</td><td>62.1</td><td>73.6</td></tr><tr><td>w/o SA&SN</td><td>79.1</td><td>82.0</td><td>73.8</td><td>63.1</td><td>76.2</td></tr><tr><td>w/o ALL</td><td>76.2</td><td>79.5</td><td>70.1</td><td>59.7</td><td>72.4</td></tr></table></body></html>

# Ablation Study

In this section, we conduct a series of ablation studies to address the following question:

How do the proposed modules improve performance? Specifically, we conduct experiments against five variants: 1) w/o SN: without siamese network, and relevance score is calculated by vector inner product; 2) w/o $S A$ : without self-alignment module; 3) w/o $R G$ : without relevance gating module; 4) w/o SA&RG: without both self-alignment and relevance gating modules, where AMAR obtains prompt embeddings with only MLP. 5) w/o ALL: directly appending the retrieval knowledge as context instead of converting retrieval knowledge to prompt embeddings. As shown in Table 2, we observe that the performance on most metrics decreases when either SN, SA, or $R G$ is removed. This validates the effectiveness of the proposed sub-modules. Furthermore, we find that performance significantly drops when the entire framework is removed, indicating that appending retrieved knowledge directly as context text introduces a large amount of noise, preventing LLMs from focusing on learning the mapping from question to logical form. Additionally, we notice that the BM is higher in AMAR w/o SN than in AMAR . This suggests that the ground-truth logical forms are mostly ranked within the top 2 or lower positions during beam search generation, leading to a lower EM.

Table 3: Quantitative comparison of the impacts of retrieval information on AMAR ’s performance.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="4">WebQSP</td></tr><tr><td>F1</td><td>Hits@1</td><td>Acc EM</td><td>BM</td></tr><tr><td>AMAR</td><td>81.2</td><td>84.3</td><td>75.2</td><td>63.9 76.4</td></tr><tr><td>w/o Relation</td><td>78.6</td><td>81.7</td><td>73.1 63.0</td><td>74.9</td></tr><tr><td>w/o Entity</td><td>79.2</td><td>82.2</td><td>73.5 63.8</td><td>74.4</td></tr><tr><td>w/o Subgraph</td><td>79.7</td><td>82.9</td><td>73.8 63.5</td><td>75.1</td></tr></table></body></html>

What impacts do different aspects of retrieval information have on performance? To preserve the integrity of AMAR, we remove retrieval knowledge by replacing the text embedding with randomly initialized ones. As shown in Table 3, the results indicate obvious performance drops after removing retrieval knowledge, including ‘subgraph’, ‘entity’, and ‘relation’. This decline highlights the significance of different aspects of the retrieval knowledge on the overall performance. Further analysis reveals that removing the ‘relation’ component results in the largest drop in performance, suggesting that ‘relation’ plays a crucial role in generating logical expressions. Instead, while the ‘subgraph’ still contributes to performance, it appears to be less critical for logical forms than ‘relation’ or ‘entity’. These findings provide valuable insights for further optimization of the model.

# Number of Retrieval Analysis

In this section, we explore the impact of the number of retrieval knowledge. We compare the approach of directly inputting retrieved data as a context prompt. If the input exceeds the maximum context limit (i.e., 4096 for LLaMA2), we truncate the retrieved information from subgraphs. As shown in figure 3, it can be observed that when the amount of retrieved data is relatively small, our method does not significantly differ from Context Prompt, which suggests that useful information recalled is still limited. However, as the quantity of retrieved data increases (e.g., reaching 64 or 100), our method achieves a substantial performance improvement, while context prompt drastically declines. This demonstrates that introducing a long context results in substantial noise, making it difficult for LLMs to learn important data. In contrast, by treating retrieved information as an individual prompt embedding, we avoid the issue of excessively long inputs and better utilize the rich information.

# Efficiency of Fine-Tuning

In this section, we analyze the efficiency of our method combined with different fine-tuning methods, including Prompt

![](images/48f44826a5f3aecd00008c896bdc581fa8cbcaa49c567048e3c6ed68f59612bb.jpg)

![](images/df3efd86ce8f7c7c228b0762af088e525014ca3fc87ad7d629587981a1c2f81b.jpg)  
Figure 3: Performance vary with the number of Re- Figure 4: Combine with diftrieval. ferent fine-tuning methods.

Tuning (PT) (Lester et. al 2021) and Low-Rank Adaptation (LoRA). To ensure fairness, we conduct fine-tuning experiments without our module by concatenating the retrieved knowledge text with the input context. As shown in Figure 4, we observe a significant improvement in performance after incorporating AMAR, Regardless of whether PT or LoRA is used, our method consistently outperforms baselines. Notably, the combination of our method with PT+LoRA finetuning yields the best results. This highlights the capability of AMAR to effectively learn from retrieved information, while the direct concatenation of context introduces considerable noise. Furthermore, we find that LoRA fine-tuning outperforms PT fine-tuning. This can be attributed to the inherent complexity of logical form generation in the KGQA task. LoRA has more tunable parameters and can act on all project layers, thus enabling LLMs to better adapt to tasks.

# Case Study

In this section, we present a case study to illustrate how our model adaptively learns the importance of retrieval knowledge. We have not presented a case for subgraph retrieval due to its extensive length. As shown in Table 4, our method is proven effective in assigning high scores to correct entities and relations for entity retrieval and relation retrieval, while irrelevant or misleading information receives low scores. For instance, the entity ‘Harper Lee’ received a score of [0.9272], and relation ‘people.person.education’ received a score of [0.9844]. Nevertheless, when retrieved text is directly used as the context prompt, it is susceptible to interference from erroneous information in retrieved data. This can result in generating incorrect relations, such as ‘education.educational.institution.school type’. This example highlights that our method not only enhances the performance of LLM in KGQA but also improves the quality of retrieved information by setting weight.

# Analysis of LLM Backbones

In this section, we investigate the question: Does the performance improvement of our method solely come from LLMs? To answer this, we conduct experiments and compare results using different LLMs as backbones: LLaMA2- 7B with fine-tuning, LLaMA2-13B with fine-tuning, and

Table 4: A case study on WebQSP, where the ‘[float]’ represents the scores assigned to each retrieval information, indicatin the level of influence it has on the model and the text with underline means erroneous generation.   

<html><body><table><tr><td colspan="2">Question</td><td>what highschool did harper lee go to?</td><td></td></tr><tr><td colspan="2">Entity Retrieval</td><td></td><td>① Harper Lee [0.9272],② Senior secondary education [0.5441], ③ Lee Remick [0.6482],④ Secondary edu- cation [0.5412],③ Barbara Kingsolver [0.4320],③High school movement [0.6758]</td></tr><tr><td colspan="2">Relation Retrieval</td><td></td><td>①people.person.education [0.9844], ②education.education.institution [0.9844], ③ educa- tion.educational_institution.school_type [0.7281]， ④ education.school.lowest-grade_taught [0.5469]，③ education.school_mascot.school [0.5431],? common.topic.notable_types [0.8252]</td></tr><tr><td>Logical AMAR</td><td>Form</td><td>by</td><td>(AND (JOIN common.topic.notable_types High school） (JOIN (R education.education.institution)(JOIN (R people.person.education)HarperLee))）</td></tr><tr><td rowspan="2">Logical Context Prompt</td><td rowspan="2">Form</td><td rowspan="2">by</td><td>(AND (JOIN education.educational.institution.school_type School) (JOIN (R education.education.institution) (</td></tr><tr><td>JOIN (R people.person.education) Harper Lee))） X</td></tr></table></body></html>

Table 5: Analysis on different LLM backbones.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="2">WebQSP</td><td colspan="2">CWQ</td></tr><tr><td></td><td>Hits @1</td><td>Acc</td><td>Hits@1</td></tr><tr><td colspan="5">GPT-4</td></tr><tr><td>ToG-R</td><td>-</td><td>82.6</td><td></td><td>69.5</td></tr><tr><td colspan="5">LLaMA2-7B</td></tr><tr><td>G-Retriever</td><td>-</td><td>70.1</td><td></td><td>1</td></tr><tr><td>GNN-RAG</td><td>-</td><td>82.8</td><td></td><td>62.8</td></tr><tr><td>ChatKBQA</td><td>73.8</td><td>83.2</td><td>73.0</td><td>82.3</td></tr><tr><td>AMAR</td><td>75.2</td><td>84.3</td><td>73.4</td><td>82.9</td></tr><tr><td colspan="5">LLaMA2-13B</td></tr><tr><td>ChatKBQA</td><td>73.1</td><td>82.7</td><td>73.3</td><td>82.7</td></tr><tr><td>AMAR</td><td>74.7</td><td>83.3</td><td>74.5</td><td>83.1</td></tr></table></body></html>

GPT-4 (frozen). In Table 5, we observe that AMAR with LLaMA2-7B and 13B as backbones outperform the baselines using the same backbone, such as ChatKBQA (Luo et al. 2023), and both even surpass ToG-R (Sun et al. 2024) using GPT-4 as the backbone on two datasets. These results suggest that the performance gain of AMAR is not solely attributed to the use of more capable LLMs but rather to the proposed utilization of the commonality between multiaspect knowledge and the relevance of the question. We found that LLaMA2-13B perform worse than LLaMA2-7B on WebQSP. We believe the reason lies in dataset characteristics: the scale of WebQSP (1) is much smaller than that of CWQ, and (2) WebQSP has a maximum complexity, only consisting of 2-hop questions. This may cause the LLaMA2- 13B to overfit, leading to reduced performance.

# Related Work

Knowledge Graph Question Answering (KGQA). KGQA aims to answer questions over KG, and previous methods are usually categorized as EM-based, IR-based, SP-based and LLM-based methods. EM-based methods encode the entities and relations in the embedding space and reason final answer using these structural embeddings (Shi et al. 2021). IR-based KBQA methods propose to retrieve and re-rank answers from KGs given information conveyed in the question (Chen et al. 2019; Zhang et al. 2022). SP-based methods, focus on transforming question into a structural query, such as SPARQL and S-expression, and reason final answers using these executable queries (Liang et al. 2017; Lan and Jiang 2020). Besides, recent attempts have been made to utilize LLM-based methods for KGQA (Jiang et al. 2023c). For instance, LUO et al. (2024) presents a planning-retrievalreasoning pipeline. ToG (Sun et al. 2024) proposes to interactively explore paths and reasoning on KGs using LLM as an agent. Despite their significant improvements, a substantial challenge persists when handling multi-aspect retrieved data as input, which may introduce irrelevant knowledge. However, our method is capable of aligning knowledge and implementing adaptive relevance gating with questions, thus addressing this issue.

Large Language Model Reasoning. Considering the impressive abilities of LLMs, some previous works focus on facilitating LLMs’ reasoning via prompting (He et al. 2021; Das et al. 2021; Jiang et al. 2023a; Wang et al. 2023). To overcome the unfaithful reasoning of LLMs, Jiang et al. (2023a) proposes StructGPT, an iterative reading-thenreasoning framework, to improve reasoning of LLMs when handling structured data. KD-CoT is designed to formulate chain-of-thought into a multi-round QA format and LLMs can retrieve external knowledge during interaction (Wang et al. 2023). However, LLMs reasoning is challenging when leveraging information present in lengthy texts (Wang et al. 2024). In contrast, our model facilitates efficient selection of information by utilizing prompt embeddings, thereby mitigating the issue of excessive context.

# Conclusion

In this work, we propose a novel approach to enhance LLMs reasoning and factual output by retrieving multiaspect knowledge from KGs. By employing self-alignment and relevance gating modules, AMAR adaptively enhances and selects relevant information. It has proven more effective than simply appending retrieved text to input context, as it minimizes noise interference. Through extensive experiments, AMAR outperforms 22 baseline models and achieves a new state-of-the-art performance.