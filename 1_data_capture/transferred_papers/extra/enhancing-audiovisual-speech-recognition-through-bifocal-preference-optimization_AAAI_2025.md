# Enhancing Audiovisual Speech Recognition Through Bifocal Preference Optimization

Yihan $\mathbf { W } \mathbf { u } ^ { 2 , 1 }$ , Yichen ${ \bf L u } ^ { 1 }$ , Yifan Peng1, Xihua Wang2, Ruihua $\mathbf { S o n g ^ { 2 * } }$ , Shinji Watanabe1\*

1Carnegie Mellon University, 2Gaoling School of Artificial Intelligence, Renmin University of China, yihanwu, xihuaw, rsong @ruc.edu.cn, yichenl5, yifanpen, swatanab @andrew.cmu.edu

# Abstract

Audiovisual Automatic Speech Recognition (AV-ASR) aims to improve speech recognition accuracy by leveraging visual signals. It is particularly challenging in unconstrained realworld scenarios across various domains due to noisy acoustic environments, spontaneous speech, and the uncertain use of visual information. Most previous works fine-tune audio-only ASR models on audiovisual datasets, optimizing them for conventional ASR objectives. However, they often neglect visual features and common errors in unconstrained video scenarios. In this paper, we propose using a preference optimization strategy to improve speech recognition accuracy for realworld videos. First, we create preference data via simulating common errors that occurred in AV-ASR from two focals: manipulating the audio or vision input and rewriting the output transcript. Second, we propose BPO-AVASR, a Bifocal Preference Optimization method to improve AV-ASR models by leveraging both input-side and output-side preference. Extensive experiments demonstrate that our approach significantly improves speech recognition accuracy across various domains, outperforming previous state-of-the-art models on real-world video speech recognition.

Code — https://github.com/espnet/espnet.

# 1 Introduction

Recently, there has been a growing demand for Automatic Speech Recognition (ASR) systems to evolve into unconstrained audiovisual scenarios, such as YouTube videos, online meetings, and live broadcasts. With both audio and visual streams as input, visual information may help improve speech recognition accuracy, especially in cases where the audio is noisy or unclear. There are usually two scenarios for AudioVisual ASR (AV-ASR), focusing solely on lip motion (Afouras et al. 2022; Ma et al. 2021; Chung et al. 2017) and using full-frame visual features. In this work, we focus on the latter scenario, improving the AV-ASR performance in unconstrained real-world video scenarios. In such settings, the entire visual frame may contribute to ASR performance by providing additional cues on specific objects, background location, or context.

![](images/94e3b8e48f2d0f56db958bb14fc8b160f7c0d52d3d8206f5b9b02c6727bbdfaa.jpg)  
Figure 1: Overview of our proposed framework. We construct bifocal preferences by augmenting transcript (T), audio (A) or video (V), resulting in two types of preference pairs: input-side (A and V) and output-side (T). Through bifocal preference optimization, BPO-AVASR learns to generate transcripts that are better aligned with these preferences.

Compared to standard ASR, performing AV-ASR on realworld videos presents the following challenges:

• Noisy Acoustic Environments. Homophone issues pose a significant challenge for both standard ASR and AVASR. Real-world recordings with variable acoustic environments further severe homophone problems. • Spontaneous Speech Scenarios. Real-world videos often contain spontaneous conversations, which are more variable than read speech or lectures, making accurate recognition more challenging. • Uncertain Use of Vision. The diversity of video domains and unconstrained scenarios leads to the uncertain use of visual information for AV-ASR systems. While some visual cues are irrelevant, others provide crucial localized details or global context for understanding speech, making it challenging to effectively leverage visual information for improved recognition accuracy.

Most current AV-ASR works (Sanabria et al. 2018; Ghorbani et al. 2021; Caglayan et al. 2019; Seo et al. 2023) build upon ASR models by incorporating visual features as conditions. These models typically adhere to the original ASR training objective, primarily involving the next-token prediction tasks based on the original audiovisual data. However, these supervised fine-tuning (SFT) based methods do not specifically optimize for visual inputs or address scenarios like noisy acoustic environments and spontaneous conversations, which are common in unconstrained AV-ASR tasks. Consequently, this approach may lead the AV-ASR model to neglect visual features or struggle to adapt to real-world videos that feature noisy and spontaneous speech.

To overcome the aforementioned challenges, we propose a Bifocal Preference Optimization based AV-ASR (BPOAVASR) framework, which aims to facilitate the development of AV-ASR models with stronger speech recognition ability in unconstrained real-world video scenarios (as shown in Figure 1). Specifically, we propose a bifocal preference optimization strategy, where one bifocal point is inputside preference and the other focal point is output-side preference. Accordingly, we construct the bifocal preference dataset by simulating common errors associated with the above challenges, including homophone errors, poor performance on spontaneous speech, and inadequate use of visual cues. Subsequently, we leverage these pairs to optimize the AV-ASR model’s preferences. By emphasizing the generation of correct transcripts considering unconstrained audiovisual inputs, BPO-AVASR can better adapt to recognizing speech for real-world videos. The main contributions of this work are summarized as follows:

• We introduce BPO-AVASR, a novel framework with Bifocal Preference Optimization specifically to align audio-only ASR models with AV-ASR tasks, optimizing AV-ASR as a preference alignment problem. • We design bifocal preference optimization, a training strategy that optimizes preference for AV-ASR tasks considering both input-side preference and output-side preference. Specifically, we construct the preference data pairs to simulate the common errors in unconstrained video AV-ASR, creating negative samples by augmenting audio, visual, and transcript information. • Through extensive experiments, we demonstrate the effectiveness of our approach, showing a notable enhancement compared with SFT-based models. Additionally, BPO-AVASR achieves superior performance compared to previous state-of-the-art models across three datasets.

# 2 Related Works

# 2.1 Audiovisual Speech Recognition

Recent state-of-the-art ASR models have achieved impressive performance on audio-only benchmarks (Li 2021; Prabhavalkar et al. 2024). Whisper-style ASR models (Radford et al. 2023; Peng et al. 2024), in particular, leverage large-scale supervised learning to achieve robust results across various benchmarks. Building on pre-trained ASR models, several studies explore ASR to audiovisual scenarios (Gabeur et al. 2022; Peng et al. $2 0 2 3 \mathrm { a }$ ; Chung et al. 2017; Ghorbani et al. 2021). Most previous AV-ASR works focus on lip motion (Chung et al. 2017; Afouras et al. 2022; Ma et al. 2021). Moreover, to explore unconstrained AV-ASR for real-world videos, recent works explore different adaptation methods to fine-tune or retrain ASR models with fullframe visual features (Paraskevopoulos et al. 2020; Gabeur et al. 2022; Ghorbani et al. 2021; Peng et al. 2023a; Kumar et al. 2023; Lu et al. 2024). AVFormer (Seo et al. 2023) shows the state-of-the-art performance by integrating visual information into a frozen ASR model and fine-tuning it on the large-scale audiovisual dataset HowTo100M (Miech et al. 2019). However, these models are all trained or supervised fine-tuned toward standard ASR optimization objectives, leading to modality inefficient utilization of audio and visual information, particularly in unconstrained real-world video scenarios. In contrast, this work introduces a novel approach designed to adapt ASR models to unconstrained AVASR using bifocal preference optimization.

# 2.2 Direct Preference Optimization

To align a pre-trained Large Language Model (LLM) with specific preferences of downstream tasks, Direct Preference Optimization (DPO) (Rafailov et al. 2023) is proposed to optimize the LLM with a single-stage policy learning. DPO demonstrates strong performance while eliminating the need for a separate reward model. Intuitively, given the input $x$ and the outputs $y _ { w }$ and $y _ { l } { } ^ { 1 }$ , DPO maximizes the difference between the reward for the chosen output $r ( x , y _ { w } )$ and that for the rejected output $r ( x , y _ { l } )$ . Specifically, given a policy model to be optimized $\pi _ { \boldsymbol { \theta } }$ and a reference model $\pi _ { \mathrm { r e f } }$ , DPO formulates the reward as:

$$
r ( x , y ) = \beta \log \frac { \pi _ { \theta } ( y | x ) } { \pi _ { \mathrm { r e f } } ( y | x ) } + Z ( x ) ,
$$

where $Z ( x )$ is a partition function, and $\beta$ is the hyperparameter that controls the deviation from the reference model. Based on the Bradley-Terry model (Bradley and Terry 1952), the optimization objective of DPO becomes

$$
\begin{array} { r } { \mathcal { L } _ { \mathrm { D P O } } = - \log \sigma \left( \beta \log \frac { \pi _ { \theta } \left( y _ { w } | x \right) } { \pi _ { \mathrm { r e f } } \left( y _ { w } | x \right) } - \beta \log \frac { \pi _ { \theta } \left( y _ { l } | x \right) } { \pi _ { \mathrm { r e f } } \left( y _ { l } | x \right) } \right) , } \end{array}
$$

where $\sigma$ is the logistic function. Furthermore, the subsequent work of DPO (Rafailov et al. 2024) proves that DPO implicitly learns a token-level reward function, highlighting its potential of using DPO in token-level preference optimization, such as ASR and machine translation (Zeng et al. 2024).

In multimodal scenarios, recent works mainly focus on constructing multimodal preference data to support DPO training in the visual language domain (Wang et al. 2024; Zhang et al. 2024; Zhu et al. 2024). MDPO (Wang et al. 2024) designs a conditional preference optimization strategy to improve multimodal preference alignment in vision language scenarios, while SeVa (Zhu et al. 2024) constructs preference datasets via self-supervised learning. In this work, to the best of our knowledge, we first introduce DPO to AV-ASR tasks, showing a significant improvement

Type Ⅰ: Masked Audio Type Ⅲ: Homophone-based Generation Ground truth transcript: b Rejected prediction 1: Mask I’m gonna use denim blue rit I’m gonna use denim blue writ dye on these and black dylon die on these and black Dylon dye on these. die on these.   
Input audio stream Rejected audio Type Ⅳ: Spontaneous-based Generation Ground truth transcript: K Rejected prediction 2: I’m gonna use denim blue rit I’m going to use denim blue dye on these and black dylon writ die on these and black   
Type Ⅱ: Flipped Vision dye on these. Dylon die on these. Type Ⅴ: Vision-based Generation Flip Dense caption: she is holding a denim Rejected garment, with a bottle of prediction 3: Rit fabric softener or Flip Dylon detergent beside her I’m gonna use denim blue rip guy on these and Ground truth transcript: black dylan dye I’m gonna use denim blue rit dye on these. on these and black dylon dye on Input video stream Rejected video these. (a) Input-side preference construction (b) Output-side preference construction

over SFT-based AV-ASR models. Also, we leverage bifocal preference optimization, specifically designed for unconstrained AV-ASR tasks. Unlike DPO, which constructs negative samples by focusing solely on the output-side, BPO optimizes the AV-ASR model using both input-side preferences and output-side preferences.

# 3 Proposed Method

We describe our proposed BPO-AVASR framework in this section. To support bifocal alignment optimization, we design two key strategies to construct the bifocal preference dataset, constructing both input-side preferences and outputside preferences (Section 3.1). Then we outline the training procedure, including the supervised fine-tuning stage (Section 3.2) and the preference optimization stage (Section 3.3).

# 3.1 Creation of Bifocal Preference Dataset

Suppose we have an audiovisual speech recognition dataset $\mathcal { D } = \{ ( a ^ { i } , v ^ { i } , t ^ { i } ) \} _ { i = 1 } ^ { K }$ , which contains $K$ elements. Each element $\tau ^ { i } = ( a ^ { i } , v ^ { i } , t ^ { i } )$ consists of a speech $a ^ { i }$ , a video $v ^ { i }$ , and the corresponding transcript $t ^ { i }$ . For AV-ASR tasks, both $a ^ { i }$ and $v ^ { i }$ served as the inputs (denoted collectively as $c ^ { i }$ ), while the corresponding transcript $t ^ { i }$ is the output. Thus each element can be better denoted as $\tau ^ { i } = ( t ^ { i } | c ^ { i } ) ^ { \dot { } } = ( t ^ { i } | a ^ { i } , v ^ { i } )$ . Given the challenges of performing AV-ASR on real-world videos (as analyzed in Section 1), we construct preference pairs by manipulating three modalities, resulting in inputside preferences (audio, video) and output-side preferences (text). By simulating common errors in AV-ASR and constructing the preference dataset accordingly, the model can be optimized by maximizing the distinction between chosen and rejected pairs. This approach enables the model to learn to avoid these kinds of errors during inference.

Input-side Preference. To simulate insufficient use of input information, we create rejected samples by independently manipulating the audio and video inputs.

• Masked Audio. We mask certain audio frames to simulate situations where speech is noisy or ambiguous. In detail, we mask content words and add Gaussian noise to the corresponding audio input, as shown in Type I of Figure 2. Therefore, we obtain rejected sample $\tau _ { l } =$ $( t _ { w } ^ { i } | \check { c } _ { l } ^ { i } ) = ( t _ { w } ^ { i } | a _ { l } ^ { i } , v _ { w } ^ { i } )$ , which serves as a hard negative example to a chosen sample $\tau _ { w } = ( t _ { w } ^ { i } | a _ { w } ^ { i } , v _ { w } ^ { i } )$ . • Flipped Vision. To mimic the situation where the AVASR model uses visual information inadequately, we intentionally construct samples with rejected visual inputs. As shown in Type $\mathrm { I I }$ of Figure 2, we create the rejected sample $v _ { l } ^ { i }$ by flipping the video $v _ { w } ^ { i }$ to simulate the situation where the ASR model lacks sufficient visual information, especially for the detailed object information. Flipped images can make optical character recognition (OCR) harder and induce potential rejected responses. Therefore, we obtain the rejected sample $\tau _ { l } = ( t _ { w } ^ { i } | c _ { l } ^ { i } ) =$ $( t _ { w } ^ { i } | a _ { w } ^ { i } , v _ { l } ^ { i } )$ for a chosen sample $\tau _ { w } = ( t _ { w } ^ { i } | a _ { w } ^ { i } , v _ { w } ^ { i } )$ .

As a result, we obtain the preference dataset $\widetilde { D } _ { c }$

Output-side Preference. To construct a preference dataset that includes rejected samples simulating common errors in recognized text, we use ChatGPT as a proxy for the human generator. This approach allows us to build a cost-effective yet efficient output-side preference dataset. More specifically, for each ground truth element $\begin{array} { r c l } { \tau _ { w } } & { = } & { \big ( { t _ { w } ^ { i } | c _ { w } ^ { i } } \big ) ^ { \ast } = ( t _ { w } ^ { i } | { \bar { a } _ { w } ^ { i } } , v _ { w } ^ { i } ) } \end{array}$ , we generate a rejected text $t _ { l } ^ { i }$ to construct the corresponding rejected element $\tau _ { l } ~ = ~ ( t _ { l } ^ { i } | c _ { w } ^ { i } ) ~ = ~ ( t _ { l } ^ { i } | a _ { w } ^ { i } , v _ { w } ^ { i } )$ . To account for the different causes of text recognition errors, we employ three distinct prompts for the ChatGPT, each designed to generate rejected text based on specific error types.

• Homophone-based Generation. Given the ground truth text $t _ { w } ^ { i }$ , we prompt ChatGPT to replace the word with their homophone, e.g., “dye” is replaced by “die” (See Type III of Figure 2). We specifically focus on content words, as ASR errors frequently involve homophone and near-homophone errors in these words.

• Spontaneous-based Generation. For real-world videos, in addition to read speech, there is a significant amount of spontaneous speech. To address this error, we use ChatGPT to generate rejected text $t _ { l } ^ { i }$ from the ground truth text $t _ { w } ^ { i }$ . It simulates typical ASR errors in spontaneous scenarios, such as generating “going to” instead of “gonna” (See Type IV of Figure 2).

• Vision-based Generation. Neglecting or misusing visual information is another common error in AV-ASR. To simulate errors where visual information is ignored during recognition, we remove parts of the ground truth text that are related to the video. Specifically, we transfer the video stream $v _ { w } ^ { i }$ into dense and high-quality caption via ShareGPT4Video (Chen et al. 2024). Then, we use this caption and ground truth text $t _ { w } ^ { i }$ as a prompt to replace the words that appear in the dense caption, simulating the omission of visual cues. For example, “Rit” in the frame is re-written as “rip” (See Type V of Figure 2). Rather than using video directly, we observe that using these detailed video captions allows for better control in the construction process.

As a result, we obtain the output-side preference dataset $\widetilde { D } _ { p }$ by rewriting the transcripts systematically.

# 3.2 Supervised Fine-tuning

Pre-trained ASR Model. We use the audio-only ASR model OWSM $\mathrm { v } 3 . 1 ^ { 2 }$ as our backbone (Peng et al. 2024). OWSM v3.1 is an open-source pre-trained speech recognition model that achieves robust performance on standard ASR benchmarks (Panayotov et al. 2015; Bu et al. 2017; Pratap et al. 2020; Hernandez et al. 2018). It utilizes an encoder-decoder architecture (Vaswani et al. 2017), with the stack of E-Branchformer encoders (Kim et al. 2022) and Transformer decoders (Vaswani et al. 2017). Being trained on large amounts of ASR data, OWSM v3.1 has a good generalization ability for AV-ASR tasks.

Visual Encoder. We leverage a pre-trained visual encoder to extract visual tokens as conditions to the audio-only ASR model. Specifically, we use CLIP (Radford et al. 2021) to ensure that all relevant information within the video frames is effectively encoded for AV-ASR. We sample $M$ frames from video ${ \dot { v } } ^ { i }$ uniformly, then use the CLIP to extract visual tokens from these frames. A visual projection layer is subsequently applied to map visual tokens into speech space.

Fine-tuning Objective Given $M$ visual tokens and $N$ speech tokens, we concatenate them along the sequence dimension, resulting in a combined sequence of length $N { + } M$ . Then, we feed them into the ASR model and fine-tune the model $\pi _ { \mathrm { r e f } }$ in Equation 1 with attention loss ${ \mathcal { L } } _ { \mathrm { A T T } }$ and the CTC loss $\mathcal { L } _ { \mathrm { C T C } }$ :

$$
\begin{array} { r l } & { \mathcal { L } _ { \mathrm { S F T } } ( \pi _ { \mathrm { r e f } } , \mathcal { D } ) = \mathcal { L } _ { \mathrm { A T T } } ( \pi _ { \mathrm { r e f } } , \mathcal { D } ) + \alpha \cdot \mathcal { L } _ { \mathrm { C T C } } ( \pi _ { \mathrm { r e f } } , \mathcal { D } ) , } \\ & { \mathcal { L } _ { \mathrm { A T T } } ( \pi _ { \mathrm { r e f } } , \mathcal { D } ) = - \displaystyle \sum _ { u } \ln P _ { \mathrm { A T T } } ( t _ { u } ^ { \star } | a , v , t _ { 1 : u - 1 } ^ { \star } ) , } \\ & { \mathcal { L } _ { \mathrm { C T C } } ( \pi _ { \mathrm { r e f } } , \mathcal { D } ) = - \ln P _ { \mathrm { C T C } } ( t ^ { \star } | a , v ) , } \end{array}
$$

where $t _ { 1 : u - 1 } ^ { \star }$ is the preceding tokens of the ground truth character sequence $y ^ { \star }$ .

# 3.3 Bifocal Preference Optimization

To align the fine-tuned ASR model $\pi _ { \mathrm { r e f } }$ (Equation 3) with the AV-ASR task, we use input-side preference dataset $\widetilde { D } _ { c }$ and the output-side preference dataset $\widetilde { D } _ { p }$ (as described ien Section 3.1) to train the BPO-AVASR meodel $\pi _ { \boldsymbol { \theta } }$ through bifocal preference optimization.

Analogous to the Equation 2, the bifocal preference optimization objective for AV-ASR is formulated as

$$
\begin{array} { r l } & { \mathcal { L } _ { \mathrm { B P O } } ( \pi _ { \theta } ; \pi _ { \mathrm { r e f } } , \widetilde { D } ) = \mathcal { L } _ { \mathrm { i n p u t } } ( \pi _ { \theta } ; \pi _ { \mathrm { r e f } } , \widetilde { D } _ { c } ) + \mathcal { L } _ { \mathrm { o u p u t } } ( \pi _ { \theta } ; \pi _ { \mathrm { r e f } } , \widetilde { D } _ { p } ) , } \\ & { \mathcal { L } _ { \mathrm { i n p u t } } ( \pi _ { \theta } ; \pi _ { \mathrm { r e f } } , \widetilde { D } _ { c } ) = - \log \sigma \left( \beta \log \frac { \pi _ { \theta } ( t _ { w } | c _ { w } ) } { \pi _ { \mathrm { r e f } } ( t _ { w } | c _ { w } ) } - \beta \log \frac { \pi _ { \theta } ( t _ { w } | c _ { l } ) } { \pi _ { \mathrm { r e f } } ( t _ { w } | c _ { l } ) } \right) , } \\ & { \mathcal { L } _ { \mathrm { o u t p u t } } ( \pi _ { \theta } ; \pi _ { \mathrm { r e f } } , \widetilde { D } _ { p } ) = - \log \sigma \left( \beta \log \frac { \pi _ { \theta } ( t _ { w } | c _ { w } ) } { \pi _ { \mathrm { r e f } } ( t _ { w } | c _ { w } ) } - \beta \log \frac { \pi _ { \theta } ( t _ { l } | c _ { w } ) } { \pi _ { \mathrm { r e f } } ( t _ { l } | c _ { w } ) } \right) , } \end{array}
$$

where ${ \mathcal { L } } _ { \mathrm { i n p u t } }$ and ${ \mathcal L } _ { \mathrm { o u t p u t } }$ are preference optimization loss for input-side preference and output-side preference respectively. $( t _ { l } | c _ { w } )$ and $\left( t _ { w } | c _ { l } \right)$ serve as hard negatives to $( t _ { w } | \bar { c } _ { w } )$ , sampled from the preference datasets $\widetilde { D } _ { p }$ and $\widetilde { D } _ { c }$ respectively. Through this approach, we tr e the BPeO-AVASR model $\pi _ { \boldsymbol { \theta } }$ to avoid generating the above errors in the application, thereby improving the ability of the AV-ASR model to take good advantage of complex audiovisual information, especially for unconstrained real-world videos.

# 4 Experimental Settings Implementation Details

As described in Section 3.2, we use OWSM v3.1 as our speech recognition backbone and develop two versions of BPO-AVASR: BPO-AVASR small and BPO-AVASR medium respectively. Specifically, BPO-AVASR-small consists of 9 E-Branchformer (Kim et al. 2022) encoder blocks and 9 Transformer decoder blocks. The hidden size is 768. BPO-AVASR-medium comprises 18 E-Branchformer encoder blocks and 18 Transformer decoder blocks with a hidden size of 1024. Following AVFormer (Seo et al. 2023),

# When vision provides content cues:

福 GT: We use the cherry tomatoes. GT: That's how you tune a ukulele. GT: Salad and I really like this dole. w/o BPO: We use the chair to mirror. w/o BPO: That's how you tuna ucal aile. w/o BPO: Spell it and I really like this dull. BPO-AVASR: We use the cherry tomatoes. BPO-AVASR: That's how you tune a ukulele. BPO-AVASR: Salad and I really like this dole. When vision offers context clues: GT: how well it's chopping off. GT: We slowly ferried over in a small boat. GT: A seething cosmic maelstrom. w/o BPO: how well it’s chap enough. w/o BPO: We slowly varied  over in a small boat. w/o BPO: A seeding cosmic maelstrom. BPO-AVASR: how well it's chopping off. BPO-AVASR: We slowly ferried over in a small boat. BPO-AVASR:A seething cosmic maelstrom. When speech is under spontaneous scenarios : w/o BPGOT: VOonae fViovae. w/o BPGOT: BCeuczawuse tawke toaukregoaumr egasemreio\*u\*s\*.\*. w/o BPGOT: I started rdirnikniknigng \*b\*ef\*o\*r\*e\*I\*li\*k\*e\*. BPO-AVASR: One five. BPO-AVASR: Cuz we take our game serious. BPO-AVASR: I started drinking before I like.

Figure 3: Qualitative Results. We show the ground truth text (GT), and predictions from the OWSM-visual small(w/o DPO) and BPO-AVASR small. We show the enhanced performance in three scenarios: when vision provides content cues (top), when vision offers context clues (middle), and when speech is under spontaneous scenarios (bottom). Errors in the predicted words compared to the GT are highlighted in red. Faces are blurred for privacy.

we utilize CLIP-Large (Radford et al. 2021) as the visual encoder. For each video segment, we sample 4 frames uniformly (i.e. $M = 4$ as described in Section 3.2). During the SFT stage, we fine-tune OWSM v3.1 for 10 epochs with a batch size of 64. We set $\alpha$ to 0.3 in Equation 3. In the BPO fine-tuning stage, we set $\beta$ to 0.1 in Equation 4. We conduct the training using 1 V100 GPU, with a batch size of 512 and a learning rate of 2e-6. We use word error rate (WER) as the evaluation metric for all experiments, with lower values indicating better performance.

# 4.2 Datasets

In the supervised fine-tuning stage, we apply How2 as the fine-tuning dataset. In the bifocal preference optimization stage, we construct the preference dataset based on the How2. BPO-AVASR models are evaluated on three datasets, How2, VisSpeech, and Ego4D.

• How2 (Sanabria et al. 2018) is an instructional video dataset designed for multimodal understanding. Following AVFormer, we use the 300-hour version of How2. These videos are segmented into short clips, averaging 5.8 seconds each, with a user-uploaded transcript. • VisSpeech (Gabeur et al. 2022) is an AV-ASR benchmark sampled from the HowTo100M dataset (Miech et al. 2019). It contains 508 video clips with manually annotated transcripts. VisSpeech uses a video-text similarity model to ensure high audiovisual correspondence.

• Ego4D (Grauman et al. 2022) is an egocentric daily-life activity video dataset. We use the audiovisual diarization benchmark and evaluate our model on the validation set with ground truth annotations. Videos are segmented into shorter clips for analysis. Unlike other datasets, Ego4D contains noisier and more spontaneous videos across different domains, increasing the difficulty of AV-ASR.

# 4.3 Baselines

We compare BPO-AVASR models to strong baselines, including robust audio-only ASR models (see the upper section of Table 1) and state-of-the-art AV-ASR models (see the middle section of Table 1).

• Audio-only ASR Models. We compare BPO-AVASR series with (i) BEST-RQ, a robust ASR model pre-trained on LibriLight (Kahn et al. 2020), LibriSpeech (Panayotov et al. 2015) and HowTo100M (Miech et al. 2019) datasets $( \sim 1 9 0 \mathrm { K }$ hours data in total); (ii) OWSM-ft small, the OWSM v3.1 small model pre-trained on public speech datasets and fine-tuned on the How2 using audio-only data $( \sim 1 8 0 \mathrm { K }$ hours data in total).

• AV-ASR Models. We compare the BPO-AVASR series with the following AV-ASR baselines: How2 base (Sanabria et al. 2018) re-trains an ASR model by learning a video-specific bias from speech features. VAT (Caglayan et al. 2019) integrates different visual features into the ASR model with adaptive training. MultiRes (Paraskevopoulos et al. 2020) fuses video features with an additional cross-modal multi-head attention layer. LLD (Ghorbani et al. 2021) uses a deliberation model to leverage video and text representations extracted from a self-supervised text-video embedding model. AVATAR (Gabeur et al. 2022) is an encoderdecoder based AV-ASR model which fuses visual and speech information through a multimodal encoder. AVFormer (Seo et al. 2023) injects visual information into the frozen ASR model, BEST-RQ, with lightweight trainable adaptors, which shows good performance by training on 131k hours of audiovisual dataset. SynesLM (Lu et al. 2024) is a unified speech language model capable of performing audiovisual speech recognition. It explores decoder-only architecture within a multitask learning objective. OWSM-visual small is the OWSM v3.1 small model fine-tuned on the How2 dataset that incorporates visual information (the first row of Table 2).

Table 1: Comparison to baseline methods across different datasets. Results are reported as $\mathrm { W E R \% }$ . For the VisSpeech and Ego4D datasets, models are evaluated without any fine-tuning. † denotes these models are trained on a much larger additional audiovisual dataset, HowTo100M. “A” and “V” refer to audio and visual, respectively. Bolded values indicate the best results, while underlined values indicate the second-best results. The numbers in parentheses indicate the improvement in model performance (relative reduction in WER) compared to the state-of-the-art model considering three test sets, AVFormer†.   

<html><body><table><tr><td>Model</td><td>Modality</td><td>How2</td><td>VisSpeech</td><td>Eg04D</td></tr><tr><td>OWSM-ft small (Peng et al. 2023b)</td><td>A</td><td>10.8</td><td>16.6</td><td>70.6</td></tr><tr><td>BEST-RQ (Chiu et al. 2022)</td><td>A</td><td>15.3</td><td>16.7</td><td>68.3</td></tr><tr><td>How2 base (Sanabria et al. 2018)</td><td>A+V</td><td>18.0</td><td></td><td></td></tr><tr><td>VAT (Caglayan et al. 2019)</td><td>A+V</td><td>18.0</td><td>一</td><td></td></tr><tr><td>MultiRes (Paraskevopoulos et al.2020)</td><td>A+V</td><td>20.5</td><td>一</td><td></td></tr><tr><td>LLD (Ghorbani et al.2021)</td><td>A+V</td><td>16.7</td><td></td><td>1</td></tr><tr><td>AVATAR (Gabeur et al. 2022)</td><td>A+V</td><td>15.6</td><td>43.4</td><td></td></tr><tr><td>SynesLM (Lu et al. 2024)</td><td>A+V</td><td>15.7</td><td>39.4</td><td>一</td></tr><tr><td>AVATAR† (Gabeur et al. 2022)</td><td>A+V</td><td>9.1</td><td>35.7</td><td>92.0</td></tr><tr><td>AVFormer† (Seo et al. 2023)</td><td>A+V</td><td>13.6</td><td>16.4</td><td>64.6</td></tr><tr><td>BPO-AVASR small</td><td>A+V</td><td>9.3 (31.6%)</td><td>15.6 (4.9%)</td><td>59.2 (8.4%)</td></tr><tr><td>BPO-AVASRmedium</td><td>A+V</td><td>9.2 (32.4%)</td><td>14.5 (11.6%)</td><td>56.5 (12.5%)</td></tr></table></body></html>

# 5 Experimental Results

# 5.1 Comparison with SOTA Models

We compare BPO-AVASR with baseline models on three test sets in Table 1. The BPO-AVASR models outperform all baselines trained on the same audiovisual dataset (How2 base, VAT, MultiRes, LLD, SynesLM, and AVATAR), particularly on the noisy and spontaneous Ego4D dataset, highlighting the effectiveness of bifocal preference optimization in adapting ASR to unconstrained AV-ASR tasks. Furthermore, BPO-AVASR medium achieves better results than BPO-AVASR small, demonstrating the benefit of using larger models. While AVATAR† achieves the best performance on How2, it shows poor generalization ability on the out-of-domain dataset Ego4D. In contrast, the BPOAVASR series shows consistently better results across different domain datasets, demonstrating the robustness and generalization capability of BPO-AVASR. Compared with the previous state-of-the-art work AVFormer, the BPO-AVASR series shows significant improvement, especially on How2 $( 3 1 . 6 \%$ and $3 2 . 4 \%$ , respectively). Notably, both AVFormer† and BPO-AVASR are based on pre-trained ASR models with over 100K hours of training data. However, while AVFormer† is further fine-tuned on $^ { 1 3 1 \mathrm { k } }$ hours of audiovisual data from HowTo100M, BPO-AVASR achieves superior results using only 300 hours of audiovisual data.

# 5.2 Qualitative Analysis

Examples in Figure 3 illustrate the effectiveness of BPOAVASR. Qualitative results show that OWSM-visual (w/o BPO) still struggles to obtain accurate transcripts in realworld video scenarios. In contrast, qualitative examples demonstrate how BPO-AVASR improves ASR performance. As shown in Figure 3, BPO-AVASR is effective in three scenarios: when vision provides content cues, when vision offers context clues, and when speech is under spontaneous scenarios. For instance, visual content helps the model recognize objects directly in the video, such as brand names (row 1 column 3) and object names (row 1 column 1, row 1 column 2). Additionally, by using visual information as contextual clues, the model can correctly distinguish between similarly pronounced words (row 2). Furthermore, by constructing rejected samples considering spontaneous words, BPO-AVASR improves the accuracy of recognizing filler words in spontaneous speech scenarios (row 3).

# 5.3 Ablation Studies

Analysis of Rejected Samples. To evaluate the effects of different preference construction strategies, we create preference datasets with eight different strategies and then use them to optimize the BPO-AVASR small model. From the results in Table 2, we have the following observations:

Table 2: Comparison of preference data constructing strategies. Results are reported as $\mathrm { W E R \% }$ . N/A indicates that preference data and preference optimization are not used. The digits in parentheses indicate the improvement in model performance (relative reduction in WER) compared to the model without preference optimization, i.e., OWSM-visual small.   

<html><body><table><tr><td colspan="2">Rejected Data Construction Strategy</td><td>How2</td><td>VisSpeech</td><td>Eg04D</td></tr><tr><td>N/A (OWSM-visual small)</td><td></td><td>10.5</td><td>15.8</td><td>59.9</td></tr><tr><td rowspan="2">Input-side preference</td><td>Masked audio</td><td>9.4</td><td>15.7</td><td>59.2</td></tr><tr><td>Random cropped vision Flipped vision</td><td>12.1 9.7</td><td>15.7 15.5</td><td>61.5 59.3</td></tr><tr><td rowspan="3">Output-side preference</td><td>Rule-based replacement</td><td>10.4</td><td>16.0</td><td>62.4</td></tr><tr><td>Homophone-based generation</td><td>9.5</td><td>15.8</td><td>59.8</td></tr><tr><td>Spontaneous-based generation</td><td>10.8</td><td>16.1</td><td>59.0</td></tr><tr><td rowspan="2">Mixture (BPO-AVASR small)</td><td>Vision-based generation</td><td>9.7</td><td>15.6</td><td>59.3</td></tr><tr><td></td><td>9.3 (11.4%)</td><td>15.6 (1.3%)</td><td>59.2 (1.7%)</td></tr></table></body></html>

Table 3: Generalization to Ego4D dataset. All models are fine-tuned on the Ego4D training set. The digits in parentheses indicate the performance improvement (relative reduction in WER) compared to the OWSM-visual small.   

<html><body><table><tr><td>Method</td><td>WER%</td></tr><tr><td>AVATAR (Gabeur etal. 2022)</td><td>55.3</td></tr><tr><td>AVFormer (Seo et al. 2023) OWSM-visual small</td><td>55.2 52.3</td></tr><tr><td>BPO-AVASR small</td><td>50.0 (4.4%)</td></tr></table></body></html>

• Overall. The bifocal preference dataset constructed by mixing all strategies leads to the best and most balanced performance across all test sets. Moreover, when comparing BPO-AVASR small with OWSM-visual, the significant improvement (especially the $1 1 . 4 \%$ improvement on How2) highlights the effectiveness of our proposed bifocal preference optimization over SFT optimization. • Importance of Hard Negatives. Constructing hard negative samples is crucial for providing effective preference optimization signals. For input-side preference, frame flipping shows better results than random cropping. We attribute this to the fact that random cropping often fails to remove useful information due to the redundancy of visual data in unconstrained AV-ASR, making it difficult to create truly hard negatives. For output-side preference construction, we compare the rule-based construction strategy with the LLM-based strategy described in Section 3.1. Specifically, the rule-based strategy uses a homophone dictionary to replace content words in the ground truth transcripts. The LLM-based preference construction shows greater improvement, due to the LLM’s strong text understanding and generation capabilities, which are more effective in constructing hard negatives.

• Effectiveness of Different Strategies. Preference optimization strategies show different improvements on three test sets. Masking audio improves speech recognition performance across all three datasets. The Ego4D dataset benefits significantly from the strategy that addresses spontaneous speech (from 59.9 to 59.0).

Generalization to Other Datasets. To demonstrate the generalization ability of BPO-AVASR, we further construct a bifocal preference dataset using the Ego4D training set (Grauman et al. 2022), following the same strategies described in Section 3.1. We then fine-tune BPO-AVASR small on this constructed preference dataset. As shown in Table 3, BPO-AVASR outperforms all previous works, demonstrating the effectiveness of bifocal preference optimization on real-world videos, including instructional (How2) and egocentric (Ego4D) videos. Moreover, fine-tuning on Ego4D with preference optimization results in a significant improvement in WER compared to zero-shot testing on Ego4D (Table 1). This highlights that constructing preferences within the same domain further enhances the model’s performance through preference optimization.

# 6 Conclusion

In this paper, we first formulate the AV-ASR task as a preference optimization problem. Accordingly, we develop BPOAVASR, an AV-ASR system optimized by bifocal preference optimization to improve speech recognition accuracy for real-world videos. First, we introduce a simple yet effective method to create preference data by simulating common errors related to different modalities in AV-ASR. Second, we propose a bifocal preference optimization strategy to optimize AV-ASR models by emphasizing the distinction between correct and incorrect answers. BPO-AVASR outperforms previous state-of-the-art models, demonstrating the effectiveness of using preference optimization to align the audio-only ASR model to real-world video scenarios. For future work, we plan to build a high-quality open-domain AV-ASR dataset to facilitate future research.