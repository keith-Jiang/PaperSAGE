# S3C-MATH: Spontaneous Step-Level Self-Correction Makes Large Language Models Better Mathematical Reasoners

Yuchen $\mathbf { Y a n } ^ { 1 , 2 * }$ , Jin Jiang2,3, Yang Liu2, Yixin Cao4, Xin Xu2,5 Mengdi Zhang2, Xunliang $\mathbf { C a i } ^ { 2 }$ , Jian Shao1‚Ä†

1Zhejiang University 2Meituan Group 3Peking University 4Fudan University 5Hong Kong University of Science and Technology yanyuchen, jshao @zju.edu.cn

# Abstract

Self-correction is a novel method that can stimulate the potential reasoning abilities of large language models (LLMs). It involves detecting and correcting errors during the inference process when LLMs solve reasoning problems. However, recent works do not regard self-correction as a spontaneous and intrinsic capability of LLMs. Instead, such correction is achieved through post-hoc generation, external knowledge introduction, multi-model collaboration, and similar techniques. In this paper, we propose a series of mathematical LLMs called $S ^ { \widehat { 3 } } \mathrm { C }$ -MATH, which are able to perform Spontaneous Step-level Self-correction for Mathematical reasoning. This capability helps LLMs to recognize whether their ongoing inference tends to contain errors and simultaneously correct these errors to produce a more reliable response. We proposed a method, which employs a step-level sampling approach to construct step-wise self-correction data for achieving such ability. Additionally, we implement a training strategy that uses above constructed data to equip LLMs with spontaneous step-level self-correction capacities. Our data and methods have been demonstrated to be effective across various foundation LLMs, consistently showing significant progress in evaluations on GSM8K, MATH, and other mathematical benchmarks. To the best of our knowledge, we are the first to introduce the spontaneous step-level self-correction ability of LLMs in mathematical reasoning.

Q: Jane eats three eggs for breakfast every morning and  bakes muffins with four. How many eggs do she need per week? She eat 3+4=6 eggs üßë She eat 3+4=7 eggs per day. So she eats per day. So she eats 6\*7=42 eggs per week There is a 7w\*e7e=k49 eggs per mistake in   
ü§ñ your first step. ü§ñ (a) She eat üßëShüè´ e eat She eat 3+4=7 eggs per day. So she eats 3+4=6 eggs per day. üßë 3+4=7 eggs per day. üßë 7w\*e7e=k49 eggs per Wrong ü§ñ Correct ü§ñ (b) She eat 3+4=6 eggs per day. Oh, I found a mistake . She eats $3 + 4 = 7$ per day Áîµ So she eats $\scriptstyle \ast 7 = 4 9$ eggs per week. (c)

# Introduction

Reasoning is one of the essential foundational abilities of large language models (LLMs), showing the capacity of LLMs to tackle complex real-world problems. Nowadays, researchers are increasingly focusing on the performance of LLMs in specific reasoning tasks like mathematics, code, logic, common-sense, etc. (Sun et al. 2024). Mathematics is one of the significant branches of reasoning ability of LLMs, and solving a mathematical problem can demonstrate an LLM‚Äôs ability to decompose, reason, and summarize complex problems. In recent works, the chain-ofthought (CoT) inference (Wei et al. 2023) has been proven to be a method that can significantly enhance an LLM‚Äôs ability to solve reasoning problems (Suzgun et al. 2022; Wang and Zhou 2024). CoT induces the model to explicitly output the reasoning process, gradually deriving a series of intermediate processes or sub-goals, thereby enabling the model to correctly answer reasoning questions. However, during the step-by-step reasoning process, an LLM may still generate errors (Chen et al. 2024; Wang et al. 2024). These errors can be propagated to subsequent reasoning processes, leading to incorrect outputs from the LLMs.

To alleviate potential errors that may occur during model reasoning, self-correction methods have been proposed. These methods can check whether the model makes mistakes during the reasoning process, pinpoint the errors, and enable the model to generate a better answer based on the provided feedback (Kamoi et al. 2024; Pan et al. 2024). Existing research has already leveraged such error correction processes to make the model‚Äôs response more reliable(Puerto et al. 2024; Paul et al. 2024; Xu et al. 2024a). The crux of self-correction lies in how to generate feedback for the model‚Äôs output, and how to generate a better response based on this feedback (Tyen et al. 2024).

In terms of generating feedback, for some specific tasks, external tools or knowledge can be used as feedback. For instance, in the task of code generation, the code generated by the model can be handed over to the compiler. If the code cannot compile, the model can be informed that an error exists, and the error stack can be provided to the LLMs (Zheng et al. 2024; Li et al. 2024a). For tasks that are difficult to judge with external tools, some research provides the model‚Äôs original answer to more capable models such as GPT4 or a trained critic model specifically for error detection, thereby generating corresponding feedback (Zhang et al. 2024b). Such self-correction is usually applied at the instance-level, i.e., after the model‚Äôs complete output, feedback generation and error correction are completed. Given that most reasoning tasks are solved in a CoT manner, these self-correction methods have begun to be applied at the steplevel, providing the model with more granular signals. We illustrate these self-correction methods in panels (a) and (b) of Figure 1.

However, we believe that the aforementioned methods still seem unnatural in LLMs‚Äô inference. Utilizing multiple models to address a single problem does not provide a fair assessment of the LLM‚Äôs reasoning ability, and such an approach cannot be considered an end-to-end capability of the model. Therefore, some researchers have begun to propose implementing all steps of self-correction within the same model, using multi-task learning to enable a single model to master both problem-solving and error correction tasks (Madaan et al. 2023; Liu et al. 2024). Yet, we do not think this is enough. Multi-stage processing requires LLMs to generate several times, thus increasing inference time and computational costs. Moreover, this behavior is not spontaneous from the model itself, but a preset fixed process in multi-stage processing, which cannot demonstrate the model‚Äôs capacity for error recognition. Based on these considerations, we propose spontaneous step-level self-correction capability for LLMs, which allows the LLM to spontaneously identify errors in its on-going output and correct them immediately, ultimately yielding more reliable responses. This process is illustrated in panel (c) of Figure 1.

In this paper, we propose an ingenious method for constructing self-correction data to achieve spontaneous steplevel self-correction. This method utilizes existing stepby-step reasoning instruction data, generating potentially erroneous steps through sampled generation, and validating whether there are indeed errors in the steps through the $p a s s @ k$ validation. We insert the erroneous steps and the markers used to trigger self-correction into the correct steps in the existing data and construct $S ^ { 3 } C$ -MATHQA, which includes 532K self-correction data, based on Meta

MathQA (Yu et al. 2024). In order to make self-correction more accurate and generalize to more mathematical problems, we annotate each correction case in a fine-grained manner, adding the reflection and improvements. During the training stage, we mix $S ^ { 3 } C$ -MATHQA and the original 395K MetaMathQA to create 927K SFT data, and use loss-masks to ignore the loss of the erroneous steps. This method ensures that the LLMs are introduced with a new self-correction capability while maintaining their original effectiveness. Our experiments show stable and consistent improvements across multiple mathematical datasets such as GSM8K (Cobbe et al. 2021) and MATH (Hendrycks et al. 2021) on both generalist LLMs like Meta-Llama-3-8B (Meta AI 2024) and meth-specialized LLMs like DeepSeekMath (Shao et al. 2024). Our main contributions are as follows:

‚Ä¢ We introduce spontaneous step-level self-correction capability of LLMs, which can automatically correct potential errors and generate a more reliable response in the reasoning process. To the best of our knowledge, we are the first to propose this capability in mathematical reasoning of LLMs.   
‚Ä¢ We propose a method for constructing spontaneous steplevel self-correction data, which could be used to quickly and effectively build self-correction data from existing step-by-step SFT data. With the proposed method, we have constructed $S ^ { 3 } C$ -MATHQA, a 532K SFT dataset to produce the ability of spontaneous step-level selfcorrection.   
‚Ä¢ We employ a fine-tuning strategy incorporating lossmask on the $S ^ { 3 } C$ -MATHQA, resulting in the training of $\mathbf { S } ^ { \mathrm { 3 } } \mathbf { C } \mathbf { - } \mathbf { M } \mathbf { A } \mathbf { T } \mathbf { H }$ , a series of models with spontaneous steplevel self-correction capabilities in mathematical reasoning. This strategy enables the LLMs to identify potential errors during the output process and autonomously correct them. The trained $\mathrm { \bf S ^ { \bar { 3 } } C - M A T H }$ , with this capability, achieves performance improvements on multiple mathematical benchmarks.

# Related Works

# Mathematical Reasoning of LLMs

Mathematical reasoning, one of the foundational abilities of LLMs, demonstrates the model‚Äôs capability to solve complex real-world mathematical problems, which can be enhanced during various stages of LLMs‚Äô training. In the pretraining stage, continuing pre-training on large amounts of mathematical corpora can improve the internal mathematical reasoning ability of LLMs from a knowledge perspective (Ying et al. 2024; Shao et al. 2024; Zhou et al. 2024). During supervised instruction tuning, mathematical abilities could be further enhanced by training on problem-answer pairs, leading to the same answer formatting and better reasoners (Yu et al. 2024; Li et al. 2024b). For the preference alignment stage, preference optimization (Chen et al. 2024) and reinforcement learning (Lightman et al. 2023; Wang et al. 2024) are used to improve LLMs at a fine-grained level.

![](images/2d45cb0a02e68a24543ae0aaaa1922f06750a3866857164819c8fe2625335d5b.jpg)  
Figure 2: An overview of our works. The left part of this figure illustrates the process of constructing $s ^ { 3 } \mathrm { c }$ -MATHQA, and the right part depicts the training of $S ^ { 3 } C$ -MATH and the procedure during its reasoning.

# Self-Correction of LLMs

Self-correction is an advanced technique to enhance an LLM‚Äôs output by refining its initial response during the inference process, which can be implemented through various methodologies (Kamoi et al. 2024; Pan et al. 2024). Strategies of self-correction center around the concept of feedback generation. Essentially, the LLM must evaluate whether errors exist in its initial output, identify these errors, and pinpoint the steps needed for correction. The timing of selfcorrection can be categorized into two main types: posthoc and real-time (Paul et al. 2024; Jiang et al. 2023). In the post-hoc approach, feedback is generated after the initial inference, based on which the LLM is prompted to regenerate the response. In contrast, the real-time approaches integrate feedback into the generation context during the inference process, enabling the LLM to produce a completion with the necessary corrections on the fly. Additionally, feedback of the LLMs‚Äô contents could be generated in two types: external knowledge and tools to such as code compilers (Zheng et al. 2024; Li et al. 2024a) or LLMs themselves. Feedback generation by LLMs can be bifurcated into crossmodel and same-model approaches. In the cross-model approach, a more advanced LLM or a dedicatedly trained critic model is employed to produce the feedback (Li, Patel, and Du 2024; Cohen et al. 2023; Du et al. 2023). On the other hand, the same-model approach utilizes the same LLM for both the initial generation and the feedback, albeit with different prompts to guide the correction process (Thorne and Vlachos 2021; Tyen et al. 2024; Paul et al. 2024).

# Synthesised Mathematical Data

Synthesized data generated from existing LLMs has been proven effective for training another LLM (Yu et al. 2024; Liu and Yao 2024; Li et al. 2024b; Zhu et al. 2024; Xu et al. 2024b; Zhou et al. 2024), which can be used in both pretraining and supervised fine-tuning. For pre-training, data can be synthesized to expand the scope of mathematical knowledge that LLMs can learn from. For instance, continuing pre-training on the synthesized Jiuzhang dataset (Zhou et al. 2024) significantly improves LLMs‚Äô performance on multiple mathematical benchmarks. For supervised finetuning, there are also various methods to synthesize supervised instruction pairs. For example, MetaMathQA (Yu et al. 2024) augments questions from the original training set and generates answers for those bootstrapped questions. More recently, KPMath (Zhu et al. 2024) extracts key points from existing data to analyze and synthesize more complex mathematical word problems.

# Approach

The main works of this paper can be divided into two parts. The first part is to construct self-correction SFT data $\mathrm { \bar { S } ^ { 3 } C - }$ MATHQA based on existing step-by-step mathematical instruction pairs. The second is to use $\mathbf { \dot { S } } ^ { 3 } \mathbf { C }$ -MATHQA to carry out instruction fine-tuning, ultimately enabling the model to acquire spontaneous step-level self-correction capabilities.

# Data Construction

We divide our proposed self-correction data construction process into two steps. The first step involves generating erroneous steps from existing correct CoT steps, which will be used to create step-level self-correction instances. The second step concerns reflections and improvements for the above step-level self-correction instances, which serve as detailed guidance for LLMs to better acquire the ability to do spontaneous step-level self-correction. In this paper, we use the MetaMathQA (Yu et al. 2024) dataset, which includes 395K mathematical reasoning step-by-step samples, as the seed dataset.

Wrong Step Sampling Before we start, we carry out instruction tuning on the Meta-Llama-3-8B (Meta AI 2024) using the MetaMathQA data, resulting in our model $M$ used for sampling erroneous steps. Since we need to sample on the training set, in order to mitigate the reduction of diversity caused by model over-fitting, we adopt the idea of crossvalidation and conduct a 5-fold cross-training. That is, we evenly divide the MetaMathQA data into five parts, use four parts of the data to train the model, and the remaining one part of the data for sampling erroneous steps. This process is repeated five times on different parts of the data. Hereafter, we denote the data used for model training as $s$ , and the remaining data used for sampling erroneous steps as $\bar { s }$ . The model that undergoes instruction tuning on $s$ is denoted as $M _ { s }$ .

First, we use the trained model $M _ { s }$ to perform step-wise sampling on the split-out data $\bar { s }$ , obtaining a set of steps that may contain errors. The specific method is that for each step $x _ { i }$ in an SFT sample response $x$ , we concatenate all the steps from the first to the $\it { i } ^ { t h }$ step into a context and give it to $M _ { s }$ for continuation, until the model outputs the next step, obtaining candidate $\boldsymbol { \mathsf { \Sigma } } _ { 3 } ( \hat { x } _ { i + 1 } )$ . We use a high temperature and sample multiple steps at the same time to ensure the diversity of steps. The candidates of the $( i + 1 ) ^ { t h }$ steps of case $x$ could be represented as:

$$
c a n d i d a t e s ( \hat { x } _ { i + 1 } ) = M _ { s } ^ { k } ( x _ { 1 } \oplus . . . \oplus x _ { i } ) , x \in \bar { s }
$$

where $\oplus$ indicates concatenation of existing correct steps, and $M _ { s } ^ { k } ( x _ { 1 } \oplus . . . \oplus x _ { i } )$ represents the set of responses generated by $M _ { s }$ for $x _ { 1 } \oplus . . . \oplus x _ { i }$ k times.

Next, we will use the trained model to evaluate the correctness of the generated candidates $\left( \hat { x } _ { i + 1 } \right)$ . Specifically, for each step $\hat { x } _ { i + 1 }$ in the candidate set, we concatenate the correct preceding steps $x _ { 1 } , \ldots , x _ { i }$ as context before the step $\hat { x } _ { i + 1 }$ , and ask the model $M _ { s }$ to continue generating until the model actively ends the generation (i.e., encounters the [endof-sequence] token). We will determine whether the step is correct based on whether the answer generated by model $M _ { s }$ matches the standard answer. In order to make more accurate judgments and reduce the cases where correct steps are mistakenly considered as wrong, we use $p a s s @ k$ to evaluate a step. Specifically, we sample multiple model outputs with a high temperature of 1.0 for 16 times, and only when none of the contents generated by the model match the correct answer do we consider the step to be wrong. Whether $\hat { x } _ { i + 1 }$ is correct can be represented as:

$$
\begin{array} { r } { \hat { x } _ { i + 1 } \left\{ \begin{array} { l l } { \mathrm { C o r r e c t } } & { \mathrm { i f } \exists o \in M _ { s } ^ { k } ( x _ { 1 } \oplus \ldots \oplus x _ { i } \oplus \hat { x } _ { i + 1 } ) , A ( o ) = A ( g t ) } \\ { \mathrm { W r o n g } } & { \mathrm { i f } \forall o \in M _ { s } ^ { k } ( x _ { 1 } \oplus \ldots \oplus x _ { i } \oplus \hat { x } _ { i + 1 } ) , A ( o ) \neq A ( g t ) } \end{array} \right. } \end{array}
$$

where $A ( o )$ represents the answer extraction for the output $o$ , while $A ( g t )$ stands for the answer extraction for the golden truth.

Be#y is saving money for a new wallet which costs \$100. Be#y has only half of the money she needs. Her parents decided to give her \$15 for that purpose, and her grandparents twice as much as her parents. How much more money does Be#y need to buy the wallet?

Step 1: In the beginning, Be#y has only 100 / $2 = \$ 50$ .   
Step 2: Be#y's grandparents gave her $\$ 15$ .   
Exis\*ng   
Correct Steps   
Mistake

Reflec\*on Improvement

Step 3: Sorry, I made a mistake. Realiza'onüßê I was wrong because I misunderstood the amount given by my parents and grandparents. I mistakenly added the grandparents' contribuIon without considering the amount given by my parents. To correct my mistake, I should have started by adding the amount given by my parents to my iniIal savings, rather than jumping to the grandparents' contribuIon. By doing so, I will correctly calculate the total amount I have aLer receiving the addiIonal money from my parents. Here is the updated step. Her parents give her an   
addiIonal \$15, so she now has $\$ 50+515=565$ .   
Step 4: Her grandparents give her twice as much as her parents, so they give her $2 \ast \varsigma 1 5 = \varsigma 3 0$ .   
Step 5: With the addiIonal money from her grandparents, Be#y now has ${ \$ 65 + 530= 59 5 }$ .   
Step 6: Be#y sIll needs $\$ 100-595=55$ more to buy the wallet. The answer is: 5

![](images/28255ac62cea55324865f9b0e396c5e6042301663b31b91b311e64c8d17ee85b.jpg)

# Exis\*ng

Correct Steps

Figure 3: An example from $S ^ { 3 } C$ -MATHQA. Steps 1, 4, 5, and 6 are copied from the correct steps already exist in MetaMathQA. Step 2 is a wrong step sampled from the method we proposed. Step 3 is the indicator of LLM‚Äôs error realization, reflection, improvement, and the new corrected step. During training, we do not compute the loss for the red texts.

Reflection and Improvement Generation Based on the correct steps in the existing data and the erroneous steps we sampled in the previous section, we inserted the erroneous steps and the flags used to trigger self-correction into the correct steps, thus forming the direct self-correction data. However, another significant challenge of self-correction is how the LLMs can produce more accurate content when they know that errors already exist. To enable the LLMs to better learn to correct the identified mistakes, we annotated the acquired step-level self-correction samples in two ways, reflection and improvement, using Meta-Llama3-70B-Instruct (Meta AI 2024). Reflection involves the model analyzing where errors occurred in the existing answers, while improvement entails generating ways to improve based on the existing output and reflection. When generating these two annotations, we provide the model with a prompt, as well as a question, previous steps, wrong step, and correct step, and ask the model to make annotations based on this information. We present a data example and its components in Figure 3.

# Tuning and Evaluation

After constructing the aforementioned data, we implement SFT on it. We test our trained model on multiple mathematical evaluation benchmarks to validate the effectiveness of our data and methods.

Table 1: Our main experimental results $( \% )$ on four mathematical reasoning tasks (GSM8K, MATH, SVAMP and Mathematics) under $P a s s @ 1$ and Majority $@ 3 2$ settings. The term ‚Äùw/o. R&I‚Äù represents that the self-correction part of the SFT samples do not include reflection and improvements. We abbreviate $P a s s @ 1$ as $P @ 1$ and Majority $@ 3 2$ as $M @ 3 2$ . The highest accuracy is indicated in bold, while the second highest is underlined.   

<html><body><table><tr><td rowspan="2">Base Model</td><td rowspan="2">SFT Data</td><td colspan="2">GSM8K</td><td colspan="2">MATH</td><td colspan="2">SVAMP</td><td colspan="2">Mathematics</td></tr><tr><td>P@1 M@32</td><td></td><td>P@1 M@32</td><td></td><td></td><td>P@1 M@32</td><td></td><td>P@1 M@32</td></tr><tr><td colspan="7">Generalist Models</td><td></td><td></td><td></td></tr><tr><td>Meta-Llama-3-8B (Meta AI 2024)</td><td>MetaMath + Sc-MATHQA- w/o.R&I 81.65 + S¬≥C-MATHQA</td><td>81.12 82.94</td><td>86.20 88.10 87.34</td><td>30.58 32.32</td><td>39.92 41.00</td><td>81.40 80.50</td><td>86.60 84.60</td><td>18.27 17.56</td><td>28.21 25.48</td></tr><tr><td>Mistral-7B-v0.3 (Mistral AI team 2023)</td><td>MetaMath + SC-MATHQA- w/o. R&I 75.36 + S¬≥C-MATHQA</td><td>73.46 75.51</td><td>82.34 81.80 82.64</td><td>33.14 24.04 27.30 25.48</td><td>41.60 31.34 32.28</td><td>81.80 77.20 75.60</td><td>86.40 83.30 83.40</td><td>19.08 17.35 13.48</td><td>27.53 27.98 25.89</td></tr><tr><td>Meta-Llama-3-70B (Meta AI 2024)</td><td>MetaMath + SC-MATHQA- W/o. R&I + Sc-MATHQA</td><td>88.55 89.16 91.66</td><td>91.74 92.72 93.33</td><td>45.70 44.16 46.22</td><td>33.74 55.40 53.46</td><td>78.40 87.40 87.60</td><td>91.00 90.70</td><td>84.90 16.07 29.08 28.48</td><td>28.10 40.30 39.02</td></tr><tr><td colspan="8">55.80 87.70 Math-specialized Models</td><td>91.00 34.17 42.65</td></tr><tr><td>Deepseek-math-base (Shao et al. 2024)</td><td>MetaMath + SC-MATHQA- w/o.R&I 82.18 + SC-MATHQA</td><td>79.30 82.49</td><td>85.90 87.72 88.17</td><td>38.22 40.08 41.40</td><td>46.86 50.92 52.14 82.20</td><td>80.60 81.80</td><td>86.40 86.40</td><td>25.98 23.75</td><td>42.95 43.93 28.27 45.68</td></tr><tr><td>Qwen2-Math-7B (Qwen Team 2024)</td><td>MetaMath + SC-MATHQA-w/o.R&I 84.38 + S¬≥C-MATHQA</td><td>84.08 84.76</td><td>89.08 89.39 89.61</td><td>51.32 51.94 51.76</td><td>60.64 62.80 62.40</td><td>85.60 86.70 87.40</td><td>90.40 89.50 89.40</td><td>42.44 39.73 39.35</td><td>52.02 51.61 52.20</td></tr><tr><td>Qwen2-Math-72B (Qwen Team 2024)</td><td>MetaMath + SC-MATHQA- W/o. R&I</td><td>88.86 88.40</td><td>91.81 91.74</td><td>54.66 53.76</td><td>64.68 64.06</td><td>87.80 87.50</td><td>91.10 91.00</td><td>45.36 44.55</td><td>59.17 58.54</td></tr><tr><td></td><td>+ S¬≥C-MATHQA</td><td>88.93</td><td>93.10</td><td>55.12</td><td>66.22</td><td>89.30</td><td>91.50</td><td>46.28</td><td>58.93</td></tr></table></body></html>

Instruction Tuning We conduct SFT on the constructed dataset. Since our SFT data is self-correction data, it contains at least one erroneous step. Learning from these erroneous steps might harm the reasoning performance of the model itself. To avoid being affected by this situation, we use a loss mask to control the learning objectives of the model. Specifically, we applied a mask operation to the erroneous step, preventing the model from learning this error, thereby maintaining the performance of the existing SFT data. As shown in the red section of Figure 3, we will not calculate the loss for it.

Evaluation Settings We test the performance of our models on the evaluation sets of GSM8K (Cobbe et al. 2021), MATH (Hendrycks et al. 2021), SVAMP (Patel, Bhattamishra, and Goyal 2021), and Mathematics (Saxton et al. 2019). Each test sample in these datasets contains a question and a golden answer. We extract the final answer from the model‚Äôs output and matched it with the golden answer to ultimately determine whether the sample was answered correctly. The evaluation uses the vllm framework (Kwon et al. 2023) for inference, and we evaluate the model‚Äôs singleinference accuracy, i.e., the pass $@ 1$ metric, using greedy decoding. Self-consistency (Wang et al. 2023) is considered a stable way to evaluate the model‚Äôs sampling method. To more accurately assess the improvements brought about by our strategy, we also test the accuracy under the $m a j @ 3 2$ setting, setting the temperature to 0.7 during sampling decoding.

# Experiments

# Baseline

Since our data is built on MetaMathQA(Yu et al. 2024) dataset, we use the model fine-tuned on MetaMathQA as the baseline for our experiments. We compare the performance difference between our trained model and the baseline model on several mathematical benchmarks. All our SFT experiments use the same configuration. We use Megatron-LM as the training framework and set the initial learning rate to 1e-5, but except 1e-6 for Mistral-7B-v0.3(Mistral AI team 2023), with a 0.03 warm-up ratio, and decay to 0 using a cosine schedule. The model‚Äôs max length is 8192, the global batch size is 128, and the number of training epochs is 3. We pack all SFT data to accelerate the training process. We conducted our SFT experiments on 32 Nvidia A100 GPUs.

# Main Results

Based on MetaMathQA 395K, We applied our proposed method to synthesize $S ^ { 3 } C$ -MATHQA, 532K self-correction data. For all instances in $S ^ { 3 } C$ -MATHQA, we generated reflection and improvement using our proposed methods. The main motivation of our paper is to seamlessly introduce the end-to-end intrinsic self-correction capability into the model. Therefore, we merged the original MetaMathQA dataset with our synthesized data $S ^ { 3 } C$ -MATHQA, yielding a total of 927K data for our SFT data corpus.

To demonstrate the generalization effect of our data, we conducted SFT experiments on different foundation LLMs using the MetaMathQA and $S ^ { 3 } C$ -MATHQA. We adopted both generalist LLMs and math-specialized LLMs. For the generalist LLMs, we utilized three different size base models: Meta-Llama-3-8B(Meta AI 2024), Mistral-7Bv0.3(Mistral AI team 2023), and Meta-Llama-3-70B(Meta AI 2024). For the math-specialized models, we used the DeepSeek-Math(Shao et al. 2024), Qwen2-Math-7B (Qwen Team 2024) and Qwen2-Math-72B(Qwen Team 2024).

We evaluated the trained model on GSMK, MATH, SVAMP, and Mathematics, with the results shown in Table 1. Our experimental results demonstrate that our model can make stable improvements across multiple mathematical evaluation benchmarks. On Meta-Llama-3-8B, we improved the accuracy of GSM8K from $8 1 . 1 \%$ to $8 2 . 9 \%$ , and the accuracy of MATH from $3 0 . 6 \%$ to $3 3 . 1 \%$ . On Deepseek-MathBase, we increased the accuracy of GSM8K from $7 9 . 3 \%$ to $8 2 . 5 \%$ , and the accuracy of MATH from $3 8 . 2 \%$ to $4 1 . 4 \%$ . Our method achieved consistent improvements on both the generalist LLMs and the math-specialized LLMs, thereby validating the effectiveness of the method we proposed.

In our experimental results, we compared three methods: direct correction, correction after reflection, and correction after reflection and generation of improvement. The results showed that on multiple datasets, training the model with fine-grained analysis information can enhance the LLM‚Äôs performance in mathematical abilities.

# Analysis

# Ablation Study

Distribution Change vs. Self-Correction Introduction Since we introduced new data into the original MetaMathQA dataset, even though our data‚Äôs queries all come from MetaMathQA, there is still a possibility of changing the query distribution of the SFT data. Our error path sampling method may generate more self-correction data on more challenging problems, thus more difficult queries may appear more frequently in our SFT data. To ablate the impact of these situations on our results and to prove that our improvement comes from self-correction itself rather than the distribution changes caused by sampling, we conducted the following ablation experiment. The specific method of this experiment is that, based on the queries of the 532K selfcorrection data we generated, we over-sampled the original data from MetaMathQA, resulting in a total of 927K data. The distribution of queries in the over-sampled data is the same as that in our proposed 927K data.

As shown in Table 2, altering the distribution of the query does not enhance the model‚Äôs performance, further validating the effectiveness of our method.

Table 2: Data Distribution Change vs. Self-correction Introduction. ‚ÄùMetaMathQA dist-aligned‚Äù represents data that has the same distribution as the query in ‚ÄùMetaMathQA $^ +$ $S ^ { 3 } C$ -MATHQA ‚Äù, but uses the response from MetaMathQA.   

<html><body><table><tr><td rowspan="2">SFT Data</td><td colspan="2">GSM8K</td><td colspan="2">MATH</td></tr><tr><td>P@1</td><td>M@32</td><td>P@1</td><td>M@32</td></tr><tr><td>MetaMathQA</td><td>81.12</td><td>86.20</td><td>30.58</td><td>39.92</td></tr><tr><td>+ S¬≥C-MATHQA- w/o.R&I 81.65</td><td></td><td>88.10</td><td>32.32</td><td>41.00</td></tr><tr><td>MetaMathQA dist-aligned</td><td>78.54</td><td>85.22</td><td>30.92</td><td>38.62</td></tr></table></body></html>

Step-Level Correction vs. Instance-Level Correction The method we propose is a step-level self-correction, the advantage of which is that the model can directly correct the errors just occurred in the output process in a timely manner, thereby avoiding unnecessary error propagation. We can also generalize self-correction case-wise, that is, allowing the model to output all the content at once, and then let the model probe the generated content for errors. If errors are found, the model will automatically do self-correction, thereby producing a more accurate answer. To prove that the step-level self-correction in our work is more effective, we carried out this ablation experiment. Specifically, we saved the complete model output used to verify the error step at the error step sampling stage. According to our method, the answer paths sampled here cannot ultimately derive the correct answer. We take the entire error path as the context to generate instance-level correction data. Consistent with our proposed method, we also do not calculate loss on the erroneous reasoning path.

As shown in Table 4, experimental results indicate that while instance-level self-correction can yield some improvement, the extent of this enhancement is not as significant as that achieved with the step-level approach we employed.

Generate From Existing Paths vs. Generate New Paths One of the key features of our method is to directly construct self-correction data from the existing step-by-step mathematical data. Therefore, in our correction data, apart from the erroneous step and the corresponding correction behavior, the remaining correct steps are completely consistent with the original SFT data. This allows us to seamlessly introduce the intrinsic self-correction ability into the LLMs. Currently, in exploring the mathematical capabilities of the LLM, scholars use Monte Carlo Tree Search (MCTS) for step sampling (Wang et al. 2024; Zhang et al. 2024a), which allows for more diverse steps and decouples from the existing SFT data. To demonstrate that our direct sampling of erroneous steps from existing SFT data and construction of self-correction data is more effective, we designed this ablation experiment. We implemented an MCTS to generate correct and incorrect steps for all queries in MetaMathQA, and used these steps to construct self-correction data. For a fair comparison, we also included the original MetaMathQA data and constructed MCTS self-correction data of the same scale as the data volume as $S ^ { 3 } C$ -MATHQA.

As shown in Table 5, the experiments have demonstrated that the step-level self-correction data generated by MCTS cannot achieve the effectiveness of our method. The data produced by our method is more closely aligned with the original SFT data, enabling the introduction of spontaneous step-level self-correction capabilities within the model while preserving the original SFT data‚Äôs effectiveness.

<html><body><table><tr><td>Question</td><td>Ceciajustboutodgteeantodeupofgodefrhfe tofeedthepupupofgfodevedfrsofsifefogofogfoodotaislupswangsofogfoodici use in the first year?</td></tr><tr><td>w/o.self-correction</td><td>Step 1: In the first18Odays,Cecilia willfeed thepuppylcupof dog food everyday,soshe willuse180cups of dog food. Step2:Inerestoftheppslife,illdthpuppyupsofgfodeyayoeilluseus6s=upsood Step 3:In total,Cecilia willuse 180 cups + 730 cups =910 cups of dog food in the first year. Step 4: Sinceonebag ofdog food contains 110cups,Cecilia willneed 910cups/110cupsperbag=8.27bagsof dog food. Step 5:Since she cannot buy a fraction of a bag,she will need to round up to the nearest whole number. Step 6: Therefore,Cecilia will use 9 bags of dog food in the first year.The answer is: 9</td></tr><tr><td>w. self-correction</td><td>Step1:Inefrstscililttuppodeveildpap Step2:Intefeppy'sifeiltesoillds food. Step3:Sorrydeae.cosudattofysfoud6isbfaidd thatas heumbofstoallatettotalupofdgfodeddforhestoftpupslife.Howeteprobtatesattf life is not just 365 days. Toecttsbepupillfse by2cups togetotapsofdedfostftpup'sifeisilllsierepossbiltateale foralogerorshorterperodofti.Hereis tupdatedtepIterestfhepup'sife,silledtoedtepupy2cupsofogfoodevey day, so she will need 2 cups x (365 days -180 days)=370 cups of dog food.</td></tr></table></body></html>

Table 3: A case study from the GSM8K test dataset. Red text denotes the process of spontaneous step-level self-correction.   

<html><body><table><tr><td rowspan="2">SFTData</td><td colspan="2">GSM8K</td><td colspan="2">MATH</td></tr><tr><td>P@1</td><td>M@32</td><td>P@1</td><td>M@32</td></tr><tr><td>MetaMathQA</td><td>81.12</td><td>86.20</td><td>30.58</td><td>39.92</td></tr><tr><td>+ instance-level correction</td><td>79.98</td><td></td><td>87.26 32.22</td><td>40.60</td></tr><tr><td>+ S¬≥C-MATHQA-W/o.R&I</td><td>81.65</td><td>88.10</td><td>32.32</td><td>41.00</td></tr></table></body></html>

Table 4: Step-Level correction vs. Instance-level correction. ‚Äùinstance-level correction‚Äù utilized the same sample size for training as $S ^ { 3 } C$ -MATHQA.   
Table 5: Generate from existing paths vs. Generate new paths. ‚ÄùMCTS corrections‚Äù utilized the same sample size for training as $S ^ { 3 } C$ -MATHQA.   

<html><body><table><tr><td rowspan="2">SFTData</td><td colspan="2">GSM8K</td><td colspan="2">MATH</td></tr><tr><td>P@1</td><td>M@32</td><td>P@1</td><td>M@32</td></tr><tr><td>MetaMathQA</td><td>81.12</td><td>86.20</td><td>30.58</td><td>39.92</td></tr><tr><td>+ MCTS corrections</td><td>79.98</td><td>86.96</td><td>30.96</td><td>40.12</td></tr><tr><td>+ SC-MATHQA-w/o.R&I 81.65</td><td></td><td>88.10 32.32</td><td></td><td>41.00</td></tr></table></body></html>

# Case Study

To more clearly demonstrate the spontaneous step-level selfcorrection ability generated by our method in LLMs, we provide an evaluation output o25 f a model trained on the DeepSeek-Math base. We present the results of greedy decoding produced by training on MetaMathQA and $\mathrm { \dot { \mathbf { S } } ^ { 3 } C } .$ - MATHQA respectively in Table 3. We can see that after training on $S ^ { 3 } \mathrm { { \bar { C } } }$ -MATHQA, the model spontaneously realizes its mistake in the second step where it incorrectly assumed ‚Äòthe rest of its life‚Äò to represent 365 days, and proceeds to recalculate, arriving at the final result. This case can also show that our method does not affect the original data to the greatest extent, as the models trained on these two sets of data produce almost identical greedy outputs on the same case in the first two steps.

# Conclusion

In this paper, we introduce a novel capability for LLMs, the spontaneous step-level self-correction ability, which allows LLMs to recognize errors in their outputs in real-time and correct them simultaneously, thereby generating a more reliable answer. We propose a method for constructing training data for this capability based on wrong step sampling and build the $S ^ { 3 } C$ -MATHQA based on MetaMathQA. We applied SFT on both the generalist and math-specialized LLMs using the proposed $\mathrm { { \bar { s } ^ { 3 } c } }$ -MATHQA, achieving consistent and stable improvements on multiple mathematical benchmarks. Our work demonstrates that LLMs can possess a spontaneous step-level self-correction ability. There are still areas for improvement in our work, such as generating diverse steps when sampling error steps, examining the quality of reflection and improvement, etc. In the future, we will continue on this work to increase the accuracy during the self-correction process and extend this method to more reasoning tasks, truly generalizing this ability and building a better LLM reasoner.