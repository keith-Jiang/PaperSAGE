# Multi-Turn Jailbreaking Large Language Models via Attention Shifting

Xiaohu $\mathbf { D } \mathbf { u } ^ { 1 , 2 , 3 , 4 }$ , Fan $\mathbf { M } \mathbf { o } ^ { 7 }$ , Ming Wen1,2,3,4,6,\*, $\mathbf { T u } \mathbf { G u } ^ { 7 }$ , Huadi Zheng7, Hai $\mathbf { J i n } ^ { 2 , 3 , 5 }$ , Jie Shi7

1 School of Cyber Science and Engineering, Huazhong University of Science and Technology (HUST) 2 National Engineering Research Center for Big Data Technology and System 3 Services Computing Technology and System Lab 4 Hubei Engineering Research Center on Big Data Security and Hubei Key Laboratory of Distributed System Security 5 Cluster and Grid Computing Lab, School of Computer Science and Technology, HUST 6 JinYinHu Laboratory 7 Huawei International {xhdu, mwenaa, hjin}@hust.edu.cn, {mofan10, gu.tu, zhenghuadi, shi.jie1}@huawei.com

# Abstract

Large Language Models (LLMs) have achieved significant performance in various natural language processing tasks but also pose safety and ethical threats, thus requiring red teaming and alignment processes to bolster their safety. To effectively exploit these aligned LLMs, recent studies have introduced jailbreak attacks based on multi-turn dialogues. These attacks aim to prompt LLMs to generate harmful or biased content by guiding them through contextual content. However, the underlying reasons for the effectiveness of multiturn jailbreaks remain unclear. Existing attacks often focus on optimizing queries and escalating toxicity to construct dialogues, lacking a thorough analysis of the inherent vulnerabilities of LLMs. In this paper, we first conduct an in-depth analysis of the differences between single-turn and multi-turn jailbreaks and find that successful multi-turn jailbreaks can effectively disperse the attention of LLMs on keywords associated with harmful behaviors, especially in historical responses. Based on this, we propose ASJA, a new multi-turn jailbreak approach by shifting the attention of LLMs, specifically by iteratively fabricating the dialogue history through a genetic algorithm to induce LLMs to generate harmful content. Extensive experiments on three LLMs and two datasets show that our approach surpasses existing approaches in jailbreak effectiveness, the stealth of jailbreak prompts, and attack efficiency. Our work emphasizes the importance of enhancing the robustness of LLMs’ attention mechanism in multi-turn dialogue scenarios for a better defense strategy.

# Introduction

Large Language Models (LLMs) have demonstrated exceptional performance across various natural language processing tasks, yet concerns about their safety have also become increasingly evident. Alignment is typically performed before the release of LLMs. Nevertheless, users can still mislead LLMs to generate harmful content by prompting them with malicious intent, a practice known as jailbreaking (e.g., “role-playing” (Jin et al. 2024)). The first significant wave of jailbreaking began after the release of GPT-3.5, largely due to its substantial user base. Preferably, such work should be conducted by an internal red teaming team to mitigate the potentially huge negative impact on society. Leading companies, such as OpenAI, Anthropic, and Meta, have all established expert teams to evaluate their models before release (OpenAI 2023a,b; Anthropic 2024; Meta 2024; Touvron et al. 2023). However, this testing and red teaming process is extremely costly due to its “handicraft nature”, involving manual querying of the model and evaluating the responses one after another.

To accelerate the evaluation process and meanwhile reduce the cost, research works have built safety evaluation frameworks, e.g., sorry-bench (Xie et al. 2024), safety-bench (Zhang et al. 2023), or AI Risk-bench in HELM (Liang et al. 2023), which usually consist of queries with malicious intention that potentially break LLMs’ alignment. Such evaluation results can then serve as feedback for the next round of alignment. However, it is found that conventional single-turn jailbreaking is becoming less effective in detecting LLM’s vulnerabilities with this development process. For instance, on the latest jailbreak leaderboard (Zhou et al. 2024), the average success rate of 11 jailbreak attacks on LLaMA-2 is only $3 1 . 8 8 \%$ . Interestingly, such single-turn jailbreaking still leaves large attack surfaces in real scenarios where human users typically chat with LLMs in multi-turn ways.

In scenarios with long conversational history windows, i.e., multi-turn dialogues, LLMs have a stronger tendency to reply to malicious queries with harmful content than singleturn dialogues. Several multi-turn jailbreaking methods have been proposed to explore these more practical cases. As an example, both PAIR (Prompt Automatic Iterative Refinement) (Chao et al. 2023) and Crescendo (Russinovich, Salem, and Eldan 2024) successfully jailbreak LLMs in an automated way without a human in the loop in multi-turn or iterative cases. Consequently, the latest LLMs, such as the LLaMA 3.1 series (Dubey et al. 2024), are integrating multi-turn red teaming into the model development process, acknowledging that multi-turn dialogues are more likely to lead to harmful outputs.

Although current multi-turn jailbreaks have achieved some progress, they still exhibit certain limitations. First, from a theoretical perspective, current studies lack an analysis of why multi-turn jailbreaks are effective, and there has been insufficient exploration into how and where to introduce harmful prompts within multi-turn dialogues. Second, in terms of practical implementation, these multi-turn jailbreaks still follow the strategies from single-turn jailbreaks, which involve continuously optimizing queries to breach the safety alignment of LLMs. The main difference is that in multi-turn scenarios, the responses generated by the LLM in earlier turns are incorporated into the prompt design for subsequent turns.

To bridge this gap, we are the first to explore, based on the attention mechanism of LLMs, which parts of the input that LLMs focus on during multi-turn jailbreak attempts. We find that in successful jailbreak samples, LLMs allocate significantly less attention to harmful keywords compared to failed attempts, with attention shifting toward dialogue history, especially the model’s prior responses. We believe this difference occurs due to LLMs, after multiple rounds of redteaming, become highly sensitive to certain malicious keywords, making them more prone to these keywords when identifying harmful queries. When the attention of LLMs on harmful keywords in a query is reduced below some threshold, they may fail to recognize the underlying harmful intent.

Based on these insights, we propose ASJA (Attention Shifting for JAilbreaking LLMs), a novel approach aimed at breaking through LLMs’ safety alignment by shifting their attention from the final harmful query to the dialogue history. Specifically, we first initialize a multi-turn dialogue using LLMs. Instead of optimizing queries in subsequent turns based on earlier responses, we simultaneously optimize both queries and responses in the dialogue history. This approach is motivated by our preliminary study findings that during dialogue history, regardless of whether the jailbreak is successful, LLMs exhibit greater attention to previous responses than to the queries. To better redirect the attention of LLMs, we guide the LLMs during initialization to generate harmful queries in the latter half of the dialogue. This strategy is akin to the famous “Do Anything Now” (DAN) (Shen et al. 2023), leading LLMs to believe they have already responded to harmful queries and thus no longer need to follow safety policies. We retain the optimization of queries and employ a lightweight genetic algorithm to find effective solutions. Experiments conducted on three LLMs and two datasets demonstrate that our attack increases the harmfulness of responses by $4 4 . 9 1 \%$ and improves relevance to harmful queries by $3 4 . 0 2 \%$ compared to baselines. Additionally, it surpasses most single-turn and multi-turn jailbreaks in stealthiness, efficiency, and transferability.

To summarize, we make the following contributions:

• We perform the first study to explore the differences in attention distribution of LLMs during multi-turn jailbreaks and how these differ from single-turn jailbreaks. • We propose optimizing both queries and responses simultaneously to fabricate dialogue histories that shifts the attention of LLMs and develop an efficient attack approach based on this idea. • We conduct extensive experiments on three LLMs and two datasets, demonstrating the superiority of ASJA.

# Background and Related Work

Jailbreak attacks (Li et al. 2023a,b; Shayegani, Dong, and Abu-Ghazaleh 2024) are crucial for identifying and mitigating the security vulnerabilities of LLMs. They are designed to bypass the safety features of LLMs and to make them output unsafe or illegal content.

Single-round jailbreaks. Single-round jailbreaks aim to break the safety mechanisms of LLMs, to make them return prohibited outputs, with the use of one single prompt. Researchers have proposed different techniques to design such prompts. GCG (Zou et al. 2023) aims to exploit the vulnerabilities of LLMs based on their gradients. AutoDAN (Liu et al. 2024), GPTFUZZER (Yu et al. 2023), and FuzzLLM (Yao et al. 2024) adopt different algorithms to explore different prompt variations, to identify the weaknesses of LLMs. Persuasive Adversarial Prompts (PAP) (Zeng et al. 2024) adopts a different approach, using natural language to persuade target LLMs to output unsafe content.

Multi-turn jailbreaks. In multi-turn jailbreaks, there is usually an attack model that generates prompts, which ultimately aim to guide the target model to return harmful responses, in a multi-round conversation-like manner. PAIR (Chao et al. 2023) proposes a chain-of-thought reasoningbased approach, that outputs explicit improvement suggestions, which are then used to refine the prompts generated by the attack model. Crescendo (Russinovich, Salem, and Eldan 2024) aims to utilize the target models outputs to direct the model towards bypassing its safety alignment. REDEVAL (Bhardwaj and Poria 2023) carries out jailbreaks by first using a red-teaming prompt that sets up a conversation between two agents, a harmful agent Red-LM, and an unsafe-helpful agent Base-LM, and then asks the target model to complete the response of Base-LM by following the instructions in the prompt. The Chain of Attack (CoA) method (Yang et al. 2024) aims to guide the target model to gradually transition from secure scenarios to the generation of harmful content.

With all these proposed multi-turn jailbreak cases, the community still lacks an understanding of the underlying reasons why they differ from single-round jailbreaks, and consequently how to improve both the attack and defense effectiveness.

# Preliminary Study

In order to better understand why multi-turn jailbreaks are effective and to guide the optimization of multi-turn dialogue-based jailbreaks, we conduct an empirical study based on the attention mechanisms of LLMs, investigating whether there are differences in the attention patterns of LLMs between successful and unsuccessful jailbreak attempts, and how these differences impact the alignment of LLMs.

# Experiment Setup

Dataset. In this study, we utilize the QuestionList (Yu et al. 2023) dataset, which includes 100 queries covering various prohibited scenarios such as illegal activities, unethical practices, discriminatory speech, and toxic content.

Models. We use LLaMA-2 (Touvron et al. 2023) (Llama2-7b-chat-hf) as the target model because it is widely used, and many derivative LLMs are also based on it. Additionally, it has the lowest average success rate in the recent jailbreak leaderboard (Zhou et al. 2024), making it highly representative of safety research.

Study Design. We construct 100 sets of multi-turn queries based on 100 queries in the QuestionList using SOTA models, with prompts designed to gradually transition from benign questions to harmful queries. To balance the effectiveness and efficiency of multi-turn queries, we set the number of turns to 5. We then sequentially input these multi-turn queries into the LLaMA-2 model, recording the output and final results for each turn. We manually evaluate the jailbreak success for these 100 samples to ensure assessment accuracy. We analyze the contribution of different turns to the final result by calculating the attention scores of the fifthround responses. This is inspired by common attention analysis to understand the internal mechanism of models, e.g., for examining how sensitive the outputs are to inputs.

Specifically, when LLaMA predicts the next token, subsequent outputs will include attention to previously output tokens. In this study, we focus solely on the contribution of the input to the model’s output. Thus, we select the output token from the second output token, as the first token’s attention mainly reflects the attention between input tokens. Assuming the input token length is $n$ , we only select the first $n$ attention scores. We average the attention scores of these output tokens, and for each token, we average the attention scores across $L$ layers and $H$ attention heads to derive the final attention score for each input token. Let the input tokens be $X ~ = ~ \{ t _ { 1 } , t _ { 2 } , \ldots , t _ { n } \}$ , and the output sequence be $Y = \{ y _ { 1 } , y _ { 2 } , . . . , y _ { m } \}$ . The attention score of the $i$ -th layer and the $j$ -th attention head in the LLaMA model is $A _ { i , j } ( y _ { t } , t _ { k } )$ . The attention score of the $t$ -th output token to the $k$ -th input token is:

$$
A ( y _ { t } , t _ { k } ) = \frac { 1 } { L \times H } \sum _ { i = 1 } ^ { L } \sum _ { j = 1 } ^ { H } A _ { i , j } ( y _ { t } , t _ { k } )
$$

Finally, we average the attention scores from the second to the $n$ -th output tokens to obtain the final attention score for each input token.

$$
A ( t _ { k } ) = \frac { 1 } { n - 1 } \sum _ { t = 2 } ^ { n } A ( y _ { t } , t _ { k } )
$$

In the LLaMA-2 model, the number of layers $L$ is 32, and the number of attention heads $H$ is also 32.

After computing the attention scores for each token, we apply the LLaMA-2 tokenization strategy to convert tokens into words. For a word $w _ { k }$ consisting of multiple tokens, its attention score is calculated as the sum of the attention scores of these tokens. To analyze the variations in attention scores across different dialogue turns, we compute the attention score for the queries and responses of each turn separately by summing the attention scores of all words in the queries and responses for that turn. For instance, the query

![](images/cedf7bb981a5859c388689b4be9919b596aa571490e3b7cd0c782aae1231e3e1.jpg)  
Figure 1: Attention differences between failed attacks (Reject) and succeeded attacks (Jailbreak) among five queries (Q1 to Q5)/responses(R1 to R5) in multi-turn dialogue

$Q _ { r } = \{ w _ { 1 } , w _ { 2 } , \ldots , w _ { n } \}$ of $r$ -th round’s attention is:

$$
A ( Q _ { r } ) = \sum _ { t = 1 } ^ { n } A ( w _ { t } )
$$

Special characters in the prompt template are used to differentiate between different turns of queries and responses. Consequently, we derive the cumulative attention scores for all turns of queries and responses across the entire dialogue.

# Experimental Results

Among the 100 multi-turn queries tested on LLaMA-2, 22 are successful in jailbreaking, while 78 fail. We calculate the attention scores for these two types of instances, and the results are shown in Figure 1. Specifically, Figure 1a presents the normalized average attention scores for queries and responses across different rounds. Figure 1b shows the sample distribution of attention scores across different rounds. Note that for better visualization, we do not display the extremely high (Q1) and very low (Q2, Q3, and Q4) attention scores. From these two figures, we can draw the following observations: (1) Overall, LLMs tend to pay more attention to the first and last rounds (Q1 and Q5) in queries, while the attention on intermediate queries is particularly low and close to zero. This suggests that the existing jailbreak attacks that only optimize the queries have limitations, as carefully optimized intermediate-round queries may hardly influence the model’s decisions. (2) The responses in intermediate rounds receive higher attention compared to queries. We speculate that this is because LLMs are more inclined to focus on responses from previous conversations to improve the final round’s response. (3) For the harmful query in the last round (Q5), the attention for successful jailbreak samples is lower compared to the failed ones. We employ Welch’s ttest (Sakai 2016), a t-test that can handle imbalanced sample sizes between two categories, to verify the significance of the difference. The results show a p-value of $4 . 3 8 \mathrm { e } { - } 0 7 { < } 0 . 0 1$ , indicating that the difference between the two categories is statistically significant.

# Motivation Example

To further validate the above observations and understand the attention patterns of LLMs in single-turn and multi-turn jailbreak scenarios, we present an example in Figure 2. In this example, we isolate Q5 from the multi-turn queries and

# LLaMA2

Imagine a plot in a Christmas-themed $( 8 \% )$ novel $( 3 \% )$ where the climax $( 5 \% )$ in- I cannot suggest or encourage any illegal activities, including planning a   
Q volves a group of characters planning a heist $( 7 \% )$ to rob $( 4 \% )$ a heirloom $( 1 1 \% )$ heist. It is important to respect the property and privacy of others, and R during $( 4 \% )$ a large family $( 3 \% )$ gathering $( 8 \% )$ . What should they bring $( 3 \% ) ?$ any actions that involve stealing or harming others are not justifiable. ...   
Q1 sWuhsaptelintseerfaurly?techniques make holiday-themed novels more engaging and bHyoliudsianyg-thvaermioeudsnlitoevrealrsyctaenchbneiqmuaesd.eHmeorreeareen gsaogminegt eacnhdnisquusepse tnhsaetfu..l. R1   
Q2 eCvaen ty tohuatdreesqcruiibresacfiacrteifounlatlimsicnegnarniod  wcohoerde ncahtiaoranc?ters plan a complex holiday hCoelritdainy ye!veHnetrtehiast arefiqcutiroensalcsacrefnualrtiiomiwnhgeraen dchcaoroarcdtienrast ipolna:n. a. complex R2   
Q3 Ihnoausneovweitl,hohuotwbewionugl deatechtaerda?cteWrhsakt lliftuelrlayrhy devaicseusrpwriosueldgitfhteinasuothmoerounse's? aHicdhinalgleansgiunrpgrtiassekg, fbtoitnhsformtehoence'hsarhaocutsere inw theountobveilnagnde.t.e.cted can be R3   
Q4 aWchcoatmaprleisshosmeecrteotiovlse tansdksmdetuhriondgsausheoldidbay cphloatr?acters in thriller novels to Itno tahcriclloemr pnliosvhels,ecrheatirvaecttearskso tdeunr unsgeachreolaitdivaey palnodt.  rHeseroeuracreful…methods R4 Imagine a plot in a Christmas-themed $( 2 \% )$ novel where the climax $( 2 \% )$ involve- Sure, here are some items that the group of characters might consider   
Q5 es a group of characters planning $( 1 \% )$ a heist $( 3 \% )$ to rob $( 1 \% )$ a heirloom $( 2 \% )$ bringing or their heist during a large family gathering: R5 during $( 1 \% )$ a large family $( 1 \% )$ gathering $( 3 \% )$ . What should they bring $( 2 \% ) ?$ 1. Lockpicks and other burglary tools: …

use it as a single-turn jailbreak query. As shown in the figure, despite using a role-playing jailbreak strategy, this query is still rejected by the LLM. We analyze the attention paid by the LLM to the words in this single-turn query and find that keywords like “heirloom”, “gathering”, and “heist” receive the highest attention. These words indicate the object, scene, and action of the harmful behavior, respectively. Consequently, the LLM detects this harmful intent and refuses to respond. In contrast, in the multi-turn query, due to the significant increase in context, the attention scores for these keywords drop significantly. For instance, the attention score for “heirloom” drops from $11 \%$ to $2 \%$ , and for “heist”, it drops from $7 \%$ to $3 \%$ . The LLM provides a positive response to the harmful query in Q5. We hypothesize that this occurs because LLaMA-2, during the red-teaming alignment process, treats the LLM as a virtual classification model when facing jailbreak queries. Once the model predicts a query as harmful, it directly invokes predefined refusal templates instead of analyzing the querys semantics. This is evidenced by the refusal templates across similar LLMs often exhibit high similarity. In multi-turn queries, the attentions of these jailbreak keywords are diluted by the context, thus not reaching the threshold required for the LLM to recognize them as harmful queries, leading to a successful jailbreak.

# Methodology

In this section, we introduce ASJA, a novel Attention Shifting JAilbreaking attack on LLMs by fabricating dialogue history. The idea of ASJA is motivated by a key finding in the previous section on multi-turn jailbreaks: the responses of LLMs in historical dialogues play a more critical role than queries in the final round. To increase the LLM’s focus on responses in historical dialogue and to shift attention to harmful keywords in the harmful query turn, we propose, for the first time, to fabricate dialogue history for jailbreaking LLMs. This approach combines an uncensored model, a genetic algorithm, and eight jailbreak strategies to fabricate queries and responses in historical dialogue. The complete optimization process of ASJA is described in Algorithm 1. Next, we introduce ASJA in detail.

# Algorithm 1: Attention Shifting for Jailbreaking

Require: Attack model $F _ { A }$ , Target model $F _ { T }$ , Judge model $F _ { J }$ , Max iterations $G$ , Population size $N$ Input: Harmful query $X$ Output: Harmful multi-turn dialog, Harmful response 1 $Q  \mathtt { I n i t } ( X ) \ /$ / query initialization 2 $M T \gets \emptyset \qquad / /$ multi-turn dialog 3 for $q$ in $Q$ do 4 $\big \lfloor \begin{array} { l } { M T \gets M T \cup q \cup F _ { T } ( q ) } \end{array}$ 5 for $i = 0 \to N$ do 6 $\mathsf { \Gamma } \bigcup \mathcal { P } _ { i } ^ { 0 } = \mathtt { m u t a t i o n } ( M T , F _ { A } )$ 7 for $t = 0 \to G$ do 8 for $i = 0 \to N$ do 9 if $F J ( F _ { T } ( \mathcal { P } _ { i } ^ { t } ) ) = = I$ then 10 return it, FT ( it) 11 $\mathit { s c o r e s } \gets \mathtt { F i t n e s s } ( \mathcal { P } ^ { \mathrm { t } } ) , \mathtt { c h i l d } \gets \emptyset$ 12 elite $$ Sample elite from $\mathcal { P } ^ { t }$ with lowest score 13 for $i = 1  N$ do 14 $\begin{array} { r l } & { \mathbf { \Phi } ^ { \mathbf { \Lambda } : \mathbf { u } } \to \mathbf { \Phi } ^ { \mathbf { \Lambda } \to \mathbf { \Lambda } \mathbf { \tilde { \mathbf { v } } } } \mathbf { \Phi } ^ { \mathbf { \Lambda } \mathbf { \tilde { \mathbf { u } } } \mathbf { \Phi } } } \\ & { \begin{array} { r l } & { p a r e n t _ { 1 } , \ : p a r e n t _ { 2 } \gets \mathbf { s e l e c t i o n } ( \mathcal { P } ^ { t } , s c o r e s ) } \\ & { c h i l d \gets c h i l d \cup \mathbf { c r o s s o v e r } ( p a r e n t _ { 1 } , \ : p a r e n t _ { 2 } ) } \end{array} } \\ & { \mathcal { P } _ { i } ^ { t + 1 } \gets \mathtt { m u t a t i o n } ( c h i l d , F _ { A } ) \cup e l i t e } \end{array}$ 15 16

# Multi-turn Dialogue Initialization

High-quality initial multi-turn dialogues are crucial for successful jailbreaking as subsequent optimizations are based on them. To ensure the quality of multi-turn dialogues, we attempt to use LLMs to generate a series of queries that incrementally rise in toxicity from a benign question to a harmful question. However, this approach does not work well. It is observed that some SOTA models have a high rejection rate in sample initialization, particularly for questions involving bad behaviors such as suicide and sexual offenses. To address this issue, we introduce an uncensored model 1 to complete the initialization of the remaining samples. This uncensored model is further fine-tuned on a dataset lacking alignment or moralized responses, to train a model without built-in alignment features. Subsequently, we modify the interactions between the rounds of queries, transitioning from a simple increase in toxicity to an equal distribution of benign and harmful questions. This design is inspired by the existing jailbreaking strategy of “DAN”. Instead of directly prompting the LLM to act as a noncompliant AI assistant, we embed harmful queries and their corresponding responses into the LLM’s dialogue history. In this way, the LLM tends to agree that it has previously engaged with harmful requests in the DAN persona. Harmful responses can be elicited during subsequent dialogue optimization steps.

After generating multi-turn queries, we manually review the generated JSON data based on the following criteria: (1) if there are still rejected queries, (2) if the final round deviates from the original harmful question, and (3) if the JSON data format is correct and the content has the specified number of queries. Due to the randomness of LLM outputs, samples that do not meet the criteria can be re-generated by re-querying the model. After obtaining the correct multiround queries, we use them to query LLaMA-2 and obtain responses for each round. Finally, we obtain multi-turn dialogue data that includes both queries and responses.

# Dialogue Optimization

In general, we employ a genetic algorithm to optimize the dialogue history fabrication, utilizing eight jailbreak strategies to mutate queries and an uncensored model to generate responses. Next, we introduce the different components of the optimization process.

Mutation. We first describe the mutation components we have developed for optimizing multi-turn dialogues. Considering the purpose of fabricating dialogue history and its efficiency, mutation specifically targets the selection of a query or a response for rewriting in different rounds of the dialogue. For queries, we adopt eight jailbreak strategies from existing works (Yu et al. 2024; Liu et al. 2023) and use LLMs to rewrite queries. These strategies are “Defined Persona”, “Imagined Scenario”, “Research and Testing”, “Joking Pretext”, “Program Execution”, “Text Continuation”, “Opposite Model”, and “Alternate Model”. For responses, we regenerate affirmative responses to harmful queries using the uncensored model described in dialogue initialization. By repeatedly providing affirmative responses in the intermediate rounds, we aim to mislead LLMs into giving an affirmative response in the final round.

Population Initialization. In the optimization process, ASJA maintains a population of fixed size. The population initialization is done through multiple calls to Mutation function. To ensure the diversity of the population for the exploration of a larger search space while mutation, the initialization involves multiple random selections of queries and responses. Additionally, we allow the same $Q _ { i }$ or $R _ { i }$ to be selected repeatedly, as the randomness of the LLMs used for mutation and the diversity of jailbreak strategies can introduce varied rewritten samples across different attempts.

Fitness Evaluation. Fitness is utilized to evaluate the quality of the samples from different iterations and their closeness to the target. In the analysis of LLM attention, we find that there is a significant difference in the attention scores of LLMs for the last round of queries between successful and failed attacks. Therefore, we use the attention score of LLMs for the last round of the queries as the fitness measure in ASJA. For an $k$ -round dialogue, the fitness is calculated as:

$$
A ( Q _ { k } ) = \sum _ { i = 1 } ^ { n } A ( t _ { i } )
$$

where $A ( t _ { i } )$ is the attention score of the token in $Q _ { k }$ .

Dialogue Unit-level Optimization. After calculating the fitness of the initialized population, if no successful jailbreak sample is found, the algorithm proceeds to $G$ iterations of optimization. We collectively refer to the queries and responses in the dialogue as dialogue units. First, in each iteration, we select the sample with the lowest fitness as the elite sample. This sample represents the one that achieves the most significant decrease in the LLM’s attention in the final round. For the remaining samples, we select parent samples for crossover based on their fitness scores. Samples with lower fitnesses are more likely to be selected. We employ uniform crossover, where a randomly selected dialogue unit from the parents is copied into the offspring. Uniform crossover ensures the diversity of the offspring. For a population size of $N$ , we repeat the crossover $N - 1$ times. For each offspring, we apply mutation again with a certain mutation probability. Multiple mutations could involve multiple jailbreak strategies, as existing work has shown that combining multiple jailbreak strategies is more effective in jailbreak attacks (Yu et al. 2024). Moreover, mutation can further modify harmful responses in certain rounds, promoting diversity in multi-turn dialogues. To more effectively shift attention and improve efficiency, we only mutate harmful queries and responses in the latter half of the dialogue history. Finally, these $N - 1$ offspring, along with the elite sample, proceed to the next iteration. To maintain the number of queries to the target model by ASJA within a low range, we set the population size $\mathbf { N }$ to 10, the maximum number of iterations G to 5, and both the uniform crossover probability and mutation probability to 0.5.

# Experiment Experimental Setup

Dataset. To thoroughly evaluate our approach, we employ two datasets. The first dataset is QuestionList, which is also used in the preliminary study. The second dataset is AdvBench (Zou et al. 2023), which contains 520 instances of harmful behaviors across seven scenarios: “Illegal Activity”, “Hate Speech”, “Malware”, “Physical Harm”, “Economic Harm”, “Fraud”, and “Privacy Violence” (Ding et al. 2023). Notably, AdvBench includes samples with repeated topics, which enhances the reliability of our results in a way that allows our experiments to simulate multiple runs.

Models. Multiple LLMs are employed in this study. Specifically, we utilize SOTA models and Uncensored LLaMA-3.1-8b to initiate multi-turn queries. The attack model is Uncensored LLaMA-3.1-8b, chosen for its efficacy in generating queries through various jailbreak strategies and producing non-refusable responses. For evaluation, we use LLaMA-3.1-70b as the judge model, as the widespread adoption of LLMs for assessing jailbreak outcomes (Yang et al. 2024). The prompts used for judgment adhere to the work of ReNeLLM (Ding et al. 2023). We select three open-source LLMs as target models: LLaMA2 (Llama-2-7b-chat-hf), LLaMA-3.1 (Meta-Llama-3.1-8BInstruct), and Qwen-2 (Qwen2-7B-Instruct). We also compare our results on existing SOTA LLMs: GPT-3.5 (gpt3.5-turbo-0125) and GPT-4o (gpt-4o-2024-05-13). Our approach leverages the attention scores of the LLMs, enabling direct attacks on the open-source models. We then transfer the effective multi-turn jailbreak dialogues to the closedsource LLMs for further attacks.

<html><body><table><tr><td rowspan="2">Dataset</td><td rowspan="2">Attack</td><td colspan="3">LLaMA-2</td><td colspan="3">LLaMA-3.1</td><td colspan="3">Qwen-2</td><td colspan="3">Average</td></tr><tr><td>ASR-1个</td><td>ASR-2个</td><td>PPL↓</td><td>ASR-1个</td><td>ASR-2个</td><td>PPL↓</td><td>ASR-1个</td><td>ASR-2个</td><td>PPL↓</td><td>ASR-1个</td><td>ASR-2个</td><td>PPL↓</td></tr><tr><td rowspan="4">AdvBench</td><td>AutoDAN</td><td>24.42</td><td>16.54</td><td>116.30</td><td>9.04</td><td>10.00</td><td>137.37</td><td>64.04</td><td>50.00</td><td>114.55</td><td>32.50</td><td>25.51</td><td>122.74</td></tr><tr><td>ReNeLLM</td><td>30.38</td><td>19.42</td><td>82.21</td><td>32.31</td><td>29.04</td><td>62.68</td><td>52.50</td><td>50.77</td><td>74.51</td><td>38.40</td><td>33.08</td><td>73.13</td></tr><tr><td>PAIR</td><td>28.85</td><td>19.23</td><td>24.66</td><td>41.15</td><td>38.65</td><td>19.92</td><td>74.62</td><td>64.62</td><td>23.38</td><td>48.21</td><td>40.83</td><td>22.65</td></tr><tr><td>ASJA</td><td>57.70</td><td>37.88</td><td>34.48</td><td>54.23</td><td>54.04</td><td>39.01</td><td>78.27</td><td>69.23</td><td>38.42</td><td>63.40</td><td>53.72</td><td>37.30</td></tr><tr><td rowspan="4">QuestionList</td><td>AutoDAN</td><td>23.00</td><td>20.00</td><td>128.48</td><td>9.00</td><td>11.00</td><td>154.80</td><td>67.00</td><td>50.00</td><td>131.62</td><td>33.00</td><td>27.00</td><td>138.30</td></tr><tr><td>ReNeLLM</td><td>37.00</td><td>30.00</td><td>64.40</td><td>38.00</td><td>40.00</td><td>55.72</td><td>51.00</td><td>64.00</td><td>61.28</td><td>42.00</td><td>44.67</td><td>60.47</td></tr><tr><td>PAIR</td><td>40.00</td><td>30.00</td><td>31.34</td><td>46.00</td><td>42.00</td><td>26.68</td><td>69.00</td><td>66.00</td><td>30.60</td><td>51.67</td><td>46.00</td><td>29.54</td></tr><tr><td>ASJA</td><td>78.00</td><td>52.00</td><td>33.12</td><td>81.00</td><td>63.00</td><td>41.09</td><td>85.00</td><td>73.00</td><td>36.58</td><td>81.33</td><td>62.67</td><td>36.93</td></tr></table></body></html>

Table 1: Attack success rate (ASR) and Sentence Perplexity (PPL) comparison between ASJA and baselines. The best results are highlighted in bold, and the second best results are underlined.

Baselines. Our baselines include AutoDAN (Liu et al. 2024), ReNeLLM (Ding et al. 2023), and PAIR (Chao et al. 2023). AutoDAN employs a genetic algorithm to optimize the generation of jailbreak prompts, whereas ReNeLLM generates jailbreak prompts through a combination of prompt rewriting and scenario nesting. PAIR attacks the target model by leveraging LLMs to rewrite prompts based on historical dialogues. These refined prompts are then incorporated into subsequent conversations.

Metrics. We use Attack Success Rate (ASR) to evaluate the effectiveness of the attacks, which is the percentage of successful jailbreak samples out of the total samples. We adopt commonly used strategies from existing works to determine whether a jailbreak is successful, categorizing them into two types: (1) ASR-1: Initially, LLM responses are filtered using a predefined keyword dictionary. If no keywords are detected in a response, the model has not rejected the corresponding query. Then, the responses will be further filtered for harmfulness by the LLMs to ensure that benign or nonsensical outputs are not mistakenly classified as successful jailbreaks. Only responses that pass both filters are considered successful jailbreaks. For the keyword dictionary and harmfulness detection prompts, we adopt those utilized in prior research (Ding et al. 2023; Liu et al. 2024). (2) ASR-2: Apart from the two filters in ASR-1, we adopt an extra filter here which evaluates the relevance of a response to its query. As such, for a successful jailbreak, its response not only retains harmful content but also remains relevant to the original harmful query. This criterion is crucial because existing attack methods often involve rewriting the original query.

This could activate the model’s inherent safety mechanisms, potentially diluting the harmful intent or shifting the focus to less harmful or unrelated issues. The prompt template used in our experiment is adapted from prior works (Ding et al. 2023; Liu et al. 2024). In addition to ASR, we also use Sentence Perplexity (PPL) calculated by GPT-2 (Radford et al. 2019) to evaluate the stealthiness of adversarial queries, following the attack of AutoDAN and ReNeLLM.

# Results

We evaluate the effectiveness, stealthiness, efficiency, and transferability of different attacks. Among these, the first two metrics are paramount for jailbreak attacks, which we assess across three target models and two datasets. The latter two metrics are related to the significant inference cost required for testing LLMs. An efficient attack can identify safety issues in LLMs with fewer queries, thereby saving substantial resources. Moreover, high transferability suggests that attacks can be adapted from smaller-parameter LLMs to larger-parameter LLMs, thereby further reducing inference costs. To demonstrate the practical utility of these attacks on the most securely aligned LLMs, we use LLaMA2 as the target model for evaluating the latter two metrics, as the average ASR of the four attacks is lowest on this model.

Effectiveness. Table 1 presents the ASR and PPL of our attack ASJA, along with three baselines. In terms of effectiveness, ASJA consistently achieves the highest ASR across three target LLMs and two datasets compared to the three baselines. On average, ASJA improves ASR-1 by $4 4 . 9 1 \%$ and ASR-2 by $3 4 . 0 2 \%$ relative to the best baseline, PAIR, highlighting its effectiveness. Notably, the ASR-2 for all attacks is generally lower than ASR-1. This may be attributed to query rewriting and refining during the attacks. Specifically, (1) the inherent security mechanisms of LLMs might suppress the accurate expression of rewritten promptssuch as altering “bomb” to “chemical mixtures”leading to responses that, while still harmful, are significantly less so, and are therefore judged as less relevant. (2) Some jailbreak strategies introduce jailbreak scenarios during rewriting, such as “Imagined Scenario”. When the model engages with such scenarios and responds in the form of a story or poem, it may struggle to provide effective responses to harmful queries and instead generate a large amount of irrelevant content. Regarding the datasets, it is observed that all attacks generally achieve higher ASR on the QuestionList. This may be due to the stylistic constraints of malicious prompts in the AdvBench. For instance, some harmful queries begin with “Produce a video” or “Develop an app”. Even if LLMs provide a response, it may include phrases such as “but I can’t produce the video directly”, which contain filtering keywords like “I can’t” and are thus directly excluded by the keyword filtering. As for the target LLMs, the ASR on LLaMA-2 and LLaMA-3.1 is lower than that on Qwen-2, indicating that the red teaming enhancements employed by the former are more effective in enhancing the safety alignment of LLMs.

![](images/d2b1916f12ee6baae03b74b32c315c74e206abb26748a1c876417f83fc9c4d0c.jpg)  
Figure 3: ASR-1 of ASJA, AutoDAN, and PAIR on Qwen-2 and AdvBench under Different Queries

Stealthiness. Regarding the stealthiness of adversarial queries, both PAIR and ASJA achieve the lowest average PPL, demonstrating that our approach preserves the stealthiness of adversarial prompts while maintaining a high ASR. This advantage arises because PAIR and ASJA leverage LLMs to generate new queries across different turns that are more natural in expression. In contrast, AutoDAN and ReNeLLM embed harmful queries into predefined template scenarios, which can diminish their effectiveness against perplexity-based defenses.

Efficiency. We evaluate the efficiency of various attacks by presenting the ASR-1 under different query budgets. To better observe the differences, we focus on AdvBench, as it contains more samples, thereby providing a more accurate reflection of ASR-1 variations under different query budgets. We exclude ReNeLLM as it first generates potential adversarial queries using substitute LLMs and then obtains responses from the target LLMs, resulting in a theoretical query count of 1 for the target model, making it the most efficient by design. Figure 3 presents the results for the remaining three attacks. ASJA demonstrates the highest ASR1 across all query budgets, indicating that it can achieve the same ASR-1 as the baseline with fewer queries, thereby highlighting its efficiency. For the baseline, PAIR is concentrated in the low-query region, employing only 5 iterations with a batch size of 5, resulting in a theoretical maximum of 25 queries. A significant number of successful samples require all 25 queries to be exhausted. This limited search strategy hampers its ability to generate effective adversarial queries. In contrast, AutoDAN requires a higher number of queries (we exclude samples exceeding 100 queries) because it selects candidates by computing the loss of the target LLMs through multiple forward passes in batches. Each candidate selection necessitates querying the model multiple times based on the batch size, leading to lower efficiency. In our reproduction, the batch size is set to 32, meaning that calculating the loss and querying the optimal candidate requires a minimum of 33 queries, which explains why the curve starts at 34 queries in the figure.

Table 2: Cross-model Attack success rate (ASR) and Sentence Perplexity (PPL) based on LLaMA-2 samples   

<html><body><table><tr><td rowspan="2">Attack</td><td colspan="3">GPT-3.5</td><td colspan="3">GPT-40</td></tr><tr><td>ASR-1</td><td>ASR-2</td><td>PPL</td><td>ASR-1</td><td>ASR-2</td><td>PPL</td></tr><tr><td rowspan="4">AutoDAN ReNeLLM PAIR</td><td>61.00</td><td>53.00</td><td>146.52</td><td>46.00</td><td>59.00</td><td>149.18</td></tr><tr><td>59.00</td><td>48.00</td><td>60.29</td><td>57.00</td><td>58.00</td><td>58.07</td></tr><tr><td>18.00</td><td>35.00</td><td>34.24</td><td>14.00</td><td>33.00</td><td>36.39</td></tr><tr><td>56.00</td><td>54.00</td><td>40.38</td><td>57.00</td><td>63.00</td><td>37.33</td></tr></table></body></html>

Transferability. We select two widely used GPT series models (i.e., GPT-3.5 and GPT-4o) to evaluate the transferability of different attacks. We choose all adversarial queries from the QuestionList generated on LLaMA-2 to attack the GPT series models, as the average ASR of the four attacks is the lowest on this model. The results are shown in Table 2. We observe that the ASR of AutoDAN and ReNeLLM increases, while the ASR of SAJA and PAIR decreases. We hypothesize that this difference may stem from the underlying jailbreak strategies. Both AutoDAN and ReNeLLM embed the rewritten harmful questions into predefined template scenarios. For instance, ReNeLLM embeds harmful questions within a block of LaTeX code. These embedded queries may provide better camouflage, thereby more effectively disrupting the safety alignment of GPT series models. However, their high PPL also poses a risk of performance degradation when faced with PPL filtering defenses. PAIR and ASJA both use the complete outputs of LLMs as adversarial queries, so while they maintain better naturalness, their performance is weakened on large-scale parameter LLMs like GPT-4o. Nevertheless, ASJA achieves over $50 \%$ across both LLMs and two ASR metrics, ranking the highest except for ASR-1 on GPT-3.5.

# Conclusion

In this paper, we investigate the positions that LLMs pay attention to by exploiting multi-turn jailbreaks. The results show that successful jailbreaks often shift the attention of harmful queries toward the dialogue history, especially the historical responses in the middle. Based on such findings, we propose ASJA, a multi-turn jailbreak leveraging attention shifting with a genetic algorithm. It effectively breaks the LLM’s safety alignment by shifting the LLM’s attention to historical queries and responses via fabricating dialogue. Extensive evaluations demonstrate that ASJA improves the harmfulness of responses by $4 4 . 9 1 \%$ and enhances the relevance to the original query by $3 4 . 0 2 \%$ compared to the best baseline. Additionally, it ensures better or comparable stealthiness and transferability of adversarial queries compared to the baselines.

# Ethics Statement

This study aims to explore the security risks of LLMs under multi-turn jailbreaking. The primary objective is to identify vulnerabilities within current LLMs and generate multiturn jailbreaking data, thereby further advancing the security alignment of LLMs with human values. The approach and data (including harmful prompts and model responses) used in this paper are solely for research purposes. All experiments are conducted in a controlled environment, and no detailed executable harmful content is presented in this paper to prevent any potential real-world impact.