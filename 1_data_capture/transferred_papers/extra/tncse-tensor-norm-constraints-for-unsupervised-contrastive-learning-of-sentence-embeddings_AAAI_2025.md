# TNCSE: Tensor’s Norm Constraints for Unsupervised Contrastive Learning of Sentence Embeddings

Tianyu Zong1, Bingkang $\mathbf { S h i } ^ { 2 }$ , Hongzhu $\mathbf { Y _ { i } ^ { \bullet 1 } }$ , Jungang $\mathbf { X } \mathbf { u } ^ { 1 * }$

1School of Computer Science and Technology, University of Chinese Academy of Sciences 2School of Cyber Security, University of Chinese Academy of Sciences zongtianyu20, shibingkang20, yihongzhu23 @mails.ucas.ac.cn xujg $@$ ucas.ac.cn

# Abstract

Unsupervised sentence embedding representation has become a hot research topic in natural language processing. As a tensor, sentence embedding has two critical properties: direction and norm. Existing works have been limited to constraining only the orientation of the samples’ representations while ignoring the features of their module lengths. To address this issue, we propose a new training objective that optimizes the training of unsupervised contrastive learning by constraining the module length features between positive samples. We combine the training objective of Tensor’s Norm Constraints with ensemble learning to propose a new Sentence Embedding representation framework, TNCSE. We evaluate seven semantic text similarity tasks, and the results show that TNCSE and derived models are the current state-of-the-art approach; in addition, we conduct extensive zero-shot evaluations, and the results show that TNCSE outperforms other baselines.

Code — https://github.com/tianyuzong/TNCSE

# Introduction

Unsupervised sentence embedding representations have always been a research focus in natural language processing. In recent years, with the proposal of excellent pretrained language models such as BERT(Devlin et al. 2019), RoBERTa(Liu et al. 2019), Sentence-BERT(Reimers and Gurevych 2019), and so on, many works have been done to optimize further the quality of sentence embedding representations based on them. Sentence embedding for unsupervised contrastive learning represented by SimCSE(Gao, Yao, and Chen 2021) has attracted extensive research interest, which uses InfoNCE(van den Oord, Li, and Vinyals 2018) as the loss function, the dropout function of BERTlike models as the positive sample generation method, other samples in the batch as the soft-negative samples, and cosine similarity as the semantic similarity metric function for the unsupervised training; on this basis, ESimCSE(Wu et al. 2022b) introduces near-antonym data augmentation, InfoCSE(Wu et al. 2022a) introduces auxiliary networks and adds masked language models, ArcCSE(Zhang et al. 2022)

introduces margin for contrastive learning, EDFSE(Zong and Zhang 2023) employs the data augmentation strategy for multilingual round-trip translation(RTT) and ensemble learning, and RankCSE(Liu et al. 2023), the current SOTA method, employs knowledge distillation of naive BERT-like models with existing checkpoints.

Although the above approaches have demonstrated in different directions that fine-tuning based on InfoNCE can further optimize SimCSE, they have all neglected an essential drawback of an InfoNCE with cosine similarity as a metric, i.e., cosine similarity can only measure the angle between the embedding representations of a pair of sentences and ignores the critical attribute of module of the embedding representations. In fact, if the embedding representations of two sentences have similar angles in the high-dimensional space, but the difference in the module is significant, their semantics may still be very different, and the cosine similarity only cannot differentiate the semantic difference between these two sentences. Based on this intuitionistic phenomenon, it is necessary to introduce the difference in the module between the representations to optimize SimCSE further.

Meanwhile, ensemble learning has been shown to improve sentence embedding representations’ quality significantly. For example, EDFSE uses six SimCSE-BERTs that have been fine-tuned by multilingual RTT data augmentation as submodels and directly sums the outputs of the six submodels to obtain an ensemble model, but such a large union brings a huge inference overhead; RankCSE, on the other hand, uses the ensemble learning of two existing checkpoints as ground-truth to distill the knowledge of a naive BERT-like model. So, is it possible to introduce norm constraints on sentence embedding representations into ensemble learning, and the model can train autonomously without relying on other checkpoints of the same type?

The answer is yes, and in this paper, we propose a new and intuitive training objective: for a pair of positive samples of the embedding tensor, the cosine similarity between them should be high, and both norms should be similar. In other words, the alignment between positive samples should constrain the embedding’s angle and modulus simultaneously. In terms of model structure, the multi-encoder structure can model richer sample features from multiple spaces compared to a single encoder. Considering the inference cost, we ensemble two encoders that have been unsupervised fine-tuned. For the semantic representation of a sample, we perform self-supervised cosine constraints within each encoder and implement mutually supervised norm constraints between the two encoders to achieve our proposed training objective.

Then, we evaluate our proposed method on seven semantic text similarity tasks. The experimental results show that TNCSE, added by norm constraints, outperforms EDFSE on BERT-base, which is three times as large as TNCSE and becomes the new state-of-the-art method. For a fair comparison, we also do ensemble learning on other baselines and comparisons, and the results show that TNCSE is still the best. In addition, we evaluate TNCSE in a wider zero-shot on the MTEB(Muennighoff et al. 2023) list, and the results show that TNCSE also outperforms the baseline. We conduct a series of ablation experiments to evaluate the role of each part of the model, which illustrates the mechanism of TNCSE’s action intuitively. We summarize the main contributions of our work as follows:

• We are the first to intuitively propose a positive sample tensor modulus-constrained training objective based on unsupervised contrastive learning and demonstrate that it is effective.   
• We combine ensemble learning with a tensor norm constraint strategy to propose a new sentence embedding representation framework, TNCSE, and generalize a series of derived models.   
• We evaluate TNCSE and derived models on seven STS tasks, and the results show that TNCSE has become a new SOTA method for STS tasks. In addition, we have done a series of zero-shot evaluations, such as multilingual and cross-linguistic STS tasks, sentence classification, retrieval, and reranking, and the results show that TNCSE outperforms the baselines.

# Related Work

InfoNCE(Noise Contrastive Estimation Loss(van den Oord, Li, and Vinyals 2018)) is a loss function for self-supervised learning, usually used to learn feature representations, which is based on the idea of information theory and learns the model parameters by comparing the similarity (usually cosine similarity) between positive and negative samples. There have been some significant works combining InfoNCE loss and pre-trained language models BERT(Devlin et al. 2019), RoBERTa(Liu et al. 2019), etc. to achieve training of sentence embedding representations for unsupervised contrastive learning. Both SimCSE(Gao, Yao, and Chen 2021) and ConSERT(Yan et al. 2021) use the idea of dropout to generate positive samples, and they have in common the use of cosine similarity as the only metric to discriminate between positive and negative samples. Subsequent work further optimizes the training of unsupervised sentence embeddings based on SimCSE, e.g., ESimCSE(Wu et al. 2022b) provides the idea of proxemic data augmentation while associating the idea of MoCo’s(He et al. 2020) Momentum Queue to improve the quality of SimCSE’s representations; DiffCSE(Chuang et al. 2022) introduces additional discriminator knowledge to assist the pre-trained models to mask linguistic modeling. SNCSE(Wang and Dou 2023) employs a contrastive learning strategy that combines soft and negative samples with a bi-directional margin loss; and InfoCSE(Wu et al. 2022a) introduces an additional network for constructing a masked language model; EDFSE(Zong and Zhang 2023) constructs huge ensemble models by training multiple encoders through RTT data augmentation; the current SOTA approach, RankCSE(Liu et al. 2023), uses dual-teacher ensemble learning and distillation to train encoder. The commonality among these existing works is that they all introduce the InfoNCE loss, which employs the cosine angle between embeddings of individual samples to determine similarity. The crucial factor of the norm of the embedding tensor has been overlooked, so we propose an unsupervised ensemble learning framework with embedding representation norm constraints, to enhance the model’s ability to discriminate the positive and negative samples.

![](images/4c32a6018c60e0206823ef754c74476245fd69307494088bc19fa4ce52943889.jpg)  
Figure 1: The figure denotes the subtraction of the semantic representation tensor in 3D space.

![](images/eabe71c1c220182fee2d92f2da0261ab9a2cec4b53b8d8d090919505e704965c.jpg)  
Figure 2: The figure denotes the binary function image of the constraint loss of the tensor norm for independent variables $k$ and $t$ .

# Method

In this section, we start by presenting the norm constraints imposed on semantic representation tensors. Subsequently,

we incorporate ensemble learning strategies, leading to the elaboration of our proposed framework, the Tensor Norm Constrained Semantic Embedding (TNCSE).

# Norm Constraints on Representations

Traditional unsupervised text embedding representation training typically uses the cosine similarity between representation tensors as the primary metric for determining the degree of similarity between sentence pairs, but this only considers the “direction” of the tensor properties and ignores the “magnitude”. Wang and Isola (2020) believes that semantic representations can be projected onto a hypersphere. The distribution of positive samples should be aligned as much as possible. The soft negative samples should be distributed uniformly on the hypersphere. However, to distinguish semantic similarity among many texts in a fine-grained way, we should consider both the direction and the magnitude of the representation tensor. For a semantic representation of a pair of positive samples, intuitively, the angle between them should be as slight as possible, and the magnitudes should be identical. Therefore, in our approach, we model the 2-paradigm(denoted hereafter as the norm) of the semantic representation tensor as its “magnitude”, to perform constraints, and propose a loss based on the norm constraints as shown in Eq. 1:

$$
l _ { T N } ( h , h ^ { + } ) = \frac { \| h - h ^ { + } \| } { \| h \| + \| h ^ { + } \| } .
$$

In Eq. 1, $h$ and $h ^ { + }$ denote the embedding representations of a sample and its positive samples, $\left\| \cdot \right\|$ denotes the norm of tensor, and we come to demonstrate the rationality of this loss. For visualization, we report this process in a 3-dimensional space. As shown in Fig. 1, the two semantic representation tensors $h$ and $h ^ { + }$ of positive samples have difference tensor $h - h ^ { + }$ . These three tensors construct a triangle with sides $\| h \| , \| h ^ { + } \|$ , and $\lVert h - h ^ { + } \rVert$ , and let the angle between $h$ and $h ^ { + }$ be $\gamma$ . Then, according to the cosine theorem, we have:

$$
l _ { T N } ( h , h ^ { + } ) = \frac { \sqrt { \left\| h \right\| ^ { 2 } + \left\| h ^ { + } \right\| ^ { 2 } - 2 \left\| h \right\| \left\| h ^ { + } \right\| \cos \gamma } } { \left\| h \right\| + \left\| h ^ { + } \right\| } .
$$

Since the norm of semantic representation are all nonzero, we make $\| h ^ { + } \| = k \cdot \| h \|$ , $t = \cos \gamma$ , where $k \in ( 0 , + \infty )$ and $t \in [ - 1 , 1 ]$ . Bringing above into the Eq. 2, there is:

$$
l _ { T N } \left( k , t \right) = \frac { \sqrt { 1 + k ^ { 2 } - 2 \cdot k t } } { 1 + k } .
$$

We plot the image of the binary function $l _ { T N }$ for the independent variables $k$ and $t$ over part of the domain of definition, as shown in Fig. 2. $l _ { T N }$ takes a minimal value when $k$ and $t$ are equal to 1, respectively, in other word, when $\cos \gamma = 1$ and $\| h ^ { + } \| = \| h \|$ . This result aligns with our intuition, our proposed $l _ { T N }$ can simultaneously constrain the two metrics of cosine similarity and modulus length between positive samples of the semantic representation tensor.

# Model Structure Design

This subsection describes our proposed sentence embedding representation framework TNCSE based on tensor norm constraints, as shown in Fig. 3b. The traditional large-scale ensemble model EDFSE (Fig. 3a) uses six encoders, each of which has been pretrained with multilingual RTT data augmentation and unsupervised SimCSE. This ensemble strategy endows the model with the advantage of diverging the embeddings from different encoders as much as possible, thereby increasing the ”intrinsic rank” of the joint representation, leading to enhanced capability in discriminating between semantically similar sentences. We follow EDFSE’s data augmentation approach, but only employ two encoders that have been data augmented and fine-tuned by unsupervised SimCSE for reducing EDFSE’s inference overhead. For each of the two encoders of TNCSE-BERT and TNCSERoBERTa, we randomly chose four target languages for RTT Data Augmentation (EN-Target-EN). Each of these encoders ends up with its pooler layer. We denote these two encoders as Encoder I and Encoder II, respectively.

When the TNCSE is training, due to the dropout in the encoder, each sentence is fed into Encoder I and Encoder II twice to obtain positive samples, respectively. The last hidden state of both encoders contains two similar embeddings, and these four representations are denoted as $h _ { \mathrm { I } } ^ { L } , h _ { \mathrm { I } } ^ { L ^ { + } } , \bar { h _ { \mathrm { I I } } ^ { L } }$ , and $h _ { \mathrm { I I } } ^ { L ^ { + } }$ , respectively. Since both are representations of the same sentence sample, two of these four hidden states are positive samples of each other, so we set the respective InfoNCE and the interaction-constrained InfoNCE(ICNCE), InfoNCE is denoted as Eq. 4:

$$
L _ { N C E } \left( h _ { i } , h _ { i } ^ { + } \right) = - \log \frac { e ^ { \frac { \sin \left( h _ { i } , h _ { i } ^ { + } \right) } { \tau } } } { e ^ { \frac { \sin \left( h _ { i } , h _ { i } ^ { + } \right) } { \tau } } + \sum e ^ { \frac { \sin \left( h _ { i } , h _ { j } ^ { - } \right) } { \tau } } } .
$$

Here, $i \in \{ \mathrm { I } , \mathrm { I I } \}$ indicates from Encoder I or Encoder II. $h _ { i }$ and $h _ { i } ^ { + }$ are mutually positive samples, and $h _ { j } ^ { - }$ is the other samples within the current batch size as soft-negative samples. We follow the SimCSE setting with $\tau = 0 . 0 5$ ; $s i m ( \cdot )$ is the cosine similarity. ICNCE is then denoted as Eq. 5:

$$
L _ { I C N C E } = L _ { N C E } \left( h _ { \mathrm { I } } ^ { L } , h _ { \mathrm { I I } } ^ { L } \right) .
$$

InfoNCE serves to continue training and maintain the general direction of the encoder’s gradient update, while ICNCE is designed to precondition subsequent tensor norm loss.

Due to the norms of the last hidden states in BERT-like models converging to a uniform value, we cannot effectively apply tensor norm constraints to the last hidden states of positive sample pairs. However, we notice that the last pooler layer in a BERT-like model is a single feedforward neural network, which can be endowed with normative features by projecting the last hidden state onto a space of the same dimension. Accordingly, we naturally employ the output of the pooler layer for tensor norm constraints. Moreover, we find that the cosine similarity of the last hidden state of the positive sample pairs can be used as a nondeterministic prompt for the $l _ { T N }$ . Thus, we amend the $l _ { T N }$ to be Eq. 6:

![](images/46ca9e0113d475ad611ca2650699f7ae3bc3e6b75726c5b821f2d5f7010e5fe7.jpg)  
Figure 3: The left side of the solid line is the traditional ensemble learning method represented by EDFSE, and the right is our proposed new method based on semantic tensor with norm constraints. When TNCSE is training, sample $A$ passes through two encoders simultaneously, obtains the last hidden state, does $\mathrm { I n f o N C E } ( L _ { N C E } )$ and $\mathrm { I C N C E } ( L _ { I C N C E } )$ , then passes through the corresponding pooler layer and does $\mathrm { T N C } ( L _ { I C T N } )$ with norm constraints crossly, respectively. When TNCSE is inference, sample $A$ passes through two encoders simultaneously, and then the two last hidden states are directly summed up as the output.

$L _ { T N } \left( h _ { i } , h _ { j } ^ { + } \right) = - \log \left( s i m \left( h _ { \mathrm { I } } ^ { L } , h _ { \mathrm { I I } } ^ { L } \right) \right) \frac { \left\| h _ { i } ^ { P } - h _ { j } ^ { P ^ { + } } \right\| } { \left\| h _ { i } ^ { P } \right\| + \left\| h _ { j } ^ { P ^ { + } } \right\| } ,$ (6) where $i , j ~ \in ~ \{ \mathrm { I } , \mathrm { I I } \}$ and $i \ne j . h _ { \mathrm { I } } ^ { P } , h _ { \mathrm { I } } ^ { P ^ { + } } , h _ { \mathrm { I I } } ^ { P }$ , and $h _ { \mathrm { I I } } ^ { P ^ { + } }$ denote the pooler output of these hidden states, respectively. Here we set up an interaction constraint on the tensor norm(ICTN), denoted as Eq. 7:

$$
{ \cal L } _ { I C T N } = { \cal L } _ { T N } \left( h _ { \mathrm { I } } , h _ { \mathrm { I I } } ^ { + } \right) + { \cal L } _ { T N } \left( h _ { \mathrm { I I } } , h _ { \mathrm { I } } ^ { + } \right) .
$$

The purpose of designing $\boldsymbol { L _ { I C T N } }$ in this way is to enhance the norm alignment of positive samples in the joint representation space of the two encoders. Finally, the loss of the TNCSE is denoted as Eq. 8:

$$
L = \sum _ { i \in \{ \mathrm { I , I I } \} } L _ { N C E } \left( h _ { i } ^ { L } , h _ { i } ^ { L ^ { + } } \right) + L _ { I C N C E } + L _ { I C T N } .
$$

We have demonstrated through ablation experiments that each of the above components is necessary. When we are testing TNCSE, for an input sentence, we use two encoders to encode the sentence separately, and the obtained last hidden state is directly summed up without going through the pooler layer as the output of TNCSE.

# Experiment

Setup In TNCSE, we first employ two BERT-bases or RoBERTa-bases for pre-training with Google Translate for two different RTT Data Augmentation and unsupervised SimCSE, respectively. The dataset employed is the 1M Wiki corpus2 and unlabelled data from SICKR(Marelli et al. 2014). Hyperparameter settings and target languages of RTT are reported in Appendix I. We follow the SimCSE setup, with Spearman’s correlation of the model on the STS-B validation set as the checkpoint saved metric. We follow SimCSE with CLS as the pooling method and report the impact of pooling in Appendix II.

Tasks We evaluate TNCSE and baseline on 7 STS tasks with the SentEval(Conneau and Kiela 2018) tool, including STS12-16(Agirre et al. 2012, 2013, 2014, 2015, 2016), STS-B(Cer et al. 2017a), and SICKR; we then conduct extensive zero-shot tests on the sentence-embedding task list MTEB(Muennighoff et al. 2023), which includes both multilingual and cross-language STS tasks, such as STS17(Cer et al. 2017b), STS22(Chen et al. 2022), and randomly select 30 sentence classification, 30 retrieval and all reranking tasks available for the test set.

Experimental Results We report the experimental results for the seven STS tasks in Table 1, where TNCSE outperforms the previous baselines. Since TNCSE is an ensemble model containing two encoders, we follow EDFSE’s distillation approach and distill the knowledge from TNCSE to a naive BERT/RoBERTa, and obtain TNCSE-D, which also outperforms baselines as a single-encoder model. Compared to a traditional ensemble model like EDFSE, we propose an efficient training objective that makes the inference overhead of TNCSE only one-third of EDFSE-BERT, but with better results on the 7 STS task.

Table 1: This table reports the results of the TNCSE and baseline evaluation on the seven STS tasks. $\star$ denotes results derived from the original paper. indicates a fusion with existing unsupervised checkpoint knowledge. ss indicates the best result on the main metric Avg. Since RankCSE(Liu et al. 2023) has not officially open-sourced any code or checkpoints, $\clubsuit$ denotes the result of a third-party open-source code replication1. D denotes distillation to a single encoder.   

<html><body><table><tr><td>Model</td><td>STS12</td><td>STS13</td><td>STS14</td><td>STS15</td><td>STS16</td><td>STSB</td><td>SICKR</td><td>Avg.</td></tr><tr><td colspan="9">BERT-base</td></tr><tr><td>SimCSE(Gao,Yao,and Chen 2021)*</td><td>68.40</td><td>82.41</td><td>74.38</td><td>80.91</td><td>78.56</td><td>76.85</td><td>72.23</td><td>76.25</td></tr><tr><td>DiffCSE(Chuang et al. 2022)*</td><td>72.28</td><td>84.43</td><td>76.47</td><td>83.90</td><td>80.54</td><td>80.59</td><td>71.23</td><td>78.49</td></tr><tr><td>ESimCSE(Wu et al. 2022b)*</td><td>73.40</td><td>83.27</td><td>77.25</td><td>82.66</td><td>78.81</td><td>80.17</td><td>72.30</td><td>78.27</td></tr><tr><td>ArcCSE(Zhang etal.2022)*</td><td>72.08</td><td>84.27</td><td>76.25</td><td>82.32</td><td>79.54</td><td>79.92</td><td>72.39</td><td>78.11</td></tr><tr><td>InfoCSE(Wu et al.2022a)*</td><td>70.53</td><td>84.59</td><td>76.40</td><td>85.10</td><td>81.95</td><td>82.00</td><td>71.37</td><td>78.85</td></tr><tr><td>PromptBERT(Jiang et al. 2022)*</td><td>71.56</td><td>84.58</td><td>76.98</td><td>84.47</td><td>80.60</td><td>81.60</td><td>69.87</td><td>78.54</td></tr><tr><td>SNCSE(Wang and Dou 2023)*</td><td>70.67</td><td>84.79</td><td>76.99</td><td>83.69</td><td>80.51</td><td>81.35</td><td>74.77</td><td>78.97</td></tr><tr><td>WhitenedCSE(Zhuo etal.2023)*</td><td>74.03</td><td>84.90</td><td>76.40</td><td>83.40</td><td>80.23</td><td>81.14</td><td>71.33</td><td>78.78</td></tr><tr><td>EDFSE(Zong and Zhang 2023)*</td><td>74.48</td><td>83.14</td><td>76.39</td><td>84.45</td><td>80.02</td><td>81.97</td><td>72.83</td><td>79.04</td></tr><tr><td>TNCSE</td><td>75.52</td><td>83.91</td><td>77.57</td><td>84.97</td><td>80.42</td><td>81.72</td><td>72.97</td><td>79.58</td></tr><tr><td>EDFSE D(Zong and Zhang 2023)*</td><td>74.50</td><td>83.61</td><td>76.24</td><td>84.02</td><td>80.44</td><td>81.94</td><td>74.16</td><td>79.27</td></tr><tr><td>TNCSE D</td><td>75.42</td><td>84.64</td><td>77.62</td><td>74.92</td><td>80.50</td><td>81.79</td><td>73.52</td><td>79.77</td></tr><tr><td>RankCSE(Liu et al. 2023)+</td><td>74.61</td><td>85.70</td><td>78.09</td><td>84.64</td><td>81.36</td><td>81.82</td><td>74.51</td><td>80.10</td></tr><tr><td>RankCSE+UC</td><td>73.29</td><td>85.90</td><td>78.16</td><td>85.90</td><td>82.52</td><td>83.13</td><td>73.36</td><td>80.32</td></tr><tr><td>TNCSE+UC</td><td>75.79</td><td>85.27</td><td>78.67</td><td>85.99</td><td>82.01</td><td>83.16</td><td>73.01</td><td>80.56</td></tr><tr><td>RankCSE+UC D</td><td>72.99</td><td>85.72</td><td>77.73</td><td>84.93</td><td>81.86</td><td>82.43</td><td>74.35</td><td>80.00</td></tr><tr><td>TNCSE+UC D</td><td>75.95</td><td>85.31</td><td>78.50</td><td>85.69</td><td>81.86</td><td>83.03</td><td>73.89</td><td>80.60</td></tr><tr><td colspan="9">RoBERTa-base</td></tr><tr><td>SimCSE(Gao, Yao,and Chen 2021)*</td><td>70.16</td><td>81.77</td><td>73.24</td><td>81.36</td><td>80.65</td><td>80.22</td><td>68.56</td><td></td></tr><tr><td>DiffCSE(Chuang et al. 2022)*</td><td>70.05</td><td>83.43</td><td>75.49</td><td>82.81</td><td>82.12</td><td>82.38</td><td>71.19</td><td>76.57 78.21</td></tr><tr><td>ESimCSE(Wu et al. 2022b)*</td><td>69.90</td><td>82.50</td><td>74.68</td><td>83.19</td><td>80.30</td><td>80.99</td><td>70.54</td><td></td></tr><tr><td>PromptBERT(2022)*</td><td>73.94</td><td>84.74</td><td>77.28</td><td>84.99</td><td>81.74</td><td>81.88</td><td>69.50</td><td>77.44</td></tr><tr><td>SNCSE(Wang and Dou 2023)*</td><td>70.62</td><td>84.42</td><td>77.24</td><td>84.85</td><td>81.49</td><td>83.07</td><td></td><td>79.15</td></tr><tr><td>WhitenedCSE(Zhuo et al.2023)*</td><td>70.73</td><td>83.77</td><td>75.56</td><td>81.85</td><td>83.25</td><td></td><td>72.92</td><td>79.23</td></tr><tr><td>IS-CSE(He et al. 2023)*</td><td>71.39</td><td>82.58</td><td>74.36</td><td>82.75</td><td>81.61</td><td>81.43 81.40</td><td>70.96</td><td>78.22</td></tr><tr><td>EDFSE(Zong and Zhang 2023)</td><td>72.67</td><td>83.00</td><td>75.69</td><td>84.07</td><td>82.01</td><td>82.53</td><td>69.99</td><td>77.73</td></tr><tr><td>TNCSE</td><td>74.11</td><td>84.00</td><td>76.06</td><td>84.80</td><td>81.61</td><td>82.68</td><td>71.92 73.47</td><td>78.84</td></tr><tr><td>EDFSE D(Zong and Zhang 2023)</td><td>71.04</td><td>81.08</td><td>77.04</td><td>83.08</td><td>81.96</td><td>82.36</td><td>74.54</td><td>79.53</td></tr><tr><td>TNCSE D</td><td>74.56</td><td>84.74</td><td>76.30</td><td>84.89</td><td>81.70</td><td>83.01</td><td>74.18</td><td>78.73</td></tr><tr><td>RankCSE(Liu et al. 2023)$</td><td>69.09</td><td>81.15</td><td>73.62</td><td>81.31</td><td>81.43</td><td>81.22</td><td>70.08</td><td>79.91</td></tr><tr><td>RankCSE+UC</td><td>74.18</td><td>84.06</td><td>77.727</td><td>83.26</td><td>79.81</td><td>81.25</td><td>72.58</td><td>76.84</td></tr><tr><td>TNCSE+UC</td><td>74.52</td><td>85.26</td><td>77.63</td><td>85.85</td><td>82.62</td><td>83.65</td><td></td><td>78.98</td></tr><tr><td>RankCSE+UC D</td><td>68.55</td><td>82.23</td><td>73.61</td><td>81.28</td><td>81.28</td><td>80.98</td><td>73.35 71.01</td><td>80.41</td></tr><tr><td>TNCSE+UCD</td><td>74.14</td><td>83.86</td><td>76.09</td><td>84.07</td><td>81.59</td><td>82.90</td><td>73.55</td><td>76.99 79.46</td></tr></table></body></html>

Since RankCSE relies on the knowledge of the existing powerful checkpoints, SimCSE-base and SimCSE-large, to train the encoder in a knowledge distillation approach, and RankCSE is not trained independently, we consider that it is inappropriate to compare RankCSE with TNCSE and other baselines directly. We ensemble and distill RankCSE and TNCSE with unsupervised checkpoint InfoCSE in the same way, respectively, and get the ensemble model (UC) and distillation model (UC D) to evaluate; in other words, we prove that TNCSE is better than RankCSE by proving that TNCSE $+$ InfoCSE is better than RankCSE $+$ InfoCSE. In summary, TNCSE and derived models are the current SOTA methods for unsupervised STS tasks. Due to computational resource constraints and the unavailability of some test sets, we randomly select 30 sentence classification tasks and 30 retrieval tasks, 29 multilingual and cross-linguistic STS tasks (STS17, STS22), and all 9 reranking tasks for which test sets are available on the MTEB, and report the experimental results in Table 2, where the overall results show that the TNCSE and derived models outperform the baseline

<html><body><table><tr><td>Tasks(Avg.)</td><td>SimCSE</td><td>ESimCSE</td><td>DiffCSE</td><td>InfoCSE</td><td>SNCSE</td><td>RankCSE</td><td>TNCSE</td><td>+D</td><td>+UCD</td></tr><tr><td>STS17(11 Acc)</td><td>34.22</td><td>35.64</td><td>31.61</td><td>35.95</td><td>22.64</td><td>37.59</td><td>36.32</td><td>35.41</td><td>37.73</td></tr><tr><td>STS22(18 Acc)</td><td>32.78</td><td>36.75</td><td>34.33</td><td>26.48</td><td>23.61</td><td>37.01</td><td>39.34</td><td>38.95</td><td>39.26</td></tr><tr><td>Classification(30 Acc)</td><td>58.15</td><td>58.26</td><td>57.44</td><td>58.15</td><td>58.05</td><td>58.15</td><td>58.38</td><td>58.59</td><td>58.84</td></tr><tr><td>Reranking(9 Map)</td><td>36.78</td><td>38.12</td><td>37.13</td><td>38.36</td><td>31.95</td><td>37.11</td><td>38.15</td><td>38.93</td><td>38.29</td></tr><tr><td>Retrieval(30 Map @10)</td><td>18.98</td><td>21.13</td><td>19.09</td><td>21.54</td><td>16.88</td><td>18.33</td><td>22.10</td><td>23.16</td><td>22.14</td></tr></table></body></html>

Table 2: This table reports the results of the zero-shot evaluation of TNCSE and baseline on MTEB, with the number of tasks and metric indicated in parentheses. Details of the results are reported in Appendix IV.

<html><body><table><tr><td>Model(Dual Encoder)</td><td>SimCSE</td><td>ESimCSE</td><td>DiffCSE</td><td>InfoCSE</td><td>RankCSE</td><td>TNCSE(Untrained)</td><td>TNCSE</td></tr><tr><td>7 STS Avg.</td><td>77.97</td><td>78.51</td><td>77.89</td><td>77.05</td><td>78.22</td><td>78.27</td><td>79.58</td></tr></table></body></html>

Table 3: We reports the results of ensemble learning done by a set of encoders obtained by adding unlabelled SICKR dataset and the 2 RTT augmented datasets to which each baseline is trained(Default hyperparameters) and compares them with TNCSE and the two encoders untrained by TNCSE. The inconsistency between the dual SimCSE and TNCSE-Untrained results is due to the fine-tuning of the learning rate we have done during pre-training, but the dependence on external knowledge is the same.

![](images/94646815bb4425653a131347b43aad08315e814983dc4671564e0096c0113ee8.jpg)  
Figure 4: The figure reports the results of the significance test. We specify the random seeds are 1 to 5, other hyperparameters are defaulted, training set is uniformly Wiki1M and unlabelled SICKR.

![](images/bffd562dab365baac87a2856fb2bc173e324fa783ff3d9d60cb8d6e447c7f13f.jpg)  
Figure 5: The bar and line graphs separately represent the norm mean of the Last hidden state and the model’s performance on the STS-B validation set for different LayerNorm settings.

Significance Test Since the BERT-like model is single encoder and training is susceptible to random seeds, we design TNCSE with dual encoder structure, which is more stable than single encoder. We report the significance test of TNCSE and baselines in Figure 4.

approach.

# Ablation Studies and Discussion Alignment and Uniformity

Wang and Isola (2020) proposes that two critical metrics for measuring sentence embedding representations are uniformity and alignment, respectively, which we have already introduced in the model section, both of which are as small as possible. We report the scores of these two metrics for TNCSE and baselines in Figure 6. Due to the introduction of the tensor-norm alignment strategy in TNCSE, the loss of TNCSE concerning conventional angle alignment is superior. Because TNCSE unites ensemble learning, uniformity is also superior. We report on the detailed formulation of alignment and uniformity in Appendix III.

Table 4: This table combines each of the loss functions to explore the contribution of each to model training. None denotes a direct ensemble of two SimCSE-trained encoders. All experiments use CLS pooling method.   

<html><body><table><tr><td>Loss Choice</td><td>7 STS Avg.</td></tr><tr><td>None(Dual Encoder Untrained)</td><td>78.27</td></tr><tr><td>LNCE</td><td>78.42</td></tr><tr><td>LICNCE</td><td>78.71</td></tr><tr><td>LICTN</td><td>78.38</td></tr><tr><td>LNCE+LICNCE</td><td>78.71</td></tr><tr><td>LNCE+LICTN</td><td>79.53</td></tr><tr><td>LICNCE+LICTN</td><td>79.43</td></tr><tr><td>LNCE+LICNCE +LICTN(Ours)</td><td>79.58</td></tr></table></body></html>

Table 5: This table compares the training results of SimCSE and TNCSE with only a single encoder under the same training set.   

<html><body><table><tr><td>Model</td><td>SimCSE</td><td>TNCSE-Single</td></tr><tr><td>STS-B testset</td><td>77.75</td><td>78.37</td></tr></table></body></html>

# Ablation of The Loss Function

Since our loss function contains three terms, InfoNCE, ICNCE, and ICTN, this subsection investigates the performance gain from each term and analyses the reasons. The results of the ablation experiments are reported in Table 4.

$L _ { N C E }$ Only Since TNCSE employs a dual encoder structure, the sentence embedding is modeled through ensemble learning. If only unsupervised training of InfoNCE is performed for each encoder, which does not directly improve the ensemble learning, and result is improved insignificantly.

$\boldsymbol { L _ { I C N C E } }$ Only ICNCE is a boosted version of InfoNCE. ICNCE optimizes the tensor angle of positive and negative samples between two encoders, which works in ensemble learning. However, since this does not introduce a new training objective, it still belongs to the incremental training of InfoNCE, so the improvement is limited.

$\boldsymbol { L _ { I C T N } }$ Only Since $\boldsymbol { L _ { I C T N } }$ (Eq. 7) accomplishes norm constraint on the output of the encoder’s pooler layer, but in the pre-training of the encoder, only the last hidden state is taken out to do the self-supervised training, and it does not go through the pooler layer, there is a margin between the last hidden state and the pooler output. We will analyze in the discussion subsection that the last hidden state cannot directly use norm loss because it lacks norm features, so using only the pooler output to constrain the tensor norm in ensemble learning is almost ineffective with the CLS pooling method.

$L _ { N C E } + L _ { I C N C E }$ Under the joint constraints of $L _ { N C E }$ and $\boldsymbol { L } _ { I C N C E }$ , the two encoders perform self-supervised continuation training and, at the same time, mutually supervise each other’s hidden states, which facilitates fine-grained representations of samples by ensemble learning, but still falls under the tensor’s angle constraints. Therefore, the improvement is minor.

![](images/1a0ce5112231776b6eedc465b652f64d0fb114e391535b883a4ad30d42eba3b0.jpg)  
Figure 6: The alignment and uniformity metrics for TNCSE and baselines; the better the distribution is down the left. Darker the circle color, better the model performs on 7 STS.

$L _ { N C E } + L _ { I C T N }$ and $L _ { I C N C E } + L _ { I C T N }$ The combination of the tensor’s norm constraints and either of the angle constraints resulted in a significant improvement in training. This is due to the two properties of the trained tensor, allowing the two encoders to more clearly distinguish between two soft negative samples with similar representations after ensemble learning, improving the model performance.

# Ablation of Ensemble Learning

Since TNCSE is dual-encoder structured, although we have distilled its knowledge to a single encoder, we still ablate its structure. We keep only one naive encoder, BERT-base, whose pooler layer outputs a pair of positive samples to accomplish the constraints of $L _ { T N }$ (Eq. 6) and the last hidden state to accomplish the constraints of self-supervised InfoNCE. In other words, this structure only increases the $L _ { T N }$ loss relative to Unsup-SimCSE, and we retrained unsupervised SimCSE in the same experimental setting with the same training set for both Wiki1M and unlabelled SICKR. We report the model’s performance on the STS-B test set in Table 5. Compared with the Unsup-SimCSE, which includes the same baseline knowledge, the model with the added $L _ { T N }$ loss is more effective than the unsupervised SimCSE. In addition, we put several representative baselines through the same two different RTT pre-training to get a dual encoder, and then compare it with TNCSE. The results are reported in Table 3.

# Impact of Unlabelled SICKR

We have found that when reproducing SimCSE, ESimCSE, and DiffCSE, we cannot reproduce the results reported in the paper using the official open-source code and default hyperparameters. For example, our reproduction of SimCSE is only about $74 \%$ , which is far from the reported $7 6 . 2 5 \%$ , so we can only improve the reproduction level by adding an unsupervised dataset, based on which we roughly improve the results of $\mathrm { S i m C S E }$ to about $76 \%$ in order to be fair enough to conduct the subsequent experiments. In Table 6, we report the results we obtained by training the model with the default hyperparameters and the results by adding the SICKR dataset. In addition, we report the results of training TNCSE using the original Wiki1M to demonstrate that adding the unlabelled dataset does not significantly improve the model performance.

Table 6: The impact of adding unlabelled SICKR datasets on model training.   

<html><body><table><tr><td>Datasets</td><td>Wiki1M</td><td>+UnlabelledSICKR</td></tr><tr><td>SimCSE</td><td>74.3</td><td>75.9(+1.6)</td></tr><tr><td>ESimCSE</td><td>75.8</td><td>76.7(+0.9)</td></tr><tr><td>DiffCSE</td><td>75.2</td><td>78.0(+2.8)</td></tr><tr><td>InfoCSE</td><td>75.8</td><td>77.4(+1.6)</td></tr><tr><td>RankCSE</td><td>77.2</td><td>77.5(+0.3)</td></tr><tr><td>TNCSE</td><td>79.3</td><td>79.6(+0.3)</td></tr></table></body></html>

# Discussion: Why Choose Pooler Output Over Last Hidden State?

In our framework, pooler output is chosen for norm constraints; in this subsection, we discuss the reason. BERT and RoBERTa, pre-trained models based on the structure of the Transformer encoder, include two LayerNorms in each of their encoder layers, which are located after the attention layer and after the FNN, respectively, which makes their last hidden state lose the norm feature. No matter the text input to SimCSE-BERT, their last hidden state’s norms are almost always distributed between 14 and 16, which cannot do norm constraints. We remove the last few LayerNorms in the encoder layer of SimCSE-BERT to make the last hidden state get the norm features and apply the Eq. 8 on the last hidden state. However, the training results we get are instead worse, and the more LayerNorms we remove, the worse the result is. We consider that this destroys the pretraining information of SimCSE-BERT, which is unfavorable to the TNCSE training. We report this finding in Figure 5. We first randomly select 100 sentences from Wiki1M. Then we remove the last several layers of LayerNorm of the encoder, and report the average norm of the last hidden state, as shown in the bar graph. We train two encoders with the LayerNorm removed using the TNCSE method and report the performance on the STS-B test set. As can be seen from the line graphs, the model almost loses its semantic discriminative ability when all the last six layers of LayerNorm are removed. Overall, the more LayerNorms are preserved, the better the model works, but both are not as good as our proposed scheme employing Pooler output for tensor constraints. The above experiment demonstrates the ingenuity of our design.

# Conclusion

In this work, we propose a new training objective of unsupervised sentence embedding, which can constrain the module length of sentence embedding representations between positive samples; we model it as a tensor’s norm and jointly train with ensemble learning. Based on these methods, we propose a new unsupervised sentence embedding representation framework, TNCSE, which becomes a new SOTA method on seven STS tasks. Moreover, nearly one hundred zero-shot tasks are used to evaluate, and the results show that TNCSE and its derived models outperform other baselines. In addition, we conduct a series of ablation experiments to explore the effects of individual components.