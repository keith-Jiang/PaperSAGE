# LAMA-UT: Language Agnostic Multilingual ASR Through Orthography Unification and Language-Specific Transliteration

Sangmin Lee1, Woojin Chung1, Hong-Goo Kang1

1Dept. of Electrical & Electronic Engineering, Yonsei University, South Korea sangmin lee, woojinchung @dsp.yonsei.ac.kr, hgkang $@$ yonsei.ac.kr

# Abstract

Building a universal multilingual automatic speech recognition (ASR) model that performs equitably across languages has long been a challenge due to its inherent difficulties. To address this task we introduce a Language-Agnostic Multilingual ASR pipeline through orthography Unification and language-specific Transliteration (LAMA-UT). LAMAUT operates without any language-specific modules while matching the performance of state-of-the-art models trained on a minimal amount of data. Our pipeline consists of two key steps. First, we utilize a universal transcription generator to unify orthographic features into Romanized form and capture common phonetic characteristics across diverse languages. Second, we utilize a universal converter to transform these universal transcriptions into language-specific ones. In experiments, we demonstrate the effectiveness of our proposed method leveraging universal transcriptions for massively multilingual ASR. Our pipeline achieves a relative error reduction rate of $45 \%$ when compared to Whisper and performs comparably to MMS, despite being trained on only $0 . 1 \%$ of Whisper’s training data. Furthermore, our pipeline does not rely on any language-specific modules. However, it performs on par with zero-shot ASR approaches which utilize additional language-specific lexicons and language models. We expect this framework to serve as a cornerstone for flexible multilingual ASR systems that are generalizable even to unseen languages.

# Introduction

Developing a model for multilingual automatic speech recognition (ASR) is appealing due to its applicability to universal languages, including low-resource or unseen languages. However, this task presents significant challenges as it requires extensive datasets and involves the complexity of capturing shared characteristics across diverse languages in both phonetic and orthographic domains.

Since the revolution in ASR technologies driven by selfsupervised learning (SSL) models (Baevski et al. 2020; Conneau et al. 2020; Hsu et al. 2021), monolingual ASR has achieved superhuman transcription performance, shifting the main focus of recent research towards developing a universal model that spans multiple languages.

There are two primary methods for building a multilingual ASR model. The first approach involves scaling the size of both the labeled dataset and the model itself, using a single universal model to enhance its capacity and cover a vast number $( 1 0 0 + )$ of languages, thereby achieving multilingual ASR (Radford et al. 2023). Another approach involves incorporating language-specific modules into the universal model to address the performance inconsistencies of previous methods. For example, MMS (Pratap et al. 2024) demonstrated the feasibility of scaling multilingual technology to over 1,000 languages by leveraging common features across languages and adding language-specific modules to improve the performance of each language. Indeed, there have been efforts to integrate both methods (Zhang et al. 2023), combining their strengths to build a more robust and versatile model.

Although these works have demonstrated strong performance across various languages, the trade-off between performance and complexity remains a substantial challenge. The first method, using a single universal pipeline, struggles to achieve consistent performance across languages, and its effectiveness in low-resource languages remains uncertain. On the other hand, despite achieving state-of-the-art performance and parameter efficiency, the second method cannot be considered a single universal model due to the inclusion of language-specific modules. Moreover, the use of language-specific modules like adapters, heads, and language models (LMs) sometimes complicates the training and inference pipeline, suggesting potential areas for future improvement.

Simultaneously, large language models (LLMs) have garnered considerable attention for their remarkable capabilities in the natural language processing (NLP) domain. Following this trend, ASR pipelines have integrated audio SSL models as encoders and LLMs as decoders to enhance transcription quality (Li et al. 2023; Fathullah et al. 2024). These approaches involve using projectors (Yu et al. 2024) or fine-tuning strategies (Tang et al. 2024; Du et al. 2024) to align modalities and improve transcription capabilities across multilingual datasets. Subsequently, shallow fusion (Chorowski and Jaitly 2016; Kannan et al. 2018) based scoring methods (Hu et al. 2023; Huang et al. 2024) were attempted to replace conventional LMs with LLMs during the decoding stage. Despite these efforts resulting in per

$\textcircled{1}$ : (Georgian Speech) Prompt Dictionary   
$\textcircled{2}$ : man chorebs politikuri laqboba da sisulele utsoda Type 1: Zero-Shot   
$\textcircled{3}$ : Transcribe the following Romanized sentence into a Georgian sentence: Type 2: Few-Shot man chorebs politikuri laqboba da sisulele utsoda. Type 3: Zero-Shot CoT Type 4: Prompt Chaining   
④:as6μ33gou 33n3s@0g3o 3syo3s gos usug3ymə‘33ns Speech Embedding Language ID   
+车 ① Audio Classification ? Prompt ③ Universal\* ④ Language-Specific Encoder Layer Universal Generation Input Prompt Converter Transcription Transcription Phase 1: Universal Transcription Generation Phase 2: Language-Specific Transliteration

formance gain across various languages, a comprehensive method to fully leverage the diverse emergent abilities of LLMs remains to be developed.

In this paper, we introduce a novel language-agnostic multilingual ASR pipeline that spans over 100 languages, including completely unseen languages. As in Fig 1, the proposed pipeline consists of two phases: universal transcription generation and language-specific transliteration. In the universal transcription generation phase, we focused on reducing orthographic complexity by unifying diverse orthographic systems into a consistent format, approximating phonetic features across multiple languages. In the language-specific transliteration phase, we regard the transformation from universal transcription to language-specific transcription as a transliteration task by leveraging a universal converter. Our experiments demonstrate notable transcription performance of LAMA-UT across over 100 languages while using significantly smaller training data (only 680 hours) compared to other state-of-the-art multilingual ASR models. Furthermore, our proposed pipeline outperforms previous methods, especially in low-resource languages, and demonstrates proficiency in completely unseen languages, achieving performance comparable to existing language-agnostic ASR methods without relying on any language-specific modules. Our contributions are summarized as follows:

• We propose a novel language-agnostic multilingual ASR pipeline consisting of two phases: universal transcription generation and language-specific transliteration. • We enabled our proposed pipeline to perform multilingual ASR with minimal data by unifying diverse orthographic systems through Romanization. • Our pipeline demonstrates consistent performance across over 100 seen languages and excels with completely unseen languages, all without relying on any languagespecific modules or additional fine-tuning.

# Related Works

der 60. However, recent advancements have led to the development of models capable of managing a broader range of languages. Whisper (Radford et al. 2023) uses a sequenceto-sequence (Sutskever, Vinyals, and Le 2014) approach with 680,000 hours of weakly supervised data, and its neural decoder serves as a LM, enhancing transcription performance. With this method, Whisper attained impressive performance across most supported languages.

Google USM (Zhang et al. 2023) employs a Conformer (Gulati et al. 2020) encoder with various types of heads (Graves 2012; Chan et al. 2016) and is trained on an extensive dataset. It also employs a three-stage training incorporating speech-only, speech-text paired, and text-only data. Furthermore, to enhance transcription performance for low-resource languages, USM integrates language-specific adapters and employs Noisy Student Training (NST) techniques (Xie et al. 2020; Park et al. 2020).

MMS (Pratap et al. 2024), a state-of-the-art multilingual ASR model, employed a Connectionist Temporal Classification (CTC) based approach (Graves et al. 2006) on a dataset covering over 1,000 languages. It utilizes a two-stage finetuning pipeline. The first stage involves Romanization-based fine-tuning to learn a global representation across diverse languages. In the second stage, language-specific adapters and heads are added to capture detailed features for each language and fine-tuned.

# Zero-Shot ASR

ASR-2K (Li et al. 2022a) is a zero-shot ASR model which utilizes three universal models to cover a range of languages: an acoustic model (Li et al. 2020), a pronunciation model (Li et al. 2022b), and a LM (Scannell 2007). This suggests the potential for a universal multilingual ASR model capable of functioning in a zero-shot environment without relying on any language-specific components. Consequently, ZeroShot MMS (Zhao, Pratap, and Auli 2024) utilized languagespecific lexicon and n-gram LMs in the decoding phase to enhance zero-shot transcription performance.

# Multilingual ASR

Initially, multilingual ASR models handled a limited number of languages (Toshniwal et al. 2018; Pratap et al. 2020a), un

# LLM-Supported Multilingual ASR

Hu et al. 2023 trained a multilingual LLM covering 84 languages and employed a shallow fusion-based per-frame scoring to enhance transcription quality in multilingual ASR. Subsequently, Huang et al. 2024 introduced nonautoregressive per-segment scoring, which improves transcription performance and reduces the computational burden. These methods primarily leveraged the strengths of a multilingual acoustic model (USM) and achieved further accuracy by incorporating LLMs into the decoding step.

# Proposed Method

The overall structure of the proposed multilingual ASR pipeline, LAMA-UT, comprises a universal transcription generation phase and a language-specific transliteration phase, as shown in Fig 1. We produce universal transcriptions by finetuning an audio encoder with an additional classification layer. Consequently, we manually select a prompt type from a predefined dictionary and combine it with language information to generate the input prompt for the universal converter. Finally, by feeding this input prompt into the universal converter, we translate the universal transcription into language-specific ones.

# Universal Transcription Generation

Previous studies in linguistics (Ladefoged and Maddieson 1996; Clark, Yallop, and Fletcher 2007) have shown that the phonological characteristics of human speech are constrained by a limited range of sounds due to the anatomical structure of the vocal tract. Similarly, in the ASR domain, prior research (Taguchi and Chiang 2024) has empirically demonstrated that the primary obstacle in multilingual ASR is the orthographic complexity across languages. Through the integration of these two insights, we aim to unify orthographic systems across diverse languages by standardization of notations into a Latin character system. This approach establishes alignment between phonetic and orthographic features through a unified transcription system. As a result, we develop a universal transcription generator capable of producing consistent transcriptions across multilingual speech corpora, including unseen languages.

International Phonetic Alphabet. The first method for orthography unification is to use the international phonetic alphabet (IPA). IPA is a phonetic notation system that includes four elements: consonants, vowels, diacritics, and suprasegmentals. IPA can precisely transcribe pronunciations in a consistent format with a combination of the four elements. There are challenges with the IPA, especially in vocabulary mapping, and one possible solution is to treat the combination of elements as a single token (e.g., ts, dz, etc.). However, due to the vast diversity of possible combinations makes this approach difficult to implement. Conversely, treating each IPA character as a distinct token introduces another issue: characters without phonetic value must be mapped to specific frames as shown in Fig 2. Since diacritics and suprasegmentals provide detailed information about pronunciation (e.g., length, tone, and stress) but do not carry specific phonetic values, mapping them to distinct frames can introduce confusion during the training process.

![](images/ad9f87f82e2e594707fb22e70f0cc5e9c80f71d6bc3f9ed51663bd1e99f86469.jpg)  
Figure 2: Problems derived in single-token IPA recognition. Diacritic ‘:’ indicates phoneme length, which has no explicit phonetic value. Epsilon denotes a blank token in CTC.

Romanization. Romanization is an alternative method for orthography unification which involves converting text from various writing systems into Latin script. While Romanization does not preserve phonetic features as precisely as the IPA, it generally retains phonetic information. Additionally, Romanization offers several advantages over the IPA. Romanization standardizes diverse writing systems using the Latin alphabet, which is already employed by the majority of languages. In contrast, IPA requires a specific set of rules for converting the orthography of each language into its IPA representation. Thus, Romanization is more efficient as it only requires conversion for languages that do not use the Latin alphabet. Furthermore, Romanization is advantageous for LLMs, as a large portion of their training data consists of Latin characters. Given these benefits, we adopt Romanization as a method for orthography unification.

Universal Transcription Generator. Since our goal is to generate a universal transcription with unified orthography, our first approach was leveraging a wav2vec2.0- phoneme (Xu, Baevski, and Auli 2021). However, we found that directly passing phoneme tokens to the universal converter is suboptimal for transliteration, as it generates phoneme-level tokens without accounting for spacing. To address this, we shifted our focus to developing a universal transcription generator that produces character-level tokens while incorporating spacing information. In this context, Romanization provides a universal character-level orthographic representation, effectively reducing the vocabulary size to around 30 tokens compared to IPA. Since Romanization aligns with the common phonetic features preserved across languages, we are also confident in the proposed method’s strong generalization ability for languages not explicitly included in the training data. We selected wav2vec2.0-XLSR (Babu et al. 2021) with 1 billion parameters, an SSL model pre-trained on 128 languages, as the audio encoder to leverage the advantages of pre-training on a diverse set of languages. We then attach a classification layer on top and finetune both the audio encoder and the classification layer with speech and Romanized transcription pairs to generate universal transcriptions.

Table 1: Specific format of the prompt. roman refers to the predicted Romanized transcription, shots indicates generated examples sampled from the training data, pred denotes output from first prompt and lang indicates the name of the language.   

<html><body><table><tr><td>Zero-Shot</td><td>Transcribe following Romanized sentence into a {lang} sentence: {roman}.</td></tr><tr><td>Few-Shot</td><td>Here are some examples of transcribing a Romanized sentence into a {lang} sentence: {shots }. Considering the examples above,transcribe the following Romanized sentence into a {lang} sentence: {roman}.</td></tr><tr><td>Zero-Shot CoT</td><td>Transcribe the following Romanized sentence into a {lang} sentence.Think step by step: {roman}.</td></tr><tr><td>Few-Shot + Zero-Shot CoT</td><td>Here are some examples of transcribing a Romanized sentence into a {lang} sentence: {shots }. Thisiderigysep: romabove transrie the folowing Rromanizd entece intoa lag) setene.</td></tr><tr><td rowspan="2">Prompt Chaining</td><td>Transcribe the following Romanized sentence into a {lang} sentence,based on its pronunciation: {roman}.</td></tr><tr><td>Correct the typographical and spacing errors in the following {lang} sentence: {pred}.</td></tr></table></body></html>

# Language-Specific Transliteration

The next step is to revert the universal transcription, which retains phonetic features, back to its original languagespecific form. Since this process involves a text-to-text transformation, we approach it as a transliteration task. Consequently, we focused on the versatility of LLMs which excel in multilingual and multitask benchmarks due to extensive training on diverse text data. Therefore, we aim to utilize LLMs as universal converters to transform Romanized transcriptions into language-specific ones.

Prompt Generation. While LLMs have brought a tectonic shift to the NLP domain, additional techniques are still needed to fully harness their emergent abilities. In this context, prompt engineering has emerged as a field focused on crafting and refining prompts to effectively utilize LLMs across diverse applications and research areas. To maximize the performance of the inversion process, in the ablation study, we empirically investigated various prompt types: zero-shot, few-shot, zero-shot chain-of-thought (CoT), and prompt chaining, to determine which is the most appropriate for this task.

Universal Converter. We transliterate the unified Romanized transcription by leveraging LLM’s multilingual and multitask language understanding ability without finetuning. Since our approach does not require any special finetuning, the universal converter can be replaced with any superior LLMs, potentially improving the performance of our proposed pipeline in line with the rapidly advancing capabilities of LLMs. For this paper implementation, we utilize LLaMA3-8B, 4-bit quantized LLaMA3-70B (Touvron et al. 2023), and GPT-4o-mini (OpenAI 2024) as the universal converter.

# Experiments

# Dataset

FLEURS. FLEURS (Conneau et al. 2022) is a multilingual speech corpus encompassing 102 languages. It provides a relatively small amount of data per language (approximately 12 hours) while ensuring an unbiased distribution of data across the languages. Given our focus on demonstrating effective multilingual ASR with minimal data, we utilize the FLEURS and its official splits for experiments.

CommonVoice. CommonVoice (Ardila et al. 2020) is a multilingual speech dataset crowdsourced from speakers of various languages. For unseen languages, we leverage the official test split of 25 languages from CommonVoice 17.0, which offers sufficient samples for evaluation.

# Data Preprocessing

We initially applied NFKC normalization and lowercase transformation to the text transcriptions. Subsequently, we excluded samples containing parentheses or numbers from the dataset for the following reasons: parentheses and digits in transcriptions introduced ambiguity, as some enclosed phrases were pronounced while others were not, and digits had one-to-many pronunciation mappings across languages (e.g. ‘1’ can be pronounced as ‘one’, ‘eins’, ‘uno’, ‘yi’, etc.). Finally, we utilized the Python library Uroman (Hermjakob, May, and Knight 2018) to obtain Romanized transcription and Phonemizer (Bernard and Titeux 2021) for IPA transcription. For Japanese, we employed Pykakasi (TAKAHASHI 1992) due to the limitation of Uroman, which treats Japanese kanji as Chinese characters. Following these preprocessing steps, we obtained approximately 6 to 8 hours of speech-transcription paired data per language on average.

# Training Detail

We performed fine-tuning on all layers except the feature extractor for 3,000 steps with a CTC loss and a batch size of 128. We bypassed the two-stage fine-tuning pipeline from prior studies (Xu, Baevski, and Auli 2021; Pratap et al. 2024) because our distinct methodology, which used a smaller dataset, caused the divided fine-tuning approach to result in premature convergence and instability. For hyperparameters, we employed the default AdamW optimizer (Kingma and Ba 2017; Loshchilov and Hutter 2019) with a tri-stage learning rate scheduler. The warm-up, hold, and decay phases were configured to $10 \%$ , $60 \%$ , and $30 \%$ of the total training steps, respectively. We then performed a series of experiments to determine the optimal learning rate schedule within the range of 5e-6 to 5e-4. Finally, the entire training pipeline was conducted on two RTX-3090 GPUs with 24GB of VRAM each, and we leveraged gradient accumulation techniques to address memory issues.

Table 2: Comparison between two orthography unification methods. We report PER and CER for seen and unseen languages. Average values are calculated over all 102 seen and 25 unseen languages, respectively. For a fair comparison, all the model sizes are set to 300 million. $\dagger$ denotes results measured by PER, which does not allow for a strict comparison with other results.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="7">Seen (PER /CER ↓)</td><td colspan="4">Unseen (PER /CER ↓)</td></tr><tr><td>de</td><td>nl</td><td>fr</td><td>es</td><td>it</td><td>pt</td><td>avg</td><td>ia</td><td>eo</td><td>eu</td><td>avg</td></tr><tr><td>wav2vec2.0-phoneme† + n-gram LM</td><td>23.8 14.8</td><td>38.0 26.0</td><td>31.0 26.4</td><td>28.7 12.3</td><td>33.5 21.7</td><td>45.0 36.5</td><td>33.0 22.9</td><td>10.7 6.1</td><td>1</td><td>20.8 13.7</td><td>31.4 22.2</td></tr><tr><td>IPA generator (LAMA-UT) Roman generator (LAMA-UT)</td><td>10.2 7.3</td><td>10.5 9.6</td><td>9.6 12.9</td><td>4.2 4.4</td><td>4.6 3.6</td><td>10.9 7.2</td><td>14.4 11.3</td><td>29.0 14.0</td><td>32.0 20.8</td><td>36.6 30.3</td><td>35.1 32.3</td></tr></table></body></html>

<html><body><table><tr><td>Model</td><td>Data (h)</td><td>Universal</td><td>Zero-Shot</td></tr><tr><td>Whisper</td><td>680k</td><td>0</td><td>X</td></tr><tr><td>MMS-1162</td><td>122k</td><td>X</td><td>X</td></tr><tr><td>LAMA-UT</td><td>0.6k</td><td>0</td><td>0</td></tr></table></body></html>

# Inference Detail

Universal Transcription Generator. We leveraged a beam search decoder from flashlight (Kahn et al. 2022) with a beam size of 100. No additional lexicons or LMs were utilized in the decoding pipeline to maintain a universal pipeline without relying on language-specific elements.

Prompting Strategy. For the prompting strategy, we utilized language information and a subset of the training data to construct our hypothesis prompt. The specific format of the prompt employed is detailed in Table 1. In zero-shot prompting, the universal converter automatically transforms Romanized transcriptions into language-specific ones using only the Romanized transcriptions and language information. We employed zero-shot prompting to evaluate the performance of the LLM with minimal input. Few-shot prompting (Brown et al. 2020) involves providing examples to help the model generate responses to subsequent instances. We hypothesized that this approach would be particularly effective for low-resource or unseen languages by inducing in-context learning. Specifically, we randomly sampled five Romanized transcription and target transcription pairs for each few-shot example. Zero-shot CoT prompting (Kojima et al. 2022) is a technique that supports complex reasoning by inducing the decomposition of intricate tasks into detailed steps. Specifically, we appended the phrase “Let’s think step by step” to the input prompt to encourage the reasoning of the model. Prompt chaining employs a sequence of prompts, with each prompt building upon the output of the previous one, to manage complex multi-step tasks. In this aspect, we concentrated on the decomposable process of converting predicted Romanized transcriptions into languagespecific transcriptions through (i) reverse-Romanization and (ii) error correction. We considered that errors in Romanized transcriptions could propagate during transliteration to language-specific ones, potentially reducing system performance.

Table 3: Comparison of previous multi-lingual ASR models with the proposed pipeline. Data denotes the total amount of training dataset, Universal indicates that no languagespecific module is needed, and Zero-Shot denotes whether inference on unseen languages is feasible.   
Table 4: Upper bound performance of the universal converter. This upper bound is assessed by feeding ground truth Romanized transcriptions into the universal converter with zero-shot prompting.   

<html><body><table><tr><td></td><td>Universal Converter</td><td>CER↓</td><td>WER↓</td></tr><tr><td rowspan="3">Seen</td><td>LLaMA-8B</td><td>26.6</td><td>46.7</td></tr><tr><td>LLaMA-70B</td><td>15.5</td><td>35.3</td></tr><tr><td>GPT-4o-mini</td><td>7.5</td><td>18.1</td></tr><tr><td rowspan="3">Unseen</td><td>LLaMA-8B</td><td>33.0</td><td>50.2</td></tr><tr><td>LLaMA-70B</td><td>27.2</td><td>58.9</td></tr><tr><td>GPT-4o-mini</td><td>15.8</td><td>38.3</td></tr></table></body></html>

Universal Converter. Finally, we required the output of the universal converter to conform to a specific format. We instruct the model to enclose the output within three backticks (e.g., ‘‘‘), which allows us to isolate and sort only the language-specific transcription from the output of the model. We set the temperature value to 0.0 for all LLMs to obtain deterministic results.

# Results

Table 3 shows that LAMA-UT effectively achieves multilingual ASR with a universal model. This approach even operates in a zero-shot environment without requiring languagespecific modules while utilizing only a minimal amount of training data. In the subsequent results, we aim to validate the performance of each component within the pipeline.

# Universal Transcription Generator

Comparison to Baseline Model. We conducted a performance comparison between our universal transcription generator and the existing baseline, wav2vec2.0-phoneme (Xu, Baevski, and Auli 2021). Our universal transcription generator focuses on generating character-level tokens and is measured using Character Error Rate (CER), while the baseline wav2vec2.0-phoneme is measured using Phoneme Error Rate (PER). However, since both metrics are fundamentally used for estimating phonetic symbols, this comparison can be considered meaningful. Following the Table 2, the results show that the proposed method demonstrated significantly better performance across a broader range of languages compared to existing approaches even not utilizing language-specific modules (e.g. n-gram LM). Furthermore, our pipeline demonstrated relatively strong transcription capabilities for unseen languages that were not explicitly included in the training data. In conclusion, transcribing diverse languages based on their pronunciation can produce a universal transcription which is highly effective.

<html><body><table><tr><td rowspan="2">Resource</td><td rowspan="2">Lang.</td><td colspan="3">Whisper-large-v3</td><td colspan="3">MMS-1162</td><td colspan="3">LAMA-UT</td></tr><tr><td>Data (h)</td><td>CER↓</td><td>WER↓</td><td>Data (h)</td><td>CER↓</td><td>WER↓</td><td>Data (h)</td><td>CER↓</td><td>WER↓</td></tr><tr><td rowspan="3">High</td><td>es</td><td>11000</td><td>1.2</td><td>3.1</td><td>2969</td><td>1.6</td><td>5.8</td><td>6.1</td><td>2.8</td><td>7.3</td></tr><tr><td>it</td><td>2585</td><td>0.5</td><td>1.6</td><td>1566</td><td>1.2</td><td>5.2</td><td>6.8</td><td>2.0</td><td>5.2</td></tr><tr><td>id</td><td>1014</td><td>1.4</td><td>5.7</td><td>71</td><td>2.9</td><td>14.2</td><td>6.8</td><td>4.2</td><td>11.5</td></tr><tr><td rowspan="3">Middle</td><td>ta</td><td>136</td><td>18.3</td><td>26.7</td><td>265</td><td>11.0</td><td>41.5</td><td>6.3</td><td>19.5</td><td>31.9</td></tr><tr><td>ur</td><td>104</td><td>30.9</td><td>65.0</td><td>57</td><td>9.0</td><td>29.0</td><td>4.9</td><td>14.9</td><td>31.9</td></tr><tr><td>sk</td><td>90</td><td>2.9</td><td>8.7</td><td>301</td><td>2.2</td><td>8.8</td><td>4.5</td><td>3.8</td><td>10.2</td></tr><tr><td rowspan="3">Low</td><td>mk</td><td>16</td><td>10.3</td><td>26.3</td><td>45</td><td>1.5</td><td>8.1</td><td>5.1</td><td>5.5</td><td>17.2</td></tr><tr><td>hi</td><td>12</td><td>35.9</td><td>43.3</td><td>57</td><td>5.8</td><td>19.6</td><td>5.0</td><td>8.2</td><td>15.0</td></tr><tr><td>kk</td><td>12</td><td>8.5</td><td>35.1</td><td>46</td><td>2.8</td><td>15.2</td><td>8.1</td><td>6.7</td><td>22.9</td></tr><tr><td colspan="2">Average</td><td></td><td>23.9</td><td>42.9</td><td></td><td>7.8</td><td>28.8</td><td></td><td>14.8</td><td>33.2</td></tr></table></body></html>

![](images/36107683df37995cbc62f5546128a4e3ab61ac63773f048c86d3b95eca3cf3ce.jpg)  
Table 5: Comparison results with the baseline models. Average CER and WER have reported over 82 languages from FLEURS that are covered by Whisper, MMS, and our method. The classification of the amount of resources is based on the volume of training data used by Whisper. We utilized MMS which encompasses 1162 languages, trained on a combined dataset from MMS-lab, FLEURS, CommonVoice, Voxpopuli (Wang et al. 2021), and MLS (Pratap et al. 2020b).   
Figure 3: CER comparison between LAMA-UT and Whisper

Orthography Unification Methods. Among the two methods for standardizing orthographic features, Romanization proved to be more effective than IPA. Its ability to represent pronunciation across languages while reducing complexity makes it a superior choice for meaningful results. Romanization balances phonetic accuracy with simplicity, providing better alignment with LLMs and ensuring efficient processing across multilingual ASR tasks. However, since these results are constrained to the first phase, we have constructed the end-to-end performance comparison between IPA-based and Romanization-based LAMA-UT, and the results are shown in the appendices1.

# Universal Converter Verification

Despite the effectiveness of orthography unification, the success of the entire pipeline hinges on the proper functioning of the universal converter. Therefore, the most critical aspect to validate before experimentation was whether a frozen LLM could effectively serve as a universal converter. To validate this objective, we passed ground truth Romanized transcriptions into the frozen universal converter and assessed its performance. This approach not only tests the converter’s capability to accurately produce language-specific transcriptions but also serves to evaluate the upper bound performance of the universal converter within the proposed ASR pipeline. In Table 4, results demonstrated that universal transcription based on pronunciation characteristics can yield significant performance improvements compared to previous works when the universal transcription generator operates ideally. However, the upper bound performance for unseen languages showed a slight decrease compared to seen languages. This decrease is likely because the unseen languages we tested are typically extremely low-resource languages within the training data of the LLM.

# Overall Pipeline

Seen Languages. We leveraged two baseline models for comparison: Whisper and MMS. In Table 5, results demonstrate that the proposed method achieved a relative reduction of $60 \%$ in CER and $30 \%$ in WER compared to Whisper. Moreover, LAMA-UT matches the performance of MMS despite the absence of language-specific adapters, heads, and n-gram LMs. Notably, the performance improvements were most pronounced for low-resource languages. While Whisper exhibited increased error rates for these languages due to limited training data, our method showed substantial performance enhancements with minimal data resources. Despite the slight performance degradation in high-resource languages, the improvement observed in low-resource languages is remarkably meaningful. The full comparison results are presented in Fig 3. It is noteworthy that these results were achieved with considerably smaller training data compared to Whisper and MMS.

Table 6: Effects of prompting strategy and model type on the universal converter. The repetition rate indicates the proportion of samples with format errors (e.g., no section enclosed in three backticks until the maximum token limit) due to word repetition.   

<html><body><table><tr><td rowspan="3">Model</td><td rowspan="3">Reate tion</td><td colspan="10">Prompting Strategy</td></tr><tr><td colspan="3">Zero-Shot</td><td colspan="2">Few-Shot (5)</td><td colspan="2">Zero-Shot CoT</td><td colspan="2">Eew-Shot C)+</td></tr><tr><td>CER↓</td><td>WER↓</td><td>CER↓</td><td>WER↓</td><td>CER↓</td><td>WER↓</td><td>CER↓ WER↓</td><td>CER↓</td><td>Praimngtg WER ↓</td></tr><tr><td>LLaMA-8B</td><td>12</td><td>35.1</td><td>70.6</td><td>22.7</td><td>49.6</td><td>37.2</td><td>77.4</td><td>22.1</td><td>49.8</td><td>35.9</td><td>70.8</td></tr><tr><td>LLaMA-70B</td><td>1</td><td>24.3</td><td>53.8</td><td>17.4</td><td>43.8</td><td>25.4</td><td>54.6</td><td>16.8</td><td>43.7</td><td>26.7</td><td>55.2</td></tr><tr><td>GPT-40-mini</td><td>0.2</td><td>16.6</td><td>39.3</td><td>15.3</td><td>37.2</td><td>18.2</td><td>41.0</td><td>15.7</td><td>37.9</td><td>16.9</td><td>38.7</td></tr></table></body></html>

Table 7: Comparison with previous zero-shot approaches. We evaluated transcription quality on 25 unseen languages from the CommonVoice 17.0 dataset. # Lang. denotes the number of languages leveraged in training.   

<html><body><table><tr><td>Model</td><td>Data (h)</td><td># Lang.</td><td>Universal</td><td>CER↓</td></tr><tr><td>ASR-2K</td><td>2k</td><td>8</td><td>0</td><td>65.5</td></tr><tr><td>LAMA-UT (Roman)</td><td>0.6k</td><td>102</td><td>0</td><td>34.7</td></tr><tr><td>MMS-ZS</td><td>40k</td><td>1078</td><td>X</td><td>29.2</td></tr><tr><td>+ n-gram LM</td><td>40k</td><td>1078</td><td>X</td><td>25.2</td></tr></table></body></html>

Unseen Languages. Our main focus was developing a generalized pipeline that demonstrates strong performance with unseen languages. To validate this objective, we utilized two zero-shot ASR models as baselines: ASR-2K and Zero-Shot MMS (MMS-ZS). In Table 7, our method demonstrated a reduction in CER by half while using significantly less training data compared to ASR-2K. Furthermore, it is noteworthy that our proposed pipeline performs remarkably well even without language-specific modules, demonstrating comparable performance to MMS-ZS which leverages language-specific lexicon and n-gram LM.

# Ablation Study

Prompting Strategy. In Table 6, few-shot prompting showed the highest performance across all models and prompting strategies. Interestingly, even with zero-shot prompting, the proposed pipeline consistently outperforms Whisper on average in CER and WER, where Whisper records $23 . 9 \%$ and $4 2 . 9 \%$ respectively, as shown in Table 5.

On the other hand, the use of sequential reasoning failed to achieve the anticipated improvements. Specifically, we observed considerable error propagation when utilizing zeroshot CoT prompts and prompt chaining techniques. Minor inaccuracies in the Romanization phase were amplified as they were processed by the LLM, leading to transcriptions that deviated in meaning from the intended output.

Model Size and Training Data. From the perspective of model size, using a relatively smaller LLM like LLaMA8B frequently resulted in issues such as word repetition, which complicated the transcription sorting process. Additionally, this model faced challenges with language misprediction, often generating transcriptions in languages other than the intended target language. This issue was particularly noticeable with low-resource languages such as Arabic. With the LLaMA-70B model, while word repetition was less pronounced compared to the LLaMA-8B model, the issue of language misprediction persisted, albeit at a reduced frequency. Among the LLMs tested, GPT-4o-mini demonstrated the best performance overall. It outperformed the other models across all prompting strategies, achieving an impressive average CER of $1 5 \%$ across 102 languages.

# Conclusion

In this paper, we introduced a generalized multilingual ASR pipeline LAMA-UT, that operates effectively without relying on language-specific modules. By utilizing Romanized transcription as a unified representation across languages, we structured the multilingual ASR pipeline into two phases. Initially, Romanization aligns phonetic and orthographic features, allowing the universal transcription to be effectively generalized across diverse languages and trained efficiently with a smaller dataset. Subsequently, we used a frozen LLM to convert the universal transcription into language-specific ones. This inversion process showed remarkable performance across languages, including those not previously encountered. Our experiments demonstrated that the proposed method not only maintains performance for high-resource languages but also significantly outperforms existing methods for low-resource languages, all while effectively handling unseen languages. Furthermore, our approach matched the performance of models that employ language-specific modules, despite not using any such components. We anticipate that this research will provide a viable alternative for utilizing LLMs to support universal multilingual ASR systems across a variety of applications.