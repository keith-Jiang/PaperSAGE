# ELLA-V: Stable Neural Codec Language Modeling with Alignment-Guided Sequence Reordering

Yakun Song1, Zhuo Chen2, Xiaofei Wang3, Ziyang $\mathbf { M } \mathbf { a } ^ { 1 }$ , Xie Chen1\*

1Shanghai Jiao Tong University, Shanghai, China 2ByteDance Inc., USA   
3Microsoft, One Microsoft Way, Redmond, USA ereboas, chenxie95 @sjtu.edu.cn

# Abstract

The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms baselines in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies.

# Demo & Code — https://ereboas.github.io/ELLAV/

# 1 Introduction

Recently, deep generative AI has achieved remarkable results in various tasks, leading to the emergence of many transformative real-world applications (Brown et al. 2020; Ramesh et al. 2022; Ho, Jain, and Abbeel 2020). There have been rapid developments in the field of speech synthesis as well. In particular, zero-shot TTS technology has gained increasing attention because it can synthesize highquality target voices without the need of specified speaker’s training data. As a state-of-the-art generative model family, diffusion models (Sohl-Dickstein et al. 2015; Ho, Jain, and Abbeel 2020; Song and Ermon 2020) progressively add noise to the training data and then learn the reverse process to generate samples. By leveraging diffusion models and their variants (Sohl-Dickstein et al. 2015; Ho, Jain, and Abbeel 2020; Lipman et al. 2023), many works have successfully applied them to the audio domain (Shen et al. 2023;

Le et al. 2023; Kim et al. 2024; Vyas et al. 2023). However, they may also face the limitation in training an additional duration predictor. Another major class of generative models is language modeling based on Transformer (Vaswani et al. 2017a). AR language models use a decoder-only architecture to predict the next token in a sequence as the training objective, which has demonstrated extremely powerful few-shot and zero-shot capabilities in many generative tasks (Brown et al. 2020; Thoppilan et al. 2022; Chowdhery et al. 2023). In light of this, VALL-E (Wang et al. 2023a) and subsequent works (Kharitonov et al. 2023; Rubenstein et al. 2023; Wang et al. 2023b) have successfully employed decoder-only language model for zero-shot TTS. These approaches first quantize the speech signal into a series of discrete acoustic tokens. Subsequently, they employ an AR language model to predict coarse-grained acoustic tokens, eliminating the necessity for explicit duration predictors or speaker encoders. Once trained on a large-scale corpus, these approaches are capable of synthesizing speech with competitive fidelity and naturalness in a zero-shot manner.

While VALL-E and its variants have achieved numerous impressive milestones, they still possess certain limitations that impact practical deployment. For instance, existing methods (Wang et al. $2 0 2 3 \mathrm { a }$ ; Kharitonov et al. 2023) directly concatenate phoneme tokens and acoustic tokens as a whole sequence to train language models. In this way, the alignment between audio and phoneme sequences is completely learned through the self-attention in the transformer, making it potentially unstable as self-attention does not explicitly capture the monotonic alignment between audio and phoneme. Additionally, the decoder-only language model architecture can lead to potential attention degradation issues (Fu et al. 2023), where the alignment quality between the target audio sequence and the source phoneme sequence deteriorates as the generated sequence increases, resulting in inaccurate or low-quality speech outputs.

Another limitation stems from the nature of AR language modeling. Specifically, given a sequence x, the standard AR language model factorizes the likelihood $p ( \mathbf { x } )$ over the dimensions of $\mathbf { x }$ via the chain rule $\begin{array} { r } { p ( \mathbf { x } ) = \prod _ { t = 0 } ^ { T } p ( x _ { t } | \mathbf { x } _ { < t } ) } \end{array}$ . AR models predict the current tokens so ely based on the historical tokens without users’ control in the inference process, and sometimes generate semantic repetitions or incoherence in the generated output (Yang et al. 2019; Brown et al. 2020). In the TTS task, correspondingly, VALL-E cannot directly determine which segment of the output audio corresponds to which prompt phoneme, thus there is no trivial way to promptly detect and prevent issues occurring in the generation process. These drawbacks can manifest as meaningless phoneme repetitions, transpositions, omissions, or even catastrophic infinite silence, i.e., during the process of generation, the model anomalously outputs silence or noise tokens for an extended period of time without stopping. Specifically, Table 1 demonstrates the word error rate (WER) and the probability of the infinite silence in VALL-E samples at different threshold top- $p$ for nuclear sampling (Holtzman et al. 2019). The detailed experimental setup is described in Section 4. Notably, a shift in the decoding strategy of VALL-E from fully sampling-based to fully greedy-based leads to a marked decline in sample quality. It should be emphasized that while sampling-based stochastic decoding strategies have advantages in terms of synthesis diversity, deterministic decoding strategies (e.g., beam search and its variants) are more suitable for cases where there is less tolerance for synthesis errors and more emphasis on fluency and coherence (Ippolito et al. 2019).

Table 1: Comparison of VALL-E’s zero-shot TTS performance across various top- $p$ thresholds in nuclear sampling. $\mathrm { I N F } \%$ denotes the probability of infinite silence, which refers to instances where generation continues without stopping when its duration exceeds twice the original length.   

<html><body><table><tr><td>Top-p</td><td>WER%</td><td>INF%</td></tr><tr><td>1</td><td>5.47</td><td>0.00</td></tr><tr><td>0.99</td><td>5.00</td><td>0.20</td></tr><tr><td>0.95</td><td>10.99</td><td>19.06</td></tr><tr><td>0.9</td><td>20.85</td><td>41.43</td></tr><tr><td>0.7</td><td>37.71</td><td>76.76</td></tr><tr><td>0.4</td><td>46.59</td><td>84.39</td></tr><tr><td>0.0 (greedy)</td><td>49.26</td><td>87.29</td></tr></table></body></html>

Faced with the pros and cons of the existing methods, we introduce ELLA-V, a simple but effective language model approach for zero-shot TTS. ELLA-V proposes a generalized AR (GAR) language model to generate the first layer of residual vector quantizer (RVQ) codes of a neural codec model. Then as with VALL-E, ELLA-V employs a nonautoregressive (NAR) language model to obtain codes of the other RVQs. Our core innovation lies in 3 fold:

• Firstly, ELLA-V inserts phone tokens into the corresponding positions of the acoustic sequence. Unlike existing methods, Connecting phoneme tokens with their corresponding acoustic tokens can help the language model capture the alignment between phoneme and acoustic modalities in local dependencies. • Secondly, instead of maximizing the expected loglikelihood of the hybrid sequence under a conventional casual mask, ELLA-V computes loss only on acoustic tokens and special tokens. This training objective provides a natural way to have fine-grained control in inference: ELLA-V’s GAR model always maintains awareness of the phoneme it is currently synthesizing, allowing it to promptly detect and truncate any abnormal phoneme to avoid any possible infinite silence issue.

• Thirdly, we further propose an improvement to the input sequence. We introduce local advance, which involves shifting the EOP token and the next-word phoneme token a few frames ahead.By advancing these special tokens, the GAR model can better utilize local dependencies to predict the pronunciation of the current phoneme.

Experimental results, using comparable model configurations and 960 hours of speech data from LibriSpeech (Panayotov et al. 2015) as a training set, demonstrate the superiority of ELLA-V. Compared to the cuttingedge zero-shot TTS baseline systems, ELLA-V significantly improves the accuracy of synthesized speech, and demonstrates comparable or superior speaker similarity and speech naturalness on a series of subjective and objective experiments. Notably, ELLA-V works well on a wide spectrum of decoding strategies – even greedy decoding, and still has a substantially better speech accuracy than the best of VALL-E. We further conducted ablation experiments to investigate the effects of our proposed modifications. The results indicate that the global advance in ELLA-V significantly improves the model’s performance, while the local advance enhances the stability of the generated output.

# 2 Related Work

speech synthesis Speech synthesis has long been a significant topic in the fields of natural language processing, and speech processing. Early methods were based on Statistical Parametric Speech Synthesis (Zen, Tokuda, and Black 2009), typically involving complex components such as text analysis models, acoustic models, and vocoders. Later, endto-end neural TTS models were introduced, which synthesize Mel spectrograms and employ a vocoder (Oord et al. 2017) for speech synthesis (Wang et al. 2017). Some methods, utilizing techniques such as VAE (Hsu et al. 2019; Lee, Shin, and Jung 2022), flow (Miao et al. 2020; Kim et al. 2020), diffusion (Jeong et al. 2021; Vyas et al. 2023), and others (Wu and Shi 2022), have achieved promising performance in end-to-end speech synthesis, although explicit duration predictors or speaker embedders are usually necessary. On the other hand, models like VALLE (Wang et al. 2023a) and AudioLM (Borsos et al. 2023a) utilize autoregressive Transformers to model discrete audio tokens, achieving great in-context learning performance. When it comes to zero-shot speech synthesis, autoregressive Transformer-based models can predict and generate audio without the need for an additional duration model, which strikes a favorable balance between efficiency and performance, and has been garnering increasing attention.

# 3 Method

# 3.1 Overview

Fig. 1 demonstrates the overall architecture of ELLA-V. ELLA-V primarily follows a two-stage framework similar to VALL-E, considering zero-shot TTS as a conditional codec language modeling task. ELLA-V maps input text prompts and speech prompts into a unified vocabulary space with a text encoder and a neural codec, respectively. Different from VALL-E, an additional sequence order rearranging step is performed to the text-audio token sequence, after which, ELLA-V utilizes a decoder-only language model to learn to perform conditional generation on the hybrid sequences of phoneme and audio tokens. Detailed information about the language model will be presented in Section 3.2.

![](images/d6add8d7cf545f23a0dc65913f57c023b6a8eea9ef00a59f45fe019a9077346a.jpg)  
Figure 1: The overall architecture of ELLA-V. Input an audio prompts and text prompts, ELLA-V first changes sequence order – sandwiching each phoneme’s audio $\left. \mathbf { C } _ { k } \right.$ : between the $k$ -th phoneme and a EOP token and prepending the phoneme sequence to the beginning. By learning on the mixed sequence, ELLA-V can generate audio sequence of the text prompts while maintaining the acoustic and environmental conditions of the audio prompts.

To obtain discrete audio representations, we employ a pretrained neural audio codec model, EnCodec (D´efossez et al. 2023), following VALL-E (Wang et al. 2023a). EnCodec transforms $2 4 ~ \mathrm { k H z }$ raw waveforms into $7 5 ~ \mathrm { H z }$ discrete tokens using $L$ RVQ layers. In our experiments, we use the same settings as VALL-E, with $L = 8$ and the codebook size is 1024. In this setting, each second of the waveform is represented by $7 5 \times 8$ discrete tokens from RVQ.

To obtain phoneme sequences, we apply the Montreal Forced Aligner (MFA) (McAuliffe et al. 2017) to the input audio. Notably, MFA not only serves as a text tokenizer but also extracts alignment relationships between phonemes and the corresponding speech. The forced alignment information is essential for ELLA-V to change sequence order. In Section 3.2, we will provide a detailed explanation of how this information is used to construct the target sequence.

# 3.2 Training: Codec Language Model

ELLA-V employs a Generalized Autoregressive Codec language model for the prediction of the first quantization layer in the EnCodec, which corresponds to capturing semantic information and coarse-grained acoustic profiles. Subsequently, a non-autoregressive language model is utilized to generate codes for the subsequent quantization layers, aimed at reconstructing fine-grained acoustic details. Specifically, given a speech corpus $\bar { \mathcal { D } } = \{ \mathbf { x } _ { i } , \mathbf { y } _ { i } \}$ , where $\mathbf { x } _ { i }$ represents the $i$ -th audio sample, and $\mathbf { y } _ { i }$ is its text transcription. We utilize the EnCodec to extract the discrete representation of $\mathbf { x }$ , formulated as $\mathbf { C } ^ { T \times 8 } = \mathrm { E n C o d e c } ( \mathbf { x } )$ , where $\mathbf { C }$ represents the two-dimensional acoustic code matrix, and $T$ is the downsampled utterance length.

We employ MFA to obtain the phoneme sequence ${ \mathbf { P } } _ { 1 : n }$ corresponding to the transcription $\mathbf { y }$ , while also extracting forced alignment information between the audio $\mathbf { x }$ and the transcription y: $( { \bf P } _ { 1 : n } , l _ { 1 : n } ) = \mathrm { M F A } ( { \bf x } , { \bf y } )$ , where $n$ is the number of phonemes of the audio sample $\mathbf { x }$ , and $l _ { i }$ denotes the length of the $i$ -th phoneme of the discrete audio sequence. MFA treats silence also as a kind of phoneme, so that the original audio sequence is partitioned into $n$ consecutive intervals corresponding to $n$ phonemes. Specifically, let ⟨Ci⟩li×8 re present the audio sequence corresponding to the $i$ -th phoneme, $\mathbf { C }$ is the concatenation of $\left. \mathbf { C } _ { i } \right.$ , and we have ⟨Ck⟩1:lk = C ik=−1 li+1: ik=1 li

After quantization, we utilize the EnCodec decoder to reconstruct the audio waveform from the discrete acoustic sequence $\mathbf { C }$ , formulated as $\hat { \mathbf { x } } \approx \mathrm { D e C o d e c } ( \mathbf { C } )$ .

For the zero-shot TTS task, the optimization objective is $\operatorname* { m a x } p ( \mathbf { C } | \mathbf { P } , \hat { \mathbf { C } } )$ , where $\hat { \mathbf { C } }$ is the acoustic prompt of the unseen speaker. We use language modeling to generate acoustic tokens, by learning on the mixed sequence composed of phonemes and codec codes, consistent with previous works (Wang et al. $2 0 2 3 \mathrm { a }$ ; Rubenstein et al. 2023).

Unlike existing approaches, ELLA-V does not concatenate phoneme tokens and acoustic tokens directly to form the target sequence for training the language model. Instead, ELLA-V interleaves phoneme and acoustic tokens in order to make it easier for language models to learn the alignment between audio and text. Specifically, we insert each phoneme token $P _ { i }$ (except the silence phoneme) into the corresponding position of the audio sequence, so that each phoneme’s audio $\left. \mathbf { C } _ { i } \right.$ is sandwiched between $P _ { i }$ and EOP tokens. We also prepend the phoneme sequence to the beginning of the mixed sequence, which is referred to as global advance. In Section 3.4, we further propose a variant sequence order with higher generation stability, named local advance, which moves the non-acoustic tokens of the sequence several frames forward.

![](images/c2bc48f05755cab520ae19afa9597b094aa3564400524d2d6057369627643972.jpg)  
Figure 2: The illustration of Generalized Autoregressive language model of ELLA-V.

Generalized Autoregressive (GAR) Codec Language Model As shown in Figure 2, ELLA-V first constructs a hybrid sequence ${ \bf { H } } _ { : , 1 }$ of acoustic and phoneme tokens, structured as: $[ P _ { 1 }$ , $P _ { 2 }$ , . . . , $P _ { n }$ , BOS, $P _ { 1 }$ , $\langle \mathbf { C } _ { 1 } \rangle _ { : , 1 }$ , EOP, , $P _ { n }$ , $\langle \mathbf { C } _ { n } \rangle _ { : , 1 }$ , EOP, EOS]. It is worth noting that the MFA (Montreal Forced Aligner) treats silence as a distinct phoneme, whereas our phoneme sequence $\mathbf { P }$ exclusively comprises phonemes other than silence. To clarify, we retain the acoustic component associated with silence but do not sandwich it with an EOP and a specific silence phoneme, nor do we use a silence phoneme in the global advance part.

We design a GAR language model to learn the continuation task on the hybrid sequence, to generate the discrete acoustic code sequence $\mathbf { C } _ { : , 1 }$ . The GAR model consists of multiple Transformer decoder layers (Vaswani et al. 2017b). After training, it can generate discrete audio codes for a specified text prompt and acoustic prompt. GAR is also responsible for predicting EOP and EOS to indicate the conclusion of a phoneme and the entire sentence, respectively.

The optimization of GAR is achieved by maximizing the likelihood of the acoustic part $\mathbf { C } _ { : , 1 }$ of the hybrid sequence ${ \bf { H } } _ { : , 1 }$ , as well as the special EOP and EOS tokens. Under forward factorization, this process is formulated as:

$$
\begin{array} { r l } & { \displaystyle \begin{array} { c } { \displaystyle \operatorname* { m a x } _ { \theta \in A ^ { R } } \log p ( \tilde { \mathbf { C } } _ { : , 1 } \mid \mathbf { P } ; \theta _ { G A R } ) } \\ { \displaystyle } \\ { \displaystyle = \sum _ { i = 1 } ^ { n } \displaystyle \sum _ { t = 0 } ^ { l _ { i } } \log p \big ( \big \langle \tilde { \mathbf { C } } _ { i } \big \rangle _ { t , 1 } \big \vert \big \langle \tilde { \mathbf { C } } _ { i } \big \rangle _ { < t , 1 } , \big \langle \tilde { \mathbf { C } } _ { < i } \big \rangle _ { ; , 1 } , } \\ { \displaystyle \mathbf { P } ; \theta _ { G A R } \big ) } \\ { \displaystyle = \sum _ { t = 0 } ^ { T _ { H } } \log p \big ( \mathbf { H } _ { t , 1 } \big \vert \mathbf { H } _ { < t , 1 } ; \theta _ { G A R } \big ) } \\ { \displaystyle \mathbf { H } _ { t , 1 } \ncong \mathrm { g n o s } } \end{array} } \end{array}
$$

where $\mathbf { H }$ has a size of $T _ { H } \times 8$ , $\{ \mathbf { P } \}$ denotes the phoneme set, $\langle \tilde { \bf C } _ { i } \rangle$ is the concatenation of $\left. \mathbf { C } _ { i } \right.$ along with its broadcast trailing EOP and/or EOS tokens, $\tilde { \mathbf { C } }$ is then the concatenation of $\left. \mathbf { C } _ { i } \right.$ , and $\theta _ { G A R }$ represents neural network parameters of GAR model. The factorization of the training objective naturally encapsulates the core intuition of the GAR model: GAR generates the audio sequence phoneme-by-phoneme. GAR produces maximum likelihood predictions for each phoneme token successively, indicating the end of generating a specified phoneme by predicting EOP. Through global advancement, GAR can directly infer the next phoneme to be generated without relying on network predictions. After the prediction for the last phoneme is completed, GAR stops the generation process by predicting EOS. The generated sequence by GAR is self-aligned, as it can instantly know the corresponding position of any generated acoustic token in relation to the phoneme prompt.

During training, we apply a bidirectional mask to the phoneme sequence before the BOS in the hybrid sequence, while a unidirectional mask is used for the part after BOS. We frame the training as a next-token-prediction language modeling task on the hybrid sequence. However, it’s important to note that the model does not predict phonemes (or BOS). As shown in Figure 2, we only compute loss when the token to be predicted is not a phoneme (or BOS). During inference, after the model predicts an EOP for a phoneme, the next phoneme token is directly appended to the end of the sequence, which will be further discussed in Section 4.

Non-Autoregressive (NAR) Codec Language Model In the second stage, the NAR language model is employed to predict the codes from the second to the last quantization layers in parallel. The input-output sequence construction of the NAR model follows the same pattern as used in the GAR model. Specifically, the $i$ -th column $\mathbf { H } _ { : , i }$ of the hybrid sequence matrix $\mathbf { H }$ is structured as: $[ P _ { 1 }$ , $P _ { 2 }$ , . . , $P _ { n }$ , BOS, $P _ { 1 }$ , $\left. \mathbf { C } _ { 1 } \right. _ { : , i }$ , EOP, . . . , $P _ { n }$ , $\langle \mathbf { C } _ { n } \rangle _ { : , i }$ , EOP, EOS].

And in practice if $P _ { i }$ represents the silence, $\mathbf { C } _ { : , i }$ will not be sandwiched by $P _ { i }$ and $\mathtt { E O P }$ .

The NAR model takes the previously generated hybrid sequence of the previous $j - 1$ layers as input and predicts the codes of the $j$ -th layer in parallel, formulated as:

$$
\begin{array} { r l } { \displaystyle } & { \displaystyle \operatorname* { m a x } _ { \theta _ { N A R } } \ \sum _ { j = 2 } ^ { 8 } \log p ( { \mathbf { C } } _ { : , j } \mid { \mathbf { H } } _ { : , < j } , { \mathbf { P } } ; \theta _ { N A R } ) } \\ & { = \displaystyle \sum _ { j = 2 } ^ { 8 } \sum _ { t = 0 } ^ { T _ { H } } \ \log p ( { \mathbf { H } } _ { t , j } \mid { \mathbf { H } } _ { : , < j } , { \mathbf { P } } ; \theta _ { N A R } ) } \end{array}
$$

where $\{ \mathbf { C } _ { : , j } \}$ denotes the acoustic token set of the $j$ -th quantizer. The embeddings of tokens from the previous $j - 1$ quantizers are summed up to feed the NAR model to predict the $j$ -th layer.

# 3.3 Inference

ELLA-V can use a short clip of speech from an unseen speaker as an acoustic prompt to synthesize speech for a specified text prompt. Figure 3 illustrates the inference process of the GAR model. While VALL-E may get stuck in an infinite loop during inference, resulting in the synthesis of

$$
\begin{array} { r l } & { \frac { \bigodot } { P _ { 1 } } \frac { \bigodot } { P _ { 2 } } \cdots \underbrace { \bigodot } _ { P _ { n } \mathtt { b o s } } \frac { \bigodot } { P _ { 1 } } \frac { \bigodot } { \langle C _ { 1 } \rangle _ { , 1 } } \underbrace { \bigodot } _ { \mathsf { b o p } } \cdots \underbrace { \bigodot } _ { P _ { k } } \underbrace { \bigodot } _ { \langle C _ { k } \rangle _ { _ { 1 } \mathtt { t } , 1 } } \underbrace { \bigodot } _ { \langle C _ { k } \rangle _ { _ { 1 } + 1 , 1 } } } \\ & { \frac { \bigodot } { P _ { 1 } } \frac { \bigodot } { P _ { 2 } } \cdots \underbrace { \bigodot } _ { P _ { n } \mathtt { b o s } } \frac { \bigodot } { P _ { 1 } } \frac { \bigodot } { \langle C _ { 1 } \rangle _ { , 1 } } \underbrace { \bigodot } _ { \mathsf { b o p } } \cdots \underbrace { \bigodot } _ { P _ { k } } \underbrace { \bigodot } _ { \mathsf { F } \mathrm { s ~ e s o n d s } } \underbrace { \bigodot } _ { \mathsf { E o p } } } \\ & { \frac { \bigodot } { P _ { 1 } } \frac { \bigodot } { P _ { 2 } } \cdots \underbrace { \bigodot } _ { P _ { n } \mathtt { b o s } } \frac { \bigodot } { P _ { 1 } } \frac { \bigodot } { \langle C _ { 1 } \rangle _ { , 1 } } \underbrace { \bigodot } _ { \mathsf { E o p } } \cdots \underbrace { \bigodot } _ { P _ { k } } \cdots \underbrace { \bigodot } _ { \langle C _ { k } \rangle _ { , 1 } } \underbrace { \bigodot } _ { \mathsf { E o p } } \underbrace { \bigcirc } _ { \bigcirc } \underbrace { \bigodot } _ { P _ { k + 1 } } } \end{array}
$$

Figure 3: Illustration of the inference process of ELLA-V.

$$
\begin{array} { r l } &  \underbrace { \mathsf { A d v a n c e } } _ { \overbrace { P _ { 1 } } } \underbrace { \mathsf { A d v a n c e } } _ { \overbrace { P _ { 2 } } } \underbrace { \mathsf { A d v a n c e } } _  \overbrace { P _ { n } } \underbrace { \mathsf { B d o s } } _ { \overbrace { P _ { 1 } } } \underbrace { \overbrace { C } \overbrace { C _ { 1 } } \overbrace { C _ { 1 } } \overbrace { C _ { 2 } } \overbrace { P _ { 2 } } \overbrace { C _ { 2 } } \overbrace { C _ { 3 } } \overbrace { P _ { 3 } } \cdots } _ { \underbrace { 4 d v \ t o k e l s } } \underbrace { \mathsf { A d v a n c e } } _ { \overbrace { C _ { 3 } } \overbrace { C _ { 2 } } \overbrace { C _ { 3 } } \overbrace { C _ { 3 } } \overbrace { C _ { 3 } } \cdots } \underbrace { \mathsf { A d v a n c e } } _ { \underbrace { 4 \overbrace { C _ { 1 } } \overbrace { C _ { 3 } } \overbrace { C _ { 3 } } \overbrace { C _ { 3 } } \cdots } } \underbrace { \mathsf { A d v a n c e } } _  \overbrace  C _ { 2 } \overbrace { C _ { 3 } } \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace  C _ { 3 } \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace  C _ { 3 } \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace  C _ { 3 } \overbrace { C _ { 3 } } \cdots \overbrace { C _ { 3 } } \cdots \overbrace  C  \end{array}
$$

Figure 4: Local advance. A phoneme can locally have access to information about the next phoneme token advanced by $A d v$ frames, allowing it to anticipate the upcoming phoneme token’s characteristics.

either infinite silence or repetitive pronunciation, ELLA-V is capable of generating EOP and promptly truncating abnormally long phonemes. Following an EOP, we can directly append the next phoneme token to the end of the generated sequence, ensuring the proper generation of speech without abnormal pauses or repetitions. For the GAR model, we employ a sampling-based decoding strategy, whereas for the NAR model, we use a greedy decoding approach to strike a balance between efficiency and performance.

# 3.4 Local Advance

One intuition is that the pronunciation of a phoneme is strongly related to the phonemes around it. However, due to the autoregressive nature of the GAR model, an acoustic token cannot attend to the following phoneme tokens, even though we can leverage the transformer’s ability to model long-term dependencies through global advance to provide complete context for the acoustic token generation. To further harness the powerful capability of the transformer in modeling local dependencies, ELLA-V introduces an additional change in the sequence order based on Section 3.2. Specifically, we move the phoneme token and the EOP token ahead by a few frames, referred to as local advance.

# 4 Experiments

# 4.1 Experimental Setup

Data & Tasks: We trained ELLA-V using the publicly available Librispeech (Panayotov et al. 2015) 960h training dataset. We utilized Montreal Forced Aligner (MFA)1 (McAuliffe et al. 2017) to obtain forced alignment information for the audio-transcription pairs. Sentences with unrecognized or unknown phones by MFA were excluded. The open-source $2 4 \mathrm { { k H z } }$ checkpoint2 of EnCodec(De´fossez et al. 2023) was used as the codec to generate discrete acoustic tokens. The LibriSpeech training data was upsampled to $2 4 \mathrm { k H z }$ before feeding it into EnCodec.

In evaluating the model, two zero-shot TTS tasks were considered. For the zero-shot TTS continuation task, we adhered to methodologies established by previous works (Wang et al. 2023a,c), selecting examples ranging from 4 seconds to 10 seconds from the LibriSpeech testclean dataset as our test set. In this task, we used the complete phoneme transcription as the text prompt and the first 3 seconds of the test audio sample as the acoustic prompt. The model was required to generate continuations.

For the zero-shot TTS cross-speaker task, we designed a hard case set comprising 100 hard sentences. They included challenging phonetic patterns, alliteration, and unusual (abnormal) combinations of words that might pose difficulties for a TTS system to generate natural-sounding speech. In this case, we randomly picked 3-second sentences from the LibriSpeech test-clean subset as the acoustic prompt. We then concatenated the transcription of this segment and the target phoneme sequence in the hard case set to form the text prompt. The model was tasked with cloning the voice of the speaker to say the specified target text in the hard case set.

Training Configuration: For both GAR and NAR models, we stacked 12 Transformer decoder layers with an embedding dimension of 1024, a hidden state dimension of 1024, and a feed-forward layer dimension of 4096. All models were trained in parallel using 8 NVIDIA Tesla V100 GPUs with a batch size of 16384 tokens for GAR and 12288 tokens for NAR per GPU, respectively, learning a total of $3 2 0 \mathrm { k }$ steps. We used the AdamW optimizer with $\beta _ { 1 } = 0 . 9$ , $\beta _ { 2 } = 0 . \bar { 9 } 9 9$ , $\epsilon = 1 0 ^ { - 9 }$ . We employed an inverse-sqrt learning rate scheduler with warm-up. For the first 32000 updates, we linearly increased the learning rate from $1 0 ^ { - 7 }$ to a peak of $5 \times 1 0 ^ { - 4 }$ . The weight decay was 0.01.

Baseline: In our research, we benchmarked the performance of zero-shot speech synthesis against VALL-E (Wang et al. 2023a). VALL-E was originally trained on a substantial $6 0 \mathrm { k }$ hours of audio from the Librilight dataset (Kahn et al. 2020). To ensure a rigorous evaluation, we reproduced the VALL-E model and adapted it to train on the LibriSpeech 960h dataset. We also adjusted the model dimensions and the number of layers to match the parameter settings of ELLAV and VALL-E. Both GAR (or AR) and NAR models of VALL-E and ELLA-V have 154.3M parameters. Moreover, to mitigate any potential bias introduced by the audio codec, we pre-processed the authentic speech samples using EnCodec’s encoder and decoder. We include the result for Encodec reconstructed speech for reference, denoted as Ground-Truth Encodec. Two cutting-edge Non-AR-based TTS models were also included for comparison with our model: YourTTS (Casanova et al. 2022) trained on the 585- hour LibriSpeech subset LibriTTS (Zen et al. 2019), and

Table 2: Main results of ELLA-V on zero-shot TTS continuation task. † indicates that ground-truth audios were passed through the encoder and decoder of Encodec to evaluate the influence of neural audio codec. The ASR model was used to obtain transcriptions of synthetic speech to calculate the WER. C-T denotes Conformer-Transducer. Since it is not open-sourced and has the same evaluation settings as this article, SoundStorm’s WER was derived directly from the original text.   

<html><body><table><tr><td>Models</td><td>WER(%) (↓)</td><td>SPK (↑)</td><td>CMOS</td><td>SMOS</td><td>Training Set</td><td>ASR Model</td></tr><tr><td>Ground Truth</td><td>1.41</td><td>0.923</td><td>0.29</td><td>4.39</td><td>/</td><td>C-T</td></tr><tr><td>Ground Truth-Encodec†</td><td>1.62</td><td>0.913</td><td>0.22</td><td>4.33</td><td>/</td><td>C-T</td></tr><tr><td>SoundStorm</td><td>2.99</td><td>/</td><td>/</td><td>/</td><td>LibriLight (60k hours)</td><td>C-T</td></tr><tr><td>YourTTS</td><td>6.34</td><td>0.822</td><td>/</td><td>/</td><td>LibriTTS (585 hours)</td><td>C-T</td></tr><tr><td>VALL-E</td><td>5.00</td><td>0.868</td><td>0.00</td><td>3.56</td><td>LibriSpeech (960 hours)</td><td>C-T</td></tr><tr><td>ELLA-V(ours)</td><td>2.28</td><td>0.870</td><td>0.10</td><td>3.56</td><td>LibriSpeech (960 hours)</td><td>C-T</td></tr></table></body></html>

Table 3: WER comparison on 100 particularly hard synthesis cases. Sub, Del, and Ins refer to Substitution, Deletion, and Insertion error rates, respectively.   

<html><body><table><tr><td>Models</td><td>WER(%)</td><td>Sub(%)</td><td>Del(%)</td><td>Ins(%)</td></tr><tr><td>VALL-E</td><td>28.39</td><td>17.79</td><td>5.36</td><td>5.24</td></tr><tr><td>YourTTS</td><td>17.61</td><td>11.00</td><td>5.04</td><td>1.57</td></tr><tr><td>ELLA-V</td><td>12.79</td><td>7.76</td><td>3.40</td><td>1.63</td></tr></table></body></html>

SoundStorm (Borsos et al. 2023b) trained on LibriLight.

Evaluation Metrics: We evaluated our system with several objective metrics. Speaker similarity (SPK) and WER served as our primary measures. SPK was assessed using the fine-tuned WavLM-TDNN model3 (Chen et al. 2022), scoring similarity on a scale of -1 to 1, with values above 0.86 indicate the same speaker identity (This value comes from the release model card page). The WER was determined by comparing the synthesized speech to the original text using the Conformer-Transducer model4 (Gulati et al. 2020).

In addition to these standard metrics, we introduced two novel measures: $\mathrm { I N F } \%$ and $C \mathrm { U T } \%$ . $\mathrm { I N F } \%$ quantified the frequency of generating infinitely long audio, indicative of a failure in synthesis. It is used to measure the likelihood of the model falling into abnormal repetition (such as infinite silence). A higher $\mathrm { I N F } \%$ indicates poorer stability in the generated output of the model. In the practical implementation, $\mathrm { I N F \% }$ referred to the proportion of sentences for which generation was not stopped when the length of the generated audio reached twice the original, serving as a proxy for infinite generation. On the other hand, as discussed in the previous session, the design of ELLA-V enables control of the duration for each phoneme during inference, thus avoiding the synthesis failure. In our experiments, we forcibly truncate the synthesis of phonemes with a length greater than 0.4 seconds. $C \mathrm { U T } \%$ is used to measure the frequency of forced cuts of phonemes in synthesis by ELLA-V. For each objective metric, we reported average values over three experimental runs with different random seeds.

Table 4: The ablation study to investigate the impact of global and local phoneme information.   

<html><body><table><tr><td>Models</td><td>WER(%) (↓)</td><td>SPK (↑)</td></tr><tr><td>VALL-E</td><td>5.00</td><td>0.868</td></tr><tr><td>ELLA-V</td><td>2.28</td><td>0.870</td></tr><tr><td>ELLA-V-noglobal</td><td>5.00</td><td>0.859</td></tr><tr><td>ELLA-V-nophn</td><td>3.51</td><td>0.868</td></tr></table></body></html>

For subjective analysis, we relied on the mean opinion score (MOS). 30 test samples were chosen for this purpose, with each sample being evaluated by at least 15 listeners for aspects like naturalness and speaker similarity. The comparative mean option score (CMOS) and the similarity mean option score (SMOS) were the key subjective metrics used. SMOS was rated on a 1 to 5 scale, in 0.5-point increments, to gauge speaker similarity, while CMOS, ranging from -1 to 1, assessed the overall naturalness and quality of the synthesized speech against the baseline.

# 4.2 Results

Zero-Shot TTS Continuation Task. We present the evaluation results in Table 2. First, regarding speaker similarity, both subjective (SMOS) and objective (SPK) results revealed that ELLA-V and VALL-E performed similarly and surpassed baseline Non-AR-based methods, which can be attributed to their shared backbone approach, combining (G)AR and NAR. Meanwhile, CMOS testing shows that ELLA-V achieved a $+ 0 . 1 0$ score, demonstrating a higher generation quality (i.e., naturalness) compared to VALL-E. Additionally, WERs calculated between the recognized text of synthesized audio and the ground-truth text show that ELLA-V is significantly better than VALL-E (2.28 versus 5.00), and better than SoundStorm (2.28 versus 2.99) trained on the dataset over 60 times larger. This underscores ELLAV’s enhanced capability in synthesizing higher-quality and more robust speech. Overall, ELLA-V substantially improved the synthesis accuracy and robustness of the language model-based TTS framework without affecting the naturalness and speaker similarity. This conclusion is not only corroborated by this easy continuation task, but also validated via the challenging sets in the subsequent section.

![](images/6db0f9de8a155fae97e1509f42190afd31b667deee628f8eab2d08f401344395.jpg)  
Figure 5: Ablations on decoding strategies. The left figure demonstrates the trends of WER for VALL-E, and ELLA-Vwith different local advance, with respect to the variations in top p in nuclear sampling. The right figure shows trends of INF for VALL-E, and CUT for ELLA-Vwith different local advance, with respect to the variations in top p in nuclear sampling.

Zero-Shot TTS Cross-Speaker Task on Hard Cases. VALL-E utilized a traditional AR model that frequently resulted in alignment errors, including repetitions, transpositions, and omissions, particularly in more challenging synthesis cases (details of the challenging synthesis set can be found in supplementary material). Table 3 presents the WER results on the 100 particularly hard synthesis sentences. In contrast to the baselines, ELLA-V demonstrates markedly lower WER, signifying its enhanced robustness. This substantial reduction in errors translates to more accurate and reliable voice synthesis applications, significantly improving user experience in real-world scenarios.

Regarding VALL-E’s tendency to fall into infinite silence, an intuitive explanation is that the silence patterns in the training data are relatively simple and many of them are repetitive. In this case, a traditional language model is prone to overfitting to these patterns. During testing, when the model encounters silence, it assigns a high probability to silence. This leads to issues such as beam search, which is based on maximum likelihood, getting stuck in a loop. However, ELLA-V does not face this problem.

Analysis of Decoding Strategies and Local Advance. To demonstrate the stability of ELLA-V under different decoding strategies, we conducted an ablation study, testing the decoding performance with different top- $p$ values for nuclear sampling, by varying $p$ . The experiments was performed under local advance values of 0, 5, and 10. The results are shown in Figure 5. We can observe that as top $p$ decreases, the accuracy of VALL-E’s synthesized speech significantly decreases. At this point, VALL-E is more prone to generating a large number of overfit silence tokens, leading to a significant increase in $\mathrm { I N F } \%$ . And compared to VALLE, the audio synthesized by ELLA-V is less sensitive to rate changes in the top $p$ sampling strategy, whose WER consistently outperforms VALL-E. When the local advance is set to 5 or 10 tokens, the generated audio exhibits significant stronger robustness. On the other hand, as shown in Figure

5 (right), as top $p$ decreases, VALL-E tends to get stuck in infinite loops of failed generation, while the generation of ELLA-V remains significantly stable. Moreover, ELLA-V can promptly handle (truncate) the synthesis of exceptional phonemes, resulting in significantly higher robustness.

Ablation Study. In this paragraph, we conduct ablation experiments. (1) To investigate the impact of global phoneme information on synthesized speech, we removed the global phoneme sequence at the beginning of the trained sequence (abbr. ELLA-V-noglobal). (2) To investigate whether it is necessary to provide the specific phoneme token before its corresponding acoustic tokens during both training and inference, rather than just using the EOP separator, we removed all phoneme tokens following BOS in the mixed sequence (abbr. ELLA-V-nophn). The experimental results are shown in Table 4. It is observed that the accuracy of synthesized speech significantly deteriorated either when global phoneme tokens were not used or when local phoneme tokens were disabled within the hybrid sequence. It is also notable that even in the absence of global advance (i.e., in the ELLA-V-noglobal configuration), the SPK and WER of the synthesized audio were comparable to those of VALL-E. These findings indicate the importance of both local and global information in achieving more accurate synthesized audios, meanwhile, combining both of them potentially leads to further enhancements in accuracy.

# 5 Conclusion

In this paper, we introduce ELLA-V, a simple and efficient two-stage zero-shot TTS framework based on language modeling. By learning interleaved sequences of acoustic and text tokens, our proposed GAR model can provide finegrained control over synthesized audio at the phoneme level and can better leverage local dependencies to predict the pronunciation of the current phoneme. Experimental results demonstrate that ELLA-V achieves higher accuracy and more stable results under different threshold top- $p$ for nuclear sampling. We aspire for this work to advance research in enhancing the robustness of speech generation.