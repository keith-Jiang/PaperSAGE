# BindGPT: A Scalable Framework for 3D Molecular Design via Language Modeling and Reinforcement Learning

Artem Zholus1,3, 4,5\*, Maksim Kuznetsov1, Roman Schutski2, Rim Shayakhmetov1, Daniil Polykovskiy1, Sarath Chandar3,4, 5, 6 and Alex Zhavoronkov2

1 Insilico Medicine Canada Inc.   
2 Insilico Medicine AI Limited   
3 Mila – Quebec AI Institute   
4 Polytechnique Montre´al   
5 Chandar Research Lab   
6 CIFAR AI Chair

# Abstract

Generating novel active molecules for a given protein is an extremely challenging task for generative models that requires an understanding of the complex physical interactions between the molecule and its environment. This paper presents a novel generative model, BindGPT, which uses a conceptually simple but powerful approach to create 3D molecules within the protein’s binding site. Our model produces molecular graphs and conformations jointly, eliminating the need for an extra graph reconstruction step. We pre-train BindGPT on a largescale dataset and fine-tune it with reinforcement learning using scores from external simulation software. We demonstrate how a single pre-trained language model can serve at the same time as a 3D molecular generative model, a conformer generator conditioned on the molecular graph, and a pocket-conditioned 3D molecule generator. Notably, the model does not make any representational equivariance assumptions about the domain of generation. We show how such a simple conceptual approach combined with pre-training and scaling can perform on par or better than the current best-specialized diffusion models, language models, and graph neural networks while being two orders of magnitude cheaper to sample.

Website — https://bindgpt.github.io/

# 1 Introduction

The drug discovery landscape presents immense challenges and risks, demanding substantial investments of time and resources to design, test, and deliver new medicines to the market. Within this context, Computer-Aided Drug Design (CADD) (Yu and MacKerell 2017) stands as a pivotal methodology, harnessing software screenings and physical simulations to facilitate a more efficient exploration of the vast space of drug-like molecules, estimated to be around $1 0 ^ { 6 0 }$ in size (Polishchuk, Madzhidov, and Varnek 2013). Deep learning advancements have revolutionized this exploration by leveraging neural generative models trained on extensive compound datasets. Notably, the textual representation of molecular structures using SMILES (Weininger 1988) has enabled the utilization of Language Models for the generation of novel, drug-like molecular compounds (Bagal et al. 2022).

Recent research has demonstrated the capability of deep generative models to generate novel molecular compounds directly in 3D, with the flexibility to incorporate protein pocket and ligand subfragment conditions. Among these, diffusion models such as EDM (Hoogeboom et al. 2022) and DiffDock (Corso et al. 2023) initiate the generation process with an arbitrary spatial distribution of atoms and progressively refine their positions to yield physically viable molecular structures. Meanwhile, autoregressive models like Pocket2Mol (Peng et al. 2022) sequentially predict the type and location of each successive atom, building upon the existing molecular framework. Additionally, work by Flam-Shepherd and Aspuru-Guzik (2023) has highlighted the proficiency of language models in handling spatial representations of molecular and protein structures through formats like XYZ, CIF, and PDB. However, it’s noteworthy that most spatial molecular generators focus exclusively on atom types and locations. They depend on supplementary tools, such as OpenBabel (O’Boyle et al. 2011), for the critical task of bond reconstruction. This reliance can introduce vulnerabilities, as the precision required for atom placement means that minor positional adjustments can significantly alter reconstructed molecular bonds or even make the molecular graph disconnected.

In this work, we introduce a novel framework that applies language modeling to the domain of 3D molecular data represented by textual tokens. This entirely data-driven approach, devoid of any inductive biases at both the model and representation levels, capitalizes on the established GPT paradigm, integrating cutting-edge techniques to enhance the scalability of model training and inference. By adopting the language model pre-training paradigm, our framework showcases the ability to foster a powerful causal language model adept at navigating the complex space of 3D molecules. This proficiency is demonstrated through successful applications in downstream tasks, including learning the distribution of molecules without conditions and conformations for a molecular graph, and, crucially, by generating specific molecules

27M pocket-ligands 200M Conformers 32B tokens 90B tokens (non-unique) 100 pockets Reinforcement Pretraining Finetuning Learning 1 <POCKET>NCACOCCCC. <LIGAND>OCc1cc2c(.. OCCCNCNNNCACOCCCN. <XYZ> 3D molecule gen.; <-X4Y.Z9>91 131.519 LanguCaguesalModel Zero-shot $2 \mathrm { D }  3 \mathrm { D }$ mPolcekceut-lecognednietiroatnieodn 4.794 6.134 -0.021 1.033 3.067 2.185 -5.773 2.950 0.599 0.112 ConfGen 0.121 -1.334 3.936 1.504 0.219 0.229 71.017374 -17.343650 -25.010995 -0.578348 0.841441 0.642764 100M parameters 2.384 -5.084 0.318 -1.115 -0.477 -0.489 -5.272 -7.393 -5.431 -0.143 -1.014 -1.304 7.391 -1.954 0.092 .165 -0.678 -1.197 Embed 3D Embed 3D molecule pocket to text to text

and conformations with targeted binding affinity to a protein. Our main contributions are the following:

• We introduce BindGPT, a Language Model for handling spatial molecular structures in text format. It uses structural SMILES and spatial XYZ formats to describe molecular graphs and atom locations, eliminating the dependency on external software for graph reconstruction.   
• We propose a scalable pre-training and fine-tuning method for drug discovery in 3D that covers several 3D molecular generation tasks in a single paradigm.   
• We show how BindGPT can create accurate and realistic 3D molecular structures both zero-shot and after finetuning, with the option to include molecular graphs or protein pocket descriptions as prompts. The method offers comparable generation quality to leading approaches with a speedup of up to 100x.   
• Finally, we demonstrate the effectiveness of the Reinforcement Learning framework to fine-tune BindGPT with external feedback from docking software. We show that the resulting model can find structures with high binding scores for any given protein as a result of the RL fine-tuning.

# 2 Background

Molecule Generation Small drug-like molecules can be represented as 2D or 3D graphs with node and edge attributes. However, one of the most popular molecular representations in the machine learning community is SMILES (Weininger 1988), which can be seen as a compressed textual encoding of the Depth-First-Search applied to the molecular graph. Its simplicity and expressive power made it work very well with language models - even a simple LSTM (Hochreiter and Schmidhuber 1997) model can outperform graph-neural networks for the molecule generation task (Flam-Shepherd, Zhu, and Aspuru-Guzik 2022).

The biological function of small molecules arises through their binding to specific protein pockets. The spatial structure of the protein pocket is an essential domain knowledge to increase the efficiency of molecular generation in drug design tasks. With the increase of molecular structure datasets sizes (Francoeur et al. 2020; Hu et al. 2005) a plethora of pocketconditioned generators emerged (Peng et al. 2022; Luo et al. 2021; Lin et al. 2022; Corso et al. 2023). The challenge with pocket-conditioned molecular generation arises from a relatively small size of existing 3D binding poses datasets, which motivated a heavy use of specialized architectures, like SE(3) equivariant neural networks (Hoogeboom et al. 2022).

Molecular Generative Models in 3D. Apart from Language Models, there are other types of models for molecule generation, including 3D-aware approaches. The first group of works uses Diffusion Models, which employ the denoising diffusion process (Ho, Jain, and Abbeel 2020) to learn to recover the data from noise. The second group of works relies on Graph Neural Networks (GNNs) to autoregressively build 2D or 3D molecular graphs. These approaches can be combined as GNNs can serve as efficient backbones for the diffusion process once they are node-equivariant (Niu et al. 2020) (to generate 2D graphs) or SE(3)-equivariant (Peng et al. 2023a) (to generate 3D graphs). SBDD (Luo et al. 2021) model uses autoregressive graph generation for pocket-conditioned molecule generation. TargetDiff (Schneuing et al. 2023) generalizes this model to use a diffusionGNN for the same task. Pocket2Mol (Peng et al. 2022) uses an informed autoregressive sampling mechanism for efficient pocket-conditioned molecule generation. Another batch of works uses the aforementioned methods for unconditional molecule generation. EDM (Hoogeboom et al. 2022) proposes an E(3) equivariant diffusion model for molecule generation. MolDiff (Peng et al. 2023a) is a diffusion model that addresses the inconsistency problem between generated atoms and bonds connecting them.

Language Models for Drug Discovery. The language models show outstanding results in the drug discovery domain. The molecular structures can be easily represented in textual formats like SMILES (Flam-Shepherd, Zhu, and Aspuru-Guzik 2022), enabling the effective training of wellknown language model architectures on large datasets of chemical entities. Recent studies reveal the potential of applying language models to address various challenges in drug discovery. For instance, LigGPT (Bagal et al. 2022) leverages the GPT (Radford et al. 2018) architecture to generate molecular structures given the conditions of molecular descriptors. MoLFormer (Katharopoulos et al. 2020) incorporates a billion-size chemical entities database to perform large-scale pre-train and further fine-tune to predict molecular properties. BARTSmiles (Chilingaryan et al. 2022) is developed atop of BART (Lewis et al. 2019) architecture, training a meaningful chemical representation and refining it for tasks such as chemical property prediction, chemical reaction prediction, and retrosynthesis.

However, the challenge of 3D molecule generation has received limited attention in the language model approach. Three notable studies in this area include the XYZtransformer (Flam-Shepherd and Aspuru-Guzik 2023), UniMol (Zhou et al. 2023), and Lingo3DMol (Feng et al. 2024). The XYZ-transformer leverages GPT to generate atom-wise descriptions of molecular and protein structures. Uni-Mol modifies BERT for large-scale pre-training on a large 3D structures dataset. In particular, Uni-Mol formulates molecular tasks as coordinates-to-coordinates mapping given molecular graph. To the best of our knowledge, we propose the first approach that applies the modern decoderonly language modeling paradigm that uses pertaining, fine-tuning, and reinforcement learning to the 3D small molecule generation with several downstream applications.1

# 3 Method

The key idea of our method is utilizing an autoregressive token generation model, influenced by GPT-based models, to solve several 3D small molecule generation tasks in one simple yet flexible paradigm. The main principle in our approach is to formulate several 3D molecular design tasks as prompted generation of text. To achieve that, we position the tokens of a condition before the tokens of the object to generate. For instance, a prompt can be the protein pocket for the pocket-conditioned generation task or the 2D molecular structure for the conformation generation task.

# 3.1 Architecture and Text Format

In our work, we follow the decoder-only paradigm in Language Models and use the GPT-NeoX2 architecture (Black et al. 2022) that utilizes rotary position embeddings (Su et al. 2021). This technique allows for length generalization, which is required since the sequence lengths may vary significantly between the pre-training and fine-tuning stages.

Similarly to (Flam-Shepherd and Aspuru-Guzik 2023), we use the XYZ representation as a base format to describe the spatial atom allocation. The idea of XYZ format is to represent the atom type and its 3D coordinates within every line of text. The main drawback of this format is the lack of charge and connectivity information. One should use external software like RDKit or OpenBabel (O’Boyle et al. 2011) to reconstruct the molecular graph. It introduces instability since even small noise in atom positions can drastically change the reconstructed graph or even break it down (Peng et al. 2023b). To alleviate that, we propose to couple the XYZ format with the SMILES format. The latter can efficiently represent the molecular structure, while the former allows for describing atom positions. To align these two formats we enforce to have the same atom ordering in both. We also remove the atom symbol from the XYZ representation as it already was shown in SMILES. For proteins, there is no need to describe their connectivity, therefore we simply write atom names grouped by amino-acids.

A schematic example of the two kinds of model input is shown in Figure 2 and a very detailed example of concrete input sequences (including its tokenization) is shown in Appendix A. In particular, the sequence starts with the <LIGAND $>$ token followed by a SMILES string, tokenized at the character level. Next, there goes the ${ \tt { < X Y Z > } }$ special token marking the end of SMILES and the beginning of the coordinate part of the string. The tokenization strategy uses 6 tokens per 3D position: we use one token for the integer part and one token for the fractional part of the number. When working with protein pockets, we use a similar strategy. Specifically, the sequence begins with the ${ \mathrm { < P O C K E T > } }$ token followed by the sequence of atoms where each atom is a separate token. Since pockets can be hundreds of atoms large, we follow the AlphaFold’s (Jumper et al. 2021) approach and retain only the 3D coordinates of the Alpha-carbon atoms in the corresponding amino acids.

# 3.2 Pre-Training

In this work, we aim to leverage insights accumulated by the NLP community in the paradigm of Large Language Models: pre-training and fine-tuning, prompting, scaling, fine-tuning with Reinforcement Learning, tool use, etc.(Hoffmann et al. 2022; Radford et al. 2019). Since our model covers only a specialized domain of molecular tasks, it does not require trillion-scale diverse datasets for good performance as NLP tasks do. Thus, we use a large-scale but specialized dataset of 3D molecules and protein pockets. During pre-training, we use the model with 108M parameters consisting of 15 layers, 12 heads, and a hidden size of 768. We found this size of the model to be enough for the tasks we care about - generating molecules in 3D (See Appendix C for a justification of the size). Every sequence in the training batch is either a ligand sequence of tokens or a pocket sequence of tokens following the scheme described earlier. Since the dataset has much fewer pockets than ligands, for one epoch of training

atom 3D pocket   
<POCKET> <XYZ> tokens coords 3D ligand   
<LIGAND> SMILES <XYZ> coords Mixture of pockets and ligands

on ligands, we do 5 epochs of training on proteins, that is, around $8 \%$ of all tokens seen by the model are pocket tokens. To speed up and stabilize pre-training, we use large batch training with $1 . 6 \mathbf { M }$ tokens per one training step. We found that having many tokens per batch is important for stable training in this task, even with smaller learning rates. A detailed description of the training implementation is provided in Appendix G. Despite the wide use of transformers in drug discovery, the majority of current works in this space do not use recent advancements of efficient Language Models pretraining: neither technical ones, such as Flash-attention (Dao 2023) or DeepSpeed (Rasley et al. 2020), nor the algorithmic ones, such as learning rate scaling. Our work aims to fill this gap by demonstrating the effectiveness of the pre-training for 3D drug discovery.

# 3.3 Fine-Tuning

Supervised Fine-Tuning (SFT) As a result of the pretraining, BindGPT gains an understanding of a broad chemical space. This comprehensive understanding enables us to efficiently narrow it down through the supervised fine-tuning on a specialized dataset. During the supervised fine-tuning phase, we continue model training on CrossDocked 2020 (Francoeur et al. 2020), which is a high-quality dataset containing aligned pocket-ligand pairs. Most of the prior methods subsample less than $1 \%$ of the best pocket-ligand pairs and they do not benefit from its diversity and scale. To obtain a bigger version of CrossDocked, we extract all intermediate ligand poses (with respect to the docking process), including the lower-quality ones. Despite quite large size, CrossDocked was created by docking 14k unique molecules into $3 \mathrm { k }$ pockets (Francoeur et al. 2020). This is why we observed a dramatic overfitting when training on the $1 \%$ version of CrossDocked and even on the full one. To alleviate that, we resort to two standard augmentation techniques used in drug discovery. First, we employ SMILES randomization (Bjerrum 2017), which can heavily randomize one molecule by yielding 100- 1000 different SMILES strings (all corresponding to that molecule). Second, we randomly rotate the 3D coordinates of the protein pocket and of the ligand (with the same rotation matrix). This way our model learns to understand structural and spatial properties of molecular binding beyond just token sequences.

Context (non-trainable) atom 3D pocket   
<POCKET> <XYZ> tokens tokens 1 3D ligand   
<LIGAND> SMILES <XYZ> tokens Pocket-conditioned finetuning   
Context (non-trainable) atom 3D pocket   
<POCKET> tokens <XYZ> tokens scalar   
<SCORE> reward 中 3D ligand   
<LIGAND> SMILES <XYZ> tokens   
Pocket and Reward conditioned finetuning

Since the pre-trained BindGPT is trained on both ligands (starting from the ${ < } \mathrm { L I G A N D > }$ token) and pockets (starting from the $< \mathrm { P O C K E T } >$ token), the information about the structure of both is learned by the model. In our fine-tuning setup, we represent each pocket-ligand pair as a string starting with the pocket string representation followed by the string representation of the ligand (See Section 3.1 for their description). Therefore, having learned them separately during pretraining, the fine-tuning exploits the independent knowledge of both pockets and ligands to learn a conditional dependency between them. In addition to that, since our version of CrossDocked contains both high and low score conformations, we test another version of the context where we condition on the pocket and binding energy score obtained from the CrossDocked dataset (which is originally computed through the docking software (Trott and Olson 2010; Eberhardt et al. 2021)). This way we can perform a variant of contrastive learning by learning the structure of good and bad examples. During an evaluation of the model, we can sample molecules conditioned on some desired value of the binding affinity. The input layout for both versions is shown in Figure 3.

Reinforcement Learning Despite the ubiquitous use of Reinforcement Learning (RL) for language models in Drug Discovery (see Section 2 and Appendix G), we did not find it being used within the pre-training paradigm of modern LLMs (Hoffmann et al. 2022; Ouyang et al. 2022). Our main motivation to use RL after the pertaining and fine-tuning stages is to use the knowledge distilled into the model from massive amounts of less structured data. We believe, this is the first work performing reinforcement learning on small molecules that utilizes knowledge from previous pretraining and supervised fine-tuning. Despite there being dozens of works doing RL with LMs on molecules, none of them do that within the LLM paradigm and none of them consider target-conditioned RL problem. In our opinion, the latter is primarily due to pocket-conditioned generation is not possible without large-scale pre-training as we show in the experimental section.

Table 1: Generative metrics for the molecule generation task after the pre-training. (H) indicates that explicit hydrogens are generated with molecules. For XYZ-TF, the RMSD calculation algorithm failed to converge.   

<html><body><table><tr><td></td><td colspan="2">XYZ-TF</td><td colspan="2">BindGPT (Ours)</td></tr><tr><td></td><td>No-H</td><td>H</td><td>No-H</td><td>H</td></tr><tr><td>Valid (↑)</td><td>12.87%</td><td>17.86%</td><td>98.58%</td><td>77.33%</td></tr><tr><td>SA (↑)</td><td>0.21</td><td>0.54</td><td>0.77</td><td>0.78</td></tr><tr><td>QED (↑)</td><td>0.30</td><td>0.37</td><td>0.59</td><td>0.61</td></tr><tr><td>Lipinski (↑)</td><td>4.79</td><td>4.82</td><td>4.86</td><td>4.91</td></tr><tr><td>RMSD (↓)</td><td>=</td><td></td><td>0.89</td><td>3.44</td></tr><tr><td>Time,s (↓)</td><td>165</td><td>394</td><td>13</td><td>156</td></tr></table></body></html>

We apply the REINFORCE algorithm for further model fine-tuning. It allows using the feedback (called reward) from an external oracle to train the model to generate even better structures than those generated after the SFT stage. The resulting RL-fine-tuned model can generalize and produce high-affinity molecules even for the new pockets. In our procedure, on each training step, we generate the 3D structure of ligands for a batch of random protein pockets. Then, we compute the reward using an external docking software that estimates the binding energy between the pocket and the generated ligand. The final step involves updating the language model with the batch of prompts (pockets), responses (ligands), and rewards (binding energies). We initially tested PPO (Schulman et al. 2017) and REINVENT (Olivecrona et al. 2017), but found REINFORCE to be more stable for our project, which aligns with another recent finding in the field of RL applied to language models in NLP (Ahmadian et al. 2024). Also, it’s important to mention, that we apply the KL-penalty between the model’s initialized and current state to stabilize the procedure. Further details, such as specific hyperparameters, can be found in the Appendix B.

# 4 Results

In this section, we describe our experimental results. We start with a brief data description followed by the description of the three 3D molecular generative tasks: the 3D generative modeling of molecules and conformation generation given molecular graph (in Section 4.1 and Appendix H) and then pocket-conditioned generation (in Section 4.2, Appendix I).

For pre-training, we use a large 3D molecular dataset proposed by the authors of the Uni-Mol model (Zhou et al. 2023). The dataset contains 208M conformations for 12M molecules and 3.2M spatial structures of protein pockets. For finetuning in the pocket-conditioned generation task, we use the aforementioned CrossDocked dataset which contains aligned pocket-molecule pairs. Our filtration of the dataset has around

Table 2: The qualities of the generated 3D molecules after pre-training on the Uni-Mol data followed by fine-tuning on GEOM-DRUGS. Note that the main metrics here are 3D ones: BindGPT outperforms baselines in nearly all of them.   

<html><body><table><tr><td>Group</td><td>Metrics</td><td>EDM</td><td>Mol Diff</td><td>BindGPT (Ours)</td></tr><tr><td>Drugli- keness</td><td>QED (↑) SA (↑) Lipinski (↑)</td><td>0.558 0.568 4.923</td><td>0.668 0.874 4.986</td><td>0.616 0.826 4.896</td></tr><tr><td>3D structures</td><td>JS.bond lengths (↓) JS.bond angles (↓) JS.dihedral angles(↓)</td><td>0.246 0.282 0.328</td><td>0.365 0.155 0.162</td><td>0.029 0.075 0.098</td></tr><tr><td>Bonds</td><td>JS.num.bonds per atoms (↓) JS.freq. bond types (↓) JS.freq.bond pairs (↓) JS.freq.bond triplets(↓)</td><td>0.139 0.378 0.396 0.449</td><td>0.115 0.163 0.136 0.125</td><td>0.160 0.045 0.043</td></tr><tr><td>Rings</td><td>JS.num. rings (↓) JS. num. n-sized rings (↓) Num. Ints rseting</td><td>0.106 0.107 3.667</td><td>0.062 0.092 8.000</td><td>0.042 0.094 0.023 9.000</td></tr><tr><td>Speed</td><td>Time for1000valid molecules, s (↓)</td><td>1.4e6</td><td>7500</td><td>200</td></tr></table></body></html>

27M pocket-ligand pairs covering a cross product of 14k molecules with 3k pockets (not all of the pairs are present, and for some of them, there is more than one pose, each with a different score). We also hold out a set of 100 pockets from the training data for evaluating the model performance. For the tasks of 3D molecule and 3D conformer generation (Section 4.1), to make comparisons with baselines more fair, we also fine-tune the model on the GEOM-DRUGS (Axelrod and G´omez-Bombarelli 2022) dataset, with drug-like molecules having high-quality 3D molecular conformations. This dataset contains 27M conformations for 300k molecules, and it serves as a standard benchmark for machine learningbased 3D molecular generators. Finally, we use the Platinum (Friedrich et al. 2017) dataset as a hold-out evaluation dataset to test our model and baselines on zero-shot conformer generation. The Platinum dataset contains the best-in-class experimentally validated conformations for testing conformer generation software.

# 4.1 Generative Modeling of 3D Molecules

Metrics. We provide the validity ( ) of generated molecules and druglikeness metrics - SA ( ), QED ( ), and Lipinski ( ) that are agnostic to 3D but measure how likely the molecule to be a drug. Also, we adopt a range of distribution metrics that were used for the MolDiff method (Peng et al. 2023b). Those metrics measure the discrepancy between true and modeled molecular distributions by computing the Jensen–Shannon divergences on the set of molecular properties and feature distributions. We compute RMSD (Root-Mean-Squared-Distance) $( \downarrow )$ - which measures the quality of 3D structures by aligning the generated one with the one from RDkit (i.e., we regenerate conformer via RDkit) and computing the atomwise distance. Finally, we measure the time needed to generate 1K valid 3D molecules on one GPU. Note that this choice of metrics is standard for this task (see Peng et al. (2023b) for a more detailed description of them). For the 3D conformation generation given molecule task, we compute the RMSD-coverage ( ) metric. This is a standard performance metric for 3D conformer generation models (see e.g. Jing et al. (2022)). It is represented by the cumulative distribution function of RMSD between generated and reference conformers. The metric is a function of the threshold $x$ : $P ( { \mathrm { R M S D } } < x )$ . An ideal model should achieve the highest possible metric values at the lowest possible thresholds.

![](images/bf638eb6d81e480e2a19d0a60134fe53fd9cc0840a3896dc30af2f273149ea7e.jpg)  
Figure 4: (left) Sampled conformations for reference molecules from the Platinum dataset (See Appendix F for a higher resolution image) (right) RMSD Coverage metric calculated on the Platinum dataset for the 3D conformation generation task.

Baselines. For the molecule generation task, we consider the current best 3D generative models. EDM (Hoogeboom et al. 2022) and MolDiff (Peng et al. 2023b) are taskspecialized diffusion models for 3D molecule generation. XYZ-Transformer (Flam-Shepherd and Aspuru-Guzik 2023) is another 3D molecular transformer that was proposed for small-scale data. Note that XYZ-TF is the only model capable of large-scale pre-training besides our model, so we pre-train only XYZ-TF and BindGPT on the Uni-Mol data. We also do the GEOM-DRUGS evaluation, where we report MolDiff and EDM trained on the full dataset and for BindGPT fine-tuned on the same version of it. For conformer generation, we compare BindGPT with the current state-of-the-art methods, Torsional Diffusion (Jing et al. 2022) and the UniMol model (Zhou et al. 2023). The former is a specialized SE(3)-equivariant diffusion model capable of conformation generation only. The latter is a modified BERT (Devlin et al. 2019). As a coordinate-level encoder LM, the Uni-Mol model needs input coordinates to generate a conformation, which is why this model uses RDKit as a tool for initializing coordinates.

Results. The molecular generative modeling results are shown in Tables 2 and 1. First, the pre-trained BindGPT model consistently outperforms the XYZ-TF baseline both without and with explicit hydrogens. The latter is a much more challenging task, and almost no baseline methods can do that (except EDM, which is not scalable) since reconstructing hydrogens can be done on a post-processing step, but explicit modeling of them makes the molecule size several times larger. BindGPT is the first model capable of modeling hydrogen explicitly at such a large scale. Also, XYZ-TF has a very low validity rate due to the need for graph reconstruction. Next, for the methods trained on the GEOM-DRUGS dataset, BindGPT (being fine-tuned on this data) shows state-of-theart performance scores for nearly all distributional evaluation metrics. Even though BindGPT does not outperform MolDiff in Druglikeness, we argue that it does not have to - 3D distributional metrics evaluate how well the model learns 3D structures in the data, while druglikeness is not tied to any dataset and BindGPT shows the same values as after pre-training (see Table 1). For the conformation generation task, the current best baseline is Torsional Diffusion (TD) (Jing et al. 2022). We use the Platinum dataset to compare TD trained on GEOM-DRUGS with Uni-Mol-BERT and BindGPT, both of which are pre-trained and fine-tuned on the same data. Figure 4(b) shows the results for zero-shot evaluations on Platinum. Surprisingly, Uni-Mol fails to generalize to this new dataset (even assisted by RDKit), which we think is because of its structural diversity. BindGPT, in contrast, is capable of matching the performance of TD when assisted by the RDKit tool and having a small gap when not. All the above results demonstrate the generalizability of our model - none of the baselines is able to solve this wide range of tasks at this level of quality.

![](images/ca2b9b98d6fd22713ba4d8434dd7347e54c630496b3c6c9895f22a2ba7105ee5.jpg)  
Figure 5: Examples of binding poses and Vina scores (↓) for 2gns and 4d7o pockets.

Table 3: Generative metrics for the pocket-conditioned generation task. Note that the reward function optimizes the raw vina score (see Appendix B.3). This is done for a fair comparison with baselines: other metrics like SA or QED could easily be included into the reward, but baselines only optimize vina score via sampling constraints so we compare them with RL optimization.   

<html><body><table><tr><td>Method</td><td>Vina score (↓)</td><td>SA (↑)</td><td>QED (↑)</td><td>Lipinski (↑)</td></tr><tr><td>Pocket2Mol</td><td>-7.15 ± 4.89</td><td>0.75 ± 0.12</td><td>0.57 ± 0.15</td><td>4.88 ± 0.37</td></tr><tr><td>TargetDiff</td><td>-7.80 ± 3.61</td><td>0.58 ±0.12</td><td>0.48 ± 0.19</td><td>4.51 ± 0.85</td></tr><tr><td>BindGPT-FT(Ours)</td><td>-5.44 ± 2.09</td><td>0.78 ±0.10</td><td>0.50 ±0.17</td><td>4.72 ± 0.70</td></tr><tr><td>BindGPT-RFT(Ours)</td><td>-7.24 ± 1.68</td><td>0.74 ± 0.11</td><td>0.48 ±0.22</td><td>4.32 ± 1.25</td></tr><tr><td>BindGPT-RL (Ours)</td><td>-8.60 ± 1.90</td><td>0.84±0.05</td><td>0.43 ±0.17</td><td>4.81 ± 0.52</td></tr></table></body></html>

# 4.2 Pocket-Conditioned Molecule Generation

Metrics. The main metrics for this task include the measure of ligand-pocket affinity; and druglikeness of the ligand. The first one is the binding energy (↓) computed by the QVINA (Alhossary et al. 2015) docking software, while the second one comprises the druglikeness metrics (SA ( ), QED ( ), and Lipinsky ( )). For each baseline, we report the time required to generate 100 valid molecules for one pocket.

Baselines. Apart from the BindGPT model, we include the baselines such as 3D diffusion model (TargetDiff (Guan et al. 2023)) and autoregressive Graph Neural Network (Pocket2Mol (Peng et al. 2022)). Note that none of the baselines perform large-scale pre-training. Instead, they resort to heavy inductive biases to efficiently learn from small data.

Results. Performance of our approach is summarized in Table 3. We depict the performance of three versions of BindGPT. First, BindGPT-FT is a model fine-tuned on the complete CrossDocked data (the data layout as described in Figure 3, top), i.e. both good and bad binding pairs. This model serves as an initialization for the RL model. Second, BindGPT-RFT is the model fine-tuned on CrossDocked with the reward in the context. To get higher affinity molecules from that model, we condition the model on random binding energy values within $[ - 1 2 , - 1 0 ]$ , which are the best scores observed by the model (in around $0 . 1 \%$ of examples). Finally, the BindGPT-RL model is trained with RL (see Section 3.3 and Appendix B.3) from the BindGPT-FT initialization. Our main conclusion is that the RL fine-tuned model can learn to search the space of binding molecules much more efficiently and significantly outperforms all the previous best baselines in terms of binding energy.

# 5 Discussion and Conclusion

We presented BindGPT, a framework for training capable language models that can generate 3D molecules. Through a series of studies on a range of different 3D molecular tasks, we demonstrate the generality of our approach as it can solve each of them by matching or surpassing the baselines. Our method does not have any inductive biases about the domain acting as a data-driven approach. Unlike all the baselines which have inductive biases, our method solves each downstream task without any such assumptions. The task of particular interest in our work is pocket-based molecule generation, where our model outperforms all the baselines by a large margin. We show that the large-scale pre-training, fine-tuning, and RL can be used in 3D drug discovery.