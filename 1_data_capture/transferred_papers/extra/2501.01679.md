# Adaptive Few-shot Prompting for Machine Translation with Pre-trained Language Models

Lei Tang1, Jinghui $\mathbf { Q i n } ^ { 1 * }$ , Wenxuan $\mathbf { Y e } ^ { 2 }$ , Hao Tan1, Zhijing Yang

1Guangdong University of Technology 2The Chinese University of Hong Kong tangtang302958@163.com, scape $1 9 8 9 @$ gmail.com, nbvincentelite $@$ gmail.com, tanhao $4 8 6 9 @$ gmail.com, yzhj $@$ gdut.edu.cn

# Abstract

Recently, Large Language Models (LLMs) with in-context learning have demonstrated remarkable potential in handling neural machine translation. However, existing evidence shows that LLMs are prompt-sensitive and it is sub-optimal to apply the fixed prompt to any input for downstream machine translation tasks. To address this issue, we propose an adaptive few-shot prompting (AFSP) framework to automatically select suitable translation demonstrations for various source input sentences to further elicit the translation capability of an LLM for better machine translation. First, we build a translation demonstration retrieval module based on LLM’s embedding to retrieve top- $\mathbf { \nabla } \cdot \mathbf { k }$ semantic-similar translation demonstrations from aligned parallel translation corpus. Rather than using other embedding models for semantic demonstration retrieval, we build a hybrid demonstration retrieval module based on the embedding layer of the deployed LLM to build better input representation for retrieving more semantic-related translation demonstrations. Then, to ensure better semantic consistency between source inputs and target outputs, we force the deployed LLM itself to generate multiple output candidates in the target language with the help of translation demonstrations and rerank these candidates. Besides, to better evaluate the effectiveness of our AFSP framework on the latest language and extend the research boundary of neural machine translation, we construct a high-quality diplomatic Chinese-English parallel dataset that consists of 5,528 parallel Chinese-English sentences. Finally, extensive experiments on the proposed diplomatic ChineseEnglish parallel dataset and the United Nations Parallel Corpus (Chinese-English part) show the effectiveness and superiority of our proposed AFSP.

Source 链博会是“链”通世界的盛会，也是“链”接未来的盛举，展现了 Text 各方构建稳定和富有韧性的产业链供应链的共同心声。 You are a professional translator. You are a professional translator. These sentences will be displayed These sentences will be displayed below. below. Chinese text: ...各方秉持“发展为 Chinese text: ... 倡导共同构筑安 先、平等协商、务实高效、开放 全稳定、畅通高效、开放包容、 互利共赢... 包容”的澜湄精神... Translation: ... relevant parties have Translation... called for joint efforts acted in the Lancang-Mekong spirit to build a global industrial and of development first, equal supply chain system that is secure, consultation, practicality and high stable, smooth, efficient, open, efficiency, and openness and inclusive and mutually beneficial inclusiveness, . Translate the text from Chinese to After the example pairs, .. After the example pairs, ... Translate the text from Chinese to English. Prompt1 English. Prompt2 Our Adaptive Prompting Framework Output1 Output2   
Blue-4: 0.2840 6 Frozen Pre-trained LLM Blue-4: 0.2302   
Comet: 0.8097 Comet: 0.7535 The fair is a grand gathering to The CIIE is a grand event connect the world connecting the world through supply chains and connecting with ”chain“ and a grand display to connect the future the future, showcasing the with ”chain“, showcasing the common aspiration of all parties common aspiration of all parties to build a stable and resilient to build a stable and resilient industrial and supply chain. industrial chain supply chain.

# Introduction

Neural Machine Translation (NMT) (Bahdanau, Cho, and Bengio 2015), the core of which lies in the encoder-decoder architecture, aims to translate texts in the source language into the target language automatically. NMT is a challenging task since it involves translating text among different languages and requires semantic alignment between languages (Fan et al. 2021; Costa-juss\`a et al. 2022; Yuan et al. 2023). Even so, it has made remarkable progress in recent years, especially with the emergence of large language models (LLMs) like ChatGPT & GPT-4 (Ouyang et al. 2022), GLM (Du et al. 2022), Llama (Touvron et al. 2023; Dubey et al. 2024), etc. Benefiting from the increasing scale of parameters and training corpus, these LLMs have gained a universal ability to handle various NLP tasks via in-context learning (ICL) (Brown et al. 2020) or prompt engineering (Chen et al. 2023), which is the process of structuring input text with exemplars and human-written instructions for LLMs, rather than conducting costly task-specific finetuning. Unsurprisingly, LLMs with ICL or prompting techniques have shown outstanding potential in machine translation (Zhang, Haddow, and Birch 2023a; Zhu et al. 2024; Zhang et al. 2023) by constructing elaborate instruction or prompts with different prompting strategies.

Pioneering work (Zhang, Haddow, and Birch 2023a) conducted a systematic study on prompting strategies for machine translation with the testbed GLM-130B (Zeng et al. 2022), including zero-shot prompting and few-shot prompting. Coincidentally, another work (Zhang et al. 2023) evaluated 15 publicly available language models on machine translation tasks with zero-shot prompting and few-shot learning. (Zhu et al. 2024) explored the multilingual translation capabilities of eight popular LLMs, including ChatGPT and GPT-4. Although all these existing works have shown promising translation performance under the settings of both zero-shot prompting and few-shot ICL, they found that the prompt examples matter the translation performance, which means that LLMs are prompt-sensitive. Using suboptimal examples or instructions can degenerate translation. For example, as shown in Figure 1, the LLM with prompt 1 which is more related to the input text can generate a better translation result than the LLM with prompt 2 according to the BLEU and Comet. In terms of semantic consistency, we can also observe that the translation quality of the LLM with prompt 1 is higher than the LLM with prompt 2. Therefore, selecting suitable adaptive translation demonstrations to elicit the translation capability of an LLM is crucial for high-quality machine translation under in-context learning.

Choosing suitable translation demonstrations for different input text is challenging and nontrivial. To address this issue, we propose an Adaptive Few-Shot Prompting (AFSP) framework to automatically select suitable translation demonstrations for various source input sentences to further elicit the translation capability of an LLM for better machine translation. First, we build a translation demonstration retrieval module based on LLM’s embedding to retrieve top- $\mathbf { \nabla } \cdot \mathbf { k }$ semantic-similar translation demonstrations from aligned parallel translation corpus. The retrieval topk translation demonstrations will be filled into the handcrafted instruction prompt template which is used for various source sentences uniformly. These translation demonstrations are crucial in eliciting the translation capability of an LLM to generate more semantic-consistent target sentences with current input source sentences. M3-Embedding (Chen et al. 2024) shows that conducting semantic retrieval with a combination of different retrieval functionalities can achieve better retrieval performance by improving the discrimination of embeddings. Inspired by this, we construct a demonstration retrieval module based on dense embedding, sparse embedding, and multi-vector embedding to build better input representation for retrieving more semantic-related translation demonstrations. The dense embedding, sparse embedding, and multi-vector embedding of a sentence are generated from deployed LLM which is also used for machine translation. Then, we use a constructed adaptive few-show prompt to obtain the translation result in the target language. There is output diversity in an LLM (Kirk et al. 2023) due to the probabilistic sampling. Different outputs can lead to different translation quality. To mitigate semantic bias caused by LLMs’ probabilistic sampling and ensure semantic-consistent translation, we force the deployed LLM to generate multiple output candidates in the target language and rerank these candidates by a rerank model based on a small language model (SLM). Since there is no available large-scale annotated corpus about the translation quality of different translation outputs and annotating such a corpus is costly, we train the rerank model at a lower cost with a self-supervision way by negative sampling with different text perturbation. With the rerank model, we can choose better translation results, ensuring better semantic consistency between source inputs and target outputs.

Besides, Language evolves throughout time. To better evaluate the effectiveness of our AFSP framework on the latest language and extend the research boundary of neural machine translation, we construct a high-quality diplomatic Chinese-English parallel dataset that consists of 5,528 parallel Chinese-English sentences about the question answers with Chinese foreign-ministry spokesman and foreign journalists. These parallel sentences have very high semantic consistency since they are diplomatically oriented and have been rigorously vetted and proofread. Extensive experiments on our proposed diplomatic Chinese-English parallel dataset and United Nations Parallel Corpus (Chinese-English part) show that the effectiveness and superiority of our AFSP.

# Related Work

The emergence of LLMs has shown outstanding potential in the field of machine translation. Unlike traditional neural machine translation methods (Bahdanau, Cho, and Bengio 2014; Sennrich, Haddow, and Birch 2015; Wang et al. 2022) which need to be trained with a large-scale machine translation dataset, LLMs were trained on general large-scale corpus and could effectively finish downstream machine translation tasks via prompt engineering or in-context learning without extra model tuning. Current research evaluating and improving the machine translation capabilities of LLMs can be included in two lines. The first line focuses on comprehensive evaluations of LLMs under various translation scenarios, including multilingual translation (Jiao et al. 2023; Hendy et al. 2023), document-level translation (Wang et al. 2023; Hendy et al. 2023), low-resource translation (Jiao et al. 2023; Bawden and Yvon 2023), etc. Another line focuses on designing novel mechanisms to improve the machine translation capabilities of LLMs, including the design of prompt templates (Zhang, Haddow, and Birch 2023b; Jiao et al. 2023), demonstration selection for in-context learning (Zhang, Haddow, and Birch 2023b; Vilar et al. 2022; Garc´ıa et al. 2023; Yao et al. 2023; Merx et al. 2024; Jiang and Zhang 2024a), self-refinement (Feng et al. 2024b,a), agentic workflow (Wu et al. 2024; Guo et al. 2024), etc.

Among these research lines, the most relevant to our work is the demonstration selection. (Vilar et al. 2022) investigated various strategies for choosing translation examples for few-shot prompting. (Garc´ıa et al. 2023) outperformed the best-performing system on the WMT’21 English-Chinese news translation task by only using five random examples of English-Chinese parallel data at inference. Both these two works found that example quality is the most important factor, but random sampling will influence their performances. (Yao et al. 2023) proposed a lowresource LLM prompting technique In-Context Sampling

印度和中国占这些服务总出口 的将近 $8 \%$ 。 Score 1 =0.978 --- Index a1 Example a1 Query 0.24\*SDMpeuanlrtseie-RVRetctrrtiioervaRalle+trieval + Score 23 =0.96484 --- Index a23 Exam…pl…e a2k   
Example 1: 中等偏上收入国家...中间产品出口 Hybrid Demonstration Top k demonstration   
总额的一半以上。Translation: Upper-middle- Retrieval Score m =0.756 --- Index am √   
income ... of intermediates from developing Scores and Indices Few-shot   
countries. Prompt   
E但xa与m总ple部 2:地尽点管相妇比女妇在女这比些例职只等有占近主一要半地。位, ... ioafatnhdesCehsienraviacecso'utontafloerxnpeoartlsy. ← $8 \%$   
Translation: Despite the dominance of women at Candidate Output 1 LLM   
these levels, ... almost half the proportion of   
women. India and China account for nearly   
Example m: 早该进行的人口普查，将是全面分 $8 \%$ of these services in total exports. Translation Result   
T析ra人nsl口ati指on数: A和h为ig人hly口o政ver策du奠e定cen良su好s基wil础l 的起点。 Candidate Output 2 0.933 Iancdcioauantn dfoCr hina   
constitute ... for laying quality foundations for a India and China account for nearly Reranker 0.984 nearly $8 \%$ of   
population policy. $8 \%$ of these service exports. 0.956 these services in Candidate Output n total exports. Demonstration Corpus

(ICS) to produce confident predictions by optimizing the construction of multiple ICL prompt inputs. Leveraging a novel corpus derived from a Mambai language manual and additional sentences translated by a native speaker, (Merx et al. 2024) examine the efficacy of few-shot prompting for machine translation (MT) in the low-resource context by prompting with the strategic selection of parallel sentences and dictionary entries, enhancing translation accuracy.

Different from them, our AFSP automatically selects suitable translation demonstrations for various source input sentences to elicit the translation capability of an LLM for better machine translation. Rather than using other embedding models for semantic demonstration retrieval, our AFSP first deploys a translation demonstration retrieval module based on the deployed LLM’s embedding to retrieve topk semantic-similar translation demonstrations from aligned parallel translation corpus. Then, to ensure better semantic consistency between source inputs and target outputs, we force the deployed LLM to generate multiple output candidates in the target language with the help of translation demonstrations and rerank these candidates with an SLM which is trained in a self-supervised way.

# Adaptive Few-shot Prompting (AFSP)

We introduce our Adaptive Few-Shot Prompting framework, which first adaptively retrieves suitable demonstrations to fill into the placeholder in the prompt template from the demonstration corpus and then sorts the multiple candidate sampled outputs generated by the deployed LLM to obtain final translation result. In this work, except for the demonstration placeholder, we use fixed prompt words in the prompt template for general task description. The overview of the inference phase in our AFSP framework is shown in Figure 2. AFSP relies on three key components: a translation demonstration corpus, a hybrid demonstration retrieval module based on the deployed LLM-driven embedding, and a re-ranker module. The translation demonstration corpus aims to provide high-quality parallel translation pairs. The retrieval module takes charge of selecting suitable demonstrations to fill into the prompt template for each input source text. The hybrid demonstration retrieval module is train-free and produces a relevance score based on multiple types of embedding ways for each input source and the text in the demonstration corpus by the deployed LLM for machine translation, rather than using third-party embedding models. With the retrieval demonstrations, we fill the demonstrations into a predefined few-shot prompt and enter it into an LLM for multiple candidate output generation. Finally, we deploy a re-ranker module, which is a small language model (SLM) trained in a self-supervised manner, to sort the generated candidate outputs and obtain the final translation result.

# Prompt Template and Demonstration Corpus

In AFSP, as shown in Table 1, we only use a fixed prompt template with variable placeholders inspired from prior works (Jiang and Zhang 2024b; Agarwal et al. 2024). We do not focus on the diversified design of prompt templates in this work and achieve adaptive prompts for different source text by filling suitable demonstrations according to the retrieved results from the demonstration corpus. Demonstration corpus can be any high-quality parallel translation corpus. In practice, it can be expanded with extra parallel translation corpus. For the sake of simplicity, we simply use the training set from specific translation tasks as the demonstration sources to build the demonstration corpus. For example, we use the training part in the UN Open Corpus v1.0 (Chinese and English versions) as the demonstration corpus when conducting Chinese-English bilingual translation. Similarly, we also use the training subset of our newly constructed Diplomatic corpus as the demonstration corpus when conducting machine translation on its test set.

<html><body><table><tr><td>You are a professional translator.Iwill give you one or more examples of text fragments,where the first one is in {src lang} and the second one is the translation of the first fragment into {tgt lang}.These sentences will be displayed below. 1.{src lang} text: {src demo 1} {tgt lang} translation: {tgt demo 1} 2.{src lang} text: {src demo 2}</td></tr><tr><td>{tgt lang} translation: {tgt demo 2} k.{src lang} text: {src demo k}</td></tr><tr><td>{tgtlang} translation:{tgtdemok} Afterthe example pairs,Iwill provide a/an{src lang} sentence andIwouldlike you to translate it into {tgt lang}.Please pro- vide only the translation result without any additional comments, formatting,or chat content. Translate the text from {src lang}</td></tr></table></body></html>

Table 1: Illustration of the few-shot prompt used in our work. The placeholders $\left\{ s r c l a n g \right\}$ and $\{ t g \bar { t } l a n g \}$ will be replaced with specific source and target language names in practice, such as Chinese, English, etc. Similarly, the placeholders $\{ s r c d e m o i \}$ and $\{ t g t d e m o i \}$ will also be replaced with retrieved parallel translation demonstrations where $i \in [ 1 , . . . , k ]$ .

# Hybrid Demonstration Retrieval

As claimed in the pioneering works (Zhang, Haddow, and Birch 2023a; Zhang et al. 2023; Zhu et al. 2024), the number, quality, and semantic similarity of prompt examples matter the translation performance. Therefore, it is crucial to adaptively retrieve high-quality and highly semantic similar demonstrations for different input texts to achieve better LLM prompting for better eliciting the translation capability of an LLM. To achieve this goal, superior embedding-based semantic representation is essential. (Chen et al. 2024) shows that a combination of different embedding-based retrieval functionalities can improve the discrimination of embedding-based semantic representation. Inspired by them, we build an embedding-based hybrid demonstration retrieval module for demonstration retrieval in a training-free way by utilizing the embedding matrix of the deployed LLM that conducts machine translation. The retrieval results are sorted by a weighted combination of relevance scores based on dense embedding, sparse embedding, and multi-vector embedding. The reason we use the deployed LLM as the embedding generator rather than other embedding models is that LLM is pre-trained with largescale general corpus and can represent text accurately.

Formally, given a query text $q$ in the source language, the demonstration retrieval module can retrieve translation demonstration $\langle d ^ { s r c } , d ^ { t g t } \rangle$ from the corpus $\mathcal { D }$ based on the hybrid relevance score $s _ { r } a n k$ of $q$ and $d ^ { s r c }$ : $\langle d ^ { s r c } , d ^ { t g t } \rangle =$ ${ \dot { f _ { h } } } ( q , \mathcal { D } )$ . Here, $f _ { h } ( \cdot )$ denotes the retrieval function based on the hybrid relevance score. For the text $q$ , dense embedding, sparse embedding, and multi-vector embedding can be formalized separately as follows: 1) dense embedding eqde $e _ { q } ^ { d e n s e }$ · the text $q$ is first transformed into the embedding vectors $\mathbf { E } _ { q }$ based on the embedding layer in the LLM. Then, we obtain $e _ { q } ^ { d e n s e }$ by conducting max pooling on ${ \bf E } _ { q }$ and normalization: $e _ { q } ^ { \bar { d } e n s e } = n o r m ( M a x P o o l i n g ( \mathbf { E } _ { q } ) ) . 2 )$ ) sparse embedding $e _ { q } ^ { s p a r s e }$ : the embedding vector $\mathbf { E } _ { q }$ is also used to estimate the importance of each token to facilitate lexical representation. For each token $t$ within the text $q$ , the token weight is calculated as $w _ { q _ { t } } = R e L U ( \mathbf { W } _ { s p a r s e } ^ { T } \mathbf { E } _ { q } [ t ] )$ , where ReLU is rectified linear unit and $\mathbf { W } _ { s p a r s e } \in \mathbb { R } ^ { H \times 1 }$ . $H$ is the dimension size of the embedding and $\mathbf { W } _ { s p a r s e }$ is a projection matrix mapping token embedding into a float number as its importance. It is only initialized by Gaussian initialization since we found Gaussian initialization is enough to make the model work fine without any model training. 3) multi-vector embedding $e _ { q } ^ { m u l t i }$ : it is an extension of dense embedding by utilizing the entire output embeddings for text representation: $e _ { q . } ^ { \bar { m } u l t i } = { n o r m } ( \mathbf { \bar { W } } _ { m u l t i } ^ { T } \mathbf { E } _ { q } )$ TmultiEq), where Wmulti ∈ RH×H is a projection matrix initialized by Gaussian initialization.

With the above three embeddings with different granularities, we can calculate three relevance scores for multigranularity retrieval. For dense retrieval, given a text $q$ and source demonstration $p$ , we can compute the relevance score $s _ { d e n s e }$ by the inner product between the two embeddings $e _ { q } ^ { d e n s e }$ and $e _ { p } ^ { d e n s e }$ as follows: $s _ { d e n s e } = e _ { q } ^ { d e n s e } \cdot e _ { p _ { . . } } ^ { d e n s e }$ . For sparse retrieval, we can compute $s _ { s p a r s e }$ by the joint importance of the co-existed tokens (denoted as $q \cap p )$ as follows: $\begin{array} { r } { s _ { s p a r s e } = \sum _ { t \in q \cap p } ( w _ { q _ { t } } \times w _ { q _ { t } } ) } \end{array}$ . For multi-vector retrieval, we can compute $s _ { m u l t i }$ by late interaction as follows: $\begin{array} { r } { s _ { s p a r s e } = \frac { 1 } { l _ { q } } \sum _ { i = 1 } ^ { l _ { q } } m a x _ { j = 1 } ^ { l _ { p } } e _ { q } ^ { m u l t i } [ i ] \cdot e _ { p } ^ { m u l t i } [ j ] ^ { T } } \end{array}$ , where $l _ { q }$ and $l _ { p }$ are the lengths of text $q$ and source demonstration $p$ .

Based on the above three relevance scores, we conduct the demonstration retrieval in a hybrid process according to $s _ { r a n k }$ which can be defined as follows:

$$
s _ { r a n k } = \alpha _ { 1 } \times s _ { d e n s e } + \alpha _ { 2 } \times s _ { s p a r s e } + \alpha _ { 3 } \times s _ { m u l t i }
$$

where $\alpha _ { 1 } , \alpha _ { 2 }$ , and $\alpha _ { 3 }$ are three hyper-parameters to adjust the weights of three retrieval functionality.

# Result Re-ranking

There is output diversity in an LLM (Kirk et al. 2023) due to the probabilistic sampling. The different outputs may have different semantic biases, which will influence the final translation performance. To mitigate this issue, we force the deployed LLM to generate multiple output candidates in the target language and rerank these candidates by a re-ranker model based on a small language model (SLM). The re-ranker takes charge of scoring the output candidates. However, training this re-ranker is challenging since there is available large-scale annotated corpus about the translation quality of different translation outputs and annotating such a corpus is costly. Therefore, we design a self-supervised training method to train such a re-ranker at a low cost by conducting negative sampling with different text perturbations.

Negative Sampling Formally, given the parallel translation corpus $\begin{array} { r l r l } { \mathcal { D } } & { { } } & { = } & { } \end{array}$ $\left\{ < d _ { 1 } ^ { s r c } , d _ { 1 } ^ { t g t } > , < d _ { 2 } ^ { s r c } , d _ { 2 } ^ { t g t } > , . . . , < \bar { d } _ { N } ^ { s r c } , d _ { N } ^ { t g t } > \right\}$ where $d _ { i } ^ { s r c }$ and $d _ { i } ^ { t g t }$ are the texts in the source language and the target language respectively. To construct a dataset $\begin{array} { r l r } { \mathcal { D } ^ { \prime } } & { = } & { \Big \{ < \breve { d } _ { 1 } ^ { t g t ^ { \prime } } , s _ { 1 } ^ { \prime } > , < { d } _ { 2 } ^ { t g t ^ { \prime } } , s _ { 2 } ^ { \prime } > , . . . , < { d } _ { M } ^ { t g t ^ { \prime } } , s _ { M } ^ { \prime } > \Big \} } \end{array}$ to train the re-ranker, we can disturb the $d _ { i } ^ { t g t }$ by multiple degeneration operation set $A$ including converting to the parallel text $( P a r a l l e l )$ , back translation $( B a c k )$ , inserting source text $( I n s e r t )$ , spelling mistake $( S e )$ , repeated translation (Ret), synonym replacement (Replace). ditgt′ and $s _ { i } ^ { \prime }$ are the degenerated text and corresponding quality score, respectively. We define the quality score of the original target text $d _ { i } ^ { t g t }$ as 1. Assuming that $B$ contains a null operation that means we just copy the original text into $\mathcal { D } ^ { \prime }$ and all possible combinations of the degeneration operation in $A$ , for each possible combination $b _ { i } \in B$ , we can obtain the degenerated text $d _ { i } ^ { t g t ^ { \prime } }$ and calculate its score $s ^ { \prime }$ as follows:

$$
\begin{array} { c } { d _ { i } ^ { t g t ^ { \prime } } = f _ { b _ { i } } ( d _ { i } ^ { t g t } ) , b _ { i } \in B } \\ { s _ { i } ^ { \prime } = 1 - 0 . 2 \cdot | b _ { i } | } \end{array}
$$

where $f _ { b _ { i } } ( \cdot )$ represents the degeneration function with the degenration operation combination $b _ { i }$ and $\left| b _ { i } \right|$ is the number of degeration operations in $b _ { i }$ . In this way, we can generate a large-scale dataset $\mathcal { D } ^ { \prime }$ from the parallel translation corpus $\mathcal { D }$ to train the re-ranker in a self-supervised manner.

Re-ranker and Learning Objectives We deploy BERT (Devlin et al. 2019) as the backbone of the SLM in the Re-ranker. For Chinese-English translation, we use Bert-large-cased1 while we use Bert-based-Chinese2 as the SLM for English-Chinese translation. Given a degenerated text dtgt′ and its quality score $s _ { i } ^ { \prime }$ , the re-ranker takes the degenerated text ditgt′ as input and predicts a quality assessment score $s _ { i } ^ { r e r a n k }$ as close to the annotated score $s _ { i } ^ { \prime }$ as possible. The re-ranker calculates the quality assessment score by applying a Linear layer to map the output encoding of [CLS] token into 1-D float number followed by the Sigmoid function to normalize the output to between 0 and 1. To optimize the re-ranker, we adopted Mean Squared Error as its objective function to enable the re-ranker to predict quality assessment scores. Therefore, the re-ranker and its learning objective can be modeled as follows:

$$
\begin{array} { r } { \mathbf { E } = B E R T ( d _ { i } ^ { t g t ^ { \prime } } ) \qquad } \\ { s _ { i } ^ { r e r a n k } = S i g m o i d ( L i n e a r ( \mathbf { E } [ 0 ] ) ) } \\ { \mathcal { L } = \left. s _ { i } ^ { r e r a n k } - s _ { i } ^ { \prime } \right. _ { 2 } \qquad } \end{array}
$$

# Experiments Experiment Settings

Datasets To validate the effectiveness of the proposed AFSP, we first crawled a high-quality parallel ChineseEnglish dataset named Diplomatic corpus from the China Diplomatic website3. The Diplomatic corpus consists of speeches made by spokespersons during routine press conferences, including questions posed by journalists and responses from Chinese spokespersons on a range of diplomatic issues. There are some advantages in the Diplomatic corpus. The first is accessibility. All data is publicly available on the China Diplomatic website and can be easily found online. The second is high quality. All bilingual materials are translated by professional translators from specialized institutions, ensuring superior quality. The third is language complexity. The Diplomatic corpus contains certain political terminology and specialized terms, which may pose challenges for the LLM in the context of China’s political landscape. The final one is recency. All texts are recorded from 2022 to 2023 which can reflect the latest advances in language. Besides, we also use a Chinese-English subset from the UN Open Corpus v1.0 as the second testbed, which can show the universality of the proposed AFSP. We use UN to denote this subset. The UN Parallel Corpus is a parallel corpus that includes official UN documents and statements from meetings. The content covers various fields such as politics, economics, culture, and technology. This corpus records texts written and manually translated from 1990 to 2014, aligned at the sentence level. We randomly selected 120,000 entries as the data for evaluation. The statistics for both two datasets are shown in Table 2. For both two datasets Diplomatic and UN, we randomly selected 500 parallel translation pairs to serve as the test set for evaluating AFSP. The remaining pairs are used as the demonstration corpus for adaptive demonstration retrieval.

Table 2: Statistics of the Diplomatic Corpus and UN. The statistics include the number of sentences (#Sent.), the number of words (#Word), and the average number of words per sentence (#Average.).   

<html><body><table><tr><td>Dataset</td><td>Language</td><td>#Sent.</td><td>#Word</td><td>#Average.</td></tr><tr><td rowspan="2">Diplomatic</td><td>English</td><td>5.528K</td><td>415K</td><td>75.20</td></tr><tr><td>Chinese</td><td>5.528K</td><td>316K</td><td>57.29</td></tr><tr><td rowspan="2">UN</td><td>English</td><td>120K</td><td>3,500K</td><td>29.11</td></tr><tr><td>Chinese</td><td>120K</td><td>3,220K</td><td>26.75</td></tr></table></body></html>

Metrics To conduct a comprehensive assessment of translation quality, we employed the most commonly used BLEU-4 (B-4) (Papineni et al. 2002), METEOR (ME) (Banerjee and Lavie 2005), ROUGE-1 (R-1), ROUGE-2 (R-2), ROUGE-L (R-L) (Lin 2004), CHRF (CH) (Popovic´ 2015), and COMET-Kiwi (CK)(Rei et al. 2022) as evaluation metrics. We also evaluate our AFSP by conducting a human evaluation of fluency (FLU), accuracy (ACC), and consistency of style (STY).

Implementation Details We evaluate AFSP on 3 opensource LLMs, ChatGLM3-6B, InternLM2-7B, and Llama3- 8B, as well as a closed-source LLM ChatGPT-3.5-turbo0125 by comparing with three baselines, Zero-shot (Jiang and Zhang 2024a), Few-shot (Jiang and Zhang 2024a), kNN-based few-shot (Nori et al. 2023) in both Chinese-toEnglish and English-to-Chinese translation directions. The $\alpha _ { 1 } , \alpha _ { 2 }$ , and $\alpha _ { 3 }$ are set to 0.4, 0.4, 0.2 for computing the final relevance score $s _ { r a n k }$ . The $k$ for few-shot prompts is set to 3 due to the limited memory of NVIDIA RTX 3090. For the closed-source ChatGPT-3.5-turbo-0125, we deploy ChatGLM3-6B as the embedding model for hybrid demonstration retrieval. We conduct top-30 sampling for ChatGLM3-6B, InternLM2-7B, and Llama3-8B and top-5 sampling for ChatGPT-3.5-turbo-0125.

Table 3: Performance Comparison on Diplomatic Corpus. The best result is highlighted in bold.   

<html><body><table><tr><td rowspan="2">Methods</td><td colspan="6">English-to-Chinese</td><td colspan="6">Chinese-to-English</td></tr><tr><td>B-4</td><td>ME</td><td>R-1</td><td>R-2</td><td>R-L CH</td><td>CK</td><td>B-4</td><td>ME</td><td>R-1</td><td>R-2</td><td>R-L CH</td><td>CK</td></tr><tr><td></td><td colspan="14">ChatGLM3-6B</td></tr><tr><td>Zero-shot</td><td>18.9</td><td>50.6</td><td>21.2</td><td>10.3</td><td>21.1 42.8</td><td>88.1</td><td>22.7</td><td>54.6</td><td>59.9</td><td>32.0</td><td>50.9 66.5</td><td>81.4</td></tr><tr><td>Few-shot</td><td>20.3</td><td>51.7</td><td>20.5 10.3</td><td>20.5</td><td>43.8</td><td>88.2</td><td>23.6</td><td>55.7 60.8</td><td>33.1</td><td>51.6</td><td>67.1</td><td>81.4</td></tr><tr><td>KNN Few-shot</td><td>20.2</td><td>51.8</td><td>20.8</td><td>10.4 20.7</td><td>43.9</td><td>88.3</td><td>22.9</td><td>55.1</td><td>60.4 32.5</td><td>51.1</td><td>66.7</td><td>81.3</td></tr><tr><td>AFSP w/o rerank (Ours)</td><td>25.9</td><td>56.8</td><td>22.5</td><td>11.4 22.4</td><td>48.6</td><td>89.0</td><td>28.1</td><td>59.3</td><td>63.8</td><td>37.3</td><td>54.9 69.3</td><td>82.3</td></tr><tr><td>AFSP (Ours)</td><td>27.4</td><td>58.2</td><td>22.9</td><td>11.5 22.9</td><td>49.8</td><td>89.1</td><td>29.2</td><td>60.0</td><td>64.0</td><td>37.6 55.2</td><td>69.7</td><td>82.7</td></tr><tr><td colspan="14">InternLM2-7B</td></tr><tr><td>Zero-shot</td><td>20.6</td><td>52.3</td><td>22.4</td><td>11.1</td><td>22.3 44.5</td><td>88.4</td><td>23.4</td><td>56.1</td><td>60.4</td><td>32.6</td><td>51.2 67.7</td><td>81.9</td></tr><tr><td>Few-shot</td><td>24.5</td><td>55.7</td><td>22.7</td><td>11.6</td><td>22.5 47.2</td><td>89.0</td><td>24.2</td><td>57.1</td><td>61.4</td><td>33.8</td><td>52.2 68.5</td><td>81.7</td></tr><tr><td>KNN Few-shot</td><td>24.9</td><td>56.3</td><td>22.8</td><td>11.7</td><td>22.6 47.6</td><td>89.0</td><td>24.7</td><td>57.4</td><td>61.7</td><td>34.3</td><td>52.6 68.7</td><td>81.9</td></tr><tr><td>AFSP w/o rerank (Ours)</td><td>30.7</td><td>60.8</td><td>22.9</td><td>11.9</td><td>22.7 52.3</td><td>89.5</td><td>30.1</td><td>61.2</td><td>65.0</td><td>39.3</td><td></td><td></td></tr><tr><td>AFSP (Ours)</td><td>31.8</td><td>61.3</td><td>22.9</td><td>11.9</td><td>22.7 52.9</td><td>89.6</td><td>31.3</td><td>61.9</td><td>64.9</td><td>39.7</td><td>56.3 71.0 56.1 70.9</td><td>82.5 82.7</td></tr><tr><td colspan="14"></td></tr><tr><td>Zero-shot</td><td>10.7</td><td>37.2</td><td>17.4</td><td>8.1</td><td>17.2 32.0</td><td>84.6</td><td>Llama3-8B 24.0</td><td>56.7</td><td>60.9</td><td>33.6 51.7</td><td>68.2</td><td>82.3</td></tr><tr><td>Few-shot</td><td>15.5</td><td>45.7</td><td>18.5</td><td>8.8</td><td>18.3 39.0</td><td>86.2</td><td>25.7</td><td>58.0</td><td>62.4</td><td>35.2</td><td>53.1 68.8</td><td>82.8</td></tr><tr><td>KNN Few-shot</td><td>16.5</td><td>46.8</td><td>18.7</td><td>9.0</td><td>18.5 39.9</td><td>86.5</td><td>25.3</td><td>57.6</td><td>62.3</td><td>35.1</td><td>53.0 68.4</td><td>82.8</td></tr><tr><td>AFSP w/o rerank (Ours)</td><td>26.3</td><td>55.6</td><td>19.8</td><td>9.6</td><td>19.6 48.0</td><td>88.0</td><td>30.7</td><td>61.5</td><td>65.7</td><td>40.2</td><td>57.1 71.1</td><td>83.3</td></tr><tr><td>AFSP (Ours)</td><td>27.7</td><td>56.7</td><td>19.9</td><td>9.9</td><td>19.7 49.1</td><td>88.4</td><td>31.0</td><td>61.5</td><td>65.7</td><td>40.3</td><td>57.0 71.0</td><td>83.3</td></tr><tr><td colspan="14">Chatgpt-3.5-turbo-0125</td></tr><tr><td>Zero-shot</td><td>23.0</td><td>54.9</td><td>22.3</td><td>11.1</td><td>22.2 46.5</td><td>89.1</td><td>27.7</td><td>59.9</td><td>64.1 37.3</td><td>55.5</td><td>70.7</td><td>83.2</td></tr><tr><td>Few-shot</td><td>24.6</td><td>56.7</td><td>22.3</td><td>11.2</td><td>22.2 48.2</td><td>89.5</td><td>28.1</td><td>60.5</td><td>64.6</td><td>37.7</td><td>56.0 71.0</td><td>83.4</td></tr><tr><td>KNN Few-shot</td><td>25.6</td><td>57.8</td><td>22.4</td><td>11.4</td><td>22.3 49.1</td><td>89.5</td><td>28.3</td><td>60.5</td><td>64.5</td><td>37.9</td><td>56.00 70.9</td><td>83.5</td></tr><tr><td>AFSP w/o rerank (Ours)</td><td>30.3</td><td>62.1</td><td>23.5</td><td>11.7</td><td>23.4 52.9</td><td>89.9</td><td>31.3</td><td>62.5</td><td>66.4</td><td>40.8</td><td>58.00 72.1</td><td>83.9</td></tr><tr><td>AFSP (Ours)</td><td>32.3</td><td>63.5</td><td>23.3</td><td>11.7</td><td>23.3 54.3</td><td>90.3</td><td>32.3</td><td>63.2</td><td>66.9</td><td>41.40</td><td>58.7 72.5</td><td>84.0</td></tr></table></body></html>

Table 4: Performance Comparison on UN. The best result is highlighted in bold.   

<html><body><table><tr><td rowspan="2">Methods</td><td colspan="6">English-to-Chinese</td><td colspan="7">Chinese-to-English</td></tr><tr><td>B-4</td><td>ME</td><td>R-1</td><td>R-2</td><td>R-L CH</td><td>CK</td><td>B-4</td><td>ME</td><td>R-1</td><td>R-2</td><td>R-L</td><td>CH</td><td>CK</td></tr><tr><td></td><td colspan="14">ChatGLM3-6B</td></tr><tr><td>Zero Few-shot</td><td>18.9</td><td>52.9</td><td>42.2</td><td>17.9</td><td>41.7</td><td>45.2 86.9</td><td>21.5</td><td>58.0</td><td>60.0</td><td>34.2</td><td>53.2</td><td>64.1</td><td>83.6</td></tr><tr><td>Few-shot</td><td>19.1</td><td>52.6</td><td>42.1</td><td>17.6</td><td>41.6</td><td>45.1 86.3</td><td>21.8</td><td>58.2</td><td>59.4</td><td>34.6</td><td>52.8</td><td>65.3</td><td>82.5</td></tr><tr><td>KNN Few-shot</td><td>18.8</td><td>52.3</td><td>41.6</td><td>17.5</td><td>41.2</td><td>44.9 86.6</td><td>22.7</td><td>59.0</td><td>60.1</td><td>35.3</td><td>53.5</td><td>65.1</td><td>83.1</td></tr><tr><td>AFSP w/o rerank (Ours)</td><td>24.8</td><td>57.3</td><td>42.8</td><td>18.3</td><td>42.5</td><td>50.2 88.0</td><td>29.4</td><td>63.4</td><td>64.6</td><td>41.7</td><td>58.2</td><td>69.0</td><td>84.6</td></tr><tr><td>AFSP (Ours)</td><td>24.6</td><td>57.6</td><td>42.8</td><td>18.3</td><td>42.4</td><td>50.5 88.2</td><td>29.1</td><td>64.5</td><td>65.1</td><td>41.8</td><td>58.4</td><td>69.6</td><td>84.9</td></tr><tr><td></td><td colspan="14">InternLM2-7B</td></tr><tr><td>Zero Few-shot</td><td>17.3</td><td>50.3</td><td>37.4</td><td>17.8</td><td>36.8</td><td>43.5 86.3</td><td>25.5</td><td>61.9</td><td>63.5</td><td>38.7</td><td>57.0</td><td>67.2</td><td>84.7</td></tr><tr><td>Few-shot</td><td>17.9</td><td>51.5</td><td>41.1</td><td>18.4</td><td>40.7</td><td>44.4 86.7</td><td>25.8</td><td>62.6</td><td>63.5</td><td>38.8</td><td>57.0</td><td>68.4</td><td>83.9</td></tr><tr><td>KNN Few-shot</td><td>17.8</td><td>51.3</td><td>39.5</td><td>18.2</td><td>39.0</td><td>44.3</td><td>86.6 26.3</td><td>62.6</td><td>64.0</td><td>39.3</td><td>57.3</td><td>68.0</td><td>84.5</td></tr><tr><td>AFSP w/o rerank (Ours)</td><td>26.2</td><td>58.5</td><td>40.5</td><td>19.2</td><td>40.1</td><td>51.6</td><td>88.3 31.3</td><td>61.9</td><td>64.9</td><td>39.7</td><td>56.1</td><td></td><td></td></tr><tr><td>AFSP (Ours)</td><td>26.1</td><td>58.8</td><td>40.5</td><td>19.3</td><td>40.1</td><td>51.8</td><td>88.9 34.0</td><td>67.4</td><td>68.6</td><td>46.3</td><td>62.5</td><td>72.8 71.9</td><td>86.1 86.5</td></tr><tr><td></td><td colspan="14"></td></tr><tr><td>Zero Few-shot</td><td>8.9</td><td>35.4</td><td>34.2</td><td>15.4</td><td>33.7</td><td>30.3 81.1</td><td>Llama3-8B 18.6</td><td>55.4</td><td>58.2</td><td>32.1</td><td>51.1</td><td>63.9</td><td>83.4</td></tr><tr><td>Few-shot</td><td>13.1</td><td>44.5</td><td>34.1</td><td>15.3</td><td>33.4</td><td>38.4 83.4</td><td>21.9</td><td>58.1</td><td>60.8</td><td>35.1</td><td>53.7</td><td>65.9</td><td>84.3</td></tr><tr><td>KNN Few-shot</td><td>12.6</td><td>43.8</td><td>29.4</td><td>15.0</td><td>28.9</td><td>37.9</td><td>83.5 21.7</td><td>57.6</td><td>60.7</td><td>35.1</td><td>53.6</td><td>65.7</td><td>84.3</td></tr><tr><td>AFSP w/o rerank (Ours)</td><td>22.7</td><td>54.0</td><td>33.1</td><td>16.5</td><td>32.5</td><td>47.8</td><td>86.5 30.1</td><td>63.9</td><td>66.8</td><td>44.2</td><td>60.7</td><td></td><td>86.3</td></tr><tr><td>AFSP (Ours)</td><td>23.6</td><td>54.5</td><td>34.3</td><td>16.9</td><td>33.8</td><td>48.5 87.2</td><td>31.0</td><td>64.9</td><td>67.1</td><td>44.9</td><td>61.0</td><td>71.3 71.7</td><td>86.2</td></tr><tr><td></td><td colspan="14">Chatgpt-3.5-turbo-0125</td></tr><tr><td>Zero Few-shot</td><td>20.0</td><td>52.9</td><td>32.0</td><td>17.5</td><td>31.5</td><td>46.3 87.1</td><td>24.5</td><td>61.3</td><td>63.3</td><td>38.0</td><td>56.9</td><td>67.8</td><td>85.4</td></tr><tr><td>Few-shot</td><td>21.3</td><td>55.4</td><td>43.0</td><td>18.6</td><td>42.5</td><td>47.9</td><td>87.9 27.0</td><td>63.9</td><td>65.3</td><td>40.6</td><td>58.9</td><td>69.7</td><td>85.9</td></tr><tr><td>KNN Few-shot</td><td>20.3</td><td>53.8</td><td>35.1</td><td>18.0</td><td>34.7</td><td>47.1</td><td>87.5 27.1</td><td>63.1</td><td>64.9</td><td>40.6</td><td>58.7</td><td>69.3</td><td>85.9</td></tr><tr><td>AFSP w/o rerank (Ours)</td><td>28.4</td><td>61.1</td><td>40.7</td><td>19.0</td><td>40.3</td><td>53.9</td><td>89.1 32.7</td><td>66.9</td><td>68.9</td><td>46.2</td><td>63.1</td><td>73.0</td><td>86.9</td></tr><tr><td>AFSP (Ours)</td><td>29.1</td><td>62.0</td><td>42.4</td><td>19.1</td><td>42.0</td><td>54.6</td><td>89.5 34.1</td><td>67.5</td><td>69.3</td><td>47.0</td><td>63.7</td><td>73.3</td><td>87.3</td></tr></table></body></html>

# Experiment Result

Main Results Table 3 and Table 4 show the performance of our AFSP and baselines on different translation datasets and different LLMs. Compared to baselines, our AFSP demonstrates superior performance by always generating higher-quality translation according to various metrics. For instance, when Llama-3-8B translates from Chinese to English on the UN, our ASFP achieves significant improvements over KNN Few-shot with 9.3 improvement in BLEU-4, 7.3 improvement in METEOR, 7.4 improvement in ROUGE-L, and 1.9 improvement in COMET-Kiwi. Other models also show significant metric improvements in translation across different datasets and LLMs, highlighting the effectiveness of our AFSP method.

Table 5: Human evaluation results on the translation performance for the Diplomatic corpus.   

<html><body><table><tr><td>Method</td><td>FLU ACC</td><td>STY FLU</td><td>ACC STY</td></tr><tr><td></td><td>Chinese-to-English</td><td></td><td>English-to-Chinese</td></tr><tr><td>Zero-shot</td><td>14.3 20.7</td><td>20.0</td><td>14.3 12.1 14.3</td></tr><tr><td>Few-shot</td><td>25.7 25.7</td><td>27.1 14.3</td><td>25.0 17.1</td></tr><tr><td>KNN Few-shot</td><td>27.1 22.9</td><td>19.3 17.8</td><td>14.3 15.0</td></tr><tr><td>ASFP(Ours)</td><td>32.9 30.7</td><td>30.7</td><td>53.6 48.6 53.6</td></tr></table></body></html>

Table 6: The translation performance of ASFP with different embedding models for ChatGLM3-6B on the Englishto-Chinese part of Diplomatic Corpus.   

<html><body><table><tr><td>Emb</td><td>BGE-M3</td><td>E5-Large</td><td>BGE-large</td><td>BCE</td><td>ChatGLM3</td></tr><tr><td>B-4</td><td>23.6</td><td>24.1</td><td>20.3</td><td>24.7</td><td>27.4</td></tr><tr><td>ME</td><td>54.9</td><td>55.1</td><td>51.8</td><td>55.6</td><td>58.2</td></tr><tr><td>R-1</td><td>21.5</td><td>22.0</td><td>20.6</td><td>21.8</td><td>22.9</td></tr><tr><td>R-2</td><td>10.8</td><td>10.9</td><td>10.4</td><td>10.8</td><td>11.5</td></tr><tr><td>R-L</td><td>21.4</td><td>21.9</td><td>20.5</td><td>21.7</td><td>22.9</td></tr><tr><td>CH</td><td>46.7</td><td>47.0</td><td>43.9</td><td>47.5</td><td>49.8</td></tr><tr><td>CK</td><td>88.7</td><td>88.7</td><td>88.3</td><td>88.9</td><td>89.1</td></tr></table></body></html>

Human Evaluation To further validate the effectiveness of the AFSP, we also conducted a human evaluation of both two datasets and two translation directions to compare AFSP with baselines. For each translation direction, we randomly selected 5 examples from each dataset. Participants judged the options based on fluency, accuracy, and style retention by selecting the sentence they deemed best. We tested the translation results generated by Llama3-8B and invited 14 teachers or students fluent in English or Chinese to participate in the evaluation for each translation direction. To avoid bias, the output order was randomized. The evaluation results in Table 5 demonstrate that our AFSP outperforms all baselines in fluency, semantic accuracy, and style consistency.

The Choice of Embedding Model In our hybrid demonstration retrieval, we use the embedding model in the deployed LLM to compute relevance scores. To show the effectiveness of using the embedding model of the deployed LLM model, we conduct an ablation study on various embedding models including BGE-M3, E5-Large, BGE-large, and BCE. The results in Table 6 show using the embedding model of the deployed LLM model is a better choice than using third-party embedding models.

Ablation on Weights of Hybrid Demonstration Retrieval The hybrid demonstration retrieval uses multiple retrieval functions to compute relevance scores. To investigate the influence of different weights $\alpha _ { 1 }$ , $\alpha _ { 2 }$ , and $\alpha _ { 3 }$ , we conduct experiments with ChatGLM3-6B on the Diplomatic Corpus by setting different weights. The results in Table 7 show that $\alpha _ { 1 } , \alpha _ { 2 }$ , and $\alpha _ { 3 }$ are set to 0.4, 0.4, and 0.2 can achieve the best performance on most of the metrics.

Table 7: The translation performance of ChatGLM3-6B on the English-to-Chinese part of Diplomatic Corpus with different weights.   

<html><body><table><tr><td>α1 α2 α3</td><td>0.2 0.4 0.4</td><td>0.3 0.3 0.4</td><td>0.4 0.3 0.3</td><td>0.25 0.25 0.5</td><td>0.25 0.35 0.4</td><td>0.35 0.35 0.3</td><td>0.4 0.4 0.2</td></tr><tr><td>B-4 ME</td><td>25.8</td><td>25.7</td><td>25.9</td><td>25.7</td><td>25.6</td><td>25.7</td><td>25.9</td></tr><tr><td>R-1</td><td>56.7</td><td>56.5</td><td>56.6</td><td>56.6</td><td>56.5</td><td>56.6</td><td>56.8</td></tr><tr><td>R-2</td><td>22.2</td><td>22.2</td><td>22.4</td><td>22.4</td><td>22.2</td><td>22.2</td><td>22.5</td></tr><tr><td>R-L</td><td>11.3</td><td>11.2</td><td>11.4</td><td>11.2</td><td>11.1</td><td>11.2</td><td>11.4</td></tr><tr><td></td><td>22.1</td><td>22.1</td><td>22.4</td><td>22.3</td><td>22.2</td><td>22.1</td><td>22.4</td></tr><tr><td>CH</td><td>48.5</td><td>48.3</td><td>48.4</td><td>48.4</td><td>48.3</td><td>48.4</td><td>49.8</td></tr><tr><td>CK</td><td>89.0</td><td>88.9</td><td>89.0</td><td>89.0</td><td>89.0</td><td>89.0</td><td>89.0</td></tr></table></body></html>

Table 8: The performance of different numbers of demonstrations on the Diplomatic Corpus with ChatGLM3-6B.   

<html><body><table><tr><td>Demos</td><td>1</td><td>2</td><td>3</td><td>1 2</td><td>3</td></tr><tr><td></td><td colspan="3">Chinese-to-English</td><td colspan="3">English-to-Chinese</td></tr><tr><td>B-4</td><td>26.7</td><td>27.7</td><td>29.2</td><td>24.2</td><td>26.0</td><td>27.4</td></tr><tr><td>ME</td><td>58.1</td><td>58.9</td><td>60.0</td><td>55.2</td><td>56.9</td><td>58.2</td></tr><tr><td>R-1</td><td>62.0</td><td>63.6</td><td>64.0</td><td>22.2</td><td>22.0</td><td>22.9</td></tr><tr><td>R-L</td><td>54.1</td><td>54.7</td><td>55.2</td><td>22.1</td><td>22.0</td><td>22.9</td></tr><tr><td>CH</td><td>68.7</td><td>69.1</td><td>69.7</td><td>47.2</td><td>47.8</td><td>49.8</td></tr><tr><td>CK</td><td>81.8</td><td>82.3</td><td>82.7</td><td>88.8</td><td>88.9</td><td>89.1</td></tr></table></body></html>

The Number of Translation Demonstrations We verify the effects of different numbers of demonstrations for fewshot prompting by using the Diplomatic Corpus dataset on ChatGLM3-6B. The results in Table 8 show the best translation performance can be achieved when the number of demonstrations is 3.

# Conclusion

In this work, we propose an adaptive few-shot prompting (AFSP) framework to automatically select suitable translation demonstrations for various source input sentences to further elicit the translation capability of an LLM for better machine translation. First, we retrieve top-k semanticsimilar translation demonstrations from aligned parallel translation corpus based on hybrid demonstration retrieval. Then, to ensure better semantic consistency between source inputs and target outputs, we force the deployed LLM itself to generate multiple output candidates in the target language with the help of translation demonstrations and rerank these candidates. Besides, to better evaluate the effectiveness of our AFSP framework on the latest language and extend the research boundary of neural machine translation, we construct a high-quality diplomatic Chinese-English parallel dataset that consists of 5,528 parallel sentences. Extensive experiments on the proposed Diplomatic dataset and UN show the effectiveness and superiority of our AFSP.