# Filling Memory Gaps: Enhancing Continual Semantic Parsing via SQL Syntax Variance-Guided LLMs Without Real Data Replay

Ruiheng Liu1, 2, Jinyu Zhang2, Yanqi Song2, Yu Zhang2\*, Bailong Yang1\*

1Xi’an Research Institute of High-Tech, Xi’an, China 2Harbin Institute of Technology, Harbin, China {rhliu, jyzhang, yqsong, zhangyu}@ir.hit.edu.cn, xa 403@163.com

# Abstract

Continual Semantic Parsing (CSP) aims to train parsers to convert natural language questions into SQL across tasks with limited annotated examples, adapting to dynamically updated databases in real-world scenarios. Previous studies mitigate this challenge by replaying historical data or employing parameter-efficient tuning (PET), but they often violate data privacy or rely on ideal continual learning settings. To address these issues, we propose a new Large Language Model (LLM)-Enhanced Continuous Semantic Parsing method, named LECSP, which alleviates forgetting while encouraging generalization, without requiring real data replay or ideal settings. Specifically, it first analyzes the commonalities and differences between tasks from the SQL syntax perspective to guide LLMs in reconstructing key memories and improving memory accuracy through calibration. Then, it uses a task-aware dual-teacher distillation framework to promote the accumulation and transfer of knowledge during sequential training. Experimental results on two CSP benchmarks show that our method significantly outperforms existing methods, even those utilizing data replay or ideal settings. Additionally, we achieve generalization performance beyond upper limits, better adapting to unseen tasks.

# Code — https://github.com/tom68-ll/LECSP Extended version — https://arxiv.org/abs/2412.07246

Limited annotated data Train For each classroom, report the classroom ！ T Student number and the number of grades using it. SELECT Classroom, COUNT(DISTINCT Grade) Overfitting SQ FROM List GROUP BY Classroom   
Sequential Training Update Overfitting Limited annotated data What are the countries for JetBlue Airways Task 2 白 flights, ordered by time? Flight Train SELECT Country FROM AIRLINES WHERE ： Airline $\mathbf { \Sigma } = \mathbf { \Sigma }$ ‘JetBlue Airways’ ORDER BY Time Forget the SQL knowledge like COUNT(DISTINCT…)…GROUP BY in Task 1 (a) Our method   
GROUP BY Task accuracy Largemloadneglsuage Previous methods   
DISTINCT LIMIT   
ASC|DESC 0 1 2 3 4 5 6 7 8 9 Additional data required Task ID (b) (c)

# Introduction

Semantic parsing offers a user-friendly interface for nonexperts to query data and perform various analyses (Hu et al. 2023; Li et al. 2023b). While most previous studies focus on static data distributions (Li et al. 2023a; Wang et al. 2024a; Guo et al. 2024), the frequent updates in real-world databases have led researchers to shift their focus to continual semantic parsing (Li, Qu, and Haffari 2021; Lialin et al. 2021; Yadav et al. 2023; Chen et al. 2023a,b).

As illustrated in Figure 1(a), a semantic parser must undergo continual training across a series of tasks from different databases, while ensuring good performance on current and historical tasks. This task faces two primary challenges: (1) The scarcity of annotated data for each task can easily result in model overfitting (Qin and Joty 2022; Chen et al. 2023a). (2) Training sequentially leads to catastrophic forgetting (Chen et al. 2023a,b; Wang et al. 2024b; Qiao and Mahdavi 2024), where the model’s performance on earlier tasks significantly worsens after learning new ones.

To tackle these challenges, several studies have explored continual learning methods, which can be divided into two types from data and model dimensions: Rehearsal-based and PET-based. The former devises specific sampling strategies to replay historical task data to prevent forgetting (Li, Qu, and Haffari 2021; Wang et al. 2022; Chen et al. 2023a). The latter freezes the model backbone and uses small-scale parameters, such as prompt (Lester, Al-Rfou, and Constant 2021), to steer representation learning (Chen et al. 2023b;

Razdaibiedina et al. 2023; Yadav et al. 2023). Although these methods have made initial progress, they still face limitations. First, the performance of Rehearsal-based methods relies heavily on the amount of historical data and may not be practical in privacy-sensitive or memory-constrained environments (Jung et al. 2023; Wang et al. 2023b, 2024b). Second, PET-based methods learn PET modules for each task and store them in a cache pool. When test samples arrive, most methods assume that the corresponding PET module for each sample is known. This ideal setting contradicts real-world scenarios and hinders generalization to unseen samples (Chen et al. 2023b; Razdaibiedina et al. 2023).

Considering the above, we believe the ideal approach should not rely on historical data or ideal continual learning settings. Instead, it should effectively bridge the gaps between tasks by identifying and analyzing the potential commonalities and critical differences in the required knowledge between incoming new tasks and previously learned ones, thereby mitigating forgetting and promoting generalization. As shown in Figure 1(b), limited annotated data can lead to uneven distribution of SQL syntax knowledge among tasks. Our motivation is to leverage these variances to guide LLMs in reconstructing relevant memories to fill these gaps.

In this work, we propose a novel LLM-Enhanced Continual Semantic Parsing method (LECSP). First, it analyzes component biases from the perspective of SQL syntax between current and past tasks, without accessing historical data. Then, these biases are used to guide LLMs in generating relevant pseudo-samples as memory from intra-task and inter-task perspectives, reinforcing commonalities and filling in differences. Additionally, a calibration strategy is introduced to enhance the accuracy and fidelity of memory. To further improve memory utilization efficiency, we design a task-aware dual-teacher distillation framework. This framework not only facilitates the transfer and accumulation of historical task knowledge across the task stream but also enables the migration of external knowledge from LLMs to smaller models. A comparison of our approach with existing methods is illustrated in Figure 1(c). Extensive experiments on the CSP benchmarks Spider-stream-semi (Chen et al. 2023a) and Combined-stream (Chen et al. 2023b) demonstrate that LECSP significantly outperforms other baselines that use data replay or ideal continual learning settings, with gains up to $8 . 8 \%$ , and effectively adapts to more challenging cold-start scenarios. Additionally, we achieve performance beyond the theoretical upper bounds in knowledge forward transfer capability. The main contributions of this work are summarized as follows:

• We propose a novel CSP framework LECSP, including memory reconstruction with LLMs and task-aware dualteacher distillation learning. These help to mitigate forgetting and promote generalization. • A memory calibration strategy is introduced, consisting of iterative self-correction and SQL skeleton-based sampling, to further improve memory accuracy and fidelity. • Extensive experiments on benchmark datasets show that LECSP achieves state-of-the-art (SOTA) performance without using historical data or ideal continual learning setups, and surpasses the upper bounds in knowledge transfer capabilities.

# Related Work

# Semantic Parsing with LLMs

The purpose of semantic parsing is to simplify data access in relational databases for users. Most previous research (Cai et al. 2022; Li et al. 2023a; Dou et al. 2023) relies on sequence-to-sequence architectures needing fine-tuning on large datasets. However, SQL annotation of natural language is costly and requires domain-specific experts. Recently, with the rise of LLMs like GPT-4 (OpenAI et al. 2024) and PaLM-2 (Anil et al. 2023), some methods have leveraged in-context learning to achieve SOTA performance and reduce labeled data needs (Pourreza and Rafiei 2023; Li et al. 2023b; Mao et al. 2024). However, relying on closedsource LLMs poses database security and privacy risks (Xue et al. 2024), incurs high inference costs, and hinders continual learning due to their black-box nature (Li et al. 2024). Unlike previous work, we leverage LLMs to guide smaller models in dynamically adapting to different tasks in CSP and facilitate capability transfer. Notably, to better meet security and cost requirements in real-world applications, we focus on open-source LLMs that can be deployed offline.

# Continual Learning

A significant feature of human intelligence is the ability to learn new tasks while accumulating experience and preventing the forgetting of old knowledge, a concept known as continual learning (d’Autume et al. 2019; Wang et al. 2023a). There are two main challenges: (1) Preventing catastrophic forgetting. (2) Enabling forward transfer of knowledge from past to new tasks. Previous research on continual learning has primarily focused on classification tasks (Han et al. 2020; Wang et al. 2021a; Razdaibiedina et al. 2023). Recently, with advancements in Pre-trained Language Models (PLMs), it has also garnered widespread attention in complex tasks such as generative tasks (Chen et al. 2023a,b; Yadav et al. 2023; Zhao et al. 2024; Liang et al. 2024). SFNET (Chen et al. 2023a) adopts a semi-supervised learning approach, expanding data through self-training on additional unsupervised data. It also designs specific strategies for replaying historical instances to mitigate catastrophic forgetting. C3 (Chen et al. 2023b) freezes the core parameters of the PLM while training small-scale prompts and performing in-context tuning with relevant instances, ideally preventing forgetting issues. In this study, we focus on more realistic and challenging CSP scenarios, without accessing any historical data or relying on ideal task settings.

# Preliminaries

Given a natural language question $\mathcal { Q }$ and a database schema $\begin{array} { r l r } { { \cal S } } & { { } = } & { ( { \cal C } , { \cal T } ) } \end{array}$ consisting of columns $\begin{array} { r l } { \mathcal { C } } & { { } = } \end{array}$ $\{ c _ { 1 } ^ { t _ { 1 } } , c _ { 2 } ^ { t _ { 1 } } , . . . , c _ { 1 } ^ { t _ { 2 } } , c _ { 2 } ^ { t _ { 2 } } , . . . \}$ and tables $\mathscr { T } = \{ t _ { i } \} _ { i = 1 } ^ { | \mathcal { T } | }$ . The goal of the semantic parsing task is to generate the corresponding SQL query $\bar { \mathcal { V } } = \bar { \mathcal { F } _ { \theta } } ( \mathcal { Q } , S )$ , where $\mathcal { F } _ { \theta }$ is the parser with

(c) Inter-Task Memory Completion QuSeQstLio:n: SWEhLicEhCcToCuonutnrtyrydFoeRsOAMirAliInReLI"NJeEtSBlWuHeEAiRrEwaAiyrsli"neb $\mathbf { \Sigma } = \mathbf { \Sigma }$ ‘oJnetgBltuoe?  ( ) Flight Prompt LLM Generate Candidate MeSmelfo-rcyorCreaclitibornation   
Flight samples (a) Domain Information Elimination Question: Show me how many flights land at each airport. QuSeQstLio:n: SWEhLicEhC[TC[OCLO1]L1d]oeFsR[OCOML[2T]A[VB1A]L1W] bHeEloRnEg[tCoO?L2] [VAL1] SQL: SDeEsLtAEirCpTo rDtestAirport, COU…NT(\*) FROM flights GROUP BY $\mathbf { \Sigma } = \mathbf { \Sigma }$ (b) Component Bias Analysis (d) Intra-Task Memory Reinforcement Clusters 山 ELECT [COL1S]QFLRSOkMe l[eTtAoBn1(]   WH)ERE [COL2] ( ) + Slot filling Csanmdipdlaetse Check & Diversity 回 $\mathbf { \Sigma } = \mathbf { \Sigma }$ Flight Paraphrase LLM Filter [SCEOLLE1C],T [OCRODLE1]R,  BFYROCMOU[TNATB(\*1)]  DGERSOCULPIBMYIT 1 Paraphrase : What is the abbreviation for "United Airlines"? SQL: SELECT Abbreviation FROM AIRLINES WHERE   
(1) , (2) ,..., (t −1)  ( ) = ( (1)  (2) ...  (t −1) ) − ( ) Airline $\mathbf { \Sigma } = \mathbf { \Sigma }$ ‘United Airlines ：

parameters $\theta$ , and $\mathcal { C }$ and $\tau$ are respectively the sets of column names and table names, with $c _ { i } ^ { t _ { j } }$ indicating the $i$ -th column in table $t _ { j }$ . Following the problem definition of previous works (Li, Qu, and Haffari 2021; Lialin et al. 2021; Chen et al. 2023a,b), we assume that the semantic parser in CSP is not confined to a single specific train and test set. Instead, it is required to handle a series of semantic parsing tasks $\{ \mathcal { D } ^ { 1 } , \mathcal { D } ^ { \dot { 2 } } , . . . , \mathcal { D } ^ { M } \}$ , referred to as a task stream. Each task ${ \mathcal { D } } ^ { i }$ originates from a different database domain, formally, for $\forall \mathcal { D } ^ { i }$ and $\forall \mathcal { D } ^ { j }$ , if $i \neq j$ , then ${ \cal S } ( D ^ { i } ) \cap { \cal S } ( D ^ { j } ) = \emptyset$ , where $S ( \mathcal { D } ^ { i } )$ represents the set of all database schemas $s$ in task ${ \mathcal { D } } ^ { i }$ . Additionally, each task ${ \mathcal { D } } ^ { i }$ has its own training, validation, and test sets. The primary objective of CSP is to develop a semantic parser that not only performs excellently on previous tasks but also effectively generalizes to future unseen tasks after being sequentially trained on all tasks.

# Methodology

Our method consists of two main stages: memory reconstruction and dual distillation learning. In the first stage (Figure 2), when a new task arrives, domain-specific information is first masked to eliminate any irrelevant context, and component biases are computed to help the model connect with prior knowledge from an SQL syntax perspective, identifying the key knowledge features of the current task and its differences from previous tasks. Then, intra-task memory is reinforced, memory gaps between tasks are filled, and memory is calibrated. In the second stage (Figure 3), after acquiring key memories, a task-aware dual-teacher distillation learning framework is designed to facilitate the accumulation and transfer of memory in the continuous learning process.

# Reconstruct Memory with LLMs

Domain Information Elimination To mitigate the impact of database domain differences between tasks, we remove domain-specific information from each sample, as detailed in Figure 2(a). This allows us to uncover the commonalities and differences between individual tasks from a syntax perspective. Specifically, following previous works ( $\mathrm { \Delta Y u }$ et al. 2021; Li et al. 2023a), we first employ the string matching method to establish entity links between questions and SQL based on the database schema $s$ . Subsequently, we mask all linked entities in the question to obtain ${ \dot { \mathcal { Q } } } ^ { d e }$ , and retain only the original syntactic structure in SQL to get $\mathcal { Z }$ , which we named SQL skeleton. Finally, we merge these two parts to form the joint input pair $( \mathcal { Q } ^ { d e } , \mathcal { Z } )$ for further analysis.

Component Bias Analysis After the above process, we analyze component bias in the current task $\mathbf { \bar { \mathcal { D } } } ^ { t }$ and all historical tasks $\cup _ { i = 1 } ^ { t - 1 } { \mathcal { D } } ^ { i }$ , identifying features that present in historical tasks but are absent in the current one, as shown in Figure 2(b). Specifically, given the program-like attributes of SQL, we use CodeT5 (Wang et al. 2021b) to encode $( \mathcal { Q } ^ { d e } , \mathcal { Z } )$ from all training samples of the current task and apply $K$ -means clustering to the obtained representations. We then extract the SQL skeleton $\mathcal { Z }$ corresponding to the sample closest to each cluster center, forming the component feature set for task $\mathcal { D } ^ { t }$ , denoted as $\boldsymbol { \mathcal { A } ^ { ( t ) } } \boldsymbol { \stackrel { \mathbf { \lambda } } { = } }$ $\{ \mathcal { Z } _ { 1 } ^ { ( t ) } , \bar { \mathcal { Z } } _ { 2 } ^ { ( t ) } , . . . , \mathcal { Z } _ { K } ^ { ( t ) } \}$ , where $K$ is the number of cluster centers. Finally, we select individuals that are present in the stored component feature sets of all previous tasks $\cup _ { i = 1 } ^ { t - 1 } { \mathcal { D } } ^ { i }$ but absent in the current task $\mathcal { D } ^ { t }$ , forming the component bias $\Delta \mathcal { A } ^ { ( t ) }$ . This bias is then utilized to guide subsequent memory reconstruction, mitigating the model’s forgetting of these crucial features. When $t > 1$ , the formula is as follows:

$$
\Delta \mathcal { A } ^ { ( t ) } = ( \mathcal { A } ^ { ( 1 ) } \cup \mathcal { A } ^ { ( 2 ) } \cup . . . \cup \mathcal { A } ^ { ( t - 1 ) } ) - \mathcal { A } ^ { ( t ) }
$$

Importantly, to get $\Delta \mathcal { A } ^ { ( t ) }$ , we need to save each task’s $\mathbf { \mathcal { A } } ^ { ( t ) }$ , which only contains the SQL skeleton related to syntax and does not involve any real historical database information or samples, ensuring minimal storage costs. This highlights our method’s feature of not requiring data replay.

Inter-Task Memory Completion We employ the obtained component bias $\Delta \mathcal { A } ^ { ( t ) }$ to guide LLMs in generating pseudo-samples for the current task, as shown in Figure 2(c). This process aims to help the model fill in the memory gaps between the current and past tasks, while also uncovering knowledge relevant to the current domain from LLMs. Unlike previous strategies that generate pseudo-data through semi-supervised learning methods such as selftraining (Chen et al. 2023a), these methods require collecting task-related questions in advance and then constructing pseudo-labels for them. In reality, obtaining accurate pseudo-labels for complex tasks like semantic parsing, relying solely on limited annotated data, is itself a challenge for most LLMs (Li et al. 2024). Therefore, we propose a soft pseudo-sample construction strategy that uses SQL skeletons to guide LLMs to simultaneously generate the natural language question $\hat { \mathcal { Q } }$ and corresponding SQL query $\hat { \mathcal { Y } }$ on the current task’s database schema $\bar { \boldsymbol { S } } ^ { ( t ) }$ . The generation process can be summarized as follows:

$$
( \hat { \mathcal { Q } } , \hat { \mathcal { Y } } ) ^ { ( t ) } = \left\{ \begin{array} { l l } { \mathcal { F } _ { \mathrm { L L M } } \big ( \mathcal { A } ^ { ( t ) } , S ^ { ( t ) } \big ) , } & { \mathrm { i f ~ } t = 1 , } \\ { \mathcal { F } _ { \mathrm { L L M } } \big ( \Delta \mathcal { A } ^ { ( t ) } , S ^ { ( t ) } \big ) , } & { \mathrm { i f ~ } t > 1 . } \end{array} \right.
$$

Where $( \hat { \mathcal { Q } } , \hat { \mathcal { V } } ) ^ { ( t ) }$ refers to the pseudo-samples generated by LLMs. On the first task, we use SQL skeletons $\mathbf { \mathcal { A } } ^ { ( t ) }$ for better initialization. Each SQL skeleton generates $N _ { s k e }$ data.

Inter-Task Memory Calibration Although our proposed soft pseudo-sample construction strategy avoids the challenge of directly requiring the model to generate complex SQL queries and further leverages the generative capabilities of LLMs, it also faces the issue of hallucinations (Ji et al. 2023), which can lead to noise. To address this, we design a memory calibration strategy that includes two stages: iterative self-correction and SQL skeleton-based sampling. This aims to improve the accuracy of generated pseudo-samples and ensure fidelity to specific SQL skeletons.

The inspiration for iterative self-correction comes from previous works that used LLMs for code debugging and fixing (Pourreza and Rafiei 2023; Chen et al. 2024). We iteratively execute SQL queries and correct the generated pseudo-samples, ultimately retaining only the samples that LLMs consider correct and whose SQL queries can be executed successfully. Additionally, we sample pseudo-samples that best match the specified SQL skeleton by calculating the edit distance after removing domain information.

Intra-Task Memory Reinforcement Following previous work (Yu et al. 2021), we introduce a data synthesis method based on context-free grammar, utilizing the component features $\mathbf { \mathcal { A } } ^ { ( t ) }$ of the current task $\mathcal { D } ^ { t }$ to enhance memory. Specifically, for each annotated sample, we synthesize $N _ { c f g }$ new instances $\hat { X } _ { c f g } ^ { ( t ) }$ by synchronously replacing entities in the natural language question $\mathcal { Q }$ and SQL query $y$ with other database content based on entity linking results, as illustrated in Figure 2(d). To ensure quality, only SQL queries that pass execution tests are retained. Additionally, we use LLMs to rephrase questions to enhance data diversity.

![](images/0bcfc7a851a3782f35cc603483e25901f6290d9a422ebb06e04d8beba65646a0.jpg)  
Figure 3: The task-aware dual-teacher distillation learning framework of LECSP.

# Task-Aware Dual-Teacher Distillation Learning Framework

After reconstructing the relevant inter-task and intra-task memories, we design a task-aware dual-teacher distillation framework to promote the efficient utilization of these memories. The training framework is shown in Figure 3. Specifically, for each task, a Student model is trained to learn from two Teacher models: one from the LLM and the other from the previous student.

LLMs as Teacher 1 This process facilitates knowledge transfer from the LLMs (Teacher $^ { l }$ ) to the smaller model (Student) via reconstructed memory, consisting of two types of pseudo-data: $\hat { X } _ { s k e }$ and $\hat { X } _ { c f g }$ . They respectively represent shared knowledge accumulated from historical tasks and specific domain knowledge related to the current task. We utilize these pseudo datasets for additional supervised training of the smaller student model. The loss function is defined as follows:

$$
\mathcal { L } _ { \mathrm { c u r / p a s t } } = \frac { 1 } { N ^ { c / s } } \sum _ { i = 1 } ^ { N ^ { c / s } } \ell ( \mathcal { F } _ { \boldsymbol { \theta } } ( \hat { x } _ { i } ^ { c / s } ) , \hat { y } _ { i } ^ { c / s } )
$$

Where $N ^ { c } , N ^ { s }$ respectively represent the number of data generated for types $\hat { X } _ { c f g }$ and $\hat { X } _ { s k e }$ . $\hat { x } _ { i }$ and $\hat { y } _ { i }$ with different superscripts denote the input and target output for the corresponding data types. $\ell$ means the cross-entropy loss.

Previous Student as Teacher 2 Although $\hat { X } _ { s k e }$ already covers knowledge from past tasks that is missing in the current task, this knowledge is primarily derived from indirect transfer by the teacher LLM, which may affect the stability and introduce inherent biases from the LLM. Therefore, we introduce another Teacher 2 (the student model from the previous task) to directly constrain the learning of the current student. We encourage both models to maintain consistent representations of the shared knowledge in memory $\hat { X } _ { s k e }$ .

Specifically, inspired by previous work (Qin and Joty 2022), after freezing the parameters of the student model from the previous task, we use KL divergence as the distillation loss to promote consistency in the output distributions of the two models. The specific process is as follows:

$$
\mathcal { L } _ { p a s t } ^ { K L } = \frac { 1 } { N ^ { s } } \sum _ { i = 1 } ^ { N ^ { s } } \sum _ { j = 1 } ^ { l } D _ { \mathrm { K L } } ( P _ { j } ( \hat { x } _ { i } ^ { s } , \theta ^ { \prime } ) | | P _ { j } ( \hat { x } _ { i } ^ { s } , \theta ) )
$$

Where $l$ is the number of tokens in the output $\hat { y } _ { i } ^ { s }$ corresponding to the input $\hat { x } _ { i } ^ { s }$ , and $P _ { j } ( \cdot )$ denotes the probability distribution over the vocabulary of model $\mathcal { F } ( \cdot )$ when generating the $j$ -th token. $\theta$ and $\theta ^ { \prime }$ respectively represent the parameters of the current and previous task student models. Note that we do not calculate $\dot { \mathcal { L } } _ { p a s t } ^ { K L }$ on the first task.

Total loss function The loss of the student model on the original annotated data follows a similar calculation process as previously described, and it can be represented as $\begin{array} { r } { \mathcal { L } _ { t a s k } = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \ell ( \mathcal { F } _ { \theta } ( x _ { i } ) , y _ { i } ) } \end{array}$ , $N$ is the count of original annotated data. The final loss is as follows, where $\lambda$ represents the weight factor:

$$
\begin{array} { r } { \mathcal { L } = \mathcal { L } _ { t a s k } + \mathcal { L } _ { c u r } + \mathcal { L } _ { p a s t } + \lambda \mathcal { L } _ { p a s t } ^ { K L } } \end{array}
$$

# Experiments

Datasets We evaluate our method on the publicly available Spider-stream-semi (Chen et al. 2023a) and Combinedstream (Chen et al. 2023b) datasets. Spider-stream-semi is derived from Spider dataset and consists of 10 tasks involving complex SQL syntax such as JOIN and GROUP BY. Each task also provides unlabeled data intended for semisupervised learning baselines, but our approach does not require these data. Most tasks have fewer than 500 annotated samples to evaluate the impact of database domain changes under limited resources. Combined-stream consists of Spider and WikiSQL datasets, with a total of 7 tasks. Unlike Spider-stream-semi, it introduces simple SQL syntax for single-table queries to assess the dual impact of table domain and SQL structure changes. In previous works, to ensure good initial performance, both Spider-stream-semi and Combined-stream set the task with the most annotated data as the first task, termed a warm start. Conversely, we also introduce a variant called cold start, which is more reflective of real-world scenarios. In this variant, tasks are randomly shuffled without assigning the first task the most annotated data. This helps us further study the effects of initial performance and task order on the model.

Evaluation Metrics Following previous works (Yu et al. 2018; Chen et al. 2023a,b), we define $a _ { m , n }$ as the model’s accuracy on the $n$ -th task’s test set after training on the $m$ -th task, split into Exact-set-Match (EM) and EXecution (EX) accuracy. EM assesses the formal accuracy of SQL queries racy: ACCa = M1 PiM=1 aM,i. It reflects the model’s overall $\begin{array} {{ r } { \mathsf { A C C } } _ { \mathrm { w } } \ = \ a _ { \mathcal { D } _ { t e s t } ^ { ( 1 : M ) } } } \end{array}$ , awllheirset $\mathcal { D } _ { t e s t } ^ { ( 1 : M ) } ~ = ~ \cup _ { i = 1 } ^ { M } \mathcal { D } _ { t e s t } ^ { i }$ a.cIcturmaecay-: sures accuracy on all tasks’ combined test sets after training on task $M$ , reflecting historical performance changes.

(3) Backward transfer: $\begin{array} { r } { \mathbf { B W T } = \frac { 1 } { M - 1 } \sum _ { i = 1 } ^ { M - 1 } ( a _ { M , i } - a _ { i , i } ) } \end{array}$ It assesses the average impact of learning the $M$ -th task on the performance of all previous tasks. (4) Forward transfer: $\begin{array} { r } { \mathbf { F W T } = \frac { 1 } { M - 1 } \sum _ { i = 2 } ^ { M } ( \bar { a _ { i - 1 , i } } - \hat { a } _ { i } ) } \end{array}$ , where $\hat { a } _ { i }$ represents the test accuracy of a randomly initialized reference model on the $i$ -th task, examining its generalization performance.

Implementation Details In our experiments, we utilize T5-base and T5-large as backbone models, applying the Adafactor (Shazeer and Stern 2018) optimizer. The maximum input and output lengths are configured to 512 and 256, respectively. In the component bias analysis, we choose CodeT5 as the encoder and set the number of cluster centers $K$ to 80. In the data generation process, we set $N _ { s k e }$ as 10 and $N _ { c f g }$ as 3. The weight hyperparameter $\lambda$ in the training process undergoes grid search on the set $\{ 0 . 0 3 , 0 . 0 5$ , $0 . 1 , 0 . 2 , 0 . 3 \}$ , and 0.1 is selected. For LLMs, We employ the publicly available Mixtral- $8 \mathrm { x } 7 \mathrm { B }$ -Instruct-v0.1, which is a sparse mixture of experts’ language model. All experiments are performed on A100 GPUs with 80GB memory.

Baselines Our proposed method is compared with three types of baselines. (1) Original method: Sequential FineTuning (SFT) (Yogatama et al. 2019). (2) Rehearsal-based methods: EMAR (Han et al. 2020), SFNET (Chen et al. 2023a). (3) PET-based methods: PEFT (Chen et al. 2023b), C3 (Chen et al. 2023b), ProgPrompt (Razdaibiedina et al. 2023). Since our method uses LLMs as Teacher 1 for zeroshot memory construction, we also report the zero-shot performance of a single LLM for comparison. The ORACLE setting trains the model incrementally on all seen tasks’ data, representing the CSP performance upper bound.

# Results and Analysis

# Overall Results

Table 1 shows the performance of each method across various backbones and benchmarks, noting whether data replay strategies are used. We also list baselines under other configurations, such as ideal settings, LLM integration, and ORACLE performance. The results show that our method is competitive across all scenarios, requiring no external data or ideal settings, and better adapts to real-world applications.

Comparison with PET-based Methods LECSP outperforms PEFT by up tp $1 1 . 1 \%$ on $\mathsf { A C C } _ { \mathrm { a } }$ . PEFT uses an ideal continual learning setup that sacrifices model generalization, as it is challenging to select the appropriate PET module for unseen samples, making FWT calculation impossible. We also note that ProgPrompt does not require ideal settings, but its performs worse than SFT. This is because ProgPrompt is designed for tasks with simpler output structures, such as classification. However, for models pre-trained on natural language corpora, adapting to complex tasks like semantic parsing with limited-length prompts is challenging (Qin and Joty 2022). The progressive prompt design of ProgPrompt further compresses the prompt length for individual tasks, worsening the issue and making it nearly non-functional in cold start scenarios. Notably, PET-based methods with

<html><body><table><tr><td rowspan="2">Method</td><td rowspan="2">DR</td><td colspan="4">Spider-stream-semi</td><td colspan="5">Combined-stream</td></tr><tr><td>ACCa</td><td>ACCw</td><td>BWT FWT</td><td>△</td><td>ACCa</td><td>ACCw</td><td>BWT</td><td>FWT</td><td>△</td></tr><tr><td colspan="10">T5-Base(220M)</td></tr><tr><td>SFT</td><td>X</td><td>41.7/45.3</td><td>40.1/43.7</td><td>-13.6/-11.7</td><td>22.5/25.2</td><td>↓1.5 ↓14.9</td><td>51.0/36.0 52.4/38.3</td><td></td><td>-11.1/-17.6</td><td>19.9/11.1 ↓11.1</td></tr><tr><td>ProgPrompt</td><td>X</td><td>14.9/16.5</td><td>14.3/16.4</td><td>-25.6/-26.3</td><td>14.7/15.8</td><td>21.3/19.2</td><td>27.2/26.8</td><td>-43.9/-41.2</td><td>6.1/3.3</td><td>↓21.1</td></tr><tr><td>EMAR</td><td>√</td><td>44.1/48.0</td><td>43.0/47.0</td><td>-11.8/-9.7</td><td>23.7/25.2</td><td>↑2.3 60.6/58.9</td><td>55.7/55.3</td><td>-7.9/-7.1</td><td>28.7/25.4</td><td>↓5.6</td></tr><tr><td>SFNET*</td><td>X</td><td>39.5/43.9</td><td>38.1/42.7</td><td>-14.6/-12.4</td><td>22.7/25.7</td><td>↑0.4 56.2/49.4</td><td>51.8/48.4</td><td>-7.9/-14.4</td><td>21.0/14.6</td><td>↓21.7</td></tr><tr><td>SFNET+</td><td>√</td><td>45.8/48.1</td><td>44.2/47.7</td><td>-6.4/-6.4</td><td>23.9/26.2</td><td>↓3.4 60.7/57.7</td><td>55.2/53.9</td><td>-6.0/-6.7</td><td>28.7/26.0</td><td>↓6.7</td></tr><tr><td>LECSP(Ours)</td><td>X</td><td>47.4/53.7</td><td>46.5/53.3</td><td>-4.7/-4.6</td><td>30.1/32.8</td><td>63.4/61.7</td><td>60.5/59.8</td><td>-4.6/4.3</td><td>32.7/31.5</td><td>↓4.8</td></tr><tr><td colspan="10">IdealSetting&UpperBound</td></tr><tr><td>PEFT</td><td>X</td><td>40.6/43.8</td><td>44.6/48.4</td><td>0.0/0.0</td><td>-/-</td><td>↓32.5 63.8/-</td><td>66.2/-</td><td>0.0/0.0</td><td>-/-</td><td>↓38.8</td></tr><tr><td>ORACLE</td><td></td><td>60.3/61.3</td><td>62.8/63.9</td><td>4.6/3.3</td><td>25.7/27.9</td><td>↓1.1 70.2/68.2</td><td>71.1/70.8</td><td>5.6/4.7</td><td>29.2/26.6</td><td>↑0.3</td></tr><tr><td colspan="10">T5-Large(770M)</td></tr><tr><td>SFT</td><td>X</td><td>46.4/50.4</td><td>44.3/48.9</td><td>-13.2/-11.5</td><td>29.0/31.5</td><td>↑1.8</td><td>57.8/48.9 58.9/53.3</td><td>-7.6/-16.4</td><td>28.3/25.4</td><td>↓8.3</td></tr><tr><td>ProgPrompt</td><td>X</td><td>22.1/23.9</td><td>22.4/24.6</td><td>-27.0/-27.5</td><td>21.1/23.1</td><td>↓20.8 23.5/20.2</td><td>29.8/27.7</td><td>-46.4/-42.7</td><td>10.2/3.7</td><td>↓22.4</td></tr><tr><td>EMAR</td><td>√</td><td>48.9/51.8</td><td>48.4/52.5</td><td>-8.5/-8.3</td><td>30.3/33.5</td><td>↑1.4</td><td>63.9/61.9</td><td>60.6/60.4</td><td>-6.6/-8.9</td><td>33.8/29.3 ↓5.5</td></tr><tr><td>SFNET*</td><td>X</td><td>44.2/48.2</td><td>43.3/47.8</td><td>-12.3/-9.9</td><td>29.9/31.0</td><td>↑3.3</td><td>65.0/60.3 61.5/59.7</td><td>-7.5/-11.9</td><td>28.2/20.4</td><td>↓24.1</td></tr><tr><td>SFNET*</td><td>√</td><td>52.2/55.0</td><td>52.0/55.8</td><td>-6.5/-4.2</td><td>34.1/35.5</td><td>↓2.2 65.3/64.7</td><td>61.8/61.9</td><td>-5.7/-5.2</td><td>34.9/31.1</td><td>↓5.6</td></tr><tr><td>LECSP(Ours)</td><td>X</td><td>58.6/63.5</td><td>57.4/63.8</td><td>-5.8/-5.0</td><td>41.4/44.3</td><td>68.2/66.7</td><td>66.5/65.4</td><td>-2.9/-3.7</td><td>37.1/34.5</td><td>↓5.1</td></tr><tr><td colspan="10">Ideal Setting & LLMs Performance & Upper Bound</td></tr><tr><td>PEFT</td><td>X</td><td>49.6/52.4</td><td>53.4/56.4</td><td>0.0/0.0</td><td>-/-</td><td>↓24.1</td><td>67.3/-</td><td>70.0/-</td><td>0.0/0.0</td><td>↓23.4</td></tr><tr><td>C3</td><td>X</td><td>-/-</td><td>--</td><td>-/-</td><td>-/-</td><td></td><td>69.0/- 71.2/-</td><td>0.0/0.0</td><td>-/-</td><td></td></tr><tr><td>C3+</td><td></td><td>-/-</td><td>-/-</td><td>-/-</td><td>-/-</td><td>67.6/-</td><td>70.0/-</td><td>0.0/0.0</td><td>-/-</td><td>1</td></tr><tr><td>Mixtral-8x7B</td><td>X</td><td>22.0/49.4</td><td>22.2/52.4</td><td>-/-</td><td>-/-</td><td>28.6/24.4 ↑0.4</td><td>27.0/26.0</td><td>-/-</td><td>-/-</td><td>，</td></tr><tr><td>ORACLE</td><td></td><td>66.6/68.2</td><td>68.6/70.1</td><td>3.7/4.0</td><td>34.3/36.0</td><td>73.7/73.2</td><td>75.8/76.0</td><td>1.9/2.3</td><td>37.3/34.2</td><td>↓0.1</td></tr></table></body></html>

Table 1: Results (EM/EX) on Spider-stream-semi and Combined-stream datasets $( \% )$ . LECSP uses Mixtral-8x7B as Teacher 1. DR indicates whether historical data replay is used, with a memory size set to 10. $\Delta$ represents the performance change ( $\mathrm { \ A C C _ { a } }$ –EM) from the current warm start to the cold start scenario. $\clubsuit$ indicates the need for additional unsupervised data, and $\diamondsuit$ means an ideal continuous learning setting is used, but forward transfer is impossible. The results of PEFT and C3 on the Combined-stream are from the original paper and its official code repository. $\dagger$ signifies the use of GPT-3.5 as the teacher. The best results are highlighted in bold, and the second-best results are underlined. Our results are the average of three random runs.

ideal settings, like PEFT, heavily rely on the initial task’s labeled data. In cold start scenarios, their performance drops sharply, with a maximum decline of $3 8 . 8 \%$ .

Comparison with Rehearsal-based Methods Compared to the previous SOTA method SFNET, LECSP achieves up to an $8 . 8 \%$ improvement in FWT on Spider-stream-semi, despite SFNET using additional unlabeled data and data replay strategies. This improvement is attributed to LECSP’s efficient use of knowledge from learned tasks and LLMs. Notably, SFNET (w/o DR) performs worse than SFT, likely due to the exacerbated noise from pseudo-labels in selftraining, highlighting its reliance on data replay. In contrast, our method operates without relying on any external data.

Comparison with ORACLE and LLMs Impressively, our method outperforms most ORACLE baselines in FWT performance, with up to an $8 . 3 \%$ improvement, demonstrating effective knowledge transfer from LLMs to smaller models and enhancing their generalization ability. In contrast, the zero-shot performance of standalone LLMs is poor, even worse than the naive SFT baseline, and continuous training of LLMs is often challenging and resource-intensive, further highlighting the importance of our approach. By combining models of different scales, we effectively address this issue, carefully balancing cost and performance.

# Detailed Results and Analysis

Results Till the Seen Tasks Figure 4 shows that as task numbers increase, LECSP (red) consistently outperforms other baselines in $\mathbf { A C C _ { a } }$ and $\mathsf { A C C } _ { \mathrm { w } }$ , closely approaching the ORACLE (grey) and effectively mitigating forgetting. It also maintains stable BWT performance, especially in the early stages, thanks to its ability to quickly connect with previously learned knowledge. Notably, LECSP surpasses ORACLE in FWT, highlighting effective knowledge accumulation and transfer from LLM and past tasks.

Influence of Task Order and Cold Start Unlike previous works that start with data-rich tasks, we further explore the cold-start scenario with random task order and limited data. The results in Table 1 show a significant performance drop for most baselines, especially PET-based methods like PEFT and ProgPrompt, reflecting their strong dependence on labeled data of the initial task. In contrast, LECSP is less affected in this challenging scenario, demonstrating strong robustness and maintaining a significant advantage.

Influence of SQL Syntax Variance To further explore the role of SQL syntax variance in LECSP, we compare the impact on model performance between our method and randomly obtained component bias. Specifically, we swap the memories constructed under different orders of Spiderstream-semi and Spider-stream-semi (cold start), keep other experimental settings unchanged, and retrain the models. The results in Table 2 show that, after the exchange, model performance decreases on most metrics but improves in FWT on $C _ { \mathrm { { S } } }$ . This improvement may be due to the introduction of unseen SQL syntax, enhancing generalization.

![](images/47d78155fa7a8ba6e2264f87d4d4601712ecc5bd00ca136eddf5bf9fcb0c3a94.jpg)  
Figure 4: Results (EX) till the seen tasks based on Spider-stream-semi (T5-large).

![](images/c756fb7ddfadd9f56e2ee4d6c6b5e02bef4fe5ce2bb18aeda97dfbd89e000cac.jpg)  
Figure 5: (a) Accuracy of synthetic data execution across different stages. (b) Impact of pseudo-sample quantity on different metrics (T5-base).

Table 2: The impact of SQL syntax variance on model performance $( \% ) . \ s _ { \mathrm { s } }$ denotes using component bias from Spider-stream-semi itself, as does $\mathbf { C } _ { \mathbf { C } }$ . $\mathsf { S } _ { \mathsf { C } }$ indicates using component bias from Spider-stream-semi (Cold start) to construct memory for Spider-stream-semi, and vice versa.   

<html><body><table><tr><td></td><td>ACCa</td><td>ACCw</td><td>BWT</td><td>FWT</td></tr><tr><td>Ss</td><td>47.4/53.7</td><td>46.5/53.3</td><td>-4.7/-4.6</td><td>30.1/32.8</td></tr><tr><td>Sc</td><td>45.0/49.1</td><td>43.7/48.0</td><td>-4.5/-5.7</td><td>28.1/31.7</td></tr><tr><td>Cc</td><td>48.8/52.6</td><td>49.9/54.2</td><td>-1.1/-0.9</td><td>24.8/27.3</td></tr><tr><td>Cs</td><td>43.6/48.1</td><td>44.2/49.2</td><td>-4.4/-5.6</td><td>27.1/30.2</td></tr></table></body></html>

Quality and Quantity of Pseudo-Samples To evaluate the quality of generated memory (pseudo-samples), we compare our method with the self-training method used in SFNET (T5-large) and the zero-shot results of Mixtral8x7B, both using additional unsupervised data. We divide the original Spider-stream-semi into early, mid, and late stages, randomly selecting 70 pseudo-samples from each stage to compare SQL execution accuracy. For our method, we use manual evaluation, whereas for the other two methods, we rely on their original labels. The results in Figure 5(a) show that our method significantly improves the quality of pseudo-samples, while the self-training method in SFNET is less stable. Figure 5(b) shows that as the proportion of pseudo-samples increases, all metrics generally improve, but several metrics stabilize in the later stages.

Table 3: Ablation study results on Spider-stream-semi.   

<html><body><table><tr><td></td><td>ACCa</td><td>ACCw</td><td>BWT</td><td>FWT</td></tr><tr><td>LECSP</td><td>47.4/53.7</td><td>46.5/53.3</td><td>-4.7/-4.6</td><td>30.1/32.8</td></tr><tr><td>-Xcfg</td><td>45.6/50.3</td><td>44.3/49.1</td><td>-5.6/-6.6</td><td>27.7/30.7</td></tr><tr><td>- MC</td><td>20.3/22.9</td><td>19.4/21.9</td><td>0.4/-0.6</td><td>15.8/17.5</td></tr><tr><td>- T2</td><td>46.6/52.8</td><td>45.5/51.8</td><td>-6.6/-5.9</td><td>30.4/33.3</td></tr><tr><td>-Xske</td><td>41.7/45.3</td><td>40.1/43.7</td><td>-13.6/-11.7</td><td>22.5/25.2</td></tr></table></body></html>

Ablation Study We decompose LECSP (T5-base) and sequentially remove each component to assess its impact on performance. Table 3 shows that the full LECSP achieves the best overall performance. Omitting intra-task memory $\hat { X } _ { c f g }$ or inter-task memory $\hat { X } _ { s k e }$ reduces performance, with the latter significantly improving forgetting mitigation, with gains of $5 . 7 \% - 9 . 6 \%$ , as it retains SQL knowledge from previous tasks. Removing the Memory Calibration (MC) strategy leads to a sharp performance drop of up to $3 1 . 4 \%$ , highlighting the importance of MC in mitigating hallucinations in LLM-generated memories (Ji et al. 2023). Ignoring Teacher 2 (T2) leads to an overall performance decline $( 0 . 8 \% - 1 . 9 \% )$ despite minor gains on FWT $( 0 . 3 \% { - 0 . 5 \% } )$ .

# Conclusion

In this paper, we introduce LECSP, a novel CSP method without real data replay or ideal settings. To handle incoming new tasks, LECSP extracts key SQL syntax features and analyzes the commonalities and differences with historical task knowledge, guiding LLMs to generate pseudo-samples to reconstruct key memories. We also design a task-aware dual-teacher distillation framework to transfer knowledge from the LLM and previous tasks to a smaller model. Experimental results show that, even with auxiliary strategies in the baseline, LECSP achieves competitive performance and overcomes certain performance limits, demonstrating strong adaptability to challenging real-world scenarios.