# BLADE: Enhancing Black-Box Large Language Models with Small Domain-Specific Models

Haitao $\mathbf { L I } ^ { 1 , 2 }$ Qingyao $\mathbf { A } \mathbf { I } ^ { 1 , 2 * }$ , Jia CHEN3, Qian DONG1,2, Zhijing WU4, Yiqun LIU1,2†

1Department of Computer Science and Technology, Tsinghua University, Beijing, China 2 Institute for Internet Judiciary, Tsinghua University, Beijing, China 3 Xiaohongshu Inc 4 School of Computer Science and Technology, Beijing Institute of Technology liht22@mails.tsinghua.edu.cn

# Abstract

Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable of addressing open-domain questionanswering(QA) tasks effectively. However, general LLMs, which are developed on open-domain data, may lack the domain-specific knowledge essential for tasks in vertical domains, such as legal, medical, etc. To address this issue, previous approaches either conduct continuous pre-training with domain-specific data or employ retrieval augmentation to support general LLMs in handling QA tasks. Unfortunately, these strategies are either cost-intensive or unreliable in practical applications. To this end, we present a novel framework named BLADE, which enhances Black-box LArge language models with small Domain-spEcific models. BLADE consists of a black-box LLM and a small domain-specific LM. The small LM preserves domain-specific knowledge and offers specialized insights, while the general LLM contributes robust language comprehension and reasoning capabilities. Specifically, our method involves three steps: 1) pre-training the small LM with domain-specific data, 2) fine-tuning this model using knowledge instruction data, and 3) joint Bayesian optimization of the general LLM and the small LM. In our experiments, we verify the effectiveness of BLADE on diverse LLMs and datasets across different domains. This shows the potential of BLADE as an effective and cost-efficient solution in adapting general LLMs for vertical domains.

# Code — https://github.com/CSHaitao/BLADE

Recently, large language models (LLMs) have attracted considerable attention in both academia and industry (OpenAI 2023; Zeng et al. 2022). These models, driven by expansive neural networks and trained on extensive data sets, exhibit remarkable ability in comprehending and generating natural language. The wide application of LLMs trained with open-domain data, denoted in this paper as General LLMs, has profoundly impacted various aspects of daily life and professional environments.

Despite their superior capabilities, large language models often face challenges in addressing QA tasks of vertical domains (e.g., medicine, legal) that require access to a large amount of domain knowledge (Chalkidis 2023; Cheng, Huang, and Wei 2023). How to adapt general LLMs for domain-specific applications has become an important problem for the research community (Arefeen, Debnath, and Chakradhar 2023).

Existing methods for adapting general LLMs to specific domains can be broadly divided into two main categories: domain data continuous pre-training and retrieval augmentation. Continuous pre-training involves infusing domain knowledge into general LLMs by training them on a domain-specific corpus (Aharoni and Goldberg 2020; Sachidananda, Kessler, and Lai 2021). While straightforward, this paradigm requires direct access to large-scale domain data and LLM parameters, which are not available in many conditions. Also, even with access to general LLM parameters and sufficient domainspecific data, directly tuning a general LLM (such as GPT-4) can be prohibitively expensive and poses a risk of overfitting. Aware of these challenges, researchers propose retrieval augmentation as a new paradigm, aiming to enhance general LLMs by leveraging their in-context learning ability (Shi et al. 2023). It involves first using a text retriever to find relevant content from the domain corpus, which is then incorporated into the LLM’s input to aid in understanding domain-specific knowledge. However, there may exist two problems in this paradigm. For example, retrievers primarily rely on exact matches or semantic similarity, lacking inferential capabilities. This limitation means they may not always retrieve documents that fully address specific queries. Additionally, retrievers can only provide content that is already available in the corpus and lack the ability to integrate and summarize different information.

# Introduction

When humans face questions in new domains, besides taking classes (i.e., continuous pre-training) or conducting online searches via platforms like Google (i.e., retrieval augmentation), a more direct and practical approach is to seek advice from experts possessing domain-specific knowledge. With this idea in mind, we present BLADE, a novel paradigm where the general LLM is viewed as a black box and the small domain-specific LM is added as a tuneable module. BLADE leverages the superior language comprehension and logical

1. DPorme-atirna-isnpinegcific 点 3. Bayesian Prompted prompt 2. Knowledge Instruction Tuning Optimization ？ ↑ → 1 1   
Small Model Small Model Small Model Small Model Question generate feedback with question   
Domain-Specific   
Knowledge Final answer Black-Box LLM dQoumeasitinokn-norwielnetdegde

reasoning capabilities of the general LLM, while also incorporating the domain-specific expertise and precision provided by the smaller, domain-focused LM. This approach includes Domain-specific Pretraining (DP) of the smaller LM and introduces two strategies: Knowledge Instruction Tuning (KIT) and Bayesian Prompted Optimization (BPO). Knowledge Instruction Tuning leverages general LLMs to generate pseudo data, which refines the smaller LM, equipping it with instructfollowing ability. Then, the Bayesian Prompted Optimization aligns the output of small LMs with general LLMs using derivative-free optimization on soft embeddings. We verify the effectiveness of our method on public legal and medical benchmarks, which require a large amount of domain knowledge and strong reasoning abilities. The experiments show that BLADE can improve the performance of diverse general LLMs across legal and medical benchmarks. Compared with existing retrieval augmentation methods, the domain-specific small LM can generate more in-depth, comprehensive, and contextually appropriate external knowledge. This capability significantly improves the application of general LLMs in specialized domains.

# Related Work

# Domain Adaptation of LLMs

The domain adaptation of LLMs is an extensively researched field. Researchers have explored methods such as continuous pre-training, and retrieval augmentation to improve the performance of LLMs in a specific domain (Aharoni and Goldberg 2020; Li et al. 2024b). The most intuitive approach is continuous pre-training a language model on domain-specific corpora. Previous research has focused on data selection (Aharoni and Goldberg 2020) as well as adjusting or extending tokenizers (Sachidananda, Kessler, and Lai 2021) to achieve superior performance in the target domain. Despite being effective, fully training these models is often impractical due to extensive computational costs. Additionally, high-quality LLMs can only accessed through the inference API as black boxes. One possible alternative is to answer domain-specific questions by retrieving relevant information from a specific knowledge base (Borgeaud et al. 2022; Shi et al. 2023; Lewis, Stenetorp, and Riedel 2020). Retrieval augmentation has been shown to be effective in improving performance on various tasks. For instance, RETRO (Borgeaud et al. 2022) modifies the model architecture to incorporate retrieved text. Furthermore, REPLUG (Shi et al. 2023) treats the language model as a black box and enhances it using a retrieval model. Additionally, recent research has explored substituting traditional document retrievers with large language model generators (Yu et al. 2022; Li et al. 2023a; Sun et al. 2022b).

# Method

# Overview

Figure 1 illustrates the overall framework of BLADE. To be specific, BLADE solves domain-specific tasks through a collaborative approach between general black-box LLMs and small white-box LMs. General black-box LLMs, such as ChatGPT and GLM-130B, excel in reasoning and inference but are usually expensive and difficult to fine-tune in downstream applications. Conversely, small white-box LMs may be weak in sufficient reasoning ability, but can easily be adapted to memorize domain-specific knowledge. Inspired by this observation, our BLADE framework proposes to build a small LM to capture domain-specific knowledge and use it to generate tailored knowledge and prompts for general LLMs to synthesize and create responses for user queries.

# Domain-specific Pre-training (DP)

A variety of studies have shown that pre-trained language models implicitly capture high-quality substantial knowledge hidden in the training data (Wang et al. 2022; Liu et al. 2021; Lewis et al. 2020; Joshi et al. 2020). This knowledge can be elicited from the language model through instructional prompts (Liu et al. 2021). Therefore, we first inject domainspecific knowledge to our small LMs via Domain-specific Pre-training (DP). Specifically, given domain-specific unsupervised text $T = \{ \bar { t } _ { 1 } , t _ { 2 } , . . . . . , \bar { t } _ { n } \}$ , we optimize the model by maximizing the following training objective:

$$
G ( T ) = \sum _ { i } \log P \left( t _ { i } \mid t _ { i - k } , \ldots , t _ { i - 1 } ; \Theta \right) ,
$$

where $\Theta$ is the parameter of the model. $P$ is the conditional probability of generating the current token based on the previous tokens.

Prompt-based Knowledge Generation Please generate knowledge related to the following questions. The knowledge should be able to help Black-Box LLM identify the correct answer. Question: {Question} Correct answer: {Answer} Knowledge: Generated knowledge Consistency Filtering Please answer the question based on the knowledge provided. Provide Black-Box LLM the answer directly without offering X an explanation. Question: {Question} 四助 Knowledge: {Generated Knowledge} Answer: corpus delete

# Knowledge Instruction Tuning (KIT)

After domain-specific pre-training, instruction tuning is often employed to equip the LMs with the ability to follow specific instructions. However the annotation of high quality instruction fine-tuning data is usually expensive. We introduce Knowledge Instruction Tuning (KIT), which focuses on leveraging LLMs-as-Judges (Li et al. 2024c) as annotators to enhance the instruction-following capabilities of smaller language models. This process enables the small LM to leverage its inherent knowledge for a particular task and provide useful information for downstream systems.

To be specific, KIT consists of three components: Promptbased Knowledge Generation, Consistency Filtering, and Instruction Tuning. Figure 2 show the process of generating data of KIT. During Prompt-based Knowledge Generation, we use the training data from a specific domain or task (e.g., QA) to formulate training query-answer pairs. Subsequently, a general black-box LLM (e.g., ChatGPT, GPT4) is employed to generate explanations that assists in accurately answering the questions. The process creates instruction-tuning data for the small LMs where the instruction is the original training query and the desired output is the explanations we need to answer the query correctly. Since the general black-box LLM in KIT may not possess enough knowledge to answer all training query-answer pairs, specifically when the data is collected from domain-specific tasks, we filtered the generated instruction data based on round consistency, i.e., whether the general black-box LLM can produce the correct answer based on the knowledge they extracted. Consistency Filtering has been shown to be effective for query generation in QA tasks and various information retrieval tasks (Dai et al. 2022; Lewis et al. 2021). The instruction prompts are structured as depicted in Figure 2. We filter the generated knowledge and only retain the information that can help the LLM to answer questions correctly. The refined data is then used to fine-tune the small LM and teach it to produce knowledge for a given query instead of predicting the next token.

It is worth noting that the goal of the process is to extracte instruction-following capabilities from LLMs into small LM, rather than extracting specific domain knowledge from LLMs. We propose a method to efficiently generate fine-tuned instructions without manual annotation.

# Bayesian Prompted Optimization (BPO)

Now we propose the Bayesian Prompted Optimization (BPO) method that teachs the domain-specific small LM to communicate with the general LLMs. Figure 3 illustrates the process of Bayesian Prompted Optimization (BPO). To be specific, the optimization objective is to enhance the performance $f ( \cdot )$ of the general LLM on domain-specific tasks. Consider an example $( X , Y )$ from the dataset $\mathcal { T } _ { t }$ . Let $k$ represents the domain knowledge that is specific to the query $X$ . In our framework, $k$ is generated by the small domain-specific LM $g ( \cdot )$ . Let $h ( \cdot , \cdot )$ be the evaluation metric for output $f ( k , X )$ and ground truth $Y$ . For example, in multiple-choice tasks, $h ( \cdot , \cdot )$ can be accuracy. The optimization objective is to maximize the performance with appropriate knowledge, i.e.,

$$
\operatorname* { m a x } _ { k } \mathbb { E } _ { ( X , Y ) \sim \mathcal { T } _ { t } } h ( f ( [ k ; X ] ) , Y ) , \mathrm { ~ s . t . ~ } k = g ( X ) ,
$$

As discussed by Chen et al (Chen et al. 2023), the above problems can be seen as a combinatorial optimization with structural constraints. Since $f ( \cdot )$ is a black-box model, traditional optimization via backpropagation is not feasible. Therefore, we apply derivative-free optimization to refine the soft prompt ${ \mathbf { } } p _ { h }$ on small model $g ( \cdot )$ . Specifically, we concatenate $n$ soft tokens $p _ { h _ { 1 } : h _ { n } } \in \bar { \mathbb { R } ^ { D } }$ with input queries $X$ as inputs to the small model to generate the domain knowledge $k = g ( p _ { h _ { 1 } ; h _ { n } } , X )$ . Therefore, our objective is to identify the optimal soft prompt:

$$
p _ { h } ^ { * } = \arg \operatorname* { m a x } _ { p _ { h } \in \mathbb { R } ^ { D } } \mathbb { E } _ { ( X , Y ) \sim \mathcal { T } _ { t } } h ( f ( [ g ( p _ { h } , X ) ; X ] ) , Y ) .
$$

Although the original optimization problem is transformed into a feasible continuous optimization problem, derivativefree black-box optimization remains challenging due to the high dimensionality of the optimized soft prompt. To address this problem, we propose to optimize a lower dimensional vector $\pmb { p } \in \mathbb { R } ^ { d }$ where $d \ll D$ and apply a random projection $A \in \mathbb { R } ^ { d \times D }$ to project $\pmb { p }$ into the original space. The intuitions behind this are two-fold: (1) As shown by previous studies (Sun et al. 2022a; Hu et al. 2021), the knowledge encoded by pre-trained LMs usually has low dimensionality by nature, which means that effective optimization doesn’t require a full exploration of the high-dimensional parameter space; (2) According to Johnson-Lindenstrauss Lemma (Kleinberg 1997), the random projection is distance-preserving, so the kernel similarity of the low dimensional knowledge representation (i.e., p) can be maintained after the projection. Thus, the optimization objective is transformed into the following formula:

$$
p ^ { * } = \arg \operatorname* { m a x } _ { A p \in \mathbb { R } ^ { D } } \mathbb { E } _ { ( X , Y ) \sim \mathcal { T } _ { t } } h ( f ( [ g ( A p , X ) ; X ] ) , Y ) .
$$

![](images/1d530284ba5463e4f92d5acb6a2adacd124b26f034e7a8fc6adbb669c3f60ff8.jpg)  
Figure 3: Illustration of the Bayesian Prompted Optimization where only soft embeddings are trainable. $F ( \pmb { p } )$ is the objective score corresponding to soft embedding $\pmb { p }$ .

We employ Bayesian optimization (BO) (Frazier 2018) to handle the above optimization. BO is an effective technique for updating the posterior distribution of the objective function by iteratively incorporating new sample points. To be specific, we first define the objective function as $F ( p ) = \mathbb { E } _ { ( X , Y ) \sim \mathcal { T } _ { t } } h ( f ( [ g ( A p , X ) ; X ] ) , Y )$ . Then, we employ the Gaussian Process (GP) as the prior to estimate the distribution of $F ( \cdot )$ , i.e.,

$$
F ( \pmb { p } ) \sim G P ( \mu , \sigma ^ { 2 } ) ,
$$

where $\mu$ is the mean function and $\sigma ^ { 2 }$ is the variance function. This GP can be updated iteratively as the optimization process progresses, incorporating new observations to better approximate the true function and reduce uncertainty w.r.t. its behavior. For each $p _ { i }$ , we can obtain a score $F ( p _ { i } )$ . Let $\mathcal { D }$ denote all collected data in previous BO steps, i.e., $\mathcal { D } = \{ ( { \pmb p _ { 1 } } , F ( { \pmb p _ { 1 } } ) ) , . . . , ( { \pmb p _ { n } } , F ( { \pmb p _ { n } } ) ) \}$ . Then the $\mu$ and $\sigma ^ { 2 }$ of the GP can be updated as follows:

$$
\mu ( p ) = c ( p , P ) \left( C + \sigma _ { n } ^ { 2 } { \cal I } \right) ^ { - 1 } F ( P ) ,
$$

$$
\sigma ^ { 2 } ( p ) = c ( { \pmb { p } } , { \pmb { p } } ) - c ( { \pmb { p } } , { \pmb { P } } ) \left( C + \sigma _ { n } ^ { 2 } { \pmb { I } } \right) ^ { - 1 } c ( { \pmb { P } } , { \pmb { p } } ) ,
$$

where $P = [ p _ { 1 } , . . . , p _ { n } ]$ and $p _ { i }$ is the soft embedding in the $i$ th exploration, $c ( \cdot , \cdot )$ is the covariance function, $C$ is the covariance matrix of $P$ , $\sigma _ { n }$ is the noise variance, $\boldsymbol { \mathit { I } }$ represents the identity matrix. ${ \pmb p } _ { \bf 1 }$ is randomly initialized. After finishing an iteration, we employ Expected Improvement (EI) to find the next $p _ { n + 1 }$ . The Expected Improvement (EI) is a popular acquisition function that balances exploration and exploitation. It quantifies the potential improvement over the current best observed value. Formally, the next soft prompt $p _ { n + 1 }$ is defined as follows:

$$
p _ { n + 1 } \in \arg \operatorname* { m a x } _ { p \in \mathbb { R } ^ { d } } \mathbb { E } _ { F ( p ) } \left[ \operatorname* { m a x } \left\{ 0 , F ( p ) - \operatorname* { m a x } _ { i \in [ n ] } F \left( p _ { i } \right) \right\} \right] ,
$$

In practice, we employ an evolutionary search algorithm known as CMA-ES (Hansen 2016) as the optimization method to identify the most effective soft prompts. When obtaining $p _ { n + 1 }$ , we evaluate performance $F ( p _ { n + 1 } )$ of

BLADE on the training batch. Subsequently, the pair $\left( p _ { n + 1 } , F ( p _ { n + 1 } ) \right)$ is incorporated into the $\mathcal { D }$ to update $\mu$ and $\sigma ^ { 2 }$ . This process is performed iteratively until convergence (effectiveness gains less than a threshold for a given number of steps) or reaches the maximum iteration number.

# Experiment

# Datasets and Metrics

We conduct extensive experiments on two widely used datasets in the legal and medical domains.

JEC-QA (Zhong et al. 2020) is the largest Chinese multiple-choice dataset in the legal domain. The legal questions in JEC-QA require high-level reasoning ability and are divided into two types: Knowledge-Driven Questions (KD-questions) and Case-Analysis Questions (CA-questions). There are 26,365 questions in JEC-QA, of which 5,289 of them comprising the test set. It’s worth noting that the number of correct options for each question is uncertain.

MLEC-QA (Li, Zhong, and Chen 2021) is the multichoice biomedical QA dataset. This dataset contains five subsets: Clinic (Cli), Stomatology (Sto), Public Health (PH), Traditional Chinese Medicine (TCM), and Traditional Chinese Medicine Combined with Western Medicine (CWM), all of them collected from the National Medical Licensing Examination in China. There are 136,236 questions in MLEC-QA, each presenting five options with one correct answer.

Accuracy serves as the primary evaluation metric. When the correct answer contains more than one option, the model prediction is considered correct only if it exactly matches the golden answer. Due to the limited availability of highquality pre-training corpora, our experiments are primarily conducted in Chinese benchmarks. For more experiments on English datasets, please refer to Appendix (Li et al. 2024a).

# Baselines

We adopt four groups of baselines for comparison: General LLMs, Legal-specific LLMs, Medical-specific LLMs, and Retrieval-augmented LLMs. General models include a range of popular LLM: ChatGLM-6B (Du et al. 2022),

ChatGLM2-6B (Du et al. 2022), Baichuan2-7B-Chat/13BChat (Baichuan 2023), Qwen-7B-Chat (Bai et al. 2023), ChatGPT (Floridi and Chiriatti 2020). Due to the high cost, we sampled a subset of the data to evaluate GPT-4 and report it in the Appendix. Legal-specific LLMs are further fine-tuned in the legal corpus to improve the understanding of the law, including LaywerLLaMA (Huang et al. 2023), LexiLaw, ChatLaw-13B/33B (Cui et al. 2023). For Medicalspecific LLMs, Taiyi (Luo et al. 2023) and Zhongjing (Yang et al. 2023) are selected as baseline models. In the Retrievalaugmented framework, BM25, BGE (Xiao et al. 2023), M3E (Wang Yuxin 2023), GTE (Li et al. 2023b) are employed as the retriever. BGE (Xiao et al. 2023), M3E (Wang Yuxin 2023), GTE (Li et al. 2023b) are advanced text embedding systems within Chinese contexts. Due to space limitations, a detailed description of the baselines and the reasons for selecting them are provided in the appendix.

# Implementation Details

We implement BLADE with BLOOMZ (Le Scao et al. 2023) as the small LM due to BLOOMZ’s popularity and availability in various sizes. BLOOMZ with 1b7 parameters are employed to complete the main experiment. To construct the Chinese legal pre-training corpus, we collect legal articles, legal books, legal cases, and other resources from official websites 1. In the medical domain, our corpus comprises medical Wikipedia entries and various medical texts. More details can be found in Appendix.

# Experiment Result

Main result Table 1 presents the results from the baselines and BLADE on the JEC-QA dataset. We derive the following observations from the experiment results: (1) Legal-specific LLMs show relatively poor results. There is even some performance degradation after domain-specific fine-tuning. For example, LexiLaw underperforms compared to ChatGLM6B. We hypothesize that although continuous tuning can enhance domain knowledge, it also significantly impacts the model’s ability in prompt processing. This observation is also in line with Cheng et al (Cheng, Huang, and Wei 2023). Furthermore, it’s worth noting that the legal-specific LLMs demonstrate substantial improvement when integrated with BLADE, implying that these models may not be fully leveraging the domain knowledge encoded in their parameters. (2) BLADE consistently enhances performance across various models. For example, Baichuan2-7B-Chat achieves $2 6 . 4 \%$ performance improvement, while ChatGPT achieves $3 1 . 3 \%$ performance improvement. This indicates that BLADE is applicable to diverse language models with different sizes. BLADE effectively utilizes domain knowledge without affecting the reasoning ability of the original model.

Table2 shows the performance of BLADE on the medical domain dataset MLEC-QA. Similar to the legal domain, the Medical-specific LLMs exhibit unsatisfactory performance. The challenge of integrating domain knowledge through continuous training without compromising the original capabilities of LLMs deserves further investigation. Another interesting finding is that, with the assistance of BLADE, the gap between general LLMs has been narrowed. We attribute this to the nature of the dataset itself, where the majority of the questions are relatively straightforward and do not require overly complex reasoning abilities. Once domain-specific knowledge is provided, the initial performance gap between these LLMs is further diminished. Overall, in the medical domain, BLADE demonstrates consistent performance improvements across all five subsets, with Bacihuan2-13B-Chat achieving the best performance.

Comparison with Retrieval Augmented LLMs To further explore the effectiveness of BLADE, we compare it to the retrieval augmentation paradigm. Due to the more challenging nature of the Case-Analysis Questions in JEC-QA, we primarily conducted our experiments on JEC-QA in this section. Experiments on other datasets can be found in Appendix. More specifically, We employ three different legal corpora, including legal article, legal book, and legal all. The legal article corpus contains all the Chinese legal provisions. Legal book denotes the National Unified Legal Professional Qualification Examination Counseling Book, which consists of 15 topics and 215 chapters organized in a hierarchical manner. Legal all corpus is consistent with the corpus in the DP phase, which contains all documents from legal article and legal book.

We select ChatGLM2-6B and ChatGPT, which have shown the best results in open-source and closed-source models respectively on the JEC-QA dataset, to conduct the experiment. To ensure a fair comparison, we exclusively use the top-1 document from the retrieval results and employ an identical prompt to BLADE, which also generates only a single document for analysis. Table 3 demonstrates the comparison results. We have the following observations: (1) Retrieval augmentation is proven to be effective in enhancing the performance of general LLMs in specific domains. However, its effectiveness is significantly influenced by the retrieval model and the corpus. Consequently, not all retrieved knowledge contributes positively to the task at hand. (2) KnowledgeDriven questions, focusing on the definition and explanation of legal concepts, tend to benefit more from the retrieved knowledge. However, Case-Analysis Questions, involving the analysis of real-life scenarios, may not see significant improvement from retrieved knowledge. This reflects the limitations of the retrieval augmentation paradigm, which lacks causal inference ability to identify question-specific knowledge. (3) Regardless of Knowledge-Driven or Case Analysis questions, BLADE consistently provides stable enhancements and achieves optimal performance. We attribute this to small LMs generating knowledge through deep tokenlevel cross-attention, unlike the shallow interactions seen in modern retrieval models. This makes the generated knowledge more in-depth and specific to the question. We provide a detailed analysis of the strengths and weaknesses of BLADE and RAG in the appendix and thoroughly explain why BLADE outperforms RAG.

We further explore the impact of the number of retrieved documents on the performance of ChagtGLM2-6B. Specifically, we use M3E as the retrieval model and legal all as the corpus because they achieve the best results in the retrieval augmentation paradigm. As shown in Table 4, when an appropriate number of documents are retrieved, there is a slight performance improvement due to more relevant documents being recalled. However, the performance of ChatGLM2-6B degrades when too many documents are retrieved, probably due to excessive noise introduced by the additional documents. In contrast, BLADE achieves the best results by generating only one piece of knowledge, suggesting its proficiency in producing more targeted and refined knowledge.

Table 1: Overall accuracy on JEC-QA dataset. The gain $\%$ shows the relative improvement of methods compared to the original language model. $^ { * } / ^ { * * }$ denotes that BLADE performs significantly better than the original language model at $p < 0 . 0 5 / 0 . 0 1$ level using the fisher randomization test (Rubin 1980). Best results are marked bold.   

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2"># Parameters</td><td colspan="2">OriginKD-questiLADE</td><td colspan="2">OriginCA-questBLADE</td><td colspan="2"></td></tr><tr><td></td><td></td><td></td><td></td><td>Original</td><td>AI+BLADE</td></tr><tr><td colspan="9">Legal Specific LLMs</td></tr><tr><td>LaywerLLaMA</td><td>13B</td><td>9.76</td><td>12.94**(32.6%)</td><td>6.05</td><td>8.66**(43.1%)</td><td>7.45</td><td>10.26**(37.7%)</td></tr><tr><td>LexiLaw</td><td>6B</td><td>15.50</td><td>19.63**(26.6%)</td><td>14.35</td><td>18.07**(25.9%)</td><td>14.78</td><td>18.66**(26.5%)</td></tr><tr><td>ChatLaw-13B</td><td>13B</td><td>10.32</td><td>17.32**(67.8%)</td><td>5.03</td><td>8.08**(60.6%)</td><td>7.01</td><td>11.55**(64.8%)</td></tr><tr><td>ChatLaw-33B</td><td>33B</td><td>15.66</td><td>21.80**(39.2%)</td><td>17.01</td><td>20.46**(20.3%)</td><td>16.50</td><td>20.96**(27.0%)</td></tr><tr><td colspan="8">General LLMs</td></tr><tr><td>ChatGLM-6B</td><td>6B</td><td>17.08</td><td>21.19**(24.1%)</td><td>16.64</td><td>18.62**(11.9%)</td><td>16.81</td><td>19.58**(16.5%)</td></tr><tr><td>ChatGLM2-6B</td><td>6B</td><td>27.39</td><td>30.81**(12.5%)</td><td>24.09</td><td>26.34**(9.3%)</td><td>25.32</td><td>28.01**(10.6%)</td></tr><tr><td>Qwen-7B-Chat</td><td>7B</td><td>25.78</td><td>31.26**(21.2%)</td><td>24.52</td><td>25.07*(2.2%)</td><td>24.99</td><td>27.39**(9.6%)</td></tr><tr><td>Baichuan2-7B-Chat</td><td>7B</td><td>19.23</td><td>24.27**(26.2%)</td><td>19.53</td><td>21.73**(11.3%)</td><td>19.41</td><td>22.68**(16.8%)</td></tr><tr><td>Baichuan2-13B-Chat</td><td>13B</td><td>25.78</td><td>28.29**(9.73%)</td><td>21.80</td><td>24.22**(11.1%)</td><td>23.29</td><td>25.75**(10.5%)</td></tr><tr><td>ChatGPT</td><td></td><td>20.53</td><td>28.45**(38.6%)</td><td>18.70</td><td>23.67**(26.6%)</td><td>19.38</td><td>25.46**(31.3%)</td></tr></table></body></html>

<html><body><table><tr><td rowspan="2">Model</td><td colspan="2">C+BLADE</td><td colspan="2">CWMLADE</td><td colspan="2"></td><td colspan="2">StBLADE</td><td colspan="2">TCMLADE</td></tr><tr><td>Ori.</td><td></td><td>Ori.</td><td></td><td>Ori.</td><td>PHBLADE</td><td>Ori.</td><td></td><td>Ori.</td><td></td></tr><tr><td colspan="2">Medical Specific LLMs</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Zhongjing_base</td><td>15.58</td><td>35.74**</td><td>19.03</td><td>37.52**</td><td>16.55</td><td>36.98**</td><td>14.48</td><td>34.86**</td><td>17.41</td><td>36.65**</td></tr><tr><td>Zhongjing_sft</td><td>16.00</td><td>47.92**</td><td>18.50</td><td>49.64**</td><td>15.85</td><td>50.24**</td><td>15.76</td><td>46.12**</td><td>18.88</td><td>47.82**</td></tr><tr><td>Taiyi</td><td>43.42</td><td>49.72**</td><td>32.71</td><td>42.99**</td><td>35.11</td><td>45.63**</td><td>31.53</td><td>41.77**</td><td>32.83</td><td>43.65**</td></tr><tr><td colspan="2">General LLMs</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ChatGLM-6B</td><td>30.04</td><td>53.42**</td><td>30.84</td><td>55.06**</td><td>30.47</td><td>55.66**</td><td>27.56</td><td>52.24**</td><td>32.96</td><td>53.64**</td></tr><tr><td>ChatGLM2-6B</td><td>48.86</td><td>60.20**</td><td>44.82</td><td>57.23**</td><td>44.39</td><td>59.75**</td><td>41.77</td><td>57.61**</td><td>46.12</td><td>55.72**</td></tr><tr><td>Qwen-7B-Chat</td><td>56.57</td><td>59.78*</td><td>52.59</td><td>58.20**</td><td>52.64</td><td>62.26**</td><td>49.33</td><td>57.39**</td><td>51.53</td><td>56.62**</td></tr><tr><td>Baichuan2-7B-Chat</td><td>51.10</td><td>59.99**</td><td>51.14</td><td>58.69**</td><td>50.00</td><td>62.45**</td><td>45.29</td><td>57.61**</td><td>51.79</td><td>56.82**</td></tr><tr><td>Baichuan2-13B-Chat</td><td>58.98</td><td>61.62*</td><td>54.39</td><td>58.79**</td><td>57.92</td><td>63.80**</td><td>50.39</td><td>57.84**</td><td>54.87</td><td>57.34*</td></tr><tr><td>ChatGPT</td><td>47.56</td><td>58.92**</td><td>38.69</td><td>57.91**</td><td>47.73</td><td>63.37**</td><td>43.32</td><td>57.58**</td><td>36.49</td><td>56.40**</td></tr></table></body></html>

Table 2: Overall performance on the medical dataset MLEC-QA. \*/\*\* denotes that BLADE performs significantly better than baselines at $p < 0 . 0 5 / 0 . 0 1$ level using the fisher randomization test (Rubin 1980). Best results are marked bold.

# Ablation Studies

To better illustrate the effectiveness of our approach, we further conduct ablation studies on JEC-QA in zero-shot setting. Table5 shows the impact of different strategies. It’s noticeable that while Domain-specific Pretraining successfully imparts domain knowledge to the small LM, it falls short in enabling instruction-following capabilities and in generating suitable knowledge, leading to a decrease in performance. With the integration of Knowledge Instruction Tuning, the small LM begins to offer beneficial knowledge. Bayesian Prompted Optimization further enhances the performance. The above experiments verify the effectiveness of each process within

our approach.

# Impact of Sizes

In this section, we aim to investigate the impact of the small LM’s size. We conducted experiments on the JEC-QA dataset, utilizing ChatGLM2-6B as the general model. Three versions of the small LM, namely BLOOMZ 560m (Le Scao et al. 2023), BLOOMZ 1b1 (Le Scao et al. 2023), and BLOOMZ 1b7 (Le Scao et al. 2023), were tested, each trained with the same training parameters and datasets. The results are shown in Table 6. We can observe that the small model with $5 6 0 \mathrm { m }$ parameters can also lead to performance gains. As the parameters of the small LM increase, the performance improvement brought by BLADE also increases. This phenomenon could be attributed to larger models’ enhanced capability to generate more accurate and reliable knowledge.

# Case Study

In this section, we conduct a case study to facilitate a clear understanding of the effectiveness of BLADE. Figure 4 in Appnendix illustrates the comparison of retrieved knowledge retrieved by M3E-base from the legal all corpus with the knowledge generated by BLADE. This question involves the assessment of civil conduct capacity in the context of a contract dispute. The appropriate legal procedure involves suspending the ongoing proceedings and initiating a specialized process by Li’s parents to affirm Li’s status as a person with limited civil capacity. The retrieval model returns the article about proceedings for people with mental illnesses, which fails to directly address the civil litigation process and the implications of limited civil capacity in contract disputes. BLADE’s response is more accurate and directly relevant to the question. It correctly identifies the key issue – the civil litigation process concerning the assessment of civil conduct capacity in the context of a contract dispute. This case shows BLADE’s strength in providing domain-specific, contextually appropriate responses. The domain-specific LM, trained on nuanced legal knowledge, is adept at interpreting the underlying legal implications of the described events. Therefore, BLADE can effectively bridge the gap between the specific details of an event and the relevant legal principles or precedents. More examples can be found in Appendix.

Table 3: The performance comparison of BLADE and Retrieval-augmented LLMs on JEC-QA. The gain $\%$ shows the relative improvement of methods compared to the original language model. $^ { * } / { ^ { * * } } { ^ { * } }$ denotes that BLADE performs significantly better than the original language model at $p < 0 . 0 5 / 0 . 0 1$ level using the fisher randomization test (Rubin 1980). The best method in each column is marked in bold.   

<html><body><table><tr><td rowspan="2">Retrieval_model</td><td rowspan="2">Corpus</td><td colspan="3">ChatGLM2-6B</td><td colspan="3">ChatGPT</td></tr><tr><td>KD-questions(%)</td><td>CA-questions(%)</td><td>All(%)</td><td>KD-questions(%)</td><td>CA-questions(%)</td><td>All(%)</td></tr><tr><td>1</td><td>-</td><td>27.39</td><td>24.09</td><td>25.33</td><td>20.53</td><td>18.70</td><td>19.38</td></tr><tr><td>BM25</td><td>legal_article</td><td>28.19*(2.9%)</td><td>24.03(-0.2%)</td><td>25.59(1.0%)</td><td>21.70**(5.7%)</td><td>19.01**(1.6%)</td><td>20.02**(3.3%)</td></tr><tr><td>BM25</td><td>legal_book</td><td>29.60**(8.1%)</td><td>24.25(0.6%)</td><td>26.25(3.6%)</td><td>22.66**(10.3%)</td><td>19.26(2.9%)</td><td>20.53**(5.9%)</td></tr><tr><td>BM25</td><td>legal_all</td><td>28.14**(2.7%)</td><td>23.16(-3.8%)</td><td>25.03*(-1.1%)</td><td>20.19**(-1.6%)</td><td>18.74**(0.2%)</td><td>19.20**(-0.5%)</td></tr><tr><td>BGE</td><td>legal_article</td><td>28.51*(5.3%)</td><td>24.95(3.6%)</td><td>26.41*(4.3%)</td><td>26.73**(30.2%)</td><td>19.73*(5.5%)</td><td>22.36(15.4%)</td></tr><tr><td>BGE</td><td>legal_book</td><td>27.54(0.5%)</td><td>23.49(-2.4%)</td><td>25.01(-1.2%)</td><td>27.19**(32.4%)</td><td>19.55(4.5%)</td><td>22.42**(15.7%)</td></tr><tr><td>BGE</td><td>legal_all</td><td>30.11**(9.9%)</td><td>24.13(0.2%)</td><td>26.38*(4.1%)</td><td>27.75**(35.2%)</td><td>20.54**(9.8%)</td><td>23.25**(19.9%)</td></tr><tr><td>GTE</td><td>legal_article</td><td>27.09(-1.1%)</td><td>23.55(-2.2%)</td><td>24.88(-1.8%)</td><td>22.15**(7.8%)</td><td>19.04(1.8%)</td><td>20.21(4.3%)</td></tr><tr><td>GTE</td><td>legal_book</td><td>25.58(-6.6%)</td><td>22.84(-5.2%)</td><td>23.86(-5.7%)</td><td>21.90**(6.6%)</td><td>19.55(4.5%)</td><td>20.43(5.4%)</td></tr><tr><td>GTE</td><td>legal_all</td><td>25.43(-7.1%)</td><td>23.28(-3.4%)</td><td>24.09(-4.9%)</td><td>22.25**(8.3%)</td><td>19.10(2.1%)</td><td>20.28(4.6%)</td></tr><tr><td>M3E</td><td>legal_article</td><td>28.55*(4.2%)</td><td>24.77(2.8%)</td><td>26.19(3.4%)</td><td>26.03**(26.7%)</td><td>20.58**(10.1%)</td><td>22.63**(16.7%)</td></tr><tr><td>M3E</td><td>legal_book</td><td>27.74(1.3%))</td><td>24.77(2.8%)</td><td>25.88(2.2%)</td><td>26.28**(28.0%)</td><td>20.98**(12.2%)</td><td>22.97**(18.5%)</td></tr><tr><td>M3E</td><td>legal_all</td><td>30.56**(11.6%)</td><td>24.88(3.3%)</td><td>27.02*(6.6%)</td><td>28.20**(37.3%)</td><td>21.19**(13.3%)</td><td>23.82**(22.9%)</td></tr><tr><td colspan="2">BLADE</td><td>30.81**(12.5%)</td><td>26.34**(9.3%)</td><td>28.01**(10.6%)</td><td>28.45**(38.6%)</td><td>23.67**(26.6%)</td><td>25.46**(31.3%)</td></tr></table></body></html>

Table 4: Impact of the number of retrieved documents on JEC-QA. Best results are marked bold.   

<html><body><table><tr><td>Model</td><td>doc_num</td><td>KD-questions</td><td>CA-questions</td><td>All</td></tr><tr><td></td><td>0</td><td>27.39</td><td>24.09</td><td>25.33</td></tr><tr><td>M3E</td><td>1</td><td>30.56</td><td>24.88</td><td>27.02</td></tr><tr><td>M3E</td><td>3</td><td>30.71</td><td>24.67</td><td>26.93</td></tr><tr><td>M3E</td><td>5</td><td>30.36</td><td>25.29</td><td>27.19</td></tr><tr><td>M3E</td><td>7</td><td>29.75</td><td>24.28</td><td>26.33</td></tr><tr><td>M3E</td><td>9</td><td>29.63</td><td>24.40</td><td>26.36</td></tr><tr><td>BLADE</td><td></td><td>30.81</td><td>26.34</td><td>28.01</td></tr></table></body></html>

Table 5: Ablation study on JEC-QA under zero-shot setting. The general LLM is ChatGLM2-6B. Best results are marked bold.   

<html><body><table><tr><td>SmallModel</td><td>KD-questions(%)</td><td>CA-questions(%)</td><td>All(%)</td></tr><tr><td></td><td>27.39</td><td>24.09</td><td>25.33</td></tr><tr><td>BLOOMZ_1b7</td><td>26.38</td><td>22.40</td><td>23.89</td></tr><tr><td>+DP</td><td>26.87</td><td>23.63</td><td>24.85</td></tr><tr><td>+ DP&KIT</td><td>28.45</td><td>24.89</td><td>26.23</td></tr><tr><td>+DP&KIT&BPO</td><td>30.81</td><td>26.34</td><td>28.01</td></tr></table></body></html>

<html><body><table><tr><td>SmallModel</td><td>KD-questions(%)</td><td>CA-questions(%)</td><td>All(%)</td></tr><tr><td>1</td><td>27.39</td><td>24.09</td><td>25.33</td></tr><tr><td>BLOOMZ_560m</td><td>29.05</td><td>24.92</td><td>26.47</td></tr><tr><td>BLOOMZ_1b1</td><td>29.80</td><td>25.52</td><td>27.13</td></tr><tr><td>BLOOMZ_1b7</td><td>30.81</td><td>26.34</td><td>28.01</td></tr></table></body></html>

Table 6: Impact of sizes on JEC-QA. The general LLM is ChatGLM2-6B. Best results are marked bold.

# Conclusion

This paper proposes BLADE, a new framework for applying general large language models to new domains. At its core, BLADE employs small language models to assimilate and continually update domain-specific knowledge. The framework solves problems by realizing collaboration between general large language models and a small domain-specific model. Through extensive experiments on legal datasets, we find BLADE consistently demonstrates performance improvement across various language models with different sizes. In the future, we will investigate approaches to minimize hallucinations in small models and explore additional methods for joint optimization. A limitation is that our experiments are conducted only in multiple-choice datasets, the feasibility of our approach in generative tasks still deserves further investigation.