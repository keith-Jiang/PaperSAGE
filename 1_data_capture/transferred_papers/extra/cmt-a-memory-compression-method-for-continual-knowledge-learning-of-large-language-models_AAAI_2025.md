# CMT: A Memory Compression Method for Continual Knowledge Learning of Large Language Models

Dongfang Li, Zetian Sun, Xinshuo Hu, Baotian Hu, Min Zhang

Harbin Institute of Technology (Shenzhen) crazyofapple@gmail.com

# Abstract

Large Language Models (LLMs) need to adapt to the continuous changes in data, tasks, and user preferences. Due to their massive size and the high costs associated with training, LLMs are not suitable for frequent retraining. However, updates are necessary to keep them in sync with rapidly evolving human knowledge. To address these challenges, this paper proposes the Compression Memory Training (CMT) method, an efficient and effective online adaptation framework for LLMs that features robust knowledge retention capabilities. Inspired by human memory mechanisms, CMT compresses and extracts information from new documents to be stored in a memory bank. When answering to queries related to these new documents, the model aggregates these document memories from the memory bank to better answer user questions. The parameters of the LLM itself do not change during training and inference, reducing the risk of catastrophic forgetting. To enhance the encoding, retrieval, and aggregation of memory, we further propose three new general and flexible techniques, including memory-aware objective, self-matching and top- $k$ aggregation. Extensive experiments conducted on three continual learning datasets (i.e., StreamingQA, SQuAD and ArchivalQA) demonstrate that the proposed method improves model adaptability and robustness across multiple base LLMs (e.g., $+ 4 . 0 7 \ : \mathrm { E M } \ : \&$ $+ 4 . 1 9 \mathrm { F } 1$ in StreamingQA with Llama $- 2 - 7 b$ ).

# Introduction

Large language models (LLMs) have become the core of natural language processing (NLP) (Touvron et al. 2023a; OpenAI 2023). The current challenge is how these LLMs adapt to rapidly changing world knowledge, especially in the context of increasing new data and growing model complexity (Shi et al. 2024). Typically, LLMs are trained on static and pre-defined datasets. For example, the Llama-3.1 model is an open-source large language model by Meta, with a training dataset over 15 trillion tokens (Llama Team 2024). However, in practical applications, language usage habits, information content, and user needs are all dynamically changing (Wu et al. 2024). On the other hand, once training is complete, the model becomes fixed, and the cost and computational demands of retraining or incremental pre-training is extremely high. For example, the GPT-3 model has 174.6 billion parameters, and retraining it once requires approximately 3640


	 
 
	   
 ?   
  
     

 

PF-days of computing power (i.e., performing 10 quadrillion calculations per second for 3640 days) (Brown et al. 2020). Therefore, how to adapt downstream tasks effectively and efficiently with updating the model with the new knowledge while retaining the existing knowledge has become an important and urgent topic.

To address these challenges, existing continual learning methods dynamically update new incremental knowledge through techniques such as data replay and incremental task parameters while balancing the generalization of new and old knowledge (Schwarz et al. 2018; Riemer et al. 2018; Shi and Wang 2024). However, data replay and incrementally adding task parameters bring non-negligible computational overhead. Additionally, methods like model editing only provide patches to the model, resulting in poor generalization ability and even causing collapse (Yao et al. 2023). While it is possible to perform thousands of edits simultaneously, scalability is low when updating a large amount of knowledge in large models, and catastrophic forgetting can occur. On the other hand, memory is the foundation of human intelligence while humans use memory to achieve continual learning. As shown in Figure 1, memory impact on intellectual activities such as learning, abstraction, association, and reasoning in the human brain spans three stages: (1) Encoding: It involves reorganizing and transforming external information. The efficiency of learning depends on the strategies used for memory encoding (Pyc and Rawson 2010). Strategies such as multi-channel encoding and contextual association can significantly enhance learning outcomes. (2) Storage: Information is stored hierarchically and categorized in long-term memory. Retaining learned content makes subsequent learning more efficient (Bjork 1994). (3) Retrieval: It involves extracting and aggregating information from long-term memory. It consolidates memory storage, stimulates meta-cognitive abilities, and promotes reasoning, abstraction, and association (Karpicke and Roediger III 2008). Hence, how to draw on human memory mechanisms to the continual learning process has become an interesting and possible direction for adapting to changing world knowledge of LLMs.

To this end, we introduce the Compression Memory Training (CMT) method that encodes knowledge extracted from new documents into a dynamic memory bank within its latent space, serving as long-term memory for subsequent retrieval and aggregation. The core idea is to freeze the parameters of the LLM itself and construct a memory-based module that learns to automatically encode and collect relevant information. Specifically, we first utilize an instantiable compressor to compress information from new documents into compact representations, which are cached to maximize the performance of the LLM on unseen tasks. Different from Tack et al. (2024), this representation is generated through memory tokens with decoder-only model representing compressed knowledge, resulting in a memory bank that is less redundant than traditional knowledge bases in retrieval-based methods or contexts in prompting compression methods. Thus, during online adaptation, each document stream instance is stored in the memory bank. It allows contexts to be pre-computed offline once and reducing the LLMâ€™s computational costs at inference. Next, we learn to aggregate representations (i.e., memory) in the feature space into a single representation based on the given query, which is then mapped into cached key-value pairs within each transformer layer of the LLM. To ensure the effectiveness and scalability of CMT, we further propose three training and inference techniques corresponding to the encoding, retrieval and aggregation stages of memory respectively: (1) memory-aware objective; (2) selfmatching; and (3) top- $k$ aggregation. The evaluation of CMT focuses on several key aspects: (1) Integration of new knowledge. The modelâ€™s performance is assessed with downstream QA tasks, where CMT demonstrates substantial improvements over existing methods, indicating the superiority of supplementing LLMs with CMT; (2) Knowledge retention. CMT is evaluated on knowledge retention experiments under scenarios with different numbers of adapted documents, showcasing its ability to recall knowledge; (3) Robustness. We use the proportion of unrelated documents as a measure to test the modelâ€™s performance in the presence of irrelevant interference. The results show that CMT outperforms competitive baselines, demonstrating superior robustness.

Our contributions are summarized as follows:

â€¢ We introduce CMT that incorporates an integrated memory bank within the latent space to address the challenges of continual learning of LLMs.

â€¢ To utilize encoded memory more efficiently, we further propose three effective training and inference strategies. â€¢ CMT demonstrates competitive performance across three benchmarks and knowledge retention settings, showcasing its versatility, effectiveness, and robustness.

# Related Work

Memory-Augmented Models Memory-augmented models are not a new concept. Early memory networks introduced computational methods to store contextual information in limited space, thereby enhancing inference efficiency (Weston, Chopra, and Bordes 2015; Sukhbaatar et al. 2015; Ba et al. 2016). Following this, Memory Transformer (Burtsev and Sapunov 2020) and RMT (Bulatov, Kuratov, and Burtsev 2022) proposed adding memory tokens when reading contexts. However, expanding memory and incorporating information without disrupting the modelâ€™s original capabilities remains a long-term challenge (Khandelwal et al. 2019; Zhong, Lei, and Chen 2022; Modarressi et al. 2023; Moro et al. 2023; Zhong et al. 2023; Wang et al. 2023; Yang et al. 2024). Recent research has also focused on compressing prompts to enhance LLM inference efficiency (Wingate, Shoeybi, and Sorensen 2022; Snell, Klein, and Zhong 2022; Phang et al. 2023). For instance, AutoCompressor (Chevalier et al. 2023) and ICAE (Ge et al. 2024) propose auto-encoding methods for compressing contexts into soft embeddings. Gisting (Mu, Li, and Goodman 2024) introduces learnable tokens to compress context information within attention hidden states. Moreover, several improvements to transformers have demonstrated the benefits of equipping LLMs with external, controllable memory (e.g., MemoryLLM) (Kim et al. 2023; He et al. 2024; Wang et al. 2024b). However, these methods have not yet been applied to continual knowledge learning for existing LLMs as they typically require training from scratch and rely on inflexible and non-reusable implementations.

Continual Learning of LLMs Continual learning aims to integrate LLMs into dynamic data distributions, task structures, and user preferences without significantly degrading performance in learned domains (Zheng et al. 2024; Shi et al. 2024). This involves sequentially training models on a series of tasks with the goal of maintaining performance across all tasks (Kirkpatrick et al. 2017; Li and Hoiem 2017; Riemer et al. 2018; Buzzega et al. 2020). During training, models often have limited or no access to previous data, making it challenging to retain past knowledge since optimization constraints from previous data are absent during current-task learning (Li and Hoiem 2017; Buzzega et al. 2020; Smith et al. 2023; Shi and Wang 2024). This challenge, known as catastrophic forgetting (McCloskey and Cohen 1989), has been a central focus since its inception. Over the years, researchers have explored various techniques to mitigate forgetting in models. These include replay-based methods (Schwarz et al. 2018; Riemer et al. 2018; Shi and Wang 2024), parameter regularization (Kirkpatrick et al. 2017; Ritter, Botev, and Barber 2018; Aljundi et al. 2018; Sprechmann et al. 2018), and model architecture expansion (Wang et al. 2022). Recently, in the context of continual learning for LLMs, the challenge has shifted from storage efficiency to computational efficiency (Song et al. 2023; Wang et al. 2024a). In this paper, we focus on integrating memory systems into continual learning, enabling encoding, retrieval, and aggregation of knowledge without the need for expensive retraining.

![](images/24243d3d54ad665494509128ca770c73f035bb46a1de48c3557373f7c48476cf.jpg)  
Figure 2: Illustration of compression memory training method. During the adaptation process, each document to be learned will be compressed into the dense vector by a compressor, and these vectors $\{ M ^ { | D | } \}$ will be aggregated through the cross attention mechanism and sent to LLMs together with the question for answer output. The training goal is the accuracy of the downstream task answer. In the online adaptation stage, all documents will be compressed into vectors and then filtered and aggregated.

# Method

# Task Formulation

We consider the scenario where an outdated model, $f _ { \theta }$ , is updated using an online stream of recent documents, $D _ { \mathrm { t e s t } } =$ $\{ x _ { i } \}$ . This process produces an updated model, $f _ { \theta + \triangle \theta }$ , which is then evaluated against a set of queries, $Q _ { \mathrm { t e s t } } = \{ q _ { i } \}$ , with corresponding labels, $Y _ { \mathrm { t e s t } } = \{ y _ { i } \}$ . Each query $q _ { i }$ and its label $y _ { i }$ are derived from a distribution related to the corresponding document $x _ { i } \colon q _ { i } , y _ { i } \sim p ( q _ { i } , y _ { i } \mid x _ { i } )$ . For instance, $q _ { i }$ could be a question about some information in document $x _ { i }$ , with $y _ { i }$ being the answer provided by the document. A key constraint is that during the update process using $D _ { \mathrm { t e s t } }$ , we do not have access to $Q _ { \mathrm { t e s t } }$ . Thus, our methodology for updating $f _ { \theta }$ must be general rather than query-specific. To make this problem tractable, we assume the availability of an additional corpus of documents $D _ { \mathrm { t r a i n } }$ and corresponding query samples $Q _ { \mathrm { t r a i n } }$ and labels $Y _ { \mathrm { t r a i n } }$ generated by a similar process to $Q _ { \mathrm { t e s t } }$ , $Y _ { \mathrm { t e s t } }$ This training set enables learning the types of queries that may be of interest, guiding us on how to update our model to optimize performance on test queries while minimizing disruption to its prior knowledge and behaviors. We define the adaptation process involving $D _ { \mathrm { t r a i n } }$ , $Q _ { \mathrm { t r a i n } }$ , and $Y _ { \mathrm { t r a i n } }$ as the learning phase, while the process involving $D _ { \mathrm { t e s t } }$ is referred to as the online adaptation phase. We next describe the method that adjusts the learning phase to more efficiently update our base model on the test stream of documents $D _ { \mathrm { t e s t } }$ .

# CMT: Compression Memory Training

Our goal is to efficiently adapt given LLMs to unseen knowledge while retaining previously learned knowledge, whether from the original pre-training stage or updates from documents in a stream of new data. To achieve this, we designed a learning method called Compression Memory Training shown in Figure 2. During the online adaptation phase, it only requires a single forward pass to compress the documents knowledge into memory, which avoids the cost of gradient computation. Here, we first introduce the general process of the method.

First, each document $d$ in the document set $D$ is compressed into condensed vectors $M$ through the compressor $\Theta$ . Then, these dense vectors corresponding to each document are further aggregated. The aggregated vectors are further mapped and input into the LLM $\Phi$ to be adapted in the form of cached key-value pairs. The LLM $\Phi$ to be adapted freeze their parameters during both the training and online adaptation phases, reducing the risk of catastrophic forgetting during continuous updating. The parameters to be learned include memory encoding, storage, and mapping parts. The entire network is trained during the learning phase, with the learning objective being to better answer $Q _ { \mathrm { t r a i n } }$ .

Compression Memory We introduce the method of compressing documents $D$ into condensed vectors. It aims to transform lengthy documents into concise, compact representations while striving to maintain the core semantics and integrity of the original knowledge. We define a document $d \in D$ as ${ \boldsymbol w } = ( w _ { 1 } , w _ { 2 } , \ldots , w _ { n } , c _ { 1 } , c _ { 2 } , \ldots , c _ { k } )$ , where $w _ { i }$ means the $i$ -th token of document $d$ , $c _ { j }$ means the $j$ -th soft virtual token adhere to this document, and $n$ is the number of actual tokens in document $d$ . Let $e ( \cdot )$ represent the word embedding lookup in the LLM and $m ( \cdot )$ represent the learnable embeddings of soft tokens $c _ { 1 } , c _ { 2 } , \ldots , c _ { k }$ . A document compressor model $\Theta$ utilizes the document embeddings $\boldsymbol { e } ( w ) =$ $( e ( w _ { 1 } ) , e ( w _ { 2 } ) , \ldots , e ( w _ { n } ) )$ and the soft token embeddings $e _ { s o f t } ( c ) = ( e _ { s o f t } ( c _ { 1 } ) , e _ { s o f t } ( c _ { 2 } ) , \dots , e _ { s o f t } ( c _ { k } ) )$ to produce compact representations M = (m1, m2, . . . , mk) âˆˆ RkÃ—d of the document $d$ , where $k$ is the length of the compressed document and $k \ll n$ . The condensed vectors $M$ can replace the original context and be combined with other prompt embeddings $\pmb { e } ( p ) = ( \pmb { e } ( p _ { 1 } ) , \dots , \pmb { e } ( p _ { l } ) )$ for input to an LLM $\Phi$ The output $y = ( y _ { 1 } , \dots , y _ { m } )$ remains faithful to the content of the original context $w$ . As illustrated in Figure 2, inspired by Ge et al. (2024), the compressor can be instantiated as a series of cross-attention layers, pre-trained decoder models, and encoder-decoder models with a set of learnable soft tokens, termed condensed tokens. Here, the compressor utilizes document tokens and condensed tokens as inputs, leveraging a causal Transformer decoder to compress the document information into condensed vectors. We leave the application of these vector across different LLMs for future work.

Memory Aggregation Given the memory bank of compressed documents D represented as {M i}|iD=|1, we aim to learn how to select most relevant information in the form of a transformation $M ^ { \ast } \in \mathbb { R } ^ { k \times d }$ for a given input $q _ { i }$ . There are two feasible methods: (1) Retrieve one or multiple memory units and map them into the LLM space. For example, xRAG (Cheng et al. 2024) addresses context aggregation from a multi-modal fusion perspective. It introduces a modality projector trained to directly project retrieved dense vectors into the LLM representation space. However, this approach has the risk of selecting incorrect memory units and requires a pre-training phase to learn how to resolve relationships among different memories. (2) Linearly interpolate multiple memory units, aggregate them by weights into a single memory unit, and map it into the LLM space. For example, Sukhbaatar et al. (2015) computes the weighted sum of the memory bank as the representative vector of the memory. The advantage of this method is that it can leverage ideas like attention mechanisms, model soups (Wortsman et al. 2022) and the mixture-of-experts method (Shazeer et al. 2017) to filter and aggregate different memory units, maintaining permutation invariance of the memory units. However, such methods do not consider the relative position information of soft tokens within memory units during aggregation. Moreover, as we discussed in the task definition, the source of accurate information for answering question $Q _ { i }$ is mostly related to the $i$ -th document. Therefore, similar to Tack et al. (2024), we select $M ^ { * }$ using cross-attention blocks (Vaswani et al. 2017; Kim et al. 2019; $\mathrm { \Delta X u }$ et al. 2020), with the set aggregation network $\psi$ :

$$
M ^ { \ast } = \psi \big ( \Theta ( q _ { i } ) , \{ M ^ { i } \} _ { i = 1 } ^ { | D | } \big )
$$

Here, for reasons of efficiency and consistency in the representation space, we use a document compressor $\Theta$ to compress the input $q _ { i }$ (e.g., user query). Using an additional question encoder is left for future work. As the vanilla crossattention mechanism suffers from capturing the relative positional relationships among soft tokens within the document $d$ . It implies that swapping any two tokens in the memory results in an identical condensed vector. Hence, in the aggregation process, we apply RoPE (Su et al. 2024) to represent the relative positional relations within the soft tokens. We only perform position embedding operations on query and key. And we allocate positional embeddings as if placing the soft tokens subsequent to the context tokens. The RoPE embeddings $\mathrm { R } _ { i }$ and $\mathrm { R } _ { j }$ manifests the relative positional relationships through the inner product between $\dot { Q } _ { \mathrm { p o s } } = \{ q _ { i } \} : = \Theta ( \bar { q } _ { i } )$ and $K _ { \mathrm { p o s } } = \{ \pmb { k } _ { i } \} : = \{ M ^ { k } \} _ { k = 1 } ^ { K }$ :

$$
( \mathrm { R } _ { i } { \pmb q } ) ^ { T } ( \mathrm { R } _ { j } { \pmb k } ) = { \pmb q } ^ { T } \mathrm { R } _ { i } ^ { T } \mathrm { R } _ { j } { \pmb k } = { \pmb q } ^ { T } \mathrm { R } _ { j - i } { \pmb k }
$$

In this way, each soft token can recognize the relative positions relations of both intra- and inter-document soft tokens.

Alignment for LLM After obtaining $M ^ { \ast } \in \mathbb { R } ^ { k \times d }$ , we do not intend to use the memory as an embedding layer input to the LLM $\Phi$ , as this does not fully leverage the memory to promote the association. Therefore, we design a network $\pi$ to map the original memory representation into cached keyvalue pairs (with the number being the actual tokens count). The purpose of $\pi$ is to perform self-attention on the memory tokens and use multiple multi-layer perceptrons to transform the memory tokensâ€™ features into actual tokensâ€™ features. Specifically, the module handles actual tokens by repeating memory tokens and recombines the processed features into the final output. Here, the actual tokens are defined as the new virtual tokens in each layer multiplied by the number of layers, then multiplied by 2 (i.e., key and value).

Training Objective To train the memory embeddings $e _ { s o f t }$ , the compressor $\Theta$ , the aggregation networks $\psi$ and alignment module $\pi$ , we optimize both networks end-to-end using the loss function $\mathcal { L }$ , which is the negative log-likelihood of the given label $y$ :

$$
\mathcal { L } = \operatorname* { m i n } _ { D _ { \mathrm { t r a i n } } , Q _ { \mathrm { t r a i n } } , Y _ { \mathrm { t r a i n } } } \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \mathcal { L } \big ( \mathbf { L M } _ { \theta } ( q _ { i } ; \pi ( M ^ { * } ) ) , y _ { i } \big )
$$

where $N$ is the number of the training queries and labels in $Q _ { \mathrm { t r a i n } }$ . Note that we do not update the static LLM $\theta$ to avoid the risk of catastrophic forgetting by overwriting important parameters. It is important to train the model using the crossentropy loss of the final QA task. We experimented with document auto-encoding pretraining tasks (Ge et al. 2024), with dividing it into two stages or using multi-task learning, but neither approach resulted in significant improvements.

Online Adaptation After training the entire network on a given training corpus $D _ { \mathrm { t r a i n } }$ , $Q _ { \mathrm { t r a i n } }$ , and $Y _ { \mathrm { t r a i n } }$ , we introduce an online adaptation phase. Formally, the CMT processes a stream of test documents $D _ { \mathrm { t e s t } }$ that are sequentially fed to the LLM. Considering that the task query $\bar { q } _ { \mathrm { t e s t } } ^ { i } \in \bar { Q } _ { \mathrm { t e s t } }$ is unavailable during the adaptation process, we first store the condensed representation of the document in the memory $M = \{ \Theta ( d _ { \mathrm { t e s t } } ^ { i } ) \} _ { i = 0 } ^ { | D _ { \mathrm { t e s t } } | }$ and later use the aggregation network to predict the modulation to adapt the LLM:

$$
\hat { y } _ { \mathrm { t e s t } } ^ { i } = \mathrm { L L M } _ { \theta } \big ( q _ { \mathrm { t e s t } } ^ { i } ; \pi ( M ^ { * } ) \big )
$$

where $M ^ { * } = \psi \bigl ( \Theta ( q _ { \mathrm { t e s t } } ^ { i } ) , M \bigr )$ .

# Effective Learning Strategy

Memory-Aware Conditional Objective To enhance the modelâ€™s utilization of memory, we propose a training objective of contrastive ensemble between the logits. It enables the model to account for external knowledge which may not be aligned with the modelâ€™s training data. Specifically, we adopt the vanilla logits from LLM as $l _ { \theta }$ for the prior knowledge $l _ { \theta } ( y _ { i } \mid \mathbf { q } _ { i } )$ . We demote this knowledge from the modelâ€™s original output distribution via $\frac { l _ { \theta } \left( y _ { i } | \boldsymbol { M } ^ { * } , \pmb { q } _ { i } \right) } { l _ { \theta } \left( y _ { i } | \pmb { q } _ { i } \right) }$ . We choose this formulation because it also represents the pointwise mutual

information between the external knowledge from the document set $M ^ { * }$ conditioned on $\mathbf { \nabla } q _ { i }$ . Optionally, one can adjust the original distribution by $\frac { l _ { \theta } \left( y _ { i } | M ^ { * } , \pmb { q } _ { i } \right) } { l _ { \theta } \left( y _ { i } | M ^ { - } , \pmb { q } _ { i } \right) }$ , where $M ^ { - }$ represents an explicit knowledge one wants to demote from (e.g., a set of other unrelated documents). We interpolate this demotion $\frac { l _ { \theta } \left( y _ { i } | M ^ { * } , \pmb { q } _ { i } \right) } { l _ { \theta } \left( y _ { i } | \pmb { q } _ { i } \right) }$ and the original output logits $l _ { \theta } ( y _ { i } \mid M ^ { * } , \pmb q )$ via a product-of-experts weighted by $\alpha$ . We sample $y _ { i }$ from the reweighted distribution:

$$
y _ { i } \propto l _ { \theta } ( y _ { i } \mid M ^ { * } , q _ { i } ) \left( \frac { l _ { \theta } ( y _ { i } \mid M ^ { * } , q ) } { l _ { \theta } ( y _ { i } \mid q _ { i } ) } \right) ^ { \alpha }
$$

Further, we normalize it across all possible $y _ { i }$ :

$$
y _ { i } \sim ( 1 + \alpha ) l _ { \theta } ( y _ { i } \mid M ^ { * } , \pmb { q } _ { i } ) - \alpha l _ { \theta } ( y _ { i } \mid \pmb { q } _ { i } )
$$

where larger $\alpha$ means more weight on our adjustment (we set it to 0.5) and $\alpha = 0$ reduces to standard negative loglikelihood. If we identify an external knowledge $M ^ { * }$ conditionally independent to the generation, $l _ { \theta } ( y _ { i } \mid M ^ { * } , q _ { i } ) =$ $l _ { \theta } ( y _ { i } \mid \mathbf { q } _ { i } )$ , even a non-zero $\alpha$ would not have an impact on the original output distribution.

Knowledge Transfer with Self-Matching Recall that during the aggregation process, we calculate the attention weights between the query and the memory bank. These weights represent the contribution of each document to answering the current question to some extent. Therefore, this inspires us to leverage the nature of the task to capture significant features of memory during continual learning. Note that the specific vector $\mathbf { q } \in \dot { \mathbb { R } } ^ { d }$ for each query has the same dimension as the memory unit $\mathbf { m } _ { i } \in \mathbb { R } ^ { d }$ . Then, we calculate the cosine similarity between the query embedding and the memory vectors for the current $i ^ { \mathrm { t h } }$ document, as the document matching score $\alpha [ : i ] = \cos ( { \bf q } _ { i } , M [ : i ] )$ . The additional training objective for updating the $i ^ { \mathrm { t h } }$ document is to maximize the cosine similarity between $\mathbf { m } _ { i }$ and the corresponding query embedding q. To prevent the model from collapsing all documents into similar vectors, we add a uniformity term to penalize excessive similarity between document embeddings, promoting diversity by encouraging larger pairwise distances between different memory vectors.

Inference with Top- $\mathbf { \nabla } \cdot k$ Aggregation Additionally, we employ a filtering mechanism to handle a large memory bank during downstream task inference. Let $n$ and $| D |$ be the number of output tokens for each context and the number of memory units, respectively. Then, the memory usage of $t$ cross-attention layers in the memory aggregation becomes $t \cdot \mathcal { O } ( | D | n ^ { 2 } )$ . It indicates that the memory cost of the aggregation process scales linearly with the size of the memory. Unlike previous work (Tack et al. 2024) that reduces memory consumption using hierarchical modulation aggregation, here we use a simple but effective top- $k$ filtering method. Specifically, for a given memory bank with $k$ units, we first compute the similarity between them and query, then sort by similarity and filter according to the window size used during training. Hence, the memory bank window size seen by the model in the aggregation process during training and testing is consistent, which requiring no additional training or modifications to the primary training objective. It also reduces the impact of overfitting and noise, and yields better results. Similar observations are reported in retrieval-augmented generation works (Qin et al. 2023; Cuconasu et al. 2024).

# Experiments

We conduct extensive experiments on these Question Answering (QA) benchmark datasets to answer the following Research Questions (RQs):

â€¢ RQ1: How does our model contribute to QA accuracy compared with other state-of-the-art methods? â€¢ RQ2: How effective are the key components in our model, such as the self-matching of memory vectors? â€¢ RQ3: Can our model demonstrate robustness against knowledge interference from irrelevant documents? â€¢ RQ4: How does our model perform in terms of the forgetting and plasticity dynamics within the document stream?

StreamingQA The StreamingQA dataset (LiË‡ska et al. 2022) features both human-written and language modelgenerated questions. These questions are sourced from English WMT news articles published between 2007 and 2020. Each question is linked to a complete WMT news article, with an average length of about 500 tokens per article. For learning purposes, question-article pairs from post-2018 publications are used, resulting in 21k training questions, $1 . 7 \mathrm { k }$ validation questions, and 5k test questions.

SQuAD The Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al. 2016) includes questions created by humans based on Wikipedia articles. The answer to each question is a text span from a specific paragraph within the article. Typically, paragraphs are around 150 tokens. We utilize the validation set of SQuAD as our test set and divide the training set into four additional splits. It results in $3 9 . 9 \mathrm { k }$ training questions, 5.6k validation questions, and $1 0 . 6 \mathrm { k }$ test questions. Additionally, we use $8 . 6 \mathrm { k }$ training documents, 1.2k validation documents, and 2.1k test documents.

ArchivalQA The ArchivalQA dataset (Wang, Jatowt, and Yoshikawa 2022) contains questions generated automatically from the New York Times Annotated Corpus (Sandhaus, Evan 2008). Each answer is a text span within an article, with questions paired to paragraphs from NYT articles. We split the validation set of ArchivalQA into five segments for our study. This setup provides us with $2 1 . 7 \mathrm { k }$ training questions, $5 . 3 \mathrm { k }$ validation questions, and $8 . 7 \mathrm { k }$ test questions. For documents, we utilize $1 2 . 8 \mathrm { k }$ training documents, $3 . 0 \mathrm { k }$ validation documents, and $5 . 0 \mathrm { k }$ test documents.

Experiment Settings We conducted extensive experiments using 4 LLMs as backbones, including the GPT-2 series (Radford et al. 2018) and the Llama-2 series (Touvron et al. 2023b). The model parameters are 82M, 774M, 1.5B, and 7B, respectively. Larger models, such as the 70B, were not included due to insufficient computational resources for training. For the compressor, unlike previous work, we chose a larger decoder model, Llama-2-7b, and used ParameterEfficient Fine-Tuning (PEFT) with a rank of 6 and a LoRA alpha set to 32, employing a different encoding method as well. We performed 1,000 steps of pre-training using English Wikipedia with auto-encoder tasks, as additional steps did not yield consistent improvements. The number of soft tokens used is 24. Additionally, we found that while increasing the number of tokens beyond 24 initially led to marginal gains, the performance plateaued or slightly decreased due to overfitting and increased computational overhead. We evaluated online adaptation performance on a test dataset composed of documents and QA pairs. Following Tack et al. (2024), we adapted the LLMs using 1,665 documents and then assessed its performance post-adaptation. To test the modelâ€™s general understanding, QA pairs were sampled from these documents. Each document contains up to 1,024 tokens. Cross-attention involves 4 blocks. The batch sizes for updates and validation are 8 and 16 respectively, with gradient accumulation over 4 steps. The optimizer is AdamW, and training will run for 50 epochs with validation every 250 steps. The learning rate is set to $1 e ^ { - 6 }$ with a warmup ratio of 0.01 and a constant-withwarmup schedule. During training, we sample the document memory in the same batch and at inference $k$ is equal to training batch size. We run on the NVIDIA A100 80G GPUs.

Table 1: Performance comparisons of online adaptation with CMT are presented. We report the Exact Match (EM) and $F _ { 1 }$ scores after adapting the LLMs on a stream of documents and subsequently conducting downstream QA for test. We use the average of 3 random seeds and baseline results are from the corresponding papers. The boldfaced means the best results for this dataset.   

<html><body><table><tr><td rowspan="2">Datasets</td><td rowspan="2">Method</td><td colspan="2">DistilGPT2</td><td colspan="2">GPT2-Large</td><td colspan="2">GPT2-XL</td><td colspan="2">Llama-2</td></tr><tr><td>EM</td><td>F1</td><td>EM</td><td>F1</td><td>EM</td><td>F1</td><td>EM</td><td>F1</td></tr><tr><td rowspan="5">StreamingQA</td><td>Uniform</td><td>1.62</td><td>3.76</td><td>4.74</td><td>7.00</td><td>5.11</td><td>7.48</td><td>12.43</td><td>13.54</td></tr><tr><td>Salient Spans</td><td>1.44</td><td>4.67</td><td>4.86</td><td>8.54</td><td>5.40</td><td>9.42</td><td>13.33</td><td>18.97</td></tr><tr><td>CaMeLS</td><td>1.62</td><td>5.79</td><td>5.35</td><td>10.60</td><td>6.55</td><td>11.67</td><td></td><td></td></tr><tr><td>MAC</td><td>5.59</td><td>10.18</td><td>7.25</td><td>13.31</td><td>8.99</td><td>15.38</td><td>14.29</td><td>21.79</td></tr><tr><td>CMT (ours)</td><td>6.43</td><td>12.32</td><td>7.32</td><td>13.43</td><td>9.61</td><td>16.48</td><td>18.36</td><td>25.98</td></tr><tr><td rowspan="5">SQuAD</td><td>Uniform</td><td>1.24</td><td>2.54</td><td>3.64</td><td>4.97</td><td>6.10</td><td>6.78</td><td>13.25</td><td>17.01</td></tr><tr><td>Salient Spans</td><td>1.03</td><td>2.47</td><td>4.03</td><td>6.48</td><td>4.55</td><td>6.74</td><td>13.74</td><td>18.66</td></tr><tr><td>CaMeLS</td><td>1.47</td><td>3.08</td><td>4.97</td><td>8.63</td><td>6.70</td><td>10.15</td><td></td><td></td></tr><tr><td>MAC</td><td>2.01</td><td>6.85</td><td>6.43</td><td>11.42</td><td>7.10</td><td>12.55</td><td>15.07</td><td>21.14</td></tr><tr><td>CMT (ours)</td><td>3.12</td><td>7.59</td><td>7.15</td><td>12.45</td><td>9.81</td><td>12.85</td><td>19.54</td><td>25.50</td></tr><tr><td rowspan="5">ArchivalQA</td><td>Uniform</td><td>4.86</td><td>4.08</td><td>7.66</td><td>8.71</td><td>8.61</td><td>10.78</td><td>18.53</td><td>21.35</td></tr><tr><td>Salient Spans</td><td>4.52</td><td>3.76</td><td>9.75</td><td>11.19</td><td>11.81</td><td>14.11</td><td>18.97</td><td>22.75</td></tr><tr><td>CaMeLS</td><td>4.62</td><td>6.19</td><td>9.92</td><td>12.41</td><td>13.87</td><td>15.74</td><td></td><td></td></tr><tr><td>MAC</td><td>7.55</td><td>10.58</td><td>11.84</td><td>15.26</td><td>14.01</td><td>17.12</td><td>20.12</td><td>23.90</td></tr><tr><td>CMT (ours)</td><td>8.15</td><td>11.03</td><td>12.28</td><td>16.12</td><td>14.55</td><td>18.01</td><td>21.73</td><td>25.40</td></tr></table></body></html>

Baseline We include the online fine-tuning baselines introduced in Tack et al. (2024), including Uniform, Salient Spans, CaMeLS and MAC. The uniform baseline uses uniform token weighting kearning documents and involves additional finetuning for question answering after adaptation. Salient Spans assigns uniform weights to tokens in salient spans (Guu et al. 2020) and no weights to other tokens. CaMeLS leverages the output of a token-weighting language model (i.e., metalearned to predict important tokens to maximize the performance of the adapted LLM). Memory of Amortized Contexts (MAC) is an efficient online learning framework that uses the modulation to integrate new document knowledge.

# Model Comparison (RQ1)

Table 1 illustrates the performance of CMT in online adaptation compared to other baselines. CMT consistently outperforms these baseline methods across all datasets and models, demonstrating its superior capabilities in continual learning and knowledge retention on online adaption. These advantages are particularly evident in larger models, indicating that CMT effectively scales with model size and complexity. For instance, in the StreamingQA dataset, CMT consistently surpasses other methods for all model variants. Specifically, with DistilGPT2, CMT achieves an EM score of 6.43 and an $F _ { 1 }$ score of 12.32, outperforming the next best method, MAC, which scores 5.59 (EM) and 10.18 $( F _ { 1 } )$ . This performance gap widens with larger models, with CMT achieving the highest scores on Llama-2 (EM: 18.36, $F _ { 1 }$ : 25.98). This demonstrates CMTâ€™s superior ability to incorporate and retain new knowledge in real-time. Furthermore, CMT is efficient in terms of memory usage and adaptation time. Unlike CaMeLS, CMT does not require gradient computation for updates. Furthermore, CMT reduces the proportion of trainable parameters by 5.4 times and inference time due to top- $k$ aggregation compared to MAC, facilitating easier scalability with larger document corpora and model sizes.

# Ablation Study (RQ2)

This section presents an ablation study to assess the impact of various components of the CMT on its performance. Table 2 summarizes the results across three datasets. We evaluate the full CMT model (1) and three variants: CMT without the Memory-Aware Objective (2), CMT without Self-Matching (3), and CMT without Top- $k$ Aggregation (4). For example, in the StreamingQA dataset, the full CMT model achieves an EM score of 18.36 and an $F _ { 1 }$ score of 25.98, outperforming all other variants. The removal of the Memory-Aware Objective (variant 2) slightly increases the EM score to $1 8 . 5 4$ but decreases the $F _ { 1 }$ score to 23.71. The absence of SelfMatching (variant 3) results in lower scores (EM: 17.87, $F _ { 1 }$ : 22.54), indicating the importance of this component. The variant without Top- $\mathbf { \nabla } \cdot k$ Aggregation (4) shows the lowest performance (EM: 16.43, $F _ { 1 }$ : 20.13), highlighting its critical role in CMT. The ablation study reveals the relative importance of each CMT component. The Memory-Aware Objective and Top- $k$ Aggregation are crucial for maximizing performance across most datasets. The Top- $k$ aggregation also helps by focusing on relevant memory units, reducing inference latency by $1 8 \%$ . Self-Matching, while generally beneficial, can sometimes be omitted without severe performance degradation, as seen in the ArchivalQA results. However, the full CMT model consistently provides the best or near-best performance, validating the integrated approach.

<html><body><table><tr><td rowspan="2">#</td><td rowspan="2">Method</td><td colspan="2">StreamingQA</td><td colspan="2">SQuAD</td><td colspan="2">ArchivalQA</td></tr><tr><td>EM</td><td>F1</td><td>EM</td><td>F1</td><td>EM</td><td>F1</td></tr><tr><td>(1)</td><td>CMT</td><td>18.36</td><td>25.98</td><td>19.54</td><td>25.50</td><td>21.73</td><td>25.40</td></tr><tr><td>(2)</td><td>w/o Memory-Aware Objective</td><td>18.54</td><td>23.71</td><td>15.38</td><td>22.77</td><td>20.89</td><td>24.18</td></tr><tr><td>(3)</td><td>w/o Self-Matching</td><td>17.87</td><td>22.54</td><td>17.97</td><td>23.40</td><td>22.43</td><td>25.68</td></tr><tr><td>(4)</td><td>w/o Top-k Aggregation</td><td>16.43</td><td>20.13</td><td>18.35</td><td>24.12</td><td>21.09</td><td>23.99</td></tr></table></body></html>

Table 2: Results of ablation study where the best results are boldfaced and the second-best results are underlined.

![](images/db4e4feca0a60ec31dede61d117654ec5cce6a04ce8cf850984276a9d1cb3adf.jpg)  
Figure 3: Knowledge retention analysis under Llama-2-7b trained on StreamingQA dataset.

# Knowledge Retention (RQ3)

Following Tack et al. (2024), we evaluate retention ratio determined by the decline in the $F _ { 1 }$ score of the initially adapted 200 documents during online adaption. As shown in Figure 3, CMT and CaMeLS lead in performance, followed by MAC, Salient, and finally the Uniform method. The plot indicates that all methods reduce from an increased number of online adaptation documents, as shown by the downward trends in both $F _ { 1 }$ score and retention rate. However, the rate of improvement varies among the methods. Methods like CMT and CaMeLS show a higher scalability factor, indicating that they are better suited for environments where the volume of adaptation data is substantial. The gap between the highest (CMT) and the lowest performing method (Uniform) widens as the number of documents increases, highlighting the importance of choosing a more efficient method for large-scale online adaptation tasks.

![](images/113c0ac636be129d0e3de5306093b39ae0ae433da10f52ab0fa2256c1f43b096.jpg)  
Figure 4: Performance of robustness analysis experiments. We use synthetic unrelated documents to test the impact of irrelevant interference brought by memory integration.

# Robustness Analysis (RQ4)

We make use of irrelevant synthetic data, which is obtained by generating text that is irrelevant to the current document using $\mathtt { g p t - 4 o }$ . As shown in Figure 4, the performance of the Uniform shows a significant decline as the proportion of irrelevant documents increases. Initially maintaining a high relative $F _ { 1 }$ score, the performance deteriorates sharply beyond the $20 \%$ , indicating a high sensitivity to irrelevant data. CaMeLS exhibits a more robust performance compared to the Uniform method. However, a noticeable performance drop is still observed beyond the $60 \%$ threshold. The MAC method demonstrates a relatively stable performance across different proportions of irrelevant documents. While there is a gradual decline in the $F _ { 1 }$ score, it is less pronounced compared to the Uniform and CaMeLS methods, highlighting MACâ€™s effectiveness in handling irrelevant data. Among the four methods, CMT shows the best performance stability. The relative change in $F _ { 1 }$ score remains minimal even as the proportion of irrelevant documents approaches $100 \%$ .

# Conclusion

We propose a continual learning method for LLMs named CMT including the memory bank in a latent space as the modelâ€™s updatable knowledge parameters. CMT can update the memory with new knowledge, enabling effective knowledge integration and slow forgetting of prior knowledge.