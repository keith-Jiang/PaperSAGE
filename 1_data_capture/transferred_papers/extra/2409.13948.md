# Aligning Language Models Using Follow-up Likelihood as Reward Signal

Chen Zhang1, Dading Chong2, Feng Jiang3,4,5\* Chengguang Tang6, Anningzhe Gao4, Guohua Tang6, Haizhou Li1,3,4

1National University of Singapore, Singapore 2Peking University, China   
3The Chinese University of Hong Kong, Shenzhen, China 4Shenzhen Research Institute of Big Data, China   
5University of Science and Technology of China, China 6Tencent AI Lab, China chen zhang@u.nus.edu, jeffreyjiang@cuhk.edu.cn

# Abstract

In natural human-to-human conversations, participants often receive feedback signals from one another based on their follow-up reactions. These reactions can include verbal responses, facial expressions, changes in emotional state, and other non-verbal cues. Similarly, in human-machine interactions, the machine can leverage the user’s follow-up utterances as feedback signals to assess whether it has appropriately addressed the user’s request. Therefore, we propose using the likelihood of follow-up utterances as rewards to differentiate preferred responses from less favored ones, without relying on human or commercial LLM-based preference annotations. Our proposed reward mechanism, “Follow-up Likelihood as Reward” (FLR), matches the performance of strong reward models trained on large-scale human or GPT-4 annotated data on 8 pairwise-preference and 4 rating-based benchmarks. Building upon the FLR mechanism, we propose to automatically mine preference data from the online generations of a base policy model. The preference data are subsequently used to boost the helpfulness of the base model through direct alignment from preference (DAP) methods, such as direct preference optimization (DPO). Lastly, we demonstrate that fine-tuning the language model that provides follow-up likelihood with natural language feedback significantly enhances FLR’s performance on reward modeling benchmarks and effectiveness in aligning the base policy model’s helpfulness.

# 1 Introduction

The recent development of large language models (LLMs) has revolutionized the field of natural language processing (Zhao et al. 2023a). Many chat-based LLMs are trained to align a large-scale pretrained language model to human preferences using techniques such as supervised fine-tuning (SFT) (Wei et al. 2022; Chung et al. 2022), reinforcement learning from human feedback (RLHF) (Christiano et al. 2017; Ziegler et al. 2019; Ouyang et al. 2022), or direct alignment from preferences (DAP) (Rafailov et al. 2023;

How can I explain quantum theory to a five year old? Quantum theory is a fundamental theory in physics that 德 You are not following R my instruction. (a) Negative follow-up utterance from the user. 酒 How can I explain quantum B theory to a five year old? Imagine you have a magical toy box that can create any toy ..... 國 Thank you for your help. R

(b) Positive follow-up utterance from the user.

Zhao et al. 2023b; Ethayarajh et al. 2024). All these techniques heavily rely on high-quality SFT or preference data, but collecting such data through human efforts is extremely expensive.

Lately, there has been a growing trend in the selfevolution of LLMs. This process allows the models to autonomously obtain, improve, and learn from their own generated experiences (Tao et al. 2024), reducing the need for costly human supervision. For instance, Yuan et al. (2024) introduces self-rewarding language models, starting with bootstrapping instructions from a base policy model to generate candidate responses. The model then employs “LLMas-a-Judge” prompting (Zheng et al. 2023a) to evaluate its own outputs, creating a collection of self-curated preference data. The data are then used to align the language model through directed preference optimization (Rafailov et al. 2023). In a similar vein, many recent works (Xu et al. 2023c; Gulcehre et al. 2023; Chen et al. 2024b; Guo et al. 2024; Li et al. 2024c; Zhang et al. 2024b, inter alia) explore using online generations of language models for self-improvement in an iterative manner. At the core of these works is an automated procedure for response quality annotation, which utilizes either self or external reward signals to rank or score the response candidates (Chen et al. 2024a). The reliability of this procedure depends significantly on the reward model’s language understanding and quality discrimination capabilities, which highly correlate with the model’s scale and the quality of its training data (Zhang et al. 2024a). For example, GPT-4 (OpenAI 2023) emerges as an ideal annotator due to its large model size and high-quality instruction-tuning data.

However, training a reward model similar to GPT-4 or using API services to prompt such a model is resourceintensive. Therefore, finding alternative indicators of response quality is necessary. Drawing inspiration from natural human-human interactions where speakers continuously adapt their speech based on real-time feedback from their conversational partners, we postulate that the quality of the LLMs’ responses may be implicitly derived from the real users’ reactions, their follow-up utterances in particular. For instance, if an AI assistant is not following the user’s instructions, the users are more likely to express disagreement or criticism than compliments or gratitude. This is illustrated by the examples in Figure 1.

Considering that it is infeasible to conduct large-scale quality annotation with an interactive setup between a real user and the language model, where the user provides realtime feedback to the language model generations, we approximate the user using an instruction-tuned LLM, which can either be the base policy model or another LM. This approach is motivated by the fact that existing instructiontuned LLMs are fine-tuned using high-quality chat-style data (Chiang et al. 2023; Xu et al. 2023a) and hence, possess the knowledge of human conversational dynamics. Consequently, their likelihood of generating negative feedback for an appropriate response is lower than that of generating positive feedback, and vice versa.

Hence, we manually curate a set of positive and negative follow-up utterances. The conditional log probability of these follow-up utterances, given a prompt-response pair, serves as a reward signal to determine the helpfulness of the response. While prior works (Mehri and Eskenazi 2020; De Bruyn et al. 2022) utilize the likelihood of follow-up utterances as an automatic evaluation metric for open-domain dialogues, our work takes a significant leap beyond mere dialogue evaluation. We leverage follow-up utterances for automatic preference annotations of online generations of the policy model without the need for an external reward model or human efforts. Such an automatic annotation procedure simplifies the alignment process and opens the door to more autonomous and efficient LLM optimization.

In summary, we make the following contributions:

• We introduce a “Follow-up Likelihood as Reward” (FLR)1 that automatically annotates preference data from base policy outputs. The annotated data is used for DAP fine-tuning, significantly enhancing the helpfulness of the base policy model. For example, Llama-3- 8B-Instruct’s length-controlled win rate is improved by $4 . 4 5 \%$ on Alpaca-Eval V2 after fine-tuning (§5.3).

• We demonstrate that FLR, without additional training, matches the performance of strong reward models, which are trained on large-scale human-annotated or GPT-4 annotated preference data, across 8 pairwise preference and 4 rating-based benchmarks (§5.1).   
• Lastly, we demonstrate that fine-tuning the LM used in FLR with natural language feedback data significantly enhances the performance on the reward modeling task (§5.1) and results in better helpfulness alignment of the base policy LLM, compared to using the original FLR without fine-tuning (§5.3).

# 2 Related Work

Reward Model Reward models play a pivotal role in the LLM alignment pipeline, serving as proxies for human judgment to guide policy models in distinguishing between desirable and undesirable outputs. Lambert et al. (2024) categorizes reward models into three primary types: 1) classifierbased, 2) DPO-based, and 3) prompting or generation-based. The first category, exemplified by models such as StarlingRM-7B-alpha (Zhu et al. 2023), Oasst RM, and ArmoRMLlama3-8B-v0.1 (Wang et al. 2024a), involves training a sequence classifier on top of a language model using humanannotated preference data. The second category, including Tulu-2 (Wang et al. 2023) and Zephyr-7b-alpha (Tunstall et al. 2023), directly computes rewards based on the conditional log probability of the response. The third category, such as auto-J (Li et al. 2024a), Prometheus (Kim et al. 2024b), and GPT-based LLMs (OpenAI 2023), uses a prompting approach to generate reward scores, following the LLM-as-a-Judge (Zheng et al. 2023a) framework. The first category demands additional training efforts on large-scale human-annotated data to achieve satisfactory performance, while the third category typically requires a powerful, largescale instruction-following LLM to attain similar levels of performance. Training such an LLM is exceedingly costly. Experiments in both Lambert et al. (2024) and ours demonstrate the performance of the second category is sub-optimal compared to the first due to the lack of additional training.

However, FLR differs from all three categories. It shares the advantage of the second category by not requiring additional training on human-annotated data, yet it achieves performance comparable to that of the first category. Additionally, FLR paves the way for future research into leveraging feedback signals from real users in reward modeling.

LLM Alignment Post-training alignment of large language models has become a key focus in LLM research. Ouyang et al. (2022) demonstrated the efficacy of the RLHF pipeline in enhancing GPT-3’s ability to follow instructions. Subsequent studies have explored alternatives to costly reinforcement training by proposing techniques for direct alignment from preferences (Rafailov et al. 2023; Ethayarajh et al. 2024; Zhao et al. 2023b). Additionally, there has been significant interest in the self-improvement capabilities of LLMs (Yuan et al. 2024; Guo et al. 2024; Wu et al. 2024b) leveraging the DAP techniques and preference annotations on the online generation of the policy model. Our work focuses on reward modeling and the automatic annotation of preference data, making it applicable to all these algorithms. We also demonstrate the superiority of FLR in automatic preference annotation.

Recent studies have also explored the use of natural language feedback, produced by either humans or large language models (LLMs), to enhance the interpretability of LLM evaluations (Cui et al. 2023; Madaan et al. 2023; Wu et al. 2024a). This approach not only improves their utility but also aids in enhancing their helpfulness. Unlike existing methods, our work transforms this feedback into real-user follow-ups and utilizes it to refine the follow-up likelihood estimation in LLMs, thereby advancing the reward modeling capabilities of the proposed FLR mechanism.

# 3 Methodology

In this section, we begin by formally defining the tasks. Next, we describe the “Follow-up Likelihood as Reward (FLR)” procedure. Finally, we outline the automatic annotation process for the base language model’s generations and explain how the data are used to align the base model.

# 3.1 Task Formulation

Rating-Based Quality Annotation Given a chat-style instruction-tuning dataset $\{ p _ { i } , r _ { i } \} \in \mathcal { D }$ , $p _ { i }$ is chat history between a user and a policy{ mode}l and $\boldsymbol { r } _ { i }$ is the model completion. $p _ { i }$ can be a single-turn instruction prompt from the user or a multi-turn interaction between the user and the policy model up to the most recent user turn. Based on certain criteria, we need to score $\boldsymbol { r } _ { i }$ . Denote the score as $s _ { r _ { i } }$ . In this paper, we focus on the helpfulness criteria of the completion as defined in Askell et al. (2021). We adopt an instruction-tuned LLM, denoted as $\mathcal { M }$ to conduct the FLR scoring mechanism. The effectiveness of FLR can be quantified by the agreement between $\{ s _ { r _ { i } } \} _ { i = 1 } ^ { | \mathcal { D } | }$ and the corresponding ground-truth ratings, $\{ g _ { r _ { i } } \} _ { i = 1 } ^ { \lvert \mathcal { D } \rvert }$ .}Common agreement metrics include Pearson, Sp{earm}an, and Kendall correlation coefficients. In most open-source instruction-tuning benchmarks (Ye et al. 2024; Kim et al. $2 0 2 4 \mathrm { a }$ ; Wang et al. 2024b), the ground-truth ratings are typically obtained via human evaluation or API-based prompting of GPT-4.

Preference Data Annotation Most formulations of preference annotations is similar to rating-based quality annotation. The difference is that instead of a single completion $\boldsymbol { r } _ { i }$ , there are $k \ \geq \ 2$ candidates to score. The preference dataset, $\mathcal { D }$ consists of ri1, ri2, . . . , rik . Our goal is to rank $\{ r _ { i } ^ { 1 } , r _ { i } ^ { 2 } , \ldots , r _ { i } ^ { k } \}$ according to their extent of helpfulness I{n this paper, w}e primarily study the special case of $k = 2$ and the effectiveness of the FLR scoring mechanism is quantified by the accuracy of scoring human preferred completion, $r _ { i } ^ { + }$ , higher than the disfavored completion, $r _ { i } ^ { - }$ , i.e., $s _ { r _ { i } ^ { + } } > s _ { r _ { i } ^ { - } }$ .

# 3.2 Follow-up Likelihood as Reward (FLR)

Instead of directly prompting LLMs (Zheng et al. 2023a) or explicitly training a reward model, we leverage natural follow-up utterances as implicit feedback to reflect the helpfulness of $r _ { i }$ . This approach is grounded in the observation that many powerful instruction-tuned LLMs (Dubey et al. 2024; Yang et al. 2024) are fine-tuned on high-quality conversational data that inherently reflect human-like feedback dynamics, whereby helpful responses are more likely to be followed by positive feedback, while unhelpful responses are more likely to trigger negative reactions. This procedure does not require additional preference data collection or the training of a separate reward model. It relies on the language-understanding capability of the LLMs.

Let us define a positive follow-up utterance as $f _ { j } ^ { + }$ and a negative follow-up utterance as $f _ { j } ^ { - }$ . The log probabilities of the instruction-tuned LLM, $\mathcal { M }$ , generating $f _ { j } ^ { + }$ and $f _ { j } ^ { - }$ conditioned on $\{ p _ { i } , r _ { i } \}$ are given by:

$$
\log p _ { \mathcal { M } } ( f _ { j } ^ { * } \mid p _ { i } , r _ { i } ) = \sum _ { t = 1 } ^ { T } \log p _ { \mathcal { M } } ( f _ { j , t } ^ { * } \mid p _ { i } , r _ { i } , f _ { j , < t } ^ { * } )
$$

where $f _ { j } ^ { * }$ can be either $f _ { j } ^ { + }$ or $f _ { j } ^ { - }$ and $T$ represents the total number of tokens in $f _ { j } ^ { * }$ .

Due to the large space of potential positive and negative follow-ups of $\{ p _ { i } , r _ { i } \}$ , relying on a single follow-up can introduce bias. T{herefo}re, it is essential to consider a diverse set of follow-up utterances to accurately assess response quality. However, conducting an exhaustive search for all contextually relevant follow-ups is intractable. As a solution, we manually curate a set of positive and negative followups based on the decomposition of helpfulness criteria. We leave the automatic search for suitable follow-ups to future work. Since helpfulness is an abstract concept, we decompose it into three fine-grained sub-categories: understanding, engagingness, and instruction-following. This decomposition is also motivated by prior works (Wu et al. 2023; Wang et al. 2024b). For each category, we come up with 10 different positive and negative follow-ups respectively. Table 1 gives some examples and the full list is in Appendix $\boldsymbol { \mathrm { A } } ^ { 2 }$ .

We define the set of positive follow-ups for a particular sub-category, $t$ , as ${ \boldsymbol { F } } _ { t } ^ { + }$ where $f _ { j } ^ { + } \in \boldsymbol { F } _ { t } ^ { + }$ and the set of negative follow-ups for a particular sub-category as $\boldsymbol { F } _ { t } ^ { - }$ where $f _ { j } ^ { - } \in \boldsymbol { F } _ { t } ^ { - }$ . To score $\{ p _ { i } , r _ { i } \}$ based on the follow-ups of a particular sub-catego {y, we d}efine the following reward:

$$
\begin{array}{c} s _ { r _ { i } } ^ { t } = \frac { \displaystyle \sum _ { f _ { j } ^ { + } \in F _ { t } ^ { + } } \log p _ { \mathcal M } ( f _ { j } ^ { + } \mid p _ { i } , r _ { i } ) } { \lvert F _ { t } ^ { + } \rvert }  \\ { - \frac { \displaystyle \sum _ { f _ { j } ^ { - } \in F _ { t } ^ { - } } \log p _ { \mathcal M } ( f _ { j } ^ { - } \mid p _ { i } , r _ { i } ) } { \displaystyle \lvert F _ { t } ^ { - } \rvert } } \end{array}
$$

The above reward signal is directly applied to solve the rating-based quality annotation task by average aggregation of scores across the three sub-categories3.

<html><body><table><tr><td>Category</td><td>Positive/Negative Follow-Up Example</td></tr><tr><td>Understanding</td><td>That makes perfect sense!(√) That makes no sense! (X)</td></tr><tr><td>Engagingness</td><td>This is otyiyintisti(g) (x)</td></tr><tr><td>Instruction-Following</td><td>Youdid tfantstic jofollowinmy instructions. (V)</td></tr></table></body></html>

Table 1: Examples of positive and negative follow-ups for each sub-category of helpfulness.

$$
s _ { r _ { i } } = \frac { \displaystyle \sum _ { t \in T } s _ { r _ { i } } ^ { t } } { | T | }
$$

For the pairwise preference annotation task, an ideal $\mathcal { M }$ satisfies the following constraint: $s _ { r _ { i } ^ { + } } ^ { t } > s _ { r _ { i } ^ { - } } ^ { t } \Rightarrow s _ { r _ { i } ^ { + } } > s _ { r _ { i } ^ { - } }$

It is worth noting that FLR can seamlessly integrate into the autonomous self-evolution pipeline of language models via online DAP procedures (details in $\ S 3 . 4 )$ . it does not rely on collecting offline human preference data or external model supervision but instead leverages the internal knowledge of strong instruction-tuned models.

# 3.3 Enhance FLR with Natural Feedback Data

To enhance the ability of $\mathcal { M }$ in assigning high likelihoods to positive follow-ups and low likelihoods to negative followups for helpful responses, and vice versa for unhelpful responses, we finetune $\mathcal { M }$ using natural language feedback data using a chat template as shown below

User: Please ask me a question or assign me a task.

LLM: [insert here the instruction prompt, $p _ { i } ]$

User: [insert here the response, $r _ { i } ]$

LLM: [insert here the feedback to $( p _ { i } , r _ { i } )$ , which is denoted as $l _ { i } ]$

In the chat template, the roles of the LLM and user are switched. A feedback data example can be found in Appendix B. During the supervised fine-tuning of $\mathcal { M }$ , the model is optimized to generate the natural language feedback, $l _ { i }$ . To curate training data, a common approach is to mine it from real user-bot interactions. However, this realworld data often contains significant noise and demands costly cleaning efforts. As an alternative, we rewrite feedback from third-party evaluators in an existing dataset, transforming it into naturally occurring utterances with a conversational, first-person tone. Specifically, we prompt GPT3.5-Turbo to rewrite the feedback sentences in a first-person tone. This strategy aligns the feedback more closely with real-user follow-ups. The instruction template to prompt ChatGPT is presented in Appendix C.

![](images/c681c14227bfc6c5aa0083cf86a39446fd05aa698e386a2339e44989548cb767.jpg)  
Figure 2: The procedure of aligning base LM. FLR denotes the “follow-up likelihood as reward” mechanism.

# 3.4 Aligning Base Language Model

Figure 2 presents the overall procedure of aligning a base policy model $\pi$ . The main idea is to first sample instruction prompts $p _ { i }$ from a prompt source and $\pi$ generates $k$ response candidates, $\{ r _ { i } ^ { 1 } , r _ { i } ^ { 2 } , \ldots , r _ { i } ^ { k } \}$ . Each candidate is assigned a reward score{as defined in E}q.3. The candidate with the highest score is treated as the positive response $r _ { i } ^ { + }$ . The candidate with the lowest score is treated as the negative response $r _ { i } ^ { - }$ . This strategy helps avoid using similar pairs with slight differences in reward scores, which may confuse the generation model. Additionally, if the obtained pair contains responses with the same reward scores, it is filtered out. We experiment with DPO (Rafailov et al. 2023) and KTO (Ethayarajh et al. 2024) fine-tuning of $p _ { i }$ using the synthetic preference dataset constructed with the above procedure.

# 4 Experiment Setup

There are two parts of our experiments: (1) reward modeling and (2) helpfulness alignment. For (1), reward models are assessed on human-annotated pairwise preference and ratingbased quality evaluation benchmarks (both tasks have been introduced in $\ S 2 . 1 \ r ,$ ). (2) involves evaluating the helpfulness alignment of policy models after fine-tuning using the automatically curated preference data with FLR. $\ S 3 . 1$ presents the training details. $\ S 3 . 2$ introduces benchmarks and evaluation methodology used in both parts of the experiments. $\ S 3 . 3$ describes the baselines for comparison.

# 4.1 Training Details

For $\mathcal { M }$ , we experiment with Llama-3-8B-Instruct and Qwen2-7B-Instruct. We utilize the UltraFeedback (Cui et al.

Table 2: Statistics of the reward model benchmarks.   

<html><body><table><tr><td>Test Datasets</td><td>Size</td><td>Avg. #Prompt Words</td><td>Avg. #Turns per Prompt</td><td>Type</td></tr><tr><td>HH-RLHFHelpfulness (2022) (HH) BeaverTails Helpfulness (2023) (BH) SHP (2022) Alpaca-Farm (2023) (AF) MT-Bench Pairwise (2023a) (MP) RewardBench ChatEasy (2024) (RCE)</td><td>6,238 2,985 18,409 17,701 916 358</td><td>93.05 13.17 148.79 28.57 235.73 41.92</td><td>2.38 1.00 1.00 1.00 3.00 1.00</td><td>Pairwise Pairwise Pairwise Pairwise Pairwise Pairwise</td></tr><tr><td>RewardBench ChatHard (2024) (RCH) Preference Bench (2024a) (PB) Feedback Bench (2024a) (FH) FLASK (2024) MT-Bench Rating (2023a) (MR)</td><td>456 1,998 1,000 1,500 1,000</td><td>30.45 79.21 79.25 70.29 141.97</td><td>1.00 1.00 1.00 1.00</td><td>Pairwise Pairwise Rating Rating</td></tr></table></body></html>

2023) dataset for rewriting, which is then employed to finetune $\mathcal { M }$ . UltraFeedback contains 255,548 instances of $( p _ { i }$ , $r _ { i } , l _ { i } )$ and we randomly sample 100K for fine-tuning.

For the helpfulness alignment experiment, we also adopt Llama-3-8B-Instruct and Qwen2-7B-Instruct as the base policy model $\pi$ . LoRA (Hu et al. 2022) is applied to all finetuning experiments on a single NVIDIA A100 80GB GPU. The experimental settings for DPO and KTO fine-tuning follow the implementations from LLaMA-Factory (Zheng et al. 2024). We adopt the Nectar dataset (Zhu et al. 2023) as the prompt source. The 183K prompts in Nectar are a mixture of diverse sources, including lmsys-chat-1M (Zheng et al. 2023b), ShareGPT, Anthropic/HH-RLHF (Bai et al. 2022), UltraFeedback, Evol-Instruct (Xu et al. 2023b), and Flan (Longpre et al. 2023). For our experiments, we randomly sampled 100K instruction prompts from Nectar. Full details on reproducibitlity can be found in Appendix E.

# 4.2 Benchmarks and Evaluation

We assess the reward models using eight pairwise preference benchmarks and four rating-based single-response benchmarks, with their statistics in Table 2. Accuracy is reported for the pairwise benchmarks and Pearson correlation for the rating-based benchmarks. Additionally, we examine the reward models’ ability to rank different LLMs by providing system-level correlations between the reward model scores and real-user ELO ratings from the LMSys Chatbot Arena (Chiang et al. 2024). We obtain the Arena dataset4 from Lin et al. (2024) and it consists of 865 data instances per LLM for a total of 30 LLMs, including GPT4-turbo (OpenAI 2023), Meta-Llama-3-70B-Instruct, and Mixtral-8x7B-Instruct (Jiang et al. 2024).

To evaluate FLR’s contribution to helpfulness alignment, we use well-established benchmarks including Alpaca-Eval V2 (Li et al. 2023), WildBench V2 (Lin et al. 2024), and FLASK (Ye et al. 2024). Alpaca-Eval V2, WildBench V2, and FLASK contain 805, 1024, and 1700 instruction prompts respectively. WildBench V2 involves pairwise comparisons with the base policy model as the reference. For

Alpaca-Eval V2, we report the length-controlled win rate against GPT-4 Preview (11/06) as per standard protocol, and the “alpaca eval gpt4 turbo fn” annotator config is adopted. For FLASK, the GPT-4 evaluator is adopted to rate the quality of the model responses according to specific prompts. Furthermore, we assess the general understanding capability of the aligned models on the Open LLM benchmark. We follow the official implementations of the benchmarks and the fp16 model variants are used for inference.

# 4.3 Baselines

Our proposed FLR mechanism is compared to stateof-the-art reward models, including Prometheus-7bv2.0 (Kim et al. 2024b), GPT-3.5-Turbo, Starling-RM-7Balpha (Zhu et al. 2023), oasst-rm-2-pythia-6.9b-epoch-1, PairRM (Jiang, Ren, and Lin 2023), and ArmoRM-Llama3- 8B-v0.1 (Wang et al. 2024a). It’s important to note that these RMs are trained on a diverse mix of high-quality, humanannotated, or GPT-annotated preference data, whereas FLR operates without such annotations. As PairRM is trained only for preference annotations, its performance on rating-based benchmarks is not reported.

Moreover, FLR is compared to the direct scoring of responses based on their log probability, as judged by DPOfinetuned LMs. These include Tulu-2-dpo-7b (Wang et al. 2023), Qwen2-7B-Instruct (Yang et al. 2024), Zephyr-7balpha (Tunstall et al. 2023), and Llama-3-8B-Instruct. We follow the official implementation of RewardBench (Lambert et al. 2024) and prometheus-eval (Kim et al. 2024a).

For the alignment experiment, we primarily study how the alignment performance changes with respect to the base policy model. Note that we focus on models at the 7B scale due to their good balance between performance and computational resource requirements.

# 5 Experiment

This section presents the results and analysis of the reward modeling and alignment experiments.

Table 3: Results on the pairwise preference and rating-based helpfulness benchmarks. AVG-P $( \% )$ and AVG-R denote the average scores of pairwise preference and rating-based benchmarks respectively. The HH score for ArmoRM is omitted as it is trained on the data, achieving nearly $100 \%$ accuracy. FT refers to fine-tuning with the natural language feedback. Top-3 scores for each benchmark are highlighted in bold. “Direct” denotes direct scoring of the response by the LM using the log probability.   

<html><body><table><tr><td>Models</td><td>HH</td><td>BH</td><td>SHP</td><td>AF</td><td>MP</td><td>RCE</td><td>RCH</td><td>PB</td><td>AVG-P</td><td>MR</td><td>FLASK</td><td>FB</td><td>HelpSteer</td><td>AVG-R</td></tr><tr><td>Direct (Tulu-2-dpo-7b)</td><td>55.59</td><td>58.99</td><td>45.83</td><td>55.90</td><td>63.65</td><td>86.59</td><td>39.04</td><td>62.51</td><td>58.51</td><td>0.114</td><td>0.111</td><td>0.206</td><td>0.167</td><td>0.150</td></tr><tr><td>Direct (Qwen2-7B-Instruct)</td><td>54.15</td><td>52.86</td><td>40.81</td><td>54.77</td><td>63.43</td><td>86.03</td><td>47.37</td><td>54.50</td><td>56.74</td><td>0.184</td><td>0.117</td><td>0.050</td><td>0.077</td><td>0.107</td></tr><tr><td>Direct (Zephyr-7b-alpha)</td><td>53.01</td><td>54.17</td><td>40.49</td><td>56.29</td><td>61.35</td><td>80.17</td><td>53.73</td><td>61.41</td><td>57.58</td><td>0.191</td><td>0.004</td><td>0.201</td><td>0.057</td><td>0.113</td></tr><tr><td>Direct (Llama-3-8B-Instruct)</td><td>55.51</td><td>54.74</td><td>42.64</td><td>54.90</td><td>62.88</td><td>86.31</td><td>43.86</td><td>49.25</td><td>56.26</td><td>0.183</td><td>0.089</td><td>0.024</td><td>0.127</td><td>0.106</td></tr><tr><td>GPT-3.5-Turbo</td><td>59.03</td><td>63.87</td><td>61.17</td><td>59.70</td><td>71.89</td><td>82.26</td><td>43.09</td><td>84.18</td><td>65.65</td><td>0.439</td><td>0.016</td><td>0.023</td><td>0.025</td><td>0.126</td></tr><tr><td>Prometheus-7B</td><td>62.95</td><td>66.03</td><td>55.13</td><td>64.01</td><td>80.35</td><td>92.18</td><td>48.03</td><td>95.45</td><td>70.52</td><td>0.312</td><td>0.251</td><td>0.871</td><td>0.390</td><td>0.456</td></tr><tr><td>Oasst-Pythia-6.9B</td><td>62.18</td><td>60.67</td><td>68.61</td><td>56.21</td><td>72.93</td><td>91.9</td><td>37.72</td><td>77.73</td><td>65.99</td><td>0.284</td><td>0.137</td><td>0.526</td><td>0.329</td><td>0.319</td></tr><tr><td>PairRM</td><td>62.06</td><td>56.82</td><td>54.91</td><td>58.04</td><td>76.09</td><td>84.64</td><td>50.66</td><td>81.03</td><td>65.53</td><td>-</td><td></td><td>-</td><td></td><td>1</td></tr><tr><td>Starling-RM-7B-alpha</td><td>63.11</td><td>71.69</td><td>59.67</td><td>60.20</td><td>78.38</td><td>94.13</td><td>40.35</td><td>86.69</td><td>69.28</td><td>0.492</td><td>0.235</td><td>0.762</td><td>0.449</td><td>0.485</td></tr><tr><td>ArmoRM-Llama3-8B-v0.1</td><td></td><td>63.72</td><td>68.87</td><td>60.04</td><td>77.40</td><td>97.21</td><td>76.54</td><td>90.59</td><td>76.34</td><td>0.406</td><td>0.348</td><td>0.778</td><td>0.452</td><td>0.496</td></tr><tr><td>FLR (Llama-3-8B-Instruct)</td><td>58.66</td><td>55.71</td><td>57.79</td><td>56.07</td><td>74.56</td><td>90.50</td><td>63.16</td><td>75.03</td><td>66.44</td><td>0.555</td><td>0.285</td><td>0.509</td><td>0.383</td><td>0.433</td></tr><tr><td>FLR (Qwen2-7B-Instruct)</td><td>57.23</td><td>58.29</td><td>59.37</td><td>53.39</td><td>71.62</td><td>80.45</td><td>55.26</td><td>73.12</td><td>63.59</td><td>0.403</td><td>0.157</td><td>0.423</td><td>0.296</td><td>0.320</td></tr><tr><td>FLR-FT (Llama-3-8B-Instruct)</td><td>62.92</td><td>65.09</td><td>61.19</td><td>59.99</td><td>79.91</td><td>95.53</td><td>55.92</td><td>87.64</td><td>71.02</td><td>0.606</td><td>0.430</td><td>0.705</td><td>0.566</td><td>0.577</td></tr><tr><td>FLR-FT (Qwen2-7B-Instruct)</td><td>62.50</td><td>61.11</td><td>57.17</td><td>59.28</td><td>72.93</td><td>92.18</td><td>69.30</td><td>85.39</td><td>69.98</td><td>0.571</td><td>0.504</td><td>0.701</td><td>0.524</td><td>0.575</td></tr></table></body></html>

# 5.1 Reward Modeling Results

Main Benchmark Results Table 3 presents the accuracy scores $( \% )$ and Pearson correlations of various reward models on 8 pairwise preference datasets and 4 rating-based helpfulness datasets respectively. First, we observe that FLR performs significantly better than directly using the response likelihood. For instance, the average pairwise preference accuracy of FLR (Llama-3-8B-Instruct) is $6 6 . 4 4 \%$ , compared to $56 . 2 6 \%$ for Direct (Llama-3-8B-Instruct), showing a roughly $10 \%$ difference. Additionally, the average Pearson correlation achieved by FLR (Llama-3-8B-Instruct) is 0.327 points higher than that of Direct (Llama-3-8BInstruct). We can also observe that FLR (Llama-3-8BInstruct) without fine-tuning outperforms GPT-3.5-Turbo, PairRM, and Oasst-rm-2-pythia- ${ \cdot 6 . 9 \mathrm { b } }$ -epoch-1, three strong lightweight reward models. These observations demonstrate that using real user follow-ups as an indication of response quality is a promising direction of reward modeling, given the presence of a strong instruction-tuned LM. Even compared to Prometheus-7b-v2.0, Starling-RM-7B-alpha, and ArmoRM-Llama3-8B-v0.1, which are trained on large-scale and high-quality human-annotated or GPT-4-annotated preference data, the performance of FLR without fine-tuning looks promising, especially on the rating-based benchmarks.

After fine-tuning with the natural language feedback data, the performance of FLR improves significantly. FLR (Llama-3-8B-Instruct - FT) achieves an average accuracy gain of $4 . 5 8 \%$ on the pairwise preference datasets and an increase of 0.144 in average Pearson correlation on the ratingbased benchmarks compared to FLR (Llama-3-8B-Instruct). Significant performance boost can also be observed when using Qwen2-7B-Instruct. Moreover, the FLR fine-tuned variants’ reward modeling performance consistently ranks among the top three of all reward models on most of the benchmarks. On average, FLR (Llama-3-8B-Instruct - FT) ranks third on pairwise preference benchmarks and first on rating-based benchmarks.

It’s important to note that while ArmoRM-Llama3-8Bv0.1 ranks first on pairwise preference benchmarks, it has been extensively trained on around 1 million annotated pairs from 10 diverse, high-quality human preference datasets. Similarly, Prometheus-7b-v2.0 has also been trained on 20K preference data and 20K single-rating data, both with highquality annotations. In contrast, vanilla FLR operates without such data, and its fine-tuning with approximately $1 0 0 \mathrm { K }$ natural language feedback samples isn’t directly optimized for pairwise preference reward modeling. Moreover, the feedback data can be curated from real user-bot interactions or evaluation logs from third-party evaluators, showcasing the potential of using more accessible and naturally occurring data sources for effective reward modeling, bypassing the need for costly human or commercial LLM annotations.

Table 4: System-level correlations on Chatbot Arena.   

<html><body><table><tr><td>Models</td><td>Pearson</td><td>Spearman</td><td>Kendall Tau</td></tr><tr><td>Prometheus-7B</td><td>0.683</td><td>0.673</td><td>0.492</td></tr><tr><td>Oasst-Pythia-6.9B</td><td>0.755</td><td>0.702</td><td>0.533</td></tr><tr><td>Starling-RM-7B-alpha</td><td>0.849</td><td>0.850</td><td>0.658</td></tr><tr><td>ArmoRM-Llama3-8B-v0.1</td><td>0.909</td><td>0.897</td><td>0.751</td></tr><tr><td>FLR (Llama3-8B-Instruct)</td><td>0.897</td><td>0.899</td><td>0.727</td></tr><tr><td>FLR(Qwen2-7B-Instruct)</td><td>0.631</td><td>0.586</td><td>0.454</td></tr><tr><td>FLR-FT(Llama3-8B-Instruct)</td><td>0.860</td><td>0.895</td><td>0.727</td></tr><tr><td>FLR-FT(Qwen2-7B-Instruct)</td><td>0.892</td><td>0.897</td><td>0.732</td></tr></table></body></html>

Correlation with Chatbot Arena In addition to ranking or rating responses, reward models can efficiently evaluate and rank the performance of different models. This capability provides researchers with quick feedback on model performance, facilitating faster model development and iteration. We examine whether FLR benefits such a use case. Table 4 presents the system-level correlations of the reward models with real-user ELO ratings on LMSys Chatbot Arena. We can see that FLR (Llama-3-8B-Instruct), FLR-FT (Llama-3-8B-Instruct), and FLR-FT (Qwen2-7B-Instruct) achieve strong performance, comparable to the state-of-theart reward model, ArmoRM-Llama3-8B-v0.1. The observations further reinforce the effectiveness of the FLR.

Table 5: Ablation study for the reward modeling task. The symbol “ ” denotes exclusion.   

<html><body><table><tr><td>Models FLR (Llama-3-8B-Instruct)</td><td>Avg-P 66.44</td><td>Avg-R</td></tr><tr><td>\Engagingness Understanding Following Positive Follow-ups Negative Follow-ups FLR(Llama-3-8B-Instruct -FT)</td><td>64.57 66.27 64.56 57.50 54.73 71.02</td><td>0.433 0.412 0.405 0.398 -0.118 0.169 0.577</td></tr><tr><td>\Engagingness Understanding Following Positive Follow-ups Negative Follow-ups</td><td>70.22 70.75 69.92 56.27 67.09</td><td>0.573 0.552 0.512 -0.034 0.484</td></tr></table></body></html>

# 5.2 FLR Ablation Study

As shown in Table 5, removing any of the three categories: engagingness, understanding, or instruction-following, leads to a performance reduction. This effect is observed in both Llama-3-8B-Instruct and Llama-3-8B-Instruct - FT. Removing the instruction-following category results in the most significant performance reduction, indicating that it is a crucial attribute of response helpfulness.

Additionally, we observe that using only positive followups or only negative follow-ups significantly reduces performance. Using only positive follow-ups means retaining only the first term in Eq.2, while using only negative follow-ups involves leveraging only the second term in Eq.2. Ideally, FLR should maximize the first term and minimize the second term when evaluating a good response, whereas a poor response would exhibit the opposite characteristics. Our proposal of using the score differences between positive and negative follow-ups is more robust and demonstrates significantly better performance.

# 5.3 Alignment Performance

Main Results Table 6 presents the alignment performance of policy models fine-tuned with DPO or KTO using preference data annotated with our proposed FLR mechanism, compared to the un-finetuned models5. Remarkably, we can observe that Llama-3-8B-Instruct can self-improve using our FLR mechanism without external supervision. For example, Llama-3-8B-Instruct $^ +$ FLR DPO achieves a $3 3 . 1 7 \%$ LC win rate on Alpaca-Eval V2, marking a $4 . 4 5 \%$ improvement. On WildBench V2, it records a $5 7 . 5 7 \%$ win rate over its base version. Meanwhile, Qwen2-7B-Instruct shows less significant self-improvement, with minor gains observed only on WildBench V2 and FLASK. This may be because FLR is sensitive to factors such as the training method and the type of training data used in the underlying language model. This insight further motivates enhancing FLR by fine-tuning the language model with natural language feedback data.

It can be observed that the fine-tuned LM delivers more accurate FLR rewards, i.e., better indication of response helpfulness. With FLR-FT DPO, both Llama-3-8B-Instruct and Qwen2-7B-Instruct demonstrate significant improvements across all benchmarks. Notably, Qwen2-7B-Instruct $^ +$ FLR DPO achieves the highest LC win rate of $34 . 2 6 \%$ on Alpaca-Eval V2 and a rating of 3.95 out of 5 on FLASK.

We can also observe that DPO fine-tuning yields superior alignment performance compared to KTO, consistent with previous findings (Rasul et al. 2024) that DPO outperforms KTO under paired preference settings. However, KTO’s flexibility lies in its ability to function without paired data, making it more versatile than DPO. It is worth noting that FLR is compatible with all the DAP algorithms.

General Understanding Capability We also evaluate whether fine-tuning policy models using preference data annotated with FLR enhances response helpfulness without compromising their general understanding capability. The average scores of different models on the Open LLM Leaderboard are presented in Table 6. The detailed results for each dataset in the leaderboard are presented in Table 13 of Appendix D. We observe that FLR does not compromise model performance on the Open LLM leaderboard; in fact, it enhances it. For instance, Llama-3-8B-Instruct $^ +$ FLR DPO achieves an average score of 0.648 compared to 0.644 for the base model. FLR-FT DPO delivers the best performance, with scores of 0.652 and 0.657 for the respective Llama-3- 8B-Instruct and Qwen2-7B-Instruct base models.

Comparative Analysis Table 7 and Table 8 present the results of various online DPO pipelines on Alpaca-Eval V2, Arena-Hard (Li et al. 2024b), and FLASK respectively. Table 7 also highlights the key differences among them. We apply the same experiment recipe across pipelines, including Llama-3-8B-Instruct as the base policy model, Lora as the fine-tuning approach, DPO as the alignment technique, and the same set of training hyperparameters (details shown in Appendix E). We observe a performance gap between the state-of-the-art reward model, ArmoRM-Llama3-8B-v0.1, and our proposed FLR mechanism. ArmoRM achieves a

<html><body><table><tr><td>Models</td><td>FLASK</td><td>AlpacaEval LC</td><td>WildBench</td><td>Open LLM</td></tr><tr><td>Llama-3-8B-Instruct</td><td>3.76</td><td>28.72</td><td>50.00</td><td>0.644</td></tr><tr><td>+ FLRDPO</td><td>3.72</td><td>33.17</td><td>57.57</td><td>0.648</td></tr><tr><td>+ FLRKTO</td><td>3.73</td><td>30.59</td><td>55.50</td><td>0.640</td></tr><tr><td>+ FLR-FTDPO</td><td>3.83</td><td>34.22</td><td>63.62</td><td>0.652</td></tr><tr><td>+ FLR-FTKTO</td><td>3.76</td><td>33.89</td><td>60.50</td><td>0.643</td></tr><tr><td>Qwen2-7B-Instruct</td><td>3.87</td><td>29.74</td><td>50.00</td><td>0.639</td></tr><tr><td>+ FLRDPO</td><td>3.92</td><td>28.78</td><td>55.57</td><td>0.632</td></tr><tr><td>+FLRKTO</td><td>3.89</td><td>26.15</td><td>54.49</td><td>0.640</td></tr><tr><td>+FLR-FTDPO</td><td>3.95</td><td>34.26</td><td>61.52</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td>0.657</td></tr><tr><td>+ FLR-FT KTO</td><td>3.92</td><td>33.17</td><td>63.93</td><td>0.643</td></tr></table></body></html>

Table 6: Alignment performance. The last column is the average score on the Open LLM Leaderboard. LC refers to the length-controlled win rate. FLR-FT refers to using the finetuned Llama-3-8B-Instruct to generate follow-up likelihood while FLR refers to using the base LM without fine-tuning.

<html><body><table><tr><td>Methods</td><td>LC/Win Rate/Length</td><td>RM</td><td>#External Anno</td><td>Reward Signal</td><td>Anno Type</td><td>Prompt Source</td></tr><tr><td>PairRM</td><td>35.92/ 33.04 /1847</td><td>PairRM</td><td>~490K</td><td>Human, GPT-4</td><td>Pairwise</td><td>UltraFeedback</td></tr><tr><td>ArmoRM</td><td>41.40 /39.13/1904</td><td>ArmoRM</td><td>~1M</td><td>Human, GPT-4</td><td>Pairwise</td><td>UltraFeedback</td></tr><tr><td>FLR (UltraFeedback)</td><td>32.60 / 34.22/2078</td><td>FLR</td><td>0</td><td>Follow-ups</td><td></td><td>UltraFeedback</td></tr><tr><td>FLR-FT(UltraFeedback)</td><td>35.18 / 35.47 /2003</td><td>FLR-FT</td><td>100K</td><td>Follow-ups</td><td>NL Feedback</td><td>UltraFeedback</td></tr><tr><td>FLR (Nectar)</td><td>33.17/35.16/2091</td><td>FLR</td><td>0</td><td>Follow-ups</td><td></td><td>Nectar-100K</td></tr><tr><td>FLR-FT (Nectar)</td><td>34.22/35.22/2049</td><td>FLR-FT</td><td>100K</td><td>Follow-ups</td><td>NL Feedback</td><td>Nectar-100K</td></tr></table></body></html>

Table 7: Comparative analysis on Alpaca-Eval V2. “LC”, “FT”, “Anno”, and “NL” denote “length-controlled”, “fine-tuned”, “annotations”, and “natural language” respectively. Results of the LLaMA-3-8B-Instruct base policy is provided for reference.

Table 8: Comparative analysis on Arena-Hard and FLASK.   

<html><body><table><tr><td>Methods</td><td>Arena-Hard</td><td>95% CI</td><td>FLASK</td></tr><tr><td>LLaMA-3-8B-Instruct</td><td>20.6</td><td>(-2.0, 1.9)</td><td>3.76</td></tr><tr><td>PairRM</td><td>25.2</td><td>(-2.1, 2.0)</td><td>3.81</td></tr><tr><td>ArmoRM</td><td>28.4</td><td>(-1.9, 1.9)</td><td>3.81</td></tr><tr><td>FLR(UltraFeedback)</td><td>22.2</td><td>(-2.0, 1.9)</td><td>3.76</td></tr><tr><td>FLR-FT(UltraFeedback)</td><td>24.5</td><td>(-1.7, 1.5)</td><td>3.83</td></tr><tr><td>FLR + (Nectar)</td><td>21.7</td><td>(-2.1, 2.4)</td><td>3.72</td></tr><tr><td>FLR-FT+ (Nectar)</td><td>24.8</td><td>(-2.6, 2.3)</td><td>3.83</td></tr></table></body></html>

$4 1 . 4 0 \%$ LC win rate on Alpaca-Eval V2 and $2 8 . 4 \%$ win rate on Arena-Hard respectively, whereas the best-performing FLR variant achieves $3 5 . 1 8 \%$ and $2 4 . 8 \%$ on the two benchmarks respectively. The gap can be attributed to the significant differences in both the quantity and diversity of annotated data utilized. ArmoRM, a Mixture-of-Experts (MoE) model, is trained on approximately 1 million pairwise annotations from humans or GPT-4, optimized across 19 reward objectives. In contrast, FLR bypasses the need for annotated preference data by deriving reward signals from naturally occurring follow-ups.

Notably, FLR performs comparably to PairRM, which has been trained on approximately 490K high-quality annotations, on all three benchmarks. This underscores the potential of leveraging naturally occurring follow-ups as a reward signal. Building on FLR’s promise, our results further demonstrate that fine-tuning with natural language (NL) feedback can enhance alignment performance between FLR and FLR-FT, though the increase in LC win rate remains modest. Future work could explore more effective ways to leverage NL feedback or focus on curating higher-quality NL feedback to further enhance FLR’s performance.

Additionally, we also examined the impact of using different prompt sources in the alignment pipeline and found no significant difference between using UltraFeedback and Nectar, such as $3 2 . 6 0 \%$ vs $3 3 . 1 7 \%$ LC win rate for FLR on Alpaca-Eval, $2 2 . 2 \%$ vs $2 1 . 7 \%$ win rate on Arena-Hard, and 3.76 vs 3.72 on FLASK. The observation is potentially due to the similarity in the distribution of their data.

# 6 Conclusion

In conclusion, our proposed ’Follow-up Likelihood as Reward’ (FLR) mechanism offers a novel approach to reward modeling by utilizing user follow-up utterances as feedback, without relying on human or LLM-based annotations. FLR matches the performance of existing strong reward models on multiple benchmarks and enables automatic mining of preference data to enhance model helpfulness through direct alignment methods. Furthermore, fine-tuning the language model with natural language feedback significantly enhances FLR’s effectiveness in reward modeling and, in turn, improves the alignment of the base policy models.