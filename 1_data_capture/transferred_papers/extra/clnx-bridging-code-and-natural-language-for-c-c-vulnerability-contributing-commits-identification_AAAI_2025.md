# CLNX: Bridging Code and Natural Language for $\mathbf { C / C } \mathbf { + + }$ Vulnerability-Contributing Commits Identification

Zeqing $\mathbf { Q i n } ^ { 1 , 2 }$ , Yiwei $\mathbf { W } \mathbf { u } ^ { 1 }$ , Lansheng Han\*1,2,3

1School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China 2Hubei Key Laboratory of Distributed System Security, Hubei Engineering Research Center on Big Data Security 3Wuhan JinYinHu Laboratory zeqing $@$ hust.edu.cn, cnwyw77777@gmail.com, hanlansheng@hust.edu.cn

# Abstract

Large Language Models (LLMs) have shown great promise in vulnerability identification. As $\mathrm { C / C } { + + }$ comprise half of the open-source Software (OSS) vulnerabilities over the past decade and updates in OSS mainly occur through commits, enhancing LLMs’ ability to identify $\scriptstyle \mathbf { C } / \mathbf { C } + +$ VulnerabilityContributing Commits (VCCs) is essential. However, current studies primarily focus on further pre-training LLMs on massive code datasets, which is resource-intensive and poses efficiency challenges. In this paper, we enhance the ability of BERT-based LLMs to identify $\mathrm { C / C } { + } { + }$ VCCs in a lightweight manner. We propose CodeLinguaNexus (CLNX) as a bridge facilitating communication between $\mathrm { C / C } { + } { + }$ programs and LLMs. Based on commits, CLNX efficiently converts the source code into a more natural representation while preserving key details. Specifically, CLNX first applies Structurelevel Naturalization to decompose complex programs, followed by Token-level Naturalization to interpret complex symbols. We evaluate CLNX on public datasets of 25,872 $\scriptstyle \mathbf { C } / \mathbf { C } + +$ functions with their commits. The results demonstrate that CLNX substantially improves the ability of LLMs to detect $\scriptstyle \mathbf { C } / \mathbf { C } + +$ VCCs. Moreover, CLNX-equipped CodeBERT achieves new state-of-the-art performance and identifies 38 OSS vulnerabilities in the real world.

# 1 Introduction

In recent years, with the rapid growth of open-source software (OSS) applications, the number of OSS vulnerabilities has been increasing significantly. According to the data from the 2023 OSSRA report (Synopsys 2023), in the 1,703 codebases analyzed by the Black Duck audit team, $84 \%$ of the codebases contained at least one known open-source vulnerability, and $48 \%$ contained high-risk vulnerabilities. Moreover, $5 2 . 1 3 \%$ of reported vulnerabilities in OSS are implemented in $\scriptstyle \mathbf { C } / \mathbf { C } + +$ (Wang et al. 2023) over the past decade. As patch commit is the primary way to update code in OSS, identifying Vulnerability-Contributing Commits (VCCs) can prevent new vulnerabilities from being introduced into OSS to a large extent (Meneely et al. 2013).

Large Language Models (LLMs), particularly those based on BERT (Devlin et al. 2018) architecture, have demonstrated their potential to identify vulnerabilities by effectively learning code dependencies and contextual nuances (Xu et al. 2022). The models’ effectiveness arises from their bidirectional encoder architecture, which captures contextual semantics around code segments. However, since these models are trained initially on natural language, their code comprehension capabilities can still be considerably enhanced. Current research primarily focuses on further pre-training LLMs on extensive code datasets. ( $\mathrm { \Delta X u }$ et al. 2022). For example, CodeBERT (Feng et al. 2020) has been pre-trained on six programming languages: Python, Java, JavaScript, PHP, Ruby, and Go. Nevertheless, it exhibits suboptimal performance in $\scriptstyle \mathbf { C } / \mathbf { C } + +$ due to the absence of specific pre-training for these languages. More importantly, the improvements remain marginal even after extensive further pre-training. For instance, CodeBERT-cpp (Zhou et al. 2023), an extension of CodeBERT further pre-trained on $^ { C + + }$ , shows only marginal improvements (a rise of $2 . 0 3 \%$ in accuracy) in identifying $\mathrm { C / C } { + } { + }$ vulnerability while consuming significant computation resources. It indicates that further pre-training is inefficient and occasionally ineffective (Liu et al. 2023a).

Specifically, we address the major challenge in our paper.

• How to enhance the effectiveness of LLMs in identifying $\scriptstyle \mathbf { C } / \mathbf { C } + +$ VCCs while ensuring a lightweight implementation?

To address this challenge, we introduce CodeLinguaNexus (CLNX), a middleware that transforms original $\mathrm { C / C } { + } { + }$ code into a format more amenable to LLMs. To do so, we first perform the Structure-level Naturalization. Specifically, we linearize the structures of the $\mathrm { C / C } { + + }$ source code with commit and shorten their length. Then, we perform Token-level Naturalization, interpreting complex $\mathrm { C / C } { + + }$ symbols into their natural language equivalents.

We implement CLNX and evaluate it on a dataset of 25, $8 7 2 ~ \mathrm { C / C + + }$ functions with corresponding commits, including 10, 894 VCCs. The result indicates that CLNX significantly improves LLMs’ ability to identify $\scriptstyle \mathbf { C } / \mathbf { C } + +$ VCCs. Moreover, equipped with CLNX, BERT undergoes an increase of $1 4 . 4 8 \%$ in precision, surpassing other models that have been further pre-trained on code. Finally, CLNXCodeBERT achieves state-of-the-art performance. Lastly, CLNX-CodeBERT finds 38 real-world OSS vulnerabilities by identifying vulnerability-contributing commits.

1 @@ -212,7 +212,7 @@   
2 diff --git a/server/util_mutex.c b/ server/util_mutex.c   
3 a/server/util_mutex.c   
4 + b/server/util_mutex.c 5 @@ -120 +120 @@ AP_DECLARE(apr_status_t)   
6 \*mutexfile $\mathbf { \tau } = \mathbf { \tau }$ ap_server_root_relative( pool, file);   
7 + \*mutexfile $\mathbf { \Sigma } = \mathbf { \Sigma }$ ap_runtime_dir_relative( pool, file);   
8 @@ -307 +307 @@ static const char   
9 return ap_server_root_relative(p,   
10 $^ +$ return ap_runtime_dir_relative(p,   
11 @@ -555 +555 @@ AP_CORE_DECLARE(void)   
12 dir $\mathbf { \Sigma } = \mathbf { \Sigma }$ ap_server_root_relative(p, mxcfg->dir);   
13 + dir $\mathbf { \Sigma } = \mathbf { \Sigma }$ ap_runtime_dir_relative(p, mxcfg->dir);

In summary, our contributions in this paper are:

• We propose CLNX, a pioneering framework for improving LLMs’ performance in $\mathrm { C / C + + V C C s }$ identification in an effective and efficient way.   
• We successfully implement a prototype of CLNX and conduct extensive experiments to evaluate its effectiveness.   
• We equip CodeBERT with CLNX to achieve the new state-of-the-art performance and demonstrate CLNXCodeBERT’s ability to identify vulnerabilities in the real world.

# 2 Preliminaries Vulnerability-Contributing Commits

In OSS development, patch commits record the differences between two versions of the source code (Zuo and Rhee 2024). They can be categorized into two types: vulnerable patch commits and non-vulnerable patch commits. Vulnerable patch commits refer to those that will introduce new vulnerabilities into the original code, which are also called Vulnerability-Contributing Commits (VCCs) (Meneely et al. 2013). In this research, a patch commit is considered “vulnerable” if it introduces vulnerabilities that belong to any of the Common Weakness Enumeration (CWE), regardless of its triggering conditions (Wang et al. 2023). Listing 1 shows a vulnerable patch commit with code revisions marked by plus and minus signs $( + / - )$ on the left side. This commit is a configuration item change aimed at improving the path settings for mutexes in the Apache HTTP Server. However, it introduces a vulnerability related to permission bypass. Vulnerable patch commits highlight critical information about vulnerabilities. When identifying VCCs at the functional level, both the patch commit and the source code of the revised function are analyzed.

# Pre-training and Fine-tuning

Pre-training in this paper refers to the training phase of LLMs conducted on large-scale unlabeled datasets. LLMs can generally be divided into two categories: BERT-based and GPT-based. Since GPT-based LLMs are composed of a decoder structure and are more suitable for generative tasks (Hadi et al. 2023), we primarily focus on the performance of BERT-based models in vulnerability identification, a code classification task (Xu et al. 2022). BERTbased LLMs are pre-trained on tens of millions of text data using techniques like Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) (Xu et al. 2022). During the pre-training phase, these models capture helpful information from the data and store it in their weights. These pre-trained models are then fine-tuned on labeled data for specific downstream tasks like text classification or question answering. While pre-training requires substantial computational resources, fine-tuning is comparatively more resource-efficient (Radiya-Dixit and Wang 2020).

# 3 Methodology

This section presents an overview of our approach and details each component, including Structure-level Naturalization and Token-level Naturalization.

# Overview

The overview of CLNX is shown in Figure 1, with CLNX’s internal structure displayed on its left side. CLNX is performed at the function level. In the process of handling input source code and patch commit, CLNX initially performs Structural-level Naturalization. The first step employs CLNX’s code analyzer to transform the source code into a graph of linear execution paths. Next, patch information is integrated to identify the critical path. Then, CLNX proceeds with Token-level Naturalization, which maps the identified critical path to the corresponding source code and converts key symbols into their natural language equivalents. Finally, CLNX outputs the fully naturalized source code. The system workflow for deploying CLNX to enhance LLMs’ performance in VCCs identification is shown on the right side of Figure 1. For a given set of programs with their corresponding patch commits, the programs are naturalized by CLNX and then provided to LLMs for fine-tuning. When an unknown program with its patch commit is analyzed, CLNX transforms the program into naturalized form and then forwards the results to the fine-tuned LLMs for vulnerability identification. The remainder of Section 3 provides details on each component of CLNX.

# Code Analyzer

$\scriptstyle \mathbf { C } / \mathbf { C } + +$ programs’ complex structures and excessive length challenge LLMs in understanding them. In response to these challenges, CLNX’s Structure-level Naturalization is designed with two primary goals: The first goal is to linearize complex program structures. The second goal is to reduce the overall program length. In particular, the code analyzer extracts linear execution paths within a program.

The concept of “basic blocks,” borrowed from LLVM (Racordon 2021), underpins the design of CLNX’s code analyzer. A “basic block” is a sequence of instructions that executes sequentially, characterized by a single entry and a single exit point, devoid of any internal branching. The code analyzer transforms programs into basic blocks and generates a graph $G = ( V , E )$ , where each vertex in $V$ corresponds to a basic block, and each edge in $E$ represents the control flow between blocks. The graph $G$ ’s entrance point $v _ { e n t r y }$ corresponds to the program’s entry basic block, and its exit point $v _ { e x i t }$ corresponds to the program’s final basic block. As a result, any path traversed from $v _ { e n t r y }$ to $v _ { e x i t }$ within $G$ delineates a linear execution path of the program. In particular, when there is a loop structure, for simplicity, we directly convert the control flow to single executions and label the corresponding nodes as loop structures. It should be noted that CLNX only uses Abstract Syntax Tree (AST) and Control Flow Graph (CFG) for code embedding. The Program Dependence Graph (PDG), integrating both control dependency graph (CDG) and data dependency graph (DDG), is commonly used to abstractly represent source code (Wang et al. 2023). However, we believe that complex structures risk subjectively introducing excessive irrelevant information, thereby complicating the accurate semantic representation of the code. We compare our method with complex graph-based approaches (embedding AST/CFG/DDG/CDG) in RQ2 to demonstrate CLNX’s effectiveness.

![](images/dd3386212721f0070ab4bc0e47636bf075580b824d0b768234e035f273dedd27.jpg)  
Figure 1: The Overview of CLNX

The code analyzer deploys Joern to generate AST. The whole process is illustrated in Step 1 of Figure 2. In contrast to LLVM, CLNX’s code analyzer does not impose requirements on the actual compilability of the program. This attribute is particularly significant for identifying functionlevel vulnerabilities, especially in scenarios where the absence of relevant header files precludes successful compilation.

# Critical Path Identification

After obtaining the graph G, composed of basic blocks, the focus of CLNX shifts to identifying a critical execution path within the graph that encompasses the maximum amount of vulnerability-related basic blocks. This process can be divided into two primary steps, as illustrated in Step 2 of Figure 2; Firstly, determining the basic blocks that are directly related to a patch commit. Secondly, the critical path within G is selected, which offers the most extensive coverage of these identified basic blocks.

Commit-related Basic Blocks Identification Based on the idea of taint analysis (Boxler and Walcott 2018), CLNX identifies the code removed in the corresponding patch commits of a program as contamination points. CLNX also considers an extended range, which includes three lines (Wang et al. 2023) before and after the lines corresponding to the removed code, as the affected tainted area. This area is represented as $\textit { S } = \ [ l _ { s } , l _ { e } ]$ , where $l _ { s }$ and $l _ { e }$ are the start and the end line numbers of the tainted area, respectively. A basic block $B B _ { i }$ , covering the line number range $[ b _ { i s } , b _ { i e } ]$ , is regarded as commit-related, denoted $B B _ { t a i n t e d _ { i } }$ , if its range intersects with the tainted area, i.e., $\{ B { B _ { t a i n t e d } } _ { i } \ | \ [ l _ { s } , l _ { e } ] \cap$ $[ b _ { i s } , b _ { i e } ] \neq \emptyset \}$ .

Critical Executing Path Selection Further, CLNX is going to select one critical executing path in graph G. CLNX designates the basic block corresponding to the program’s entry point as the source $( B B _ { s o u r c e } )$ and the basic block corresponding to the program’s exit point as the sink $( B B _ { s i n k } )$ . Based on this, the critical linear execution path $P$ in the graph structure, which originates from $B B _ { s o u r c e }$ and terminates at $B B _ { s i n k }$ , aims to maximize the coverage of vulnerability-related basic blocks $B B _ { t a i n t e d }$ . CLNX designs its critical path selecting algorithm based on dynamic programming to circumvent the issue of path explosion. critical path selecting algorithm selects the critical execution path in graph $G$ by satisfying three primary criteria: first, the path covers as many $B B _ { t a i n t e d }$ as possible; second, the path minimizes length; and third, if two paths have the same length and contain the same number of $B B _ { t a i n t e d }$ , then select the one with the highest information entropy value. The Pseudo code of the critical path selecting algorithm is shown in the extended version as Algorithm 1 (Qin, Wu, and Han 2024).

![](images/eb35b4e721be03fbd1c2ad699dba515de5c0ff8e5ece3e62786fbb7e3b4768cf.jpg)  
Step 1: Code Analyzer   
Figure 2: The Workflow of CLNX

# Reverse Mapping

As a compiled programming language, $\scriptstyle \mathbf { C } / \mathbf { C } + +$ has more low-level symbols compared to natural languages. In response, CLNX’s Token-level Naturalization is designed to translate complex symbols into their natural linguistic equivalents. Initially, CLNX undertakes the task of reverse mapping the critical path, composed of basic blocks, back to the source code. This step involves reconstructing the source code information by tracing the sequence of basic blocks within the critical path. Owing to the grand architecture of CLNX basic blocks, the implementation of reverse mapping is straightforward and efficient.

# Key Symbols Transformation

Given the source code, CLNX deploys appropriate transformations based on the symbols’ types and rewrites the symbols to their natural language equivalents. This part corresponds to Step 3 of Figure 2. First, we extract all keywords and symbols following the lexical rules in the $^ { C + + }$ reference documentation, then provide natural language equivalents based on these descriptions. To further refine the output, we apply the Longest Common Subsequence (LCS) algorithm to remove duplicates, enhancing the distinctions between different equivalents. For instance, the symbols $\cdots < < 3 3$ and “ $> > ^ { \prime \prime }$ are defined in the $^ { C + + }$ reference as “Bitwise left shift”

Table 1: Examples of Key Symbols Transformations   

<html><body><table><tr><td>Type</td><td>Example</td><td>Natural Language Equivalents</td></tr><tr><td rowspan="6">Operator</td><td>&var *p</td><td>dereference p address of var</td></tr><tr><td>a->b</td><td>access b via pointer a</td></tr><tr><td></td><td>access a's membervia b</td></tr><tr><td>a.*b</td><td></td></tr><tr><td>a^b</td><td>a XOR b</td></tr><tr><td>~a a<<b</td><td>Bitwise NOT a</td></tr><tr><td rowspan="6">Memory</td><td>a>>b</td><td>a left shift by b a right shift by b</td></tr><tr><td>alignas(16)</td><td>alignment as 16 bytes</td></tr><tr><td>alignof(type)</td><td> get type alignment</td></tr><tr><td>decltype(x) y</td><td>get x's type to y</td></tr><tr><td>nullptr</td><td>null pointer</td></tr><tr><td>Concurrency Keywords</td><td>co_await task(）wait for task co_yield value</td><td></td></tr><tr><td>Preprocessor #include<h> Directive</td><td>#undef M</td><td>produce value temporarily include headerfile<h> undefine the macro M</td></tr></table></body></html>

and “Bitwise right shift.” Our approach simplifies these to “left shift” and “right shift,” enabling the language model to recognize these symbols accurately while distinguishing their meanings. Table 1 provides some illustrative examples.

# 4 Experimental Setup

Our evaluation is designed to answer the following research questions:

• RQ1: How does CLNX enhance LLMs for the $\mathrm { C / C } { + + }$ VCCs identification task?   
• RQ2: How does the performance of CLNX-equipped LLMs compare to other vulnerability identificationrelated methods?   
• RQ3: How does CLNX-equipped LLM perform in identifying real-world OSS vulnerabilities that are contributed through commits?

# Evaluation Task

The evaluation task of our paper is Vulnerable-Contributing Commits (VCCs) identification, where the input is the source code and the corresponding patch commit, and the output is a label denoting whether the commit will introduce vulnerabilities into the original code or not.

# Datasets

To evaluate our research questions using real-world data, we construct experimental datasets based on the publicly available Devign dataset (Zhou et al. 2019). Our dataset contains 25,872 pairs of vulnerable and non-vulnerable functions, along with their associated commit IDs, from two major open-source $\mathrm { C / C } { + + }$ projects: FFmpeg and Qemu. The dataset is randomly split into training, validation, and test sets in an 8:1:1 ratio.

# Evaluation Metrics

In our experiments, different metrics are used to evaluate downstream tasks. We follow the metrics that CodeXGLUE (Lu et al. 2021) used for evaluation, and the details are listed below:

• Prec: Precision measures the proportion of correct positive identifications made by the model compared to the total predicted positives. • Acc: Accuracy defines the ratio of correct predictions (i.e., the exact match) in the test set. • Recall: This metric concentrates on the model’s ability to correctly identify all genuine positive instances. It calculates the proportion of true positives accurately detected by the model out of the total positives. • F1: This metric is the harmonic mean of precision and recall, balancing these two metrics. It is advantageous when class distribution is imbalanced.

# Baselines

We evaluate both BERT-based and GPT-based LLMs for this study, with a primary focus on CLNX’s effectiveness in improving BERT-based LLMs since their proficiency in comprehensively understanding vulnerability through their bidirectional encoder structure. For comparison, we include vulnerable patch commit identification methods, deep learning vulnerability identification methods, and a traditional vulnerability identification tool. The details of the baselines are listed in Table 2 of the extended version (Qin, Wu, and Han 2024). Notably, as GPT-based LLMs with parameter quantities scaling up to Billion, BERT-based LLMs have a relatively lower parameter scale. To maintain fairness in the assessment, we have chosen the 7b edition of CodeLlama (Roziere et al. 2023) for comparison.

# Experimental Settings

In our evaluation tasks, we utilize the established configuration parameters for LLMs following the standardized settings provided by CodeXGLUE (Lu et al. 2021). All the compared methods are reimplemented to adhere to the default specifications outlined in their foundational papers. Our implementation of CLNX utilizes Joern v2.0.120 and Scala v3.3.1. All operations of CLNX, including the code analyzer, critical path selection, and key symbol transformation, are executed on an Intel Xeon(R) Gold 6326 CPU $@$ 2.90GHz. We perform LLMs fine-tuning on a dedicated machine with an NVIDIA A100 GPU featuring 64GB of memory. The fine-tuning parameters and the process are in accordance with the defect-detection subject of CodeXGLUE (Lu et al. 2021), where the block size is 400, the train batch size is 32, the eval batch size is 64, and the learning rate is 2e-5. Additionally, the dataset is split into training, testing, and validation sets in an 8:1:1 ratio.

# 5 Experimental Result

Table 2: Results of LLMs in $\mathrm { C / C } { + + }$ VCCs identification   

<html><body><table><tr><td>Technique</td><td>Prec</td><td>Acc</td><td>Recall</td><td>F1</td></tr><tr><td>GPT-3.5 Turbo</td><td>16.78%</td><td>31.88%</td><td>11.05%</td><td>34.84%</td></tr><tr><td>GPT-4.0</td><td>37.08%</td><td>42.68%</td><td>33.16%</td><td>42.05%</td></tr><tr><td>CodeLlama-7b</td><td>50.39%</td><td>49.16%</td><td>50.23%</td><td>49.69%</td></tr><tr><td>BERT</td><td>58.60%</td><td>59.85%</td><td>48.94%</td><td>54.66%</td></tr><tr><td>CLNX_s-BERT</td><td>70.33%↑</td><td>62.53%↑</td><td>46.52%</td><td>55.99%↑</td></tr><tr><td>CLNX-BERT</td><td>73.08%↑</td><td>63.19%↑</td><td>49.91%↑</td><td>59.98%↑</td></tr><tr><td>DistilBERT</td><td>63.94%</td><td>61.47%</td><td>46.56%</td><td>53.88%</td></tr><tr><td>RoBERTa</td><td>65.85%</td><td>61.21%</td><td>47.64%</td><td>55.28%</td></tr><tr><td>ContraBERT</td><td>64.78%</td><td>63.89%</td><td>48.92%</td><td>55.74%</td></tr><tr><td>CodeBERT-cpp</td><td>62.97%</td><td>64.21%</td><td>48.37%</td><td>54.71%</td></tr><tr><td>CodeBERT</td><td>66.89%</td><td>62.18%</td><td>45.16%</td><td>53.91%</td></tr><tr><td>CLNX_s-CodeBERT</td><td>71.66%↑</td><td>63.97%个</td><td>43.47%</td><td>53.95%↑</td></tr><tr><td>CLNX-CodeBERT</td><td>75.16%↑</td><td>65.47%↑</td><td>51.83%↑</td><td>60.64% 个</td></tr></table></body></html>

# RQ1: Effectiveness

We conduct extensive experiments and an ablation study to assess the effectiveness of CLNX’s two sequential naturalization phases in enhancing LLMs’ ability to identify $\scriptstyle \mathbf { C } / \mathbf { C } + +$ VCCs. It should be noted that RoBERTa, ContraBERT, CodeBERT, and CodeBERT-cpp have undergone further pre-training with programming data. The results, including precision, accuracy, recall, and F1 score, are presented in Table 2. “CLNX s-” denotes models equipped only with CLNX’s Structure-level Naturalization, while “CLNX” signifies models that completed both naturalization phases.

From Table 2, we can see that GPT-based models like CodeLlama- $\cdot 7 6$ -Instruct do not perform well on this task, so we mainly focus on BERT-based LLMs. There are significant improvements in $\mathrm { C / C } { + + }$ VCCs identification for BERT and CodeBERT after the sequential deployment of CLNX’s two-phase naturalization. Specifically, BERT’s precision improved by $1 4 . 4 8 \%$ , and CodeBERT’s by $8 . 2 7 \%$ , with CLNX-CodeBERT outperforming all LLMs across all metrics, highlighting CLNX’s impact. Although BERT’s initial precision $( 5 8 . 6 0 \% )$ is relatively low compared to CodeBERT $( 6 6 . 8 9 \% )$ , BERT with only CLNX’s Structure-level Naturalization achieves a precision result of $70 . 3 3 \%$ . It surpasses all the models that have been further pre-trained on program data, including CodeBERT and RoBERTa. These results directly validate that CLNX yields a better effect than pre-training strategies. We attribute this improvement to CLNX’s effectiveness in simplifying complex structures and emphasizing critical vulnerability information. However, we notice that accuracy and precision values are improved for both BERT and CodeBERT after CLNX’s Structure-level Naturalization; the recall values decreased by $2 . 4 2 \%$ and $1 . 6 9 \%$ , respectively. These results suggest that the models miss some vulnerabilities. We believe this phenomenon is caused by CLNX’s mission to reduce the source code length. In CLNX’s Structure-level Naturalization stage, it excessively prioritizes program length reduction when dealing with multiple paths with consistent coverage of critical nodes, which may result in the loss of certain vulnerabilityrelated information. Yet, the complete CLNX process eventually led to the highest recall rates for both models, indicating the Token-level Naturalization phase’s effectiveness in enhancing the understanding of retained information.

Answer to RQ1: Both the Structure-level and Token-level Naturalization phases play crucial roles in CLNX’s effectiveness. CLNX enhances LLMs’ performance in $\scriptstyle \mathbf { C } / \mathbf { C } + +$

Table 3: Results of comparative analysis   

<html><body><table><tr><td>Technique</td><td>Prec</td><td>Acc</td><td>Recall</td><td>F1</td></tr><tr><td>Cppcheck</td><td>37.02%</td><td>50.65%</td><td>17.13%</td><td>23.43%</td></tr><tr><td>GraphSPD</td><td>64.57%</td><td>62.65%</td><td>40.75%</td><td>50.12%</td></tr><tr><td>VulFixMiner</td><td>50.35%</td><td>53.61%</td><td>11.72%</td><td>19%</td></tr><tr><td>Russell et al.</td><td>53.02%</td><td>57.93%</td><td>39.67%</td><td>45.38%</td></tr><tr><td>VulDeePecker</td><td>48.42%</td><td>53.55%</td><td>26.40%</td><td>34.17%</td></tr><tr><td>SySeVR</td><td>48.52%</td><td>52.67%</td><td>64.67%</td><td>55.44%</td></tr><tr><td>REVEAL</td><td>56.95%</td><td>62.43%</td><td>67.80%</td><td>59.76%</td></tr><tr><td>Devign</td><td>53.62%</td><td>58.62%</td><td>61.44%</td><td>57.26%</td></tr><tr><td>CLNX-CodeBERT</td><td>75.16%</td><td>65.47%</td><td>51.83%</td><td>60.64 %</td></tr></table></body></html>

VCCs identification significantly.

# RQ2: Comparision

To further evaluate the performance of CLNX-equipped LLM, we compare it with popular deep learning vulnerability identification methods, traditional tools, and vulnerable commit identification methods. We use CodeBERT as the base model for this comparison. The results are presented in Table 3.

As shown in Table 3, CLNX-CodeBERT significantly outperforms all the compared methods in precision (improve $1 0 . 5 9 \%$ ), accuracy, and F1 score, achieving new state-of-the-art performance for this task. Notably, CLNXCodeBERT excels over three graph-based methods (GraphSPD, Devign, REVEAL) that use complex code embedding methods. This success can be attributed to two factors: The first factor is that BERT-based LLMs can perform comprehensive code analysis by considering surrounding elements like variables and functions. The second factor is that CLNX’s simple code embedding method enables LLMs to emphasize key semantic information and operate more efficiently, addressing the redundancy issue commonly seen in graph-based models.

However, while CLNX-CodeBERT achieves the highest recall score among LLMs (Table 2), it does not achieve the top recall score when compared to other methods in Table 3. This discrepancy likely arises from the pre-training objectives of LLMs, which emphasize precision-focused tasks over exhaustive recall coverage, as discussed in (Le Bronnec et al. 2024). Nevertheless, as previously noted, CLNXCodeBERT excels across all other metrics. In particular, the F1 score provides a balanced and comprehensive measure of vulnerability detection performance, validating the effectiveness of CLNX.

Answer to RQ2: With the help of CLNX, LLM achieves new state-of-the-art performance in $\mathrm { C / C + + V C C s }$ identification. The simple and lightweight code embedding approach of CLNX enables the LLM to capture key semantic information effectively.

# RQ3: Real World Vulnerabilities

To evaluate the performance of CLNX-equipped LLM in detecting real-world vulnerabilities, we deploy the finetuned CLNX-CodeBERT to scan the repositories of $3 5 ~ \mathrm { C / C + + }$ open-source projects. Notably, VCCs are relatively rare in open-source software. CLNX-CodeBERT reports 106 vulnerable commits, 38 of which are verified as valid. Finally, we categorize these vulnerabilities by CWE, and the results are shown in Table 5 of the extended version (Qin, Wu, and Han 2024). The vulnerabilities cover types of Improper Permission Assignment for Critical Resource (CWE264), Cryptographic Issues (CWE-310), Information Disclosure (CWE-200), Null Pointer Dereference (CWE-476), Out-of-Bounds Read (CWE-125), Resource Management Errors (CWE-399), Buffer Error (CWE-119), Race Condition (CWE-362), Improper Input Validation (CWE-20), Use After Free (CWE-416), Numeric Errors (CWE-189), and Double Free (CWE-415).

The results indicate that CLNX-CodeBERT can identify vulnerabilities of real-world $\scriptstyle \mathbf { C } / \mathbf { C } + +$ open-source projects introduced by commits. Furthermore, we observe that the model is proficient at identifying specific types of vulnerabilities, which can be ascribed to the CLNX’s capability to distill critical information from vulnerability functions, thereby aiding CodeBERT in learning the specific patterns of these vulnerabilities. For instance, the model detected six Null Pointer Dereference (CWE-476) vulnerabilities and nine Buffer Error (CWE-119) vulnerabilities, which become more apparent without extraneous information. We attribute this to CLNX’s effectiveness in refining key information from vulnerability functions, thus reducing the interference of irrelevant information on LLMs. However, the model only detects one Cryptographic Issue (CWE-310). This result is because vulnerabilities of such type often involve complex processing logic and do not have relatively uniform patterns.

Answer to RQ3: CLNX-CodeBERT effectively finds real-world vulnerabilities in open-source $\mathrm { C / C } { + + }$ repositories, demonstrating CLNX’s potential to help LLMs report 0-day $\mathrm { C / C } { + + }$ vulnerabilities in OSS.

# 6 Discussion

This section discusses the implications, limitations, and potential threats to the validity of our work.

# Implications

We propose a novel, cost-effective framework that enhances the effectiveness of LLMs in identifying $\scriptstyle \mathbf { C } / \mathbf { C } + +$ VCCs. The findings in our research are expected to inspire researchers to improve LLMs’ ability to identify VCCs across more programming languages. CLNX offers guidelines for improving LLMs’ performance in VCCs identification of specific programming languages in a lightweight manner, moving beyond the traditional reliance on extensive pre-training, which requires substantial computational resources.

# Limitations

The experimental results demonstrate that CLNX significantly enhances the performance of LLMs in VCCs identification. The advancement is mainly due to CLNX’s effective two-stage naturalization, making the code more compatible with LLMs. However, challenges arise from a decline in the Recall score, mainly due to its Structure-level Naturalization, which might inadvertently omit important code information. When confronted with multiple paths having equivalent coverage of tainted basic blocks, CLNX’s critical path selecting algorithm prioritizes the shortest path for length minimization at the risk of overlooking important details. A more effective approach could involve considering data flow more substantially in the critical path selection process. However, it involves dynamic program analysis. We will explore it in our future work.

# Threats to Validity

Internal Validity: Our analysis identifies two potential threats to internal validity. Firstly, the uniform standard requirement of CLNX’s code analyzer necessitates standardizing source code format before its use. Secondly, CLNX calculates path length by counting the number of basic blocks, assuming each block adds uniformly to the total length. To maintain algorithmic integrity in our critical key path selection algorithm, all edges of the input graph structure must be of equal length (by default, set to one).

External Validity: Regarding external validity, the performance of the original GPT-based LLMs is significantly lower than that of BERT-based models, so we mainly focus on how CLNX improves BERT-based LLMs’ performance in $\mathrm { C / C } { + } { + }$ VCCs identification.

# 7 Related Work

Large Language Models: In recent years, there has been a notable emergence of LLMs, which are increasingly recognized as promising solutions for the field of vulnerability identification (Thapa et al. 2022) (Ding et al. 2022) (Feng et al. 2020) (Hanif and Maffeis 2022). BERT (Devlin et al. 2018) is a deep bidirectional encoder based on the transformer architecture, pre-trained by Google on a vast corpus comprising millions of text passages and billions of words. BERT-based LLMs are usually pre-trained on two tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP), thus equipping them with robust semantic understanding and endowing them with substantial knowledge, making it suitable for fine-tuning on specific tasks with limited data, such as vulnerability identification $\mathrm { \Delta X u }$ et al. 2022). Successful applications include BERT’s superior detection accuracy on the SARD database, outperforming traditional machine learning models like LSTM and BiLSTM. Similarly, further pre-training of CodeBERT (Feng et al. 2020) and its variants, such as DistilBERT (Sanh et al. 2019), RoBERTa (Liu et al. 2019), ContraBERT (Liu et al. 2023b), and CodeBERT-cpp (Zhou et al. 2023), enhances LLM performance on programming languages.

Deep Learning Vulnerability Identification: These methods train various deep learning models with existing datasets (Steenhoek et al. 2023) (Russell et al. 2018) (Okun et al. 2013) (Black 2018) (Booth, Rike, and Witte 2013). Subsequently, these models are deployed to identify undetected vulnerabilities. They generally fall into two primary categories: token-based methods (Li et al. 2018) (Russell et al. 2018) (Li et al. 2021b) and graph-based (Zhou et al. 2019) (Chakraborty et al. 2021). Token-based approaches process the source code as sequences of tokens, leveraging models such as RNN (Li et al. 2021a) (Li et al. 2021b) (Li et al. 2018) (Zhang et al. 2019), CNN (Russell et al. 2018), and MLP (Coimbra et al. 2021) for training purposes. Some strategies utilize code slices to distill pivotal information. Conversely, graph-based methods seek to encapsulate the source code’s multifaceted information into graphs, then analyze using various GNN (Cheng et al. 2021) (Cao et al. 2022). For example, the Code Property Graph (CPG) leverages information from abstract syntax trees, control flow graphs, and program dependency graphs to model the combined semantic and syntactic information of a program.

Patch Commit Identification: In OSS, code commits serve as the core building block units of a version control system in software development (Zuo and Rhee 2024). The patch commit (i.e., code changes $^ +$ description of changes), or patch for short, is a general concept involving modifications that are specifically focused on code updates, such as introducing new features. However, this process may introduce new vulnerabilities into the original code. To address this, a significant amount of work has focused on patch commit analysis targeting vulnerability discovery (Zuo and Rhee 2024) (Meneely et al. 2013). In the early stage, handcrafted features-based methods are proposed. For example, VCCFinder (Perl et al. 2015) utilized an SVM model to automatically identify commits that might introduce vulnerabilities. Wang et al. (Wang et al. 2019) studied code diffs exclusively, employing 61 features, including 22 from previous work (Tian, Lawall, and Lo 2012), to form an input vector for their machine learning model. In recent years, advancements in neural networks, particularly in natural language processing (NLP) and applied graph theory, have revolutionized this field. E-SPI (Wu et al. 2022), for instance, analyzes both code diffs and commit messages by first extracting a contextual abstract syntax tree (AST) from code changes, then encoding it into paths using a BiLSTM. Commit messages are converted into graphs and processed with a graph neural network (GNN). However, the quality of commit messages can limit the usefulness of such analyses. In VulFixMiner (Zhou et al. 2021), the authors only consider code change information. To extract semantics from the code changes, they adopt CodeBERT. It is noteworthy that VulFixMiner only investigates Python and Java projects (Zuo and Rhee 2024). Most recently, a detection system called GraphSPD is proposed (Wang et al. 2023), which proposes a novel graph structure called PatchCPG to represent patches.

# 8 Conclusion

In this research, we propose CLNX, a middleware designed to make $\mathrm { C / C } { + } { + }$ programs compatible with LLMs, thereby improving their ability to identify $\mathrm { C / C + + \Delta \ V C C s }$ . CLNX operates with minimal resource overhead, offering efficiency advantages over pre-training methods. Extensive experiments confirm that CLNX-equipped LLMs demonstrate robust improvements in $\mathrm { C / C } { + + }$ VCCs identification, achieving new state-of-the-art performance. We anticipate that CLNX will allow developers to effectively improve the performance of LLMs in identifying VCCs of specific programming languages without additional pre-training.