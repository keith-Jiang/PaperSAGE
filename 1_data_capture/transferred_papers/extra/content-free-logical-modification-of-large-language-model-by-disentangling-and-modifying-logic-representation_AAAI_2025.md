# Content-free Logical Modification of Large Language Model by Disentangling and Modifying Logic Representation

Xin ${ \bf W } { \bf u } ^ { 1 , 2 }$ , Yuqi $\mathbf { B u } ^ { 1 , 2 }$ , Yifei Chen1, Yi Cai1,2\*

1South China University of Technology, Guangdong, China 2Key Laboratory of Big Data and Intelligent Robot (South China University of Technology) Ministry of Education ycai@scut.edu.cn

# Abstract

Despite extensive training on diverse datasets and alignment with human values, large language models (LLMs) can still generate fallacious outputs. Additionally, the validity of LLM’s outputs varies significantly depending on the content. It is crucial to ensure LLMs’ logical consistency across different contexts. Drawing inspiration from cognitive psychology studies, we propose a Logic Control Framework (LCF) that disentangles LLMs’ hidden representations into separate content and logic spaces. Within the logic space, we use logically valid and invalid samples to construct distinct regions through contrastive learning. By moving logic representations to logically valid regions and fusing them with unchanged content representations, we significantly reduce logical fallacies in LLM outputs while maintaining content coherence. We demonstrate the effectiveness of LCF through experiments on conclusion generation and fallacy identification tasks, showing a significant improvement in logical validity and a reduction in fallacious outputs.

Premise:   
If Ifeel well,I go to work,andIdidn't feel well yesterday.Therefore?   
Llama2:   
Therefore,you didn't go towork yesterday. × Logical invalid (a) A fallacious conclusion generated by Llama2 Content space Logic space   
Valid region Invalid region ： . Unchanged . Hidden Repr. Hidden Repr.   
Modified Llama2: Llama2+LCF(Ours):   
You may have stayed home or You may go to work yesterday   
contacted a specific individual or not, since the information   
to assess their health, but you about what happened when you   
did not receiveany information didn't feel well isn't provided.   
Cordocumentation Valid in logic and XUnrelatedcontent coherent in content   
(b) Modifying representation in (c) Modifying representation   
original hidden space in logic space (Ours)

# Introduction

Logical fallacies (Jason 1989; Tindale 2007), which are errors in reasoning that undermine the logic of an argument, can lead to misinformation (Musi and Reed 2022) and flawed decision-making processes (Bregant 2014; HaitaFalah 2017). To be trusted and effective, large language models (LLMs) are expected to produce coherent, rational, and logically sound outputs (Liu et al. 2023; Naveed et al. 2024), particularly in sensitive fields such as healthcare (Goyal et al. 2024) and legal advice (Sun et al. 2024).

Despite being trained on extensive datasets and aligned with human values, LLMs can still produce fallacious outputs (Naveed et al. 2024). For instance, as shown in Figure 1(a), the premise indicates that feeling well is a sufficient condition for going to work. However, Llama2 incorrectly infers that not feeling well is also a sufficient condition for not going to work. This conclusion is invalid without additional information about what occurs when one does not feel well. In our experiments, nearly half of the conclusions generated by the Llama2-7b-chat model exhibit logical fallacies including false causality and deductive fallacies.

Additionally, the hidden representations of LLMs are content-dependent, which causes them to exhibit contententangled reasoning patterns. i.e., the validity of their outputs varies with content, even if the logical structure remains similar (Dasgupta et al. 2024). Consequently, modifications to representations can simultaneously impact logical validity and content. For instance, Figure 1(b) illustrates that adjusting Llama2’s representation toward regions associated with logically valid samples improves the validity of its conclusions but introduces unrelated content. This demonstrates that altering the hidden representation of LLMs affects both logical validity and content, hindering the ability to enhance logical validity while maintaining content consistency.

In contrast, humans can separate content from logical structure during reasoning, maintaining logical validity regardless of the content (Wason and Johnson-Laird 1972; Cheng and Holyoak 1985). Moreover, research in cognitive psychology (Monti and Osherson 2012) shows that the human brain has specialized content-independent and contentdependent regions to process different combinations of logical structures and content. Consequently, this raises an interesting question: Can we isolate and modify the logical information in LLM representations to enhance logical validity without altering the content? As illustrated in Figure 1(c), the Llama2 representation is decoupled into content and logical spaces, allowing for independent modification of the logical space to improve validity.

In this paper, we investigate how modifying LLMs’ hidden representations can improve logical validity while maintaining coherence in content. We propose a Logic Control Framework (LCF) to address this challenge. Specifically, LCF separates LLMs’ hidden representations into distinct content and logic spaces. Within the logic space, we use contrastive learning to define regions corresponding to logically valid and invalid samples. During inference, we improve the logical validity of LLM outputs by adjusting logic representations towards the valid regions. We then combine the unchanged content representation with the modified logic representation to return to the original hidden space. Experimental results on tasks involving conclusion generation and fallacy identification demonstrate that LCF significantly reduces logical fallacies in LLM outputs, thereby improving their overall reliability and validity.

In summary, the key contributions of our work are:

• To correct logical validity without being influenced by content, we propose Logic Control Framework (LCF), which modifies LLMs in an content-free logic space derived from their hidden states, enabling modifications to logical validity without affecting content coherence. • Building on LCF, we propose using contrastive learning to define logically valid regions in the logic space and relocating representations into these regions to enhance the logical validity of LLMs. • We validate the effectiveness of LCF through experiments on conclusion generation and fallacy identification tasks, demonstrating significant improvements in logical validity and reductions in fallacious outputs.

# Related Work

In recent years, several studies have increasingly focused on understanding and detecting logical fallacies. (Jin et al. 2022) introduce the Logical Fallacy Detection dataset, which covers 13 types of fallacies and investigate how a structure-aware classifier can improve detection performance. This work inspires our research into decoupling content and logic. Similarly, (Li et al. 2024b) introduce the LFUD dataset, encompassing 12 types of fallacies, and propose additional testing scenarios such as classification and rewriting. With the advancement of LLMs, recent research has also examine LLMs’ performance in fallacy classification and their responses to fallacious inputs (Sourati et al. 2023; Lim and Perrault 2024; yuan, Cai, and Huang 2024; Payandeh et al. 2024; Li et al. 2024c; Bu et al. 2024). Our contributions surpass existing research in two key areas: (1) Unlike current studies that mainly focus on fallacy identification or categorization, we examine fallacy issues arising during the generation processes of LLMs; and (2) While previous work has largely concentrated on prompt-based fallacy understanding, it has not addressed the impact of LLM representations on fallacies.

Several studies propose methods to enhance the truthfulness of LLMs by adjusting their representations. For example, (Li et al. 2024a) propose a method called InferenceTime Intervention (ITI) to increase the truthfulness of responses from LLMs by modifying activation patterns in specific attention heads during inference. (Liu et al. 2024a) argue that in-context learning involves altering latent states. Furthermore, (Liu et al. 2024b) propose a novel method, Representation Alignment from Human Feedback (RAHF), to align LLMs with human preferences. This approach uses representation engineering to control model behavior based on specific patterns in neural activity. (Kong et al. 2024) ntroduce a novel approach for aligning LLMs with human objectives using representation editing from a control perspective. (Zhang, Yu, and Feng 2024) suggest reducing hallucinations by editing LLMs within a truthful space. Building on these works, we introduce a novel approach to improve LLM logical validity through representation modification. We show that LLM representations can be projected into content and logic spaces, and that modifying representations in the logic space can enhance or correct the logical validity of LLMs.

# Method

# Overview

LCF is a neural network that can be added after the attention and MLP modules of a transformer(Vaswani et al. 2017). It takes the raw output from attention or the MLP (i.e., the hidden representation) as input and produces a modified hidden representation as output. LCF is illustrated in Figure 2.

# Content and Logic Projectors

Given a hidden representation $R _ { i n p u t }$ , i.e., the output of the self-attention or MLP module in the transformer. LCF maps $R _ { i n p u t }$ to both content and logic spaces using trained content and logic projectors, resulting in content representation $R _ { c o n t e n t }$ and logic representation $R _ { l o g i c }$ . Both the content and logic projectors are multi layer perceptron. To preserve the content of the representation, LCF only modifies $R _ { l o g i c }$ while keeping $R _ { c o n t e n t }$ unchanged. The modification of $R _ { l o g i c }$ is similar to gradient updating, adjusting $R _ { l o g i c }$ towards the logically valid region. The modification formula and the calculation formula for the logically valid direction are as follows:

$$
\begin{array} { r l } { R _ { l o g i c + } = R _ { l o g i c } + \mathbb { V } , } & { } \\ { \mathbb { V } = C _ { l o g i c } ^ { p o s } - C _ { l o g i c } ^ { n e g } , } & { } \end{array}
$$

where $\mathbb { V }$ is the logical valid direction. $C _ { l o g i c } ^ { p o s }$ and $C _ { l o g i c } ^ { n e g }$ represent the central points of logically valid region and logically invalid region in the logic space, calculated as the mean of all the logically valid representation and all the invalid representation, respectively.

![](images/0d5bc603b072f1caa2ba8ca6988de331cc66af010a8b9f77b855db58e50f9b6f.jpg)  
Figure 2: Logic Control Framework to modify the hidden representation in Transformers.

# Representation Fusion

LCF contains a decoder to maps $R _ { c o n t e n t }$ and the modified $R _ { l o g i c + }$ back to the original representation space. The decoder is also a multilayer perceptron. Specifically:

$$
\begin{array} { l } { R _ { + } = D e c o d e r ( R _ { c o n t e n t } , R _ { l o g i c + } ) , } \\ { = M L P ( R _ { c o n t e n t } + A t t n ( R _ { c o n t e n t } , R _ { l o g i c + } ) ) , } \end{array}
$$

where $_ { A t t r a }$ is a cross attention between $R _ { c o n t e n t }$ and $R _ { l o g i c + }$ . $R _ { + }$ includes both the content representation and the modified logical representation. We use it as an anchor point to adjust the original hidden representation $R _ { i n p u t }$ . Specifically, we adjust $R _ { i n p u t }$ to move closer to $R _ { + }$ :

$$
\begin{array} { c } { \mathbb { D } = R _ { + } - R _ { i n p u t } , } \\ { R _ { i n p u t + } = R _ { i n p u t } + \displaystyle \frac { \mathbb { D } } { \| { \mathbb { D } } \| _ { 2 } } \times \eta , } \end{array}
$$

where $\frac { \mathbb { D } } { \| \mathbb { D } \| _ { 2 } }$ is the modification direction and $\eta$ represents the magnitude of the modification. $R _ { i n p u t + }$ will replace $R _ { i n p u t }$ as input to the next module of the Transformer.

# Training

In LCF, both projectors and the decoder are trainable. We use a commonly used reconstruction objective to train these three modules:

$$
\begin{array} { c } { \hat { R } = D e c o d e r ( R _ { c o n t e n t } , R _ { l o g i c } ) , } \\ { \int _ { r e c } = M S E ( R _ { i n p u t } , \hat { R } ) . } \end{array}
$$

Relying solely on reconstruction loss does not ensure that the content projector learns logic-independent representations or that the logic projector learns content-independent representations(Li, Cai, and $\mathrm { \sf W u } 2 0 2 4 .$ ). If there is overlap between these representations, modifying the logic representation may impact the content. To achieve more precise disentanglement, we introduce two additional constraints for the projectors.

Logic Projector Constraint. For the logic projector, we expect it to extract significantly different representations from logically valid and logically invalid samples. Therefore, we introduce contrastive learning (Wang and Isola 2020; Xu et al. 2023) to constrain the logic projector. Specifically, we minimize the distance among representations of logically valid samples, denoted as $S _ { l o g i c + } ~ =$ $\{ R _ { l o g i c + } ^ { 1 } , R _ { l o g i c + } ^ { 2 } , . . . , R _ { l o g i c + } ^ { k } \}$ hwehedries $k$ niscethbeedtawteaesen seiazceh. $R _ { l o g i c + } ^ { i }$ and all representations of logically invalid samples,

$$
\begin{array} { r l r } {  { S _ { l o g i c - } ^ { \mathrm { ~ } , \mathrm { ~ } } = \big \{ R _ { l o g i c - } ^ { 1 } , R _ { l o g i c - } ^ { 2 } , . . . , R _ { l o g i c - } ^ { k } \big \} \colon } } \\ & { \mathscr { L } _ { \mathrm { l o g i c + } } = } \\ & { } & { \stackrel { \mathbb { E } } { \underset { ( x , y ) \sim S _ { \mathrm { l o g i c + } } } { \underbrace { \mathbb { E } } } } [ - \log \frac { e ^ { s i m ( x , y ) / \tau } } { e ^ { s i m ( x , y ) / \tau } + \sum _ { j } ^ { k } e ^ { s i m ( R _ { l o g i c - } ^ { j } , y ) / \tau } } ] . } \end{array}
$$

Similarly, we minimize the distance between representations of logically invalid samples $S _ { l o g i c - }$ , while maximize the distance between each $R _ { l o g i c - } ^ { i }$ and all representations of logically invalid samples $S _ { l o g i c + }$ :

$$
\begin{array} { r l } { \mathcal { L } _ { \mathrm { l o g i c - } } = } & { } \\ { \frac { \mathbb { E } } { ( x , y ) \sim S _ { \mathrm { l o g i c - } } } \left[ - \log \frac { e ^ { s i m ( x , y ) / \tau } } { e ^ { s i m ( x , y ) / \tau } + \sum _ { j } ^ { k } e ^ { s i m ( R _ { l o g i c + } ^ { j } , y ) / \tau } } \right] . } \end{array}
$$

Table 1: LCF improves the logical validity of LLMs in both generation and discrimination tasks. The $\mathrm { V a l i d } \mathcal { V } _ { O } ( \mathbf { G P T } 4 )$ and Valid%(Trained) represent that the validity of generated conclusions are determined by GPT4 or a trained model, respectively.   

<html><body><table><tr><td colspan="2"></td><td colspan="3">Conclusion Generation</td><td colspan="2">Fallacy Identification</td></tr><tr><td>LLMs</td><td></td><td>Valid%(GPT4) ↑</td><td>Valid%(Trained)↑</td><td>Perplexity ↓</td><td>Accuracy ↑</td><td>△Prob.↑</td></tr><tr><td>Llama2</td><td>Original</td><td>70.58</td><td>58.84</td><td>21.08</td><td>51.47</td><td>-1.89</td></tr><tr><td>(Touvron et al. 2023)</td><td>+LCF</td><td>83.82</td><td>96.56</td><td>12.12</td><td>75.00</td><td>6.29</td></tr><tr><td>Llama3</td><td>Original</td><td>70.58</td><td>51.47</td><td>32.54</td><td>50.00</td><td>-0.90</td></tr><tr><td>(Dubey et al. 2024)</td><td>+LCF</td><td>82.84</td><td>93.13</td><td>17.76</td><td>76.96</td><td>5.12</td></tr><tr><td>Vicuna</td><td>Original</td><td>72.54</td><td>58.82</td><td>22.81</td><td>49.01</td><td>-1.21</td></tr><tr><td>(Chiang et al. 2023)</td><td>+LCF</td><td>78.92</td><td>75.00</td><td>20.39</td><td>71.56</td><td>4.71</td></tr><tr><td>Mistral</td><td>Original</td><td>80.88</td><td>67.64</td><td>30.53</td><td>57.84</td><td>-1.66</td></tr><tr><td>(Jiang et al. 2023)</td><td>+LCF</td><td>85.71</td><td>94.60</td><td>21.17</td><td>74.01</td><td>3.39</td></tr><tr><td>ChatGLM3</td><td>Original</td><td>74.50</td><td>52.94</td><td>96.95</td><td>47.54</td><td>-3.29</td></tr><tr><td>(GLM et al. 2024)</td><td>+LCF</td><td>77.94</td><td>93.62</td><td>42.47</td><td>73.03</td><td>3.02</td></tr><tr><td>Baichuan</td><td>Original</td><td>78.32</td><td>68.62</td><td>35.63</td><td>50.49</td><td>-3.26</td></tr><tr><td>(Yang et al. 2023)</td><td>+LCF</td><td>81.86</td><td>91.17</td><td>29.82</td><td>66.17</td><td>1.59</td></tr></table></body></html>

Content Projector Constraint. For the content projector, we expect that the content representations it extracts are logically independent. Consequently, the same content representation should reconstruct into hidden representations with varying logical validity when combined with different logical representations. Specifically, for a pair of hidden representations $R _ { i n p u t + }$ and $R _ { i n p u t - }$ that share the same content but exhibit opposite logical validity, the constraint on the content projector is:

$$
\begin{array} { r } { \hat { R } _ { - } = D e c o d e r ( R _ { c o n t e n t + } , R _ { l o g i c - } ) , \qquad ( 1 0 } \\ { \hat { R } _ { + } = D e c o d e r ( R _ { c o n t e n t - } , R _ { l o g i c + } ) , \qquad ( 1 1 } \\ { \mathcal { L } _ { \mathrm { c o n t e n t } } = M S E ( R _ { i n p u t - } , \hat { R } _ { - } ) + M S E ( R _ { i n p u t + } , \hat { R } _ { + } ) . } \end{array}
$$

Finally, we optimize the reconstruction loss and the constraints of the logic projector and content projector:

$$
\mathcal { L } = \mathcal { L } _ { r e c } + \mathcal { L } _ { l o g i c + } + \mathcal { L } _ { l o g i c - } + \mathcal { L } _ { c o n t e n t } .
$$

# Experimental Setup

# Dataset

We use the LFUD dataset (Li et al. 2024b) for testing. LFUD is a logical fallacy understanding dataset featuring 12 types of fallacies across 67 scenarios, encompassing a total of 804 data points. We partition the dataset into training, validation, and test sets in a 45:5:17 ratio by scenario, yielding 540, 60, and 204 data points, respectively. The test set consists of scenarios that are distinct from those in the training and validation sets, enabling us to assess the proposed method’s effectiveness in separating content and logic in unseen scenarios. We introduce two types of evaluation tasks: conclusion generation and fallacy identification. These tasks assess whether LCF can mitigate fallacies in LLMs from both generative and discriminative perspectives.

Conclusion Generation. Given several natural language premises, an LLM must generate a valid conclusion based on these premises. The evaluation metric, $V \mathrm { a l i d } \%$ , is defined as the proportion of valid conclusions generated, calculated as the number of valid conclusions divided by the total number of conclusions generated. We use two discriminators to assess conclusion validity: (1) the GPT-4 Discriminator, which achieves an accuracy rate of $80 \%$ (Li et al. 2024b), though its judgments occasionally do not align with the fallacy types in LFUD; and (2) the Trained Llama-2 Discriminator, specifically trained to identify the 12 types of fallacies in LFUD, achieving over $90 \%$ accuracy in our manual verification. The metrics computed by these discriminators are referred to as $\mathbf { V a l i d \% ( G P T 4 ) }$ and Valid $\%$ (Trained), respectively. Additionally, Perplexity is used to measure the text generation capability of LLMs.

Fallacy Identification. Given several natural language premises, an LLM must select the valid conclusion from four options: one valid conclusion, two invalid conclusions, and $^ { \circ } \mathrm { I }$ have no comment.” We employ two evaluation metrics: 1. Accuracy: the proportion of correct samples to the total number of samples in the test set. 2. $\Delta$ Probability: the average difference between the probability assigned to the correct answer and the probabilities assigned to the incorrect answers. This metric assesses the LLM’s ability to distinguish between valid and invalid conclusions. A larger $\Delta$ Probability indicates a stronger ability of the LLM to identify fallacies.

# Base LLMs

We select several widely used open-source LLMs to evaluate the effectiveness of LCF. Including Llama2 (Llama2-7b-chat-hf (Touvron et al. 2023)), Llama3 (Meta-Llama3.1-8B-Instruct (Dubey et al. 2024)), Vicuna (vicuna-7bv1.5 (Chiang et al. 2023), Mistral (Mistral-7B-Instruct-v0.2) (Jiang et al. 2023)), ChatGLM3 (chatglm3-6b (GLM et al.

Case 1 (Intentional fallacy) Case 2 (Ad hominem)   
Premise No one has ever seen Brenda not practicing You’re always complaining about something. the violin.   
Llama2 Brenda is an excellent violinist. You never seem to be satisfied with anything.   
Llama2+LCF Therefore, it is likely that Brenda is a Based on this statement, it may be helpful to identify the specific skilled violinist. issue or issue(s) that are being complained about and to explore possible ways of addressing them. Case 3 (Ad populum) Case 4 (Deductive fallacy)   
Premise A lot of people think that the garden needs If some students do not take their studies seriously, failure is inevitable. watering. Some students take their studies seriously.   
Llama2 The garden needs watering. Students who take their studies seriously will not fail.   
Llama2+LCF The garden may need watering, but it is It is important to note that those who do take their studies seriously not a certainty may still experience failure, as success is not solely dependent on academic effort.

2024)), BaiChuan (Baichuan2-7B-Chat (Yang et al. 2023)).

# Implementation Details

Training data. Training LCF involves using hidden representations with identical content but opposite logical validity $( R _ { i n p u t + }$ and $R _ { i n p u t - } )$ . However, LFUD’s training data includes only 540 fallacious samples and lacks logically valid counterparts. To address this, we use GPT-3.5- turbo to generate logically valid conclusions for each sample. We manually review and revise these generated samples to ensure their validity, resulting in 540 logically valid samples and 540 logically invalid samples. From these, we extract $R _ { i n p u t + }$ and $R _ { i n p u t - }$ pairs. Details on the specific data used for training each LLM’s LCF are provided in the supplementary materials.

Training details. In LCF, both the content projector and the logic projector are two-layer MLPs with projection dimensions of 2048 and 1024. The decoder has dimensions of 1024 and 2048. We use AdamW (Loshchilov and Hutter 2017) to optimize for 10 epochs with a learning rate of 1e-3. The values of $\eta$ are 0.5 and 4.5 for conclusion generation and fallacy identification tasks, respectively. LCF only modifies the 10 attention or MLP layers with the highest distinctiveness.

# Results and Analysis Overall Performance

Generation Ability. As shown in Table 1, the LCF method significantly enhances the validity of conclusions generated by LLMs. According to the $V \mathrm { a l i d } \%$ (Trained) metric, LCF yields a $10 \% { - } 4 0 \%$ improvement across all six tested LLMs. The most substantial increase is $4 1 . 6 6 \%$ for Llama3, while the smallest is $1 6 . 1 6 \%$ for Vicuna. After incorporating LCF, $9 6 . 5 6 \%$ of Llama2-generated conclusions are deemed valid, followed by Mistral at $9 4 . 6 0 \%$ . Moreover, all LLMs with LCF produced conclusions with at least $90 \%$ validity. These results demonstrate that LCF effectively improves the performance of existing transformer-based LLMs.

For the GPT-4 evaluation, LCF results in a $3 \% - 1 3 \%$ improvement in validity. The Llama2 $_ { \mathrm { + L C F } }$ combination provides the largest increase of $1 3 . 2 4 \%$ among all LLMs. In the GPT-4 evaluation, Mistral demonstrates the highest conclusion validity at $8 0 . 8 8 \%$ , which rises to $8 5 . 7 1 \%$ with LCF. This suggests that LCF positively enhances the logical validity of all LLMs. Additionally, perplexity metrics show that LCF does not negatively impact the inherent language modeling capabilities of LLMs; all LLMs exhibit comparable or reduced perplexity after incorporating LCF. We also conduct human evaluations: three annotators analyzed 50 randomly selected samples. The results show that all three annotators observed a clear improvement in the logical validity of LCF for LLMs. More anlysis and cases are shown in Supplementary Materials 1.

Identification Ablitiy. In the fallacy identification task, LLM+LCF significantly improves performance, with Llama3 showing the largest increase in accuracy at $2 6 . 9 6 \%$ , and Baichuan the smallest at $1 5 . 6 8 \%$ . This suggests that LCF enhances not only the generation capabilities of LLMs but also their ability to identify fallacies. Through LCF modifications, LLMs’ hidden representations tend to produce logically valid outputs, thereby increasing the probability of such outputs. This enhancement helps LLMs better distinguish between valid and fallacious conclusions, as confirmed by the $\Delta$ Probability metric. Prior to LCF incorporation, LLMs’ $\Delta$ Probability was negative, indicating a poor distinction between valid and invalid conclusions, with a higher probability of invalid conclusions. After LCF incorporation, all LLMs showed improved discernment, with $\Delta$ Probability values becoming positive, and Llama2 demonstrated the highest increase at $6 . 2 9 \%$ . This indicates that contrastive learning, which amplifies the gap between logically valid and invalid representations, enhances LLMs ability to differentiate valid conclusions from fallacies.

Table 3: Ablation studies of LCF.   

<html><body><table><tr><td></td><td>Valid %</td><td>Accuracy</td><td>△Prob.</td></tr><tr><td>Llama2</td><td>57.84</td><td>51.47</td><td>-1.89</td></tr><tr><td>Llama2+LCF</td><td>96.56</td><td>75.00</td><td>6.29</td></tr><tr><td>w/o Lrec</td><td>93.13</td><td>76.96</td><td>6.95</td></tr><tr><td>w/o Lcontent</td><td>94.60</td><td>75.00</td><td>5.46</td></tr><tr><td>w/o Llogic</td><td>93.62</td><td>51.47</td><td>-1.83</td></tr><tr><td>w/o Content Proj.</td><td>82.84</td><td>73.52</td><td>5.64</td></tr></table></body></html>

# Case Study

We show examples in Table 2 of how modifying LLMs with LCF improves their logical validity. Overall, LCF enhances the objectivity of LLMs’ responses and the comprehensiveness of their problem analysis. For instance, in Case 1 and Case 4, the responses generated by LLMs+LCF are less absolute and retain the possibility of other factors being involved. Additionally, LCF reduces the risk of LLMs echoing others and potential personal attacks. For example, in Case 3, LLMs+LCF do not assume that a viewpoint is correct just because it is held by the majority. In Case 2, LLMs+LCF also focus more on the events themselves rather than evaluating people. More cases are in the supplementary materials.

# Ablation Study

Ablation results are shown in Table 3. Removing the content projector from LCF (without content projector) results in decreases ( $1 3 . 7 2 \%$ and $0 . 6 5 \%$ )in both $V \mathrm { a l i d } \%$ and Accuracy metrics, but still higher than original Llama2. This suggests that modifying the hidden representations of LLMs directly can enhance logical validity, but it is susceptible to content interference. This finding supports the effectiveness of separating content from logic. Additionally, ablation experiments on the three types of loss used to train LCF reveal that individually removing Lcontent, Llogic, and $\mathcal { L } _ { r e c }$ reduces the $V \mathrm { a l i d \% }$ of conclusions generated by LLMs to $9 3 . 1 3 \%$ , $9 4 . 6 0 \%$ , and $9 3 . 6 2 \%$ , respectively.

In contrast, the performance on discrimination tasks, such as accuracy and $\Delta$ Probability, significantly declines when $\mathcal { L } _ { l o g i c }$ is omitted. Accuracy drops from $7 5 . 0 0 \%$ to $5 1 . 4 7 \%$ , and $\Delta$ Probability decreases from $6 . 2 9 \%$ to $- 1 . 8 3 \%$ . This shows that contrastive learning significantly helps LCF learn to distinguish between logically valid and invalid content.

# Visualization

We conduct a visual analysis of the content and logic spaces in LCF. Using the t-SNE (Van der Maaten and Hinton 2008) method, we reduce the dimensionality of validation set samples to 2D. Figure 3 (a) illustrates the distribution of logically valid and invalid samples in both spaces after processing through the content and logic projectors. The content space exhibits a uniform distribution due to LCF’s lack of control over content representation, while the logic space reveals a clear boundary between valid and invalid logical representations. This separation suggests that contrastive learning effectively distinguishes between valid and invalid representations in the logic space. Additionally, Figures 3(b)

![](images/a6dd3a1b58d0abdb15517404eb24cd3e7e0323c048b9f1bfdf64d2f564ae6c31.jpg)  
Figure 3: Logic Control Framework to modify the hidden representation in Transformers.   
Figure 4: Comparison between LLMs with valid and invalid modification using LCF.

100% Conclusion Generation 80% Fallacyldentification 3560 8 10 R 150 GA 86 3060 34 18 LcFivalid) LCF(Invalid) 00 46 2 300 Og 3 5 260 8 4 Llama2 Mistral Vicuna ChatGLM Llama2 Mistral Vicuna ChatGLM

and (c) display eight different logic spaces across layers. The first four layers (Figure 3(b)) show logic spaces with higher discriminative power, where LCF can clearly differentiate between valid and invalid representations. In contrast, the latter four layers (Figure 3(c)) present logic spaces with lower discriminative power, where valid and invalid representations are intermixed with blurred boundaries. This indicates that not all hidden representations across layers are equally suitable for modification.

# Invalid Modification

In addition to enhancing the logical validity of LLMs, LCF can also be used to reduce it. By reversing the direction in Formula (4), we modify the hidden representations to move away from the logically valid region. Figure 4 and Table 4 illustrate the performance comparison and examples following invalid modifications. After applying invalid modifications to the tested LLMs, there is a marked decline in the validity of the generated conclusions. The Valid $\%$ metric drops by $2 0 . 5 8 \% - 4 2 . 1 5 \%$ compared to the original LLMs and by $3 6 . 7 6 \% - 7 4 . 5 0 \%$ compared to $\mathbf { L L M s + L C F }$ (Valid). For fallacy detection, accuracy decreases by $2 5 . 4 8 \% - 3 7 . 2 5 \%$ compared to the original LLMs and by $4 8 . 5 2 \% - 6 0 . 7 8 \%$ compared to LLMs+LCF (Valid). As shown in Table 4, conclusions generated by LLMs after LCF invalid modification exhibit strong absoluteness, a characteristic typical of conclusions with logical fallacies. These results demonstrate that LCF can alter logical validity in both directions.

Table 4: Generated conclusions of LLMs with valid (LCFV) and invalid (LCF-I) modification.   

<html><body><table><tr><td>Case1</td><td></td></tr><tr><td>Premise</td><td>Gardeners claim that all flowers don't stay open forever.Gardeners are experts in the field concern- ing flowers.</td></tr><tr><td>+LCF-V</td><td>While gardeners may have extensive knowledge about flowers and their behavior,theirclaim that all flowers don't stay open forever may not be uni- versally true.</td></tr><tr><td>+LCF-I</td><td>Flowers don't stay open forever.</td></tr><tr><td></td><td>Case2</td></tr><tr><td>Premise</td><td>Anna claims she finished the report on time,but we all know she took a nap in the afternoon. She's a slacker.</td></tr><tr><td>+LCF-V</td><td>Anna's claim that she finished the report on time may not be entirely accurate due to her afternoon nap.</td></tr><tr><td>+LCF-I</td><td>Anna is not trustworthy.</td></tr></table></body></html>

# Modification Layer Analysis

Based on the visualization results, the decoupling and modification capabilities of the LCF vary across different layers of hidden representations. We limit modifications to one layer at a time and analyze their impact on the results. Specifically, we examine how modifying layers 10-30 affects conclusion generation and fallacy identification. Figure 5 (top) presents the $V \mathrm { a l i d } \%$ of conclusion generation for modifications to individual attention (a) and MLP (b) layers. Figure 5 (bottom) shows the Accuracy of fallacy identification for the same modifications (c and d). The results reveal a clear pattern: modifying attention layers between 10 and 20 significantly improves both conclusion generation and fallacy identification, whereas modifications to attention layers beyond 20 show diminished performance. In contrast, modifying MLP layers between 20 and 30 yields better results compared to modifying MLP layers between 10 and 20. Additionally, the optimal number of layers to modify is taskdependent, but overall, modifications to attention and MLP layers in the 15-20 range generally enhance performance.

# Supervised Finetuning Comparison

We conduct comparison between two truthfulness-oriented baselines, ITI (Li et al. 2024a) and RAHF (Liu et al. 2024b).

![](images/01576fce32c3f2a6cc6e98b91b5dc6a9824956b4ebe90db2740bbdb95e8ce6c0.jpg)  
Figure 5: Modifying LLMs of single attention or MLP layer. (a) Attention Layer on Conclusion Generation $( \mathrm { V a l i d } \% )$ . (b) MLP Layer on Conclusion Generation $( \mathrm { V a l i d } \% )$ ). (c) Attention Layer on Fallacy Identification $( \operatorname { A c c } \% )$ . (d) MLP Layer on Fallacy Identification $( \operatorname { A c c } \% )$ . Llama2(blue), Mistral(orange), Vicuna(green).

Table 5: Comparison with Supervised Fine-tuning.   

<html><body><table><tr><td></td><td>Valid% (GPT-4)</td><td>Valid%(Trained)</td></tr><tr><td>Llama2</td><td>70.58</td><td>58.84</td></tr><tr><td>ITI</td><td>69.60</td><td>62.25</td></tr><tr><td>RAHF</td><td>71.56</td><td>46.56</td></tr><tr><td>SFT</td><td>79.90</td><td>78.43</td></tr><tr><td>LCF(Ours)</td><td>83.82</td><td>96.56</td></tr></table></body></html>

Results in Table 5 suggest that these methods designed for improving truthfulness have limited improvement in logical validity of LLMs. On the contrary, supervised finetuning is a more straightforward and powerful baseline. After SFT training on Llama2, logical validity improves by $20 \%$ , though this enhancement is not as substantial as with LCF $( 3 8 \% )$ . These findings demonstrate the effectiveness of the proposed LCF method.

# Conclusion

We propose a logic control framework (LCF) designed to decouple and modify the hidden representations of LLMs. LCF projects the attention and MLP outputs of LLMs into separate content and logic spaces. By adjusting representations in the logic space to align with regions of logically valid representations, LCF significantly enhances the logical validity of LLMs. Evaluations across six LLMs in tasks such as conclusion generation and fallacy identification demonstrate LCF’s effectiveness and robustness. We also quantitatively assess the impact of factors like modification magnitude and layer count. Furthermore, case studies and visualizations confirm that LCF substantially improves the validity of conclusions generated by LLMs and their ability to identify fallacies.