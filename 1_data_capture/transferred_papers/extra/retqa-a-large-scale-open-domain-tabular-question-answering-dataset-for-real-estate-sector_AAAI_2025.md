# RETQA: A Large-Scale Open-Domain Tabular Question Answering Dataset for Real Estate Sector

Zhensheng Wang1,Wenmian Yang2\*,Kun Zhou1,Yiquan Zhang3,Weijia Jia2,4\*

1School of Artificial Intelligence, Beijing Normal University, Beijing 100875, PR China 2Institute of Artificial Intelligence and Future Networks, Beijing Normal University, Zhuhai 519087, PR China 3Elmleaf Ltd., Shanghai 200082, PR China 4BNU-UIC Institute of Artificial Intelligence and Future Networks, Beijing Normal University (Zhuhai), Guangdong Key Lab of AI and Multi-Modal Data Processing, BNU-HKBU United International College, Zhuhai, Guang Dong, PR China {jensenwang,zhoukun}@mail.bnu.edu.cn, {wenmianyang,jiawj} $@$ bnu.edu.cn, zhangyq987@hotmail.com

# Abstract

The real estate market relies heavily on structured data, such as property details, market trends, and price fluctuations. However, the lack of specialized Tabular Question Answering datasets in this domain limits the development of automated question-answering systems. To fill this gap, we introduce RETQA, the first large-scale open-domain Chinese Tabular Question Answering dataset for Real Estate. RETQA comprises 4,932 tables and 20,762 question-answer pairs across 16 sub-fields within three major domains: property information, real estate company finance information and land auction information. Compared with existing tabular question answering datasets, RETQA poses greater challenges due to three key factors: long-table structures, open-domain retrieval, and multi-domain queries. To tackle these challenges, we propose the SLUTQA framework, which integrates large language models with spoken language understanding tasks to enhance retrieval and answering accuracy. Extensive experiments demonstrate that SLUTQA significantly improves the performance of large language models on RETQA by incontext learning. RETQA and SLUTQA provide essential resources for advancing tabular question answering research in the real estate domain, addressing critical challenges in opendomain and long-table question-answering.

Code — https://github.com/jensenw1/RETQA

# Introduction

With rapid advancements in artificial intelligence and natural language processing, Tabular Question Answering (TQA) has garnered attention for its ability to extract accurate answers from structured data across domains like finance and healthcare (Zhu et al. 2024). Despite progress, specialized datasets for the real estate domain are scarce, limiting research and practical applications. The real estate market relies on structured data, such as property details and market trends, which are crucial for stakeholders like homebuyers and investors. For instance, homebuyers can make informed decisions by querying historical transaction prices and property details, while investors can assess the viability of investments by querying land information, asset status, and the developer’s transaction history. However, the lack of tailored TQA datasets in this field hampers the development of automated QA systems.

To address this gap, we introduce RETQA, the first largescale open-domain Chinese TQA dataset specifically designed for the Real Estate sector. RETQA is built from publicly available real estate data and comprises 4,932 tables and 20,762 QA pairs spanning 16 sub-fields within three major domains: property information, real estate company finance, and land auction information. Given the complex and data-intensive nature of the real estate market, RETQA is crafted to generate queries targeting longer tables, with an average of 252.9 rows per query-related table. Moreover, queries in RETQA may related to more than one table. The above design captures the real estate domain’s complexity, making RETQA a challenging TQA dataset.

The open-domain nature of RETQA requires models to retrieve relevant tables from the entire dataset, rather than being directly provided, effectively mirroring real-world scenarios. However, this process makes open-domain TQA more challenging than closed-domain, as retrieval accuracy directly affects overall TQA performance. To facilitate retrieval, RETQA assigns a summary caption to each table. Furthermore, RETQA integrates the labels of Spoken Language Understanding (SLU) for each query. These SLU labels, including intent and slot labels, commonly employed in task-oriented dialogue systems (Qin et al. 2021; Cheng, Yang, and Jia 2023; Zhu et al. 2024; Qin et al. 2024), are instrumental in discerning user query intent and extracting pertinent details. As depicted in Figure 1, SLU labels enable more accurate parsing of user intent and relevant information, thereby enabling more precise retrieval and accurate answering. For each query, RETQA provides intent labels, slot labels, table captions, and answers in Markdown, SQL-style, and natural language formats. To the best of our knowledge, RETQA is the first TQA dataset to integrate SLU labels.

RETQA is created through four key steps: First, we compile a dataset of 4,932 real estate-related tables from eight major Chinese cities—Beijing, Shanghai, Guangdong, Shenzhen, Suzhou, Hangzhou, Nanjing, and

Utterance: What is the average price ini Jiangning District Nanjing October 2021for Poly Central   
Slot: 0 0 0 D 0 B-D I-D io' B-C B-M B-YO B-DN I-DN 1-DN O 1   
Intent: real_estate_project_average_price_query F ■   
Table Names: ■ 1 Residential Property Sales Price Table for Jiangning District of Narjing City, November 2022 China's Real Estate Companies' Financial Statemerlts for 2021 Land Auctionnformation Table for Jiangning DistricyNanjing City ? Wuyi Oasis 2   
ResidentialProperty Sales PriceTableforJiangningDistrictofNanjingCity,October2021 China's Real Estate Companies'Financial Statements for 2022 Poly Central Park Residential Property Sales Price Table for Chaoyang District of Beijing City, October 2021 Land Auction Information Table for Gulou District, Nanjing City Greenland Ideal City 17742.69

Wuhan—spanning the years 2019 to 2022. These tables include property information, real estate company finances, and land auction data. The publicly available data is meticulously cleaned and organized, and each table is assigned a summary caption to aid retrieval. Second, we develop 90 real estate-specific question templates, addressing factual, inferential, and comparative queries. These templates are populated with real data to generate accurate QA pairs, with answers provided in Markdown, SQL-style, and natural language formats. Third, we annotate intent and slot labels by reverse engineering, classifying queries into 16 intent categories and identifying six slot categories based on entity types found in the query templates and table headers. Finally, we use in-context learning to rewrite the templategenerated questions, enhancing their naturalness and diversity. After filtering, RETQA retains 20,762 QA pairs.

In summary, RETQA accurately mirrors the real-world challenges users encounter when querying real estate-related data, including open-domain queries, long-table queries, and multi-domain and multi-table queries, making RETQA a particularly demanding task. To address these challenges, we propose SLUTQA as a benchmark for RETQA, which improves the performance of large language models (LLMs) on TQA tasks by leveraging SLU labels through in-context learning (ICL).

The SLUTQA framework consists of three key modules: the SLU module, the SLU label-based Retrieval (SR) module, and the SLU label-based Filtering-Answer (SFA) module. For each query, the SLU module generates intent and slot labels at first. Depending on the availability of SLU labels, we use two approaches: (1) fine-tuning a BERT (Devlin et al. 2019) model when labels are sufficient, and (2) employing in-context learning with large language models (LLMs) when labels are limited (few-shot). The SR module then uses those SLU labels to create a query summary via in-context learning, improving retrieval accuracy by replacing the original query when searching for the relevant table caption by BM25 (Robertson and Walker 1994). Finally, the SFA module generates accurate SQL statements or refined Markdown answers based on the predicted SLU labels and retrieved tables. This framework leverages the strengths of large language models while integrating SLU task labels, enhancing the performance of open-domain long-table question answering tasks. To the best of our knowledge, this is the first work to combine SLU tasks with TQA tasks.

Extensive experiments on RETQA demonstrate that SLUTQA significantly enhances LLM performance without requiring fine-tuning. Our dataset and framework offer valuable resources for advancing TQA research in the real estate domain, promoting further development in this field. The code and data will be released after the blind review. The main contributions of this paper are as follows:

• We introduce the first large-scale open-domain Chinese TQA dataset in the real estate domain, enriched with SLU labels. This dataset is challenging due to its longtable, multi-table, and open-domain characteristics.   
• We propose the SLUTQA framework, which integrates LLMs with SLU tasks, significantly improving TQA accuracy on the RETQA dataset. To the best of our knowledge, this is the first work to combine SLU tasks with table QA tasks.   
• Extensive experimental results show that SLUTQA improves the performance of existing LLMs on RETQA without the need for fine-tuning.

# Related Works

Our work is related to two areas, i.e., TQA datasets, and TQA methods.

# TQA Datasets

Early research, such as WikiTableQuestions (Pasupat and Liang 2015) and WikiSQL (Zhong, Xiong, and Socher 2017), collected tabular data from web sources like Wikipedia. However, WikiTableQuestions only provides text answers, and WikiSQL’s question descriptions are too vague to reliably locate relevant tables. The Spider dataset (Yu et al. 2018), designed for complex, cross-domain Textto-SQL tasks, is not suited for open-domain scenarios and only offers SQL-format answers. NQ-TABLES (Herzig et al. 2021), the first open-domain table QA dataset, uses a reader model to extract answers from $K$ candidate tables by selecting a single cell, limiting the need for complex reasoning (Kweon et al. 2023). Open-WikiTable (Kweon et al. 2023), based on WikiSQL and WikiTableQuestions, offers both SQL and text answers but typically features short tables. To the best of our knowledge, we are the first TQA dataset providing the SLU labels. The comparison of RETQA to other related datasets is shown in Table 1.

<html><body><table><tr><td>Dataset</td><td>Open Domain</td><td># of QA pairs</td><td># Tables</td><td>Answer format</td><td>Multi-table</td><td>SLU</td><td>Long Table</td></tr><tr><td>WikiSQL</td><td>X</td><td>80654</td><td>24241</td><td>SQL</td><td>X</td><td>X</td><td>X</td></tr><tr><td>WikiTableQuestion</td><td>×</td><td>22033</td><td>2108</td><td>Text</td><td>×</td><td>×</td><td>×</td></tr><tr><td>Spider</td><td>×</td><td>10181</td><td>1020</td><td>SQL</td><td>√</td><td>X</td><td>√</td></tr><tr><td>Open-WikiTable</td><td>√</td><td>67023</td><td>24680</td><td>Text,SQL</td><td>X</td><td>×</td><td>X</td></tr><tr><td>NQ-TABLES</td><td>√</td><td>11628</td><td>169898</td><td>Text</td><td>×</td><td>X</td><td>二</td></tr><tr><td>RETQA (Ours)</td><td>√</td><td>20762</td><td>4932</td><td>Text,SQL</td><td>√</td><td>√</td><td>√</td></tr></table></body></html>

Table 1: Dataset Comparison, where “–” indicates data currently inaccessible. We define a dataset as a long table dataset if the average number of rows in the query-related tables exceeds 100.

# TQA Methods

Before the emergence of large language models (LLMs), researchers explored methods to combine tabular data with neural networks for natural language processing and data management tasks (Badaro, Saeed, and Papotti 2023; Fang et al. 2024; Shwartz-Ziv and Armon 2022). LLMs gained attention for their strong cross-task generalization, adapting to new tasks with minimal examples. (Chen 2023) first demonstrated LLMs’ ability to reason over tables through in-context learning. TAP4LLM (Sui et al. 2023) addresses noisy table data by enhancing sub-tables. OPENTAB retrieves tables, generates SQL as intermediate steps, and relies on a reader for final answers, though it may produce incorrect SQL (Kong et al. 2024). StructGPT (Jiang et al. 2023) uses interfaces to handle large structured data efficiently. MultiTabQA (Pal et al. 2023) introduces an LLMbased framework for answering questions across multiple tables, but it is limited to queries in closed domains. In comparison, our novel integration of LLMs with SLU tasks significantly enhances open-domain TQA accuracy.

# Dataset Construction and Analysis

This section elaborates on how RETQA is created, including details on table collection, QA pair generation, intent and slot annotation, and query rewriting and quality control.

# Table Collection

Our table data comes from publicly available real-world sources, including property data, real estate company financial data, and land auction data. The specific data sources and descriptions are as follows:

Property Data: We collected commercial housing transaction data from eight major Chinese cities (Beijing, Shanghai, Guangzhou, Shenzhen, Suzhou, Hangzhou, Nanjing, Wuhan) for 2019-2022, sourced from http://www.fangdi.com.cn/. The tables, keyed by development name, include district, average transaction prices, year and month of transaction dates, number of transactions, developers, etc. This resulted in 4,825 standardized tables, captioned with the administrative region and year-month (e.g., “Residential Property Sales Price Table for Jiangning District of Nanjing City, May 2022”).

Real Estate Company Finance Data: Financial disclosures from Chinese real estate companies for 2019-2022 are collected from https://aur.elmleaf.com.cn. Each table, keyed by company name, includes total operating revenue, operating profit, total operating costs, total assets, total liabilities, state-owned status, credit bond status, and risk level. The data is organized by year, resulting in four tables captioned with the year (e.g., “China’s Real Estate Companies’ Financial Statements for $2 0 2 0 ^ { , \cdot } ,$ ).

Land Auction Data: Land auction information from 2016- 2022 for the same eight cities is collected from city-specific public websites: Beijing1, Shanghai2, Guangzhou3, Shenzhen4, Suzhou5, Hangzhou6, $\mathrm { { N a n j i n g } ^ { \mathrm { 7 } } }$ , and Wuhan8. Tables, keyed by land parcel name, include details like development name, transaction date, affiliated group, total transaction price, floor area ratio, building density $( \% )$ , green coverage ratio $( \% )$ , etc. This yielded 103 tables, captioned by district(e.g., “Land Auction Information Table for Xuhui District, Shanghai”).

In total, our dataset integrates 4,932 tables across these three domains, with the longest table containing 465 rows. Each table is annotated with a standardized caption for retrieval. For more details about the table, see Appendix C.

# QA Pair Generation

We utilize a template-based approach to generate QA pairs automatically. Specifically, we define 90 query templates covering various types of inquiries, including factual, inferential, and comparative questions, derived from extracted tables and create corresponding SQL templates for each query. Among these, 23 templates are designed to generate queries that require multi-table support. Given the complex and data-intensive nature of the real estate market, we focus on generating queries that target longer tables to ensure that the dataset accurately reflects this complexity. Consequently, the average number of rows per query-related table is 252.9. Additional details on the templates are provided in Appendix A.

Using these templates, we generated 300 natural language queries and corresponding SQL statements per template, resulting in a total of 27,000 pairs. We then filtered out QA pairs with duplicate queries, non-executable SQL statements, and SQL statements that return empty results, leaving us with 20,471 QA pairs. For each remaining pair, we also provide a matching Markdown-formatted input table (with multiple tables concatenated into a single row) and the corresponding output answer table. Since the real estate data is collected from Chinese public websites, all queries are generated in Chinese. We plan to publish an English version soon.

Intent and Slot Annotation To facilitate researchers effectively extracting key information from queries and achieve accurate retrieval and answers, RETQA includes additional Spoken Language Understanding (SLU) labels. SLU consists of two types of labels: intent and slot labels, commonly used in task-oriented dialogue systems. Intent labels identify the user’s query intent, while slot labels extract key information pertinent to that intent.

We annotate the intent and slot labels through reverse engineering. Specifically, for intent labels, we categorized the queries into 16 types based on the 90 templates mentioned earlier. Some queries in the templates may involve multiple intents. For example, in the query, “Which of the top 5 best-selling residential compounds in Bao’an District, Shenzhen, has the lowest housing price”, both average price and sales volume information are required. In this case, the query would be labeled with two intents: “real estate project sales volume query” and “real estate project average transaction price query.” For more details and statistics about the intent labels, please see Appendix C.

For slot labels, we categorize the entities into six types: “city”, “district”, “development name”, “company name”, “year”, and “month’,’ based on the entity types in the query templates and corresponding table headers. Following previous work on SLU tasks (Qin et al. 2022), we adopt the Inside–Outside–Beginning (IOB) tagging format (Ramshaw and Marcus 1999). In this format, the B- prefix indicates the beginning of a slot chunk, the I- prefix indicates that the tag is inside a slot chunk, and the O tag indicates that a token does not belong to any slot chunk. An example of SLU labels is shown in Figure 1.

LLM-based Query Rewriting and Quality Control Despite our extensive template library, the sentences generated from these templates can be grammatically monotonous, differing significantly from real human queries. This disparity may cause models trained on template-based data to struggle with actual user queries. To address this, we use large language models (LLMs) to rewrite the queries generated by templates, creating more diverse and human-like expressions while retaining their original meaning. Techniques such as synonym replacement and sentence inversion are employed in this process.

We used Qwen2 72B (Yang et al. 2024) for rewriting and DeepSeek-V1 (Dense-67B) (DeepSeek-AI 2024) to evaluate the results. The evaluation scores questions on a scale from 0 (“completely like template generation”) to 5 (“like human writing”). After rewriting, the average score increased from 2.56 to 2.95, indicating a significant improvement in naturalness and diversity. The distribution of scores before and after rewriting is shown in Appendix B.

After rewriting, we manually reviewed and removed 291 incorrect QA pairs, leaving 20,762 pairs in the RETQA dataset, and split them into training, testing, and validation sets at a ratio of 0.8:0.1:0.1. To ensure that each template was represented in each set, we perform sampling based on the templates. Finally, for each query, RETQA provides the corresponding intent labels, slot labels, and the caption of the relevant table(s), as well as answers in three formats: Markdown, SQL-style (with both the SQL statements and their resulting answers), and natural language. Detailed statistics of RETQA are shown in Appendix C.

# Method

In this section, we introduce the SLUTQA framework, which consists of three key modules: the SLU module, the SLU label-based Retrieval (SR) module, and the SLU labelbased Filtering-Answer (SFA) module. SLUTQA aims to enhance the performance of LLMs on TQA tasks through SLU labels-based in-context learning (ICL). The general framework of SLUTQA is shown in Figure 2.

# SLU Module

SLU involves two label types, i.e., intent and slot, commonly used in task-oriented dialogue systems. Intent labels identify the user’s query intent, while slot labels extract relevant key information. In this paper, we design a SLU module to predict the SLU labels of each query, and leverage these labels to enhance information extraction and ensure accurate retrieval and answers within the SLUQTA framework.

Based on the availability of labeled SLU data, we design two approaches to implement the SLU module: (1) finetuning a BERT model (Devlin et al. 2019) when sufficient labeled data are available, and (2) utilizing ICL with LLMs in scenarios where labeled data are limited (few-shot).

In the first scenario, given the current query $\textbf { X } =$ $\{ x _ { 1 } , . . . , x _ { n } \}$ as input, we utilize a pre-trained BERT 9 as the encoder. The model returns the hidden states $\begin{array} { l l } { H } & { = } \end{array}$ $\{ h _ { c l s } , h _ { 1 } , . . . , h _ { n } \ \in \ \mathbb { R } ^ { d _ { m o d e l } } \}$ , where CLS is a special token representing the entire input sequence within BERT, and $d _ { m o d e l }$ is the output dimension of BERT.

Then, we predict the intent labels, number of intents, and slot labels by:

$$
\begin{array} { r l } & { \quad \mathbf { y } ^ { I } = \mathrm { S i g m o i d } ( \mathbf { W } ^ { I } \cdot \mathbf { h } _ { c l s } + \mathbf { b } ^ { I } ) , } \\ & { \quad \mathbf { y } ^ { N } = \mathrm { S o f t m a x } ( \mathbf { W } ^ { N } \cdot \mathbf { h } _ { c l s } + \mathbf { b } ^ { N } ) , } \\ & { \quad \mathbf { y } _ { j } ^ { S } = \mathrm { S o f t m a x } ( \mathbf { W } ^ { S } \cdot \mathbf { h } _ { j } + \mathbf { b } ^ { S } ) , } \end{array}
$$

9https://huggingface.co/google-bert/bert-base-uncased

SLU Module   
SLU Examples Markdown Answer SFA SQL Examples col:deverapmentnpmeces in Jiangning District, Nanjing? the lowest average price in Baoshan District, Shanghai? row 1 : Jinqiu Garden| Intent:real_estateprojectaverage_priceinquiry LLMOr BERT 2A Name:Poly Central Park, PropertySalesPriceTableforNanshanDistrictofShenzhen Year:2020,onth:November} [Jinqiu Garden]] November 2019" ORDER BY .·. Intent Slots "average_transaction_prices"DESCLIMIT1;   
SR Examples SR Module SFA Module SOLuton SAMarldonExaplesape Qner l LN ? 王 LLM C Name:Poly Central Park, Ltd.|2021row2:TianlinMansion|Tianhe|138955.46] Year:2020,Monthovember Summary September|2|Yingguang Holdings Limited|2021 row 3 5 Apartmen

where $\mathbf { y } ^ { I } \in \mathbb { R } ^ { d _ { i } }$ , $\mathbf { y } ^ { N } \in \mathbb { R } ^ { d _ { n } }$ , and $\mathbf { y } ^ { S } = \mathbf { y } _ { 1 } ^ { S } , . . . , \mathbf { y } _ { n } ^ { S } \in \mathbb { R } ^ { d _ { s } }$ represent the predicted results of intents, number of intent, and slots, respectively, $\mathbf { W } ^ { I } ~ \in ~ \mathbb { R } ^ { d _ { i } \times d _ { m o d e l } }$ , ${ \bf W } ^ { N } \in \left( \begin{array} { l } { \begin{array} { r l } \end{array} } \end{array} \right)$ $\mathbb { R } ^ { d _ { n } \times d _ { m o d e l } }$ , and $\mathbf { W } ^ { S } \in \mathbb { R } ^ { \tilde { d _ { s } } \times d _ { m o d e l } }$ are fully connected matrices, $\mathbf { b } ^ { I } \in \mathbb { R } ^ { d _ { i } } , \mathbf { b } ^ { N } \in \mathbb { R } ^ { d _ { n } }$ and $\mathbf { b } ^ { S } \in \mathbb { R } ^ { d _ { s } }$ are bias vectors. In this paper, $d _ { i } = 1 6$ , $d _ { n } = 2$ , and $d _ { s } = 1 3$ (6 kinds of slots with B-tags, I-tags, and the O-tag), representing the categories of intent labels, number of intents, and categories of slot labels, respectively.

During fine-tuning, cross-entropy loss is employed for both intent number prediction and slot filling, while binary cross-entropy is used for intent detection. During inference, the top 1 or 2 predicted intents are selected based on the predicted number of intents.

In the second scenario, where labeled SLU data are limited (few-shot) and insufficient for training or fine-tuning a model, we predict SLU labels using ICL in LLMs. Specifically, we randomly select 22 examples from the training set to serve as prompt examples for the LLMs. As illustrated in Figure 2 (top left, SLU Examples), the LLM leverages these few-shot examples as prompts to jointly predict intent and slot labels for each input query.

# SR Module

Although our dataset provides corresponding tables (i.e., gold tables) for each query, in real-world open-domain scenarios, these tables must be retrieved based on the query, and the retrieval accuracy directly affects overall TQA performance. Previous works (Kong et al. 2024) often use BM25 (Robertson and Zaragoza 2009) for table retrieval due to its scalability and competitive performance. However, in the real estate data scenario, user queries can be diverse and nonstandard, while table captions are often similar (e.g., differing only by years or months). This means that relying solely on BM25 for table retrieval may not capture the critical information needed, leading to lower accuracy in our scenario.

To address this issue, we propose the SR module, which utilizes ICL with SLU labels to enable the large LLMs to generate a query summary in the form of a table caption. For each query, five examples from the training set that share the same intent (as predicted by the SLU module) are selected and used as prompts for the LLM. As illustrated in Figure 2 (bottom left, SR Examples), each example comprises four components: the query, intent(s), slots, and query summary, with the query summary corresponding to the table caption(s) in the example, which may include multiple tables. This approach enables the LLM to learn to generate a summary in the form of a table caption relevant to the query. The intent and slot information allow the LLM to more accurately capture the key details in the query, resulting in a more precise summary. Finally, the SR-generated summary, rather than the original query, is used to retrieve the table caption via BM25. It is important to note that the generated summary may consist of multiple entries, with each summary being used to retrieve one table using BM25, selecting the top result for each.

# SFA Module

After retrieving the corresponding tables from the SR module, we designed the SFA module to generate final answers in two formats: SQL-style and markdown.

In markdown formats, existing methods typically flatten entire tables into a single row for processing by large language models (LLMs). However, RETQA includes many lengthy tables, and current LLMs, with their limited input lengths (usually 32k tokens), struggle to effectively handle queries involving these extensive tables. To address this issue, the SFA module for markdown formats is designed to filter out rows and columns that are irrelevant to the query. This is achieved with the help of SLU labels, which contain key information that guides the LLM to focus on the relevant portions of the table. Specifically, the SFA module operates in two steps. First, the original markdown table is simplified by removing irrelevant rows and columns based on the identified slots and intent. Second, the simplified table is used to generate the final output. This process is facilitated through ICL. For each query, 2-5 examples (according to the total length) from the training set, all sharing the same intent, are selected. As illustrated in Figure 2 (bottom right, SFA Markdown Examples), the first stage of the process includes examples consisting of five components: the query, intent(s), slots, original table, and simplified table. In the second stage, the examples include the query, intent(s), slots, simplified table, and the final answer.

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Method</td><td rowspan="2">Table EM(%)</td><td colspan="3">Row EM(%)</td><td colspan="3">Column EM(%)</td><td colspan="3">Cell EM(%)</td></tr><tr><td>P</td><td>R</td><td>F1</td><td>P</td><td>R</td><td>F1</td><td>P</td><td>R</td><td>F1</td></tr><tr><td rowspan="3">Qwen2 7b</td><td>Vanilla</td><td>15.05</td><td>24.43</td><td>31.71</td><td>27.60</td><td>25.10</td><td>22.11</td><td>23.51</td><td>36.19</td><td>43.35</td><td>39.45</td></tr><tr><td>SLUTQA (ICL)</td><td>16.65</td><td>26.73</td><td>25.34</td><td>26.02</td><td>23.36</td><td>26.06</td><td>24.64</td><td>40.67</td><td>40.45</td><td>40.56</td></tr><tr><td>SLUTQA (FT)</td><td>18.94</td><td>32.36</td><td>29.61</td><td>30.92</td><td>23.14</td><td>28.90</td><td>25.70</td><td>45.07</td><td>43.24</td><td>44.14</td></tr><tr><td rowspan="3">GLM4 9b</td><td>Vanilla</td><td>4.73</td><td>26.55</td><td>45.71</td><td>33.59</td><td>31.17</td><td>30.36</td><td>30.76</td><td>35.77</td><td>59.42</td><td>44.66</td></tr><tr><td>SLUTQA (ICL)</td><td>9.15</td><td>34.88</td><td>40.42</td><td>37.44</td><td>28.19</td><td>34.25</td><td>30.92</td><td>44.66</td><td>55.80</td><td>49.61</td></tr><tr><td>SLUTQA (FT)</td><td>9.08</td><td>37.62</td><td>43.44</td><td>40.32</td><td>28.73</td><td>33.93</td><td>31.11</td><td>47.75</td><td>58.01</td><td>52.38</td></tr><tr><td rowspan="3">Qwen2 72b</td><td>Vanilla</td><td>11.14</td><td>36.27</td><td>54.54</td><td>43.57</td><td>32.16</td><td>32.93</td><td>32.54</td><td>44.64</td><td>58.46</td><td>50.62</td></tr><tr><td>SLUTQA (ICL)</td><td>14.81</td><td>49.79</td><td>53.74</td><td>51.69</td><td>44.01</td><td>42.75</td><td>43.37</td><td>64.51</td><td>67.68</td><td>66.06</td></tr><tr><td>SLUTQA (FT)</td><td>15.02</td><td>51.26</td><td>55.61</td><td>53.34</td><td>43.39</td><td>41.90</td><td>42.63</td><td>65.01</td><td>68.29</td><td>66.61</td></tr></table></body></html>

Table 2: Overall performance of markdown format answer.

Table 3: Overall performance of SQL format answer.   

<html><body><table><tr><td>Model</td><td>Method</td><td>ECR(%)</td><td>pass@1</td></tr><tr><td rowspan="3">Qwen2 7b</td><td>Vanilla</td><td>60.11</td><td>39.55</td></tr><tr><td>SLUTQA (ICL)</td><td>85.46</td><td>64.46</td></tr><tr><td>SLUTQA(FT)</td><td>87.28</td><td>70.16</td></tr><tr><td rowspan="3">Qwen2 72b</td><td>Vanilla</td><td>68.49</td><td>51.40</td></tr><tr><td>SLUTQA (ICL)</td><td>92.06</td><td>75.65</td></tr><tr><td>SLUTQA(FT)</td><td>97.23</td><td>82.71</td></tr><tr><td rowspan="3">GLM49b</td><td>Vanilla</td><td>56.62</td><td>34.52</td></tr><tr><td>SLUTQA (ICL)</td><td>84.97</td><td>60.99</td></tr><tr><td>SLUTQA (FT)</td><td>89.90</td><td>66.81</td></tr></table></body></html>

For SQL-style answers, the primary challenge is accurately extracting key information from queries to generate the corresponding SQL statements. To address this, the SLA module for SQL formats leverages SLU labels and ICL to generate SQL statements. As with the markdown process, we select five examples from the training set for each query, all sharing the same intent. As illustrated in Figure 2 (top right, SFA SQL Examples), these examples consist of the query, intent(s), slots, retrieved table caption(s), and SQL statements. The final output is produced by executing the generated SQL statement.

# Experiments

In this section, we first introduce the experiment setup. Then, we show the experiment results and conduct ablation

studies.

# Experimental Settings and Baselines

Evaluation Metrics: In this paper, we provide answers to each query in three formats: Markdown, SQL-style, and natural language. However, since natural language output is subjective, we focus on evaluating the two objective formats, Markdown and SQL-style.

For SQL-style answers, we assess performance using Executable Code Ratio (ECR) and Pass Rate $( \mathrm { p a s s } @ 1 )$ , as outlined in (He et al. 2024). For Markdown answers, we evaluate using Table Exact Match accuracy (Table EM) and Precision (P), Recall (R), and F1 score for exact matches of rows, columns, and cells, following the metrics described in (Pal et al. 2023). Additionally, for table retrieval performance, we also employ Precision, Recall, and F1 score as evaluation metrics.

Baselines: To prove that our designed SLUTQA could enhance the performance of existing LLMs on the RETQA dataset, we utilize our framework on three GPT family models, i.e., Qwen2-7b, Qwen2-72b (Yang et al. 2024), and Glm4-9b (GLM et al. 2024).

# Result and Analysis

Main Results: To evaluate the performance of SLUTQA on RETQA, we generated markdown and SQL-style answers using baseline LLMs through two approaches: the vanilla implementation and our SLUTQA framework. In the vanilla implementation, we generated a query summary for BM25 retrieval and then produced the final answers using ICL with five randomly selected examples. The key difference is that the vanilla approach does not include SLU labels and lacks a simplification step for markdown formatting.

The overall performance of the markdown and SQL-style answers is presented in Tables 2 and 3, respectively. In these tables, “Vanilla” refers to the baseline implementation, “SLUTQA (ICL)” denotes the SLUTQA framework with the SR module implemented via in-context learning (ICL), and “SLUTQA (FT)” represents the SLUTQA framework with the SR module implemented by fine-tuning a BERT model. The results clearly demonstrate that our SLUTQA framework outperforms all baseline methods in both markdown and SQL formats, and further confirms that incorporating SLU labels can significantly enhance the performance of LLMs in open-domain TQA tasks.

Table 4: Table retrieval performance of different methods.   

<html><body><table><tr><td colspan="2">Method</td><td>P</td><td>R</td><td>F1</td></tr><tr><td>BM25</td><td>Top 1</td><td>76.86</td><td>56.02</td><td>64.80</td></tr><tr><td>Qwen2 7B</td><td>Vanilla SR(ICL) SR(FT)</td><td>93.87 95.80 97.53</td><td>89.09 92.32 95.05</td><td>91.42 94.03 96.27</td></tr><tr><td>Qwen2 72B</td><td>Vanilla SR(ICL) SR(FT)</td><td>97.23 97.66 97.85</td><td>96.30 97.35 97.57</td><td>96.76 97.50 97.71</td></tr><tr><td>GLM49B</td><td>Vanilla SR(ICL) SR(FT)</td><td>93.73 95.12 97.67</td><td>92.02 92.75 94.29</td><td>92.87 93.92 95.95</td></tr></table></body></html>

Table 5: Ablation study of SFA module in markdown format.   

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Method</td><td colspan="3">Cell EM(%)</td></tr><tr><td>P</td><td>R</td><td>F1</td></tr><tr><td rowspan="3">Qwen2 7b</td><td>Vanilla</td><td>37.67</td><td>43.58</td><td>40.41</td></tr><tr><td>Simplified</td><td>34.07</td><td>37.88</td><td>35.87</td></tr><tr><td>SFA</td><td>39.62</td><td>44.02</td><td>41.71</td></tr><tr><td rowspan="3">GLM4 9b</td><td>Vanilla</td><td>39.85</td><td>61.96</td><td>48.51</td></tr><tr><td>Simplified</td><td>42.25</td><td>57.75</td><td>48.80</td></tr><tr><td>SFA</td><td>51.89</td><td>62.87</td><td>56.85</td></tr><tr><td rowspan="3">Qwen2 72b</td><td>Vanilla</td><td>45.72</td><td>58.97</td><td>51.51</td></tr><tr><td>Simplified</td><td>62.35</td><td>69.20</td><td>65.59</td></tr><tr><td>SFA</td><td>66.36</td><td>70.27</td><td>68.26</td></tr></table></body></html>

Specifically, for markdown format answers, comparing SLUTQA (FT) with the vanilla implementation, Table EM accuracy improves by $3 . 8 9 \%$ , $4 . 3 5 \%$ , and $3 . 8 8 \%$ for Qwen2 7b, GLM4 9b, and Qwen2 72b, respectively. Even when comparing SLUTQA (ICL), which does not fine-tune with a large number of SLU labels, with the vanilla implementation, Table EM accuracy still improves by $1 . 6 \%$ , $4 . 4 2 \%$ , and $3 . 6 7 \%$ for Qwen2 7b, GLM4 9b, and Qwen2 72b, respectively. For SQL format answers, both SLUTQA (FT) and SLUTQA (ICL) outperform the vanilla implementation by more than $20 \%$ on both ECR and pass $\ @ 1$ scores.

These results indicate that: (1) Current LLMs are capable of generating accurate SLU labels with just few-shot (22 in practice) examples. (2) SLU labels facilitate more precise parsing of user intent and relevant information, significantly enhancing the overall performance of open-domain TQA tasks. Detailed results of the SLU tasks are provided in Appendix D. This also demonstrates that incorporating traditional SLU labels into open-domain TQA tasks is a valid and effective approach, offering valuable insights for future TQA research.

Ablation Study: We conducted an ablation study to evaluate the effectiveness of the SR and SFA modules in SLUTQA.

First, we evaluated the SR module’s impact on retrieval performance, as shown in Table 4. We compared several methods: BM25, which retrieves the top 1 table based on the original query; the vanilla approach, which generates query summaries replacing the original query for BM25 retrieval; and our SR module. In SR (ICL) and SR (FT), summaries are generated using our SR module, with SLU labels predicted by ICL or a fine-tuned (FT) BERT model. The results show that BM25 performs significantly worse than the other methods, highlighting the effectiveness of LLM-generated table summaries for TQA tasks. Additionally, both SR (ICL) and SR (FT) outperform the vanilla approach across all LLMs, demonstrating that SLU labels enhance retrieval accuracy.

<html><body><table><tr><td>Model</td><td>Method</td><td>ECR(%)</td><td>pass@1</td></tr><tr><td rowspan="2">Qwen2 7b</td><td>Vanilla</td><td>64.99</td><td>42.24</td></tr><tr><td>SFA</td><td>90.17</td><td>74.58</td></tr><tr><td rowspan="2">Qwen2 72b</td><td>Vanilla</td><td>71.76</td><td>53.70</td></tr><tr><td>SFA</td><td>98.86</td><td>86.26</td></tr><tr><td rowspan="2">GLM4 9b</td><td>Vanilla</td><td>60.01</td><td>35.25</td></tr><tr><td>SFA</td><td>90.36</td><td>69.65</td></tr></table></body></html>

Table 6: Ablation study of SFA module in SQL format.

Next, we evaluated the SFA module. To isolate its impact, we used the gold table(s) and ground-truth SLU labels, ensuring the SFA module’s performance is assessed independently of retrieval accuracy and SLU prediction.

The markdown results are shown in Table 5. Due to the space limit, we only show the performance of exact matches of cells. Here “Vanilla” refers to the vanilla implementation described in the Main Results section, where the LLM generates the final answer from the query and gold markdown table(s) using in-context learning with 5 randomly selected examples. “Simplified” is an ICL-based method similar to the SFA module but without SLU labels in the prompts. “SFA” represents our SFA module. The results demonstrate that the SFA module outperforms both Vanilla and Simplified across all LLMs, indicating that SLU labels help simplify tables and reduce noise, addressing the challenges of long tables. Notably, Vanilla outperforms Simplified on Qwen2 7B, suggesting that without SLU labels, direct table simplification through ICL can inadvertently remove important information, reducing performance.

The SQL results are shown in Table 6. Again, “Vanilla” refers to the vanilla implementation in the Main Results section, while “SFA” denotes our SFA module. The results show that the SFA module significantly outperforms Vanilla across all LLMs, indicating that SLU labels help LLMs capture key information and generate more accurate SQL statements.

# Conclusion

In this paper, we introduce RETQA, the first large-scale open-domain Chinese TQA dataset for the real estate sector, comprising 4,932 tables and 20,762 QA pairs. RETQA addresses the lack of specialized datasets by providing a challenging resource for TQA research, particularly in handling open-domain, long tables and multi-domain queries. To enhance performance, we developed the SLUTQA framework, which leverages LLMs and SLU labels for improved retrieval and answer generation. Extensive experiments show that SLUTQA significantly boosts LLM performance, offering valuable insights and resources for future TQA research.