# DOGR: Leveraging Document-Oriented Contrastive Learning in Generative Retrieval

Penghao Lu, Xin Dong\*, Yuansheng Zhou, Lei Cheng, Chuan Yuan, Linjian Mo

Ant Group {lupenghao.lph,zhaoxin.dx,zhouyuansheng.zys,lei.chenglei,yuanzheng.xy,linyi01}@antgroup.com

# Abstract

Generative retrieval constitutes an innovative approach in information retrieval, leveraging generative language models (LM) to generate a ranked list of document identifiers (docid) for a given query. It simplifies the retrieval pipeline by replacing the large external index with model parameters. However, existing works merely learned the relationship between queries and document identifiers, which is unable to directly represent the relevance between queries and documents. To address the above problem, we propose a novel and general generative retrieval framework, namely Leveraging Document-Oriented Contrastive Learning in Generative Retrieval (DOGR), which leverages contrastive learning to improve generative retrieval tasks. It adopts a two-stage learning strategy that captures the relationship between queries and documents comprehensively through direct interactions. Furthermore, negative sampling methods and corresponding contrastive learning objectives are implemented to enhance the learning of semantic representations, thereby promoting a thorough comprehension of the relationship between queries and documents. Experimental results demonstrate that DOGR achieves state-of-the-art performance compared to existing generative retrieval methods on two public benchmark datasets. Further experiments have shown that our framework is generally effective for common identifier construction techniques.

# Introduction

Information retrieval aims to satisfy user queries by retrieving documents. With the rapid increase in the volume of information, the importance of retrieval technology is becoming gradually prominent. In recent years, a novel retrieval method known as generative retrieval has emerged. Unlike traditional retrieval methods (sparse retrieval and dense retrieval), generative retrieval assigns an identifier to each document and generates relevant document identifiers end-toend through language models. In addition, generative retrieval can better utilize the capabilities of the large language models(LLMs), especially the knowledge learned during the pre-training phase.

Most generative retrieval methods (Cao et al. 2021; Tay et al. 2022; Wang et al. 2022a) first define an identifier for each document and then treated document retrieval as a standard sequence-to-sequence task. Specifically, this task learns the generation of identifiers through the interaction between queries and identifiers in an end-to-end manner. However, the identifier length is often much smaller than documents, identifiers may experience a certain degree of information loss compared to the complete document. Existing generative retrieval methods mainly concentrate on learning the relationship between the queries and the identifiers, thus they are unable to capture the complete semantic relationships between queries and documents.

There are some studies introducing additional optimization objectives to comprehensively characterize the relationship between queries and documents. A previous approach (Li et al. 2024) proposed a document-oriented margin-based loss based on generation probability to optimize the document ranking. Another work (Lee et al. 2023) modeled query-document relevance by separately learning the generation probabilities of the query and document to the target identifier. However, these work only focused on the intermediate target of learning identifiers and lack the interaction between query and documents. This could potentially affect the retrieval performance. Since the model can only learn the nuanced interactions through the identifier, it is insufficient to model the relevance between queries and documents by solely relying on the generation probability of identifiers.

To address the above problems, we introduce a novel and general framework, namely Leveraging Document-Oriented Contrastive Learning in Generative Retrieval (DOGR), which captures the relationship between queries and documents comprehensively through their direct interactions. DOGR proposed a two-stage learning strategy consisting of an identifier generation stage and a document ranking stage. First, in the identifier generation stage, we train an encoderdecoder architecture model based on the generation loss to learn the connections between the document identifier and the query. Next, in the document ranking stage, we fine-tune the model with contrastive learning objectives to directly learn the correlation between queries and documents. We enhance document representation through the elaborate design of two negative sampling methods. Specifically, DOGR explicitly leverages prefix-oriented negative sampling to learn semantic representations of documents with the same identifier prefix and retrieval-augmented negative sampling to comprehend the representation of documents with distinct identifier prefixes. Drawing on the above mentioned negative samples, two corresponding contrastive losses are utilized to enhance the semantic similarity between query and its positive document, aiming to achieve analogous representations for both by increasing direct interaction. During the inference phase, we not only consider the generation probability of document identifiers, but also leverage the semantic score to jointly model the relevance between the query and documents. By means of ranking the documents corresponding to the generated identifiers, the performance of generative retrieval is further enhanced.

In summary, our main contributions are as follows:

• We propose DOGR, a novel and general framework which adopts contrastive learning to enhance generative retrieval. It incorporates a two-stage learning strategy that jointly models the relevance of query-document by combining the generation probability of identifiers with document semantics. • According to the characteristics of generative retrieval, we introduced two negative sampling methods and corresponding contrastive learning objectives to help the model fully capture the correlation between query and documents through direct interaction. • Comprehensive experiments show that DOGR outperforms existing state-of-the-art generative retrieval methods on two benchmark datasets. Further experiments have demonstrated that our method is effective for several common identifier construction approaches.

# Related Work

# Document Retrieval

Traditional document retrieval methods mainly include sparse retrieval and dense retrieval. Sparse retrieval (classic methods such as BM25 (Robertson, Zaragoza et al. 2009) and TF-IDF (Robertson and Walker 1997)) has been widely used in industry due to its high efficiency. However, sparse retrieval relies on lexical overlap and finds it challenging to match semantically related but literally different documents. Dense retrieval involves representing queries and documents as dense vectors(Lee, Chang, and Toutanova 2019), and calculating the similarity between the query and the document using inner product or cosine distance. Benefiting from the continuous development of pre-trained language models, the performance of dense retrieval is also constantly improving. Contrastive learning(Khosla et al. 2020; Gao, Yao, and Chen 2021) and hard negative sampling techniques (Xiong et al. 2021; Karpukhin et al. 2020; Ni et al. 2021, 2022) have become another important driving force in the development of dense retrieval. Therefore, it has gradually become another mainstream retrieval method in industrial implementation. Although dense retrieval has achieved great success, it suffers from issues such as embedding space bottleneck and a lack of fine-grained interactions between embeddings.

# Generative Retrieval

Different from traditional document retrieval methods, generative retrieval uses only the language model to directly generate the identifiers of relevant documents for a given query. GENRE (Cao et al. 2021) is the first step in performing information retrieval tasks using an autoregressive language model. It implemented constrained beam search based on a prefix tree to generate entity titles within a given candidate set. Since document identifiers largely determines the effectiveness of generative retrieval, many studies have explored the construction methods of identifiers. In general, identifiers can be categorized into lexical and numerical types. Lexical identifiers allow for explicit expression including n-grams(Bevilacqua et al. 2022; Wang et al. 2023), titles(Cao et al. 2021; Li et al. 2023), etc, while numerical identifiers(Tay et al. 2022; Wang et al. 2022a,b) mainly capture similarities and differences among documents through clustering methods. However, these works focused on the learning process of mapping queries to identifiers, which cannot directly represent the relevance between queries and documents.

Recently, there have been studies that introduce additional optimization objectives to help characterize the relationship between queries and documents. LTRGR (Li et al. 2024) proposed a margin-based ranking loss based on generation probability from the document level to improve document ranking. GLEN (Lee et al. 2023) utilized identifiers as a bridge to model query-document relevance by separately learning the generation probabilities of both the query and the document in relation to the target identifier. Although they open up new ideas for generative retrieval, they still only utilized the generation probability of document identifiers, which has a lack of direct interaction between the query and the document. To address the aforementioned issues, we propose a two-stage learning strategy to learn the generation probabilities at the identifier level and the semantic representation at the document level. Through direct interaction between queries and documents, we enhance generative retrieval through contrastive learning objectives based on negative sampling methods.

# The Proposed Scheme

In this work, we propose a two-stage learning strategy that enhances query representations to better align with document representations for generative retrieval. Figure 1 shows an illustration of the core procedure of DOGR, including an identifier-level generation stage and a document-level ranking stage. First, in the Identifier Generation Stage, DOGR trains an encoder-decoder architecture model to learn the connections between the keyword-based document identifier with query.

Then, in the Document Ranking Stage, DOGR fine-tunes the trained model by introducing contrastive learning to depict the relationship between the query and the complete document. We improve document representation by elaborately designing two negative sampling methods. In particular, we use prefix-oriented negative sampling to capture the semantic representations of documents sharing the same identifier prefix. On the other hand, we propose retrievalaugmented negative sampling to better understand the representation of documents with different identifier prefixes.

Two-stage Learning Doc-ID Generation Loss Query-ID Generation Loss Auxiliary Generation Loss Prefix-oriented Contrastive Loss Retrieval-aug Contrastive Loss Query Rep. Pos Doc Rep. PNrefgixD-orcieRnetep.d NRetgriDevoacl-Raeupg. gymnasts-olympic Identifier Generation Semantic Representation Stage 1 Keyword Stage 1 GR Model GR Model Extractor GR Model ↑ gymnasts-olympic gymnast-olympian olympics-skiing Prefix-oriented Retrieval-augmented Document Query Negative Docs Negative Docs LUinsitteofd OStlaytemspiGcyfemnasltei cgsyemvneansttsshfaovrethe gwyhmonwasotnictshegowldomendsal Query DPocsuitmivent Prefix-oriented Retrieval-augmented been staged at the Olympic Games Negative Sampling Negative Sampling since Stage 1 Identifier Generation Stage GR Model Document Ranking Stage Inference Generated DocIDs & Corresponding Probabilities Semantic Score Relevance Score Rank 1 olympic-gymnasts doc_108101 0.75 doc_109303 0.6642 Query GR Model 0.85 doc_67652 0.70 doc_108101 0.6375 gymnasts-olympic doc_109303 Rank 3 0.82 Ranking doc_21876 0.5913 who is the best olympic 0.81 doc_21876 0.73 doc_67652 0.5670 gymnast of all time? gymnast-olympian doc_103098 0.68 doc_103098 0.5304 0.78 Query representation Doc representation Candidate docs GR Model

Informed by the aforementioned negative samples, two related contrastive losses are applied to maximize the semantic score between the query and the positive document. This approach strives to attain comparable representations for both.

In the inference phase, both the generation probability of document identifiers and the semantic score are taking into account to collaboratively model the relevance between query and documents. Through ranking the documents corresponding to the generated identifiers, the performance of generative retrieval is further enhanced.

# Identifier Generation Stage

Keyword-Based Lexical Identifier. Existing generative retrieval methods require constructing identifier for each document. Since keywords are important carriers of document information, we extract keywords as the document identifier. Specifically, for each document $d$ , pre-trained KeyBert (Grootendorst 2020) is used to extract the top keywords and sort the keywords in descending order of importance. After encoding the keyword sequence with the tokenizer, the first $n$ tokens are selected as the document identifier $z ^ { d } = \{ z _ { 0 } ^ { d } , z _ { 1 } ^ { d } , \cdot \cdot \cdot , z _ { n - 1 } ^ { d } \}$ . As a result, each document $d$ has its corresponding identifier $z ^ { d }$ . While a document identifier $z$ can correspond to a collection of documents $\{ d _ { i } | z ^ { d _ { i } } = z \}$ , since similar documents may have the same identifier. In addition to keyword-based identifier, our framework is applicable to common identifier types including both lexical and numerical type, and its generalization will be validated in the experiment section.

Pointwise Generation Task. Similar to the previous identifier generation tasks, the training objective in this stage is a standard sequence-to-sequence task. This task aims at learning the relationship between query and document identifiers.

For each sample $( q , d )$ in training data, we adopt sequence-to-sequence cross-entropy loss with teacher forcing to maximize the log-likelihoods of $q  z ^ { d }$ . It is far from sufficient to simply model the generation task of query to target document identifier. This is because the query cannot represent the entirety of the document and it cannot assist the model in fully learning the relationship between the document and the target identifier, which may lead to poor robustness. Thus we utilize the same loss function to maximize the loglikelihoods of $d \to z ^ { d }$ . Different from the query input, document segments are randomly sampled as input to learn the relationship between document and its identifier more comprehensively. It was observed that training the document and query tasks together in a multi-task setting leads to better performance in generating identifiers than training these tasks separately. Accordingly, the generation loss is formulated as follows,

$$
L _ { 1 } = - \sum _ { t = 1 } ^ { n } \log P ( z _ { t } ^ { d } | \boldsymbol { q } , z _ { < t } ^ { d } ) - \sum _ { t = 1 } ^ { n } \log P ( z _ { t } ^ { d } | \boldsymbol { d } , z _ { < t } ^ { d } ) ,
$$

where $z _ { t } ^ { d }$ is the token at $t$ -th step of $z ^ { d }$ .

# Document Ranking Stage

Since the identifier generation task mainly focuses on learning the relationship between queries and identifiers, which has the limited ability to comprehensively understand the relationship between queries and documents. Therefore, the purpose of the document ranking stage is to learn the semantic representation of document through its direct interaction with the query. In light of the characteristics of generative retrieval, two negative sampling strategies and corresponding contrastive learning objectives are implemented to optimize document representation.

Prefix-oriented Negative Sampling. We introduce contrastive loss to help the model to represent queries as close to positive documents $d ^ { + }$ and far from negative documents $d ^ { - }$ . Then, we propose prefix-oriented negative sampling to sample negative documents with the same identifier prefix within the batch. Specifically, we first select documents with the same first $n$ tokens as $z ^ { d ^ { + } }$ (excluding $d ^ { + }$ itself). Next, documents that have the same first $n - 1$ tokens as $d ^ { + }$ are fetched. We repeatedly reduce the prefix length until $| \mathcal { N } _ { z } |$ negative samples are obtained. Since identifiers with the same prefix tend to have similar document information, this negative sampling method can help the model better distinguish difficult samples. Based on the semantic representation, we define the semantic score between the query and document as follows,

$$
s ( q , d ) = h _ { q } \cdot h _ { d } ^ { T } ,
$$

where $h _ { x } \in \mathbb { R } ^ { m }$ is the average pooling of the hidden representations at each position of the encoder for a given input $x$ . According to the relationship between the positive and negative samples mentioned above, the prefix-oriented contrastive learning loss function can be formulated:

$$
L _ { c _ { i } } = - \log \frac { \exp ( s ( q , d ^ { + } ) / \tau ) } { \exp ( s ( q , d ^ { + } ) / \tau ) + \sum _ { d ^ { - } \in \mathcal { N } _ { z } } \exp ( s ( q , d ^ { - } ) / \tau ) } ,
$$

where $\tau$ is the temperature and $\mathcal { N } _ { z }$ is the set of negative documents obtained via prefix-oriented negative sampling.

Retrieval-augmented Negative Sampling. Then, we introduce retrieval-augmented negative sampling to sample negative documents with distinct identifier prefixes. To be specific, it samples negative documents by generating highranking identifiers for the given query. For each query $q$ during training, the top $k$ document identifiers $\{ z _ { 1 , } , \cdot \cdot \cdot , \bar { z } _ { k } \}$ are retrieved based on the model trained in the first stage. The document collection corresponding to these identifiers is $\cup _ { j = 1 } ^ { k } \{ d _ { i } | z ^ { d _ { i } } \ = \ z _ { j } \}$ . In these documents, the docum nts that have the same identifier as the positive document are removed, as they have already been sampled in the prefix-oriented negative sampling. Then the negative documents $\mathcal { N } _ { q }$ are sampled from the remain documents, i.e. $\cup _ { j = 1 } ^ { k } \{ d _ { i } | \hat { z } ^ { d _ { i } } = z _ { j } \} / \{ d _ { i } | z ^ { d ^ { + } } = z _ { j } \}$ , where $| { \mathcal N } _ { q } | <$ $k$ . Thanks to the negative samples provided by the first stage learning, a retrieval-augmented contrastive learning loss function is formulated as follows,

$$
L _ { c _ { q } } = - \log \frac { \exp ( s ( q , d ^ { + } ) / \tau ) } { \exp ( s ( q , d ^ { + } ) / \tau ) + \sum _ { d ^ { - } \in \mathcal { N } _ { q } } \exp ( s ( q , d ^ { - } ) / \tau ) } .
$$

Training. We also incorporate the generation loss into the document ranking stage. It ensures that the model’s ability to generate document identifiers does not deteriorate due to the introduction of ranking learning objectives. Moreover, we use an additional loss to learn the corresponding document identifiers for the negative documents obtained by the retrieval-augmented negative sampling. We add this loss to the generation loss $L _ { 1 }$ as the auxiliary generation loss $L _ { g }$ , which is calculated as follows,

$$
L _ { g } = L _ { 1 } + \big ( \sum _ { d ^ { - } \in \mathcal { N } _ { q } } \sum _ { t = 1 } ^ { n } - \log P ( z _ { t } ^ { d ^ { - } } | d ^ { - } , z _ { < t } ^ { d ^ { - } } ) \big ) .
$$

Therefore, the final loss function is defined as,

$$
\begin{array} { r } { L = L _ { c _ { i } } + L _ { c _ { q } } + \lambda _ { g } \cdot L _ { g } , } \end{array}
$$

where $\lambda _ { g }$ is the hyperparameter used to control the importance of generation loss, which balances the generation objective and contrastive learning objectives.

# Inference

During the inference phase, we integrate the identifier generation probabilities with the semantic representation to retrieve and rank documents.

First, for a given query $q$ , we apply constrained beam search (Cao et al. 2021) to generate a ranked list of top $k$ document identifiers $\{ z _ { 1 } , z _ { 2 } , \cdot \cdot \cdot , z _ { k } \}$ with the corresponding generation probabilities $P ( z _ { 1 } | q ) , \dot { P } ( z _ { 2 } | q ) , \cdot \cdot \cdot , P ( z _ { k } | q )$ . Next, we calculate the query representation $h _ { q }$ and document representations $\{ h _ { d } | d \in \mathcal { D } _ { q } \}$ , where $\mathcal { D } _ { q }$ is the corresponding document collection to the top $k$ document identifiers. Based on the generation probability of the identifier and overall semantic information, the fusion relevance score between query and document is defined as follows:

$$
r e l ( q , d ) = P ( z ^ { d } | q ) \cdot s ( q , d ) .
$$

Finally, we rank the obtained document collection $\mathcal { D } _ { q }$ based on their relevance scores and the top $k$ documents with the highest relevance scores are selected as the final retrieval results for the given query. Meanwhile, the document ranking issue under identifier conflicts can be simply addressed.

# Experiments

# Datasets

Natural Questions(NQ320k) (Kwiatkowski et al. 2019) contains $3 2 0 \mathrm { k }$ training data (relevant query-document pairs),

Table 1: Performance comparison for the proposed framework and baseline methods for $\mathrm { N Q } 3 2 0 \mathrm { k }$ . Bolded numbers are the best performance of each column and the second best method is underlined. The number in parentheses indicates the number of queries. We refer to the results of baselines reported by (Lee et al. 2023). All the numbers in the table are percentage numbers with ${ } ^ { 6 } \%$ ‘ omitted. Results not available are denoted as $\ ' _ { - } ,$   

<html><body><table><tr><td></td><td colspan="3">NQ320K(7830)</td><td colspan="2">Seen test(6075)</td><td colspan="3">Unseen test(1755)</td></tr><tr><td>Model</td><td colspan="6">R@1R@10 MRR@100R@1R@10 MRR@100R@1R@10 MRR@100</td></tr><tr><td>Sparse &Dense Retrieval</td><td></td><td></td><td>29.159.8</td><td></td><td>39.5</td><td>61.9</td><td>42.7</td></tr><tr><td>BM25(Robertson, Zaragoza et al. 2009) DocT5Query(Nogueira,Lin,and Epistemic 2019)</td><td>29.7 60.3 38.0 69.3</td><td>40.2 48.9</td><td>35.1 50.2</td><td>68.3 78.7</td><td>46.7</td><td>32.3 48.572.9 50.0</td><td>57.0</td></tr><tr><td>DPR(Karpukhin et al. 2020) ANCE(Xiong et al. 2021)</td><td>50.2 77.7 50.2 78.5</td><td>59.9 60.2</td><td>49.7</td><td>79.2</td><td>60.2 60.1</td><td>74.2 52.0 75.9</td><td>58.8 60.5</td></tr><tr><td>SentenceT5(Ni etal. 2021)</td><td>53.6 83.0</td><td>64.1</td><td>53.4</td><td>83.9</td><td>63.8</td><td>56.5 79.5</td><td>64.9</td></tr><tr><td>GTR-base(Ni et al. 2022)</td><td>56.0 84.4</td><td>66.2</td><td>54.4</td><td>84.7</td><td>65.3</td><td>61.9 83.2</td><td>69.6</td></tr><tr><td>Generative Retrieval</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GENRE(Cao et al. 2021)</td><td>55.2 67.3</td><td>59.9</td><td>69.5</td><td>83.7</td><td>75.0</td><td>6.0 10.4</td><td>7.8</td></tr><tr><td>DSI(Tay et al. 2022)</td><td>55.2 67.4</td><td>59.6</td><td>69.7</td><td>83.6</td><td>74.7</td><td>1.3 7.2</td><td>3.5</td></tr><tr><td> SEAL(Bevilacqua et al. 2022)</td><td>59.9 81.2</td><td>67.7</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DSI-QG(Zhuang et al. 2022)</td><td>63.180.7</td><td>69.5</td><td>68.0</td><td>85.0</td><td>39.5</td><td>32.3 61.9</td><td>42.7</td></tr><tr><td>NCI(Wang et al. 2022a)</td><td>66.4 85.7</td><td>73.6</td><td>69.8</td><td>88.5</td><td>76.8</td><td>54.5 75.9</td><td>62.4</td></tr><tr><td>GENRET(Wang et al. 2022b)</td><td>68.1 88.8</td><td>75.9</td><td>70.2 70.2</td><td>90.3</td><td>77.7</td><td>62.5 83.6</td><td>70.4</td></tr><tr><td>LTRGR(Li et al. 2024)</td><td>67.5 86.2</td><td>74.8</td><td>72.5</td><td>88.7</td><td>77.3</td><td>58.1 77.3</td><td>66.2</td></tr><tr><td>GLEN(Lee et al. 2023)</td><td>69.1 86.0</td><td>75.4</td><td></td><td>88.9</td><td>78.5</td><td>57.6 75.9</td><td>63.9</td></tr><tr><td>DOGR(Ours)</td><td>70.2 89.1</td><td>76.8</td><td>72.6</td><td>90.5</td><td>78.9</td><td>61.9 84.3</td><td>69.5</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

100k documents, and 7,830 test queries, and is widely used in existing generative retrieval methods. We follow the same setup as previous work (Lee et al. 2023) and split the test set into two subsets: seen test and unseen test. The seen test consists of queries with target documents that are covered in the training data, while the documents in the unseen test are not covered.

MS MARCO passage ranking (MS MARCO) (Nguyen et al. 2016) is a large-scale benchmark dataset that includes 8.8 million passages collected from Bing search results and 1 million real-world queries, with the test set containing 6980 queries.

# Metrics

According to the evaluation methods of previous works, we adopt the widely accepted metrics, namely Recall $\ @ 1$ , Recall $@ 1 0$ , and Mean Reciprocal Rank (MRR), to comprehensively assess retrieval performance.

# Baselines

We compare with several types of baseline models, including traditional document retrieval methods and generative retrieval models.

Traditional Document Retrieval Methods In traditional document retrieval methods, we consider several commonly used highlighting models, covering both sparse retrieval (BM25 (Robertson, Zaragoza et al. 2009), DocT5Query (Nogueira, Lin, and Epistemic 2019)) and dense retrieval techniques (ANCE (Xiong et al. 2021), DPR (Karpukhin et al. 2020), SentenceT5 (Ni et al. 2021), GTR-base (Ni et al. 2022)).

Generative Retrieval Methods Furthermore, we compare DOGR with several generative retrieval baselines including GENRE (Cao et al. 2021), DSI (Tay et al. 2022), SEAL (Bevilacqua et al. 2022), DSI-QG (Zhuang et al. 2022), NCI (Wang et al. 2022a), GENRET (Wang et al. 2022b) , and two methods (GLEN (Lee et al. 2023) and LTRGR (Li et al. 2024)) introducing additional objectives to model query-document relationship. For the sake of experimental fairness, we replaced the backbone of LTRGR from BART (Lewis et al. 2020) to T5-base (Raffel et al. 2020) for training.

# Implementation Details

Keyword-based Lexical Identifier For each document, we first extract the top 20 diverse keywords based on KeyBert using maximal marginal relevance, while the diversity parameter is set to 0.5 and n-gram is set to 1. After experimenting with different identifier token length, we set the token length of document identifiers to 8 and 12 for NQ320k and MS MARCO respectively. We will analyze in detail the impact of identifier length on retrieval performance below.

Query Generation It has been proved that augumented queries related to the document can effectively help the model capture the relationship between query and documents. Therefore, we follow the previous work (Wang et al. 2022a) and augment the query through Document as Query and DocT5Query.

Training and Inference To ensure a fair comparison with previous work, we use T5-base (Raffel et al. 2020) as the generative language model backbone. In the training phase, batch size is set to 256 and 32, and the model is optimized for up to 3M and 1M steps using the Adam optimizer with learning rates 5e-5 for identifier generation stage and document-level ranking stage, respectively. The number of negatives from retrieval-augmented negative sampling is set to 4 per query, while the prefix-oriented negative sampling employs in-batch negatives. In the document ranking stage, $\tau$ is set to 0.5 as the temperature parameter for contrastive learning, and $\lambda _ { g }$ is set to 0.1 to balance the generative task and the contrastive learning task. In the inference phase, we use the beam search with constrained decoding during inference and set the beam size to 100. Our experimental code is implemented on Python 3.8 using transformers 4.37.0, while experiments are conducted on 6 NVIDIA A100 GPUs with 80 GB of memory.

Table 2: Performance comparison for the proposed framework and baseline methods for MS MARCO. Bolded numbers are the best performance of each column and the second best method is underlined. All the numbers are percentage numbers with ${ } ^ { 6 0 } \%$ ‘ omitted.   

<html><body><table><tr><td>Model</td><td>MS MARCO Dev(6980) MRR@10</td></tr><tr><td>BM25 DocT5Query GTR-base(Ni et al. 2022)</td><td>18.4 27.2 34.8</td></tr><tr><td>DSI(Tay et al. 2022) DSI-QG(Zhuang et al. 2022) NCI(Wang et al. 2022a)</td><td>3.1 11.8 17.4</td></tr><tr><td>LTRGR(Li et al. 2024) GLEN(Lee et al. 2023) DOGR</td><td>21.6 20.1 22.5</td></tr></table></body></html>

# Experimental Results

# Main Results

Evaluation on NQ320k. Table 1 shows the retrieval performance on NQ320k. The main observations are as follows:

• DOGR surpasses traditional document retrieval methods and outperforms existing generative retrieval methods. Specifically, DOGR outperforms the best competitive generative retrieval model on the full test set of $\mathrm { N Q } 3 2 0 \mathrm { k }$ by $+ 1 . 6 \%$ , $+ 0 . 3 \%$ , and $+ 1 . 2 \%$ on Recall $\ @ 1$ , Recall $@ 1 0$ and MRR $@ 1 0 0$ , respectively. • DOGR not only outperforms existing baseline models on the full and seen test sets, but also achieves competitive performance with the previous best model on the unseen test set, which shows that our method has good robustness. • Our method exhibits higher performance than other methods adopting additional objectives to model querydocument relationship in generative retrieval. This demonstrates the effectiveness of enhancing generative retrieval via document-oriented contrastive learning.

Evaluation on MS MARCO. Table 2 presents the retrieval performance on MS MARCO. DOGR improves MRR $@ 1 0$ by $4 . 2 \%$ compared to the best competitive generative retrieval method and by $2 2 . 3 \%$ compared to BM25(Robertson, Zaragoza et al. 2009). Existing generative retrieval methods still struggle to memory knowledge of the document corpus solely through identifiers, making it difficult to operate effectively in large-scale corpora. In contrast, DOGR enhances generative retrieval through contrastive learning and document semantics, thus it is successfully performed in largescale corpora.

Table 3: Retrieval performance of different methods on various identifier types. All the numbers are percentage numbers with ${ } ^ { 6 } \%$ ‘ omitted. Models with only generation task are denoted as $\cdot _ { - } ,$   

<html><body><table><tr><td rowspan="2">DocID Type</td><td colspan="3">NQ320K(7830)</td></tr><tr><td>Enhance Type</td><td>R@1 R@10</td><td>MRR@100</td></tr><tr><td rowspan="4">keyword</td><td>DOGR</td><td>70.2 89.1</td><td>76.8</td></tr><tr><td>LTRGR</td><td>69.2 88.3</td><td>76.3</td></tr><tr><td>GLEN</td><td>68.7 88.0</td><td>75.9</td></tr><tr><td>-</td><td>68.2 87.7</td><td>75.5</td></tr><tr><td rowspan="4">n-gram</td><td>DOGR</td><td>67.2 86.9</td><td>74.3</td></tr><tr><td>LTRGR</td><td>66.2 86.2</td><td>73.5</td></tr><tr><td>GLEN</td><td>65.4 85.8</td><td>72.8</td></tr><tr><td>-</td><td>65.2 85.4</td><td>72.8</td></tr><tr><td rowspan="4">first n-tokens</td><td>DOGR</td><td>66.3 86.7</td><td>73.9</td></tr><tr><td>LTRGR</td><td>65.4 85.7</td><td>73.0</td></tr><tr><td>GLEN</td><td>64.5 85.1</td><td>72.3</td></tr><tr><td>-</td><td>64.3 84.8</td><td>71.8</td></tr><tr><td rowspan="4">numerical</td><td>DOGR</td><td>67.8 86.8</td><td>74.9</td></tr><tr><td>LTRGR</td><td>67.1 86.1</td><td>74.3</td></tr><tr><td>GLEN</td><td>66.2 85.9</td><td>73.5</td></tr><tr><td>1</td><td>66.4 85.7</td><td>73.6</td></tr></table></body></html>

# In-depth Analysis

Generalization of DOGR. We verify the generalization of DOGR, i.e. whether it can provide a certain improvement for common identifier construction methods including both lexical and numerical type in generative retrieval. Table 3 presents the improvements of DOGR for common identifiers on $\mathrm { N Q } 3 2 0 \mathrm { k }$ , along with a comparison to two other competitive methods. We find that DOGR significantly improves the performance of lexical and numerical identifiers. For keyword DocID, we improve Recall $\ @ 1$ from 68.2 to 70.2 (a relative increase of $2 . 9 \%$ ). Meanwhile, for other DocID types, DOGR achieves at least a $2 . 1 \%$ , $1 . 2 \%$ and $1 . 7 \%$ enhancement in Recall $\ @ 1$ , Recall $@ 1 0$ , and $\mathbf { M M R } @ 1 0 0$ respectively, and demonstrates the generalization of the framework proposed. Additionally, DOGR outperforms two other methods that introduce additional objectives to capture querydocument correlation in generative retrieval, achieving at least a $1 . 0 \%$ improvement in $\mathbb { R } \ @ 1$ . This may be attributed to integrating the generation probability of identifiers with semantic representations, allowing for a more comprehensive modeling of the query-document relevance.

Impact of Identifier length. Since our framework allows different documents to share the same identifier, it may lead to identifier conflicts. Table 4 shows the impact of identifier length on performance of NQ320k, where the conflict rate is defined as the ratio of the number of documents with identifier conflicts to the total number of documents. We find that when the identifier length is short, there are significant identifier conflict issues, which reduces the learning ability of the identifier generation task while increasing the ranking difficulty at the document level. Additionally, the shorter the identifier, the more limited its ability to convey information about the document. These two reasons may lead to a decline in model performance when the identifiers is short. Specifically, when the identifier length decreases from 8 to 3, Recall $@ 1$ drops by $5 . 0 \%$ . On the other hand, as the length of the identifiers increases, the issue of identifier conflicts gradually diminishes, and the performance improves. Once the length reaches 8, the Recall $\ @ 1$ does not improve further. One possible reason is that the model needs to memorize the order of keywords as the length increases, this may lead to a decrease in robustness. Moreover, the training and inference speeds also decrease with increasing identifier length.

Table 4: Impact of Identifier length on $\mathrm { N Q } 3 2 0 \mathrm { k }$ . All the numbers are percentage numbers with ${ } ^ { 6 } \%$ ‘ omitted.   

<html><body><table><tr><td>Identifier length</td><td>Conflict Rate</td><td>NQ320K(7830) R@1 R@10 MRR@100</td></tr><tr><td>=3</td><td>46.9</td><td>66.7 87.2 75.3</td></tr><tr><td>l=4</td><td>28.6</td><td>67.9 88.0 75.7</td></tr><tr><td>l=6</td><td>8.9</td><td>68.9 88.7 76.2</td></tr><tr><td>l=8</td><td>3.0</td><td>70.2 89.1 76.8</td></tr><tr><td>l=10</td><td>1.1</td><td>69.8 88.6 75.8 75.5</td></tr><tr><td>l=12</td><td>0.4</td><td>69.6 88.3</td></tr></table></body></html>

Ablation Study. We also conducted detailed experiments to investigate the roles of different strategies in the training and inference phase:

• “w/o doc segment sampling”: The document segment sampling is removed in identifier generation stage.   
• “w/o prefix-ori contrastive / retrieval-aug contrastive / auxiliary generation loss”: The prefix-oriented contrastive / retrieval-augmented contrastive / auxiliary generation loss is removed in document ranking stage, respectively.   
• “w/o semantic score”: Semantic score is removed during the inference phase. This means that ranking is performed only based on generation probability, and in particular, random ranking is used when encountering identifier conflicts.   
• “w/o generation probability”: The generation probability is removed during the inference phase. This means that after generating identifiers, the corresponding documents are ranked only based on the semantic score.   
• “w/o fusion rank”: We adopt the same strategy as (Lee et al. 2023) instead of ranking by the fusion relevance score, i.e. overall ranking by generation probability and only rank by semantic score when encountering identifier conflicts.

The effectiveness of various strategies in DOGR on $\mathrm { N Q } 3 2 0 \mathrm { k }$ is demonstrated in Table 5, which provides the following insights:

• Sampling of document segments can help the model capture the relationship between documents and identifiers more comprehensively, enhancing robustness and yielding a $2 . 3 \%$ improvement in Recall $\ @ 1$ .

Table 5: Ablation study of DOGR with different strategy on NQ320k. All the numbers are percentage numbers with ${ } ^ { 6 0 }$ ‘ omitted.   

<html><body><table><tr><td>Model</td><td>NQ320K(7830) R@1R@10 MRR@100</td></tr><tr><td>DOGR</td><td>70.2 89.1 76.8</td></tr><tr><td>w/o doc segment sampling</td><td>68.6 88.2 75.9</td></tr><tr><td>W/o prefix-ori contrastive loss</td><td>69.0 88.4 76.0</td></tr><tr><td>w/o retrieval-aug contrastive loss w/o auxiliary generation loss</td><td>69.3 88.5 76.2 65.8 84.3 72.9</td></tr><tr><td>w/o semantic score</td><td>68.2 87.7 75.5</td></tr><tr><td>w/o generation probability</td><td>63.3 87.6 72.0</td></tr><tr><td>w/o fusion rank</td><td>69.9 88.8 76.5</td></tr></table></body></html>

• Two contrastive learning tasks extract negative samples from different perspectives, resulting in gains of $1 . 7 \%$ and $1 . 3 \%$ in Recall $\ @ 1$ , respectively. This indicates that contrastive learning can effectively enhance generative retrieval by document ranking. Additionally, we found that without the assistance of generative loss, the generative task experiences a decline, with Recall $\ @ 1$ dropping by $6 . 3 \%$ , highlighting the necessity of generative loss.

• During the inference phase, relying solely on the generation probability cannot address identifier collision issues, which results in a $2 . 8 \%$ drop in Recall $\ @ 1$ . On the other hand, discarding the generation probability in ranking causes a $9 . 8 \%$ drop in Recall $\ @ 1$ , which is mainly due to the lack of interaction between query and documents. Moreover, integrating both the generation probability and semantic information yields a further $0 . 4 \%$ improvement in Recall $\ @ 1$ compared to ranking solely based on semantic information under identifier collisions.

# Conclusion

In this paper, we propose DOGR, a novel and general framework that enhancing generative retrieval via documentoriented contrastive learning. It employs a two-stage learning strategy that simultaneously models the relevance between the query and documents by integrating the generation probability of identifiers with document semantic representation. Moreover, two negative sampling methods and contrastive learning objectives are employed to comprehensively learn the relationship between the query and the document through their direct interaction. Experimental results demonstrate that DOGR achieves state-of-the-art among generative retrieval methods on two benchmark datasets and brings improvements on common identifier construction methods. In the future, we will continue to explore how to end-to-end integrate information from documents to enhance the retrieval capabilities of generative retrieval in large-scale corpora.