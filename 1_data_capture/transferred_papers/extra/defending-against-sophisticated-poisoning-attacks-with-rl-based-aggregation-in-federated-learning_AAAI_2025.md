# Defending Against Sophisticated Poisoning Attacks with RL-based Aggregation in Federated Learning

Yujing Wang1,2, Hainan Zhang1,2\*, Sijia $\mathbf { W e n } ^ { 1 , 2 }$ , Wangjie $\mathbf { Q i u } ^ { 1 , 2 }$ , Binghui Guo2

1Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing 2 School of Artificial Intelligence, Beihang University, China wangyujing, zhanghainan @buaa.edu.cn

# Abstract

Federated learning is susceptible to model poisoning attacks, especially those meticulously crafted for servers. Traditional defense methods mainly focus on updating assessments or robust aggregation against manually crafted myopic attacks. When facing advanced attacks, their defense stability is notably insufficient. Therefore, it is imperative to develop adaptive defenses against such advanced poisoning attacks. We find that benign clients exhibit significantly higher data distribution stability than malicious clients in federated learning in both CV and NLP tasks. Therefore, the malicious clients can be recognized by observing the stability of their data distribution. In this paper, we propose AdaAggRL, an RLbased Adaptive Aggregation method, to defend against sophisticated poisoning attacks. Specifically, we first utilize distribution learning to simulate the clients’ data distributions. Then, we use maximum mean discrepancy (MMD) to calculate the pairwise similarity of the current local model data distribution, its historical data distribution, and global model data distribution. Finally, we use policy learning to adaptively determine the aggregation weights based on the above similarities. Experiments on four real-world datasets demonstrate that the proposed defense model significantly outperforms widely adopted defense models for sophisticated attacks.

![](images/40183a47c2f6dd82ca3324379446ce9491efec569b603d42d76bd9ee94937030.jpg)  
Figure 1: The statistical results of the similarity between the current client data distribution and its historical data distributions under four types of attacks vary with the training epochs on MNIST dataset. The $\mathbf { \boldsymbol { x } }$ -axis denotes the number of client update rounds, and the y-axis represents the similarity between the current and its historical data distributions.

Code — https://github.com/TAP-LLM/AdaAggRL

# Introduction

Federated Learning (FL) enables distributed model training across local devices, preserving data privacy while leveraging diverse local data to improve performance. It is widely applied in areas including smart healthcare (Chaddad, Wu, and Desrosiers 2023), financial services (Byrd and Polychroniadou 2020), IoT (Nguyen et al. 2021), and intelligent transportation (Yamany, Moustafa, and Turnbull 2021). However, FL systems are vulnerable, and the performance of the aggregated model is susceptible to model poisoning attacks from unknown clients (Zheng et al. 2024), especially the sophisticated poisoning strategies tailored for central servers. In this work, we focus on untargeted model poisoning attacks, where malicious devices aim to maximally reduce the accuracy of the global model by sending customized gradients to the server.

Traditional defense methods mainly rely on designing local model update assessment mechanisms or using robust aggregation methods to mitigate the impact of poisoning attacks. However, these defense methods are primarily targeted at manually crafted myopic attack strategies, and their defense stability is lacking when facing advanced attacks. For example, Li, Sun, and Zheng propose using distribution learning to simulate the data distribution of the central server and employing reinforcement learning (RL) to tailor attack policy for the aggregation process, making it less detectable.Therefore, it is urgent to develop adaptive defenses against such learnable advanced poisoning attacks.

Most benign clients exhibit significant data distribution stability in FL. In normal cases, the current training data distribution simulated by the client’s parameters should align with its historical simulated data distribution. This is because benign clients typically employ random sampling and undergo multi-round training based on their local data, ensuring the stability of the simulated data distribution. However, malicious clients require gradient attacks, so its simulated data distribution between current and history lacks regularity. To validate this, we conduct a statistical analysis of mainstream attack methods, namely IPM (Xie, Koyejo, and Gupta 2020), LMP (Fang et al. 2020), EB (Bhagoji et al. 2019) and RL-based attacks (Li, Sun, and Zheng 2022), measuring the similarity of their data distribution with history after each round of updates on MNIST, as shown in Figure 1. We find that benign clients maintain higher data distribution similarity, while attack clients show no discernible patterns. We also observe the same phenomenon on NLP tasks, as shown in Appendix1. Therefore, malicious clients can be recognized through the stability of their simulated data distribution.

This paper proposes an RL-based Adaptive Aggregation method, AdaAggRL, to thwart sophisticated poisoning attacks. It determines aggregation weights of local models by comparing the stability of client data distributions. Specifically, we first use distribution learning to emulate client data distributions based on the uploaded model parameters. Next, we use the maximum mean discrepancy (MMD) to calculate the pairwise similarity of the current local model data distribution, its historical data distribution, and the global model data distribution, to evaluate the stability of the client’s data distribution. Considering that the accuracy of distribution learning can potentially impact the calculation of the similarities above, we use reconstruction similarity as an evaluation metric for the quality of distribution learning. Finally, we use the policy learning method TD3 to adaptively determine the aggregation weights based on these similarities.

Experimental results on four real-world datasets demonstrate that AdaAggRL defense method significantly outperforms existing defense approaches (Blanchard et al. 2017; Yin et al. 2018; Sun et al. 2019) and achieves a more stable global model accuracy even facing sophisticated attacks, such as RL-based attacks (Li, Sun, and Zheng 2022).

The innovations of this paper are as follows:

• We propose an RL-based adaptive aggregation method AdaAggRL to defend against sophisticated untargeted model poisoning attacks tailored for servers in FL, aiming to advance the development of defense systems. • We observe stable training data distribution in benign clients over time, contrasting with irregular distribution caused by disruptive attempts from malicious clients in both CV and NLP tasks. Thus, we propose four metrics as RL environmental cues, utilizing policy learning to determine local model aggregation weights based on observed cue changes. • Experimental results on four datasets demonstrate that the proposed AdaAggRL defense method can maintain more stable global model accuracy than baselines, even when more advanced customized attacks are applied.

# Related Work

# Poisoning Attacks

Based on the attacker’s objectives, poisoning attacks can be classified into targeted poisoning attacks aiming to misclassify specific input sets (Bhagoji et al. 2019; Baruch, Baruch, and Goldberg 2019; Bagdasaryan et al. 2020) and untargeted attacks aimed at reducing the overall accuracy of the global model (Fang et al. 2020; Xie, Koyejo, and Gupta 2020; Shejwalkar and Houmansadr 2021). Current untargeted attack methods typically employ heuristic-based approaches (Xie, Koyejo, and Gupta 2020) or optimize myopic objectives (Fang et al. 2020; Shejwalkar and Houmansadr 2021; Shejwalkar et al. 2022). Bhagoji et al. generate malicious updates through explicit enhancement, optimizing for a malicious objective strategically designed to induce targeted misclassification. Xie, Koyejo, and Gupta manipulate the attacker’s gradients to ensure the inner product with the true gradients becomes negative. Fang et al. generate malicious updates by solving an optimization problem. However, these attack methods require local updates from benign clients or accurate global model parameters to generate significant adversarial impacts and often yield suboptimal results when robust aggregation rules are employed.

To address these deficiencies, Li, Sun, and Zheng propose a model-based RL framework for guiding untargeted poisoning attacks in FL system. They utilize the server’s updates to approximate the server’s data distribution, subsequently employing the learned distribution as a simulator for FL environment. Based on the simulator, they utilize RL to automatically generate effective attacks, resulting in significantly reducing the global accuracy. Even when the server employs robust aggregation rules, this customized method can still maintain a high level of attack effectiveness.

# Defenses for Poisoning Attacks

Current defense strategies against FL poisoning attacks can be categorized into two types: one involves designing local model update evaluation mechanisms to identify malicious client-submitted model parameters (Cao et al. 2020; Sattler et al. 2020; Zhang et al. 2022), and the other is based on designing novel Byzantine fault-tolerant aggregation algorithms using mathematical statistics to improve the robustness of aggregation (Blanchard et al. 2017; Yin et al. 2018; Sun et al. 2019; Rajput et al. 2019; Xie, Koyejo, and Gupta 2019). In evaluation mechanisms, Sattler et al. divide updates into different groups based on cosine similarity between the model parameters submitted by clients, mitigating the impact of poisoning attacks. In response to more covert poisoning attacks, some evaluation methods (Cao et al. 2020; Park et al. 2021) require the server to collect a portion of clean data as a basis for validating model updates. In robust aggregation algorithms, statistical methods (Blanchard et al. 2017; Yin et al. 2018; Sun et al. 2019) compare local updates and remove statistical outliers before updating the global model. For example, Blanchard et al. employ the square-distance metric to measure distances among local updates and select the local update with the minimum distance as the global parameters. Yin et al. sort the values of parame

1. benign benign   
similarity malicious similarity malicious 0. 0. 0 epochs 100 0 epochs 100 (a) IPM (b) LMP 1. 1. benign benign   
similarity malicious similarity malicious 0. 0. 0 epochs 100 0 epochs 100 (c) EB (d) RL-attack MMD Environmental Cues st   
Distribution Learning Server NPetolwicoyrk At--p Policy   
g:=(0+1-0t)/a 10t 0t+1 Learning 0t 0t 0t 0t Benign Malicious Clients Clients

ters in all local model updates and consider the median value of each parameter as the corresponding parameter value in the global model update. Sun et al. perform gradient clipping on parameter updates before aggregation. Due to the susceptibility of statistical estimates to an outlier, existing aggregation methods cannot guarantee accuracy well and are still susceptible to local model poisoning attacks (Bhagoji et al. 2019; Fang et al. 2020).

These defense methods mainly rely on the local model parameters. For sophisticated attacks, such as RL-based customized attacks, it is difficult to identify malicious updates from the parameter information alone accurately. Moreover, statistics-based robust aggregation is not flexible enough.

# Motivation

During FL, benign clients are selected by the server through random sampling and trained on their local data for a certain number of rounds. Therefore, the simulated data distribution obtained through gradient inversion should align with their historical distribution. But the data distribution of malicious clients lacks regularity, due to their need to conduct model attacks. To verify this, we conduct a statistical analysis of the similarity between data distributions for mainstream attack methods such as IPM, LMP, EB, and RL-attack, as shown in Figure 1. The similarity of data distributions for benign clients remains high and stable, while malicious clients lack any regular pattern. We also observe the same phenomenon on NLP tasks, as shown in Appendix.

Moreover, the data distribution of benign clients for current steps is similar to the global model and remains stable, but the similarity of malicious clients is lower, as shown in Figure 2. Since the data distribution of the global model represents the average state of normal data, the distribution of benign clients should always be consistent with the global model. Instead, malicious clients do not possess this property. Therefore, we can compare data distribution similarity between the current client and global model to observe variations and assess the degree of malicious behavior. Similarly, the similarity between historical distribution and global model distribution is higher and more stable for benign clients than malicious ones, as shown in Appendix.

Since the quality of distribution learning greatly affects the accuracy of distribution similarities, the reconstruction similarity of distribution learning is used as an evaluation metric for assessing the quality of distribution learning. Reconstruction similarity measures the similarity between the inversion gradients from distribution learning and the gradients updated by clients. Higher reconstruction similarity indicates higher confidence in current distribution learning.

# RL-based Adaptive Aggregation Methods Task Definition

In the context of $\mathrm { F L }$ (McMahan et al. 2017), a system comprises $K$ clients, with each client $k$ possessing a fixed local dataset $\begin{array} { r c l } { D _ { k } } & { = } & { \{ ( x _ { k j } , y _ { k j } ) \} _ { j = 1 } ^ { n _ { k } } } \end{array}$ , where $n _ { k }$ is the size of $D _ { k }$ . The local objective of client $k$ is $F _ { k } ( \theta ) \ =$ n1 jn=k 1 l(θ, xkj, ykj), where l is the loss function. And $( x _ { k j } , y _ { k j } )$ is the $j$ -th sample drawn i.i.d. from some distribution $P _ { k }$ . $\hat { P } _ { k }$ denotes the empirical distribution of $n _ { k }$ data samples. The optimization objective of FL is: minθ $f ( \theta ) =$ $\textstyle \sum _ { k = 1 } ^ { K } p _ { k } F _ { k } ( \theta ) .$ , $p _ { k }$ represents the weight of client $k$ . During FL, in each epoch $t \geq 0$ , the server randomly selects a subset $C ^ { t }$ from all clients and distributes the latest global model parameters $\theta ^ { t }$ . The chosen clients train on their local datasets, updating parameters as $\theta _ { k } ^ { t + 1 } = \theta ^ { t } - \alpha \nabla F _ { k } ( \theta )$ , where $\alpha$ is the learning rate. Then they upload the updated model parameters. The server aggregates the received parameters according to a specific aggregation rule Aggr, to obtain the new global model parameters $\theta ^ { t + 1 } = A g g r ( \theta _ { C ^ { t } } ^ { t + 1 } )$ .

We assume that the server is non-malicious. Malicious clients have sufficient knowledge of the server, including model structure, loss function, learning rate, and other key parameters, to demonstrate the effectiveness of AdaAggRL in defending against strong attacks.

# Framework Overview

In AdaAggRL framework (see Algorithm in Appendix), the server determines the weights for local model aggregation by assessing the stability of client data distributions, as shown in Figure 3. Firstly, we employ distribution learning to simulate the client’s data distribution $\hat { P } _ { k } ^ { t }$ based on the locally uploaded model parameters $\theta _ { k } ^ { t + 1 }$ . Secondly, we calculate similarity metrics $S _ { k , c l }$ between the current client and historical data distribution, $S _ { k , c g }$ between the current client and global model data distribution, and $S _ { k , l g }$ between the client’s historical and global model data distribution. Finally, RL is utilized to adaptively determine the weight $p _ { k }$ for local model aggregation based on these metrics and the reconstruction similarity $S _ { k , R }$ from distribution learning.

# Distribution Learning

According to local model parameters $\boldsymbol { \theta } _ { k } ^ { t + 1 }$ uploaded by clients, the server simulates the local data distribution $\hat { P } _ { k } ^ { t }$ of clients by gradient inversion. In this work, we adapt the inverting gradients (IG) method (Geiping et al. 2020) for distribution learning. The IG method reconstructs the data sample by optimizing the loss function based on the cosine similarity between the real gradient and the gradient generated by the reconstructed data.

For each epoch $t ~ \geq ~ 0$ , the server receives the clients’ locally updated model parameters and calculates the corresponding batch level gradient $\begin{array} { r c l } { \bar { g } _ { k } ^ { t } } & { \colon = } & { ( \theta _ { k } ^ { t + 1 } - \theta ^ { t } ) / \alpha } \end{array}$ . The server then solves the following optimization problem with a batch of randomly generated dummy data and labels $D _ { d u m m y }$ : $\begin{array} { r l r } { \operatorname* { m i n } _ { D _ { \mathrm { d u m m y } } } 1 } & { { } - } & { \frac { \langle \nabla _ { \theta } F _ { D _ { \mathrm { d u m m y } } } ( \theta _ { k } ^ { t \mp 1 } ) , \bar { g } _ { k } ^ { t } \rangle } { \| \nabla _ { \theta } F _ { D _ { \mathrm { d u m m y } } } ( \theta _ { k } ^ { t + 1 } ) \| \cdot \| \bar { g } _ { k } ^ { t } \| } + } \end{array}$ $\textstyle { \frac { \beta } { B ^ { \prime } } } \sum _ { ( x , y ) \in D _ { \mathrm { d u m m y } } } \mathrm { T V } ( x )$ , where $\beta$ is a fixed parameter, $B ^ { \prime }$ is the size of the dummy data batch, $\begin{array} { r l } { F _ { D _ { \mathrm { d u m m y } } } ( \theta ) } & { { } = } \end{array}$ $\begin{array} { r } { \frac { 1 } { B ^ { \prime } } \sum _ { ( x , y ) \in D _ { \mathrm { d u m m y } } } l ( \theta , x , y ) } \end{array}$ , TV calculates the total variation(Rudin, Osher, and Fatemi 1992). While solving the optimization problem, $D _ { d u m m y }$ is continuously updated. The optimization terminates after max iters iterations, then outputs the updated data as the reconstructed data samples $D _ { \mathrm { r e c } }$ , and the reconstruction similarity of client $k$ , denoted $\begin{array} { r } { S _ { k , R } = \frac { \langle \nabla _ { \theta } F _ { D _ { \mathrm { r e c } } } ( \theta _ { k } ^ { t + 1 } ) , \bar { g } _ { k } ^ { t } \rangle } { \| \nabla _ { \theta } F _ { D _ { \mathrm { r e c } } } ( \theta _ { k } ^ { t + 1 } ) \| \cdot \| \bar { g } _ { k } ^ { t } \| } } \end{array}$

# Environmental Cues

The distribution of samples is extracted by employing a pre-trained CNN to convert the image samples $D _ { \mathrm { r e c } }$ into a collection of feature vectors $V$ . For each client $k$ , the difference $d _ { k , c l }$ between this feature distribution and the history distribution is calculated using the maximum mean discrepancy (MMD) (Arbel et al. 2019; Wang et al. 2021) as $d _ { k , c l } = \mathrm { M M D } ( V _ { k } ^ { \mathrm { c u r r e n t } } , V _ { k } ^ { \mathrm { h i s t o r y } } ) \in [ 0 , + \infty )$ hHisetroer,i $V _ { k } ^ { \mathrm { c } }$ udrraetnat $V _ { k } ^ { \mathrm { h i s t o r y } }$   
distributions respectively. $V _ { g }$ represents feature vectors of the global model data distribution, obtained by averaging feature vectors from all participating clients. Subsequently, the dissimilarity between the current data distribution of client $k$ and the global one is $d _ { k , c g } = \mathrm { M M D } ( V _ { k } ^ { \mathrm { c u r r e n t } } , V _ { g } )$ . Similarly, the dissimilarity between the historical data distribution and the global one is $d _ { k , l g } = \mathrm { M M D } ( V _ { k } ^ { \mathrm { h i s t o r y } } , V _ { g } )$ . And the similarity between the current data distribution and the historical data distribution $S _ { k , c l }$ is calculated using the following formula:

$$
S _ { k , c l } = 2 \cdot \cos ( \operatorname { t a n h } ( \frac { d _ { k , c l } } { 2 } ) ) - 1
$$

So $S _ { k , c l } \in \mathsf { ( 0 , 1 ] }$ , and as the current data distribution obtained through gradient inversion becomes more consistent with the historical data distribution, the value of $S _ { k , c l }$ increases. Therefore, we can determine $S _ { k , c g }$ between the current client data distribution and the global model data distribution, as well as $S _ { k , l g }$ between the historical client data distribution and the global model data distribution.

# Actions Learning

The server dynamically adapts the aggregation weights based on three similarity metrics and the reconstruction similarity associated with each client. By utilizing experiences sampled from the simulated environment, the server engages in learning a collaborative defense strategy aimed at minimizing empirical loss. This learning process employs the RL algorithm TD3 (Fujimoto, Hoof, and Meger 2018).

State: To simulate the training process of FL, including malicious clients and their behaviors, an environment for RL is set up. For each epoch $t$ in FL, let $\begin{array} { r l } { \mathbf { s } ^ { t } } & { { } = } \end{array}$ $( s _ { k _ { 1 } } ^ { t } , s _ { k _ { 2 } } ^ { t } , . . . , s _ { k _ { | C ^ { t } | } } ^ { t } ) ^ { T } \in ( 0 , 1 ] ^ { | C ^ { t } | \times 4 }$ be the state of the reinforcement learning simulation environment, where $k _ { j }$ denotes the client identifier participating in the training. Here, $s _ { k } ^ { t } : = ( S _ { k , R } , S _ { k , c l } , S _ { k , c g } , \bar { S } _ { k , l g } )$ represents the state of client $k$ with the reconstruction similarity and three obtained metrics. So the state search space is $\bar { ( 0 , 1 ] } ^ { | C ^ { t } | \times 4 }$ , where $\vert C ^ { t } \vert$ is the number of clients participating in federated aggregation.

Action: Through RL, the server is trained to make the decision $\mathbf { A } ^ { t } \ = \ \mathsf { \Gamma } ( \mathbf { a } ^ { t } , b ^ { t } ) ^ { T } \ \in \ [ 0 , 1 ] ^ { 5 }$ based on the current FL environment state $\mathrm { \mathbf { s } } ^ { t }$ . Here, $\mathbf { a } ^ { \bar { t } }$ is a four-dimensional vector, and $\begin{array} { r c l } { \Sigma _ { i = 1 } ^ { 4 } a _ { i } ^ { t } } & { = } & { 1 } \end{array}$ , where $a _ { i } ^ { t } \in [ 0 , 1 ]$ represents the weighted weight for four environmental parameters $( S _ { k , R } , S _ { k , c l } ^ { - } , S _ { k , c g } , \overset { \cdot } { S } _ { k , l g } )$ , and $b ^ { t } \in [ 0 , 1 ]$ represents a threshold. So the action space is $[ 0 , 1 ] ^ { 5 }$ .

State transition: The FL system changes based on the server’s decision $\mathbf { A } ^ { t }$ . Specifically, the $\mathrm { F L }$ system first obtains $\hat { \mathbf { w } } = \mathbf { s } ^ { t } \cdot \mathbf { a } ^ { t } \in \mathbb { R } ^ { | C ^ { t } | }$ , i.e., weighting the environmental parameters. $\hat { w } _ { k } = s _ { k } ^ { t } \cdot { \bf a } ^ { t }$ indicates the score of client $k$ Then, function $g ( \cdot )$ maps wˆ to $[ 0 , 1 ]$ interval and normalizes it, resulting in $\tilde { \mathbf { w } } = g ( \hat { \mathbf { w } } )$ . Let $\dot { \delta } = \dot { \operatorname* { m a x } } ( \tilde { \mathbf { w } } ) \cdot b ^ { t }$ , define

$$
f _ { \delta } ( x ) = { \left\{ \begin{array} { l l } { x } & { { \mathrm { i f ~ } } x > \delta } \\ { 0 } & { { \mathrm { i f ~ } } x \leq \delta } \end{array} \right. }
$$

Then $\textbf { w } = ~ f _ { \delta } ( \tilde { \mathbf { w } } )$ . The function $f _ { \delta }$ denotes clients with lower scores, which are considered to exhibit malicious behavior and are excluded from aggregation. The generation process of the server policy reveals that the threshold $\delta$ within $f _ { \delta }$ is also adaptively adjusted based on the state.

Additionally, a vector $\bar { \mathbf { h } } ^ { t } \in \mathbb { N } ^ { K }$ is introduced in the FL system environment to record the malicious behaviors of each client. $h _ { k } ^ { 0 } = 0$ , when $\tilde { w } _ { k } \le \delta$ , $h _ { k } ^ { t + 1 } = h _ { k } ^ { t } + 1$ , otherwise, htk+1 = $h _ { k } ^ { t + 1 } = \operatorname* { m a x } ( h _ { k } ^ { t } - 1 , 0 )$ , representing the occurrence of malicious behavior by client $k$ . Based on these outcomes, the FL system determines the aggregation strategy for global model parameters of the new round as follows,

$$
\theta ^ { t + 1 } = \Sigma _ { k \in C ^ { t } } \frac { w _ { k } } { \lambda ^ { h _ { k } ^ { t + 1 } } } \cdot \theta _ { k } ^ { t + 1 }
$$

Table 1: The training time of different algorithms for one round of FL (s)   

<html><body><table><tr><td></td><td>Our</td><td>Krum</td><td>Median</td><td>C-Median</td><td>FLtrust</td><td>Clipping</td></tr><tr><td>MNIST 1.652</td><td></td><td>1.744</td><td>1.069</td><td>1.556</td><td>1.107</td><td>1.327</td></tr><tr><td>Cifar10</td><td>7.201</td><td>8.698</td><td>3.537</td><td>4.469</td><td>3.589</td><td>3.696</td></tr></table></body></html>

$\lambda \in [ 1 , + \infty )$ is a hyperparameter used to indicate the severity of the penalty for malicious behavior. A higher value of $\lambda$ corresponds to a stronger punitive impact. In the subsequent epoch $t + 1$ , the $\mathrm { F L }$ system selects a new subset of clients for training, denoted as $C ^ { t + 1 }$ , and disseminates the updated model parameters $\theta ^ { t + 1 }$ to the clients. Each client then locally trains on its dataset to obtain local parameters θtk+1, resulting in a new state st+1.

Reward: The FL system calculates the reward at step $t$ as $r : = f ( \theta ^ { t } ) - f ( \theta ^ { t + 1 } )$ based on the newly obtained global model parameters θt+1.

# Computational Complexity

Compared to classical FL, AdaAggRL’s increased computational complexity mainly stems from EnvironmentalCues step. In IG algorithm, computation involves model forward propagation, loss function calculation, and backpropagation for optimization. The computational complexity depends on model complexity $M$ , optimization rounds max iters, and reconstructed images num images, totaling $O ( m a x \mathrm { { } } _ { - } i t e r s \times M \times n u m \mathrm { { } } _ { - } i m a g e s )$ . Extracting image features via pre-trained CNN incurs a complexity of $O ( C _ { C N N } \times n u m . i m a g e s )$ . Considering MMD’s constant complexity $O ( C _ { M M D } )$ , the overall computational complexity for EnvironmentalCues is ${ \cal O } ( | C ^ { t } | \ \times \ n u m . i m a \bar { g } e s \ .$ $( M \times m a x \_ i t e r s + C _ { C N N } ) ) + O ( 3 | C ^ { t } | \cdot C _ { M M D } ) + O ( 1 )$ . Gradient inversion simulates data distributions, but the server’s goal isn’t precise image reconstruction. Thus, in our experiments, gradient inversion optimization is restricted to 30 steps, with 16 dummy images. Table 1 compares AdaAggRL’s training time per FL round with other algorithms. AdaAggRL’s time increases by an average of $2 1 . 4 \%$ on MNIST and $5 0 . 1 \%$ on CIFAR-10 compared to baselines.

# Experiments

# Experimental Settings

Dataset We conduct experiments on four datasets: MNIST (LeCun et al. 1998), F-MNIST (Xiao, Rasul, and Vollgraf 2017), EMNIST (Cohen et al. 2017), and Cifar10 (Krizhevsky, Hinton et al. 2009). Addressing the non-i.i.d. challenge in FL, we follow the approach from prior work (Fang et al. 2020) by distributing training examples across all clients. Given an M-class dataset, clients are randomly divided into M groups. The probability $q$ of assigning a training sample with label $l$ to its respective group is set, with the probability of assigning it to other groups being M1−q1 . Training samples within the same client group adhere to the same distribution. When $q = 1 / M$ , the distribution of training samples across M groups is uniform, ensuring that all clients’ datasets follow the same distribution. In cases where $q > 1 / M$ , the datasets among clients are not identically distributed. Using MNIST dataset, we set $q = 0 . 5$ to distribute training samples among clients unevenly, denoted as MNIST-0.5. MNIST-0.1 represents the scenario where MNIST is evenly distributed among clients $( q = 0 . 1 )$ ).

![](images/d9affa27e9f925bff6b38a2924cec4c9060309b48267635832955824c82847b4.jpg)  
Figure 4: The testing accuracy variation of the global model on Cifar10 dataset under four attacks.

Metrics We assess FL defense methods by evaluating the global model’s image classification accuracy after 500 epochs of FL training since these attacks aim to diminish testing accuracy. Higher accuracy of the global model under various attacks indicates stronger defense robustness.

Baselines To verify the effectiveness and stability of $\operatorname { A d a A g g R L }$ , we mainly compare it with five other defense algorithms: Krum (Blanchard et al. 2017), coordinate-wise median (Median) (Yin et al. 2018), norm clipping (Clipping) (Sun et al. 2019), an extension of the vanilla coordinatewise median (C-Median) where a norm clipping step is applied (Li, Sun, and Zheng 2022), and FLtrust (Cao et al. 2020). Krum filters malicious updates at the client level, Clipping performs gradient clipping on parameter updates before aggregation, Median and C-Median select the median or clipped median of individual parameter values from all local model updates as global model parameters, and FLtrust requires the server to have access to an amount of root data. To further illustrate its performance, AdaAggRL is compared with Feddefender (Park et al. 2023) and FedVal (Valadi et al. 2023) on CIFAR-10 dataset.

We consider four poisoning attacks in FL: explicit boosting (EB) (Bhagoji et al. 2019), inner product manipulation (IPM) (Xie, Koyejo, and Gupta 2020), local model poisoning attack (LMP) (Fang et al. 2020), and RL-based model attack (RL-attack) (Li, Sun, and Zheng 2022). IPM manipulates the attacker’s gradients to ensure the inner product with true gradients becomes negative during aggregation. LMP generates malicious model updates by solving an optimization problem in each FL epoch. EB generates malicious updates through explicit enhancement, optimizing for a malicious objective designed to induce targeted misclassification. RL-attack adaptively generates attacks on the FL system using RL.

![](images/85bc1d5570c0b93a0ccc8699192c5bd143e46aa78d5f93fc076d9a9aed513fd1.jpg)  
Figure 5: The testing accuracy of FL methods on MNIST0.5 under LMP and EB as the proportion of malicious clients increases (a-b). The defense performance of AdaAggRL on MNIST-0.5 compared to the case where $S _ { c l }$ is not considered under LMP (c) and the case where $S _ { c g }$ and $S _ { l g }$ are not considered under EB (d).

Parameter Settings For MNIST, F-MNIST, and EMNIST, a Convolutional Neural Network (CNN) serves as the global model. In the case of Cifar10, the ResNet18 architecture (He et al. 2016) is utilized as the global model. In FL, there are 100 clients, denoted as $K = 1 0 0$ , with 20 malicious clients. For defense strategies based on RL, given the continuous action and state spaces, we select Twin Delayed DDPG (TD3) (Fujimoto, Hoof, and Meger 2018) algorithm to train the defense policies in experiments. Details on parameter determination are provided in the Appendix.

# Defense Performances

Table 2 reports the testing accuracy of various FL aggregation methods across four datasets, indicating AdaAggRL’s robustness. Across different datasets and models, AdaAggRL demonstrates consistent defensive efficacy against all four attack scenarios. Notably, AdaAggRL maintains stable effectiveness against RL-attack, where other defense methods face significant challenges and experience a noticeable degradation in defensive performance. Figure 4 illustrates the testing accuracy of the global model on Cifar10 dataset under four attacks, with different aggregation rules. AdaAggRL consistently outperforms other methods, achieving superior accuracy and convergence speed, particularly against RL-attack. Unlike FLtrust, it requires no root dataset and demonstrates more stable performance after 200 epochs, surpassing Krum and C-Median in all scenarios. AdaAggRL’s performance on other datasets is shown in Appendix.

LMP and EB attempt to optimize the objective function to make the poisoned gradients statistically inconspicuous, while RL-attack mimics the behavior of normal clients. As a result, defense methods like Median, C-Median, and Clipping, which rely solely on mean or median information through gradient clipping or selection, are prone to misjudge poisoned gradient updates. These methods may end up clipping correct updates while preserving erroneous ones.

Table 2: The testing accuracy of different FL aggregation methods under various attacks   

<html><body><table><tr><td></td><td>EB</td><td>IPM</td><td>LMP</td><td>RL-attack</td></tr><tr><td>C-Median</td><td>0.9598</td><td>0.9537</td><td>0.9329</td><td>0.5550</td></tr><tr><td>Clipping</td><td>0.9151</td><td>0.9654</td><td>0.1944</td><td>0.5750</td></tr><tr><td>FLtrust</td><td>0.9231</td><td>0.9591</td><td>0.9618</td><td>0.7300</td></tr><tr><td>Krum</td><td>0.9325</td><td>0.7897</td><td>0.9458</td><td>0.6875</td></tr><tr><td>Median</td><td>0.0981</td><td>0.9549</td><td>0.1135</td><td>0.2750</td></tr><tr><td>AdaAggRL</td><td>0.9659</td><td>0.9658</td><td>0.9636</td><td>0.9655</td></tr><tr><td colspan="5">(a) CNN global model, MNIST-0.1</td></tr><tr><td></td><td>EB</td><td>IPM</td><td>LMP</td><td>RL-attack</td></tr><tr><td>C-Median</td><td>0.9466</td><td>0.9479</td><td>0.9198</td><td>0.4250</td></tr><tr><td>Clipping</td><td>0.7867</td><td>0.9576</td><td>0.0986</td><td>0.4900</td></tr><tr><td>FLtrust</td><td>0.9488</td><td>0.9414</td><td>0.9412</td><td>0.4375</td></tr><tr><td>Krum</td><td>0.9274</td><td>0.7608</td><td>0.9259</td><td>0.1563</td></tr><tr><td>Median</td><td>0.0974</td><td>0.9448</td><td>0.1032</td><td>0.1563</td></tr><tr><td>AdaAggRL</td><td>0.9608</td><td>0.9617</td><td>0.9604</td><td>0.9559</td></tr><tr><td colspan="5">(b) CNN global model, MNIST-0.5</td></tr><tr><td></td><td>EB</td><td>IPM</td><td>LMP</td><td>RL-attack</td></tr><tr><td>C-Median</td><td>0.7935</td><td>0.8123</td><td>0.7722</td><td>0.6300</td></tr><tr><td>Clipping</td><td>0.7249</td><td>0.8261</td><td>0.6015</td><td>0.4600</td></tr><tr><td>FLtrust</td><td>0.8154</td><td>0.8344</td><td>0.8386</td><td>0.6150</td></tr><tr><td>Krum</td><td>0.8082</td><td>0.6610</td><td>0.7942</td><td>0.5625</td></tr><tr><td>Median</td><td>0.1002</td><td>0.8171</td><td>0.1001</td><td>0.0938</td></tr><tr><td>AdaAggRL</td><td>0.8411</td><td>0.8398</td><td>0.8400</td><td>0.8337</td></tr><tr><td colspan="5">(c) CNN global model, F-MNIST</td></tr><tr><td></td><td>EB</td><td>IPM</td><td>LMP</td><td>RL-attack</td></tr><tr><td>C-Median</td><td>0.8780</td><td>0.8724</td><td>0.8289</td><td>0.1857</td></tr><tr><td>Clipping</td><td>0.6694</td><td>0.8834</td><td>0.0008</td><td>0.1700</td></tr><tr><td>FLtrust</td><td>0.8618</td><td>0.8741</td><td>0.8684</td><td>0.2400</td></tr><tr><td>Krum</td><td>0.8309</td><td>0.5418</td><td>0.8135</td><td>0.0312</td></tr><tr><td>Median AdaAggRL</td><td>0.0388</td><td>0.8716</td><td>0.4331</td><td>0.1850</td></tr><tr><td></td><td>0.8816</td><td>0.8805</td><td>0.8776</td><td>0.8786</td></tr><tr><td colspan="5">(d) CNN global model,EMNIST</td></tr><tr><td></td><td>EB</td><td>IPM</td><td>LMP</td><td>RL-attack</td></tr><tr><td>C-Median</td><td>0.7091</td><td>0.7364</td><td>0.6048</td><td>0.1150</td></tr><tr><td>Clipping</td><td>0.1002</td><td>0.6589</td><td>0.1388</td><td>0.0850</td></tr><tr><td>FLtrust</td><td>0.5786</td><td>0.6709</td><td>0.7312</td><td>0.6450</td></tr><tr><td>Krum</td><td>0.7238</td><td>0.7028</td><td>0.7283</td><td>0.0900</td></tr><tr><td>Median</td><td>0.4484</td><td>0.7309</td><td>0.1018</td><td>0.0900</td></tr><tr><td>AdaAggRL</td><td>0.7452</td><td>0.7497</td><td>0.7583</td><td>0.7531</td></tr></table></body></html>

(e) ResNet18 global model, Cifar10

AdaAggRL’s performance is evaluated against Feddefender and FedVal on the CIFAR-10 dataset using ResNet18 over $1 0 0 ~ \mathrm { F L }$ epochs, shown in Table 3. AdaAggRL is still significantly better than the latest baselines.

![](images/2893aefc20bec7dc364624e17e4e54b60268e5f91a1dd938e6e824ed23787409.jpg)  
Figure 6: The performance of FL defense algorithms under different distribution conditions.

Table 3: The accuracy of aggregation methods under attacks   

<html><body><table><tr><td></td><td>IPM</td><td>LMP</td><td>EB</td><td>RL-attack</td></tr><tr><td>Feddefender</td><td>0.4711</td><td>0.4196</td><td>0.1008</td><td>0.4150</td></tr><tr><td>FedVal</td><td>0.6052</td><td>0.0980</td><td>0.4338</td><td>0.1392</td></tr><tr><td>AdaAggRL</td><td>0.6984</td><td>0.7063</td><td>0.7143</td><td>0.7106</td></tr></table></body></html>

# Ablation Studies

Impact of Current-history Similarity To illustrate the impact of the similarity metrics $S _ { c l }$ on the stability of the FL process, Figure 5c depicts the defense performance of AdaAggRL compared to the case where $S _ { c l }$ is not considered under LMP attack. We observe that considering the variations in $S _ { c l }$ indeed enhances the convergence speed and reduces the oscillation amplitude of testing accuracy.

Impact of Current (history)-global Similarity Figure 5d illustrates the defense performance of AdaAggRL compared to the case where $S _ { c g }$ and $S _ { l g }$ are not considered under EB attack. We observe that the malicious client data distribution obtained through gradient reversal may stably deviate from the normal distribution, leading to an inflated $S _ { c l }$ . If $S _ { c g }$ and $S _ { l g }$ are not considered, the defense effectiveness degrades.

# Analysis

Impact of the Number of Attackers Figures 5a and 5b show testing accuracy under LMP and EB attacks as malicious client proportion increases from $0 \%$ to $9 5 \%$ . Both AdaAggRL and FLtrust can tolerate up to $90 \%$ of malicious clients. AdaAggRL shows a slight accuracy decline as malicious client proportions increase, while the remaining FL aggregation algorithms can only tolerate malicious clients below $30 \%$ under LMP attack. This highlights AdaAggRL’s stability against high percentages of malicious clients.

Impact of non-i.i.d. Degree Table 2b reports testing accuracy of FL defense algorithms on MNIST-0.5 under various attacks, while Figure 6 compares their performance under different distribution conditions. With non-i.i.d. data $\scriptstyle ( { \mathrm { q } } = 0 . 5 )$ , baselines show reduced accuracy under LMP and increased oscillations under EB, and non-i.i.d. conditions significantly impact their performance against RL-attack. AdaAggRL demonstrates faster convergence, particularly against IPM, LMP, and EB, with minimal performance decline across attacks, demonstrating robustness to non-i.i.d. data. Under significant non-i.i.d. impact, RL can adaptively lower the weights of global model-related similarity scores. More results for $\mathsf { q } { > } 0 . 5$ can be found in the Appendix.

# Conclusion

This paper proposes AdaAggRL, an RL-based Adaptive Aggregation method, to counter sophisticated poisoning attacks. Specifically, we first utilize distribution learning to simulate clients’ data distributions. Then, we use MMD to calculate the pairwise similarity of the current local model data distribution, its historical data distribution, and the global model data distribution. Finally, we use policy learning to adaptively determine the aggregation weights based on the above similarities and the reconstruction similarity. Experiments on four real-world datasets demonstrate that AdaAggRL significantly outperforms the state-of-the-art defense model for sophisticated attacks. Future work could explore novel attacks on adaptive defense. One potential scheme to construct malicious update parameters is to solve an optimization problem under the condition of controlling the stability of simulated data distribution changes.