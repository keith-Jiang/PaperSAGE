# VCR: A “Cone of Experience” Driven Synthetic Data Generation Framework for Mathematical Reasoning

Sannyuya Liu1,2, Jintian Feng1,2, Xiaoxuan Shen1,2, Shengyingjie Liu1,2, Qian Wan1,2\*, Jianwen $\mathbf { S u n } ^ { 1 , 2 * }$

1Faculty of Artificial Intelligence in Education, Central China Normal University, Wuhan 430079, China 2National Engineering Research Center of Educational Big Data, Central China Normal University, Wuhan 430079, China liusy027, shenxiaoxuan, wanq8228, sunjw @ccnu.edu.cn, fjt2018, lsyj @mails.ccnu.edu.cn

# Abstract

Large language models (LLMs) have shown excellent performance in natural language processing but struggle with mathematical reasoning. As the training mode gradually solidifies, researchers propose a data-centric concept of artificial intelligence, emphasizing the development of higher-quality data to empower LLMs. Existing studies construct synthetic data for mathematical reasoning by expanding public datasets, thereby performing supervised fine-tuning of LLMs. However, these methods mostly focus on quantity while neglecting quality. The challenging samples fail to receive adequate consideration during data synthesis process, resulting in high construction costs, low-quality density, and serious data homogenization. This paper proposes a multi-agent environment called Virtual ClassRoom (VCR), which leverages various agents driven by LLM to construct high-quality diversified synthetic data. Inspired by the “Cone of Experience” educational theory, VCR introduces three experience levels (direct, iconic, and symbolic) into data synthesis process by analogy with human learning. A user-friendly instruction set and role-playing system are carefully designed, enabling VCR to autonomously plan the scale of synthetic data. This system covers various educational scenarios, including lecture, discussion, problem design and problem-solving. The Adaboost idea embodied in the global iterative process further promotes steady performance improvement. Extensive experiments show that the synthetic data generated by VCR possess higher quality density and generalization capability, which can give LLMs superior mathematical reasoning performance with the same scale.

# Introduction

Large Language Models (LLMs) have demonstrated impressive performance in areas such as dialogue and code generation (Minaee et al. 2024; Luo and Yang 2024; OpenAI 2024; Dubey et al. 2024), but they continue to face challenges in mathematical reasoning tasks (Ahn et al. 2024).

The rapid development in the field of LLMs has led to a gradual homogenization of model architectures and training methods. From the perspective of Data-centric AI (Zha et al. 2023), constructing higher-quality synthetic data for supervised fine-tuning (SFT) (Ouyang et al. 2022) of LLMs is expected to further enhance their mathematical reasoning capabilities (Ahn et al. 2024; Yu et al. 2024; Li et al. 2024a,b; Zeng et al. 2024; Liu et al. 2024a).

Research shows that synthetic data, as an effective data augmentation method, can expand existing datasets to achieve data enrichment. For example, MetaMathQA (Yu et al. 2024) and XwinMathQA (Li et al. 2024a) enhanced the queries and responses in the GSM8K and MATH using prompts with strong closed-models such as GPT-3.5 and GPT-4. The quality of synthetic data directly determines the performance of LLMs (Zhang et al. 2024a; Cao et al. 2024; Gunasekar et al. 2023), and its construction process must consider factors such as scale, relevance, accuracy, and diversity (Zeng et al. 2024). Previous works have overly focused on data scale while neglecting quality dimensions, leading to high construction costs, low-quality density, and severe data homogenization issues.

To generate high-quality synthetic data, some works drew an analogy between LLM training and human learning processes, and proposed various methods for generating synthetic data. For instance, LEMA (An et al. 2024) introduced a strategy of learning from mistakes, creating erroneous reasoning data and its correction as training data. MMIQC (Liu et al. 2024a) and SkyworkMathQA (Zeng et al. 2024) rewrote the training set questions into other formats to generate synthetic data, somewhat mimicking the human learning process through diverse practice formats. However, these simple analogy methods cannot fully transfer the multi-level experience in the human learning process, hence they struggle to profoundly guide the data synthesis process.

Human learning is an extremely complex process that includes both observing others and self-practice. The “Cone of Experience” educational theory (Dale 1947) posits that human learning consists of three levels of experience: direct experience (Learning by Doing), iconic experience (Learning through Observation), and symbolic experience (Learning through Abstractions) 1. Only through the full integration of these three levels of experience can high-quality learning be achieved. Inspired by “Cone of Experience”, we design three levels of experiences for LLM training, aiming to enhance existing datasets and integrate process-oriented information to generate high-quality synthetic data within real educational scenarios. However, due to limitations in ethical concerns and speed of data collection in real educational settings (Yue et al. 2024), we use the strong humanlike abilities of LLMs to simulate an educational scenario with LLM-based agents.

In summary, this paper proposes a multi-agent environment called Virtual ClassRoom (VCR), which leverages various agents driven by LLMs to construct high-quality, diversified synthetic data. To embody the three levels of human learning experiences, VCR includes the following stages: i) Doing Stage. This stage aims to distinguish the attention of each problem in the training set. Our data augmentation process focuses on challenging problems that LLMs have not been able to solve correctly. ii) Observation Stage. In the observation stage, we set multiple roles such as teacher, teaching assistant, student, and classroom noter. Each problem is analyzed in-depth through steps including teacher guidance, student discussion, and summary analysis. iii) Abstraction Stage. In this stage, agents summarize problems, extract key elements, and create similar problems for further practice.

Based on these stages, VCR can autonomously analyze sample biases in the target problem domain and adaptively plan the expected scale of synthetic data. A user-friendly set of instructions and a role-playing system are carefully designed to cover teaching activities such as lecturing, discussion, problem design, and problem-solving. The integration of the Adaboost (Ying et al. 2013) ideas into the global iterative process can further promote the steady enhancement of performance. Extensive experiments demonstrate that the synthetic data generated by VCR has higher quality density and generalization performance, providing superior mathematical reasoning capabilities to LLMs at the same scale. Notably, through extensive qualitative analysis, we discover for the first time that indirect data related to thought discussions also contributes to enhancing LLMs’ mathematical reasoning abilities, further proving the similarity between LLMs and human learning.

Our contributions can be summarized as follows:

• This paper proposes a multi-agent environment based on the “Cone of Experience” simulation teaching scene, named Virtual Classroom (VCR). VCR achieves the concretization of multi-level experience by analogizing human learning, providing high-quality synthetic data for LLM mathematical reasoning.   
• Based on a well-designed instruction set and role-playing system, VCR is entirely driven by LLM-based agents, which not only realize adaptive planning of data scale, but also cover a wide range of educational environments such as lecture, discussion, problem design and problemsolving.   
• Extensive experiments demonstrate the effectiveness and advancement of VCR. Under the same data scale, VCRgenerated data significantly enhances mathematical reasoning performance more effectively than baselines.   
• Qualitative analysis further shows that task-related in

direct data is significantly beneficial to LLMs learning, which indicates the feasibility of analogy learning by human experience and can lead to further development for SFT data construction.

# Related Work

Mathematical Reasoning with LLMs Although LLMs have demonstrated exceptional capabilities across many tasks (Minaee et al. 2024), significant challenges remain in complex mathematical reasoning tasks (Ahn et al. 2024) such as those in MATH (Hendrycks et al. 2021) and GSM8K (Cobbe et al. 2021). To enhance the mathematical reasoning abilities of LLMs, some work focuses on continual pre-training (CPT) LLMs on large-scale mathematical corpora (Azerbayev et al. 2023; Jiang et al. 2023; Paster et al. 2024), which is believed to supplement LLMs’ mathematical knowledge and improve reasoning abilities. Additionally, strategies using reasoning frameworks, such as CoT (Wei et al. 2022; Wu, Jiang, and Shen 2024), guide LLMs to decompose reasoning tasks into sub-steps through prompts. Although these strategies do not involve updating model parameters, they can effectively activate LLMs’ reasoning abilities (Wang et al. 2023b). Additionally, recent works in this field have emerged based on large multimodal models, including benchmarks (Zhang et al. 2024b) and construction frameworks (Liu et al. 2024b). In this work, we focus on another strategy, namely SFT-based methods (Yu et al. 2024; Li et al. 2024a), which fine-tune LLMs by constructing query-response pairs related to downstream tasks.

SFT Data Construction The core of SFT-based methods is the collection of high-quality synthetic data. Recent research (Li et al. 2024b; Zeng et al. 2024) indicates that Data Scaling Laws (Kaplan et al. 2020) also apply to mathematical reasoning in LLMs, meaning that a model’s mathematical reasoning abilities improve with increasing data size. To achieve performance comparable to GPT-4 in open-source LLMs, large-scale SFT datasets are often constructed by augmenting queries or responses of problems, leveraging powerful closed-source models like GPT-4 or using rejection sampling (Luo et al. 2023; Li et al. 2024a,b; Zeng et al. 2024; Liu et al. 2024a; Shao et al. 2024). However, constructing SFT data is typically time-consuming and costly, so the quality of SFT data is crucial. Higher quality data can mean fewer samples, translating to lower costs. Diversity is one indicator of data quality (Li et al. $2 0 2 4 \mathrm { c }$ ; Zhou et al. 2023; Zhang et al. 2024a), and SFT data with higher diversity can bring greater performance improvements to models. In this work, we propose a new SFT data construction approach that goes beyond simple augmentation of queries and responses and achieves higher diversity.

LLM-based Agents for Human Simulation The strong linguistic capabilities of LLMs endow LLM-based agents with excellent anthropomorphic abilities, allowing LLMs to simulate human behaviors with user prompts. LLMbased agents have been applied to simulations in various fields, including psychology (Yang et al. 2024), political science (Moghimifar et al. 2024), social interactions (Gu¨rcan

I aim to enhance my mathematical problem solving ability. Pv LLM   
A   
Okay! Welcome to our class, please finish the exam first.😄 VCR O Student Exercises Answers XX<XX Weakness Strength by Doing Learning P 1 Teacher Taking Exam analyze Set Direct Exp. 3 TA   
Teacher: Come up focus   
with a ques. to test 窗灵r Learning through   
each other. Observation   
aAlqeune:s.HimBaodbe bTyhimsei.s 388 Iconic Experience Discussion Other’s Solution Lectures   
Bob: Let me try… Summary   
mWinhadtoifstyhoiusrqduesi?gn Learning through Back to Abstractions   
百 Doing Symbolic Experience 。 𝒓𝒆<𝒔𝒒𝒑𝒐𝒖𝒏𝒆𝒓𝒔𝒆𝒚,> Review Reflection   
Noter The three stages of VCR Cone of Experience

2024), and software development (Qian et al. 2024). Agents have also shown impressive anthropomorphic abilities in education. Existing work indicates that agents can act as teachers or students in educational settings, simulating teaching and discussion scenarios, and can formulate teaching plans (Hu et al. 2024). Additionally, LLM-based agents can effectively simulate traditional classroom interaction patterns (Zhang et al. $2 0 2 4 \mathrm { c }$ ; Yue et al. 2024). In this work, we use a multi-agent system to simulate the educational process and construct SFT training data by collecting data from these processes.

# Methods

# Problem Definition

(i) Input/Output of VCR: Given a public dataset $\mathcal { D } =$ $\{ ( q _ { i } , \bar { r } _ { i } ) \} _ { i = 1 } ^ { N }$ , where $N$ denotes the size of $\mathcal { D }$ , $q _ { i }$ and $\boldsymbol { r } _ { i }$ denote the query and response for the $i ^ { \mathrm { t h } }$ question, respectively. VCR sequentially performs $R$ expansions on $\mathcal { D }$ . The $r ^ { \mathrm { t h } }$ expansion results in $\mathcal { D } _ { r } ^ { \prime } = \{ ( q _ { i } ^ { \prime } , r _ { i } ^ { \prime } ) \} _ { i = 1 } ^ { N ^ { \prime } }$ , where $N ^ { \prime }$ is the userspecified size of the expanded dataset.

(ii) Supervised Fine-Tuning (SFT): For an LLM parameterized by $\theta$ , we perform SFT $R$ times, with $\mathcal { D } _ { r } ^ { \prime }$ used for the $r ^ { \mathrm { t h } }$ SFT. SFT is conducted by maximizing the log-likelihood of the response given the prompt query. The SFT loss is defined as

$$
\mathcal { L } ( \boldsymbol { \theta } ) = - \frac { 1 } { N ^ { \prime } } \sum _ { i = 1 } ^ { N ^ { \prime } } \log \mathbb { P } ( r _ { i } ^ { \prime } \mid q _ { i } ^ { \prime } ; \boldsymbol { \theta } )
$$

# The Proposed Framework (VCR)

Overview This section introduces the workflow of VCR, with an overview illustrated in Figure 1. VCR is a multiagent environment which involves the following four roles:

Teacher: Responsible for planning the instructional process, providing problems, guiding the students’ solution process, and supervising their learning progress; TA (Teaching Assistant): Assists in discussions, evaluates, and analyzes the students’ responses to problems; Student: Obtains knowledge from the Teacher, and engages in learning and practice through participating in discussions and solving problems; Noter: Records dialogue data and feedback throughout the learning process into SFT data format.

We set up profiles for each role, and all teaching activities are completed through multi-agent collaboration. The design of VCR is based on the “Cone of Experience” educational theory, incorporating human multi-level learning experiences. It divides the learning process of LLMs into three categories: doing, observation, and abstraction. By engaging with experiences corresponding to these three processes, LLMs enhance their mathematical reasoning capabilities. Note that, just as human learning is a repetitive process, VCR is also iterative rather than linear. In the following, we will describe how to acquire experiences related to each of these three learning processes. Algorithm 1 details the specific workflow.

Stage 1: Learning by Doing Given to-be-trained LLM $\mathcal { M }$ and public dataset(e.g., GSM8K, MATH, etc.) $\textit { \textbf { D } } =$ $\{ ( q _ { i } , r _ { i } ) \} _ { i = 1 } ^ { \boldsymbol { N } }$ , and expected synthetic dataset size $N ^ { \prime }$ . The process begins with a teacher agent instructing $\mathcal { M }$ to answer each question $q _ { i }$ from $\mathcal { D }$ for $t$ times, allowing the teacher agent to compute each question’s error rate $k _ { i }$ . Subsequently, VCR generates $\alpha k _ { i }$ synthetic data for $q _ { i }$ , where

$$
\alpha = \frac { N ^ { \prime } } { \sum _ { i = 1 } ^ { N } k _ { i } }
$$

The scaling factor $\alpha$ serves as a metric for assessing the performance and stability of $\mathcal { M }$ .

Algorithm 1: VCR for synthetic data construction   

<html><body><table><tr><td>1:Input: Public training set D = {(qi,ri)}=1,LLM Mo,re- peat times R and expected synthetic datasize N'.</td></tr><tr><td>2:Output: SFT training set D', final LLM M.</td></tr><tr><td>3:M←M,D← Initialization</td></tr><tr><td>4:forr ∈{1,2...,R} do 5: for qi∈D do Learning by Doing</td></tr><tr><td>6: Have M respond to qi with t times</td></tr><tr><td>7: Calculate the error rate ki</td></tr><tr><td>8: Teacher analyze the weakness of M</td></tr><tr><td>9: end for</td></tr><tr><td>10: α↑ N Scaling factor for data size</td></tr><tr><td>11: Construct D'=@ Training data for rth SFT</td></tr><tr><td>12: for qi∈D do</td></tr><tr><td>13: Syn(qi)=Ω Synthetic data for qi</td></tr><tr><td>14: while(|Syn(qi)|<αki)do</td></tr><tr><td>15: Augment(ri) = Lecture(Teacher, Students, qi)</td></tr><tr><td>16: Expand(ri)= Solution(Students,qi)</td></tr><tr><td>17: Augment(qi) = Discussion(Students,qi)</td></tr><tr><td>18: Mind(qi) = Discussion(Students, qi)</td></tr><tr><td>19: >Learning through Observation</td></tr><tr><td>20: Key(qi)=Review(TA,qi)</td></tr><tr><td>21: Expand(qi) = Reflection(TA,qi)</td></tr><tr><td>22: Learning through Abstractions</td></tr><tr><td>23: Noter records all content to Syn(qi)</td></tr><tr><td>24: end while</td></tr><tr><td>25: end for</td></tr><tr><td>26: Train M on D'. for producing Mr SFTM</td></tr><tr><td>27: Update M ←Mr,D'←D'U{D'}</td></tr><tr><td></td></tr><tr><td>28:end for 29:Return:SFT training set D', final model M.</td></tr></table></body></html>

At this stage, VCR identifies challenging samples in $\mathcal { D }$ based on the error rate $k _ { i }$ of $\mathcal { M }$ on $q _ { i }$ , and sets the desired amount of synthetic data for each question. Simultaneously, VCR analyzes $\mathcal { M }$ ’s weakness from responses, which will serve as seeds for generating questions in the third stage. This stage corresponds to lines 5-9 in Algorithm 1.

Stage 2: Learning through Observation The observation experience corresponding to this stage is central to VCR. Before this stage begins, the teacher agent has already identified the challenging samples in $\mathcal { D }$ for $\mathcal { M }$ . The primary task of this stage is to design teaching activities that address these weaknesses, with the activities being carried out by the agent. It is important to note that the LLM $\mathcal { M }$ does not participate in this stage; all learning activities are entirely simulated by the agents. Thus, the data from this stage can be considered as observation experience.

Each question $q _ { i }$ is handled individually, with all teaching activities planned by the teacher. Initially, teacher guides students through answering the questions via lecture, generating Augment $( r _ { i } )$ . This provides a more detailed solution process compared to CoT and can be considered as an augment of the answers. Notably, VCR predefines multiple students with different characteristics, meaning their solutions will offer diverse perspectives on the same question, which essentially serves as a method for enhancing diversity.

$$
\begin{array} { r l } & { \mathrm { A u g m e n t } ( r _ { i } ) = L e c t u r e ( \mathbf { T e a c h e r } , \mathbf { S t u d e n t s } , q _ { i } ) } \\ & { \mathrm { E x p a n d } ( r _ { i } ) = S o l u t i o n ( \mathbf { S t u d e n t s } , q _ { i } ) } \end{array}
$$

Subsequently, the teacher plans the topics for discussion and asks the students to initiate the discussions. The goal of this stage is to adapt $q _ { i }$ to form Augment $\left( q _ { i } \right)$ and to develop a question design mind for each question.

$$
\begin{array} { r } { \mathbf { A u g m e n t } ( q _ { i } ) = D i s c u s s i o n ( \mathbf { S t u d e n t s } , q _ { i } ) } \\ { \mathbf { M i n d } ( q _ { i } ) = D i s c u s s i o n ( \mathbf { S t u d e n t s } , q _ { i } ) } \end{array}
$$

Stage 3: Learning through Abstractions The purpose of this stage is to have the student agent reflect on the teaching activities of the second stage. VCR requires the TA agent to review key information from the questions. Then, based on the weakness of $\mathcal { M }$ , new questions are designed and extended by reflection.

$$
\begin{array} { r } { \mathrm { K e y } ( q _ { i } ) = R e v i e w ( \mathbf { T A } , q _ { i } ) } \\ { \mathrm { E x p a n d } ( q _ { i } ) = R e f l e c t i o n ( \mathbf { T A } , q _ { i } ) } \end{array}
$$

Finally, the Noter agent organizes all data collected from the second and third stages into the SFT data format, resulting in the synthetic dataset $\mathcal { D } _ { r } ^ { \prime } = \{ ( q _ { i } ^ { \prime } , r _ { i } ^ { \prime } ) \} _ { i = 1 } ^ { N ^ { \prime } }$ .

$$
\begin{array} { c } { \mathcal { D } _ { r } ^ { \prime } = \mathbf { N o t e r } ( \{ \mathrm { A u g m e n t } ( r _ { i } ) , \mathbf { M i n d } ( q _ { i } ) , \mathrm { K e y } ( q _ { i } ) , } \\ { \mathrm { E x p a n d } ( q _ { i } ) , \mathbf { A u g m e n t } ( r _ { i } ) , \mathrm { E x p a n d } ( r _ { i } ) \} ) } \end{array}
$$

Global Iterative Process Just as human learning is a repetitive process, LLM training also benefits from iterative improvements. In learning doing experiences, VCR adjusts its focus on each question in the training data based on performance feedback, similar to how the Adaboost algorithm iteratively reweights data to improve model accuracy. It is important to note that during this stage, VCR diagnoses the LLM with the core goal of adjusting the weights of the SFT training data, emphasizing a focus on difficult questions. However, this does not mean completely ignoring questions that the LLM already handles well. VCR still allocates a smaller amount of synthetic data to these simpler questions to prevent the LLM from forgetting them.

Specifically, for the LLM $\mathcal { M }$ , VCR can repeat the data generation process $R$ times. Before the $r ^ { t h }$ generation, the model undergoes the doing experience learning and allocates focus to training set samples. Then, based on observational and abstract experience learning, the dataset $\mathcal { D } _ { s f t } ^ { r }$ is created. The model $\mathcal { M } _ { r }$ is trained on this dataset, and the final model obtained is $\mathcal { M } _ { R }$ . The SFT dataset is:

$$
\mathcal { D } ^ { \prime } = \bigcup _ { 1 \leq r \leq R } \{ \mathcal { D } _ { r } ^ { \prime } \}
$$

This global iterative process allows VCR to enhance the quality of synthetic data, progressively improving the model’s performance over time.

# Experiments

# Datasets

We evaluated the mathematical reasoning abilities of LLMs using 5 benchmarks, including 4 publicly datasets and one modified dataset based on GSM8K, named GSM-Distractor.

• GSM8K (Cobbe et al. 2021) is a high-quality dataset of grade school math problems, consisting of 7,473 training samples and 1,319 testing samples.   
• MATH (Hendrycks et al. 2021) consists of high school math competition problems, containing 7,500 training samples and 5,000 test samples.   
• SVAMP (Patel, Bhattamishra, and Goyal 2021) includes elementary-level math problems with contextual descriptions. we use its 1,000 test samples for evaluation.   
• ASDiv (Miao, Liang, and $\mathrm { S u } 2 0 2 1$ ) contains 2,305 math problems, covering a broad range of text patterns and question types.   
• GSM-Distractor modifies the GSM8K test set, incorporating 1 to 3 distractors per question, following the setup of CMATH (Wei et al. 2023). The GSM-Distractor benchmark includes 3 times the original 1,319 problems.

# Baselines

• Evol-Instruct (Luo et al. 2023) is a reinforcement learning-based method used to construct a dataset of 96K samples, resulting in the WizardMath model.   
• LEMA (An et al. 2024) created 89K error reasoning paths from LLMs, along with correction data generated by GPT-4.   
• MetaMathQA (Yu et al. 2024) expanded the training sets of GSM8K and MATH using strategies like FOBAR (Jiang et al. 2024) and Self-Verification (Wang et al. 2023a) with GPT-3.5, resulting in a total of 395K samples.   
• XwinMathQA (Li et al. 2024a) used a three-step approach with the assistance of GPT-4-Turbo to extend GSM8K and MATH, totally generating 1.44M samples.   
• SkyworkMathQA (Zeng et al. 2024) combined the strategies of MetamathQA, XwinMathQA, and EvolInstruct but only extended the MATH dataset, resulting in 2.5M samples, including 0.4M hard questions.

# Implementation Details

We use AutoGen (Wu et al. 2023) to simulate the entire VCR process. For SFT training, we follow the settings used in the baselines and employ the LLaMA-2 (Touvron et al. 2023) series models and the Mistral (Jiang et al. 2023) model, including LLaMA-2-7B/13B/70B and Mistral-7B, to facilitate a thorough comparison with baseline methods. Additionally, we include the state-of-the-art open-source model LLaMA-3-8B (Dubey et al. 2024) to evaluate VCR’s performance improvements on more advanced models. Given computational resource constraints, we use QLoRA for finetuning LLaMA-2-70B and fully fine-tune all other models. The Adam optimizer is used with a weight decay of $3 \%$ linear warmup is applied with a warmup ratio of $3 \%$ . The learning rates are set as follows: 5e-5 for LLaMA-3-8B, 1e4 for LLaMA-2-70B, and 2e-5 for other models. All SFTs are conducted for 3 epochs with a batch size of 128. In the QLoRA fine-tuning, LoRA rank and LoRA alpha are set to 96 and 16, respectively, with a dropout rate of 0.05. We employ DeepSpeed with ZeRO-2 stage to accelerate training. Model performance evaluation after SFT is carried out using the vLLM library (Kwon et al. 2023). All experiments are conducted on 8 NVIDIA A800 (80G) GPUs.

# Results

# Details about VCR-generated Data

In the experiment, we set the SFT dataset size $N ^ { \prime } = 6 0 , 0 0 0$ . For evaluating the training set, we require LLM to answer $t = 1 0$ times for each question. We use the following abbreviations: Aug-MATH/GSM8K refers to augmenting a single dataset, while Aug-Mix refers to augmenting both of them.

# Main Results

Data quality of VCR Since baseline methods use onetime data training for 3 epochs, to ensure a fair comparison between VCR-generated data and other methods’ data quality, we maintain the same settings and do not introduce the global iterative process of VCR in this section, i.e., setting the number of iterations $R = 1$ . We will discuss the impact of including global iterative process in the following section.

Table 1 shows that VCR consistently outperforms baselines in enhancing model performance across five different base models. When using a fixed dataset size of 60K, VCR achieves performance improvement of $5 . 6 \% - 1 0 . 3 \%$ over MetaMathQA in GSM8K, and $5 . 6 \% - 7 . 8 \%$ in MATH. For XwinMathQA, VCR shows improvements of $2 . 3 \% 4 . 5 \%$ in GSM8K and $2 . 4 \% 5 . 2 \%$ in MATH. This advantage can be attributed to the higher quality density of VCR, as it focuses on challenging questions for the model. Notably, when compared with SkyworkMathQA, which only includes AugMATH and specifically for MATH, although VCR includes Aug-Mix, its performance on MATH is comparable.

Discussion under more challenge dataset VCR emphasizes identifying and focusing more on challenging samples within the training set. Given that MATH contains more high-difficulty problems compared to GSM8K, we only perform augmentation on the MATH training set, i.e., AugMATH, we also keep the dataset size fixed at 60K.

The results in Figure 2 show that, when using only 60K of Aug-MATH data, VCR exhibits the strongest performance compared to baseline methods. The results show that VCR surpasses the current SOTA method SkyworkMathQA by $+ 1 . 5 \%$ and $+ 3 . 0 \%$ on GSM8K and MATH using the base LLaMA-2-7B model, respectively, and by $+ 1 . 3 \%$ and $+ 3 . 1 \%$ using the base Mistral-7B model.

Furthermore, we analyzed the relationship between model performance on GSM8K and MATH and the scale of SFT data when using only Aug-MATH. Figure 3 shows a performance comparison between VCR and SkyworkMathQA at different data scales. The results indicate that, with smaller data sizes, VCR performs similarly to SkyworkMathQA;

![](images/37c3b47537e83e0e68e5b991a16dbf0332327bb087ecb217646e893a8b5019e1.jpg)  
Figure 2: Model performance with various synthetic data generation methods, where SMQ denotes SkyworkMathQA

![](images/1da9b4a9f6a3629caee2b8225fe156fcd5637915ef606945121f447a03fcc0a6.jpg)  
Figure 3: Model performance with varying training data sizes. Here, $\mathrm { ~ L ~ }$ and M refer to LLaMA-2-7B and Mistral-7B, respectively, while SMQ denotes SkyworkMathQA.

however, as the data size increases, VCR demonstrates a clear advantage.

These results are consistent with the Scaling Law (Kaplan et al. 2020) and recent studies (Zeng et al. 2024; Yuan et al. 2023), which reveal that increasing data size typically leads to significant improvements in model performance. Consequently, VCR shows greater potential with larger data scales, further validating its effectiveness and superiority.

OOD results We fine-tuned LLaMA-2-7B using 60K Aug-Mix data and evaluated its performance on SVAMP, ASDiv, and GSM-Distractor, which are three out-ofdistribution datasets. Table 3 demonstrates the advantages of our method, indicating that the data constructed using the VCR method has higher quality.

# Effect of Learning by Doing

In this stage, we assign weights to each question by evaluating the model’s performance on the training set. This is based on a personalized approach, where, with a fixed amount of data, we focus on expanding those questions that are more challenging for the model. Table 2 shows the performance of LLaMA-2-7B without incorporating doing experience. In this case, we maintain a total data size of 60K but treat approximately 15K questions from the GSM8K/MATH training sets equally, expanding each question with about 4 synthetic data points using VCR. We observe a performance decline of $1 . { \bar { 8 } } \%$ and $1 . { \bar { 9 } } \%$ on GSM8K

and MATH, respectively.

Although increasing the data scale can mitigate the performance loss due to the absence of the doing experience, it comes at a higher cost. Baselines use an equal treatment strategy, which leads to the generation of a large amount of synthetic data for simpler questions, where the LLM has already achieved high accuracy.

# Effect of Learning through Observation

VCR-generated data exhibits greater richness and diversity due to the additional information introduced by its extensive educational activities. Previous research has demonstrated that diversity significantly enhances the mathematical reasoning abilities of LLMs (Zeng et al. 2024; Li et al. 2024a), a viewpoint also validated by the results in Table 2.

We do not rehash the effectiveness of common methods that involve rewriting queries or responses but instead focus on the role of the problem design mind introduced in our responses. The results in Table 2 show a performance decline when the problem design mind is removed. This is an intriguing phenomenon, indicating that problem design mind have a substantial impact. To our knowledge, we are the first to present this novel finding. The reason may be that the problem design mind stimulates the model’s creativity and stability. Even 7B-level models already possess strong mathematical capabilities but may struggle with stability (Li et al. 2024a). Problem design mind might alleviate stability issues to some extent, contributing to the enhancement of LLMs’ reasoning abilities.

# Effect of Learning through Abstractions

The results in Table 2 show that removing the third stage leads to a loss in model performance, with a decrease of approximately $0 . 6 \%$ in accuracy for both GSM8K and MATH. To further investigate the role of this stage, we also introduced VCR w/o abstraction in the OOD experiments. Table 3 lists its performance. We found that while there was little change in accuracy on SVAMP and ASDiv, VCR with the complete process exhibited greater robustness. As the number of distractors increased in the GSM-Distractor dataset, VCR’s performance degradation was significantly slower compared to VCR w/o abstraction.

<html><body><table><tr><td>Method</td><td>DataSize</td><td>GSM8K</td><td>MATH</td></tr><tr><td>LLaMA-2-7B (Touvron et al.2023)</td><td>NA</td><td>14.6</td><td>2.5</td></tr><tr><td>+MuggleMathQA† (Li et al.2024b)</td><td>82K</td><td>43.2</td><td>14.4</td></tr><tr><td>+Evol-Instruct (Luo et al. 2023)</td><td>96K</td><td>54.9</td><td>10.7</td></tr><tr><td>+LEMA (An et al. 2024)</td><td>89K</td><td>54.1</td><td>9.4</td></tr><tr><td>+MetaMathQA(Yu et al.2024)</td><td>60K</td><td>52.3</td><td>8.9</td></tr><tr><td>+XwinMathQA (Li etal.2024a)</td><td>60K</td><td>58.1</td><td>12.8</td></tr><tr><td>+SkyworkMathQA† (Zeng et al.2024)</td><td>60K</td><td>38.1</td><td>15.2</td></tr><tr><td>+VCR</td><td>60K</td><td>62.6</td><td>15.2</td></tr><tr><td>Mistral-7B (Jiang et al. 2023)</td><td>NA</td><td>52.2</td><td>13.1</td></tr><tr><td>+MetaMathQA(Yu et al.2024)</td><td>60K</td><td>64.5</td><td>19.3</td></tr><tr><td>+XwinMathQA (Li etal.2024a)</td><td>60K</td><td>67.9</td><td>22.8</td></tr><tr><td>+SkyworkMathQA† (Zeng et al. 2024)</td><td>60K</td><td>67.6</td><td>27.8</td></tr><tr><td>+VCR</td><td>60K</td><td>70.5</td><td>27.1</td></tr><tr><td>LLaMA-3-8B (Dubey et al.2024)</td><td>NA</td><td>59.9</td><td>21.4</td></tr><tr><td>+MetaMathQA (Yu et al. 2024)</td><td>60K</td><td>65.4</td><td>24.2</td></tr><tr><td>+XwinMathQA (Li etal.2024a)</td><td>60K</td><td>68.9</td><td>26.8</td></tr><tr><td>+VCR</td><td>60K</td><td>71.1</td><td>30.1</td></tr><tr><td>LLaMA-2-13B (Touvron et al.2023)</td><td>NA</td><td>28.7</td><td>3.9</td></tr><tr><td>+Evol-Instruct (Luo et al.2023)</td><td>96K</td><td>63.9</td><td>14.0</td></tr><tr><td>+LEMA (An et al. 2024)</td><td>89K</td><td>65.7</td><td>12.6</td></tr><tr><td>+MetaMathQA(Yu etal.2024)</td><td>60K</td><td>61.3</td><td>11.6</td></tr><tr><td>+XwinMathQA (Li etal.2024a)</td><td>60K</td><td>64.9</td><td>13.9</td></tr><tr><td>+VCR</td><td>60K</td><td>68.0</td><td>19.1</td></tr><tr><td>LLaMA-2-70B (Touvron et al.2023)</td><td>NA</td><td>56.8</td><td>13.5</td></tr><tr><td>+Evol-Instruct (Luo et al. 2023)</td><td>96K</td><td>81.6</td><td>22.7</td></tr><tr><td>+LEMA (An et al. 2024)</td><td>89K</td><td>83.5</td><td>25.0</td></tr><tr><td>+MetaMathQA(Yu etal.2024)</td><td>60K</td><td>79.9</td><td>22.4</td></tr><tr><td>+XwinMathQA (Li et al.2024a)</td><td>60K</td><td>82.1</td><td>23.2</td></tr><tr><td>+VCR</td><td>60K</td><td>85.5</td><td>28.0</td></tr></table></body></html>

Table 1: The performances of LLMs on GSM8K and MATH with approximately 60K SFT data after training 3 epochs. Bold highlights the optimal performance with the data size limit. Results obtained from training primarily on Aug-Mix, with some exceptions indicated by †, which represent results trained on Aug-MATH.   
Table 2: Ablation studies with different experience types in VCR. Here we use LLaMA-2-7B as the backbone model and finetune it with 60K VCR-generated Aug-Mix data.   

<html><body><table><tr><td rowspan="2">Method</td><td colspan="3">ExperienceType</td><td rowspan="2">GSM8K MATH</td><td rowspan="2"></td></tr><tr><td></td><td>Doing Observation Abstraction</td><td></td></tr><tr><td>SFT</td><td>X</td><td>X</td><td>X</td><td>41.6</td><td>4.7</td></tr><tr><td rowspan="4">VCR</td><td>X</td><td>√</td><td>√</td><td>60.8</td><td>13.3</td></tr><tr><td>√</td><td>X</td><td>√</td><td>61.5</td><td>13.9</td></tr><tr><td>√</td><td>√</td><td>×</td><td>62.0</td><td>14.6</td></tr><tr><td>√</td><td>√</td><td>√</td><td>62.6</td><td>15.2</td></tr></table></body></html>

# Effect of Global Iterative Process

Previous analysis has demonstrated that the data generated by VCR has higher quality, but it only considered a single generation( $\left. R = 1 \right.$ ). Therefore, in this section, we consider the global iterative process with $R = 3$ ). For $\operatorname { V C R } ^ { R = 1 }$ , we fine-tuned LLaMA-2-7B with 3 epochs in a single run, We saved the checkpoints after each epoch and evaluated them. For $\mathrm { V C R } ^ { R = 3 }$ , we fine-tune 3 times, each with only 1 epoch. After each fine-tuning, we save the checkpoints and evaluate performance on the test set, while also examining the model’s performance on the training set in the Doing Stage. This allows us to adjust the composition of the SFT data for the next epoch, with each epoch fixed at 60K training data.

Table 3: OOD performance of LLaMA-2-7B after SFT on different Aud-Mix data. Here, GSM-Dis. refers to the GSMDistractors dataset, with numbers 1, 2, and 3 indicating the number of distractors.   

<html><body><table><tr><td rowspan="2" colspan="2">Method SVAMP ASDiv</td><td rowspan="2"></td><td colspan="2">GSM-Dis.</td></tr><tr><td>1</td><td>2 3</td></tr><tr><td>Evol-Instruct</td><td>57.3</td><td>59.1</td><td>46.640.1 33.4</td><td></td></tr><tr><td>LEMA</td><td>54.1</td><td>65.5</td><td>45.438.9 32.4</td><td></td></tr><tr><td>MetaMathQA</td><td>58.2</td><td>63.6</td><td>43.9 37.6 32.4</td><td></td></tr><tr><td>XwinMathQA</td><td>64.1</td><td>67.4</td><td>48.5 39.8 33.6</td><td></td></tr><tr><td>VCRw/o Abstraction VCR</td><td>65.2 66.5</td><td>67.1 68.9</td><td>50.0 42.2 34.8 53.2</td><td>45.8 38.9</td></tr></table></body></html>

Table 4: The performance of LLaMA-2-7B over 3 epochs. $\operatorname { V C R } ^ { R = 1 }$ : training 3 epochs in a single run, data remains consistent across epochs; $\mathrm { V C R } ^ { R = 3 }$ : Add global iterative process, training 3 times with one epoch each time, the SFT data differs in each training.   

<html><body><table><tr><td rowspan="2">Epoch</td><td colspan="2">VCRR=1</td><td colspan="2">VCRR=3</td></tr><tr><td>GSM8K</td><td>MATH</td><td>GSM8K</td><td>MATH</td></tr><tr><td>1</td><td>60.0</td><td>13.6</td><td>60.0 (+0)</td><td>13.6 (+0)</td></tr><tr><td>2</td><td>60.9</td><td>14.3</td><td>62.2 (+1.3)</td><td>15.7 (+1.4)</td></tr><tr><td>3</td><td>62.6</td><td>15.2</td><td>66.9 (+4.3)</td><td>17.8 (+2.6)</td></tr></table></body></html>

Results in Table 4 show that $\operatorname { V C R } ^ { R = 3 }$ achieved gradual performance improvement across the 3 epochs, showing better results compared to directly training 3 epochs with 60K VCR data $( + 4 . 3 \%$ in GSM8K, $+ 2 . 6 \%$ in MATH). This suggests that adjusting the SFT data composition based on model performance on the training set allows the model to focus on more challenging problems, further validating the effectiveness of the Doing stage. It is worth noting that although the data used across the 3 epochs is not completely different, there is a significant overlap, with a total of approximately 71K unique samples after de-duplication.

# Conclusion

In this paper, we propose VCR, a virtual learning environment simulated using LLM-powered agents. VCR enhances public mathematical datasets with targeted improvements. Through extensive experiments, we demonstrate that, with the same scale of data, the data generated by VCR can more effectively improve the mathematical reasoning performance of LLMs compared to previous methods. Unlike earlier research that focuses on redesigning prompts to augment queries or answers, VCR is based on the “Cone of Experience” educational theory. It parallels LLM learning with human learning processes, collecting the multi-level experience required for LLM learning through a simulated learning environment, including lecture, discussion, problem design, and problem-solving. We also make a new discovery, indirect data, such as question design mind, which positively impacts model performance. This not only supports the feasibility of analogizing LLM learning to human learning but also provides a new perspective for future research.