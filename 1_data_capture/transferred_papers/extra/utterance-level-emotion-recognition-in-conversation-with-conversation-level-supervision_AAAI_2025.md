# Utterance-level Emotion Recognition in Conversation with Conversation-level Supervision

Ximing $\mathbf { L i } ^ { 1 , 2 }$ , Yuanchao Dai1,2, Zhiyao $\mathbf { Y a n g } ^ { 1 , 2 * }$ , Jinjin Chi1,2, Wanfu Gao1,2, Lin Yuanbo Wu3

1College of Computer Science and Technology, Jilin University, China 2Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, China 3 Swansea University, United Kingdom {liximing86, yuanchaodai, yangzy9529, chijinjin616} $@$ gmail.com, gaowf $@$ jlu.edu.cn, l.y.wu@swansea.ac.uk

# Abstract

Emotion Recognition in Conversations (ERC) involves automatically identifying the emotion of each utterance in conversations. The emotion of an utterance is contingent to the conversation context, and thus, annotating each utterance in ERC entails repetitive screening the whole conversation from annotators. Such a requirement leads to prohibitive cost in fine-grained labeling on utterance. In this paper, we propose an efficient coarse-grained labeling strategy for ERC, which assigns a set of emotions for each conversation. In specific, we reformulate the ERC predictors with conversation-level emotion sets as weakly-supervised learning to optimise a potential candidate for ERC, which is termed as Dataless ERC (DERC). To validate this, we propose a simple-yet-flexible DERC framework with Progressive Learning (DERC-PL). We jointly update pseudo-utterance-level emotions and the ERC predictor in a self-training manner, where we progressively update the ERC predictor from training subsets with lower noise densities to the ones with higher noise densities. We implemented several versions of DERC-PL by incorporating various off-the-shelf ERC methods. Extensive experimental results demonstrate that the proposed DERC-PL can be on par with existing weakly-supervised learning baselines and supervised learning ERC methods.

# Introduction

Emotion Recognition in Conversations (ERC) refers to the task of identifying emotions, such as sadness, happiness, or anger within conversations, which specifically focuses on the emotion conveyed in each utterance (Zhang, Chen, and Chen 2023; Hu et al. 2023; Li et al. 2023; Qin et al. 2023; Zhang et al. 2023b). Recently, ERC has garnered significant attention from the natural language processing community due to its applicability in diverse real-world scenarios, including medical conversation analysis (Barnes 2019; Priya, Firdaus, and Ekbal 2023), social media analysis (Brambilla et al. 2022), and dialogue system construction (Ma et al. 2020; Liu et al. 2021b), among others.

Typically, addressing ERC requires the collection of finegrained training datasets where each utterance within a conversation is manually labeled. However, unlike traditional emotion recognition (Mittal et al. 2020; Alhuzali and Ananiadou 2023; Ouyang et al. 2024), the emotion of an utterance in ERC is heavily influenced by the conversational context (Zhang et al. 2023a). As shown in Fig. 1, the same utterance may convey different emotions depending on the context, necessitating annotators to repeatedly review the entire conversation. This makes the fine-grained labeling process highly time-consuming. According to labeling statistics from ERC studies (Zahiri and Choi 2018; Chen et al. 2018; Poria et al. 2019) and our preliminary experiments, we estimate that the average time required to label a single utterance in ERC is approximately 10 seconds, which can be prohibitively expensive for certain real-world applications.

Inspired by the cognitive observation that humans can rapidly grasp emotions from lengthy texts even with quick skimming (Duggan and Payne 2011), we propose a more efficient labeling approach for ERC. Instead of labeling each utterance individually, we suggest that annotators assign a set of emotions to each conversation. To validate this approach, we conducted preliminary experiments, manually labeling conversation-level emotion sets across benchmark ERC datasets. The results indicate that this method maintains accuracy while significantly enhancing efficiency, reducing the average labeling time for a conversation with 5- 10 utterances to just 13 seconds. This leads us to pose the question “Is it possible, and if so, how, to utilize these efficient yet coarse-grained datasets with conversationlevel emotion sets to train ERC predictors that can accurately identify utterance-level emotions in a weaklysupervised manner”. We formally introduce this new weakly-supervised learning task as Dataless ERC (DERC), and demonstrate the distinction between conventional ERC and DERC through a comparative example in Fig. 1.

To address this question, we present, to the best of our knowledge, the first attempt to tackle DERC. Drawing inspiration from prior weakly-supervised learning techniques (Liu et al. 2021a; Matsuo et al. 2023; Li et al. 2024; Yang et al. 2023), we propose a straightforward solution based on pseudo-labeling. Specifically, we initialize pseudoutterance-level emotions using conversation-level emotion sets and iteratively update them alongside the ERC predictor in a self-training framework. Typically, pseudo-utterancelevel emotions are refined based on the current predictions of the ERC predictor. To enhance accuracy, we divide all

ERC: Utterance-level Emotion   
I've been planning very much for my Neutral Neutral What is our vacation plan?   
upcoming trip.   
Really? Is going shopping part of the Surprise Neutral I plan to stay at home.   
plan?   
Of course, shopping is a top priority. Joy Anger What? Why not go for a trip, it's the only vacation we have this year. Don't you happy with lying on the couch   
I am so happy. Joy Neutral watching TV?   
tYhiesahf,orI avelobnegentilmoeo.king forward to Joy Disgust I am so happy. {Neutral, Surprise, Joy} DERC: Conversation-level {Neutral, Anger, Disgust} Emotion Set

training conversations into subsets categorized by noise density, determined by the number of emotions assigned within each conversation-level emotion set. We then progressively train the ERC predictor, starting with subsets of lower noise density and gradually moving to those with higher noise, thereby achieving more precise pseudo-utterance-level emotions during the initial training stages. Building on these ideas, we propose a simple yet flexible DERC framework called Progressive Learning (DERC-PL), within which most existing ERC methods can be directly integrated as ERC predictors.

We have developed several versions of DERC-PL by incorporating various ERC methods, and our extensive experimental results demonstrate that DERC-PL not only surpasses existing weakly-supervised learning baselines but also competes closely with supervised ERC methods. Importantly, our empirical findings suggest that the coarse-grained DERC may serve as a viable alternative to traditional fine-grained ERC approaches. The key contributions of this paper are as follows:

• We evaluate the accuracy and efficiency of manually labeling conversation-level emotion sets and introduce a new weakly-supervised learning task for ERC, termed DERC.   
• We propose a simple yet flexible DERC framework named DERC-PL, which progressively learns from training subsets with varying noise densities.   
• We conduct extensive experiments to validate the effectiveness of DERC-PL and provide empirical evidence that coarse-grained DERC can be a strong candidate for finegrained ERC.

# Manual Labeling Evaluations of ERC and DERC

In this section, we conduct preliminary experiments to empirically evaluate the accuracy and efficiency of manual labeling for ERC and DERC. To this end, we invited $2 0 \mathrm { u n } .$ - dergraduates from diverse backgrounds at our university as volunteers and evenly divided them into 2 groups to separately conduct manual labeling for ERC and DERC. In the following, we introduce the manual labeling rules and empirical results.

Manual labeling rules. Referring to (Zahiri and Choi 2018; Chen et al. 2018; Poria et al. 2019), we build the labeling rules of ERC and DERC for annotators. We employ the benchmark dataset MELD (Poria et al. 2019), and divide it into 4 sets of conversations whose utterance numbers belong to [1, 5], [6, 10], [11, 15], and [16,20], respectively. We randomly select 50 conversations from each set, leading to a collection of 200 conversations containing 1,964 utterances for manual labeling evaluation. The labeling range (i.e., the label space $y$ ) contains 7 emotions, including Neutral, Joy, Sadness, Fear, Anger, Disgust, Surprise . In terms of ERC and DERC, annotators are asked to assign emotions and emotion sets to utterances and conversations, respectively. They should meet the required labeling type, labeling order, and labeling demand. We build rules of golden label to determine the assigned emotions, and also rules of conflict handling to deal with controversial utterances and conversations. All details of labeling rules are listed in Table 1.

Empirical results. First, we evaluate the labeling accuracy of our emotion annotations. We apply the original utterance-level emotions of MELD as the ground-truth emotions, and add them into the corresponding conversationlevel emotion sets as the ground-truth emotion sets. The labeling accuracy scores of ERC and DERC are defined as the proportions of our emotion annotations matching groundtruth emotions and ground-truth emotion sets, respectively. The results are shown in Table 2. We can observe that the labeling accuracy scores of both ERC and DERC are above $90 \%$ , indicating the emotion annotations from our evaluations are almost consistent with the ones from the public MELD dataset. That is, our manual labeling results are convincing so that the corresponding efficiency results can be believable. Further, we evaluate the labeling efficiency of our emotion annotations. We present the average labeling time, including labeling and conflict handling time, for one utterance and conversation, respectively. As shown in Table 2, we can observe that labeling conversation-level emotion sets is as efficient as labeling utterance-level emotions. Specifically, the time cost of labeling conversations containing $1 { \sim } 5$ and $6 { \sim } 1 0$ utterances is almost the same as the time cost of labeling utterances; and the time cost of labeling conversations containing $1 1 { \sim } 1 5$ and $1 6 { \sim } 2 0$ utterances is only about $2 \sim 3$ times than that of labeling utterances. In summary, the empirical results indicate that one can collect accurate and efficient conversation-level emotion sets, to support the potential task of DERC.

Table 1: Details of labeling rules for ERC and DERC.   

<html><body><table><tr><td colspan="2">ERC rule</td><td>DERC rule</td></tr><tr><td>labeling range</td><td>Angtr,lDisgust,adurpri Fear,</td><td>Aeutral,isgust,aduepr, Fear,</td></tr><tr><td>label type</td><td>One emotion label for each utterance.</td><td>One emotion set for each conversation.</td></tr><tr><td>labeling order</td><td>From short conversations to long conversations.</td><td>From short conversations to long conversations.</td></tr><tr><td>labeling demand</td><td>Thinking at least 3 seconds before assigning the emotion.</td><td>Reading the conversation at least 3 times before as- sign the emotion set.</td></tr><tr><td>golden label</td><td>For each utterance,we count the assigned numbers of emotions by annotators.If the majority emotion is assigned by over half of the annotators,it will be the assigned emotion finally. Otherwise,the ut- terance will be marked with“conflict".</td><td>For each conversation,we count the assigned num- bers of emotions by annotators.If any emotion is as- signed by over half of the annotators,it will be added to the emotion set.If the emotion set is empty, the conversation will be marked with“conflict".</td></tr><tr><td>conflict handling</td><td>For each utterance marked with “conflict”, we revise the labeling range to the top-3 voted emotions and then re-label it. If the re-labeling result cannot meet the rule of golden label,we will drop this utterance.</td><td>For each conversation marked with “conflict",we re- label it over all assigned emotions.If the re-labeling result cannot meet the rule of golden label,we will drop this conversation.</td></tr></table></body></html>

# The Proposed DERC-PL

In this section, we detail the proposed DERC framework named DERC-PL.

Definition of DERC. Formally, the training dataset of DERC is composed of $N$ coarse-grained labeled samples $\mathcal { D } = \{ ( \mathcal { C } _ { i } , \mathbf { y } _ { i } ) \} _ { i = 1 } ^ { i = N }$ , where each $\mathcal { C } _ { i }$ denotes a conversation, and $\mathbf { y } _ { i } \in \{ 0 , 1 \} ^ { | \mathcal { V } | }$ is its corresponding emotion set. We review that $y$ denotes the emotion space. To be specific, each $\mathcal { C } _ { i }$ cch stis tohfe $M$ wrocuondtsenotfbut eitrsangcreosu $\{ \mathbf { u } _ { i j } \} _ { j = 1 } ^ { j = M }$ m,1otiwohneries $\mathbf { u } _ { i j }$   
unknown. The goal of DERC is to apply the coarse-grained training dataset $\mathcal { D }$ to induce an ERC predictor $\mathcal { F } _ { \theta } ( \cdot )$ , which can predict the emotion of each utterance in future conversations.

Table 2: Accuracy and time cost (seconds) of manual labeling evaluations. TimeUtt. and $\mathbf { T i m e } _ { \mathbf { C o n } . [ \mathbf { a } , \mathbf { b } ] }$ denote the average labeling time for one utterance and one conversation whose utterance number belongs to [a,b], respectively.   

<html><body><table><tr><td></td><td>ERC</td><td>DERC</td></tr><tr><td>Accuracy</td><td>92.9%</td><td>94.5%</td></tr><tr><td>TimeUtt.</td><td>10s~15s</td><td>1</td></tr><tr><td>TimeCon.[1,5]</td><td>1</td><td>5s~ 11s</td></tr><tr><td>TimeCon.[6,10]</td><td>1</td><td>10s~ 16s</td></tr><tr><td>TimeCon.[11,15]</td><td>1</td><td>18s~ 25s</td></tr><tr><td>TimeCon.[16,20]</td><td>1</td><td>31s~ 44s</td></tr></table></body></html>

# DERC-PL Synopsis

Our DERC-PL is based on pseudo-labeling. For each utterance $\mathbf { u } _ { i j }$ , we initialize its pseudo-utterance-level emotion $\hat { \mathbf { y } } _ { i j } \in [ \bar { 0 } , 1 ] ^ { | \mathcal { V } | }$ using the emotion set $\mathbf { y } _ { i }$ of the conversation from which it comes:

$$
\hat { \bf y } _ { i j } = \frac { { \bf y } _ { i } } { \vert { \bf y } _ { i } \vert } , i = [ N ] , j = [ M ] ,
$$

where $\left| \mathbf { y } _ { i } \right|$ denotes the number of assigned emotions in $\mathbf { y } _ { i }$ ; and $\begin{array} { r } { \sum _ { h = 1 } ^ { | \mathcal { V } | } \hat { \mathbf { y } } _ { i j h } = 1 } \end{array}$ . Therefore, we can initialize a pseudoERCPdataset $\mathbf { \hat { \mathcal { D } } } = \{ ( \mathcal { C } _ { i } , \{ \hat { \mathbf { y } } _ { i j } \} _ { j = 1 } ^ { j = M } ) \} _ { i = 1 } ^ { i = N }$ . We begin with $\hat { \mathcal { D } }$ and jointly update $\hat { \mathbf { y } }$ and a base ERC predictor $\mathcal { F } _ { \theta } ( \cdot )$ , parameterized by $\theta$ , in a self-training manner. We update each $\hat { \mathbf { y } } _ { i j }$ by using the predictions of the current ERC predictor $\mathbf { p } _ { i j } ^ { \prime } ~ = ~ \mathcal { F } _ { \theta } ( \mathbf { u } _ { i j } )$ . To achieve more precise $\hat { \mathbf { y } }$ at the early training periods, we divide $\hat { \mathcal { D } }$ into several training subsets with different noise densities, and then progressively update $\mathcal { F } _ { \theta } ( \cdot )$ from training subsets with lower noise densities to the ones with higher noise densities. In the following, we introduce the processes of generating training subsets and progressive updating in more detail.

# Generating Training Subsets

As each pseudo-utterance-level emotion $\hat { \mathbf { y } } _ { i j }$ must be covered by its corresponding conversation-level emotion set $\mathbf { y } _ { i }$ , we can use the number of assigned emotions $\left| \mathbf { y } _ { i } \right|$ to express the noise densities of utterances from $\mathcal { C } _ { i }$ . Accordingly, we can divide $\hat { \mathcal { D } }$ into several disjoint training subsets $\hat { \mathcal { D } } = \hat { \mathcal { D } } _ { 1 } \cup \dots \cup \hat { \mathcal { D } } _ { | \mathcal { y } | }$ ,2 where each subset $\hat { \mathcal { D } } _ { g }$ is defined by:

$$
\begin{array} { r } { \hat { \mathcal { D } } _ { g } = \{ ( \mathcal { C } _ { i } , \{ \hat { \mathbf { y } } _ { i j } \} _ { j = 1 } ^ { j = M } ) \} _ { i = 1 } ^ { i = N _ { g } } , \quad \forall | \mathbf { y } _ { i } | = g , } \end{array}
$$

where $N _ { g }$ is the number of conversations in this subset. To some extent, the noise density corresponds to the learning difficulty.

# Progressive Updating

Given the training subsets, we progressively update the pseudo-utterance-level emotions $\hat { \mathbf { y } }$ and the ERC predictor $\mathcal { F } _ { \theta } ( \cdot )$ jointly from $\hat { \mathcal { D } } _ { 1 }$ to $\hat { \mathcal { D } } _ { | \mathcal { Y } | }$ . Specifically, we perform the following learning process:

1: Initialize a training pool $\tilde { \mathcal { D } }$ by $\hat { \mathcal { D } } _ { 1 }$ , and jointly update $\hat { \mathbf { y } }$ and $\mathcal { F } _ { \theta } ( \cdot )$ by using $\tilde { \mathcal { D } }$ wi e $T$ epochs.   
2: For $t = 2 , \cdots , | \mathcal { V } |$   
3: Add $\hat { \mathcal { D } } _ { t }$ into $\widetilde { \mathcal { D } } = \widetilde { \mathcal { D } } \cup \hat { \mathcal { D } } _ { t }$   
4: Continue to jeoint ye update $\hat { \mathbf { y } }$ and $\mathcal { F } _ { \theta } ( \cdot )$ by using $\tilde { \mathcal { D } }$ with $T$ epochs.

Given any $\widetilde { \mathcal { D } }$ , we apply the following training objective with respect  ey and $\theta$ :

$$
\mathcal { L } ( \widetilde { D } ; \hat { \mathbf { y } } , \theta ) = - \frac { 1 } { | \widetilde { D } | M } \sum _ { i = 1 } ^ { | \widetilde { D } | } \sum _ { j = 1 } ^ { M } ( 1 - \mathbf { p } _ { i j } ) ^ { \gamma } \ell _ { c e } ( \mathbf { p } _ { i j } , \hat { \mathbf { y } } _ { i j } ) ,
$$

where $\ell _ { c e }$ is the cross-entropy loss; and $\gamma$ is a hyperparameter used to mitigate the data imbalance (Lin et al. 2017). In terms of the parameter $\theta$ , we can update it by using gradient-based methods. We update $\hat { \mathbf { y } } _ { i j }$ by refining the current predictions $\mathbf { p } _ { i j }$ as follows:

$$
\begin{array} { r } { \hat { \mathbf { y } } _ { i j } = \left\{ \begin{array} { l l } { \frac { \hat { \mathbf { p } } _ { i j } \circ \mathbf { y } _ { i } } { \vert \hat { \mathbf { p } } _ { i j } \circ \mathbf { y } _ { i } \vert _ { 1 } } , } & { \mathrm { i f ~ } \mathrm { m a x } ( \frac { \hat { \mathbf { p } } _ { i j } \circ \mathbf { y } _ { i } } { \vert \hat { \mathbf { p } } _ { i j } \circ \mathbf { y } _ { i } \vert _ { 1 } } ) > \alpha } \\ { \quad \hat { \mathbf { y } } _ { i j } , } & { \mathrm { o t h e r w i s e } } \end{array} \right. } \end{array}
$$

where $\scriptscriptstyle \mathrm { ~ o ~ }$ denotes the operation of element-wise product; $| \cdot | _ { 1 }$ is the $\ell _ { 1 }$ norm; $\operatorname* { m a x } ( \cdot )$ is the maximum operation for a vector; $\alpha$ is the confidence threshold; and $\hat { \mathbf { p } } _ { i j }$ is the sharpened version of $\mathbf { p } _ { i j }$ computed with a temperature parameter $\tau$ as follows:

$$
\hat { \mathbf { p } } _ { i j } = \frac { \mathbf { p } _ { i j } ^ { 1 / \tau } } { \sum _ { k = 1 } ^ { | \mathcal { V } | } \mathbf { p } _ { i k } ^ { 1 / \tau } } .
$$

The above updating strategy with respect to $\hat { \mathbf { y } }$ indicates that (1) each of $\hat { \mathbf { y } } _ { i j }$ must be covered by $\mathbf { y } _ { i }$ , and (2) only the high-confidence prediction $\mathbf { p } _ { i j }$ , measured by $\alpha$ , will be used to update $\hat { \mathbf { y } } _ { i j }$ . In DERC-PL, we can apply any off-the-shelf ERC method as the base ERC predictor $\mathcal { F } _ { \theta } ( \cdot )$ . The computational steps of DERC-PL is outlined in Algorithm 1.

Input: Training dataset $\mathcal { D }$ , parameters $\gamma$ , $\alpha$ , number of epochs $T$ .   
Output: An ERC predictor. 1: Apply an off-the-shelf ERC method as the base ERC predictor $\mathcal { F } _ { \theta } ( \cdot )$ 2: $\theta \gets$ Initialize the predictor parameter randomly   
3: $\hat { \mathbf { y } } \gets$ Initialize the pseudo-utterance-emotions by Eq. 1   
4: $\{ \hat { \mathcal { D } } _ { g } \} _ { g = 1 } ^ { g = | \mathcal { V } | }  \mathrm { I }$ Divide $\hat { \mathcal { D } }$ into training subsets   
5: $\tilde { \mathcal { D } } \gets$ Initialize a training pool by $\hat { \mathcal { D } } _ { 1 }$   
6: Uepdate $\theta$ with $\tilde { \mathcal { D } }$ by minimizing Eq.3 over $T$ epochs 7: Update $\hat { \mathbf { y } }$ by Eqe.4 per-epoch   
8: for $t \in 2 , \ldots , | \mathcal { V } |$ do   
9: $\widetilde { \mathcal { D } } \gets \mathrm { A d d } \hat { \mathcal { D } } _ { t }$ into $\tilde { \mathcal { D } }$   
10: Uepdate $\theta$ by $\widetilde { \mathcal { D } }$ by meinimizing Eq.3 over $T$ epochs   
11: Update $\hat { \mathbf { y } }$ by Eeq.4 per-epoch   
12: end for

# Related Works Emotion Recognition in Conversations

The primary challenge of ERC is how to capture the contextual information of conversations. The body of ERC methods can be categorised into three streams: content-based, knowledge-assisted, and relation-based methods. The idea of content-based methods is straightforward, where, as the name suggests, they integrate utterances with the historical content from the same conversation before feeding them into various text encoders (Jiao et al. 2019; Lu et al. 2020; Li et al. 2020; Hu, Wei, and Huai 2021; Tu et al. 2022; Song et al. 2022; Zhang et al. $2 0 2 3 \mathrm { a }$ ; Wei et al. 2023). In parallel, the knowledge-assisted methods promote utterance representations by applying external knowledge tools such as knowledge bases (Zhong, Wang, and Miao 2019; Zhang et al. 2020; Ghosal et al. 2020; Jiang et al. 2022; Li et al. 2023)), and pieces of auxiliary information such as speaker background (Majumder et al. 2019; Chen et al. 2023; Hu et al. 2023; Zhang et al. 2023b) and discourse role (Ong et al. 2022)). Additionally, the relation-based methods capture the contextual information of conversations by transforming each conversation into a graph, whose nodes are utterances and edges are generated by various relations and dependencies, e.g., speaker-utterance relations (Song et al. 2023), speaker dependency (Ghosal et al. 2019; Ishiwatari et al. 2020; Zhang, Chen, and Chen 2023), and discourse dependency (Zhang, Chen, and Chen 2023). They then apply graph neural networks to generate discriminative utterance representations.

The aforementioned ERC methods are all built on finegrained training datasets with utterance-level emotions, which are expensive to collect. In contrast, in this work, we investigate DERC, a weakly-supervised learning task of ERC, and suggest a new DERC framework named DERCPL, which is built on coarse-grained training datasets with conversation-level emotion sets. Despite its superior performance, DERC-PL can be treated as an efficient candidate for ERC methods in real-world scenarios.

# Weakly-supervised Learning with Bag-level Supervision

To some extent, DERC can be considered a special case of Multi-Instance Multi-Label learning (MIML) (Zhou et al. 2012), a prevalent paradigm of weakly-supervised learning in which the bag of instances is associated with a bag-level label set, instead of instance-level labels. Early MIML methods use traditional machine learning methods such as ensembling (Wu, Huang, and Zhou 2014), boosting (Zhang and Zhang 2006) and maximum margin (Zhang and Zhou 2008). Recently, MIML works utilize many cutting-edge technologies such as self-training (Wang et al. 2023) and contrastive learning (Liu et al. 2023). MIML has been applied to many fields such as sentiment analysis (Li et al. 2020; Ji et al. 2020; Ouyang et al. 2024; Yang et al. 2023), offensive detection (Liu et al. 2022) and relation extraction (Surdeanu et al. 2012).

In parallel, another relevant paradigm of weaklysupervised learning to DERC is Learning from Label Proportions (LLP), where the bag of instances is associated with the bag-level proportion of labels. They mainly concentrate on the bag-level learning objectives, such as proportion loss (Liu et al. 2021a), forward correction loss (Zhang, Wang, and Scott 2022), consistency regularization (Tsai and Lin 2020), contrastive loss (Yang, Zhang, and Lam 2021) and so on (Dulac-Arnold et al. 2019). Such methods integrate instance-level prediction results into bag-level outputs and calculate the empirical risk loss with truth bag-level label proportion.

In contrast to MIML and LLP, one distinction of DERC is that each conversation can be considered as a coupled bag of utterances, i.e., the utterances of one conversation are dependent. Besides, in many real-world scenarios, the training dataset of DERC may contain auxiliary information such as speakers’ backgrounds, which can be leveraged to promote the predictor induction.

# Experiments

In this section, we conduct experiments to evaluate DERCPL, and attempt to answer the following questions:

$\varrho \boldsymbol { \imath } : \mathrm { C a n }$ DERC-PL compete with the existing weaklysupervised learning methods in DERC settings?   
$\textstyle Q 2 : \operatorname { C a n }$ DERC-PL compete with the existing supervised learning ERC methods?

# Experimental Settings

Datasets. We employ three benchmark ERC datasets: MELD (Poria et al. 2019), IEMOCAP (Busso et al. 2008), and EmoryNLP (Zahiri and Choi 2018). Statistics of these datasets are listed in Table 3. For each dataset, we generate its DERC version by directly adding the utterance-level emotions into the corresponding conversation-level emotion sets.

Baseline methods. We consider four ERC methods as the base ERC predictors, including naive $\mathbf { B E R T _ { b a s e } + M L P }$ , RGAT (Ishiwatari et al. 2020), SACL (Hu et al. 2023), and DualGATs (Zhang, Chen, and Chen 2023). To evaluate the performance on DERC datasets, each of the ERC methods is integrated with three weakly-supervised learning methods including PLOT (Liu et al. 2021a), ItS2CLR (Liu et al. 2023), and the proposed DERC-PL. Besides, to address $Q 2$ , we supplement each ERC method with an additional supervised learning scenario under ERC datasets for comparison.

Implementation details. Our experiments are conducted on Ubuntu 20.04 with a single RTX-4090 GPU with 24G memory. For BERT-based methods (i.e., $\mathrm { B E R T _ { b a s e } + M L P } ,$ RGAT (Ishiwatari et al. 2020)) / RoBERTa-based methods (i.e., SACL (Hu et al. 2023), DualGAT (Zhang, Chen, and Chen 2023)), we use the AdamW optimizer (Loshchilov and Hutter 2019), with learning rates of $2 e ^ { - 5 }$ and $1 e ^ { - 4 }$ , respectively. The layer dropout rate, batch size, and the number of epochs $T$ are configured to $0 . 1 / 0 . 2 , 1 6 / 1 6$ , and $3 0 / 2 0$ , respectively. The hyperparameter $\alpha$ is adjusted to 0.8 for IEMOCAP (Busso et al. 2008), 0.3 for EmoryNLP (Zahiri and Choi 2018), and 0.4 for MELD (Poria et al. 2019). The pre-trained BERTbase backbone can be downloaded from Huggingface.3

In terms of the two weakly supervised learning baselines PLOT (Liu et al. 2021a) and ItS2CLR (Liu et al. 2023), we effectuate modifications to fit DERC. Specifically, since PLOT requires the conversation-level proportion of emotions, we calculate the proportion by aggregating the utterance-level emotions for each conversation. 4 For ItS2CLR, we first conduct a warm-up stage using the training subset $\hat { \mathcal { D } } _ { 1 }$ , followed by updating the predictor with the entire training dataset. As an MIML method, ItS2CLR outputs the emotion with the highest predicted score.

Evaluation metrics. Inspired by (Zhang and Zhou 2013), we employ two widely-used binary-based metrics to measure the performance, including Macro Averaging F1 (Macro-F1) and Micro Averaging F1 (Micro-F1). In terms of all baseline methods, we run their source codes 5 times for each dataset and report their average results.

# Main Results

Results for $\varrho \boldsymbol { I }$ . Table 4 presents the empirical comparison results between two weakly supervised methods (i.e., PLOT and ItS2CLR) and our proposed DERC-PL across the three benchmark datasets, using four different base ERC predictors (i.e., $\mathbf { B E R T _ { b a s e } + M L P }$ , RGAT, SACL, and DualGATs). We can observe that DERC-PL consistently outperforms them with all base ERC predictors across all datasets, where the performance gain is about $3 \% \sim 6 \%$ . One exception is in EmoryNLP, where PLOT outperforms our method over Micro-F1 evaluation metric using $\mathrm { B E R T _ { b a s e } + M L P }$ ERC predictor. This likely stems from the severe imbalance in the EmoryNLP dataset, and an excessively high proportion of Neutral emotions can adversely affect the calculation of Micro-F1, which is sensitive to category distribution. Furthermore, the $t$ -test $\mathrm { W u }$ and Zhang 2018) at 0.05 significance level is conducted to analyze whether DERC-PL achieves statistically superior performance to other weaklysupervised methods.

Table 3: The statistics of benchmark datasets.   

<html><body><table><tr><td rowspan="2">Dataset</td><td colspan="4">Conversation</td><td colspan="4">Utterance</td></tr><tr><td>Total</td><td>Train</td><td>Validation</td><td>Test</td><td>Total</td><td>Train</td><td>Validation</td><td>Test</td></tr><tr><td>IEMOCAP</td><td>151</td><td>120</td><td></td><td>31</td><td>7,433</td><td>5,810</td><td></td><td>1,623</td></tr><tr><td>EmoryNLP</td><td>827</td><td>659</td><td>89</td><td>79</td><td>9,489</td><td>7,551</td><td>95</td><td>984</td></tr><tr><td>MELD</td><td>1,432</td><td>1,039</td><td>114</td><td>280</td><td>13,708</td><td>9,989</td><td>1,109</td><td>2,610</td></tr></table></body></html>

Table 4: Empirical results of Marco-F1 and Micro-F1 on benchmark datasets, where $\bullet / \circ$ indicates whether DERC-PL is significantly superior/inferior to one weakly-supervised learning baseline via paired $t$ -test at 0.05 significance level. The best scores among weakly-supervised learning methods are indicated in bold.   

<html><body><table><tr><td rowspan="2">Method</td><td colspan="2">IEMOCAP</td><td colspan="2">EmoryNLP</td><td colspan="2">MELD</td></tr><tr><td>Micro-F1</td><td>Macro-F1</td><td>Micro-F1</td><td>Macro-F1</td><td>Micro-F1</td><td>Macro-F1</td></tr><tr><td>BERTbase+MLP</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>with PLOT (Liu et al. 2021a)</td><td>54.13·</td><td>52.94·</td><td>34.41 o</td><td>27.52 ·</td><td>53.67 ·</td><td>50.00·</td></tr><tr><td>with ItS2CLR (Liu et al.2023)</td><td>56.25 ·</td><td>53.32 </td><td>32.01 ·</td><td>26.55 ·</td><td>56.44·</td><td>50.72 </td></tr><tr><td>With DERC-PL (Ours)</td><td>60.04</td><td>58.07</td><td>33.65</td><td>30.38</td><td>58.74</td><td>53.54</td></tr><tr><td>supervised learning</td><td>64.98</td><td>60.15</td><td>36.74</td><td>33.98</td><td>61.84</td><td>57.24</td></tr><tr><td>RGAT (Ishiwatari etal. 2020)</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>with PLOT (Liu et al. 2021a)</td><td>56.78 ·</td><td>55.88 </td><td>28.84</td><td>27.47 ·</td><td>55.21·</td><td>52.07 ·</td></tr><tr><td>with ItS2CLR(Liu etal.2023)</td><td>58.78</td><td>57.65 </td><td>31.30 ·</td><td>28.98</td><td>58.19 </td><td>54.32</td></tr><tr><td>With DERC-PL (Ours)</td><td>61.61</td><td>59.62</td><td>34.95</td><td>30.14</td><td>62.21</td><td>56.79</td></tr><tr><td>supervised learning</td><td>一</td><td>65.22</td><td>1</td><td>34.42</td><td>一</td><td>60.91</td></tr><tr><td>SACL (Hu et al. 2023)</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>with PLOT (Liu et al. 2021a)</td><td>57.39 ·</td><td>56.30 ·</td><td>32.75</td><td>29.97 ·</td><td>56.59 </td><td>54.85 </td></tr><tr><td>with ItS2CLR (Liu et al. 2023)</td><td>59.58 </td><td>57.14 ?</td><td>33.43</td><td>29.68</td><td>60.77 ·</td><td>55.52 </td></tr><tr><td>with DERC-PL (Ours)</td><td>63.74</td><td>60.15</td><td>36.38</td><td>32.48</td><td>62.24</td><td>58.12</td></tr><tr><td>supervised learning</td><td>69.08</td><td>69.22</td><td>42.21</td><td>39.65</td><td>67.51</td><td>66.45</td></tr><tr><td>DualGATs (Zhang, Chen,and Chen 2023)</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>with PLOT (Liu et al. 2021a)</td><td>57.45 </td><td>56.39 </td><td>32.68</td><td>29.63</td><td>56.97 ·</td><td>53.28·</td></tr><tr><td>with ItS2CLR (Liu et al. 2023)</td><td>62.60·</td><td>57.62 ?</td><td>33.94</td><td>28.65</td><td>59.57 </td><td>56.52</td></tr><tr><td>With DERC-PL (Ours)</td><td>64.37</td><td>61.05</td><td>37.68</td><td>32.78</td><td>63.82</td><td>59.32</td></tr><tr><td> supervised learning</td><td></td><td>67.68</td><td></td><td>40.69</td><td>1</td><td>66.90</td></tr></table></body></html>

A more granular analysis from the perspective of each dataset reveals that DERC-PL outperforms PLOT and ItS2CLR in Micro-F1 (Macro-F1) by approximately $4 . 8 \%$ $( 4 . 3 \% )$ and $3 . 1 \%$ $( 3 . 3 \% )$ on IEMOCAP, $3 . 5 \%$ $( 2 . 8 \% )$ and $3 \%$ $( 3 \% )$ on EmoryNLP, and $6 . 1 \%$ $( 4 . 4 \% )$ and $3 \%$ $( 2 . 7 \% )$ on MELD, respectively. A similar analysis by base ERC predictors shows that DERC-PL’s Micro-F1 (Macro-F1) scores surpass those of PLOT and ItS2CLR by approximately $3 . 4 \%$ $( 3 . 8 \% )$ and $2 . 6 \%$ $( 3 . 8 \% )$ on $\mathbf { B E R T _ { b a s e } + M L P }$ , $6 \%$ $( 3 . 6 \% )$ and $3 . 5 \%$ $( 1 . 9 \% )$ on RGAT, $5 . 2 \%$ $( 3 . 2 \% )$ and $2 . 9 \%$ $( 2 . 8 \% )$ on SACL, and $6 . 3 \%$ $( 4 . 6 \% )$ and $3 . 3 \%$ $( 3 . 5 \% )$ on DualGATs. Besides, we would remind that PLOT has applied the conversation-level proportion of emotions, so the performance gain on PLOT further indicates the effectiveness of DERC-PL on the task of DERC.

Results for $\varrho 2$ . For each base ERC predictor, we compare the performance with various weakly supervised methods in the DERC scenario against the supervised ERC dataset scenario. The empirical findings are presented in Table 4. Overall, DERC-PL’s F1 scores in the weakly supervised scenario are notably close to those in the supervised setting, with performance gaps across all benchmark datasets ranging from $2 . 1 \% { \sim } 9 . 1 \%$ . Surprisingly, under the $\mathbf { B E R T _ { b a s e } + M L P }$ predictor, DERC-PL consistently shows a gap of approximately $3 \%$ compared to supervised learning, particularly with the Macro-F1 gap on IEMOCAP being as low as $2 \%$ . These competitive results, particularly when compared with supervised learning, suggest that DERC-PL is a strong candidate for ERC methods in real-world applications.

# Ablation Study

We conduct the ablation study to evaluate two ablative versions of DERC-PL: (1) without updating the pseudoutterance-level emotions using Eq.4 (w/o label updating) and (2) without progressively learning from $\hat { \mathcal { D } } _ { 2 }$ to $\hat { \mathcal { D } } _ { | \mathcal { Y } | }$ after initialization with $\hat { \mathcal { D } } _ { 1 }$ (w/o training subset). Moreover, we also evaluate their combination. The three ablation versions are compared with the full DERC-PL, and the results are shown in Table 5. Overall, the empirical results demonstrate that the full version of DERC-PL significantly outperforms all ablative versions, proving that both techniques have positive effect on promoting the classification performance. For example, the Macro-F1 scores of DERC-PL are approximately $5 . 2 \%$ higher than those of w/o label updating on IEMOCAP, and about $2 . 4 \%$ higher than those of w/o training subset on MELD. Moreover, the performance gain of DERC-PL over the version w/o both techniques are even about $12 \% \sim 1 3 \%$ across IEMOCAP. In terms of EmoryNLP and MELD, the gain of Macro-F1 is much higher (i.e., $6 . 5 \% \sim 9 . 3 \%$ than the gain of Micro-F1 (i.e., $2 . 6 \% { \sim } 5 . 5 \% ,$ ), likely due to the datasets’ imbalance with a high proportion of Neutral. All those results further indicate that the two key techniques of DERC-PL can significantly promote the classification performance, and make DERC-PL more robust even for imbalanced datasets.

Table 5: The empirical results of ablation study based on $\mathbf { B E R T _ { b a s e } + M L P }$ . The best scores among different versions of DERCPL are indicated in bold.   

<html><body><table><tr><td rowspan="2">Method</td><td colspan="2">IEMOCAP</td><td colspan="2">EmoryNLP</td><td colspan="2">MELD</td></tr><tr><td>Micro-F1</td><td>Macro-F1</td><td>Micro-F1</td><td>Macro-F1</td><td>Micro-F1</td><td>Macro-F1</td></tr><tr><td>w/o training subset + label updating</td><td>47.78</td><td>46.87</td><td>31.02</td><td>24.72</td><td>53.21</td><td>46.24</td></tr><tr><td>w/o label updating</td><td>53.51</td><td>52.84</td><td>32.53</td><td>26.91</td><td>54.25</td><td>51.12</td></tr><tr><td>w/o training subset</td><td>54.74</td><td>53.87</td><td>32.60</td><td>27.91</td><td>55.21</td><td>52.07</td></tr><tr><td>DERC-PL</td><td>60.04</td><td>58.07</td><td>33.65</td><td>30.28</td><td>58.73</td><td>53.54</td></tr></table></body></html>

![](images/71f06ee1ce031741570f02017425de560b33e49ede602ae45d767459ad30f493.jpg)  
Figure 2: Sensitivity analysis of confidence threshold $\alpha$ of pseudo-utterance-level emotion updating based on $\mathbf { B E R T _ { b a s e } + M L P }$

# Sensitivity Analysis of $\alpha$

In this experiment, we investigate the confidence threshold $\alpha$ of pseudo-utterance-level emotion updating. We report the Macro-F1 scores of DERC-PL using $\mathbf { B E R T _ { b a s e } + M L P }$ with $\alpha$ ranging from $\{ 0 . 1 , 0 . 2 , \ldots , 0 . 9 \}$ , as shown in Fig.2. It can be seen that the performance trends of different $\alpha$ values are similar across EmoryNLP and MELD, whose the number of utterances per conversation is relatively smaller. The higher scores are achieved at lower $\alpha$ values such as $\{ 0 . 2 , 0 . 3 , \bar { 0 . 4 } \}$ . Another reason for such phenomenon is the imbalanced emotion distributions in these datasets, where the Neutral emotion occupies a high percentage of utterances, making precise predictions for other emotions challenging. In this case, only smaller values of $\alpha$ can maintain the update of pseudo-utterance-level emotions. In contrast, on IEMOCAP, the higher scores can be achieved by larger values of $\alpha$ such as $\{ 0 . 7 , 0 . 8 \}$ . The major reason is that IEMOCAP does not suffer from the imbalanced problem, leading to more precise predictions. Therefore, one needs higher values of $\alpha$ to retain high-confidence pseudo-utterance-level emotions.

# Conclusion

In this paper, we investigate a new weakly-supervised learning task of EDR named DERC, in which the training dataset is associated with conservation-level emotion sets, instead of utterance-level emotions. To examine the labeling accuracy and efficiency of DERC, we conduct preliminary experiments of manual labeling by inviting 20 volunteers on the public benchmark dataset MELD. Our empirical results demonstrate that labeling conversation-level emotion sets can be simultaneously accurate and efficient, to support the potential task of DERC. We then propose a novel DERC framewokr named DERC-PL built on the spirit of pseudolabeling, which jointly updates pseudo-utterance-level emotions and the ERC predictor in a self-training manner. We conduct loads of experiments to evaluate the performance of DERC-PL. The empirical results demonstrate that DERC-PL consistently outperforms the competitive weakly-supervised learning methods PLOT and ItS2CLR; and it can be on par with the supervised learning methods, further indicating DERC-PL can be an effective candidate for ERC methods.