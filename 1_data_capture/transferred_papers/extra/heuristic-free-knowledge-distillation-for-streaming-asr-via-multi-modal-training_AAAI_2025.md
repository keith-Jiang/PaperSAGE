# Heuristic-free Knowledge Distillation for Streaming ASR via Multi-modal Training

Ji Won Yoon

Department of AI, Chung-Ang University, Seoul, South Korea. jiwonyoon@cau.ac.kr

# Abstract

Existing knowledge distillation (KD) studies for streaming automatic speech recognition (ASR) adopt a non-streaming model as the teacher and a streaming model as the student, respectively. Since the non-streaming teacher usually has less emission latency compared to the streaming student, the teacher’s prediction is typically shifted by $\tau$ frames, where the parameter $\tau$ is selected heuristically. In this paper, we observe that this manual shifting is sub-optimal and propose a novel framework, namely Heuristic-free $K D$ . Instead of leveraging knowledge from the non-streaming teacher model, we employ a self-distillation setup, distilling the knowledge within the streaming architecture itself. Since the teacher and student share the same streaming ASR backbone, the alignment mismatch issue can be effectively mitigated without requiring any time shifting by $\tau$ . Additionally, we incorporate full-context textual information as an auxiliary multi-modal input for the proposed teacher. Although the streaming architecture lacks future context, the additional linguistic input enables it to generate more accurate knowledge for selfdistillation. We empirically demonstrate that the proposed KD approach significantly improves the performance of the streaming ASR model, outperforming conventional methods that rely on the offline teacher and heuristic parameter.

# Introduction

In recent years, a streaming automatic speech recognition (ASR) framework has attracted significant attention for its ability to transcribe spoken language into text in real time. Such capability is crucial for enhancing interactions with various applications, including real-time captioning, on-thefly language translation, voice command recognition, and dialogue systems, by providing immediate feedback and interactions. However, despite its potential, the streaming ASR model often underperforms compared to its non-streaming (a.k.a. offline or full-context) counterpart. This performance degradation stems from a significant constraint: streaming ASR lacks future context, a limitation designed to minimize the delay between spoken input and textual output. Improving performance has proven to be challenging in streaming ASR (He et al. 2019; Li et al. 2020; Sainath et al. 2020).

To bridge the gap between offline and online ASR models, there have been numerous efforts to adopt knowledge distillation (KD) (Hinton, Vinyals, and Dean 2014; Bucila, Caruana, and Niculescu-Mizil 2006). In the context of KD for streaming ASR, the non-streaming and streaming models are typically adopted as the teacher and student, respectively. Since non-streaming models benefit from having access to the entire speech utterance, they can offer more accurate hypotheses, thus producing better knowledge for the ASR task. By leveraging the knowledge from a powerful offline teacher during training, the online student can improve its performance compared to its baseline, which is trained solely with ground truth.

Despite the widespread adoption of KD for streaming ASR, most existing methods share a common limitation: they manually shift the output of the non-streaming teacher in a heuristic manner (Yang, Li, and Woodland 2022; Weninger et al. 2022; Yu et al. 2021b). Since the nonstreaming model (teacher) has no incentive to delay its output, its predictions usually have lower latency compared to those of the streaming model (student). Due to significant misalignment between the teacher and student models, it has been confirmed that naive frame-to-frame KD can misguide the streaming student (Yu et al. 2021b; Inaguma and Kawahara 2021; Liang et al. 2023). Therefore, the teacher’s predictions are typically required to be shifted by $\tau$ frames before being transferred to the student model. Here, the shifting parameter $\tau$ is determined heuristically. Setting a sensible value for $\tau$ is crucial for successful knowledge transfer, as the performance of the student model varies with different $\tau$ values (Yang, Li, and Woodland 2022).

In this paper, we observe that using a fixed parameter $\tau$ for KD is sub-optimal and introduce a novel framework for streaming ASR, namely Heuristic-free KD. Specifically, Heuristic-free KD encapsulates two key components. First, we employ a self-distillation setup for streaming ASR, which aims at distilling the knowledge within the streaming architecture itself without any extra non-streaming teacher. Rather than extracting knowledge from the non-streaming teacher, the proposed teacher and student share the same streaming ASR model parameters, ensuring that the framelevel alignment from the teacher is highly similar to that of the student. This allows the teacher’s knowledge to be transferred without requiring any time shifting by $\tau$ . Second, we incorporate full-context textual information as an auxiliary multi-modal input for the proposed teacher. In particular, the student equipped with multi-modal fusion acts as the teacher, and the predictions benefiting from linguistic information are then transferred to the original streaming student. Even within the self-distillation framework using the streaming architecture, the proposed multi-modal training not only mitigates the streaming model’s context scarcity but also generates a more accurate guidance for self-distillation. We implement cross-attention in multi-modal fusion, enabling the proposed teacher to learn the inter-modal dependencies between acoustic and linguistic features.

Extensive experiments are conducted on the LibriSpeech (Panayotov et al. 2015) benchmark, utilizing two ASR architectures: connectionist temporal classification (CTC) (Graves et al. 2006) and hybrid Transducer-CTC (Noroozi et al. 2024), since both CTC and Transducer (Graves 2012) models have gained favor in real-world streaming settings recently (Wang et al. 2023; Tian et al. 2023a,b; Yao et al. 2021; Zhang et al. 2022; Yu et al. 2021a). Compared to existing KD methods that rely on a powerful offline teacher and the shifting parameter $\tau$ , the proposed KD significantly improves the student’s performance, achieving the best results in all configurations. It is important to note that Heuristicfree KD does not require any extra teacher model and time shifting by $\tau$ . In a detailed case study, we also confirm that the proposed teacher can generate more accurate knowledge for KD while preserving the alignment of the student.

# Related Work

KD aims at transferring knowledge from a computationally intensive teacher model to a more efficient student model. By transferring the knowledge, the distilled student performs better than its baseline. It has been shown that the distillation framework is effective for compressing full-context ASR models (Li et al. 2014; Kim and Rush 2016; Kim et al. 2019; Senior et al. 2015; Takashima, Li, and Kawai 2018, 2019; Yoon et al. 2021, 2022).

In the context of KD for streaming ASR, the nonstreaming and streaming models are typically adopted as the teacher and student, respectively. Due to the significant alignment mismatch problem between the two models, previous studies have suggested shifting the teacher model’s knowledge by $\tau$ frames (Shim et al. 2023; Yang, Li, and Woodland 2022; Weninger et al. 2022), employing multistage training (Kurata and Saon 2020), and using CTC alignments (Inaguma and Kawahara 2021). Among these methods, the most widely-used approach is time shifting with $\tau$ .

Recently, dual-mode training (Yu et al. 2021b; Liang et al. 2023; Liu et al. 2022) has been proposed, based on selfdistillation and using the same model for both streaming and non-streaming scenarios. Despite its promise, there are significant limitations. Firstly, this approach requires many architectural modifications, including dual-mode convolution, dual-mode average pooling, dual-mode self-attention, and dual-mode normalization, making it difficult to reproduce. Additionally, the dual-mode framework still requires the heuristic parameter $\tau$ since it involves KD from nonstreaming to streaming models. In contrast to the conventional dual-mode training, the Heuristic-free KD does not modify the original ASR model architecture and does not rely on both non-streaming teacher and time shifting with $\tau$ , providing a more general solution for KD. Furthermore, the primary focus of our proposed KD method is on improving the overall quality of the ASR student model rather than reducing latency.

![](images/6191b3a04666ff694b08a564b8cba7f87105b50f15f024bd5e01fc0a169f5f4e.jpg)  
Figure 1: Frame-wise alignment example of streaming student baseline for utterance 7902-96594-0016 in LibriSpeech test-other dataset, where the target is ‘YES SIR OF COURSE’. The red boxes highlight the non-blank predictions from the non-streaming teacher model. The two-way arrows in various colors indicate different optimal $\tau$ shifts. $\mathbf { \boldsymbol { \cdot } } _ { \epsilon } ,$ represents the blank label in CTC.

# Proposed Method

# Motivation

When applying the conventional time shifting with $\tau$ , two primary challenges arise. Firstly, identifying the proper $\tau$ value requires additional parameter tuning, which involves further training sessions and subsequent performance evaluations. The ideal $\tau$ is variable and affected by factors such as model architecture, dataset characteristics, and streaming context. For example, Dual-mode ASR (Yu et al. 2021b) performs a small-scale hyper-parameter sweep, shifting from -2 to 2 frames. According to Yang, Li, and Woodland (2022), the best-performing $\tau$ is 7, while in our experiments, $\tau = 5$ yielded the best performance for competing KD methods on LibriSpeech. Secondly, our empirical observations suggest that adopting a single $\tau$ value for all alignments could be sub-optimal. As shown in Figure 1, to align the ‘s’ label of the teacher model with the ‘s’ label of the student model, the $\tau$ value should be adjusted to 5. Meanwhile, other labels might achieve better performance with $\tau$ values of 4, 3, or 2. This variation indicates that employing a single $\tau$ value for time shifting may not be optimal for all scenarios, potentially resulting in less effective KD for the streaming student. Given the importance of the role of streaming ASR models in various industries, it’s surprising how little research has been devoted to eliminating the heuristic parameter $\tau$ . This motivated us to ask a simple question: how can we provide optimal guidance for a streaming student model without using a non-streaming teacher and manual shifting with $\tau$ ?

# Heuristic-free KD

In this paper, we introduce Heuristic-free KD for streaming ASR that does not require the manual shifting of $\tau$ frames during the distillation process.

Student ASR Teacher S T frames s T   
(Inference) LOss asr Oss asr × □0:z After that they will preparae After that theywill repair to their country home £kd to their country home Feed-forward KD Loss Qkyk □0e ASRDecoder Self-attention N lengths □□:□□ □0:00 Multi-modal ASRDecoder Fusion T frames 7 00:00 F □□:□□ text QK : Cross-attention AcousticEncoder AcousticEncoder V Text Encoder (Streaming) (Streaming) Speech Signals Speech Signals Frame-level Alignment After that they will repair Transcriptions ) : Parameter Sharing to their country home

Self-distillation As aforementioned, it has been proven that the predictions of the non-streaming teacher have a significant mismatch with those of the streaming student. The proposed framework refers to the idea that instead of using the knowledge of the non-streaming model, we rather employ a self-distillation setup, distilling the knowledge within the streaming architecture itself. In Heuristic-free KD, the teacher and student share the same streaming ASR backbone, ensuring that the frame-level alignment of the teacher is highly similar to that of the student. Therefore, the alignment mismatch issue can be effectively mitigated, allowing the teacher’s knowledge to be transferred without requiring any time shifting by $\tau$ . Since the teacher and student utilize the same model parameters, the proposed KD can be considered as self-distillation, thus using the terms “teacher mode” and “student mode” rather than “teacher model” and “student model”.

Multi-modal Training The limited context available to the conventional streaming model restricts its ability to generate informative knowledge for self-distillation. The key idea behind the proposed approach is to refine the output of the student model via multi-modal training and then use it as guidance for KD. Inspired by prior research (Shim, Choi, and Sung 2022) that revealed the significance of linguistic information for predicting ASR targets, our framework injects full-context textual information into the streaming model, which serves as the teacher mode (see Figure 2). Even though the streaming model lacks future context, the additional target text input provides full-context linguistic information. This enables the streaming architecture to generate more accurate knowledge for self-distillation.

Student Mode. Without loss of generality, we consider an ASR model that maps the speech input $x$ to a latent representation $z$ and predicts the transcription $y$ with an acoustic encoder $\mathcal { F } _ { e n c }$ and a decoder $\mathcal { F } _ { d e c }$ , as follows:

$$
\mathcal { F } _ { e n c } ( x )  z , \quad \mathcal { F } _ { d e c } ( z )  y .
$$

This sequence represents the forward pass of the student model in our KD framework. It is important to note that during inference, only the parameters of the student’s encoder $\mathcal { F } _ { e n c }$ and decoder $\mathcal { F } _ { d e c }$ are utilized.

Teacher Mode. In the Heuristic-free KD framework, the student equipped with multi-modal fusion serves as the teacher. Before the modality fusion, the target transcription $y$ is transformed by a text encoder $\mathcal { F } _ { t e x t }$ , which is composed of an embedding layer followed by a single self-attention layer, to effectively incorporate $y$ into the fusion process. Meanwhile, the speech input $x$ is transformed by the acoustic encoder $\mathcal { F } _ { e n c }$ . The process can be described as follows:

$$
\mathcal { F } _ { e n c } ( x )  z , \quad \mathcal { F } _ { t e x t } ( y )  e .
$$

The multi-modal fusion block $\mathcal { F } _ { m u l t i }$ is inserted before the ASR decoder $\mathcal { F } _ { d e c }$ . Motivated by recent multi-modal studies (Radhakrishnan et al. 2023; Nadeem et al. 2024; Yoon et al. 2023a,b; Qian et al. 2023), we leverage a crossattention mechanism within $\mathcal { F } _ { m u l t i }$ for fusion, enabling the teacher to effectively learn inter-modal dependencies between acoustic and textual features. Specifically, the acoustic embedding is used as the query, and the textual embedding is leveraged as key-value pairs. Though the acoustic embedding from the streaming encoder $\mathcal { F } _ { e n c }$ lacks future context, this selective fusion approach not only facilitates the integration of auditory and linguistic information but also addresses the context scarcity of the streaming architecture, thus generating better guidance for self-distillation. This modification restructures the ASR framework as outlined below:

$$
\mathcal { F } _ { m u l t i } ( z , e ) \to z ^ { \prime } , \quad \mathcal { F } _ { d e c } ( z ^ { \prime } ) \to y .
$$

Since the teacher’s forward pass shares the parameters $\mathcal { F } _ { e n c }$ and $\mathcal { F } _ { d e c }$ with the student, the additional parameters employed during this forwarding are only $\mathcal { F } _ { m u l t i }$ and $\mathcal { F } _ { t e x t }$ .

Training Objective. Our self-distillation framework consists of three training objectives: the ASR loss for the student $\mathcal { L } _ { a s r } ^ { s }$ , the ASR loss for the teacher $\mathcal { L } _ { a s r } ^ { t }$ , and the KD loss $\mathcal { L } _ { k d }$ . Both $\mathcal { L } _ { a s r } ^ { s }$ and $\mathcal { L } _ { a s r } ^ { t }$ are original ASR losses. When training the CTC model, they represent CTC losses; for the hybrid CTC-Transducer model, they are hybrid CTCTransducer losses. For $\mathcal { L } _ { k d }$ , we use the SKD loss (Yoon et al. 2021) to transfer the frame-level predictions from the teacher to the student. Therefore, the final objective is given as follows:

$$
\mathcal { L } _ { t o t a l } = \mathcal { L } _ { a s r } ^ { s } + \mathcal { L } _ { a s r } ^ { t } + \lambda \cdot \mathcal { L } _ { k d }
$$

where $\lambda$ is a tunable parameter to balance the two ASR losses and the KD loss.

Comparison with Text Injection ASR. Our approach appears similar to recent text injection ASR approaches at first glance (Kang et al. 2022; Thomas et al. 2022b; Zhang et al. 2024; Yao et al. 2022; Yue et al. 2023; Chen et al. 2022; Thomas et al. 2022a; Yoon et al. 2023b,a). However, our motivation is different. The main purpose of multi-modal training in our framework is to provide full-context textual information to the streaming model, thereby enabling it to extract more accurate guidance for self-distillation. Unlike the offline ASR model that can consider the entire utterance, the streaming ASR model lacks future context, which makes it difficult to generate informative knowledge from the streaming teacher. Motivated by prior work (Shim, Choi, and Sung 2022) that underscored the significance of linguistic information for the ASR task, we use the target text as an auxiliary multi-modal input for the teacher. This approach overcomes the context scarcity in the streaming scenario and effectively improves self-distillation. Interestingly, although the target text directly provides informative context for the ASR task, it does not lead to a trivial solution where the model merely copies the target input as its output, as will be shown in the analysis section. This behavior is related to the inherent property of the CTC model. In the CTC framework, the relationship between the CTC alignment $\pi$ and target $y$ is many-to-one. Since it is extremely difficult to predict the “many” from the “one”, the CTC model cannot directly copy the target input (e.g., $\{ \mathrm { c , a , t } \} ,$ ) into the output CTC alignment (e.g., $\{ \mathrm { c , c , c , c , a , a , a , a , a , a , t , t } \} ,$ ), even when the target is used as additional input.

# Experiments Experimental Setup

Dataset and Metrics. We evaluated the performance of models using the LibriSpeech (Panayotov et al. 2015) benchmark, the most widely used ASR dataset, which is freely available under the CC BY 4.0 license. During the training, we employed ‘train-clean-100’, ‘train-clean- $3 6 0 ^ { \circ }$ , and ‘train-other- $5 0 0 ^ { \circ }$ . For evaluations, we utilized ‘devclean’, ‘dev-other’, ‘test-clean’, and ‘test-other’. The experimental results on the Common Voice 7.0 Spanish dataset (Ardila et al. 2020) can be found in the extended Appendix. Two widely-used metrics were used to evaluate performance: word error rate (WER) and relative error rate reduction (RERR). WER is a standard metric for assessing speech recognition accuracy, while RERR quantifies the proportionate reduction in WER compared to a baseline.

Implementation. Our experiments were mainly conducted with the NeMo (Kuchaiev et al. 2019) toolkit, and greedy decoding was applied to compare WERs. We implemented two ASR architectures for comparison: CTC and hybrid Transducer-CTC. For the CTC, we adopted cacheaware streaming FastConformer (Noroozi et al. 2024) as the student baseline. Two streaming settings were considered: look-aheads of $1 0 4 0 ~ \mathrm { m s }$ and $4 8 0 ~ \mathrm { m s }$ , with corresponding right context sizes of 13 and 6, respectively. Although no look-ahead will decrease latency, we kindly note that using a look-ahead framework is generally accepted in the streaming ASR scenario (Noroozi et al. 2024; Chen et al. 2024; Kumar et al. 2024; Zhang et al. 2020). To reproduce conventional KD methods, the FastConformer (Rekesh et al. 2023) was employed as the non-streaming CTC teacher model. Guided CTC (Kurata and Audhkhasi 2019) and SKD (Yoon et al. 2021) were adopted as competing approaches. Since Dual-mode ASR (Yu et al. 2021b) required many architectural modifications, it was difficult to fairly compare the performance of KD methods while preserving the original ASR model structure. Given that Dual-mode ASR employed the conventional KD approach using the shifting parameter $\tau$ , we believe our experiments with various KD methods and settings sufficiently demonstrated the effectiveness of our approach. In the case of the hybrid TransducerCTC, cache-aware streaming FastConformer was utilized as both the teacher and student baseline since we conducted a streaming-to-streaming KD scenario. For all conventional teacher models, we used the pre-trained checkpoints provided by the NeMo. The training details will be provided in Appendix.

# Experimental Results

Table 1 reports the WER and RERR results on the LibriSpeech benchmark. We adopted the cache-aware streaming FastConformer as the student baseline, which had $1 2 . 8 \textbf { M }$ parameters. For competing KD methods, including Guided CTC training (Kurata and Audhkhasi 2019) and SKD (Yoon et al. 2021), the large size (about $1 1 5 \mathrm { ~ M ~ }$ parameters) of FastConformer-CTC was utilized as the nonstreaming teacher model. According to the original papers of Guided CTC and SKD, it has been confirmed that both KD frameworks were effective in minimizing the alignment mismatch between non-streaming CTC models, without using manual frame shifting. However, as reported in the results, both SKD and Guided CTC with $\tau = 0$ failed to mitigate the misalignment between the non-streaming teacher and streaming student, resulting in the distilled student performing worse than its baseline. This finding suggests that the issue at hand was particularly challenging. While varying the value of $\tau$ , we found that $\tau = 5$ performed well with conventional KD methods compared to other settings. The results in Table 1 show that the proposed framework achieved the best performance in all configurations. It yielded an RERR of $20 . 2 4 \%$ on dev-clean and $12 . 9 2 \%$ on dev-other, respectively. Considering that the best results of SKD and GuidedCTC were RERR $1 1 . 5 3 \%$ and RERR $5 . 7 6 \%$ on dev-clean, the Heuristic-free KD demonstrated a significant improvement over these conventional methods. Note that the proposed method did not require any pre-trained teacher and the shifting parameter $\tau$ . While the other approaches used the extra teacher that had about $1 1 5 { \mathrm { ~ M ~ } }$ parameters, Heuristicfree KD required only $1 . 3 { \bf M }$ additional parameters for KD.

Table 1: Streaming CTC student with a look-ahead of $1 0 4 0 \mathrm { m s }$ : comparison of WER and RERR on the LibriSpeech benchmark. Note that the proposed method did not require an additional teacher model. The offline teacher model was only used for conventional KD methods. Results that represent superior performance are highlighted in bold.   

<html><body><table><tr><td rowspan="2">Method</td><td rowspan="2">7</td><td colspan="2">dev-clean</td><td colspan="2">dev-other</td><td colspan="2">test-clean</td><td colspan="2">test-other</td></tr><tr><td>WER</td><td>RERR</td><td>WER</td><td>RERR</td><td>WER</td><td>RERR</td><td>WER</td><td>RERR</td></tr><tr><td>Teacher (Offline)</td><td></td><td>1.87 %</td><td></td><td>4.22 %</td><td></td><td>2.08 %</td><td></td><td>4.22 %</td><td></td></tr><tr><td>Student (Online)</td><td></td><td>7.46 %</td><td>1</td><td>17.10 %</td><td>一</td><td>7.42 %</td><td>1</td><td>17.49 %</td><td></td></tr><tr><td rowspan="4">SKD</td><td>0</td><td>8.77 %</td><td>-17.56 %</td><td>18.37 %</td><td>-7.42 %</td><td>8.99 %</td><td>-21.16 %</td><td>19.00 %</td><td>-8.63 %</td></tr><tr><td>3</td><td>6.79 %</td><td>8.98 %</td><td>16.42 %</td><td>4.04 %</td><td>6.97 %</td><td>6.06 %</td><td>16.07 %</td><td>8.12 %</td></tr><tr><td>5</td><td>6.60 %</td><td>11.53 %</td><td>16.12 %</td><td>5.73 %</td><td>6.78 %</td><td>8.63 %</td><td>15.79 %</td><td>9.71 %</td></tr><tr><td>7</td><td>7.08 %</td><td>5.09 %</td><td>16.19 %</td><td>5.32 %</td><td>7.21 %</td><td>2.83 %</td><td>16.16 %</td><td>7.61 %</td></tr><tr><td rowspan="4">Guided CTC</td><td>0</td><td>10.58 %</td><td>-41.82 %</td><td>20.49 %</td><td>-19.82 %</td><td>10.49 %</td><td>-41.37 %</td><td>20.84 %</td><td>-19.08 %</td></tr><tr><td>3</td><td>7.58 %</td><td>-1.61 %</td><td>17.80 %</td><td>-4.09 %</td><td>7.51 %</td><td>-1.21 %</td><td>17.63 %</td><td>-0.80 %</td></tr><tr><td>5</td><td>7.03 %</td><td>5.76 %</td><td>17.13 %</td><td>-0.18 %</td><td>7.16 %</td><td>3.50 %</td><td>17.18 %</td><td>1.77 %</td></tr><tr><td>7</td><td>7.88 %</td><td>-5.63 %</td><td>17.78 %</td><td>-3.98 %</td><td>8.10 %</td><td>-9.16 %</td><td>17.41 %</td><td>0.46 %</td></tr><tr><td>Ours,Heuristic-free KD</td><td></td><td>5.95 %</td><td>20.24 %</td><td>14.89 %</td><td>12.92 %</td><td>6.24 %</td><td>15.90 %</td><td>15.05 %</td><td>13.94 %</td></tr></table></body></html>

Table 2: Streaming CTC student with a look-ahead of $4 8 0 \mathrm { m s }$ : comparison of WER and RERR on the LibriSpeech benchmark. Results that represent superior performance are highlighted in bold.   

<html><body><table><tr><td rowspan="2">Method</td><td rowspan="2">T</td><td colspan="2">dev-clean</td><td colspan="2">dev-other</td><td colspan="2">test-clean</td><td colspan="2">test-other</td></tr><tr><td>WER</td><td>RERR</td><td>WER</td><td>RERR</td><td>WER</td><td>RERR</td><td>WER</td><td>RERR</td></tr><tr><td>Teacher (Offline)</td><td></td><td>1.87 %</td><td></td><td>4.22 %</td><td>一</td><td>2.08 %</td><td>一</td><td>4.22 %</td><td>一</td></tr><tr><td>Student (Online)</td><td></td><td>5.45 %</td><td>一</td><td>13.89 %</td><td>一</td><td>5.59 %</td><td>1</td><td>13.89 %</td><td>一</td></tr><tr><td>Guided CTC</td><td>5</td><td>5.34 %</td><td>2.02 %</td><td>13.82 %</td><td>0.50 %</td><td>5.52 %</td><td>1.25 %</td><td>13.93 %</td><td>-0.29 %</td></tr><tr><td>SKD</td><td>5</td><td>5.36 %</td><td>1.65 %</td><td>13.85 %</td><td>0.29 %</td><td>5.78 %</td><td>-3.40 %</td><td>13.67 %</td><td>1.58 %</td></tr><tr><td>Ours,Heuristic-free KD</td><td></td><td>4.68 %</td><td>14.13 %</td><td>13.24 %</td><td>4.68 %</td><td>4.96 %</td><td>11.27 %</td><td>12.80 %</td><td>7.85 %</td></tr></table></body></html>

To further validate the effectiveness of Heuristic-free KD, we considered a different setting: the look-ahead was set to $4 8 0 \mathrm { m s }$ , which is suitable for low-latency streaming applications. Since reduced right context may lead to performance degradation, a larger student baseline (about $2 0 . 4 \mathbf { M }$ parameters) was leveraged to improve the WER performance. Note that this configuration did not make the results incomparable across different look-aheads. In KD, enhancing an already strong student model is generally more difficult than improving a weaker one. By using a larger student model with a 480 ms look-ahead, we mitigated the degradation issue caused by the reduced right context, ensuring a fair but more challenging comparison within the KD framework. We set $\tau = 5$ for competing methods, as this setting performed well in previous experiments. As presented in Table 2, the Heuristic-free KD outperformed the conventional methods. Interestingly, we found that the distilled students using SKD and Guided CTC did not always perform better than the student baseline; in some cases, their performance was even worse. In configurations where conventional methods did improve the student’s performance, the RERR values were minimal, indicating that the low-latency scenario with the look-ahead of $4 8 0 \mathrm { m s }$ was more challenging than the previous one. In contrast, the Heuristic-free KD showed significant WER improvements in all cases, achieving RERR of $1 1 . 2 7 \%$ on test-clean and $7 . 8 5 \%$ on test-other, respectively.

We proceeded to verify how well the proposed KD approach can distill knowledge using the hybrid TransducerCTC architecture, a recent version of the Transducer with

<html><body><table><tr><td rowspan="2">Method</td><td colspan="2">dev-clean</td><td colspan="2">dev-other</td><td colspan="2">test-clean</td><td colspan="2">test-other</td></tr><tr><td>WER</td><td>RERR</td><td>WER</td><td>RERR</td><td>WER</td><td>RERR</td><td>WER</td><td>RERR</td></tr><tr><td>Teacher (Online)</td><td>2.20 %</td><td></td><td>5.60 %</td><td></td><td>2.33 %</td><td></td><td>5.54 %</td><td></td></tr><tr><td>Student (Online)</td><td>5.28 %</td><td>一</td><td>13.32 %</td><td>一</td><td>5.53 %</td><td>1</td><td>13.68 %</td><td>一</td></tr><tr><td>SKD</td><td>5.20 %</td><td>1.52 %</td><td>13.21 %</td><td>0.83 %</td><td>5.39 %</td><td>2.53 %</td><td>13.49 %</td><td>1.39 %</td></tr><tr><td>Ours,Heuristic-free KD</td><td>5.00 %</td><td>5.30 %</td><td>13.06 %</td><td>1.95 %</td><td>5.14 %</td><td>7.05 %</td><td>12.87 %</td><td>5.93 %</td></tr></table></body></html>

Table 3: Streaming hybrid Transducer-CTC with a look-ahead of $1 0 4 0 \mathrm { m s }$ : comparison of WER and RERR on the LibriSpeech benchmark. Results that represent superior performance are highlighted in bold.

![](images/e8ab84dbb7b586e22d03677d3e4e36cab322e38f2e977a422372686f5e06a8e0.jpg)  
Figure 3: From top to bottom: frame-wise alignments generated with the teacher with multi-modal fusion, the distilled student in the proposed framework, and the corresponding spectrogram, where the target transcription is “HE’S A SCHOOLMATE OF MINE”. For the frame-level alignment, the $\mathbf { \boldsymbol { x } }$ -axis and y-axis represent frames and hypotheses, respectively. The red boxes highlight the non-blank predictions.

an added CTC layer on top of the encoder. Since we experimentally found that KD using the CTC outputs was more effective than using those of the Transducer, we used the SKD approach with CTC outputs as a comparative method. For fair comparison, the $\mathcal { L } _ { k d }$ for Heuristic-KD also minimized the distance between the CTC outputs. In this case, $\mathcal { F } _ { e n c }$ and $\mathcal { F } _ { d e c }$ equated to the encoder and CTC decoder, respectively. Unlike previous experiments focused on KD from non-streaming to streaming models, this experiment considered KD from streaming to streaming models, which was an unexplored setting so far. Since both the teacher and student had the same look-ahead setting of $\mathrm { 1 0 4 0 ~ m s }$ , the prediction from the streaming teacher did not need to be shifted using parameter $\tau$ . Table 3 summarizes the WER and RERR results on LibriSpeech. Compared to SKD, Heuristic-free KD achieved better WER performance in all configurations, providing RERR of $7 . 0 5 ~ \%$ and $5 . 9 3 \ \%$ on test-clean and testother, respectively. We confirmed that the proposed framework still performed well with the Transducer-based model.

# Analysis

Frame-level Alignments in Heuristic-free KD. As shown in Figure 3, we contrasted frame-wise alignments of the distilled student and teacher in the proposed framework. The distilled student produced the prediction “HE A SCHOOL MADE A MI”, struggling to predict the correct sentence except for the words “HE” and “SCHOOL”. However, the proposed teacher with multi-modal training offered an accurate prediction “HE’S A SCHOOLMATE OF MINE” while maintaining a similar frame-wise alignment to that of the student. As shown in Figure 3, the proposed teacher not only minimized the alignment mismatch issue but also successfully refined the output of the student, converting “MADE A” to “MATE OF” and “MI” to “MINE”, respectively. This implies that it can provide more optimal and accurate knowledge for the streaming student, eliminating the need for the shifting parameter $\tau$ . Though the conventional streaming model struggled to generate accurate knowledge due to limited context, Heuristic-free KD could provide informative knowledge for the student within the streaming architecture.

Table 4: Teacher performance comparison on the LibriSpeech test datasets. CTC (Offline) refers to the nonstreaming CTC teacher model in Table 1, and TransducerCTC (Online) refers to the streaming hybrid TransducerCTC teacher model in Table 3.   

<html><body><table><tr><td>TeacherModel</td><td>Params.</td><td>clean</td><td>other</td></tr><tr><td>CTC (Offline)</td><td>115 M</td><td>2.08 %</td><td>4.22 %</td></tr><tr><td>Proposed Teacher</td><td>14M 22M</td><td>1.92 % 1.52 %</td><td>5.04 % 4.09 %</td></tr><tr><td>Transducer-CTC (Online)</td><td>114 M</td><td>2.33 %</td><td>5.54 %</td></tr><tr><td>Proposed Teacher</td><td>16M</td><td>1.28 %</td><td>3.21 %</td></tr></table></body></html>

WER Performance of Teachers. We compared the proposed teacher with the non-streaming and streaming teacher models used for competing KD methods. As mentioned earlier, we used the checkpoints provided by the NeMo (Kuchaiev et al. 2019) for the conventional teacher models, as they required a significant amount of GPU resources and training time. Table 4 reports the WER performance of the teacher models. Unlike the conventional teacher models, the proposed teacher required fewer additional parameters for KD. For example, the proposed teacher for CTC had about $1 4 ~ \mathrm { M }$ parameters, with $1 2 . 8 \ \mathrm { M }$ for the ASR student model $\mathcal { F } _ { e n c }$ and $\mathcal { F } _ { d e c } )$ and $1 . 3 \ \mathrm { M }$ for the multi-modal fusion ( $\mathcal { F } _ { t e x t }$ and $\mathcal { F } _ { m u l t i }$ ). For hybrid Transducer-CTC framework, the proposed teacher had about $1 6 \mathbf { M }$ parameters, with the multi-modal fusion requiring $1 . 2 \ \mathrm { M }$ . From the results, we confirmed that the WER results of the proposed teacher in Heuristic-free KD were more accurate than those of the conventional teacher models in most configurations, though not zero. If the proposed framework copied the target text input as its output, the WER performance would be zero, indicating that the model perfectly predicted the target and resulted in a trivial solution during training. It is verified that the Heuristic-KD not only performed better in training the streaming student but also prevented the trivial solution while using the target text input.

![](images/c958ee929796d27a70be540becb71b741a710a7bd1373f7fea5c0b3a49c03b87.jpg)  
Figure 4: Frame-level alignment examples in the test-other dataset, where the target is “THE DOSE FOR AN ADULT IS TEN MINIMS”. The x-axis and y-axis represent acoustic frames and hypotheses, respectively. The red boxes highlight the non-blank predictions.

Alignment Comparison with Conventional KD. As illustrated in Figure 4, we presented four frame-level alignments generated by the offline teacher, the distilled student using SKD, the proposed teacher, and the distilled student in the Heuristic-free KD, respectively. Since $\tau = 5$ yielded the best performance for SKD, we shifted the prediction of the non-streaming teacher model by 5 frames. In Figure 4, the check marks indicate frames where the prediction was correct and the frame-level alignment between the teacher and student matched. While the distilled student with SKD only matched four frames with its teacher, the student in the proposed framework aligned with its teacher on most of the frames. This means that the Heuristic-free KD minimized the gap between the teacher and student more effectively than conventional KD.

# Ablation Study

When training the proposed framework, we considered one tunable parameter $\lambda$ in Eq. (4). As shown in Figure 5, we evaluated the WER performance while varying the parameter $\lambda$ from $\{ 0 . 5 0 0 , \bar { 0 . 2 } 5 0 , 0 . 6 6 7 , 0 . 1 2 5 \}$ From the results, it is verified that the best WER performance on LibriSpeech was obtained when $\lambda = 0 . 2 5 0$ .

![](images/e825951cc05d0ab07718f9f3247495fdc508bf8dd0528cf8f424d61eb8e20b6a.jpg)  
Figure 5: WER performance on LibriSpeech. The evaluation is conducted with varying values of $\lambda$ .

# Conclusions

In this paper, we introduced a novel framework for streaming ASR, termed Heuristic-free KD. By leveraging the selfdistillation setup, we effectively mitigated the alignment mismatch during KD, eliminating the requirement for nonstreaming teacher and shifting parameter $\tau$ . Additionally, we enhanced the overall quality of knowledge within the streaming architecture by incorporating linguistic information as an auxiliary multi-modal input. Our empirical results demonstrated that the proposed KD approach significantly improved the performance of the streaming model, surpassing conventional methods that used the extra non-streaming teacher model and heuristic parameter.