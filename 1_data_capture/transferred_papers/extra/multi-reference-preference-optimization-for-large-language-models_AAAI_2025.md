# Multi-Reference Preference Optimization for Large Language Models

Hung $\mathbf { L e } ^ { 1 }$ , Quan Hung Tran2, Dung Nguyen1, Kien $\mathbf { D _ { \mathbf { 0 } } } _ { } ^ { 1 }$ , Saloni Mittal2, Kelechi Ogueji2, Svetha Venkatesh1

1Applied AI Institute, Deakin University, Geelong, Australia 2ServiceNow Research, USA {thai.le, dung.nguyen, k.do, svetha.venkatesh}@deakin.edu.au hungquan.tran, saloni.mittal, kelechi.ogueji @servicenow.com

# Abstract

How can Large Language Models (LLMs) be aligned with human intentions and values? A typical solution is to gather human preference on model outputs and finetune the LLMs accordingly while ensuring that updates do not deviate too far from a reference model. Recent approaches, such as direct preference optimization (DPO), have eliminated the need for unstable and sluggish reinforcement learning optimization by introducing close-formed supervised losses. However, a significant limitation of the current approach is its design for a single reference model only, neglecting to leverage the collective power of numerous pretrained LLMs. To overcome this limitation, we introduce a novel closed-form formulation for direct preference optimization using multiple reference models. The resulting algorithm, Multi-Reference Preference Optimization (MRPO), leverages broader prior knowledge from diverse reference models, substantially enhancing preference learning capabilities compared to the single-reference DPO. Our experiments demonstrate that LLMs finetuned with MRPO generalize better in various preference data, regardless of data scarcity or abundance. Furthermore, MRPO effectively finetunes LLMs to exhibit superior performance in several downstream natural language processing tasks such as HH-RLHF, GSM8K and TruthfulQA.

# Introduction

Large Language Models (LLMs) have emerged as powerful tools in natural language processing, capable of generating human-like text and performing a myriad of language-related tasks (Lewkowycz et al. 2022; Achiam et al. 2023; Touvron et al. 2023). However, aligning these models with human intentions and values remains a challenging endeavor (Wang et al. 2023). Aligning LLMs with curated human feedback emerges as a critical solution to guide LLM response behavior and address this challenge. Preference models like the Bradley-Terry model (Bradley and Terry 1952) are often used to measure the alignment of reward functions with empirical preference data, facilitating an alignment framework using reinforcement learning with human feedback (RLHF (Christiano et al. 2017)). The framework aims to optimize the preference models (maximizing preference reward) while ensuring that LLM updates do not stray too far from a base reference LLM model (minimizing a Kullback-Leibler (KL) divergence). While RLHF has been successful in enhancing the helpfulness and accuracy of model-generated content (Ouyang et al. 2022; Stiennon et al. 2020), it is unstable, complicated, and resource-intensive.

Recent advancements, such as direct preference optimization (DPO (Rafailov et al. 2023)) and other likelihood-based preference learning (Zhao et al. 2023; Azar et al. 2023; Ethayarajh et al. 2024; Chen et al. 2024), have sought to replace the cumbersome RLHF with closed-form supervised losses. Although they have demonstrated impressive performance compared to RLHF and supervised finetuning (SFT), their exclusive reliance on a single reference model restricts their potential, overlooking the advantages of harnessing multiple pretrained LLMs. Using multiple reference models offers two key benefits: (i) it enhances robustness and generalization by combining models that excel at different inputs to improve overall performance across diverse data and tasks; (ii) multiple references introduce stronger regularization, leveraging diverse constraints to build a more resilient system less prone to overfitting and reward hacking. This is increasingly important as the open-source community consistently introduces new pretrained/SFT LLMs of varying scales, trained on diverse datasets (Touvron et al. 2023; Penedo et al. 2023; Jiang et al. 2023). It underscores the necessity for a solution that employs multiple references for LLM finetuning, enabling the distillation of knowledge from existing LLMs to enhance the alignment training stage. Unfortunately, none of the prior works have proposed a solution for utilizing multiple reference LLMs in direct preference optimization.

The absence of such solutions stems from three challenges in formulating closed-form multiple-reference preference learning. Firstly, deriving a closed-form solution for the RLHF objective with multiple referencing constraints is nontrivial due to the non-linearity of multiple KL terms. Secondly, reference models with varying architecture, size, and pretraining data may produce diverging outputs given the same input. This divergence could potentially confuse the learning process, leading to unstable training, worse than single-reference approaches. Thirdly, determining the contribution of each reference model during training poses a challenge, requiring extensive tuning. In this paper, we tackle these three challenges, presenting a simple and viable framework for direct preference optimization utilizing multiple reference models.

To address the non-linearity of KL divergence, we propose maximizing a simpler surrogate lower bound that allows for the derivation of a novel closed-form solution incorporating multiple reference models. Our solution is theoretically and empirically proven superior to combining multiple DPO losses. Next, we propose a clipped trust-regions optimization (CTRO) to address the second challenge. By clipping the log probability of diverging reference policy, we force the mismatch to be minimal to facilitate stable training while retaining useful information from the reference policy to guide the optimization. More importantly, the clipping rate is dynamically adjusted according to the predicted likelihood of the data, enabling a more adaptable update. Lastly, to automate the process of determining the contribution of each reference model, we introduce a dynamic mechanism (ARWC) to calculate the weight of each KL term based on the confidence of the referencing LLMs.

Our holistic framework, dubbed Multiple Reference Preference Optimization (MRPO), undergoes evaluation across various tasks. In preference learning tasks involving 6 preference datasets, MRPO demonstrates significant superiority over DPO and multi-reference baselines, especially when preference data is limited with improvement of up to $7 \%$ . In terms of helpfulness evaluation, MRPO significantly outperforms DPO by $1 3 . 7 \%$ . Furthermore, on general language understanding benchmarks like the HuggingFace Open LLM Leaderboard (Beeching et al. 2023), MRPO exhibits average enhancements of $3 \%$ compared to SFT and $1 . 2 \%$ compared to DPO. Certain tasks show more than $5 \%$ improvements over DPO. Importantly, these enhancements are evident across various configurations, including different numbers of reference models (2 or 3) and architectures of LLMs (Llama, Mistral, and Qwen). We perform a comprehensive ablation study to demonstrate the efficacy of CTRO and ARWC mechanisms. Finally, we demonstrate that MRPO, when implemented correctly, adds minimal computation overhead compared to DPO, maintaining efficiency while enhancing performance.

# Background Problem Formulation and Notations

We rely on Azar et al. (2023) to formally define the problem and notations. Given an input $x \in \chi$ where $\chi$ is the finite space of input texts, a policy $\pi$ models a conditional probability distribution $\pi ( y | x )$ where $y \in \mathcal { V }$ is the output in the finite space of output texts. From a given $\pi$ and $x$ , we can sample an output as ${ \bar { y } } \sim \pi \left( \cdot | x \right)$ . Preference data is generated by sampling two outputs $( y , y ^ { \prime } | x )$ from policies $\pi$ and $\mu$ and presenting them to an agent, normally a human, for rating to indicate which one is preferred. For example, $y \succ y ^ { \prime }$ denotes $y$ is preferred to $y ^ { \prime }$ . A preference dataset is then denoted as $\mathcal { D } = \left\{ y _ { w } ^ { i } , y _ { l } ^ { i } | x ^ { i } \right\} _ { i = 1 } ^ { N }$ where $N$ is the number of data points, $y _ { w }$ and $y _ { l }$ denote the preferred (chosen) and dispreferred (rejected), respectively. Assuming that there exists a true model of preference of the agent $\bar { p } ^ { * } \left( y \succ y ^ { \prime } | x \right)$ that assigns the agent’s probability of $y$ being preferred to $y ^ { \prime }$ given $x$ . Using dataset $\mathcal { D }$ , our goal is to find a policy $\pi$ maximizing the expected preference while being close to a reference policy $\pi _ { r e f }$ , which results in the following optimization problem:

$$
\begin{array} { c } { { m a x \displaystyle \begin{array} { c } { { E } } \\ { { \pi \sim \sim \rho } } \end{array} [ { \boldsymbol { \varPsi } } \left( p ^ { * } \left( { \boldsymbol { y } } \succ { \boldsymbol { y } } ^ { \prime } | { \boldsymbol { x } } \right) \right) ] - \beta D _ { K L } \left( \pi \left. \pi _ { r e f } \right. \right) } } \\ { { \displaystyle \begin{array} { c } { { y ^ { \prime } \sim \mu ( \cdot | { \boldsymbol { x } } ) } } \\ { { \boldsymbol { y } } \sim \pi ( \cdot | { \boldsymbol { x } } ) } \end{array} } } \end{array}
$$

where $\rho$ is the input distribution, $\varPsi$ is a scaled function, $D _ { K L }$ is the Kullback–Leibler divergence and $\beta$ is a hyperparameter. Usually, $\pi$ is initialized as $\pi _ { r e f }$ for stable optimization.

# Preference Learning with Reward Function and Reinforcement Learning

In this approach, Bradley-Terry model (Bradley and Terry 1952) is employed as the preference model:

$$
p \left( y \succ y ^ { \prime } | x \right) = \sigma \left( r _ { \theta } \left( x , y \right) - r _ { \theta } \left( x , y ^ { \prime } \right) \right)
$$

where $\sigma$ denotes the sigmoid function and $r : \mathcal { X } \times \mathcal { Y }  \mathbb { R }$ is a reward model parameterized by $\theta$ , which assigns a scalar score to indicate the suitability of output $y$ for input $x$ . In earlier works (Christiano et al. 2017), the reward model is trained on $\mathcal { D }$ to minimize the negative log-likelihood loss:

$$
\mathcal { L } _ { R } = - \mathbb { E } _ { x , y _ { w } , y _ { l } \sim \mathcal { D } } \left[ \log \sigma \left( r _ { \theta } \left( x , y _ { w } \right) - r _ { \theta } \left( x , y _ { l } \right) \right) \right]
$$

Given a trained reward model $r$ , and the scaled function as $\begin{array} { r } { \varPsi \left( q \right) = \log \left( \frac { q } { 1 - q } \right) \forall q : 0 < q < 1 } \end{array}$ , the objective in Eq. 1 can be rewritten as:

$$
\begin{array} { r l r } & { } & { \underset { \pi } { m a x } \underset { x \sim \rho } { E } \left[ r \left( x , y \right) \right] - \beta D _ { K L } \left( \pi \left. \pi _ { r e f } \right. \right) } \\ & { } & { y { \sim } \pi ( \cdot | x ) } \end{array}
$$

This RLHF objective is employed to train LLMs such as Instruct-GPT (Ouyang et al. 2022) using PPO (Schulman et al. 2017).

# Direct Preference Optimization

Reward training and RL finetuning require significant resources and can be cumbersome. Recent approaches circumvent these challenges by directly optimizing the policy via minimizing a preference-based negative log-likelihood loss (Rafailov et al. 2023), $\mathcal { L } _ { D P O }$ :

$$
- \mathbb { E } _ { \stackrel { x } { y _ { w } } \sim \mathcal { D } } \log \sigma \left( \beta \log \frac { \pi _ { \theta } \left( y _ { w } | x \right) } { \pi _ { r e f } \left( y _ { w } | x \right) } - \beta \log \frac { \pi _ { \theta } \left( y _ { l } | x \right) } { \pi _ { r e f } \left( y _ { l } | x \right) } \right)
$$

aTnhei tmeprlmic $\begin{array} { r } { r _ { \theta } \left( x , y | \pi _ { r e f } \right) = \beta \log \frac { \pi _ { \theta } \left( y | x \right) } { \pi _ { r e f } \left( y | x \right) } } \end{array}$ lpolvayes ahle. r(o2l0e2o3f) proved that minimizing this loss is equivalent to solving the optimization problem in Eq. 4.

# Method Multi-Reference Preference Optimization

In this paper, we are focused on situations involving $K$ reference policies $\left\{ \pi _ { r e f } ^ { k } \right\} _ { k = 1 } ^ { K }$ . Therefore, extending from

Eq. 4, our objective can be formulated as a multi-reference RLHF objective:

$$
\underset { \underset {  y \sim \pi ( \cdot \vert x ) } {  \vphantom { \int } }  \kern - delimiterspace } { m a x } \underset {  x \sim \rho  } { E } [ r ( x , y ) ] - \beta ( \sum _ { k = 1 } ^ { K } \alpha _ { k } D _ { K L } ( \pi  \pi _ { r e f } ^ { k } ) )
$$

where $\alpha _ { k }$ are weighting coefficients for each reference policy and $\textstyle 1 \ = \ \sum _ { k = 1 } ^ { K } \alpha _ { k }$ . The main policy can only be initialized as on oPf $\{ \pi _ { r e f } ^ { k } \} _ { k = 1 } ^ { K }$ . Without loss of generality, we denote $\pi _ { r e f } ^ { 1 }$ as the initializing reference policy for the main policy $\pi _ { \boldsymbol { \theta } }$ . Previous studies have explored this objective, showing improvements over single-reference constraints and convergence proof in pure RL settings (Le et al. 2022).

However, addressing this optimization problem in LLMs through reward learning and RL finetuning poses similar challenges to using RL for Eq. 4. Hence, we propose an alternative approach that leverages direct preference optimization for the scenario involving multiple reference policies. We aim to find a closed-form solution for the multireference RLHF objective in Eq. 6. Unfortunately, deriving an exact closed-form solution is challenging due to the nonlinearity of $D _ { K L }$ terms. To circumvent this, we suggest obtaining a closed-form solution for a surrogate objective, which serves as a lower bound for the multi-reference RLHF objective. We summarize our findings as a proposition below.

Proposition 1. The following policy is the optimum for a lower bound of the RLHF objective (Eq. 6):

$$
\pi ^ { * } \left( y | x \right) = \frac { 1 } { Z \left( x \right) } \tilde { \pi } _ { r e f } \left( y | x \right) \exp \left( \frac { 1 } { \beta } r \left( x , y \right) \right)
$$

Proof. See Appendix A.1.

Following the derivation in Rafailov et al. (2023) with our proposed optimal policy $\pi ^ { * }$ , we have the associated direct preference loss function, $\mathcal { L } _ { M R P O }$ , as follow,

$$
\begin{array} { r } { - \mathbb { E } \underset { y _ { w } \sim \mathcal { D } } { x } \log \sigma \left( \beta \log \frac { \pi _ { \theta } \left( y _ { w } | x \right) } { \tilde { \pi } _ { r e f } \left( y _ { w } | x \right) } - \beta \log \frac { \pi _ { \theta } \left( y _ { l } | x \right) } { \tilde { \pi } _ { r e f } \left( y _ { l } | x \right) } \right) } \end{array}
$$

The loss function is similar to the DPO loss (Eq. 5), but instead of using a single reference policy $\pi _ { r e f }$ , we substitute it with a ”virtual” reference policy $\tilde { \pi } _ { r e f }$ that aggregates information from all multiple reference policies.

# Clipped Trust-Regions Optimization (CTRO)

An issue that may arise when multiple reference policies are involved is the mismatch between the reference policy and the main policy. This is less common with singlereference DPO, as the main policy and the reference policy share the same origin, ensuring a small mismatch across training. However, with multiple-reference policies, those not chosen to initialize the main policy can result in significantly different probabilities compared to the main one (due to differences in architectures, pretraining, and tokenizers), potentially leading to unstable training and, at times, loss divergence. Yet, it is crucial to leverage diverse likelihood views on the generated output to ensure generalization.

To address this dilemma, we propose to constrain the virtual reference policy $\tilde { \pi } _ { r e f }$ in the vicinity of the initializing reference policy $\pi _ { r e f } ^ { 1 }$ . As such, we propose to clip the logprobability of the other reference policies $\pi _ { r e f } ^ { k > 1 }$ as follows,

$$
\begin{array} { r l } & { \log \hat { \pi } _ { \mathrm { r e f } } ^ { k > 1 } \left( y | x \right) = \operatorname* { m i n } \Big ( \operatorname* { m a x } \left( \log \pi _ { r e f } ^ { k > 1 } \left( y | x \right) , \right. } \\ & { \qquad \left. ( 1 + \epsilon ) \log \pi _ { r e f } ^ { 1 } \left( y | x \right) \right) , ( 1 - \epsilon ) \log \pi _ { r e f } ^ { 1 } \left( y | x \right) \Big ) } \end{array}
$$

where $\epsilon$ defines the vicinity range around $\pi _ { r e f } ^ { 1 }$ . Then we will replace $\pi _ { r e f } ^ { k > 1 }$ with $\hat { \pi } _ { r e f } ^ { k > 1 }$ in the $\tilde { \pi } _ { r e f }$ defined in Proposition 1.

Using a fixed $\epsilon$ is overly restrictive and suboptimal since different data and policies may require different trust region ranges. Thus, we suggest an adaptive approach to define $\epsilon$ based on the predicted likelihood of the data. Essentially, suppose the log probability of a reference model for a given data point is large, indicating high reliability. A conservative update should be applied (smaller $\epsilon$ ) to exploit the reference policy. Conversely, for lower log probabilities, we may prefer more exploration, allowing reference values to diverge from the initial $\pi _ { r e f } ^ { 1 }$ (bigger $\epsilon$ ). That is,

$$
\epsilon \left( y | x \right) = \epsilon _ { m a x } \frac { \left| \sum _ { k = 1 } ^ { K } \log \pi _ { r e f } ^ { k } \left( y | x \right) \right| } { \sum _ { y ^ { \prime } } \left| \sum _ { k = 1 } ^ { K } \log \pi _ { r e f } ^ { k } \left( y ^ { \prime } | x \right) \right| }
$$

where $\epsilon _ { m a x }$ is a hyperparameter specifying the maximum ratio for an updating range. It is worth noting that since $0 ~ \leq ~ \pi \left( \cdot \right) ~ \leq ~ 1$ , the log-probability is always negative, meaning a larger absolute value of the log-probability indicates less confidence. The denominator represents the sum of all possible output $y ^ { \prime }$ , serving as a normalization factor. Usually, we only have two outputs $( y _ { w } , y _ { l } )$ per input $x$ so the denominator is the sum of two terms.

# Adaptive Reference Weighting Coefficients (ARWC)

If we have no preference or prior knowledge of the reference policies, we can simply use $\alpha _ { k } = 1 / K \forall \bar { k }$ . However, if we assume that the reference policy obtains a reasonable ability to differentiate between $y _ { w }$ and $y _ { l }$ such that even when it make wrong preference (i.e $\log y _ { l } { > } \log y _ { w } )$ the likelihood difference should not be too large, we can introduce an automatic mechanism to determine the value of $\alpha _ { k }$ based on the confidence of the reference policy. Specifically, we examine the absolute difference between the log-probability of the two outputs $( y _ { w } , y _ { l } )$ as an indicator of the policy’s confidence in its ability to discriminate between two outputs. In essence, a larger difference suggests that the policy distinguishes one output from another more decisively. Formally, we propose to adaptively compute reference weighting coefficients as:

$$
\alpha _ { k } = \frac { \left| \log \pi _ { r e f } ^ { k } \left( y _ { w } | x \right) - \log \pi _ { r e f } ^ { k } \left( y _ { l } | x \right) \right| } { \sum _ { i = 1 } ^ { K } \left| \log \pi _ { r e f } ^ { i } \left( y _ { w } | x \right) - \log \pi _ { r e f } ^ { i } \left( y _ { l } | x \right) \right| }
$$

The coefficient is normalized across reference policies, giving greater weight to those with higher discriminative confidence.

# Comparison with Multiple DPO

When having multiple reference policies, a naive solution for direct preference learning is to combine multiple DPO losses (Multi-DPO):

$$
\begin{array} { l } { \mathcal { L } _ { M u l t i - D P O } = - \mathbb { E } _ { x , y _ { w } , y _ { l } \sim \mathcal { D } } } \\ { \displaystyle \sum _ { k = 1 } ^ { K } \alpha _ { k } \log \sigma \left( \beta \log \frac { \pi _ { \theta } \left( y _ { w } | x \right) } { \pi _ { r e f } ^ { k } \left( y _ { w } | x \right) } - \beta \log \frac { \pi _ { \theta } \left( y _ { l } | x \right) } { \pi _ { r e f } ^ { k } \left( y _ { l } | x \right) } \right) } \end{array}
$$

We can show that our proposed MRPO is more desirable than this Multi-DPO loss. As such, let the implicit reward $\begin{array} { r l r } { r _ { \theta } ( x , y | \pi _ { r e f } ^ { k } ) } & { { } = } & { \beta \log \pi _ { \theta } ( y | x ) / \pi _ { r e f } ^ { k } ( y | x ) } \end{array}$ , we find that the gradients of the likelihood of outputs are scaled by the reward error $\sigma \left( r _ { \theta } \left( x , y _ { l } \vert \tilde { \pi } _ { r e f } \right) - r _ { \theta } \left( x , y _ { w } \vert \tilde { \pi } _ { r e f } \right) \right)$ and $\begin{array} { r l } & { \sum _ { k = 1 } ^ { K } \alpha _ { k } \sigma \left( r _ { \theta } \left( x , y _ { l } \middle | \pi _ { r e f } ^ { k } \right) - r _ { \theta } \left( x , y _ { w } \middle | \pi _ { r e f } ^ { k } \right) \right) } \end{array}$ corresponding to MRPO and Multi-DPO, respectively. Under mild assumptions, we can prove the following result:

Proposition 2. Assume that reference policies are constrained to be relatively close to each other, ensuring $\left\{ d _ { k } = r _ { \theta } \left( x , y _ { l } \middle | \pi _ { r e f } ^ { k } \right) - r _ { \theta } \left( x , y _ { w } \middle | \pi _ { r e f } ^ { k } \right) \right\} _ { k = 1 } ^ { K }$ share the same sign $\forall k$ , then

$$
\begin{array}{c} \begin{array} { r l } { \left\{ \lvert \nabla _ { \theta } \mathcal { L } _ { M R P O } \rvert \right.} & { { } \geq \lvert \nabla _ { \theta } \mathcal { L } _ { M u l t i - D P O } \rvert \ \forall d _ { k } \geq 0 } \\ { \lvert \nabla _ { \theta } \mathcal { L } _ { M R P O } \rvert } & { { } \leq \lvert \nabla _ { \theta } \mathcal { L } _ { M u l t i - D P O } \rvert \ \forall d _ { k } \leq 0 } \end{array}   \end{array}
$$

Proof. See Appendix A.2.

As a result, when the reward estimations are wrong, i.e., $\forall d _ { k } > 0$ , MRPO update magnitude will be greater than that of Multi-DPO, which is desirable because we want to fix the likelihood faster to correct the implicit reward. On the contrary, when the reward estimations are right, i.e., $\forall d _ { k } < 0$ , MRPO update magnitude will be smaller than that of MultiDPO, stabilizing the convergence and reducing over-fitting.

# Experimental Results

In our experiments, we will always refer to the first (initializing) reference model as RefM1, the second as RefM2, and so forth. The Base model is the original LLM that will undergo finetuning on preference data and is initialized as RefM1. Throughout experiments, if not stated otherwise, Llama (L), Mistral (M), and Qwen (Q) refer to Llama-2- 7b-chat-hf, OpenHermes-2.5-Mistral-7B, and $\ Q w e n l . 5 – 7 B \cdot$ Chat, respectively. Unless specified otherwise, we finetune these LLMs using LoRA 4-bit quantization to enable faster training and accommodate our hardware of a single Tesla A100 GPU with 32GB of memory. Further training details are provided in Appendix B.1. The source code can be accessed at: https://github.com/thaihungle/MRPO.

To assess model performance, we finetune the model with preference data and evaluate it on preference learning and general language understanding (GLU) tasks. In preference learning, we measure the Preference Accuracy in predicting whether two responses, $y _ { 1 } | \boldsymbol { x }$ and $y _ { 2 } | { x }$ are chosen (preferred) or rejected (dispreferred). In particular, for MRPO, we use $\dot { \beta } \log \frac { \pi _ { \theta } ( y | x ) } { \tilde { \pi } _ { r e f } ( y | x ) }$ as the implicit reward $r ( x , y )$ for each response, with the response classified as chosen or rejected other methods, we use $\beta \log { \frac { \pi _ { \theta } ( y | x ) } { \pi _ { r e f } ( y | x ) } }$ as the implicit reward considering is the Reward Margin, which quantifies the difference between the chosen and rejected rewards: $r ( x , y _ { w } ) - r ( x , y _ { l } )$ . In helpfulness evaluation, we adopt Win Rate suggested by GPT-4 evaluator (Achiam et al. 2023). In GLU, we employ the GLU Metric provided by the task. All measurements are desirable when they are higher.

# Performance when Preference Data is Scarce

Datasets In real-world scenarios, human feedback is limited. Here, we curate 3 small preference datasets (hundreds to a few thousand data points) to simulate the scarcity of feedback data. Each training dataset comprises a random subset from a larger public preference dataset available in the Hugging Face data repository. The remaining portions of the dataset will be utilized as testing data. These datasets are generated with inputs, outputs, and preference rankings often produced by powerful LLMs like GPT-4, making them suitable for training smaller LLMs such as Llama. The datasets are labeled as S1, S2, and S3, and their details are given in Appendix Table 6 and Appendix B.2.

Baselines DPO represents the standard single-reference approach. We note that, as will be shown in our experiments, this is a strong baseline because it is non-trivial to leverage multiple reference models. Multiple-reference methods include Multi-DPO as described earlier, RLHFour RL version using reward model (Christiano et al. 2017) and PPO to directly optimize Eq. 6, and KD which leverage a knowledge distillation loss (Agarwal et al. 2024) $\begin{array} { r l } {  { ( \beta \sum _ { k = 2 } ^ { K } \alpha _ { k } D _ { K L } \big ( \pi \big | | \pi _ { r e f } ^ { k } \big ) \big ) } } \end{array}$ together with the original DPO loss. KD aims for DPO optimization using one reference model $k = 1 \AA ,$ ) while forcing the optimized policy (student) similar to other reference models (teachers, $k > 1$ ). Note that this method requires sampling $y$ from the main policy during training to estimate the $D _ { K L }$ , which makes the process slow.

Unless stated otherwise, all baselines the same common hyperparameters such as learning rate $( 1 0 ^ { - 5 } )$ , batch size (8), number of epochs (3), and $\beta = 0 . 1$ . For Multi-DPO, we have to use clipped trust regions to ensure RefM2 is close to RefM1. Otherwise, the learning will not converge. To make a fair comparison, both MRPO Multi-DPO, and KD use $\epsilon _ { m a x } = 0 . 1$ and incorporate the adaptive $\epsilon$ . Multi-DPO, RLHF and KD use their best fixed $\alpha \in \{ 0 . 1 , 0 . 5 , 0 . 9 \}$ tuned using dataset S1. For Multi-DPO, RLHF, KD, and MRPO, we consider 2 reference models ( $K = 2$ ), and examine 2 possible modes of initialization: (1) $\mathbf { L }  \mathbf { M }$ , the Base model is initialized as Llama (RefM1) for all baselines, and the RefM2 is Mistral for multi-reference methods; (2) $\mathbf { M }  \mathbf { L }$ , the order is reversed.

Table 1: Final mean $\pm$ std. testing accuracy $( \times 1 0 0 )$ on small datasets over 5 runs. Bold denotes the best, statistically different from the others as Cohen effect size $\ge 0 . 5$ . Italic denotes the second best.   

<html><body><table><tr><td rowspan="2">Dataset</td><td colspan="2">S1</td><td colspan="2">S2</td><td colspan="2">S3</td></tr><tr><td>L←M</td><td>M←L</td><td>L←M</td><td>M←L</td><td>L←M</td><td>M←L</td></tr><tr><td>DPO</td><td>93.9±1.4</td><td>98.7±0.7</td><td>94.4±2.9</td><td>96.7±1.8</td><td>54.8±5.4</td><td>52.2±3.7</td></tr><tr><td>RLHF</td><td>67.6±3.6</td><td>71.7±1.2</td><td>83.3±2.0</td><td>88.8±0.7</td><td>50.6±1.2</td><td>49.9±5.5</td></tr><tr><td>KD</td><td>78.4±7.3</td><td>86.8±2.8</td><td>94.3±2.4</td><td>96.3±2.4</td><td>46.9±7.0</td><td>51.8±3.8</td></tr><tr><td>Multi-DPO</td><td>95.6±1.7</td><td>98.0±0.8</td><td>95.5±1.2</td><td>96.3±1.2</td><td>41.1±3.6</td><td>50.8±0.8</td></tr><tr><td>Mean Ref</td><td>95.9±1.6</td><td>97.9±2.1</td><td>95.6±1.8</td><td>96.6±1.2</td><td>45.6±4.4</td><td>51.3±1.2</td></tr><tr><td>MRPO(Ours)</td><td>97.2±1.7</td><td>99.5±0.8</td><td>97.0±3.2</td><td>97.0±1.6</td><td>61.3±5.2</td><td>56.0±1.9</td></tr></table></body></html>

Results Table 1 reports the final preference accuracy on test sets. MRPO consistently outperforms DPO by a significant margin, $3 - 7 \%$ and $1 { - } 4 \%$ for $\mathbf { L }  \mathbf { M }$ and $\mathbf { M } \gets$ L, respectively. Mode $\mathbf { L }  \mathbf { M }$ observes more improvement because Mistral is stronger than Llama in these tasks and thus, using Mistral as RefM2 will bring more benefits than using Llama. Overall, MRPO regularizes the main policy using both Llama and Mistral, creating a model that surpasses each individually. On the other hand, Multi-DPO underperforms DPO and MRPO in many cases, indicating that combining multiple references in a naive way, even when equipped with our CTRO and ARWC, cannot yield favorable results. RLHF performs the worst, likely due to the instability of RL optimization, especially under multiple reference constraints. KD also underperforms compared to DPO and MRPO, highlighting the challenges of applying knowledge distillation to alignment problems without theoretical support for the naive KD approach.

Appendix Fig. 1 depicts the testing preference accuracy curves over the training duration for all methods across 2 initialization modes. These curves demonstrate that all methods, except RLHF in some cases, boost the chosen/rejection prediction accuracy of the Base model (the first evaluation point in each graph is lower than the following points). Among all, MRPO exhibits early outperformance compared to other baselines and maintains its superior performance until convergence.

# Can MRPO Scale to Big Preference Datasets?

Datasets To assess the scalability of MRPO to real and large datasets, we utilize 3 big preference datasets: HelpSteer, Ultrafeedback, and Nectar (see Appendix Table 7). Each dataset employs human rankings to assess the outputs generated by powerful LLMs. Finetuning LLMs for just one epoch is sufficient for large datasets to achieve learning convergence. We use the provided train/test split for HelpSteer and Ultrafeedback. We randomly allocate $90 \%$ of the data for training purposes and $10 \%$ for testing for Nectar. Baselines and Results In this task, we evaluated MRPO $\ : ( K = 2 ) \ :$ ) against DPO, the top two methods from our prior tests, using the same initialization approaches detailed earlier. The preference accuracy result, reported in Table 2’s upper row and Appendix Fig. 2, demonstrates that

MRPO consistently surpasses DPO in real-world preference datasets, showing an improvement gain of approximately 3- $5 \%$ and up to $1 \%$ for ${ \bf L } { \bf {  } } { \bf M }$ and $\scriptstyle \mathbf { M }  \mathbf { L }$ modes, respectively. Since Preference Accuracy can sometimes be unclear in demonstrating performance, especially with borderline inputs where the chosen and rejected ground truth may not be entirely accurate, we also examine the Reward Margin of DPO and MRPO on these datasets. The result, displayed in Table 2’s lower row and Appendix Fig. 3, demonstrates MRPO’s superior ability to separate chosen and rejected outputs, as evidenced by a significantly higher Reward Margin of $10 \%$ compared to DPO. The findings confirm MRPO’s capability to effectively scale with large datasets for preference learning tasks.

# MRPO Helpfulness Assessment

To assess the generalization of MRPO in generating helpful responses, we employ the MRPO and DPO models finetuned on the Nectar dataset (as in the previous section). These models are evaluated on 2 datasets: the helpful and harmless HH-RLHF (Bai et al. 2022) and Alpaca-Eval 2.0 (Li et al. 2023). We input queries from this dataset into LLMs trained by MRPO and DPO, record their respective responses (up to 200 tokens), and utilize GPT-4o to determine the more helpful method.

Given budgetary constraints, our evaluation was limited to the initial 300 test set samples. Table 3 presents the results. On HH-RLHF data, MRPO outperformed DPO with a win rate of $4 5 . 0 \%$ to $3 1 . 3 \%$ . On Alpaca-Eval, MRPO outperforms DPO by achieving a higher win rate $2 4 . 0 \%$ vs $1 5 . 3 \%$ ). These results suggest that training an LLM with MRPO enhances its ability to generate helpful responses due to exposure to diverse reference models. Details of the evaluation and sampled results are provided in Appendix B.2.

# How Effective is MRPO on General Language Understanding Benchmarks?

For our evaluation benchmark, we utilized the Huggingface Open LLM Leaderboard, a standard in the field (Beeching et al. 2023). In this benchmark, we explore a variety of datasets, collectively covering tasks such as math (GSM8k) multi-task language understanding (MMLU), human falsehood understanding (TruthfulQA), and commonsense reasoning (Arc, HellaSwag, Winogrande). The evaluation process presents the language models with few-shot incontext examples and questions. We apply the standard evaluation protocol to evaluate and report average scores (GLU Metrics) across all datasets.

<html><body><table><tr><td rowspan="2">Metric</td><td rowspan="2">Dataset</td><td colspan="2">HelpSteer</td><td colspan="2">Ultrafeedback</td><td colspan="2">Nectar</td></tr><tr><td>L←M</td><td>M←L</td><td>L←M</td><td>M←L</td><td>L←M</td><td>M←L</td></tr><tr><td rowspan="2">Accuracy</td><td>DPPO</td><td>68.9±0.4</td><td>70.8±5.3</td><td>69.9±0.9</td><td>72.0±1.9</td><td>75.6±3.9</td><td>78.7±3.6</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="2">Margin</td><td>DPO</td><td>0.64±0.01</td><td>0.95±0.20</td><td>0.70±0.07</td><td>1.14±0.12</td><td>1.52±0.08</td><td>2.65±0.29</td></tr><tr><td>MRPO</td><td>0.77±0.07</td><td>1.05±0.20</td><td>0.82±0.10</td><td>1.27±0.26</td><td>1.73±0.05</td><td>3.13±0.25</td></tr></table></body></html>

Table 2: Final mean±std. testing preference accuracy (upper, $\times 1 0 0 \AA$ and reward margin (lower) on big datasets over 3 runs. Bold is best, statistically different from others as Cohen effect size $\ge 0 . 5$ .

Table 3: Win, Tie, and Lose rate $( \% )$ : MRPO vs DPO in measuring helpfulness of responses. Bold denotes the better.   

<html><body><table><tr><td colspan="3">HH-RLHFDataset</td></tr><tr><td>Model Win ↑</td><td>Tie</td><td>Lose↓</td></tr><tr><td>DPO MRPO</td><td>31.3 23.7 45.0 23.7</td><td>45.0 31.3</td></tr><tr><td>Alpaca-EvalDataset</td><td></td><td></td></tr><tr><td>Model Win个</td><td>Tie</td><td>Lose↓</td></tr><tr><td>DPO 15.3</td><td>60.7</td><td>24.0</td></tr><tr><td>MRPO 24.0</td><td>60.7</td><td>15.3</td></tr></table></body></html>

In this experiment, we consider the third reference model RefM3 as Qwen to verify the scalability of our method to $K = 3$ on the standard GLU benchmark. We examine the following initialization modes: (1) $\mathbf { L }  \mathbf { M }$ , Q where Mistral and Qwen are additional reference models for Base model Llama, (2) $\mathbf { M }  \mathbf { L }$ , $\mathrm { \Delta Q }$ where Llama and Qwen are additional reference models for Base model Mistral, and (3) $\mathbf { Q } \gets \mathbf { M }$ , L where Mistral and Llama are additional reference models for Base model Qwen. Following prior practices (Chen et al. 2024), we adopt full finetuning to finetune the Base models on Ultrafeedback dataset using DPO and MRPO and evaluate the LLMs using Language Model Evaluation Harness library (Gao et al. 2024). We report all results in Table 4. MRPO leads to a notable enhancement in the Base model of $3 . 5 \%$ , $1 . 4 \%$ , and $0 . 8 \%$ , surpassing DPO with an average improvement of $1 . 1 \%$ , $1 . 0 \%$ and $1 . 3 \%$ for initialization (1), (2) and (3), respectively. Notably, there are several cases in which MRPO can outperform DPO by a huge margin, such as $6 . 8 \%$ in GSM8K $\mathbf { M }  \mathbf { L }$ , Q), $5 . 8 \%$ in TruthfulQA $\mathbf { L }  \mathbf { M }$ , Q) and $5 \%$ in GSM8K $\mathbf { \Omega } ( \mathbf { Q } \gets \mathbf { M }$ , L). We also explore MRPO $K = 2$ , $\mathbf { L }  \mathbf { M }$ and $\mathbf { M }  \mathbf { L }$ ) and observe that this variant consistently surpasses DPO in performance, albeit falling short of MRPO $\ K \ = \ 3 ,$ ), suggesting the advantages of incorporating more reference models (see Appendix Table 8).

# Ablation Study

The Importance of Clipped Trust-Region Optimization (CTRO) To investigate the role of CTRO, we utilize the small yet relatively challenging dataset S3 and conduct experiments with various $\epsilon _ { m a x }$ values: $\epsilon _ { m a x } = 0$ (MRPO equals DPO), $\begin{array} { c c l } { { \epsilon _ { m a x } } } & { { = } } & { { 1 0 ^ { 6 } \mathrm { ( N o ~ c l i p ) } } } \end{array}$ , and $\begin{array} { r l } { \epsilon _ { m a x } } & { { } = } \end{array}$ $\{ 1 , 0 . 1 , 0 . 0 1 \}$ . We also examine different reference models: (i) We employ RefM2 (Llama supervised tuning on Alpaca dataset) as a finetuned model of RefM1 (Llama), aiming to ensure that RefM2 is closely related to RefM1 (same family); and (ii) RefM2 (Mistral) is from a different family from RefM1 (LLama), which indicates a larger mismatch between the two reference models. Here, different families mean that LLMs can vary in architecture, tokenizer and/or be pretrained on distinct corpora, where the sentence-level log probability of these LLMs for the same input can differ by hundreds of units. Table 5 reports the final preference accuracy.

We observe that for setting (ii) when $\epsilon _ { m a x } \ \geq \ 1$ , the training loss can escalate significantly, reaching values as high as 10, and occasionally even infinity, highlighting the instability of training in the absence of CTRO. This is evident in the poor performance of $\epsilon _ { m a x } ~ = ~ \{ 1 0 ^ { 6 } , 1 \}$ . Utilizing CTRO with small $\epsilon _ { m a x }$ results in more stable training. However, excessive constraint, where $\epsilon _ { m a x }$ is too small, can lead to nearly identical performance compared to DPO. In setting (i), the training is stable even with big $\epsilon _ { m a x }$ . However, the performance is not as good as setting (ii). In particular, with the best $\epsilon _ { m a x } ~ = ~ 0 . 1$ , in setting (i), MRPO only achieves a $2 \%$ improvement over DPO, whereas in setting (ii), the improvement gap widens to $7 \%$ . This discrepancy is understandable because without a diverse reference source, RefM2 may not exhibit significant advantages over RefM1, thereby limiting the extent of improvement. Therefore, we conclude that it is more advantageous to leverage diverse reference models, and employing CTRO is required to ensure training stability.

Is adaptive $\epsilon$ necessary? We conduct more experiments with fixed $\epsilon = 0 . 1$ and adaptive $\epsilon _ { m a x } = 0 . 1$ on S1, S2 and S3 using Llama and Mistral for RefM1 and RefM2, receptively. The results depicted in Appendix Fig. 4 (top) illustrate that fixed $\epsilon$ is still better than DPO, and adaptive $\epsilon$ outperforms significantly fixed $\epsilon$ across all datasets, emphasizing the importance of this mechanism in MRPO.

Analyzing Reference Weighting Coefficients In this section, using adaptive $\epsilon _ { m a x } = 0 . 1$ , we compare the adaptive $\alpha$ (ARWC) mechanism proposed in $\ S$ with different fixed values of $\alpha ~ = ~ \{ 0 . 1 , 0 . 5 , 0 . 9 \}$ on S1, S2 and S3 using Llama and Mistral for RefM1 and RefM2, receptively. As shown in Appendix Fig. 4 (bottom), adaptive $\alpha$ demonstrates competitive performance, either outperforming or closely matching the performance of the best fixed $\alpha$ across all datasets. Given the minor discrepancies observed and the associated cost of hyperparameter tuning, we have opted to utilize adaptive $\alpha$ for all other experiments.

Table 4: $\mathbf { M } {  } \mathbf { L }$ : test performance $\left( \times 1 0 0 \right)$ across HuggingFace Open LLM Leaderboard datasets. Bold denotes bes   

<html><body><table><tr><td>Dataset</td><td>GSM8K</td><td>TruthfulQA</td><td>HellaSwag</td><td>MMLU</td><td>Arc-easy</td><td>Winograde</td><td>Avg.</td></tr><tr><td>Base (Mistral)</td><td>49.6</td><td>44.5</td><td>62.8</td><td>60.8</td><td>83.5</td><td>74.4</td><td>62.6</td></tr><tr><td>DPO</td><td>53.7</td><td>53.3</td><td>66.6</td><td>60.1</td><td>82.7</td><td>73.6</td><td>65.0</td></tr><tr><td>MRPO (M←L,Q)</td><td>60.5</td><td>51.43</td><td>65.83</td><td>61.5</td><td>84.0</td><td>73.4</td><td>66.1</td></tr><tr><td>Base (LLama)</td><td>23.9</td><td>37.8</td><td>57.8</td><td>46.4</td><td>60.8</td><td>66.4</td><td>51.0</td></tr><tr><td>DPO</td><td>22.0</td><td>39.5</td><td>59.4</td><td>46.3</td><td>73.5</td><td>67.3</td><td>51.4</td></tr><tr><td>MRPO (L←M,Q)</td><td>24.3</td><td>45.3</td><td>57.9</td><td>46.4</td><td>74.1</td><td>66.7</td><td>52.4</td></tr><tr><td>Base (Qwen)</td><td>21.1</td><td>53.6</td><td>58.8</td><td>60.1</td><td>68.3</td><td>65.2</td><td>54.5</td></tr><tr><td>DPO</td><td>18.7</td><td>54.8</td><td>60.4</td><td>60.3</td><td>65.2</td><td>64.3</td><td>54.0</td></tr><tr><td>MRPO (Q←M,L)</td><td>23.7</td><td>54.9</td><td>59.0</td><td>60.1</td><td>68.4</td><td>65.4</td><td>55.3</td></tr></table></body></html>

<html><body><table><tr><td>Setting</td><td></td><td>(max =0 (DPO)</td><td></td><td>(ma = 10 (No cip)</td><td>(ma =1)</td><td></td><td>(m= (i1</td><td></td><td>m</td><td>= 0:01</td></tr><tr><td>Test Acc.</td><td>0.54</td><td>0.54</td><td>0.51</td><td>0.45</td><td>0.53</td><td>0.49</td><td>0.56</td><td>0.61</td><td>0.54</td><td>0.55</td></tr></table></body></html>

Table 5: Clipped Trust-Region Optimization impact on S3. In setting (i), two reference models belong to the same family but differ in the finetuning dataset, whereas in setting (ii), they are from different families of LLMs. The reported numbers are the mean accuracy over 5 runs.

# Related Works

The integration of human input has been instrumental in advancing the performance of Large Language Models (LLMs) in diverse domains, such as question answering (Nakano et al. 2021), document summarization (Stiennon et al. 2020), and dialog applications (Thoppilan et al. 2022). Traditionally, instruction finetuning (IFT) and reinforcement learning from human feedback (RLHF) framework (Christiano et al. 2017; Ouyang et al. 2022; Lee et al. 2023) has employed RL to align Large Language Models (LLMs). The RLHF objective is to maximize a reward score derived from human preferences (chosen or rejected) while simultaneously minimizing the disparity between the new and initial policy. Recently, there has been a significant move towards closed-form losses, exemplified by DPO (Rafailov et al. 2023), which directly finetunes LLM on offline preference datasets, consolidating RLHF’s reward learning and policy adjustments into a single stage. This “direct” approach is favored over RLHF due to their maximum-likelihood losses, demonstrating superior speed and stability than RL pipeline.

Other direct preference optimization methods (Zhao et al. 2023; Azar et al. 2023) formulate various adaptations of closed-form losses to attain the RLHF objective. Recent advancements have expanded beyond the traditional binary preference data, focusing on novel human preference models like Kahneman-Tversky value functions (Ethayarajh et al. 2024). All these methods adhere to the fundamental framework of RLHF, striving to fit a preference model while ensuring the updates remain close to a reference model. In contrast, our approach is the first direct preference finetuning framework with multiple reference models.

Another line of work creates new preference data from LLM’s own generated outputs, typically through self-training paradigms (Chen et al. 2024; Yuan et al. 2024; Pattnaik et al. 2024), employing multiple reference data pairs (Pattnaik et al. 2024). Our method is orthogonal to self-playing approaches, featuring a faster alternative procedure as it does not require additional data generation. In contrast to methods employing multiple rewards, mixture of experts, and model merging (Jang et al. 2023; Rame et al. 2024), our approach does not involve training/loading multiple LLMs, which is expensive. Instead, we train a single LLM using (pre-computed) log-probability outputs of multiple reference LLMs, and thus much more time/memory-efficient. During testing, our inference cost is the same as using a single LLM.

While knowledge distillation (Lin et al. 2020; Agarwal et al. 2024) could simplify constraining LLMs with multiple reference models, finding a compatible KD loss for direct preference optimization with strong theoretical support is non-trivial. Additionally, directly using RL to optimize preference rewards with multiple KL constraints, as in previous studies (Le et al. 2022), is slow, and unstable—similar to the issues faced by RLHF. In contrast, our MRPO provides a more direct and efficient solution.

# Discussion

In this paper, we present Multi-Reference Preference Optimization (MRPO), a novel method leveraging multiple reference models to improve preference learning for Large Language Models (LLMs). We theoretically derive the objective function for MRPO and conduct experiments with LLMs like LLama2 and Mistral, demonstrate their enhanced generalization across six preference datasets, helpfulness evaluation, and competitive performance in six downstream natural language processing tasks. Our study is limited by using only up to three reference models of modest size. Future research will explore the scalability of MRPO, examining its performance with larger $K$ values and a broader range of LLM sizes across diverse benchmarks.