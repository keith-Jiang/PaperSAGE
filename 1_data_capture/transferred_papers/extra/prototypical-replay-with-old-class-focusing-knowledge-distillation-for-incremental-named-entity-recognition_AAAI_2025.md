# Prototypical Replay with Old-class Focusing Knowledge Distillation for Incremental Named Entity Recognition

Zesheng $\mathbf { L i u } ^ { 1 , 2 }$ , Qiannan $\mathbf { Z } \mathbf { h } \mathbf { u } ^ { 4 , 5 }$ ,Cuiping $\mathbf { L i } ^ { 1 , 2 * }$ , Hong Chen1,3

1School of Information, Renmin University of China, Beijing, China 2Key Laboratory of Data Engineering and Knowledge Engineering, MOE, China 3Engineering Research Center of Database and Business Intelligence, MOE, China 4School of Artificial Intelligence, Beijing Normal University, Beijing, China 5Engineering Research Center of Intelligent Technology and Educational Application, MOE, China {lzs2022,licuiping,chong}@ruc.edu.cn, zhuqiannan $@$ bnu.edu.cn

# Abstract

Catastrophic forgetting is a key challenge in incremental named entity recognition (INER). Existing methods often address this issue through distillation-based approaches, which involve transferring previously learned knowledge from the old model to the new one. However, these methods may not fully equip the new model with an adequate understanding of the characteristics about old entity types, leading to confusion when classifying tokens associated with these entity types. To address this challenge, we propose a novel method called Prototypical Replay with Old-class Focusing Knowledge Distillation (POF) for INER. Our approach focuses on preserving the main characteristics of each previous entity type by storing compact prototypes and replaying them with appropriate frequency. This replay strategy makes the new model review the knowledge of old entity types while minimizing storage needs. Additionally, we introduce an old-class focusing knowledge distillation (OFKD) loss, which distills features only in old-class regions to maintain the quality of old-class prototypes and prevent ineffective prototypical replay while preserving sufficient plasticity for learning new entity types. We conduct experiments on three benchmark datasets (i.e., Few-NERD, I2B2 and OntoNotes5), and the results demonstrate that our method outperforms all previous state-of-the-art methods.

# 1 Introduction

Named Entity Recognition (NER) plays a crucial role in the field of Natural Language Processing (NLP), benefiting various downstream applications such as question answering (Li et al. 2019; Longpre et al. 2021), web search (Fetahu et al. 2021; Guo et al. 2009; Mokhtari et al. 2019), sentiment analysis (Yasavur et al. 2014; Yang et al. 2018), and more. NER involves extracting entities from unstructured text and categorizing them into predefined entity types or non-entity type (Ma and Hovy 2016). In traditional NER approaches, it is typically assumed that no new entity types will emerge during the testing phase, and all predefined entity types are learned simultaneously during training. However, real-world scenarios often demand continuous recognition of newly encountered entity types, namely Incremental Named Entity

Recognition (INER) (Parisi et al. 2019; Thrun 1998). A challenge arises as traditional NER methods struggle to handle entity types not seen during training, necessitating the development of effective INER techniques (Monaikul et al. 2021; Xia et al. 2022; Zheng et al. 2022) that can incrementally learn NER models using training samples containing only new entity types.

INER must address a major challenge: catastrophic forgetting (Goodfellow et al. 2013; Kirkpatrick et al. 2017; McCloskey and Cohen 1989; Robins 1995), which it inherits from incremental learning. This occurs because fine-tuning only on new data can cause previously learned knowledge to be forgotten. Consequently, previous INER methods (Zhang et al. 2023a; Zheng et al. 2022; Monaikul et al. 2021; Qiu et al. 2024) have adopted distillation-based approaches to effectively tackle this challenge by transferring knowledge from the old model to the new one. However, these methods fall short in enabling the new model to fully understand the characteristics of the old entity types. Because these methods mainly rely on constraints on top-level model parameters and lack the process of relearning low-level features of old entity types. As the learning steps increase, the new model will gradually weaken its memory about old entity types, leading to confusion when classifying tokens from these entity types and resulting in numerous false positives. That would mean that a token of an old entity type may be incorrectly recognized as non-entity type, other old entity types, or new entity types. A simple solution is to save raw texts of all previous steps and replay them in subsequent steps, but this way requires a significant amount of storage.

To address aforementioned issue, we propose a novel method called Prototypical Replay with Old-class Focusing Knowledge Distillation (POF) for Incremental Named Entity Recognition (INER). Our method employs a prototypical replay (PR) strategy, which involves the storage of key information after each training step. Specifically, we retain a compact prototype for each entity type, generated from associated features, as well as relevant statistical data linked to these prototypes. This enables the PR strategy to replay previously learned entity types at strategic intervals during subsequent steps, facilitating a revisit of old entity types. In contrast to save and replay raw texts, PR strategy also significantly reduces storage requirements duo to it only stores very little information. Additionally, to complement our PR strategy and prevent catastrophic forgetting, it is crucial to maintain the features of old entity types during subsequent steps. Otherwise, the features of old entity types may drift away from the stored prototypes, compromising the effectiveness of the PR strategy. To tackle this problem, we introduce an old-class focusing knowledge distillation (OFKD) loss, which constrains the changes in the features of old entity types, while still allowing enough flexibility to learn new entity types.

In summary, this paper makes several key contributions:

• We propose a prototypical replay (PR) strategy, which stores compact prototypes with essential statistical data for old learned entity types, enabling the new model to revisit the knowledge of old entity types while learning new entity types, thus eliminating the classification confusion towards the tokens belonging to old entity types. • We introduce an old-class focusing knowledge distillation (OFKD) loss which is designed to preserve the learned knowledge or features of old entity types, effectively integrating with our PR strategy while ensuring sufficient flexibility for learning new entity types. • We conduct extensive experiments on eight INER settings of three datasets (i.e., Few-NERD (Ding et al. 2021), I2B2 (Murphy et al. 2010) and OntoNotes5 (Hovy et al. 2006)). Experimental results demonstrate that our POF achieves significant improvements over the previous state-of-the-art methods in INER.

# 2 Related Work

# 2.1 Incremental Learning

Incremental learning learns continuous tasks without compromising the performance of previous tasks (Dong et al. 2023a,b; Wang et al. 2024). We mainly classify existing incremental learning methods into three ways: replay-based, regularization-based, and dynamic architecture-based. The replay-based methods (Channappayya, Tamma et al. 2024; Liu et al. 2023; Lin et al. 2023; Qi, Zhao, and Li 2022) learn new tasks by integrating saved or generated old samples into the current training samples. Regularization-based methods impose constraints on network weights (Schwarz et al. 2018; Zenke, Poole, and Ganguli 2017), intermediary features (Zhai et al. 2024; Song et al. 2023; Han et al. 2023), or output probabilities (Zajac, Tuytelaars, and van de Ven 2023) to alleviate catastrophic forgetting. The methods based on dynamic architecture (Razdaibiedina et al. 2023; Gao et al. 2023) dynamically extends model architecture to learn new tasks.

# 2.2 Incremental Named Entity Recognition

INER seamlessly combines the principles of incremental learning with traditional NER approaches, resulting in a powerful framework that facilitates continuous learning and adaptation of the model to new entity types. ExtendNER (Monaikul et al. 2021) utilizes knowledge distillation by transferring output probabilities from the old model (teacher model) to the new model (student model).

L&R (Xia et al. 2022) introduces a two-stage framework called learning and review: the learning phase uses knowledge extraction similar to ExtendNER, while the review phase uses replay-based methods to expand the current training set with composite samples of old entity types. CFNER (Zheng et al. 2022) introduces a causal framework for INER and uses the old model to identify non-entity type belonging to previous entity types to extract causal effects, and utilizing course learning strategies to mitigate recognition errors. DLD (Zhang et al. 2023b) divides the logits into positive and negative two parts for knowledge distillation. RDP (Zhang et al. 2023a) designs a task relation distillation scheme among different incremental steps and develop a prototypical pseudo-labeling strategy for classification. IS3 (Qiu et al. 2024) addresses both E2O and O2E semantic shifts in INER. However, they primarily rely on distillation-based methods to prevent catastrophic forgetting. Nonetheless, these methods may not fully enable the new model to grasp the nuances of old entity types, potentially leading to classification confusion in those cases.

In contrast, we design a strategy termed prototypical replay (PR) for INER. This approach aims to enable the new model to revisit the features of old entity types while learning new knowledge. Instead of replaying raw texts, PR only replays prototypes, which incurs a minimal storage cost and does not necessitate extra data. Additionally, we introduce an old-class focusing knowledge distillation (OFKD) loss to coordinate with our PR strategy.

# 3 Task Formulation

INER is a model training approach that aims to progressively learn and expand its knowledge of different entity types over a series of steps, denoted as $t = \{ 1 , . . . , T \}$ . Each step has its own unique training set, denoted as $D ^ { t }$ , which consists of $( X ^ { t } , Y ^ { t } )$ pairs. Here, $X ^ { t }$ represents an input token sequence with length of $\vert X ^ { t } \vert$ , and ${ \bar { Y } } ^ { t }$ represents the corresponding ground truth labels. It’s important to note that the labels in $Y ^ { t }$ only include the current entity types $\mathcal { E } ^ { t }$ . Any labels for future entity types $\scriptstyle { \mathcal { E } } ^ { t + 1 : T }$ or previous entity types $\mathcal { E } ^ { 1 : t - 1 }$ are collapsed into non-entity type $e _ { o }$ . It’s worth mentioning that the datasets between different steps have no overlap, meaning that $\mathcal { E } ^ { t } \cap \mathcal { E } ^ { 1 : t - 1 } = \emptyset$ and $\mathcal { E } ^ { t } \cap \mathcal { E } ^ { \dot { t } + 1 : T } = \varnothing$ . At each step $t$ $( t > 1 )$ , the objective is to update a new model $M ^ { t }$ that is capable of recognizing entities from all entity types seen up to that point, represented by $\textstyle \bigcup _ { i = 1 } ^ { t } { \mathcal { E } } ^ { i }$ , and $M ^ { t }$ consists of an encoder $\Phi _ { \Theta ^ { t } }$ with paramete sS $\Theta ^ { t }$ and a classifier $\Psi _ { \Omega ^ { t } }$ with parameters $\Omega ^ { t }$ .

# 4 Methodology

This paper proposes a novel method called POF to address classification confusion of the new model towards old entity types in INER, shown in Figure 1. Our method includes three parts: Class-incremental Learning (CIL), Prototypical Replay Strategy (PR), and Old-class Focusing Knowledge Distillation (OFKD). Among them, CIL is the fundamental learning process of INER task, PR is to solve the above challenge, and OFKD is mainly to team up with our PR strategy.

![](images/daa862ee4a7202db499e126fff2320c9b03228a72e82afa7fe9a3b87280cde3f.jpg)  
Figure 1: The overall framework of our POF, demonstrated by a simplified INER example. Firstly, old entity type prototypes have been saved in earlier steps. Nextly, pseudo label $\hat { Y } ^ { t }$ combined by old prediction and ground truth is to learn new entity types at current step $t$ . Simultaneously, old entity type prototypes are replayed into the current classifier $\Psi _ { \Omega ^ { t } }$ . Finally, we update parameters of the new model $M ^ { t }$ with all cross entropy losses (e.g., $\mathcal { L } _ { c e } ^ { o }$ and $\mathcal { L } _ { c e . } ^ { p }$ ) and old-class focusing knowledge distillation loss (i.e., $\mathcal { L } _ { o f k d } )$ , and update the prototypes of new entity types to old ones.

And they will be introduced in detail separately in the following subsections.

# 4.1 Class-incremental Learning

At incremental step $t$ , since the dataset $D ^ { t }$ only contains the labels of the new entity types $\mathcal { E } ^ { t }$ , if the model $M ^ { t }$ is learning directly on $D ^ { t }$ , the model will forget the recognition ability of the old entity types, resulting in catastrophic forgetting. Moreover, the learned old entity types are treated as non-entity type in $D ^ { t }$ , which shifts the semantics of nonentity type, thus aggravating catastrophic forgetting. To address this problem, we adopt a naive pseudo-labeling which makes use of prediction of the old model in the last step. Specifically, given one sample pair $( X ^ { t } , Y ^ { t } )$ , we first feed ${ \bar { X ^ { t } } }$ into the old model $M ^ { t - \hat { 1 } }$ and the new model $M ^ { t }$ to generate the old prediction $P ^ { t , t - 1 }$ with old features $f ^ { t , t - 1 }$ and the new prediction $P ^ { t }$ with new features $f ^ { t }$ , respectively. Then, we re-label the tokens marked as $e _ { o }$ in the ground truth of current sample $Y ^ { t }$ according to the entity types gained by $P ^ { t , t - 1 }$ , and ultimately obtain the combined label $\hat { Y } ^ { t }$ . Nextly, we use $\hat { Y } ^ { t }$ to supervise the current prediction $P ^ { t }$ with the cross-entropy loss:

$$
\mathcal { L } _ { c e } ^ { o } = - \frac { 1 } { | X ^ { t } | } \sum _ { i = 1 } ^ { | X ^ { t } | } \hat { Y } _ { i } ^ { t } \log P _ { i } ^ { t }
$$

where $P _ { i } ^ { t }$ represents the probability score that the model predicts for $i$ -th token in $X ^ { t }$ .

# 4.2 Prototypical Replay Strategy

As previously mentioned, the new model struggles with classifying tokens from old entity types due to its limited knowledge or memory of them. To address this issue, we propose to create prototypes for each old entity type and replay these prototypes at an appropriate frequency during subsequent steps. This process makes the prototypes pass through the classifier alongside the features extracted from input texts, which allows the new model to revisit and reinforce its understanding for old entity types, thereby reducing its classification confusion. Thus, this strategy ensures that the new model can better distinguish between each old entity type and other entity types, ultimately improving the overall performance.

Specifically, after training at each step $\tau ( \tau < t )$ , we firstly calculate the number of tokens from each current entity type $e \in \mathcal { E } ^ { \tau }$ in the single-step training set $D ^ { \tau }$ to determine the replay frequency of subsequent steps:

$$
\mathcal { N } _ { e } = \sum _ { i = 1 } ^ { | D ^ { \tau } | } \sum _ { j = 1 } ^ { | X _ { i } ^ { \tau } | } \mathbb { I } \{ Y _ { i , j } ^ { \tau } = e \}
$$

where $X _ { i } ^ { \tau }$ indicates the $i$ -th token sequence in $D ^ { \tau }$ , $j$ indicates the $j$ -th token in $X _ { i } ^ { \tau }$ , and $\mathbb { I }$ is the indicator function. Since each entity type is considered as a foreground class in a single step, $\mathcal { N } _ { e }$ is the number of tokens that appear in entity type $e$ running through the entire dataset, which can decide the frequency of replaying prototype of entity type $e$ in subsequent steps.

Then, in order to construct replay samples for each entity type $e \in \mathcal { E } ^ { \tau }$ in future steps, we calculate the feature center of $e$ to capture its main characteristics based on new features $f _ { i , j } ^ { \tau }$ in current step $\tau$ :

$$
\mu _ { e } = \frac { \frac { 1 } { \mathcal { N } _ { e } } \sum _ { i = 1 } ^ { | D ^ { \tau } | } \sum _ { j = 1 } ^ { | X _ { i } ^ { \tau } | } ( \hat { f } _ { i , j } ^ { \tau } \odot \mathbb { I } \{ Y _ { i , j } ^ { \tau } = e \} ) } { \left\| \frac { 1 } { \mathcal { N } _ { e } } \sum _ { i = 1 } ^ { | D ^ { \tau } | } \sum _ { j = 1 } ^ { | X _ { i } ^ { \tau } | } ( \hat { f } _ { i , j } ^ { \tau } \odot \mathbb { I } \{ Y _ { i , j } ^ { \tau } = e \} ) \right\| _ { 2 } }
$$

where $\odot$ denotes the element-wise multiplication, $\left\| \cdot \right\| _ { 2 }$ computes the L2-norm, and $\hat { f } _ { i , j } ^ { \tau }$ refers to the L2-normalized fea

$\begin{array} { r } { \hat { f } _ { i , j } ^ { \tau } = \frac { f _ { i , j } ^ { \tau } } { \left\| f _ { i , j } ^ { \tau } \right\| _ { 2 } } } \end{array}$ $\mu _ { e }$ cates the mean direction of all feature vectors for $e$ serving as a prototype that embodies the typical characteristic of $e$ . However, relying solely on this prototype during the replay in subsequent steps may result in inadequately robust training outcomes, as a prototype only provides a single representation of entity type features without encompassing any variability. Therefore, we also compute the standard deviation of the feature vectors to introduce noise during the replay process. The calculation is given by:

$$
\sigma _ { e } = \sqrt { \frac { 1 } { \mathcal { N } _ { e } } \sum _ { i = 1 } ^ { | D ^ { \tau } | } \sum _ { j = 1 } ^ { | X _ { i } ^ { \tau } | } [ ( \hat { f } _ { i , j } ^ { \tau } - \mu _ { e } ) ^ { 2 } \odot \mathbb { I } \{ Y _ { i , j } ^ { \tau } = e \} ] }
$$

Additionally, since we remove the length information of feature vectors through L2-normalization, we need to restore it during replay. Hence, we also store the mean and standard deviation of the L2-norms of all features for entity type $e$ , denoted as $\hat { \mu } _ { e }$ and $\hat { \sigma } _ { e }$ , which are formulated by:

$$
\hat { \mu } _ { e } = \frac { 1 } { \mathcal { N } _ { e } } \sum _ { i = 1 } ^ { | D ^ { \tau } | } \sum _ { j = 1 } ^ { | X _ { i } ^ { \tau } | } ( \left. \hat { f } _ { i , j } ^ { \tau } \right. _ { 2 } \odot \mathbb { I } \{ Y _ { i , j } ^ { \tau } = e \} ) \in \mathbb { R } ^ { 1 }
$$

$$
\hat { \sigma } _ { e } = \sqrt { \frac { 1 } { \mathcal { N } _ { e } } \sum _ { i = 1 } ^ { | D ^ { \tau } | } \sum _ { j = 1 } ^ { | X _ { i } ^ { \tau } | } ( \left\| \hat { f } _ { i , j } ^ { \tau } \right\| _ { 2 } - \hat { \mu } _ { e } ) ^ { 2 } \odot \mathbb { I } \{ Y _ { i , j } ^ { \tau } = e \} } \in \mathbb { R } ^ { 1 }
$$

After obtaining $n _ { e }$ $\imath _ { e } , \mu _ { e } , \sigma _ { e } ,$ $\hat { \mu } _ { e }$ and $\hat { \sigma } _ { e }$ , robust replay can be implemented in subsequent steps. Specifically, in a future step $t$ , the classifier is typically fed with features extracted solely from the input text. To ensure the new model reviews the knowledge of old entity types, we introduce replayed samples from these old entity type prototypes into the classifier. For an old entity type $e \in \mathcal { E } _ { 1 : t - 1 }$ , we represent one replayed sample using a random variable $r _ { e }$ , which is the product of two Gaussian-distributed random variables: the direction of the feature vector $\xi _ { e } \sim \mathcal G ( \mu _ { e } , \sigma _ { e } ^ { 2 } )$ and the L2-norm $\eta _ { e } \sim \mathcal G ( \hat { \mu } _ { e } , \hat { \sigma } _ { e } ^ { 2 } )$ :

$$
r _ { e } = \xi _ { e } \times \eta _ { e }
$$

Nextly, for each old entity type $e$ , we replay $\mathcal { N } _ { e }$ times for $r _ { e }$ per training epoch. Assuming each epoch consists of $k$ iterations, we evenly distribute $\mathcal { N } _ { e }$ among these iterations. This means $r _ { e }$ is sampled $\frac { \mathcal { N } _ { e } } { k }$ times in each iteration. These random samples, denoted as ${ \mathcal { P } } ^ { t }$ , are then fed into the classifier $\Psi _ { \Omega ^ { t } }$ to compute the total gradient for parameter updating with cross-entropy loss. Formally:

$$
\mathcal { L } _ { c e } ^ { p } = - \frac { 1 } { | \mathcal { P } ^ { t } | } \sum _ { i = 1 } ^ { | \mathcal { P } ^ { t } | } \tilde { Y } _ { i } ^ { t } \log { \tilde { P } _ { i } ^ { t } }
$$

where $\tilde { P } _ { i } ^ { t }$ and $\tilde { Y } _ { i } ^ { t }$ represent output probability and label corresponding to the $i$ -th prototype vector in ${ \mathcal { P } } ^ { t }$ , respectively.

In this way, since each sampling of $r _ { e }$ produces different features and the distribution is controlled by the stored

<html><body><table><tr><td>Datasets</td><td>#Entity Types</td><td>#Training Instances</td><td># Test Instances</td></tr><tr><td>Few-NERD</td><td>66</td><td>131758</td><td>230025</td></tr><tr><td>I2B2</td><td>16</td><td>59376</td><td>41397</td></tr><tr><td>OntoNotes5</td><td>18</td><td>59922</td><td>23836</td></tr></table></body></html>

Table 1: The statistical information for each NER dataset.

standard deviation, the new model can capture the ample latent characteristics of the old entity types to strengthen its memory of them and achieve robust discriminability. Consequently, we can rewrite the original loss in Eq.(1) by adding this loss to it, which is given by:

$$
\mathcal { L } _ { c e } = \mathcal { L } _ { c e } ^ { o } + \alpha \mathcal { L } _ { c e } ^ { p }
$$

where $\alpha$ is a hyper-parameter to balance losses.

# 4.3 Old-class Focusing Knowledge Distillation

The aforementioned PR strategy can enable the new model to revisit and retain knowledge of the old entity types, helping to prevent confusion of classification caused by faint memory. However, the success of this approach depends on the stability of the old-class features across different model training steps. If significant changes occur in these features between steps, discrepancies may arise between the features extracted by the current model and the replayed prototypes generated by the old model. In such cases, the PR strategy could become ineffective due to the outdated nature of the prototypes. Furthermore, since there are no constraints on parameter updates, the model may suffer from uncontrolled catastrophic forgetting of old knowledge. Consequently, our goal is to develop a strategy that maintains features of old entity types, cooperating with our PR strategy and resisting catastrophic forgetting. At the same time, we do not want the model to lose plasticity for learning new entity types.

By balancing above objectives, we propose an old-class focusing knowledge distillation (OFKD) loss, which focuses on distilling the features in the old-class regions. The OFKD loss is constructed to ensure that the consistency of feature representations in old-class regions is preserved. Simultaneously, features in other regions can be updated without constraints, ensuring that the flexibility to learn new entity types remains uncompromised. It achieves this by using the current ground truth to define the old-class regions and employing a constraint based on Mean Squared Error (MSE) in the distillation loss. This constraint helps maintain similarity measurement consistency through a hard loss function, such as L2-distance. The OFKD loss, designed around token features, is formulated as follows:

$$
\mathcal { L } _ { o f k d } = \frac { 1 } { N } \sum _ { j = 1 } ^ { N } w _ { j } * M S E ( f _ { i , j } ^ { t } , f _ { i , j } ^ { t , t - 1 } )
$$

where $N$ represents the number of features, and

$$
w _ { j } = \left\{ \begin{array} { l l } { \begin{array} { r l } { 0 , } & { \mathrm { i f } \quad \hat { Y } _ { i , j } ^ { t } \in \mathcal { E } ^ { t } \quad o r \quad \hat { Y } _ { i , j } ^ { t } = e _ { o } } \\ { 1 , } & { \mathrm { i f } \quad \hat { Y } _ { i , j } ^ { t } \in \mathcal { E } ^ { 1 : t - 1 } } \end{array} } \end{array} \right.
$$

Table 2: Comparisons with baselines on Few-NERD at each incremental step (total 11 steps). The bold denotes the highes result, and the underline denotes the second highest result.   

<html><body><table><tr><td>Method</td><td>A1</td><td>A2</td><td>A3</td><td>A4</td><td>A5</td><td>A6</td><td>A7</td><td>A8</td><td>Ag</td><td>A10</td><td>AT</td><td>A</td></tr><tr><td>PODNet (Douillard et al. 2020)</td><td>48.73</td><td>11.77</td><td>10.33</td><td>9.66</td><td>7.14</td><td>4.89</td><td>5.16</td><td>4.73</td><td>5.18</td><td>3.99</td><td>3.79</td><td>10.49</td></tr><tr><td>LUCIR (Hou et al. 2019)</td><td>48.73</td><td>46.21</td><td>43.23</td><td>37.76</td><td>25.47</td><td>20.31</td><td>24.12</td><td>26.91</td><td>23.33</td><td>21.12</td><td>18.59</td><td>30.52</td></tr><tr><td>ST (De Lange et al. 2019)</td><td>48.73</td><td>41.15</td><td>35.50</td><td>32.94</td><td>27.77</td><td>26.03</td><td>30.34</td><td>32.73</td><td>32.85</td><td>32.14</td><td>30.71</td><td>33.71</td></tr><tr><td>ExtendNER (Monaikul et al.2021)</td><td>48.73</td><td>40.40</td><td>38.42</td><td>32.20</td><td>27.26</td><td>25.48</td><td>30.38</td><td>33.68</td><td>33.69</td><td>32.73</td><td>31.61</td><td>34.05</td></tr><tr><td>CFNER (Zheng et al. 2022)</td><td>48.73</td><td>50.45</td><td>46.33</td><td>47.38</td><td>37.74</td><td>33.65</td><td>36.40</td><td>39.33</td><td>40.09</td><td>40.15</td><td>39.76</td><td>41.82</td></tr><tr><td>DLD (Zhang et al. 2023b)</td><td>48.73</td><td>52.00</td><td>46.40</td><td>40.92</td><td>34.74</td><td>32.22</td><td>35.55</td><td>37.81</td><td>36.94</td><td>37.24</td><td>37.56</td><td>40.01</td></tr><tr><td>RDP (Zhang et al. 2023a)</td><td>48.73</td><td>50.36</td><td>46.40</td><td>45.20</td><td>40.19</td><td>37.57</td><td>38.02</td><td>39.56</td><td>39.76</td><td>38.96</td><td>38.68</td><td>42.13</td></tr><tr><td>IS3 (Qiu et al. 2024)</td><td>48.73</td><td>50.74</td><td>45.87</td><td>39.61</td><td>34.44</td><td>33.71</td><td>37.20</td><td>39.44</td><td>37.57</td><td>37.60</td><td>36.82</td><td>40.16</td></tr><tr><td>POF(Ours)</td><td>48.73</td><td>52.36</td><td>51.31</td><td>50.93</td><td>50.28</td><td>46.52</td><td>47.59</td><td>48.77</td><td>48.59</td><td>48.21</td><td>48.41</td><td>49.25</td></tr><tr><td>Imp.</td><td>-</td><td>1</td><td>-</td><td>-</td><td></td><td>-</td><td></td><td>-</td><td>-</td><td>-</td><td>介9.73</td><td>介7.12</td></tr></table></body></html>

Based on the refined label $\hat { Y } _ { i } ^ { t }$ , we set the distillation loss weight $w _ { j }$ to 1 in the old-class region and to 0 in both the new-class and non-entity type regions. Hence, the final overall loss of our POF is:

$$
\mathcal { L } = \mathcal { L } _ { c e } + \beta \mathcal { L } _ { o f k d }
$$

with a hyper-parameter $\beta$ for balancing losses.

# 5 Experimental Setup

Datasets We conducted experiments on three benchmark datasets, namely Few-NERD (Ding et al. 2021), I2B2 (Murphy et al. 2010), and OntoNotes5 (Hovy et al. 2006). We summarized the statistical data of these datasets in Table 1. The data processing process is consistent with CFNER (Zheng et al. 2022).

Baseline Methods We compare POF with recent INER methods in Sec 2.2, namely ExtendNER, CFNER, DLD, RDP and IS3. Following CFNER, we also compare POF with incremental learning methods used in the field of computer vision, such as Self-Training (ST) (De Lange et al. 2019; Rosenberg, Hebert, and Schneiderman 2005), LUCIR (Hou et al. 2019), and PODNet (Douillard et al. 2020).

INER Settings We utilize FG entity types to train the base model, and PG entity types are introduced at each subsequent incremental learning step, denoted as FG- $\mathbf { \nabla } _ { a }$ -PGb. Specifically, we only employ the FG-6-PG-6 setting for Few-NERD. And for I2B2 and OntoNotes5, we employ four INER settings: FG-4-PG-2, FG-8-PG-1, FG-8-PG-2, and FG-12-PG-1.

Implementation Details Following the baseline methods, we also adopt the ”BIO” labeling scheme across all datasets. Our model utilizes a BERT-based encoder (Devlin et al. 2018) and employs a fully connected layer as the classifier. We use the PyTorch (Paszke et al. 2019) framework to implement the model, which is built on top of the Huggingface (Wolf et al. 2019) implementation. We train the model for 20 epochs if PG is 2, and 10 epochs otherwise. Hyper-parameters $\alpha$ and $\beta$ are set to 1.0 and 2.0, respectively. Meanwhile, we set the batch size and learning rate to 8 and 4e-4, respectively. Also, we introduce distillation loss in ExtendNER to better overcome forgetting. All experiments were conducted on NVIDIA GeForce RTX 3090 with 24GB of memory.

Metrics Following IS3, we compute the final performance report using the Macro-F1 score of the last task $\mathcal { A } _ { T }$ and the average Macro-F1 score across all tasks $\bar { \mathcal { A } }$ . Additionally, we present a line chart for task performance comparison to provide a more detailed analysis.

# 6 Results and Analysis

# 6.1 Main Results

In order to verify the effectiveness of our POF under various INER settings, we conducted experiments on Few-NERD, I2B2 and OntoNotes5 three datasets. The step-wise results of Few-NERD under FG-6-PG-6 setting is shown in Table 2. The results obtained from the I2B2 and OntoNotes5 are shown in Table 3, and the step-wise Macro-F1 score comparisons under eight INER settings on these two datasets are shown in Figure 2.

As shown in Table 2, we can observe that our POF achieves the best result at each incremental step under the FG-6-PG-6 setting of Few-NERD and achieves $9 . 7 3 \%$ improvement in $\mathcal { A } _ { T }$ and $7 . 1 2 \%$ improvement in $\bar { \mathcal { A } }$ compared to the baseline RDP of SOTA. Meanwhile, as depicted in the upper part of Table 3, our POF achieves improvements over the best results of previous SOTA baselines ranging from $0 . 3 4 \%$ to $7 . 2 0 \%$ in $\mathcal { A } _ { T }$ , and $0 . 4 1 \%$ to $5 . 4 6 \%$ in $\bar { \mathcal { A } }$ , under four INER settings of I2B2. Similarly, in the lower part of Table 3, our POF achieves improvements over the best results of previous SOTA baselines ranging from $1 . 4 5 \%$ to $6 . 5 0 \%$ in $\mathcal { A } _ { T }$ , and $3 . 0 8 \%$ to $3 . 8 5 \%$ in $\check { \bar { A } }$ , under four INER settings of OntoNotes5. These results quantitatively demonstrate that our POF outperforms all baselines, as we replay old entity type prototypes when learning new entity types, eliminating classification confusion for the new model on tokens of old entity type and improving the overall performance.

Especially compared to two SOTA baselines RDP and IS3, our POF performs better because these two methods mainly preserve the memory of old entity types mainly through knowledge distillation. However, as the learning steps continue to increase, they still lose or weaken their memory about old entity types, resulting in classification confusion. In the meantime, we can avoid this problem by replaying old entity type prototypes as a forgetting compensation. In addition, although the old entity type prototypes were also used in IS3, this method simply uses the mean

<html><body><table><tr><td rowspan="2">Dataset</td><td rowspan="2">Method</td><td colspan="2">FG-4-PG-2</td><td colspan="2">一 FG-8-PG-1</td><td colspan="2">FG-8-PG-2</td><td colspan="2">一 FG-12-PG-1</td></tr><tr><td>AT</td><td>A</td><td>AT</td><td>A</td><td>AT</td><td>A</td><td>AT</td><td>A</td></tr><tr><td rowspan="10">I2B2</td><td>PODNet (Douillard et al. 2020)</td><td>7.32</td><td>25.48</td><td>8.81</td><td>26.87</td><td>10.47</td><td>30.56</td><td>22.67</td><td>45.38</td></tr><tr><td>LUCIR (Hou et al. 2019)</td><td>29.02</td><td>43.23</td><td>23.68</td><td>36.89</td><td>32.81</td><td>48.62</td><td>33.57</td><td>54.78</td></tr><tr><td>ST (De Lange et al.2019)</td><td>29.75</td><td>40.01</td><td>19.36</td><td>26.90</td><td>22.61</td><td>35.80</td><td>11.18</td><td>33.94</td></tr><tr><td>ExtendNER(Monaikul etal.2021)</td><td>28.53</td><td>38.52</td><td>18.99</td><td>27.78</td><td>25.65</td><td>39.16</td><td>10.27</td><td>32.03</td></tr><tr><td>CFNER (Zheng et al.2022)</td><td>32.82</td><td>47.39</td><td>19.77</td><td>37.18</td><td>37.18</td><td>49.60</td><td>31.50</td><td>48.52</td></tr><tr><td>DLD (Zhang et al. 2023b)</td><td>43.93</td><td>51.46</td><td>28.47</td><td>38.41</td><td>38.76</td><td>50.61</td><td>36.68</td><td>55.49</td></tr><tr><td>RDP (Zhang et al.2023a)</td><td>44.56</td><td>55.21</td><td>50.53</td><td>62.73</td><td>58.50</td><td>64.92</td><td>56.25</td><td>67.03</td></tr><tr><td>IS3 (Qiu et al. 2024)</td><td>45.97</td><td>55.88</td><td>52.24</td><td>60.25</td><td>56.06</td><td>63.33</td><td>69.14</td><td>72.74</td></tr><tr><td>POF (Ours) Imp.</td><td>46.31</td><td>56.29</td><td>59.44</td><td>66.21</td><td>65.49</td><td>70.38</td><td>72.78</td><td>75.70</td></tr><tr><td colspan="2"></td><td>介0.34</td><td>介0.41</td><td>介7.20</td><td>介3.48</td><td>介6.99</td><td>介5.46</td><td>介3.64</td><td>介2.96</td></tr><tr><td rowspan="10"></td><td>PODNet (Douillard et al. 2020)</td><td>16.46</td><td>23.73</td><td>11.06</td><td>21.32</td><td>15.50</td><td>27.56</td><td>6.54</td><td>33.29</td></tr><tr><td>LUCIR (Hou et al. 2019)</td><td>49.17</td><td>53.44</td><td>35.55</td><td>48.51</td><td></td><td>58.25</td><td>37.22</td><td>57.20</td></tr><tr><td>ST (De Lange et al. 2019)</td><td>52.47</td><td></td><td></td><td></td><td>50.47</td><td>54.02</td><td>48.55</td><td>60.72</td></tr><tr><td>ExtendNER (Monaikul et al. 2021)</td><td>49.73</td><td>52.84 51.95</td><td>43.38 38.07</td><td>49.43 48.33</td><td>46.29 47.59</td><td>55.33</td><td>45.02</td><td>55.61</td></tr><tr><td>CFNER (Zheng et al.2022)</td><td>54.83</td><td>57.78</td><td>47.46</td><td>55.68</td><td></td><td>60.35</td><td>59.26</td><td>70.30</td></tr><tr><td>DLD (Zhang et al. 2023b)</td><td>55.67</td><td>56.11</td><td>46.11</td><td>55.03</td><td>51.89 51.73</td><td>59.66</td><td>56.67</td><td></td></tr><tr><td>RDP (Zhang et al. 2023a)</td><td>57.38</td><td>60.17</td><td>56.24</td><td>62.68</td><td>57.83</td><td>64.83</td><td>64.16</td><td>68.15 71.55</td></tr><tr><td>IS3 (Qiu et al. 2024)</td><td>59.29</td><td>60.28</td><td>54.06</td><td>61.06</td><td>62.23</td><td>66.57</td><td>59.90</td><td>66.52</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>POF (Ours) Imp.</td><td>62.67 介3.38</td><td>63.99 介3.72</td><td>57.69 介1.45</td><td>66.53 介3.85</td><td>63.91 介1.68</td><td>69.69 介3.12</td><td>70.66 介6.50</td><td>74.63 介3.08</td></tr></table></body></html>

Table 3: Comparisons with baselines on I2B2 and OntoNotes5. The bold denotes the highest result, and the underline denotes

I2B2 (FG-4-PG-2) I2B2 (FG-8-PG-1) I2B2 (FG-8-PG-2) I2B2 (FG-12-PG-1)   
2460 460 460 460 0 Nu6mbe8r of 1E0 ntit12y Ty14pes 16 N9um1b0er11of 1E2nt1i3ty 1T4yp15es16 Num1b0er of 1E2ntity 1T4ypes 16 0 12 Num1b3er of 1E4ntity 1T5ypes16 OntoNotes5 (FG-4-PG-2) OntoNotes5 (FG-8-PG-1) OntoNotes5 (FG-8-PG-2) OntoNotes5 (FG-12-PG-1) 80 80 80 80   
240 240 240 460 0 4 Nu6 mb8er o10f En12tity1 Ty1p6es 18 0 8 N9 1m0b1e1 1o2 1E3n1t4i 1y5T1y6p1e7s18 8 Nu1m0ber 1o2f En1t4ity T1y6pes18 0 12 Nu13mbe1r4 of 1E5ntit16y Ty17pes18 PODNet LUCIR ST ExtendNER CFNER DLD RDP IS3 POF(Ours)

feature of each old entity type, lacking sufficient diversity to achieve the robust training outcome mentioned in Sec 4.2, so its discriminability is not as good as our POF.

<html><body><table><tr><td></td><td></td><td colspan="4">I2B2</td><td colspan="4">OntoNotes5</td></tr><tr><td>PR</td><td>OFKD</td><td>AT</td><td>Aold</td><td>Anew</td><td>A</td><td>AT</td><td>Aold</td><td>Anew</td><td>A</td></tr><tr><td>×</td><td>√</td><td>55.01</td><td>66.06</td><td>57.27</td><td>67.98</td><td>64.45</td><td>73.47</td><td>53.64</td><td>72.57</td></tr><tr><td>√</td><td></td><td>66.75</td><td>70.10</td><td>73.32</td><td>72.07</td><td>67.77</td><td>73.72</td><td>53.85</td><td>72.81</td></tr><tr><td>√</td><td>√</td><td>72.78</td><td>74.67</td><td>76.57</td><td>75.70</td><td>70.66</td><td>74.70</td><td>61.59</td><td>74.63</td></tr></table></body></html>

Furthermore, as illustrated in Figure 2, our POF outperforms the INER baselines in step-wise Macro-F1 comparisons across the eight settings of I2B2 and OntoNotes5. These results qualitatively confirm the superiority and effectiveness of our POF in continuously learning new entity types compared to competitive baselines and the importance of reviewing the knowledge of old entity types through prototypical replay in our method.

Table 4: The ablation study of our POF under the FG-12-PG1 setting of the I2B2 and OntoNotes5 datasets. For a more detailed analysis, we separately calculate the mean performance on old, new, and all entity types. Compared to our POF, all ablation variants significantly reduced the performance of INER, validating the importance of all components working together to solve INER.

Sentence Global plans to introduce new services including car rental and plane ticket reservations and confirmation RDP PL O O O O O O O O O O O O O O O O O IS3 PL O O O O O O O O O O O O O O O O O POF PL B-ORG O O O O O O O O O O O O O O O O GL B-ORG O O O O O O O O O O O O O O O O Sentence Has Iraq been overshadowed by Katrina and Tom Delay and Harriet Meyers and the Pakistan quake ? RDP PL O B-GPE O O O B-PER O B-PER I-PER O B-PER I-PER O O B-GPE O O IS3 PL O B-GPE O O O B-PER O B-PER I-PER O B-PER I-PER O O B-GPE O O POF PL O B-GPE O O O B-EVE O B-PER I-PER O B-PER I-PER O O B-GPE O O GL O B-GPE O O O B-EVE O B-PER I-PER O B-PER I-PER O O B-GPE O O Sentence Dongguan Hsu Fu Chi Foods which sells candy cakes and can fruit in the mainland domestic market is another good example RDP PL B-PER I-ORG I-WOA I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O IS3 PL B-PER I-PER I-WOA I-PRO I-ORG O O O O O O O O O O O O O O O O O O O O POF PL B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O GL B-ORG I-ORG I-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O

# 6.2 Ablation Study

Table 4 shows the ablation study results of our POF under the FG-12-PG-1 setting, evaluating its two core components: prototypical replay (PR) and old-class focusing knowledge distillation loss (OFKD). Each component is individually removed from the model to evaluate its contribution to overall performance.

Effectiveness of the PR The first row of Table 4 presents the results without PR. Compared to the complete model’s results shown in the last row, it is clear that PR boosts performance by $1 7 . 7 7 \%$ in $\mathbf { \mathcal { A } } _ { T }$ and $7 . 7 2 \%$ in $\bar { \mathcal { A } }$ of I2B2, and $6 . 2 1 \%$ in $\mathcal { A } _ { T }$ and $2 . 0 6 \%$ in $\bar { \mathcal { A } }$ of OntoNotes5. Although PR involves replaying prototypes of old entity types, it also enhances the average recognition accuracy of new entity types, demonstrating $1 9 . 3 0 \%$ improvement of I2B2 dataset, and $7 . 9 5 \%$ improvement of OntoNotes5 dataset. This result stems from PR providing new model with sufficient access to the features of old entity types, which mitigate classifier bias and lessen the chance of misclassifying. Thus, we can observe significant performance improvements in the recognition of both old and new entity types.

Effectiveness of the OFKD The significance of OFKD can be recognized when comparing the second and the last row of Table 3. After removing OFKD loss, the results experience some breakdown, with $6 . 0 3 \%$ $\mathcal { A } _ { T }$ and $3 . 6 3 \%$ $\bar { \mathcal { A } }$ drop of I2B2, and $2 . 8 9 \%$ $\mathcal { A } _ { T }$ and $1 . 8 2 \%$ $\bar { \mathcal { A } }$ drop of OntoNotes5 compared to the full model. Meanwhile, the average recognition rates of both new and old entity types in two datasets have also significantly decreased. So that OFKD is a crucial mechanism in our method which can be used to maintain prior features while flexibly learning new knowledge. Its absence not only reduces the effectiveness of PR, but may also lead to uncontrolled parameter updates, which may result in unexpected catastrophic forgetting. Therefore, the learning of both old and new entity types will be affected, thereby having a marked impact on the overall performance.

# 6.3 Case Study

Figure 3 utilizes the challenging FG-8-PG-2 setting and performs qualitative comparisons on OntoNotes5, demonstrating the superiority of our POF over previous state-of-the-art INER methods. A common bad case in other competitive baselines is the occurrence of false positive samples of old entity types. For instance, in the first example, token Global labeled as old entity type Organization is incorrectly classified as non-entity type. In the second example, token Katrina of the old entity type Event is erroneously recognized as another old entity type Person. In the last example, tokens belonging to old entity type Organization are misclassified as other old entity types Person and Product, as well as new entity type Work of art. These errors stem from inadequate representation and limited knowledge of old entity types during the training of new entity types at each incremental step, leading to classification confusion. In contrast, our POF effectively addresses these issues by providing access to prototypes of old entity types as augmented training samples for the classifier of new entity types.

# 7 Conclusion

In this paper, we present a novel INER method named POF, which addresses the classification confusion associated with old entity types due to the new model’s limited memory about them. POF employs a key strategy known as prototypical replay (PR), enabling the new model to revisit the knowledge of old entity types, thereby eliminating classification confusion to reduce recognition errors from them. By storing and replaying prototypes instead of raw texts, POF can minimize storage costs. Moreover, we propose an oldclass focusing knowledge distillation loss to complement the PR strategy while preserving the flexibility to learn new entity types. Our extensive comparisons with state-of-the-art INER methods and ablation study demonstrate the superiority of our method, as well as significance of each individual component within our approach.