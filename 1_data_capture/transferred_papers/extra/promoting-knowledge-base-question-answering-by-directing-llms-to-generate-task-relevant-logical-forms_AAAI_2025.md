# Promoting Knowledge Base Question Answering by Directing LLMs to Generate Task-relevant Logical Forms

Jianqi Gao1, Jian $\mathbf { C a o } ^ { 1 * }$ , Ranran $\mathbf { B } \mathbf { u } ^ { 1 }$ , Nengjun $\mathbf { Z } \mathbf { h } \mathbf { u } ^ { 2 }$ , Wei Guan1, Hang $\mathbf { Y } \mathbf { u } ^ { 2 }$

1Department of Computer Science and Engineering, Shanghai Jiao Tong University 2School of Computer Engineering and Science, Shanghai University {193139, cao-jian, buranran, guan-wei} $@$ sjtu.edu.cn, {zhu nj, yuhang}@shu.edu.cn

# Abstract

Knowledge base question answering (KBQA) refers to the system that produces answers to user queries by reasoning with a large-scale structured knowledge base. Advanced works have achieved great success either by generating logical forms (LF) or directly generating answers. Although the former typically yields better performance, these generated LF could be inaccurate, e.g., non-executable. In this regard, large language models (LLMs) have shown exciting potential for accurate generation. However, it is challenging to finetune LLMs to generate LF. This is because the context retrieved for prediction typically leads to an excessive number of reasoning paths. In this context, LLMs can generate numerous LF corresponding to these reasoning paths, but a few LF can result in correct answers. Thus, fine-tuning LLMs to generate answer-relevant LF would conflict with the prior knowledge of the LLMs. In this work, we propose a novel learning framework, FM-KBQA, to fine-tune LLMs using multi-task learning for KBQA. Specifically, we propose to fine-tune LLMs using an additional objective: generating the index of reasoning paths that lead to correct answers. This will direct LLMs to pay attention to answer-relevant paths among numerous reasoning paths by completing a simple task where the selected reasoning paths can be supplementary for non-executable LF. Directly generating answers can make LLMs pay attention to the answer-relevant reasoning paths, but it is much more challenging than generating the index of reasoning paths. To verify FM-KBQA’s effectiveness, we conduct experiments on mainstream benchmarks, such as WebQuestionsSP (WQSP) and ComplexWebQuestions (CWQ). Extensive evaluations across two public benchmark datasets underscore the superiority of FM-KBQA over current state-of-the-art methods.

# Introduction

Knowledge base question answering (KBQA) refers to the system that answers natural language questions based on a large-scale structured knowledge base (Miller et al. 2016). Existing methods can be divided into two categories based on the answer-generation approach. Some works propose to generate answers in a retrieving-then-generating manner (Sun, Bedrax-Weiss, and Cohen 2019; Saxena,

Kochsiek, and Gemulla 2022), where question-related information is first retrieved and then processed to generate answers. This is known as the direct-answer-prediction approach. The other works propose to produce answers in a generating-then-retrieving scheme (Luo et al. 2023b; Gu and $\mathtt { S u } 2 0 2 2 )$ , where the question is parsed into some specific forms used to retrieve answers from a knowledge base (KB). This is known as the semantic parsing-based approach.

Producing answers using question-related information, i.e., a subgraph of a structured KB, is a straightforward approach. For example, PullNet (Sun, Bedrax-Weiss, and Cohen 2019) retrieves a subgraph of KB related to the input question and applies graph neural networks to predict the answer entities in the subgraphs. KGT5 (Saxena, Kochsiek, and Gemulla 2022) uses a sequence-to-sequence framework to directly generate answers only based on the input question. Although this approach is intuitive and can always produce answers, it usually underperforms semantic parsingbased methods on public benchmarks (Talmor and Berant 2018a; Gu et al. 2021, 2022). Semantic parsing-based methods mainly focus on parsing the input question into logical forms (LF), which is executed by an external executor, e.g., a SPARQL server, to retrieve answers. To make the generated LF accurate, ReTrack (Chen et al. 2021) uses a grammar-based decoder to generate LF based on pre-defined grammar rules, and a semantic checker to discourage generating programs that are semantically inconsistent with KB. TIARA (Shu et al. 2022a) proposes a multi-grained retrieval method to select relevant KB context for LF generation. Although previous empirical results (Ye et al. 2021; Das et al. 2021; Gu et al. 2022) show that the semantic parsing based methods can produce more accurate answers over benchmark datasets, these generated LF could be inaccurate, e.g., non-executable (Yu et al. 2022a).

Regarding accurate generation, large language models (LLMs) (Touvron et al. 2023; Luo et al. 2023c; Sui et al. 2024; Lu et al. 2022) have shown exciting success in various scenarios. Thus, employing LLMs for LF generation is a promising approach to promoting KBQA. Despite their impressive performance, LLMs have substantial limitations when facing complex knowledge reasoning tasks (Luo et al. 2023c; Sui et al. 2024) that require deep and responsible reasoning. Thus, some works leverage retrieved facts from the KB to prompt LLMs to improve reasoning performance (Karpukhin et al. 2020b; Sun et al. 2023a). Meanwhile, advanced works propose to fine-tune LLMs to generate LF based on retrieved context and queries (Yu et al. 2022b; Luo et al. 2023a).

![](images/e2cfebffa2654dac1ded4b5b32646db1d50f45c38b6c1d479af4460522da7995.jpg)  
Figure 1: Overview of FM-KBQA. FM-KBQA enhances KBQA by providing the correct retrieval paths through an enhanced retriever, and guiding LLMs to generate task-relevant logical forms.

However, it could be challenging to fine-tune LLMs to generate logical forms (LF). This is because the context retrieved for answer prediction typically leads to an excessive number of reasoning paths. In this context, LLMs can generate numerous LF corresponding to these reasoning paths, but merely a few LF can result in correct answers. Thus, finetuning LLMs to generate answer-relevant LF would conflict with LLMs’ prior knowledge, i.e., generating numerous LF. This aligns well with our experimental observations, i.e., results shown in Table 3. Namely, fine-tuning LLMs with queries and the retrieved context would underperform the approach of fine-tuning LLMs with only input queries (Luo et al. 2023b; Yu et al. 2022b). In this regard, recent work proposes to encourage LLMs to predict the answers when fine-tuning LLMs to predict LF, which makes LLMs pay attention to the answer-relevant reasoning paths. However, it is challenging to encourage LLMs to generate both LF and answers simultaneously. The intuition is consistent with the observation in the literature. In particular, directly generating answers is a different approach than generating LF. Therefore, there could be compromises in the optimization process of these objectives, leading to suboptimal performance of either the LF prediction or the direct answer prediction. Although multi-objective optimization has been widely applied to improving multi-task learning (Sener and Koltun 2018), it is challenging to model and mitigate objective conflicts in LLMs fine-tuning. This challenge motivates a fundamental question:

Can we design a simple objective to encourage LLMs to focus on answer-relevant reasoning paths when predicting logical forms?

If the additional objective is simple, we can safely make the main objective, predicting LF, dominate the optimization process. In this work, we give an affirmative answer to this question by proposing a novel learning framework FM-KBQA, to f ine-tune LLMs using multi-task learning for KBQA. Specifically, we propose to fine-tune LLMs using an additional simple objective: generating the index of reasoning paths that lead to correct answers. This will direct LLMs to pay attention to answer-relevant paths among numerous reasoning paths by completing a simple task where the selected reasoning paths can be supplementary for nonexecutable LF. Directly generating answers can make LLMs pay attention to the answer-relevant reasoning paths, but it is much more challenging than generating the index of reasoning paths. To verify FM-KBQA’s effectiveness, we conduct experiments on mainstream benchmarks, such as WebQuestionsSP (WQSP) and ComplexWebQuestions (CWQ). Comprehensive results show that Llama-2-7B, fine-tuned with the proposed FM-KBQA, can outperform all baselines (e.g., GPT4) with the retrieved context, achieving new state-ofthe-art performance.

Our contribution can be summarized as follows.

• We empirically find that, in KBQA, augmenting user queries with the retrieved context to prompt LLMs may fail to improve the quality of predicted logical forms.

• In this work, we attribute the counterintuitive observation to the conflict between the fine-tuning objective and the LLMs’ prior knowledge. Namely, the retrieved context can lead to numerous reasoning paths, thus LLMs can generate numerous corresponding LF. However, the finetuning objective enforces LLMs to generate a few logical forms, leading to conflict.   
• We propose a novel learning framework, FM-KBQA, to fine-tune LLMs with a simple additional objective function, which directs LLMs to focus on answer-relevant reasoning paths by encouraging LLMs to predict the index of answer-relevant reasoning paths. Comprehensive experimental results demonstrate that FM-KBQA can outperform baselines and achieve new state-of-theart performance.

# Related Work

The current KBQA primarily consists of two types: Information Retrieval-based KBQA and Semantic Parsing-based KBQA.

Information Retrieval-based KBQA retrieves information related to the question, then processes it to generate an answer (Sun, Bedrax-Weiss, and Cohen 2019; Saxena, Kochsiek, and Gemulla 2022). It can be divided into two stages: Information Retrieval (Sun et al. 2018; He et al. 2021a) and Knowledge Reasoning (Miller et al. 2016; Sun et al. 2018; Sun, Bedrax-Weiss, and Cohen 2019; Zhang et al. 2021). Information Retrieval aims at selecting relevant triples from the large-scale knowledge graph (KG) to form paths relevant to the question. Recent advancements in dense retrieval such as BM25 (Robertson, Zaragoza et al. 2009), Dense Passage Retrieval (DPR) (Karpukhin et al. 2020a), and Contriever (Izacard et al. 2021) convert queries and documents into low-dimensional dense vectors, and semantic similarity is measured by vector distance metrics (e.g., cosine similarity) to select question-relevant retrieval paths. Knowledge reasoning focuses on inferring the final answer based on the retrieved paths (Sun, Bedrax-Weiss, and Cohen 2019; Jiang et al. 2022a). Recent studies such as UniK-QA (Oguz et al. 2020a) and PullNet (Sun, BedraxWeiss, and Cohen 2019) use specialized network architectures (e.g., graph convolutional networks) to simulate multihop reasoning processes. However, these methods have limited reasoning capabilities. In the area of LLMs, ToG (Sun et al. 2023b) leverages the reasoning capabilities of opensource LLMs to iteratively explore various possible reasoning paths on KG to obtain the final answer. RoG (Luo et al. 2023c) proposes a planning retrieval reasoning framework that collaborates fine-tuning LLMs with KG to achieve faithful and explainable reasoning. FiDeLiS (Sui et al. 2024) proposes a retrieval-exploration interactive method that applies the logic and common sense reasoning of LLMs to the retrieval and reasoning of KG, achieving state-of-the-art performance. However, a large portion of the paths retrieved in large-scale KG are erroneous and invalid, which severely limits the performance of the model during reasoning.

Semantic Parsing-based KBQA focuses on parsing questions into a structured query language (e.g. logical forms) and executes them through a query engine to obtain answers (Lan, Wang, and Jiang 2019; Das et al. 2021; Huang, Kim, and Zou 2021). RnG-KBQA (Ye et al. 2021) enumerates potential LF based on input entities and employs a ranking and generation framework to determine LF. ArcaneQA (Gu and $\mathrm { S u } 2 0 2 2 \AA ,$ ) dynamically generates LF based on intermediate execution results. TIARA(Shu et al. 2022b) enumerates candidate entities, logical forms, and related schemas, feeding these multi-grained contexts into PLMs, and then decodes the final logical form using constraints. In the area of LLMs, DECAF (Yu et al. 2022a) combines LF and retrieved paths to generate answers using LLMs. ChatKBQA (Luo et al. 2023b) generates LF through finetuning LLMs based solely on the input question, and obtains the final answers through query engines. Compared with Information Retrieval-based KBQA, Semantic Parsing-based KBQA gets rid of the more complicated path retrieval process and has a higher accuracy. However, this method may generate some non-executable or invalid LF, which greatly limits its performance.

# Proposed Method

In this section, we begin by presenting the problem’s definition. Subsequently, we outline the implementation of FMKBQA (as shown in Figure 1), which mainly includes three parts: (1) Constructing an enhanced retriever to retrieve reasoning paths from the KG; (2) Building a task-related LF generation module to generate task-related LF; (3) Performing faithful reasoning based on the generated LF and the selected reasoning paths.

# Problem Definition

KBQA is a complex reasoning task aimed at producing answers from KG to respond to user questions. Specifically, given a natural language question $q$ and a KG $\mathcal { G }$ , the goal is to use the relational information $r$ embedded in $\mathcal { G }$ to construct a mapping function $f$ that can accurately predict answers $a$ , where $a \in \mathcal { A } _ { q }$ and $\textstyle { \mathcal { A } } _ { q }$ is the set of possible answers for the question $q$ . Consistent with the previous work (Sun et al. 2020; Jiang et al. 2022b), our work assumes that the entities $e _ { q }$ referenced in $q$ are pre-identified and have been appropriately aligned with their counterparts residing in the KG $\mathcal { G }$ .

# Enhanced Retriever

The retrieval module retrieves the relation paths related to the question from the knowledge base according to the input question. In this regard, there are two main methods: sparse retrieval (Robertson, Zaragoza et al. 2009) and dense retrieval (Karpukhin et al. 2020a). For an input question $q$ , these methods apply the encoder $\mathrm { E C } ( \cdot )$ to obtain its representation and then retrieve passages based on the similarity: Iretrieve $\mathbf { \Sigma } =$ argtop $k _ { i } \big ( \bar { \mathrm { E C } } ( p _ { i } \big ) \cdot \mathrm { E C } ( q ) \big )$ . However, these methods mainly involved setting similarity thresholds for filtering, leading to challenges in adapting to different KG.

In this work, we employ a generative model T5 (Raffel et al. 2020a) for a nuanced understanding of facts, enabling rapid adaptation to filter information across diverse KG. Specifically, we employ a prompt $X \ =$ Question : $q$ , Fact $: r \ :$ to textually represent both the question $q$ and the factual relation $r$ . Then, the prompt $X$ serves as the input for the pre-trained generative model. Our learning objective is to maximize the likelihood of the token $t _ { i }$ given the input text $X$ and the tokens $t _ { < i }$ in the ground class $\bar { k } \in K = \{ \bar { y } e s , n o \}$ with the objective function as follows:

$$
\mathcal { L } _ { \mathrm { e r } } = - \sum _ { i = 1 } ^ { | k | } \mathrm { l o g } P \left( t _ { i } | t _ { < i } , X \right)
$$

where $P \left( t _ { i } | t _ { < i } , X \right)$ is the log-likelihood of the $i$ - th token of the ground class $K$ . We use special tokens “ $< e x t r a _ { - } i d _ { - } O > y e s < e x t r a _ { - } i d _ { - } I >$ or $^ {  } { < } e x -$ tr $\scriptstyle \lambda _ { - } i d _ { - } O > n o < e x t r a _ { - } i d _ { - } I > ^ { , }$ to represent $k$ , making it easier to extract labels “yes” or $^ { \bullet } n o ^ { \cdot \cdot }$ from the generated content. Besides, we propose a training method based on the confidence advantage of the positive set to reduce model sensitivity during the learning process. Specifically, we seek to ensure that the probability of the positive sample set outputting “yes” exceeds the probability of the negative sample set outputting $^ { \bullet } n o ^ { \cdot \cdot }$ .

$$
\mathcal { L } _ { \mathrm { c o n } } = \log \left( 1 + \sum _ { i \in \Omega _ { \mathrm { n e g } } , j \in \Omega _ { \mathrm { p o s } } } e ^ { \lambda ( s _ { i } - s _ { j } ) } \right)
$$

where $\lambda$ is a margin value, $\Omega _ { \mathrm { n { e g } } }$ refers to the negative sample set, and $\Omega _ { \mathrm { p o s } }$ represents the positive sample set and $s$ is the similarity score.

# Task-relevant Logical Forms Generation

Training data preparation: To construct instruction finetuning training data, we first convert the SPARQL corresponding to the natural language questions of the training set in the KBQA dataset into equivalent LF, and then replace the entity IDs in these LF (e.g., “ m.03 dwn”) with corresponding entity labels (e.g., “ Lou Seal”) to allow LLMs to better understand the entity semantics. We then convert natural language questions (e.g., “ Lou Seal is the mascot for the team that last won the World Series when?”) into logical forms (e.g., “ ( ARGMAX ( JOIN ( R [ sports, sports team, championships $\jmath$ ) ( JOIN [ sports, sports team, team mascot $\boldsymbol { { J } } \boldsymbol { { l } }$ Lou Seal ] ) ) $I$ time, event, start date $I ^ { \prime } ) ^ { \prime \prime } )$ .

Task-relevant logical forms generation. According to the transformed LF, we formulate the generation of LF as an optimization problem, aiming to maximize the probability of LF to question $q$ from the KG $\mathcal { G }$ .

$$
P _ { \theta } ( l | q , \mathcal { G } ) = \sum _ { z \in \mathcal { Z } _ { l } } P ( l | q , z , \mathcal { G } ) P _ { \theta } ( z | q )
$$

where $l$ is the ground truth LF, $\theta$ denotes the parameters of LLMs, $z$ denotes the LF generated by LLMs, and $\mathcal { Z } _ { l }$ denotes the set of possible LF. The latter term $P _ { \theta } ( z | q )$ is the probability of generating a LF grounded by KG, the former term $P ( l | q , z , \mathcal { \bar { G } } )$ is the probability of reasoning a LF given the question $q$ , logical forms $z$ , and KG $\mathcal { G }$ , which is computed by the reasoning-retrieval module.

Despite the great potential of LLMs, these generated LF may be inaccurate, e.g., non-executable. In particular, the retrieved context can lead to many reasoning paths, so that the LLMs can generate many corresponding LF. However, the fine-tuning objective forces the LLMs to generate answerrelated LF, which may lead to conflicts. Namely, LLMs can generate numerous LF, while the designed objective is to generate answer-relevant LF. This leads to the conflict of prior knowledge of LLMs. To address this issue, we propose to fine-tune LLMs with a simple additional objective function, which guides the LLMs to focus on answer-related reasoning paths by encouraging it to predict the index of answer-related reasoning paths. Specifically, we formulate the generation of reasoning paths as an optimization problem, aiming to maximize the probability of the reasoning path to question $q$ from the $\operatorname { K G } { \mathcal { G } }$ .

$$
P _ { \theta } ( I _ { r } | q , \mathcal { G } ) = \sum _ { r \in \mathcal { R } _ { \mathrm { p } } } P ( I _ { r } | q , r , \mathcal { G } ) P _ { \theta } ( r | q )
$$

where $I _ { r }$ stands for the index of the reasoning path $r$ , and $R _ { p }$ denotes the set of possible reasoning paths. Note that we are merely interested in the answer-relevant reasoning path. Thus, we have $I _ { r } = \mathsf { N o n e }$ if $r$ is not answer-relevant. The final objective function of FM-KBQA is the combination of LF optimization and reasoning path optimization, which can be formulated as follows.

$$
\mathcal { L } = \log P _ { \theta } ( l | q , \mathcal { G } ) + \log P _ { \theta } ( I _ { r } | q , \mathcal { G } )
$$

Here, we adopt the same LLMs for both LF generation and reasoning path selection, which are jointly trained in a multitask learning manner. We discuss the implementation details of these two tasks in the following subsections.

Fine-tuning process. Given the retrieval relation path and the training data, we design a simple instruction template that takes the question and reasoning path as input:

Given a question: (Question), and a series of Propositions:   
Proposition i: the premise is (Reasoning path), its conclusion is (the statement of the question $^ +$ the tail entity)   
Task 1: Generate logical forms just based on the question.   
Task 2: Verify which deductive reasoning is correct for the given question in a deductive manner, if it exists, return the correct number of deductive reasoning, if doesn’t, return “no”.

Here, Question is the question $q$ . Reasoning path is the reasoning path retrieved through fine-tuned T5, $\mathbf { \chi } _ { i }$ is the $i$ -th proposition generated based on the $i$ -th path. For the statement of the question, we transform the question into a conclusion and fill the position of the conclusion answer entity with a placeholder. This process is generated by designing the prompt input into LLMs GPT3.5. For example, for the question: “what is the name of Justin bieber’s brother”, its conclusion is described as “Justin Bieber’s brother’s name is \*placeholder\*.” Then we can use the last tail entity of the reasoning path to replace \*placeholder\*. This design enables LLMs to select faithful reasoning paths based on the questions, using their expertise in deductive reasoning (Ling et al. 2024; Sui et al. 2024). Then, the LLMs need to generate LF and further output the number of the correct proposition. It is trained according to Eq. 5 through multi-task learning.

# Faithful Reasoning

The faithful reasoning module takes the question $q$ and a set of reasoning paths $r$ as input and generates LF and the correct path index. Similarly, we design a reasoning instruction prompt to guide LLMs in conducting reasoning based on the question and the retrieved reasoning paths. The reasoning process can be written as follows.

$$
\begin{array} { r } { \mathsf { A n s w e r s } = \left\{ \begin{array} { l l } { \arg \operatorname* { m a x } ( P _ { \theta } ( l _ { i } | q , \mathcal { G } ) ) } & { \mathrm { i f ~ } l _ { i } \mathrm { ~ i s ~ e x e c u t a b l e } } \\ { r _ { \mathrm { c o r r e c t } } } & { \mathrm { o t h e r w i s e } , } \end{array} \right. } \end{array}
$$

where $l _ { i }$ represents the $i$ -th logical form. We perform beam search to generate LF and reasoning path index, selecting the executable LF with the highest probability (arg max), and convert it into a SPARQL query to retrieve the answer from the KG. If none of the generated LF are executable, we directly select the reasoning path corresponding to the output index of the LLMs and combine the tail entities of these reasoning paths as the final answer. Namely, we construct $r _ { \mathrm { c o r r e c t } } = \bar { \{ r | I _ { r } \neq \mathrm { N o n e } \} }$ .

# Experiments

# Experiment Settings

In this section, we present the experimental setup, main results, and analysis of our proposed approach.

Datasets. In this paper, we employ two standard KBQA datasets: WebQuestionSP (WQSP) (Yih et al. 2016) and ComplexWebQuestions (CWQ) (Talmor and Berant 2018b), both of which can be reasoned on Freebase KG (Bollacker 2008). For the WQSP dataset, it contains 4,737 simple natural language questions paired with SPARQL queries, For the CWQ dataset, it contains 34,689 more complex questions with SPARQL queries.

Baselines. We compare our method with several existing KBQA approaches.

Rigel (Sen, Saffari, and Oliya 2021) proposes a technique to improve end-to-end question answering by leveraging differentiable KG and adding an intersection operation to handle multiple-entity questions. TIARA (Shu et al. 2022b) enhances question answering over knowledge bases by focusing on relevant contexts and using constrained decoding to reduce errors. UniK-QA (Oguz et al. 2020b) integrates structured, unstructured, and semi-structured knowledge sources by flattening them into text and applying a unified retriever-reader model. UniKGQA (Jiang et al. 2022b) combines retrieval and reasoning by using a unified architecture with semantic matching and information propagation modules for multi-hop KBQA. HGNet (Chen et al. 2022)

Table 1: Comparison results with baseline methods   

<html><body><table><tr><td rowspan="2">Method</td><td colspan="2">WebQSP</td><td colspan="2">CWQ</td></tr><tr><td>F1</td><td>Hits@1</td><td>F1</td><td>Hits @1</td></tr><tr><td colspan="5">Non-LLMs methods</td></tr><tr><td>Rigel (Sen, Saffari,and Oliya 2021)</td><td>-</td><td>73.3</td><td></td><td>48.7</td></tr><tr><td>TIARA (Shu et al. 2022b)</td><td>78.9</td><td>75.2</td><td></td><td>，</td></tr><tr><td>UniK-QA (Oguz et al. 2020b)</td><td>79.1</td><td></td><td>1</td><td>-</td></tr><tr><td>UniKGQA (Jiang et al. 2022b)</td><td>72.2</td><td>77.2</td><td>49.4</td><td>51.2</td></tr><tr><td>HGNet (Chen et al.2022)</td><td>76.6</td><td>76.9</td><td>68.5</td><td>68.9</td></tr><tr><td colspan="5">Prompting-LLMs Only method</td></tr><tr><td>Zero-shot(gpt-4)</td><td>59.71</td><td>62.32</td><td>37.93</td><td>42.71</td></tr><tr><td>Few-shot(gpt-4)</td><td>62.71</td><td>68.75</td><td>43.70</td><td>51.52</td></tr><tr><td>CoT(gpt-4)</td><td>65.37</td><td>72.11</td><td>44.76</td><td>53.51</td></tr><tr><td colspan="5">Prompting- LLMs + KG</td></tr><tr><td>ToG(gpt-3.5)</td><td>72.32</td><td>75,13</td><td>56.96</td><td>57.59</td></tr><tr><td>ToG(gpt-4) (Sun et al. 2023a)</td><td>75.97</td><td>81.84</td><td>60.20</td><td>68.51</td></tr><tr><td>FiDeLiS(gpt-3.5) (Sui et al. 2024)</td><td>76.78</td><td>79.32</td><td>63.12</td><td>61.78</td></tr><tr><td>FiDeLiS(gpt-4)</td><td>78.32</td><td>84.39</td><td>64.32</td><td>71.47</td></tr><tr><td colspan="5">Finetuning- LLMs + KG</td></tr><tr><td>NSM (He et al.2021b)</td><td></td><td>74.31</td><td>1</td><td>53.92</td></tr><tr><td>DeCAF(Yu et al. 2022b)</td><td>-</td><td>82.1</td><td></td><td>70.42</td></tr><tr><td>KD-CoT (Wang et al.2023)</td><td>50.2</td><td>73.7</td><td>1</td><td>50.5</td></tr><tr><td>RoG (Luo et al. 2023d)</td><td>69.81</td><td>83.15</td><td>56.17</td><td>61.39</td></tr><tr><td>FM-KBQA (Ours)</td><td>84.24</td><td>87.34</td><td>68.68</td><td>79.50</td></tr></table></body></html>

proposes a hierarchical query graph generation method, consisting of an outlining stage for structural constraints, followed by a filling stage for instance selection. RoG (Luo et al. 2023d) uses a planning retrieval reasoning framework to combine LLMs with KG, enabling faithful and explainable reasoning. ToG (Sun et al. 2023a) leverages LLMs to iteratively explore reasoning paths on the KG until the question can be answered based on the current path. NSM (He et al. 2021b) uses a sequential model to replicate the multihop reasoning process. KD-CoT (Wang et al. 2023) retrieves relevant knowledge from KG to generate faithful reasoning paths for LLMs. DECAF (Yu et al. 2022a) combines LF and retrieved paths to generate answers using LLMs, achieving strong performance in KBQA tasks. FiDeLiS (Sui et al. 2024) proposes a retrieval exploration method that incorporates both the logical and common sense reasoning of LLMs and the topological connectivity of KG into KBQA.

Evaluation metrics. Following prior research (Yu et al. 2022b; Luo et al. $2 0 2 3 \mathrm { d }$ ; Jiang et al. 2022b), we use the F1 score and Hits $\ @ 1$ metric to represent the overall coverage of the answers and the top-ranked single answer, respectively.

Models. (1) Backboned PLMs. For candidate reasoning path retrieval, we leverage T5 (Raffel et al. 2020b) to preserve relevant relations while filtering out irrelevant relations for each given question. (2) Backbone LLMs. LF generation and path reasoning are performed using open-source LLMs. We adopt Llama-2-7B (Luo et al. 2023b) for fine-tuning and evaluation on the two different datasets.

Hyperparameters and environment. For the PLMs, we train T5 100 epochs on both datasets with a batch size 4 and a learning rate 2e-5. For LLMs, Llama-2 is fine-tuned with a batch size of 4 and a learning rate of 5e-5. The beam size is set to 5 during beam search in the evaluation process. All experiments are conducted on a single NVIDIA A40 GPU.

Overall results. To verify the effectiveness of the proposed method, we compare our method with four types of methods, namely, non-LLMs methods, prompting-LLMs Only, prompting- $. { \mathrm { L L M s } } + { \mathrm { K G } }$ , and Finetuning- $\mathbf { \cdot L L M s } + \mathbf { K G }$ The corresponding F1 and Hits $\ @ 1$ are shown in Table 1.

Table 2: Ablation result of FM-KBQA on WQSP and CWQ dataset. Only LF means only use logical forms, only RP means only use reasoning path.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="2">WQSP task</td><td colspan="2">CWQ Task</td></tr><tr><td>F1</td><td>Hits@1</td><td>F1</td><td>Hits@1</td></tr><tr><td>Only LF(Executable)</td><td>84.12</td><td>90.2</td><td>79.21</td><td>88.8</td></tr><tr><td>Non-executableLF%</td><td>8.7</td><td></td><td>33.08</td><td></td></tr><tr><td>FM-KBQA (Only LF)</td><td>77.29</td><td>82.75</td><td>53.01</td><td>59.43</td></tr><tr><td>FM-KBQA (Only RP)</td><td>79.96</td><td>83.92</td><td>55.16</td><td>68.88</td></tr><tr><td>FM-KBQA (LF+RP)</td><td>84.24</td><td>87.74</td><td>68.68</td><td>79.50</td></tr></table></body></html>

First, FM-KBQA significantly outperforms the baselines across all metrics on the two public datasets. When using the F1 as the evaluation metric, the improvements over the stateof-the-art baseline, FiDeLiS (gpt-4.0), are $5 . 9 2 \%$ and $4 . 3 6 \%$ on the WQSP and CWQ datasets, respectively. When using the Hits $\ @ 1$ as the evaluation metric, the improvements are $2 . 9 5 \%$ and $8 . 0 3 \%$ , respectively.

Among the baselines, non-LLMs methods simulate the reasoning process of KG using specific neural networks (e.g., graph neural networks). However, the reasoning capabilities of non-LLMs models are limited, and their performance needs to be further improved. Prompting-LLMs only methods perform poorly because the LLMs lack knowledge from the KG. For prompting-LLMs $+ \operatorname { K G }$ based methods, ToG and FiDeLiS first retrieve relational paths from the KG and then use these paths to perform logical reasoning with open-source LLMs (e.g., GPT-3.5 and GPT-4), achieving remarkable results. FiDeLiS takes advantage of the LLMs’ ability to perform deductive reasoning, thus achieving stateof-the-art performance in this kind of method. However, too many retrieval paths may introduce noise, resulting in limited improvement in model performance. For finetuningLLMs $+ ~ \mathrm { K G }$ , DeCAF and RoG are two strong baselines. Among them, DeCAF combines semantic parsing and information retrieval to directly generate answers. However, directly generating answers may cause LLMs to lack clear and orderly thinking for the given question and context. RoG employs a planning retrieval reasoning framework to combine LLMs with KG to achieve faithful and explainable reasoning. However, this approach struggles with noise caused by path retrieval. In contrast, our method generates taskspecific LF and selects the correct path to effectively make up for the shortcomings of some generated LF that cannot be executed. As a result, our method achieves the state-ofthe-art performance.

Ablation analysis. To validate the effectiveness of each part in FM-KBQA, we conduct ablation experiments by systematically removing each module, as shown in Table 2. Only LF means only use logical forms, only RP means only use reasoning path. It can be seen that for executable LF generated by the LLMs, the answers obtained by executing the LF have high accuracy. This also verifies the conclusions of some previous research (Ye et al. 2021; Das et al. 2021). However, the LF generated by the LLMs also have the problem of being partially non-executable. For example, in the WQSP task, the proportion of non-executable LF is $8 . 7 \%$ , and in the more complex CWQ task, it reaches $3 3 . 0 8 \%$ . The problem of non-executable generated LF seriously limits the performance of this type of method.

Table 3: Task-related logical forms analysis on the WQSP dataset. We further use the question and the retrieved path as input and fine-tune the LLMs (Llama-2-7B) to generate only LF, forming the comparative method LF-KBQA.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="2">WQSP task</td></tr><tr><td>F1</td><td>Hits@1</td></tr><tr><td>LF-KBQA (Executable)</td><td>68.28</td><td>77.7</td></tr><tr><td>Non-executableLF% FM-KBQA(OnlyLF-executable)</td><td>14.39 84.12</td><td>90.2</td></tr><tr><td>Non-executable LF%</td><td colspan="2">8.7</td></tr></table></body></html>

Moreover, in the reasoning stage, it is evident that the performance of the FM-KBQA method that only uses LF has degraded. This is because some non-executable LF fail to retrieve answers, thereby reducing the overall performance of FM-KBQA (Only LF). However, it can be seen that our method achieves the best performance after combining the LF and RP strategies. This is because (1) we design a multitask learning approach to enable the LLMs to generate taskrelated LF (see Table 3), and (2) we use deductive reasoning to enable LLMs to select effective reasoning paths. The answers obtained from these reasoning paths effectively make up for the problems caused by non-executable LF.

Task-relevant logical forms analysis. To verify that the auxiliary task path selection module can help the LLMs generate task-related LF, we conduct the experiments shown in Table 3. It can be seen that when there is no auxiliary task path selection, the performance of LF-KBQA with executable LF drops by $1 5 . 8 4 \%$ and $1 2 . 5 \%$ in F1 and Hits $@ 1$ respectively, while the proportion of non-executable LF increases by $5 . 6 9 \%$ . This suggests that too many retrieval paths may introduce unnecessary noise paths, which may confuse the LLMs during the generation of LF, resulting in a serious degradation in the quality of the generated LF. Our method addresses this issue by introducing auxiliary tasks to guide the LLMs in generating task-related LF, leading to state-of-the-art performance.

T5 retriever performance analysis. To validate the effectiveness of the proposed enhanced T5 retriever, we compared our method with standard KBQA retrievers such as BM25, DPR, and Sentence-BERT. As shown in Figure 2, the accuracy of our method significantly outperforms the aforementioned methods, indicating that our approach can provide accurate contextual information needed for the reasoning stage of LLMs with fewer path searches. This approach not only reduces the impact of excessive path retrieval on the reasoning performance of LLMs but also decreases computational resource consumption.

Hyperparameter beam size selection. To choose an appropriate beam size, we conduct the experiment shown in Figure 3. The results reveal that as the beam size increases, the performance of our method gradually improves. When the beam size reaches 2, the model’s performance begins to stabilize. This suggests that the model has identified the necessary answers with a beam size of 2, and demonstrates that the proposed method is not only capable of reducing computational consumption but also exhibits strong effectiveness and robustness.

Table 4: Case study of ChatGPT-CoT and FM-KBQA   

<html><body><table><tr><td>Question</td><td>WhatwasDr Seuss education?</td></tr><tr><td>Answer</td><td>Dartmouth College;University of Oxford; Lincoln College, Oxford GPT+CoT Think step by step.Given a question: (What was Dr Seuss education?)and a series of candidate triplets: O:[TheodoreLsieg,ople.psoeductio,Onyd,.Od,ducatiodcatoiutio,Uivsityd</td></tr><tr><td></td><td>1:[TheodoreLesieg,eople.personducatio,m.O4yt85],[m.O4ytk85,ducationducationistitution,Dartmouthle 2:[TheodoreLsieg,opleersoduatio,p],[mpducaioduatiostitution,incolCollgeod First,selecttherelated triplets according tothe given question.Then,retrievetheanswer from thereasoning path. Answer:Dartmouth College in triplet1. proposed Given a question: (What was Dr Seuss education?) and a series of reasoning paths: O:Thepremiseis:TodreLsi>peoppersodcatio>Oydeducatioeducatiositutio-UnisityfOfod</td></tr><tr><td></td><td>clusion is:Dr. Seuss's education was University of Oxford. 1:Thepremiseis:TeodoreLsieg->peoplepersoducatio>O4yt5-educatioeducatiostitutio->DartmouthColgItso clusion is: Dr. Seusss education was Dartmouth College. 2:Thepremies:edorLsi->people.psoducati-p>ducatioucatistuti-colColleOfodIs conclusion is: Dr. Seuss's education was Lincoln College, Oxford. Task1:Generatelogicalforsjustbasedonthequestion.Task2:Verifwhichdeductivereasoingiscorectforthegivenquestionina deductive manner,if it exits,return the correct number of deductive reasoning,if doesn't,return “no".</td></tr></table></body></html>

![](images/edc72955dd77d211c39b171b267ade55c2933743b158021aad365034d5be1b97.jpg)  
Figure 2: The accuracy of different retrieval methods on the WQSP and CWQ datasets. Top-5 Acc means that all gold relations are in the top 5 predicted by the models.

![](images/af17428ecc4f38095f5a5aeaba755bb5a95165ee749677db860d911bc77ad856.jpg)  
Figure 3: The F1 score under different beam size on WQSP (left) and CWQ (right) datasets. LF means the logical forms, RP represents reasoning path, and $\mathrm { \cdot L F { + } R P ^ { \prime } }$ denotes our method FM-KBQA.

Case study. We also present the case study in the Table 4, it can find that $\mathbf { G P T + C O T }$ only correctly selects one reasoning path for the multi-answer question. In contrast, our method not only generates the correct LF but also selects the correct reasoning path through deductive reasoning. This suggests that LLMs may be better at reasoning with logical knowledge through deductive methods, while their performance in graph-based reasoning is limited. Furthermore, in our method, the LF and path selection can complement each other to generate more accurate answers.

# Conclusion

In this paper, we introduce a novel learning framework, FM-KBQA, which leverages multi-task learning to fine-tune LLMs to generate task-relevant LF. Specifically, we design an additional objective function that instructs the LLMs to output the index of answer-relevant reasoning paths through deductive reasoning. This enables the LLMs to focus more on answer-relevant paths and guides them in generating task-related LF. Furthermore, the reasoning paths can serve as an effective supplement to the question’s answer when the LF are non-executable. This approach significantly enhances the performance of LLMs in KBQA.