# KnowPO: Knowledge-Aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models

Ruizhe Zhang1,2\*, Yongxin $\mathbf { X } \mathbf { u } ^ { 1 , 2 * }$ , Yuzhen Xiao1,2\*, Runchuan Zhu1,2, Xinke Jiang1,2, Xu Chu1,2,4,5, Junfeng Zhao1,2,6†, Yasha Wang2,3,5†

1 School of Computer Science, Peking University, Beijing, China   
2 Key Laboratory of High Confidence Software Technologies, Ministry of Education, Beijing, China   
3 National Engineering Research Center For Software Engineering, Peking University, Beijing, China 4 Center on Frontiers of Computing Studies, Peking University, Beijing, China 5 Peking University Information Technology Institute (Tianjin Binhai) 6 Nanhu Laboratory, Jiaxing, China {nostradamus,xuyx,xiaoyuzhen}@stu.pku.edu.cn; zhaojf $@$ pku.edu.cn; wangyasha@pku.edu.cn

# Abstract

By integrating external knowledge, Retrieval-Augmented Generation (RAG) has become an effective strategy for mitigating the hallucination problems that large language models (LLMs) encounter when dealing with knowledgeintensive tasks. However, in the process of integrating external non-parametric supporting evidence with internal parametric knowledge, inevitable knowledge conflicts may arise, leading to confusion in the model’s responses. To enhance the knowledge selection of LLMs in various contexts, some research has focused on refining their behavior patterns through instruction-tuning. Nonetheless, due to the absence of explicit negative signals and comparative objectives, models fine-tuned in this manner may still exhibit undesirable behaviors such as contextual ignorance and contextual overinclusion. To this end, we propose a Knowledge-aware Preference Optimization strategy, dubbed KnowPO, aimed at achieving adaptive knowledge selection based on contextual relevance in real retrieval scenarios. Concretely, we proposed a general paradigm for constructing knowledge conflict datasets, which comprehensively cover various error types and learn how to avoid these negative signals through preference optimization methods. Simultaneously, we proposed a rewriting strategy and data ratio optimization strategy to address preference imbalances. Experimental results show that KnowPO outperforms previous methods for handling knowledge conflicts by over $3 7 \%$ , while also exhibiting robust generalization across various out-of-distribution datasets.

# Introduction

Large Language Models (LLMs) (Taylor et al. 2022; Zhao et al. 2023b) have been widely applied in various fields, such as natural language processing, question-answering systems, and text generation, giving rise to numerous AI applications (Kaplan et al. 2020; Vu et al. 2024; Li et al. 2023, 2024). These models exhibit outstanding performance in many tasks, primarily due to their large-scale parameters and extensive pre-training data (Ziegler et al. 2020; Wang et al. 2023b; Ma et al. 2024; Lin et al. 2024). However, because of the static nature of the training data, LLMs may generate seemingly coherent but actually unreliable information, a phenomenon known as “hallucination” (Ji et al. 2023a,b; Cao et al. 2020), due to outdated knowledge and long-tail knowledge (He, Zhang, and Roth 2022; Kandpal et al. 2023; Jiang et al. 2024). Retrieval-Augmented Generation (RAG) paradigm (Izacard et al. 2022; Asai et al. 2023b,a), within a retrieve-and-read framework, leverages information from reliable knowledge bases to compensate the static nature of the Internal knowledge of LLM. However, the performance of RAG framework is limited by the knowledge conflicts between internal knowledge stored in LLM parameters and external database (Xu et al. 2024; Jin et al. 2024). In this paper, we focus on resolving knowledge conflict by adhere to the retrieved knowledge, and meanwhile, improving the robustness against noise in the retrieved context.

In response to the aforementioned issue, a mainstream approach is to construct specific instruction-tuning datasets to optimize the knowledge prioritization of LLMs in contexts with varying degrees of relevance (Li et al. 2022; Xue et al. 2023). However, as shown in Figure 1, achieving a balance between adherence capability and noise robustness is highly challenging. On one hand, when the LLM heavily relies on external knowledge, it risks over-focusing on irrelevant retrieval contexts, struggling to effectively discern noise. On the other hand, an excessive emphasis on enhancing the LLM’s noise resistance can inadvertently filter out useful contextual information (Wu, Wu, and Zou 2024). Moreover, the manifestation of these capabilities is closely related to the complexity of the context in real-world RAG scenarios (Longpre et al. 2022; Xie et al. 2024). Therefore, it is crucial to address the balance between adherence capability and noise robustness in real RAG scenarios.

Therefore, our insights stem from the error types observed in real-world scenarios involving RAG. We observe that existing research fails to distinguish between supervisory signals, leading to adherence capability and noise robustness being treated as analogous instruction-following pos

Which country has the most Nobel Prize winners? Base In the history of Nobel Prize awarding, UK is the country with the most awards. × The United States Among Asian countries, The United States of 图 has the most Nobel Japan has the most America has 400 Nobel world.... Prize winners in the Nobel Prize winning scientists… of any country…. Prize winners, the most Documents Retrieved RAG Contextual Ignorance Ideal Behavior Contextual Overinclusion According to my knowledge, Based on extra knowledge, Based on extra knowledge, UK is the country with the the United States has the Japan has the most Nobel most Nobel Prize awards. most Nobel Prize winners. Prize winners. Lack of Adherence Capability Balanced Knowledge Select Lack of Noise Robustness

itive examples. This leads to contradictory signals during instruction-following training, causing learning variance and impeding the effective acquisition of both capabilities. To address this, we propose a more nuanced optimization approach that introduces preference data specifically describing adherence capability and noise robustness. Leveraging efficacious and extensively utilized Direct Preference Optimization (DPO) (Rafailov et al. 2024), we optimize the model’s ability to leverage external knowledge, thereby enhancing the overall efficacy of RAG.

Although seemingly straightforward, implementing this intuition faces these challenges: (C1) How to more accurately simulate complicated context in real-world RAG scenarios and introduce more comprehensive, fine-grained negative signals? (C2) How to resolve data discrepancies in preference learning to avoid behavior pattern imbalances?

By jointly considering the above issues, we propose KnowPO, a Knowledge-aware Preference Optimization strategy, which constructs comprehensive and balanced preference relations to optimize LLMs’ knowledge selection in different contexts. i) Specifically, we simulated real-world RAG scenarios at the input level. We perform refined noise classification based on the relevance between the knowledge context and the question topic, and explore combination methods with evidence to form conflicting context and irrelevant context. At the output level, we simulate two common error types in different context relevance scenarios: Contextual Ignorance and Contextual Overinclusion, and develop training strategies to avoid these errors. ii) Secondly, we propose a rewriting strategy to address length imbalance and a data ratio balancing strategy to address behavior pattern imbalance, using DPO to optimize LLMs’ adherence capability and noise robustness. These strategies not only eliminate length biases and imbalances in behavior pattern distribution but also enhance the exhaustiveness of the model’s responses. This prevents degradation of conversational abilities that can occur when training on datasets with shorter answers, such as in reading comprehension tasks. Our main contributions are summarized as follows:

• We observed that LLMs in RAG scenarios fail to effectively balance adherence capability and noise robustness. and thus proposed the KnowPO framework, which refines negative supervisory signals to enhance LLM behavior when encountering knowledge conflicts. • We proposed a general paradigm for constructing knowledge conflict datasets, comprehensively covering various error types and generalizable to different model architectures. We also proposed a rewriting strategy and data ratio optimization strategy to address preference imbalances. • We validated our method’s training effectiveness on multiple models and datasets and tested its generalization ability in out-of-distribution (OOD) scenarios. The results indicate that our method not only improves the performance of models on test sets but also enhances their adaptability and robustness when confronted with unknown data.

# Related Work

Knowledge Conflicts. Numerous studies have explored LLMs’ behavior in knowledge conflict scenarios, providing valuable insights for our work. Longpre et al. (2022) discovered that large Pre-trained Language Models often prefer parametric knowledge over contextual information when facing knowledge conflicts. Wu, Wu, and Zou (2024) highlighted that this tendency to disregard context is influenced by the model’s prior token probability, with high-probability parametric knowledge being harder to override. Kassner and Schu¨tze (2019) demonstrated that LLMs are susceptible to being misled by task-irrelevant context. Furthermore, Tan et al. (2024) indicated that the model’s contextual preferences are linked to the semantic completeness of the context and its relevance to the question.

Several studies aim to improve the adherence of LLMs to context amid knowledge conflicts. For instance, Knowledge Aware Fine-Tuning (KAFT) (Li et al. 2022) enhances models’ ability to use external knowledge by creating challenging counterfactual knowledge from training datasets and incorporating irrelevant knowledge to boost noise resistance. However, as previously mentioned, the applicability of this approach in real-world RAG scenarios is limited. Additionally, decoding-based methods (Jin et al. 2024; Chen, Zhang, and Choi 2022), like Context-Aware Decoding (CAD) (Shi et al. 2023b), adjust LLMs’ output probabilities during token generation, akin to contrastive decoding, conditioned on relevant context. However, this approach may impact the semantic coherence of long responses. Moreover, promptbased methods employ sophisticated designed prompts to ensure that LLMs adhere to the provided context (Si et al. 2023; Zhou et al. 2023). However, research shows that merely modifying prompts doesn’t significantly alter LLMs’ internal prior token probabilities (Wu, Wu, and Zou 2024), potentially limiting the effectiveness of this approach.

# Methodology

# Task Definition

Given an LLM $\Theta$ and an input natural language question $q$ , we ask $\Theta$ to generate a response $\alpha = \Theta ( q )$ , representing the parametric knowledge for $q$ . Assume in a typical retrieveand-read framework, context $\tau$ is a permutation of $D _ { j } ^ { r } , j =$ $1 , 2 , \ldots , K$ , which represents a set of documents retrieved based on $q$ . And ${ \cal { S } } = \{ a _ { i } \} , i = 1 , 2 , \ldots , N$ constitutes the set of contextual answer, each of which is derived from a retrieved document $D _ { \tau _ { i } } ^ { r }$ . We can simplify the RAG task into $y = \Theta ( q \| \tau )$ , where $y$ is output of $\Theta$ based on context $\tau$ . Note that $K$ is not necessarily equals with $N$ , because some retrieved documents may not contain any answer for $q$ and are known as noises.

It’s clearly that $\alpha$ and $a _ { i }$ are independent. Knowledge conflict appears when $\alpha \not \in S$ , and at this time response $y$ of $\Theta ( q \| \tau ) \bar { }$ can be uncertain. To simplify the discussion, we limit $N$ to a maximum of 1, which means context $\tau$ contains at most one document $D _ { \epsilon } ^ { r }$ from which the answer can be derived. Our purpose is to make sure $y = a _ { \epsilon }$ when $| S | = 1$ and $y = \alpha$ when $| S | = 0$ . In other word, LLM $\Theta$ should use appropriate external knowledge when there exists a document which contains the necessary knowledge regardless of conflicting with parameter knowledge, while use its parameter knowledge when retrieved documents are all irrelevant.

# Contradictory Knowledge

Constructing knowledge that conflicts with LLM’s parameter knowledge is crucial to condition $| S | = 1$ . For question $q$ in RAG scenarios, this conflict is reflected in conflicting answers $\boldsymbol { a } _ { c f }$ which are inconsistent with LLM’s parameter answer $\alpha$ . It is important to note that these conflicting answers $\boldsymbol { a } _ { c f }$ do not necessarily have to be correct, nor is the LLM’s parameter answer $\alpha$ always incorrect. In our approach, both answers can be incorrect to the question as long as they conflict with each other. The key to knowledge conflict lies in the conflict itself, regardless of correctness. This addresses a common misconception in previous work, where researchers often ensured that one answer was correct and the other incorrect (Tan et al. 2024; Wu, Wu, and Zou 2024), which not only increased the difficulty of data filtering but also overlooked some knowledge conflict scenarios.

Specifically, we first extract world knowledge acquired during the pretraining phase of the large model, marked as parameter answer $\alpha$ . We encourage LLM to abstain from answering when uncertain. Additionally, we refine the response formats for other parametric knowledge. The revised results are presented in Table 1.

For a given question $q$ and LLM’s parameter answer $\alpha$ , there are two potential sources of conflicting answers $\boldsymbol { a } _ { c f }$ . The first is the realistic answer $a _ { r e a l }$ to the question. The second is a fabricated answer $a _ { c t f }$ generated using GPT-4 that deviates from the realistic answer $a _ { r e a l }$ . The latter is often referred to as a counterfactual answer, which we require to be as plausible as possible. Thus, for a question $q$ and LLM’s parameter answer $\alpha$ , we can obtain at least one conflicting answer, ensuring it is not overly far-fetched.

# Context Formulation

In this section, we illustrated how to formulate context $\tau$ based on different kinds of knowledge conflict.

To align with the RAG scenario, we utilized the SQuAD2.0 dataset (Rajpurkar, Jia, and Liang 2018), a reading comprehension dataset encompassing multiple general domains, with a substantial corpus of documents and associated QA tasks. Notably, besides corpora collected from Wikipedia, SQuAD2.0 is also annotated by humans to determine whether a document can yield an answer for a specific question. Previous research has highlighted that treating a relevant yet non-informative document as a reference external knowledge source can impair LLM’s adherence capabilities (Li et al. 2022). Following the chunk-size commonly used in RAG tasks (Shi et al. 2023a), we set the length of context $\tau$ to $K = 4$ .

For scenarios with $| S | = 1$ , we initially select pertinent documents from SQuAD2.0 based on the conflicting knowledge: For question $q$ and realistic answer $a _ { r e a l }$ , we directly select the corresponding document $D _ { \epsilon } ^ { r }$ from the original dataset; and for question $q$ and counterfactual answer $a _ { c t f }$ , we replace all occurrences of $a _ { r e a l }$ with $a _ { c t f }$ in $D _ { \epsilon } ^ { r }$ . Subsequently, we select one relevant document on the same topic and two documents on different topics based on semantic similarity. We ensure that these three documents are incapable of answering the question $q$ . These four documents are then shuffled to constitute the conflicting context $\tau _ { c f }$ .

For scenarios with $| S | = 0$ , we distinguish between hard and easy irrelevant documents. Hard documents, derived from human annotations, consist of two documents that are on related topics but cannot answer the question. Easy documents are randomly selected, consisting of two documents on unrelated topics. These four documents are then shuffled to constitute the irrelevant context $\tau _ { i r }$ .

# Error Type Analyse

As previously mentioned, we expect LLMs to utilize contextual knowledge when encountering conflicting context, while relying on parameter knowledge when faced with irrelevant context. These two modes of handling context reflect the model’s adherence capability and noise robustness, respectively. In practical RAG scenarios, deficiencies in these capabilities manifest as two distinct error types: one in which the LLM incorrectly uses irrelevant contextual information to construct answers, termed Contextual Overinclusion; and another where the LLM disregards the context entirely and relies exclusively on its parameter knowledge, termed Contextual Ignorance. These errors can occur with both types of contexts as illustrated in Table 1. To address these issues, we have meticulously designed a dataset comprising positive and negative sample pairs to specifically target and mitigate these errors.

Table 1: An example of how the KnowPO dataset is formulated. LLM’s parameter knowledge are highlighted in bold, while conflicting knowledge in context is shown with light gray, and noisy information is presented in dark gray.   

<html><body><table><tr><td colspan="4">Sample Question: Who is the Democratic presidential candidate in the 2O24 US presidential election? Parameter Answer: The Democratic candidate is Joe Biden.</td></tr><tr><td colspan="2">Conflicting Context</td><td>With President Joe Biden dropping out of the race</td><td>Irrelevant Context ...a decision not without precedent in political</td></tr><tr><td colspan="2">Context Example</td><td>on July 21, Vice President Kamala Harris became the presumed Democratic nominee.The search for her vice presidential running mate is closely watched,with top contenders including Secretary of Transportation Pete Buttigieg,Arizona Senator</td><td>history. Back in 1968, President Lyndon B. Johnson of the Democratic Party also opted out amidst intense political challenges.Following Johnson's withdrawal, Hubert Humphrey took over as the Democratic</td></tr><tr><td rowspan="2">Gold Output</td><td>Adsaer</td><td>Mark Kelly, Ilinois Governor J.B. Pritzker.. The Democratic candidate is Kamala Harris.</td><td>nominee for president. The Democratic candidate is Joe Biden.</td></tr><tr><td>Revised Result</td><td>Bdedonsipeetaloadshat the Democratic candidate is Kamala Harris.</td><td>qustlemebutknod onmy knswledges,the ahiswer to this question is that the Democratic candidate is Joe Biden.</td></tr><tr><td rowspan="2">Coerincdusion</td><td>AErrorr</td><td>The Democratic candidate is Mark Kelly.</td><td> The Democratic candidate is Hubert Humphrey.</td></tr><tr><td>Revised</td><td></td><td></td></tr><tr><td rowspan="2">Contextual Ignorance</td><td>Error Answer</td><td>The Democratic candidate is Joe Biden.</td><td>/</td></tr><tr><td>Revised Result</td><td>quplementalknownotstethiser to this questionis that the Democratic candidate is Joe Biden.</td><td></td></tr></table></body></html>

Contextual Overinclusion Error. In situations with conflicting contexts, the ideal behavior of the LLM demonstrating adherence capability, as shown by positive samples in Table 1, is to answer using the conflicting knowledge present in the context. However, when contextual overinclusion occurs, LLM often utilizes inappropriate information from the context due to insufficient noise robustness and contextual understanding capability. For instance, in the example presented in Table 1, LLM chooses noisy information marked in red. To address this error, we constructed negative samples by using a prompt mechanism to guide GPT-4 to generate incorrect answers from conflicting contexts. To ensure the quality of the generated data, we adhered to stringent validation criteria: (1) The generated answers must be derived from the context, ensuring that the error is unequivocally attributable to contextual overinclusion; (2) The generated answers should be as plausible as possible and distinctly different from the conflicting answers, thereby ensuring the high

quality of the data.

In situations with irrelevant contexts, it is evident that positive sample for noise robustness is to use LLM’s parametric knowledge to respond. When this error occurs, LLM may fail to recognize the context as irrelevant, leading it to use contextual information instead of disregarding it. Similar to contextual overinclusion in conflicting contexts, we constructed corresponding negative samples by using GPT-4 to extract incorrect answers from irrelevant contexts.

# Prompt: Generate Contextual Overinclusion

Please select a word from the provided context as an alternative answer to this question.   
Question: $\{ Q u e s t i o n q \}$   
Potential answer: {Conflicting Answer $\left. a _ { c f } \right\}$   
Context: $\{ C o n t e x t \tau \}$   
Please follow these requirements:   
1. The answer must not be the same as the potential answer.   
2. The alternative answer does not need to be correct, but it must appear in the context.   
3. The alternative answer must be in a form that can answer the question and should be as reasonable as possible.

Contextual Ignorance Error. Contextual Ignorance occurs when the LLM disregards the context in its response, a behavior deemed erroneous solely in conflicting contexts. During such episodes, LLM may either fail to recognize the utility of the context or, even upon recognizing it, may opt to disregard the conflicting answer in favor of relying on its parameter knowledge. For instance, in the example shown in Table 1, LLM answers the question without utilizing supplemental knowledge. To simulate this error, we constructed negative samples by extracting LLM’s response to the query in the absence of any contextual support, ensuring that the answer aligns with an inappropriate erroneous response.

# Training Method

Our training consists of two phases. First, we perform instruction tuning using the conflicting knowledge and contexts to enhance the LLM’s adherence capability and noise robustness in RAG task scenarios. Next, we utilize the preference dataset for DPO training to further improve the LLM’s ability to avoid the two types of errors, while ensuring that its final responses align with user preferences.

Instruction Tuning. Instruction tuning is a multi-task learning framework that enables the use of human-readable instructions to guide the output of LLMs. Given a source text and task-specific instructions, the model is trained to generate a sequence of tokens representing the desired output structure and its corresponding labels. Reviewing our definition of adherence capability and noise robustness, we would like to get a finetuned model $\Theta _ { f t }$ from original LLM $\Theta$ that satisfies the following criteria:

$$
\begin{array} { r l } & { | S | = 1 : \Theta _ { f t } ( q \| \tau _ { c f } ) = a _ { c f } , \mathrm { ~ w h e r e ~ } \exists D _ { \epsilon } ^ { r } \in \tau _ { c f } , D _ { \epsilon } ^ { r } \to a _ { c f } } \\ & { | S | = 0 : \Theta _ { f t } ( q \| \tau _ { i r } ) = \alpha , \mathrm { ~ w h e r e ~ } \Theta ( q ) = \alpha } \end{array}
$$

Note that although the presence of the answer in $\tau$ was distinguished during dataset construction, the LLM does not possess this prior knowledge. The model must independently determine the context type and formulate a response during the RAG task.

Direct Preference Optimization. As previously discussed, LLMs may exhibit errors contextual overinclusion and contextual ignorance in real-world RAG scenarios. To further enhance adherence capability and noise robustness, we propose a Knowledge-aware Preference Optimization(KnowPO) training strategy. This strategy employs three types of preferences between positive and negative samples in two different contextual settings to conduct DPO training on the LLM. Using this approach, we train the LLM to avoid these errors and improve its ability to utilize different contexts.

During preparing data for DPO, we also identified two preference imbalances that impact training effectiveness.

• Length Imbalance. Some studies suggest that reward hacking observed in RLHF can also negatively impact DPO training (Gao, Schulman, and Hilton 2022; Park et al. 2024). We observed that in our previously constructed dataset, for the same preference pair, the positive sample was often the better-formatted and longer response, while the negative sample was a shorter conflicting answer. Due to the tendency of LLMs to be influenced by length bias during DPO (Singhal et al. 2024), they might prefer generating longer responses, which overall manifests as a greater tendency to refuse answering rather than providing a conflicting answer. To mitigate this issue, we standardized the format for all positive and negative samples in Table 1, aligning their lengths to ensure that the average length $l e n _ { w i n }$ approximately equals $\boldsymbol { l e n } _ { l o s s }$ .

• Error Type Imbalance. Given that the preference pairs related to error contextual ignorance in conflicting context guide the LLM to “utilize contextual knowledge without rejecting it”, while the preference pairs associated with error contextual overinclusion in irrelevant context exhibit a tendency towards “rejecting the use of contextual knowledge”, we realized that the ratio of these two contrasting preference pairs could significantly influence training efficacy. During KnowPO training, we ensured that the proportion $\mathcal { R } _ { e r r o r }$ of these two types of data was maintained at approximately 1:1. Furthermore, we validated the importance of this ratio $\mathcal { R } _ { e r r o r } $ in subsequent experiments.

# Experiments

In this section, we conduct a series of experiments on two base models to answer the following research questions:

• RQ1: Does KnowPO outperform other approaches for resolving knowledge conflict across various base models and datasets?   
• RQ2: What impact does each component has on the overall performance?   
• RQ3: How does KnowPO alter the way LLMs utilize parametric knowledge?   
• RQ4: How sensitive is KnowPO to hyper-parameters data ratio $\mathcal { R } _ { e r r o r } $ ?   
• RQ5: Does KnowPO training conducted in general domains remain effective in out-of-distribution (OOD) scenarios?

# Code — https://github.com/Nostradamus4869/KnowPO

# Experimental Setup

Datasets We constructed the KnowPO training dataset based on SQuAD 2.0 (Rajpurkar, Jia, and Liang 2018). The test datasets comprise the following three types: (1) SQuAD 2.0-Eval, a validation set partitioned using the same construction method. (2) Open-source counterfactual datasets: RGB (Chen et al. 2023) and KNOT (Liu et al. 2024) are two general-domain QA datasets containing counterfactual knowledge and contexts. We augmented these datasets with irrelevant contexts for testing purposes. Notably, RGB is a Chinese dataset. (3) Domain-specific dataset: CMB (Wang et al. 2023a) is a multi-task QA dataset in the medical domain, encompassing 269,359 questions across four clinical medicine specialties of physicians, nurses, medical technicians, and pharmacists. Due to quantity constraints, we randomly sample 4,000 questions for testing.

Compared Methods In order to explore the advantages of the KnowPO, we compare the KnowPO results against five other models: (1) Base Model (Base) answers user questions based on supplementary external knowledge, which can be considered as fundamental retrieve-and-read framework in RAG (Lewis et al. 2021). We selected Baichuan2- 7B-chat (Yang et al. 2023) and Llama2-13B-chat (Touvron et al. 2023) as the base model and explored the gains brought by KnowPO: (2) Naive Prompt-based Method (Prompt) employs meticulously designed prompts to enhance the model’s capability to adhere to external knowledge (Zhou et al. 2023). (3) Advanced Prompt-based Method: Chain of Thought (COT) (Wei et al. 2023) is a common method to enhance the performance of LLMs in downstream tasks. COT-VE (Zhao et al. 2023a) extends COT by guiding LLM to identify conflicting knowledge and modify its responses accordingly. (4) Finetuning: KAFT (Li et al. 2022) employs instruction fine-tuning to improve the LLM’s adherence to contexts of varying relevance. (5) Decode-Based Method: CAD (Shi et al. 2023b) uses a contrastive decoding-like method to adjust the probabilities of output tokens.

<html><body><table><tr><td>LLMTurbo</td><td>LLM</td><td colspan="6"></td><td colspan="6">Llama2-13B-Chat</td></tr><tr><td>Method</td><td></td><td colspan="2">Squad2.0-Rral</td><td colspan="2">Baichuan2-7B-Chat RARGBRRo</td><td colspan="2">RKNOT</td><td colspan="2">SqAad2.0Rval</td><td colspan="2">RARGBRRo</td><td colspan="2">RKNORRo</td></tr><tr><td></td><td>Detret</td><td></td><td></td><td></td><td></td><td></td><td>RRo</td><td></td><td>RRo</td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="6">Baselines</td><td>Base</td><td>43.51</td><td>9.80</td><td>65.00</td><td>24.00</td><td>26.42</td><td>7.65</td><td>52.71</td><td>11.95</td><td>69.00</td><td>25.00</td><td>49.66</td><td>21.67</td></tr><tr><td>Prompt</td><td>53.74</td><td>8.60</td><td>79.50</td><td>19.50</td><td>44.65</td><td>14.51</td><td>60.76</td><td>10.59</td><td>73.50</td><td>19.50</td><td>41.14</td><td>22.62</td></tr><tr><td>COT</td><td>54.65</td><td>10.20</td><td>77.50</td><td>21.00</td><td>44.13</td><td>15.29</td><td>57.13</td><td>12.85</td><td>70.50</td><td>25.00</td><td>41.06</td><td>23.53</td></tr><tr><td>COT-VE</td><td>44.83</td><td>8.41</td><td>66.00</td><td>19.50</td><td>27.71</td><td>13.88</td><td>54.52</td><td>10.17</td><td>70.00</td><td>14.50</td><td>52.33</td><td>18.35</td></tr><tr><td>KAFT</td><td>58.83</td><td>21.43</td><td>75.00</td><td>27.00</td><td>54.45</td><td>17.93</td><td>65.73</td><td>34.34</td><td>73.50</td><td>29.50</td><td>62.21</td><td>24.47</td></tr><tr><td>CAD</td><td>35.83</td><td>7.50</td><td>55.50</td><td>22.50</td><td>21.72</td><td>6.99</td><td>41.73</td><td>10.94</td><td>64.50</td><td>23.50</td><td>35.71</td><td>19.96</td></tr><tr><td colspan="2">KnowPO Ours</td><td>80.64</td><td>38.77</td><td>93.50</td><td>37.00</td><td>69.95</td><td>39.73</td><td>76.11</td><td>44.64</td><td>83.50</td><td>37.50</td><td>77.03</td><td>38.28</td></tr><tr><td colspan="2">Performance Gain 个</td><td>37.07~</td><td>80.91~</td><td>17.61~</td><td>37.04~</td><td>28.47~</td><td>121.58~</td><td>15.79~</td><td>29.58~</td><td>13.61~</td><td>27.12~</td><td>23.82~</td><td>49.94~</td></tr><tr><td rowspan="3">Ablation</td><td></td><td>125.06</td><td>416.93</td><td>68.47</td><td>89.74</td><td>222.05</td><td>468.38</td><td>82.39</td><td>338.94</td><td>29.46</td><td>158.62</td><td>115.71</td><td>108.61</td></tr><tr><td>KnowPO (w/o DPO)</td><td>71.09</td><td>36.50</td><td>89.50</td><td>31.00</td><td>64.45</td><td>36.50</td><td>75.96</td><td>42.87</td><td>80.00</td><td>35.00</td><td>70.28</td><td>36.21</td></tr><tr><td>KnowPO (w/o SFT) KnowPO (w/o Aligned)</td><td>69.39</td><td>37.50 43.45</td><td>92.50 71.50</td><td>35.00 43.00</td><td>66.92 48.29</td><td>38.76 45.17</td><td>74.73 61.36</td><td>42.86 50.30</td><td>81.00 70.00</td><td>34.00 42.50</td><td>69.39 50.71</td><td>36.74 46.27</td></tr></table></body></html>

Table 2: Performance comparison (in percent) on Squad2.0-Eval, RGB and KNOT. The best-performing model is underlined.

Metrics We designed statistical metrics to evaluate the two capabilities of LLMs. For adherence capability, we utilized the conflicting contexts from the test set as supplementary knowledge, measuring the proportion $R _ { A d }$ of LLM responses that align with the conflicting knowledge within these contexts. For the RGB and KNOT datasets, the conflicting knowledge exclusively consists of counterfactual knowledge. For noise robustness, we employed the irrelevant contexts from the test set as supplementary knowledge, examining the proportion $R _ { R o }$ of LLM responses that correspond with the model’s parameter knowledge.

# Performance Comparison(RQ 1)

To answer RQ1, we conduct experiments and report results of the two metrics on Squad2.0-Eval, RGB and KNOT with two LLM turbos, as illustrated in Table 2. From the reported results, we can find the following observations:

Comparison of Baseline Methods and Base LLMs. Through comparison, we observe that the KAFT method, fine-tuned with instructions, consistently outperforms across all experimental groups. This superior performance is primarily attributed to the use of contexts with varying degrees of relevance during fine-tuning, which significantly enhances the LLM’s ability to focus on pertinent data while filtering out noise. In contrast, methods relying on the LLM’s inherent capabilities for single or multiple interactions, such as Prompt or COT, tend to indiscriminately depend on external knowledge due to the LLM’s limited noise recognition ability, leading to an increase in $R _ { A d }$ , but a sharp decline in $R _ { R o }$ . Particularly, COT-VE introduces additional noise by incorporating external knowledge for verification and editing , further complicating the model’s ability to discern relevant information. As for CAD, as noted in related research, the contrastive decoding strategy compromises response coherence and utility, performing well on simple datasets like RGB but failing on more complex ones like Squad2.0-Eval and KNOT, thereby losing its practical value.

Comparison of KnowPO and other methods. Firstly, it is evident that our mothed, KnowPO, outperforms the baseline methods across all metrics. For instance, the $R _ { A d }$ and $R _ { R o }$ scores see an improvement of approximately $3 7 . 0 7 \% - 1 2 5 . 0 6 \%$ and $\mathbf { 8 0 . 9 1 \% - 4 1 6 . 9 3 \% }$ for the Squad2.0- Eval dataset with Baichuan2-7B-Chat. Moreover, compared to KAFT, best model in baselines, KnowPO uses more complicated contexts and comprehensive negative signals to enhance LLM’s adherence capability and noise robustness.

# Ablation Study(RQ 2)

To answer RQ2, we perform ablation studies to verify the effectiveness of KnowPO, as illustrated in Table 2. Our observation can be summarized as follows:

Effect of training phase. Both the SFT and DPO phases positively contribute to enhancing the adherence capability and noise robustness of LLMs. Additionally, the preference learning method incorporating negative signals slightly outperforms SFT in improving the LLM’s ability to utilize external knowledge, demonstrating the effectiveness of both training approaches.

Effect of length imbalance. When data length are not aligned, we observe a significant impact of length bias, which slightly enhances $R _ { R o }$ but substantially reduces $R _ { A d }$ . This is due to the model’s inherent tendency to generate more verbose parametric answers, while the conflict answers derived through dataset construction are relatively short. Consequently, the model develops a preference for generating longer responses. Without length alignment between

100 RAd(Oursw/oDPO)   
90   
80 RRo(Ours.w/o.DPO)   
70 Ours RAd   
60 Base Model RRo 50 0.30.40.50.60.70.80.91.0 0.2 0.3 0.5 2 3 5 Prior Probability Data Ratio Rerror

conflict and parametric answers, the model tends to consistently rely on parametric answers, thereby neglecting external knowledge and disrupting the balance between adherence capability and noise robustness.

# Model Prior Analyse(RQ 3)

The LLM’s confidence in its responses is one of the factors influencing whether it prefers internal or external knowledge (Wu, Wu, and Zou 2024). We recorded the LLM’s prior probability for parameter knowledge on the RGB dataset and measured the proportion of instances in each prior probability interval where the LLM followed external knowledge in conflicting contexts. The model’s prior response probability is computed from the average log probability of the response tokens without external knowledge. The results in Figure 2 show that, for base LLM, there is a general negative correlation between the prior probability of an answer and the proportion of following external knowledge; that is, the higher the prior probability, the less likely the answer is to be altered. However, after fine-tuning with KnowPO, although the overall trend remains negatively correlated, the trend is significantly mitigated, indicating that our method effectively enhances the LLM’s adherence to external knowledge.

# Hyper-parameter Study(RQ 4)

As analyzed in Methodology, two types of preference pairs exhibit distinctly opposite behavioral tendencies: those simulating error contextual ignorance in conflicting contexts and those simulating error contextual overinclusion in irrelevant contexts. We conducted a series of analyses by adjusting the ratio $R _ { e r r o r } \$ between these two types of preference pairs from the list $[ 0 . 2 , 0 . 3 , 0 . 5 , 1 , 2 , 3 , 5 ]$ . The results in Figure 2 indicate that as the proportion of the first type of preference pairs increases, the LLM becomes more inclined to utilize contextual knowledge, enhancing its adherence capability but also becoming more susceptible to noise, which in turn reduces its noise robustness. Conversely, as the proportion of the second type increases, the LLM tends to disregard contextual information and respond directly, resulting in reduced $R _ { A d }$ but improved $R _ { R o }$ . Notably, as the ratio

Table 3: Performance comparison (in percent) on CMB   

<html><body><table><tr><td rowspan="2"></td><td colspan="2">Baichuan2-7B-Chat</td><td colspan="2">Llama2-13B-Chat</td></tr><tr><td>RAd</td><td>RRo</td><td>RAd</td><td>RRo</td></tr><tr><td>Base</td><td>58.69</td><td>10.66</td><td>60.95</td><td>8.21</td></tr><tr><td>KnowPO(w/o DPO)</td><td>95.66</td><td>23.70</td><td>83.88</td><td>16.53</td></tr><tr><td>KnowPO</td><td>96.23</td><td>24.12</td><td>87.46</td><td>21.24</td></tr></table></body></html>

Table 4: The match rate between LLM’s parameter answers and conflicting answers after training.   

<html><body><table><tr><td>Model</td><td>KnowPO(w/o DPO)</td><td>KnowPO</td></tr><tr><td>Baichuan2-7B-Chat</td><td>3.65%</td><td>4.70%</td></tr><tr><td>Llama2-13B-Chat</td><td>3.51%</td><td>4.31%</td></tr></table></body></html>

$R _ { e r r o r } \$ increases from 1, the rate of improvement in adherence slows, while the decline in robustness becomes more pronounced. When the ratio $R _ { e r r o r } \$ decreases from 1, the curvature of $R _ { A d }$ and $R _ { R o }$ also shows the opposite trend. Based on these findings, we ultimately selected $R _ { e r r o r } = 1$ as the optimal ratio, ensuring balanced improvements in both capabilities compared to SFT training.

# Generalization Analysis(RQ 5)

To demonstrate the robust generalization capability of our method beyond general domain, we conducted experiments on the CMB medical test set. Using medical triplets and documents, we created supplementary contexts and a conflict dataset for 4,000 CMB questions in medical domain. Results in Table 3 show that KnowPO-trained models effectively enhance adherence capability and noise robustness when transferred to domain-specific contexts. The higher scores on CMB compared to those in Table 2 can be attributed to the fact that the contexts we constructed were less challenging than the real-world RAG knowledge.

A potential risk of incorporating QA pairs and conflicting knowledge into the training data is the inadvertent introduction of harmful information to the model. To evaluate whether the KnowPO-trained model retained conflicting knowledge, we utilized prompts designed to extract LLM’s parameter knowledge. The results, presented in Table 4, indicate that the model retained virtually no conflicting knowledge after the SFT and DPO phases. This finding corroborates that our method enhances the LLM’s ability to leverage external knowledge rather than injecting specific knowledge.

# Conclusions and Future Works

In this paper, we propose KnowPO, a Knowledge-aware Preference Optimization strategy to enhance LLM’s adherence capability and noise robustness to external knowledge. We simulate two error types—Contextual Ignorance and Contextual Overinclusion—and use negative gradient terms in DPO objectives to minimize undesired responses. By aligning data lengths and balancing ratios, we mitigate preference imbalances in DPO. Experiments across diverse datasets confirm KnowPO’s efficacy and generalization. In the future, we will explore how different context compositions affect LLMs’ ability to utilize external knowledge.