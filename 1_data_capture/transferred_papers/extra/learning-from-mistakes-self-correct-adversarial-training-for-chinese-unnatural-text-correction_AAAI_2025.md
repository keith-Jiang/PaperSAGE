# Learning from Mistakes: Self-correct Adversarial Training for Chinese Unnatural Text Correction

Xuan Feng12, Tianlong $\mathbf { G u } ^ { 1 2 * }$ , Xiaoli $\mathbf { L i u } ^ { 1 2 3 }$ , Liang Chang4

1College of Cyber Security, Jinan University   
2Engineering Research Center of Trustworthy AI (Ministry of Education) 3Graduate School of Engineering, Chiba University 4Guangxi Key Laboratory of Trusted Software   
fenffef $@$ 163.com, {gutianlong, txlliu}@jnu.edu.cn, changl $@$ guet.edu.cn

# Abstract

Unnatural text correction aims to automatically detect and correct spelling errors or adversarial perturbation errors in sentences. Existing methods typically rely on fine-tuning or adversarial training to correct errors, which have achieved significant success. However, these methods exhibit poor generalization performance due to the difference in data distribution between training data and real-world scenarios, known as the exposure bias problem. In this paper, we propose a selfcorrect adversarial training framework for LearnIng from MIsTakes (LIMIT), which is a task- and model-independent framework to correct unnatural errors or mistakes. Specifically, we fully utilize errors generated by the model that are actively exposed during the inference phase, i.e., predictions that are inconsistent with the target. This training method not only simulates potential errors in real application scenarios, but also mitigates the exposure bias of the traditional training process. Meanwhile, we design a novel decoding intervention strategy to maintain semantic consistency. Extensive experimental results on Chinese unnatural text error correction datasets show that our proposed method can correct multiple forms of errors and outperforms the state-of-the-art text correction methods. In addition, extensive results on Chinese and English datasets validate that LIMIT can serve as a plugand-play defense module and can extend to new models and datasets without further training.

Adversarial Training Stage TrainPair   
走进荀子的(s→世)界,触摸(liang $\cdot$ 两)千年前(白勺 $\cdot$ 的心灵温度   
Enter the $\cdot$ World) of Xunzi,and touch the temperature ( $\mathrm { \bf ~ o f { \sigma } }$   
the heart from $\mathrm { ( t u ) . } $ two) thousand years ago   
赠品不能(射→设)置(拥 $$ 用)花呗付款?   
Can't (shot → set) up a giveaway to pay (embrace $$ with) Huabei?   
Testing Stage Predict Error   
事情就是这样，不需呀哦道歉   
That's the way it is, no need ya o apologize X Under-correction   
女生适合学什么技巧?   
Which skill is suitable for girls? × Over-correction

and trustworthiness of the system (Liu et al. 2023). Therefore, correcting unnatural text errors or mistakes is crucial for content moderation and robust defense.

# Introduction

Unnatural text correction (UTC) is a task that automatically corrects a variety of textual errors or mistakes in a given sentence, including spelling errors (such as visual and phonetic errors), and adversarial perturbation errors. It has attracted much attention in academia and industry due to the important role of UTC in improving text accuracy and readability (Liu, Wu, and Zhao 2024). With the widespread of unnatural texts and euphemisms on the Internet, it has become increasingly significant in various domains (Feng et al. 2024). For example, UTC can automatically fix errors in user-generated content and improve the quality of content moderation (Dai et al. 2023). Moreover, it is possible for UTC to detect and correct adversarial perturbations, enhancing the robustness

Existing text correction methods typically rely on finetuning or adversarial training paradigms Liu et al. (2021); Li et al. (2022b,c,a); Wu et al. (2023b); Liu, Wu, and Zhao (2024). Although these methods have achieved significant success in common text correction tasks, they often exhibit poor generalization performance in real-world applications (Gupta et al. 2023). As shown in Figure 1, when encountering unnatural text errors, the model may be under-corrected or over-corrected. On the one hand, the under-correction is mainly due to the difference in data distribution between the training data and the real-world scenario, which is known as the exposure bias problem (Bengio et al. 2015). Specifically, training data are usually constructed or preprocessed manually with a relatively fixed and uniform distribution, while real-world data distributions are more complex and variable. In this case, the patterns and representations learned during training may not be sufficient to serve in the reasoning process, leading to unsatisfactory corrections. Thus, effectively solving the exposure bias problem and enhancing the model’s generalization ability in real-world applications has become an important challenge. On the other hand, the model will also over-correct characters without errors in the text (Liang, Quan, and Wang 2023). In general, overcorrection will distort the original meaning of the text and affect the reader’s understanding. It can also affect the user’s trust and experience with the error correction system. Therefore, maintaining semantic consistency while correcting text is also an important concern.

In these regards, we propose a self-correct adversarial training framework for LearnIng from MIsTakes (LIMIT), which effectively copes with multi-type spelling errors and adversarial perturbations without external knowledge effectively. Specifically, we first implement a generative correction mechanism that enables models to correct multi-type errors or mistakes. As a unified mechanism, it corrects adversarial perturbations that are specific to different models and tasks. Second, we introduce self-correct adversarial training to fine-grain the contrasting examples according to the ranking loss, thereby obtaining robust representations. During the training process, incorrect examples generated based on the model’s own predictions (e.g., samples inconsistent with the target generated by a beam search algorithm) are also incorporated into the learning process. This training process motivates the model to identify and correct its own biases by actively exposing its prediction errors in the inference phase. It not only mitigates the exposure bias in traditional training but also improves the robustness and reliability of the model against unnatural errors. In addition to the training phase, we also utilize semantic information in the inference process. Traditional decoding methods assign equal or probabilitybased weights to all candidate outputs, which leads to upvoting more erroneous answers with higher co-occurrence. To address this problem, we design a novel decoding intervention strategy to maintain semantic consistency. This helps the language model to maintain semantic consistency in decoding and thus reduces the over-correction problem.

Our main contributions are summarized as follows: (1) We implement a generative correction mechanism that enables models to correct multi-type errors. (2) We introduce self-correcting adversarial training that derives adversarial examples from the model’s predictions, allowing the model to learn from its mistakes and effectively mitigate the exposure bias. (3) To address the over-correction problem of language models, we design a novel decoding intervention strategy to maintain semantic consistency. (4) Extensive experimental results on Chinese and English datasets show that our proposed method can correct multi-type errors or mistakes and can serve as a defense module in various natural language understanding and generation tasks.

# Related Work

# Unnatural Text Correction

Traditional Chinese text correction aims to address visual and phonetic errors caused by spelling errors. Early text correction methods adopted the process of recognizing and then correcting errors (Zhang et al. 2000). However, the effectiveness of these methods is limited by the varying accuracy of the identification and correction phases. To overcome these limitations, researchers have begun to explore end-to-end error correction methods. Wang, Tay, and Zhong (2019) utilized confusion sets and gating mechanisms, while Zhang et al. (2020a) optimized detection and correction using BERT (Jin et al. 2020). Liu et al. (2021) introduced PLOME, which leverages a pre-trained masked language model that incorporates misspelling knowledge. Li et al. (2022b,c,a) advanced text Correction techniques by learning heterogeneous knowledge from dictionaries, refining knowledge representations, and employing iterative correction strategies. Wu et al. (2023b) improved language model performance through random masking, and Liu, Wu, and Zhao (2024) rephrased sentences by filling slots.

Recently, Feng et al. (2024) extended this task to nonnatural text correction to address additional challenges facing Chinese text correction, such as errors arising from perfect pinyin, abbreviation pinyin, and character split. However, perturbations in the form of insertions, deletions, inversions, and Unicode are still unexplored.

# Adversarial Training

Adversarial Training (AT) (Goodfellow, Shlens, and Szegedy 2014) is a method used to improve a model’s defense against adversarial perturbations by training the model with adversarial examples, thereby enhancing its robustness against deceptive inputs. Most AT methods tend to defend against suboptimal adversarial examples that deceive the decoder (Zhu et al. 2020; Jiang et al. 2020; Aghajanyan et al. 2020). More recently, Wu et al. (2023a) proposed contextualized representation adversarial training to deviate the contextual representation of the encoder from potential adversarial influences. Additionally, Gupta et al. (2023) designed a text rewriting module to eliminate perturbations in the input. Although these methods have made some progress in enhancing the model’s generalization ability, they can lead to significant performance degradation on the original task. Therefore, it is crucial to mitigate the exposure bias and improve the model’s adaptability and robustness to unseen scenarios.

# Method

AT is usually applied to defend specific seen errors and perturbations. In contrast, real-world spelling errors and adversarial perturbations are subject to evolving model architectures and changing task contexts. To address the exposure bias problem associated with AT, as well as the inherent over-correction problem of language models, we propose a self-correct adversarial training framework for learning from mistakes (LIMIT), which consists of a generative correction mechanism, self-correct adversarial training, and a decoding intervention strategy. It corrects the text from unnatural errors through conditional generation. In this section, we illustrate the general design of the framework as well as the individual components. The overall process of our framework is shown in Figure 2.

Generative Correction Mechanism Self-correct Adversarial Training Decoding Intervention Strategy Predict:与其感慨路难行，不如马上出发 Translate:Better to run than curse the road Diverse :路 Candidate 通 难 不 Decoder y:…路nan行… Max Translate pass curse no Search k3:rs the road) γ2:…路南行· ↑ ↑ (… south the road) Encoder N 2：路不行 Predict 0.53 0.47 0.43 (… no the road) Similarity 0.13 0.49 0.12 ↑ Iraut:与其感ter nan行，不如马上出d .路.d (x ：路) Target × ×

Figure 2: The overall correction process of LIMIT. For easier understanding, pinyin errors in Chinese are represented by phonetic symbols in English.

# Generative Correction Mechanism

Mask-then-recovery is a commonly used correction mechanism for textual error correction. However, it fails to consider unseen multi-type errors and mistakes (such as pinyin, insertion, deletions, inversions, Unicode, character splitting, etc.) and unequal length errors (i.e., input and output lengths do not match) (Feng et al. 2024). In contrast to the maskthen-recovery process, we propose a generative correction mechanism explicitly trained for eliminating spelling errors and adversarial perturbations.

Formally, given a clean text $ { \mathcal { X } } ~ = ~ \{ x _ { 1 } , x _ { 2 } , . . . , x _ { n } \}$ , a bounded, imperceptible perturbation $\delta$ is added to produce an adversarial example $\hat { \mathcal { X } } ^ { \prime } = \{ x _ { 1 } ^ { \prime } , x _ { 2 } ^ { \prime } , . . . , x _ { m } ^ { \prime } \} .$ Notably, the length $m$ of $\mathcal { X } ^ { \prime }$ possibly differs from the original input sentence $\chi$ , i.e., the length of the input sentence $\mathcal { X } ^ { \prime }$ is independent of the length of target sentence $y$ . More specific perturbation processes can be found in the Technical Appendix. Traditional correction mechanisms can learn the optimal corrector by optimizing the following objectives:

$$
\operatorname* { m i n } _ { f \in \mathcal { H } } \mathbb { E } _ { ( \mathcal { X } , \mathcal { Y } ) \in \mathcal { D } } \operatorname* { m a x } _ { | \delta | \leq \epsilon } \ell [ f ( \mathcal { X } + \delta ) , \mathcal { Y } ]
$$

Instead, our goal is to correct unnatural text errors and mistakes from the input and preserve the semantics of the original sentence. To this end, we implement a text-to-text generative mechanism, denoted by $\mathcal { P } _ { r }$ .

To effectively eliminate adversarial examples, the correct function $\mathcal { P } _ { r }$ must recover the input $\mathcal { X } ^ { \prime }$ to the target $y =$ $\{ y _ { 1 } , y _ { 2 } , . . . , y _ { n } \}$ :

$$
\mathcal { V } _ { i } = \mathcal { P } _ { r } \left( \mathcal { X } _ { i } ^ { \prime } \right) , \forall i \in \{ 1 , n \}
$$

where the perturbed characters in $\mathcal { X } ^ { \prime }$ are replaced with the original ones to obtain $y$ .

Compared to the mask-then-recovery process, the generative correction mechanism allows easier transfer to new error forms and tasks. This is a promising mechanism for correcting multi-type unnatural textual errors, in turn defending against potentially adversarial perturbations with word substitutions.

# Self-correct Adversarial Training

The exposure bias problem in text correction occurs when a model is trained on a data distribution that does not accurately reflect the real-world scene. Unfortunately, real-world data distributions are more complex and variable, which may lead to poor generalization performance of the trained model. To address this problem, we introduce self-correct adversarial training. It constructs adversarial examples from its own predictions via a beam search algorithm and implements the ranking loss to help calibrate robust representations.

Following the contrastive learning framework (Chen et al. 2020), we train the model by comparing positive and negative sentence pairs to learn representations of ground truth sentences. By maximizing the similarity between source and target sequences while minimizing the similarity between negative sequences:

$$
L ^ { \mathrm { N L L } } = - \log \frac { \exp \left( \sin \left( z _ { x ^ { \prime } } , z _ { y } \right) / \tau \right) } { \sum \exp \left( \sin \left( z _ { x ^ { \prime } } , z _ { y ^ { \prime } } \right) / \tau \right) }
$$

where $z _ { x ^ { \prime } }$ , $z _ { y }$ , $z _ { y \prime }$ denote vector representations of input $\mathcal { X } ^ { \prime }$ , target $y$ , and negative sample $y \prime$ , respectively. $\tau$ is the temperature, and $\sin ( \cdot , \cdot )$ defines the cosine similarity.

However, training models using naive contrastive learning frameworks typically yield error corrections. In light of this, we propose a principled method for automatically constructing adversarial negative and positive examples that allow the model to fully utilize mistakes. Specifically, we employ diverse beam search algorithms to dynamically create negative examples $\tilde { \mathcal { Y } } = \left\{ \tilde { \mathcal { P } } _ { 1 } , \tilde { \mathcal { V } } _ { 2 } , \cdot \cdot \cdot , \tilde { \tilde { \mathcal { N } } } _ { K } \right\}$ from the top- $K$ list of model predictions. These self-generated negative examples are intended to enrich the generalization capability of the model by providing more realistic test-time predictions.

We expect to fully utilize the model’s mistakes, so we design a self-correcting loss function that realizes this property through pairwise comparisons. Specifically, we employ the sequence-level scores BLEU and similarity to quantify the generated examples. All examples were ranked according to their relative difference from the original sentence. Besides, the ranked example pairs are appended to the batch to form pairwise examples $( \tilde { \mathcal { V } } _ { 1 } ^ { + } , \tilde { \mathcal { V } } _ { 2 } ^ { - } )$ , where $^ +$ and are determined by their ranks. We optimize the model parameters using the weighted sum of the negative log-likelihood loss $L ^ { \mathrm { N L L } }$ and the self-correct ranking loss $L ^ { \mathrm { R A N K } }$ as the training loss for each training pair $( \mathcal { X } ^ { \prime } , \mathcal { Y } )$ as follows:

$$
\begin{array} { c } { { \displaystyle = } } \\ { { \displaystyle \sum _ { a \notin \mathcal { V } } \sum _ { b \in \mathcal { V } } \operatorname* { m a x } \left( 0 , \gamma + \sin \left( z _ { \mathcal { X } ^ { \prime } } , z _ { \tilde { \mathcal { V } } _ { a } ^ { + } } ^ { \sim } \right) , \sin \left( z _ { \mathcal { X } ^ { \prime } } , z _ { \tilde { \mathcal { V } } _ { b } ^ { + } } ^ { \sim } \right) \right) } } \\ { { L = L ^ { \mathrm { N L L L } } + L ^ { \mathrm { R A N K } } } } \end{array}
$$

During training, the $L ^ { \mathrm { N L L } }$ increases the similarity between the model output $\mathcal { X } ^ { \prime }$ and the target sentence $y$ . The $L ^ { \mathrm { R A N K } }$ prevents the model from generating each counter-example containing an adversarial perturbation $\tilde { \mathcal { V } } _ { k }$ , $\gamma$ is the margin.

# Decoding Intervention Strategy

The semantics of discrete text may be affected by even subtle errors and perturbations. Traditional decoding strategies may lead to a dramatic performance degradation under adversarial perturbations. Therefore, we design a decoding intervention strategy to address the over-correction problem and further improve the model’s robustness. Specifically, we incorporate a similarity function into the decoding phase to dynamically evaluate the correctness of the next token predicted by the decoder. The decoding goal in LIMIT is to find the sequence $y$ that maximizes the likelihood of the learned similarity score and the regular language model:

$$
\begin{array} { r } { s ( \mathcal { X } ^ { \prime } , \mathcal { Y } ) = \displaystyle \sum _ { t = 1 } ^ { | \mathcal { Y } | } ( \log p _ { \theta } ( \mathcal { Y } _ { t } \mid \mathcal { Y } _ { < t } , \mathcal { X } ^ { \prime } ) } \\ { + \alpha \times \sin { ( \mathcal { Y } _ { t } , \mathcal { X } ^ { \prime } ) } ) } \end{array}
$$

where the first term is the original probability of the language model, and the second term is the similarity score between the given the input sentence $\mathcal { X } ^ { \prime }$ and the generated $\mathcal { { D } } _ { t }$ , and $\alpha$ is the hyper-parameter that balances the contribution of each term.

# Experiments

In this section, we compare LIMIT with a range of text correction methods on unnatural text correction datasets. We also evaluate the adoption of LIMIT as a defense method against perturbations on natural language generation (NLG) tasks and natural language understanding (NLU) compared to adversarial training methods.

# Datasets

Unnatural Text Correction Datasets: PROTECT (Feng et al. 2024) includes unnatural text errors that are possible in Chinese characters. There are four subdatasets Perfect Pinyin, Abbreviation Pinyin, Character Split, and $H \boldsymbol { y }$ - brid. It covers common spelling errors involving visually or phonetically similar characters, splitting characters into radicals, and converting characters to perfect or abbreviated pinyin forms. Hybrid-v2 Based on the Hybrid perturbations, we construct insertion, deletion, inversion, and Unicode perturbations.

To verify that the proposed method can effectively serve as an adversarial defense method, following (Su et al. 2022) and (Feng et al. 2024), we perturb the NLU and NLG datasets. The specific perturbation process is detailed in the Technical Appendix.

NLU Datasets: For Chinese datasets, TNEWS (Xu et al. 2020) is a Chinese dataset for text classification. AFQMC (Xu et al. 2020) is a Chinese question-matching dataset designed to evaluate the performance of natural language processing models. CMNLI (Xu et al. 2020) is a Chinese multigenre cross-domain natural language reasoning dataset that asses a model’s ability to determine the relationships between premises and hypotheses. IFLYTEK (Xu et al. 2020) is a Chinese long-text classification dataset. COLD (Deng et al. 2022) is a Chinese offensive speech detection dataset.

For English datasets, we conduct our experiments on advSST-2, advQQP, advMNLI, and advRTE (Wang et al. 2021), which applies 14 state-of-the-art textual adversarial attack methods to GLUE tasks.

NLG Datasets: ADGEN (Shao et al. 2019) is an advertisement generation dataset. CSL (Zhang, Li, and Li 2021) is an academic domain text summarization dataset consisting of abstracts and titles of publications in the field of computer science. LCSTS (Hu, Chen, and Zhu 2015) is a large Chinese short text summarization dataset.

# Baselines

Text Correct Baselines: BERT (Devlin et al. 2019) is a pretrained language model that can be used for fine-tuning various natural language processing tasks. SoftMasked (Zhang et al. 2020b) used a pipeline structure of detection network and correction network to implement text error correction. MDCSpell (Zhu et al. 2022) employed a late fusion strategy to integrate the hidden states of the corrector with those of the detector, aiming to mitigate the adverse effects caused by misspelled characters. PLOME (Liu et al. 2021) designed a masking strategy based on a semantic confusion set when training pre-trained language models. MFT (Wu et al. 2023b) randomly masked $20 \%$ of the non-error tokens in the input sequence during the fine-tuning process, which is enough to learn a better language model without sacrificing the error model. RobustGEC (Zhang et al. 2023) proposed an effective post-training method Context Perturbation Robustness to enhance the stability and reliability of these systems in real-world applications. ATINTER (Gupta et al. 2023) is a module that intercepts and learns to rewrite adversarial inputs, making it non-adversarial for downstream text classifiers. ReLM (Liu, Wu, and Zhao 2024) trained the model to restate entire sentences by filling in extra slots instead of marking them word by word.

Adversarial Training Baselines: FreeLB (Zhu et al. 2020) is a fast adversarial training algorithm that integrates each intermediate example into a backward pass. SMART (Jiang et al. 2020) introduced smoothness-induced regularization in adversarial training for better generalization performance. $R 3 F$ (Aghajanyan et al. 2020) replaced the previously used adversarial targets with parametric noise (sampled from a normal or uniform distribution). CreAT (Wu et al. 2023a) presented a simple and effective contextual representation-adversarial training, where the attack is to explicitly optimize the contextual representation of the deviation encoder. Match-Tuning (Tong et al. 2022) added regularization between examples in the same batch.

Table 1: Performance of the baseline model and our approach on five Chinese unnatural text correction datasets. The best and second-best results are highlighted in bold and underline. Where Abb. Pinyin and Char. Split represents the Abbreviation Pinyin and Character Split respectively. The superscript $\dagger$ indicates $\mathsf { p } < 0 . 0 5$ for the t-test of the LIMIT vs. the PROTECT-Finetune.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="2">Perfect Pinyin</td><td colspan="2">Abb.Pinyin</td><td colspan="2">Char. Split</td><td colspan="2">Hybrid</td><td colspan="2">Hybrid-v2</td></tr><tr><td>Pre</td><td>F1</td><td>Pre</td><td>F1</td><td>Pre F1</td><td></td><td>Pre</td><td>F1</td><td>Pre</td><td>F1</td></tr><tr><td>BERT</td><td>31.0</td><td>33.0</td><td>41.0</td><td>43.0</td><td>43.4</td><td>54.4</td><td>25.7</td><td>28.3</td><td>14.6</td><td>15.8</td></tr><tr><td>SoftMasked</td><td>32.0</td><td>34.5</td><td>45.7</td><td>46.8</td><td>43.4</td><td>50.6</td><td>22.0</td><td>25.8</td><td>15.3</td><td>16.7</td></tr><tr><td>MDCSpell</td><td>32.6</td><td>34.6</td><td>45.5</td><td>46.5</td><td>42.3</td><td>49.8</td><td>23.1</td><td>27.0</td><td>14.3</td><td>15.5</td></tr><tr><td>PLOME</td><td>59.5</td><td>59.7</td><td>33.5</td><td>35.1</td><td>2.9</td><td>3.2</td><td>48.9</td><td>48.8</td><td></td><td></td></tr><tr><td>MFT</td><td>30.8</td><td>32.8</td><td>30.2</td><td>31.7</td><td>39.6</td><td>46.8</td><td>48.0</td><td>56.2</td><td></td><td></td></tr><tr><td>ReLM</td><td></td><td></td><td>46.7</td><td>47.9</td><td></td><td></td><td>37.0</td><td>43.5</td><td></td><td>、</td></tr><tr><td>RobustGEC</td><td>58.6</td><td>53.2</td><td>46.9</td><td>30.2</td><td>67.7</td><td>65.5</td><td>51.7</td><td>38.5</td><td>16.4</td><td>15.4</td></tr><tr><td>ATINTER</td><td>70.0</td><td>61.2</td><td>58.0</td><td>35.6</td><td>82.0</td><td>78.7</td><td>59.1</td><td>41.3</td><td>54.1</td><td>23.5</td></tr><tr><td>PROTECT-Fewshot</td><td>73.4</td><td>67.0</td><td>68.7</td><td>45.4</td><td>81.4</td><td>78.7</td><td>66.8</td><td>47.4</td><td>77.1</td><td>50.3</td></tr><tr><td>PROTECT-Finetune</td><td>90.2</td><td>82.1</td><td>84.8</td><td>57.7</td><td>94.4</td><td>91.8</td><td>90.4</td><td>71.2</td><td>83.1</td><td>59.6</td></tr><tr><td>LIMIT(Ours)</td><td>90.2†</td><td>84.6†</td><td>69.8</td><td>63.5†</td><td>91.9</td><td>93.2†</td><td>91.6†</td><td>81.2†</td><td>84.8†</td><td>66.8†</td></tr><tr><td>GPT-3.5-Turbo-10shot</td><td>23.2</td><td>22.1</td><td>2.7</td><td>3.4</td><td>1.0</td><td>1.2</td><td>22.2</td><td>19.4</td><td>12.5</td><td>11.0</td></tr></table></body></html>

Large Language Models: Llama1, Baichuan2 (Baichuan 2023), OPT-66B (Zhang et al. 2022), BLOOM (Le Scao et al. 2023) and ChatGPT. More experimental results for large language models are presented in the Technical Appendix.

# Implementations

To obtain robust textual representations against unnatural textual errors. For the Chinese corpus, we constructed adversarial examples using $3 0 0 \mathrm { k }$ randomly extracted texts from Chinese Wikipedia and continued pre-training on the T5- Base-Chinese 2 model. Similarly, for the English corpus, we constructed adversarial examples using $3 0 0 \mathrm { k }$ randomly extracted texts from Comments2019. Likewise, we continued pre-training on the T5-Large3 model.

To obtain robust representations, we pretrained the generation model after constructing adversarial examples. LIMIT has 12 layers/heads and 768 hidden neurons. It undergoes training on a scale of $6 0 \mathrm { k }$ with a batch size of 32, a learning rate of 1e-5, and a warm-up stage of 6k. The English version consists of 48 layers, 24 attention heads, and 1024 hidden neurons. It follows a learning rate of 1e-5, a warm-up stage of 6k, a batch size of 32, and a training stage of $6 0 \mathrm { k }$ .

LIMIT introduces three additional hyperparameters. The first one is the diversity of beam search size, denoted as $K$ . The second one is the boundary strength, denoted as $\gamma$ . The third one is the balancing factor, denoted as $\alpha$ . For all datasets, we set $K$ to 12 and $\gamma$ to 0.01. We tune $\alpha$ on the validation set using values from [0.3, 0.4, 0.5, 0.6, 0.7]. In practice, increasing the number of dynamic negative samples continually improves performance.

For the unnatural text correction task, we evaluate performance using precision (Pre) and the F1 score. In the NLU task, accuracy (Acc) serves as our primary metric. For NLG tasks, we employ Rouge-1 (R-1), Rouge-2 (R-2), and Rouge-L (R-L) to assess the quality of the generated text in comparison to the target text. These three metrics provide different perspectives on the quality of the generated text.

# Results on Chinese Unnatural Text Correction Datasets

In the unnatural text correction task, we evaluate the performance of several baseline models and our proposed LIMIT in five different types of unnatural text correction tasks: perfect pinyin, abbreviation pinyin, character split, hybrid, and hybrid-v2. The experimental results are shown in Table 1. To guarantee the reliability of the experiments, all results are averaged over five experiments.

We analyze the performance of traditional Bert-based text correction methods. PLOME performed best in the perfect pinyin task with an F1 score of $5 9 . 7 \%$ , but worst in the character split task with an F1 score of only $3 . 2 \%$ . ReLM performed well in the abbreviation pinyin task with an F1 score of $4 7 . 9 \%$ but failed to correct the errors in the other tasks. Overall, traditional text correction methods perform poorly in unnatural text correction tasks with generally low F1 scores. For the harder Hybrid- $\mathbf { \sigma } \cdot \mathbf { v } 2$ dataset, PLOME, MFT, and ReLM cannot handle such errors.

Furthermore, we analyze the performance of the generative correction methods. RobustGEC is the first to consider the robustness of a text error correction task against perturbations, however, the method performs poorly on the unnatural text correction tasks. The text rewriting strategy adopted by ATINTER performed well in the perfect pinyin and Character split tasks, with F1 scores of $6 1 . 2 \%$ and $78 . 7 \%$ , respectively, but did not perform well in the more complex tasks. The PROTECT-Fewshot model performed well on the perfect pinyin, abbreviation pinyin, and character split tasks with F1 scores of $67 . 0 \%$ , $4 5 . 4 \%$ , and $78 . 7 \%$ respectively, but performed slightly poorer on the hybrid task with F1 scores of $6 1 . 2 \%$ and $78 . 7 \%$ respectively. This is because the method excels at predicting error accuracy but is less effective at correcting errors.

Table 2: Performance of the adversarial training baseline models and our method on the Chinese NLU dataset. The best results are labeled with bold.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="2">TNEWS</td><td colspan="2">AFQMC</td><td colspan="2">CMNLI</td><td colspan="2">IFLYTEK</td><td colspan="2">COLD</td></tr><tr><td>Clean</td><td>Adv (Acc)</td><td>Clean (Acc)</td><td>Adv</td><td>Clean</td><td>Adv</td><td>Clean</td><td>Adv</td><td>Clean</td><td>Adv</td></tr><tr><td>BERT</td><td>(Acc) 66.6</td><td>65.4</td><td>75.1</td><td>(Acc) 72.4</td><td>(Acc) 80.8</td><td>(Acc) 77.5</td><td>(Acc) 58.4</td><td>(Acc) 56.2</td><td>(Acc) 93.1</td><td>(Acc) 80.5</td></tr><tr><td>FreeLB</td><td>67.1</td><td>65.5</td><td>74.2</td><td>70.9</td><td>80.1</td><td>77.4</td><td>59.3</td><td>57.6</td><td>93.1</td><td>80.5</td></tr><tr><td>SMART</td><td>66.6</td><td>64.7</td><td>73.1</td><td>70.9</td><td>79.4</td><td>76.3</td><td>58.3</td><td>55.5</td><td>93.1</td><td>80.5</td></tr><tr><td>R3F</td><td>67.1</td><td>65.5</td><td>74.1</td><td>71.0</td><td>80.1</td><td>77.5</td><td>58.7</td><td>56.5</td><td>93.1</td><td>80.5</td></tr><tr><td>CreAT</td><td>66.8</td><td>65.4</td><td>73.4</td><td>70.5</td><td>79.0</td><td>76.0</td><td>58.9</td><td>57.2</td><td>93.1</td><td>80.5</td></tr><tr><td>BERT+LIMIT(Ours)</td><td>66.6</td><td>66.0</td><td>75.1</td><td>72.4</td><td>80.8</td><td>79.1</td><td>58.4</td><td>59.7</td><td>93.1</td><td>82.4</td></tr><tr><td>Llama-7B</td><td>13.0</td><td>10.7</td><td>43.5</td><td>49.1</td><td>34.9</td><td>34.9</td><td>47.6</td><td>48.7</td><td>50.0</td><td>43.8</td></tr><tr><td>Baichuan2-13B</td><td>33.2</td><td>27.9</td><td>69.0</td><td>69.0</td><td>34.4</td><td>33.5</td><td>44.8</td><td>44.0</td><td>48.2</td><td>47.5</td></tr><tr><td>GPT-3.5-Turbo-10shot</td><td>49.9</td><td>47.9</td><td>69.0</td><td>68.9</td><td>52.9</td><td>51.0</td><td>49.8</td><td>37.1</td><td>51.9</td><td>50.0</td></tr></table></body></html>

Table 3: Performance of the adversarial training baseline models and our method on the English AdvGLUE dataset. Partial experimental results are from (Wu et al. 2023a)♮ and (Wang et al. 2023)♭, with the best performing scores shown in bold. More results for the large language model are presented in the Technical Appendix.   

<html><body><table><tr><td>Model</td><td>Adv SST-2 (Acc)</td><td>Adv QQP (Acc)</td><td>Adv MNLI-m (Acc)</td><td>Adv RTE (Acc)</td></tr><tr><td>BERT b FreeLBb R3F b CreAT b Match-Tuning b</td><td>32.3 31.6 38.5 35.3 51.4</td><td>50.8 51.0 40.6 51.5 41.5</td><td>32.6 33.5 35.8 36.0 35.5</td><td>37.0 42.0 50.1 45.2 47.5</td></tr><tr><td>BERT+LIMIT(Ours) OPT-66B b BLOOM-176Bb GPT-3.5-Turbo-10shot</td><td>66.2 52.4 51.3 60.1</td><td>78.8 46.1 41.0 72.0</td><td>69.4 39.7 26.4 67.8</td><td>84.0 42.0 43.2 75.3</td></tr></table></body></html>

It is noteworthy that our proposed LIMIT model demonstrates exceptional performance across all tasks, while the large language model is ineffective at correcting errors in unnatural text. This suggests that the LIMIT has significant advantages and potential in the Chinese unnatural text correction task.

# Results on Chinese NLU Datasets

Table 2 shows the experimental results on five Chinese NLU datasets. The experimental demonstrates show that the adversarial robustness of our proposed LIMIT achieves consistent improvement on the NLU task. On the perturbed TNEWS, AFQMC, CMNLI, IFLYTEK, and COLD datasets, LIMIT obtains an improvement of $0 . 6 \%$ , $1 . 9 \%$ , $3 . 1 \%$ , $2 . 5 \%$ , and $1 . 9 \%$ , respectively. We find that all the adversarial training methods suffer a loss of performance in Chinese tasks. The trade-off between performance and robustness is consistent with previous findings. For LIMIT, however, it is only responsible for removing adversarial perturbations from the input text. This preserves the performance of the language model to some extent. For example, the FreeLB and R3F outperform vanilla BERT on clean TNEWS and IFLYTEK datasets, while the SMART and CreAT sacrifice prediction performance on all tasks. On the relatively easier classification adversarial datasets TNEWS and IFLYTEK, most of the methods provide performance gains. However, for the inference tasks AFQMC and CMNLI, both lead to performance loss when trading off performance and robustness.

# Results on English NLU Datasets

Table 3 shows the experimental results on four English NLU adversarial datasets (AdvGLUE). Experimental results illustrate that LIMIT outperforms state-of-the-art methods and achieves the best performance on four randomly selected datasets. For the pre-training and fine-tuning methods, Match-Tuning with BERT-large achieves competitive results by adding regularization. For the large language models, ChatGPT exhibits better performance than the specifically designed model, achieving accuracy scores of $6 0 . 1 \%$ , $7 2 . 0 \%$ , $6 7 . 8 \%$ , and $6 5 . 5 \%$ on the four datasets, respectively. However, models with the same parameter sizes show considerable variation in performance, with an average accuracy of only $4 2 . 2 \%$ for BLOOM. For adversarial training methods, FreeLB, R3F, and CreAT perform poorly, which has validated their struggles to cope with a multitype of errors and perturbations.

# Results on Chinese NLG Datasets

The experimental results on the Chinese NLG dataset are demonstrated in Table 4. The results show that LIMIT exhibits the best adversarial robustness. This reflects its transferability to new tasks and models with competitive performance. For the experimental results with BobustGEC as the backbone, there is an average improvement of $1 . 0 \%$ , $6 . 2 \%$ , and $3 . 9 \%$ for ADGEN, CSL, and LCSTS adversarial datasets at Rouge-1, Rouge-2, and Rouge-L, respectively. Although PROTECT is designed to correct unnatural errors, it performs poorly on all three adversarial perturbed datasets. While the ATINTER, which employs rewriting to mitigate adversarial perturbations, incurs a performance loss on the ADGEN and CSL adversarial datasets. Specifically, on the ADGEN adversarial dataset, it degrades by $2 . 8 \%$ , $1 . 5 \%$ , and $0 . 3 \%$ on Rouge-1, Rouge-2, and Rouge-L, respectively. Likewise, it drops by $4 . 1 \%$ , $2 . 9 \%$ , and $3 . 8 \%$ in the CSL dataset, respectively.

Table 4: Performance of the generative correction baseline models and our method on the NLG dataset. The best results are labeled with bold.   

<html><body><table><tr><td>Model</td><td>(R-1)</td><td>Clean (R-2) (R-L)</td><td>(R-1)</td><td>Adv (R-2)</td><td>(R-L)</td></tr><tr><td colspan="6">ADGEN</td></tr><tr><td>RobustGEC</td><td>43.9</td><td>18.9</td><td>26.8 40.6</td><td>15.6</td><td>23.9</td></tr><tr><td>PROTECT</td><td>42.7</td><td>18.9 27.3</td><td>39.0</td><td>15.4</td><td>24.1</td></tr><tr><td>ATINTER</td><td>43.9</td><td>18.9 26.8</td><td>37.8</td><td>14.1</td><td>23.6</td></tr><tr><td>LIMIT(Ours)</td><td>43.9</td><td>18.9 26.8</td><td>41.6</td><td>16.7</td><td>24.9</td></tr><tr><td colspan="6">CSL</td></tr><tr><td>RobustGEC</td><td>64.6</td><td>52.6</td><td>61.4 52.9</td><td>38.2</td><td>49.9</td></tr><tr><td>PROTECT</td><td>63.6</td><td>52.0 60.7</td><td>52.8</td><td>37.7</td><td>49.0</td></tr><tr><td>ATINTER</td><td>64.6</td><td>52.6 61.4</td><td>48.8</td><td>35.3</td><td>46.1</td></tr><tr><td>LIMIT(Ours)</td><td>64.6</td><td>52.6 61.4</td><td>58.9</td><td>45.3</td><td>55.5</td></tr><tr><td colspan="6">LCSTS</td></tr><tr><td>RobustGEC</td><td>44.0</td><td>29.3</td><td>40.7</td><td>35.0 21.0</td><td>32.4</td></tr><tr><td>PROTECT</td><td>42.0</td><td>27.4</td><td>38.9 35.1</td><td>21.1</td><td>32.6</td></tr><tr><td>ATINTER</td><td>44.0</td><td>29.3</td><td>40.7</td><td>39.3 24.5</td><td>36.0</td></tr><tr><td>LIMIT(Ours)</td><td>44.0</td><td>29.3</td><td>40.7</td><td>39.4 24.6</td><td>36.1</td></tr></table></body></html>

Table 5: Ablation results in different components of LIMIT. The best-performing scores are in bold. Results for additional datasets are provided in the Technical Appendix.   

<html><body><table><tr><td>Dataset Model</td><td>Hybrid (F1)</td><td>CMNLI (Acc)</td><td>(R-1)</td><td>CSL (R-2)</td><td>(R-L)</td></tr><tr><td>Fine-tuning</td><td>75.2</td><td>77.5</td><td>52.8</td><td>38.2</td><td>49.9</td></tr><tr><td>+SC</td><td>75.6</td><td>78.8</td><td>58.0</td><td>45.1</td><td>55.4</td></tr><tr><td>+DI</td><td>81.2</td><td>79.1</td><td>58.9</td><td>45.3</td><td>55.5</td></tr></table></body></html>

![](images/60a4164457809e7149458ace10db2ec6fb5b180c9b005988f850b42efc12ccb1.jpg)

Figure 3: Relationship between $\alpha$ and BLEU under different training losses on the Chinese unnatural text correction dataset (Hybrid).   

<html><body><table><tr><td>Task</td><td>Pinyin</td><td>Abb.</td><td>Char. #Overcorrections／#Undercorrections</td><td>Hybrid</td></tr><tr><td>Vanilla</td><td>151/636</td><td>107/428</td><td>131/286</td><td>124/522</td></tr><tr><td>PROTECT</td><td>92/423</td><td>22/319</td><td>26/72</td><td>48/310</td></tr><tr><td>LIMIT(Ours)</td><td>43/149</td><td>31/310</td><td>24/67</td><td>33/134</td></tr></table></body></html>

Table 6: The empirical analysis results of the Vanilla finetune, PROTECT, and the proposed method in this paper. Where Abb. and Char. are Abbreviation Pinyin and Character Split respectively. Following Feng et al. (2024), we statistically counted the quantity of overcorrected samples.

racy of the NLU dataset is improved by an average of $0 . 4 \%$ , and an average of $0 . 3 \%$ improves the Rouge- $\boldsymbol { \cdot } \boldsymbol { \mathrm { L } }$ of the NLG dataset.

# Empirical Analysis on Hyper-parameter

Figure 3 shows the impact of the parameter $\alpha$ on BLEU scores and accuracy under different training losses. The proposed self-correct ranking loss achieves the best performance at $\alpha = 0 . 5$ , with a BLEU score of 0.57 and an F1 score of $8 1 . 2 \%$ . In comparison, the traditional adversarial training loss, InfoNCE, reaches a BLEU score of 0.56 and an accuracy of $7 9 . 3 \%$ at $\alpha = 0 . 5$ . It demonstrated the effectiveness of the self-correct ranking loss in Chinese unnatural text correction.

# Empirical Analysis on Over-correction

LIMIT achieves the best performance in perfect pinyin, character split, and hybrid, as shown in Table 6. Nevertheless, improving correction accuracy for abbreviated pinyin remains a necessary direction that requires further effort.

# Ablation Study

Table 5 shows the ablation studies of the different components of LIMIT on the Chinese and English adversarial datasets. It indicates that the components of self-correct adversarial training (SC) and decoding intervention (DI) both play key roles in enhancing adversarial robustness. Specifically, with the addition of SC, the average accuracy of the NLU dataset is improved by an average of $2 . 2 \%$ , and the Rouge-L of the NLG dataset is improved by an average of $3 . 1 \%$ . Similarly, with the addition of DI, the average accu

# Conclusion

In this paper, we propose a self-correct adversarial training framework for learning from mistakes, LIMIT, that enhances model robustness and adaptability to evolving spelling errors and adversarial perturbations. LIMIT offers a modeland task-agnostic solution for correcting unnatural text errors, ensuring robustness in error correction. Furthermore, it showcases transferability to various natural language understanding and natural language generation tasks, effectively resisting multi-type errors and perturbations.