# Multi-Granular Multimodal Clue Fusion for Meme Understanding

Li Zheng1, Hao Fei2, Ting Dai1, Zuquan Peng1, Fei $\mathbf { L i } ^ { 1 , 3 * }$ , Huisheng $\mathbf { M } \mathbf { a } ^ { 4 }$ , Chong Teng1, Donghong Ji1

1Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China 2National University of Singapore, Singapore, Singapore $^ 3$ Laboratory for Advanced Computing and Intelligence Engineering, Wuxi, China 4North China Institute of Computing Technology, beijing, China {zhengli,daiting cs,pzq cse,lifei csnlp,tengchong,dhji} $@$ whu.edu.cn haofei37@nus.edu.sg, mhs $@$ bupt.cn

# Abstract

With the continuous emergence of various social media platforms frequently used in daily life, the multimodal meme understanding (MMU) task has been garnering increasing attention. MMU aims to explore and comprehend the meanings of memes from various perspectives by performing tasks such as metaphor recognition, sentiment analysis, intention detection, and offensiveness detection. Despite making progress, limitations persist due to the loss of fine-grained metaphorical visual clue and the neglect of multimodal text-image weak correlation. To overcome these limitations, we propose a multigranular multimodal clue fusion model (MGMCF) to advance MMU. Firstly, we design an object-level semantic mining module to extract object-level image feature clues, achieving fine-grained feature clue extraction and enhancing the model’s ability to capture metaphorical details and semantics. Secondly, we propose a brand-new global-local cross-modal interaction model to address the weak correlation between text and images. This model facilitates effective interaction between global multimodal contextual clues and local unimodal feature clues, strengthening their representations through a bidirectional cross-modal attention mechanism. Finally, we devise a dual-semantic guided training strategy to enhance the model’s understanding and alignment of multimodal representations in the semantic space. Experiments conducted on the widely-used MET-MEME bilingual dataset demonstrate significant improvements over state-of-the-art baselines. Specifically, there is an $8 . 1 4 \%$ increase in precision for offensiveness detection task, and respective accuracy enhancements of $3 . 5 3 \%$ , $3 . 8 9 \%$ , and $3 . 5 2 \%$ for metaphor recognition, sentiment analysis, and intention detection tasks. These results, underpinned by in-depth analyses, underscore the effectiveness and potential of our approach for advancing MMU.

# Introduction

Memes, as a popular form of online communication, express viewpoints, sentiments, and intentions in a concise and humorous manner. With the development of social networks, Multimodal Meme Understanding (MMU) (Wang et al. 2024; Xu et al. 2022), as an emerging research area in Natural Language Processing (NLP), plays a crucial role in many downstream applications, such as question answering (Zheng et al.

The fine-grained metaphorical visual information HOW ITALIANSFIGHTCORONA VIRUS "Peaceis onlyanarmistice in anendlesswar VonderWoman Knows-Thucydide Doyou The weak correlation between the text and image modalities (a) How Italians fight (b) Peace is only an armistice corona virus? in an endless war.

2024b) and sentiment analysis (Zheng et al. 2023a,b). The definition of the MMU task involves predicting understanding from four dimensions: metaphor, sentiment, intention, and offensiveness. However, memes are nuanced, and accurately grasping the underlying meaning embedded within the combination of text and images poses a crucial challenge.

Several studies have made commendable efforts in MMU. Kiela et al. (2020); Kirk et al. (2021) introduced multimodal hate meme datasets specifically designed for hate detection. However, these studies overlooked the crucial aspect of metaphorical features in memes. Therefore, Xu et al. (2022) considered the richer metaphorical features in memes and constructed a baseline model and a bilingual dataset called MET-MEME for this purpose. Furthermore, Wang et al. (2024) proposed a metaphor-aware multimodal multitask framework on this dataset to capture the interactions between text and images. Despite achieving notable success, current researches in this field face two significant limitations: 1) the loss of fine-grained metaphorical visual clues and 2) the neglect of multimodal text-image weak correlation. These limitations hinder its further flourishing and widespread adoption.

On the one hand, existing works (Qu et al. 2023; Ji, Ren, and Naseem 2023) exhibit a lack of emphasis on images and simply encode broad visual representations at the image-level, ignoring metaphorical clues at the fine-grained object-level of images. This neglect leads to a critical absence of key visual metaphorical details, resulting in semantic ambiguity and omissions, ultimately failing to comprehensively capture the complexity and diversity of memes. As shown in Figure 1 (a), encoding visual features solely at the image-level falls short in capturing the crucial metaphorical clues of a pizza being used as a mask. Detecting the presence of metaphors and accurately predicting the conveyed sentiments, intentions, and potential offensiveness becomes exceedingly challenging in such cases.

On the other hand, existing methods (Xu et al. 2022; Wang et al. 2024) primarily focus on directly integrating textual and visual information to comprehend memes, overlooking the issue of weak correlations between modalities and disregarding the intrinsic crucial metaphorical clues within each modality. This oversight lead to the loss of crucial details and clues, thereby limiting a comprehensive understanding of memes. For instance, in the illustrated example in Figure 1 (b), there is a weak correlation between the image and text, where the image conveys peace while the text reveals hatred towards war. Merely concatenating and fusing the image and text information directly could lead to a misinterpretation of the meme as peaceful.

In this paper, motivated by the aforementioned observations, we propose a Multi-Granular Multimodal Clue Fusion model (MGMCF) to improve multimodal meme understanding. First, we design an object-level semantic mining module to extract fine-grained object-level feature clues from images. We then integrate these object-level feature clues with the overall image-level feature clues to obtain a multi-granular representation. This enables our model to better capture the metaphorical details and semantics of images, offering a more comprehensive visual understanding. Second, considering the weak correlation between text and images, we not only focus on the interactions between different modalities but also emphasize the crucial metaphorical clues within each modality, integrating multi-granular clues to enhance the ultimate understanding of multimodal memes. Therefore, we propose a novel global-local cross-modal interaction model to enable effective interaction between the global multimodal contextual clues and local unimodal feature clues. Specifically, the global multimodal context enhances the local unimodal features through a symmetric cross-modal attention mechanism. This interaction process is bidirectional, allowing the global context to extract useful clues from the local unimodal features and update itself. Through multi-level stacking, the global multimodal context and local unimodal features mutually enhance each other and gradually improve. Moreover, to enhance semantic alignment, we devise a dual-semantic guided training strategy. By bringing related image-text pairs closer in the forward direction and pushing unrelated pairs apart in the reverse direction, we aim to foster a more robust understanding of complex multimodal semantic clues.

To verify the effectiveness of our model, we conduct experiments on the benchmark MET-MEME bilingual dataset (Xu et al. 2022), which contains both English and Chinese memes. The results demonstrate that our model significantly outperforms all state-of-the-art (SoTA) baselines across all evaluation metrics in the four tasks. On the English meme dataset, the precision in the offensiveness detection task increased by $8 . 1 4 \%$ , and the accuracy in metaphor recognition, sentiment analysis, and intention detection tasks improved by $3 . 5 3 \%$ ,

$3 . 8 9 \%$ , and $3 . 5 2 \%$ , respectively. Additionally, extensive experiments validate the effectiveness of our fine-grained visual information enhancement and global-local interactions.

Our main contributions are summarized as follows:

• We analyze and summarize two intrinsic challenges in the MMU task and propose a multi-granular multimodal clue fusion model, the first to consider fine-grained visual clues and integrate unimodal feature clues to enhance MMU. • We design an object-level semantic mining module and a global-local cross-modal interaction model to facilitate effective interaction between global multimodal clues and local unimodal clues, achieving multi-granular understanding of meme metaphorical clues and semantics. • Our extensive experimental results on MET-MEME dataset demonstrate that our scheme achieves state-ofthe-art performance on the MMU task.

# Related Work

# Multimodal Meme Understanding

Recently, multimodal meme understanding (Lin et al. 2024; Hee, Chong, and Lee 2023; Qu et al. 2023; Fang et al. 2024b, 2023, 2024c, 2025; Zheng et al. 2024a) has attracted increasing attention. Unlike general multimodal learning tasks (Ji et al. 2021, 2022; Li et al. 2022b,a; Wu et al. 2023; Li et al. 2023a; Fei et al. 2024a; Luo et al. 2024), meme understanding relies more heavily on contextual and metaphorical information. Existing research has mainly focused on hateful memes. Yang et al. (2023) proposed a scalable invariant and specific modality representation learning framework based on graph neural networks for harmful meme detection. Ji, Ren, and Naseem (2023) introduced a prompt-based method to identify harmful memes. Beyond just focusing on hateful meme detection, Xu et al. (2022) introduced a fine-grained multimodal meme understanding dataset, which includes tasks such as sentiment analysis, intention detection, offensiveness detection, and metaphor recognition, to analyze memes at a finer granularity. Wang et al. (2024) created cross-modal and intra-modal attention mechanisms on this dataset to capture the interactions between text and images for multimodal meme understanding. However, existing works overlook the object-level fine-grained clues in images, potentially leading to the loss of crucial metaphorical visual details. Moreover, these methods do not address the issue of cross-modal weak correlations, neglecting the essential clues within unimodal, which result in semantic ambiguity and confusion, failing to fully capture the complexity and diversity of memes.

# Metaphorical Information Processing

In the field of NLP, there has been growing interest in developing models for metaphor detection (He et al. 2024; Fang et al. 2024a; Elzohbi and Zhao 2024; Zhang and Liu 2023). Understanding the essence of memes relies critically on identifying the metaphorical information embedded within them. Existing researches (Qiao, Zhang, and Ma 2024; Elzohbi and Zhao 2024; Badathala et al. 2023) have primarily focused on unimodal metaphor detection. Zhang and Liu (2023) proposed a novel multi-task learning framework based on a metaphor recognition program, a set of linguistic rules. Li et al. (2023b) performed metaphor detection by explicitly modeling the basic meanings of concepts. Tian et al. (2023) designed a domain contrastive learning strategy to capture the semantic inconsistencies. While these unimodal metaphor detection methods have achieved promising results, there has been relatively less exploration in the area of multimodal metaphor detection. Alnajjar, H¨am¨al¨ainen, and Zhang (2022) introduced a multimodal metaphor annotated corpus and designed a video-text content-based method for metaphor detection. He et al. (2024) developed a multi-interactive cross-modal residual network for multimodal metaphor recognition.

![](images/e87d61c7bec46e52dba9a8d4a8ee73611963607330210d26a0f9e528c164b6ef.jpg)  
Figure 2: The overall architecture of our model. “mr” means metaphor recognition, “sa” means sentiment analysis, “id” means intention detection, “od” means offensiveness detection.

# Methodology

# Task Definition

This paper addresses the task of multimodal meme understanding, encompassing metaphor recognition, sentiment analysis, intention detection, and offensiveness detection. Specifically, given an example consisting of an image $I$ , its corresponding text $T$ , a source domain $T _ { s }$ , and a target domain $T _ { a }$ , the objective of multimodal meme analysis is to predict the categories of metaphor $y _ { m r }$ , sentiment $y _ { s a }$ , intention $y _ { i d }$ , and offensiveness $y _ { o d }$ . As shown in Figure 2, the source domain serves as the basis for the metaphor, while the target domain embodies the concept or idea metaphorically conveyed, typically in textual form.

# Feature Extraction

Text Encoder. In accordance with the approach described in Wang et al. (2024), we utilize Multilingual BERT (Wang et al. 2019) to extract textual features from the corresponding text $x ^ { t }$ , source domain $x ^ { s }$ , and target domain $x ^ { a }$ . The encoding process can be formulated briefly as:

$$
\begin{array} { r } { \{ \pmb { h } _ { 1 } ^ { t } , . . . , \pmb { h } _ { n } ^ { t } \} = M . B E R T ( \{ \pmb { x } _ { 1 } ^ { t } , . . . , \pmb { x } _ { n } ^ { t } \} ) } \\ { \{ \pmb { h } _ { 1 } ^ { s } , . . . , \pmb { h } _ { p } ^ { s } \} = M . B E R T ( \{ \pmb { x } _ { 1 } ^ { s } , . . . , \pmb { x } _ { p } ^ { s } \} ) } \\ { \{ \pmb { h } _ { 1 } ^ { a } , . . . , \pmb { h } _ { q } ^ { a } \} = M . B E R T ( \{ \pmb { x } _ { 1 } ^ { a } , . . . , \pmb { x } _ { q } ^ { a } \} ) } \end{array}
$$

where n, p, and q represent the word counts of the corresponding text, source domain, and target domain, respectively.

# Image Encoder.

Multimodal meme images contain rich metaphorical details that are crucial clues for understanding memes. Therefore, when extracting visual features from memes, we cannot solely focus on image-level visual semantic clues as with other multimodal tasks. It is imperative to capture object-level fine-grained clue features that encompass these metaphorical details. To achieve this goal, we devise a visual information enhancement strategy for extracting feature clues of different granularities.

For image I, following (Wang et al. 2024), we first employ a pretrained convolutional neural network classifier, VGG16 (Simonyan and Zisserman 2014), to extract image-level features $\pmb { h } ^ { \dot { c } } = V G G 1 6 ( I )$ . Then, we transform the input image I into a series of embedded blocks to capture fine-grained image features. By integrating object detection, attribute recognition, and positional information, we enrich the representation of image features and enhance enhance image metaphor comprehension. Specifically, we design an object-level semantic mining module (Anderson et al. 2018) to identify and localize objects in an image. For each visual region $I _ { i }$ represented by a bounding box, we resize the region to a standard size of $2 2 4 \times 2 2 4$ pixels. Following Xu, Zeng, and Mao (2020), we reshape the resized region $I _ { i }$ into a sequence $I _ { i } = \{ r _ { 1 } , . . . , r _ { m } \}$ , where each region is represented by a block. This reshaping divides the region into a grid of blocks, with m being the total number of blocks. Next, we flatten each block $r _ { j }$ and project it into a $d ^ { I }$ -dimensional vector. This projection is performed using a trainable linear projection matrix E, and the resulting embedded representation of block $r _ { j }$ is denoted as $z _ { j } = r _ { j } E$ . To incorporate contextual information and retain positional information, we prepend a [class] token embedding at the beginning of the patch sequence. Position embeddings are also appended to the patch embeddings, indicating their relative positions within the sequence. The input representation of each visual region $I _ { i }$ is expressed as:

$$
Z _ { i } = [ z _ { [ c l a s s ] } ; z _ { 1 } , . . . , z _ { m } ] + E _ { p o s }
$$

where $Z _ { i }$ represents the input matrix of image patches, and $E _ { p o s }$ denotes the position embedding matrix. Subsequently, we feed the input matrix $Z _ { i }$ into the VGG16 encoder to obtain the visual region $I _ { i }$ representation $h _ { i } ^ { v } = V G G 1 6 ( Z _ { i } )$ Finally, the representation of the image I is defined as:

$$
\pmb { h } _ { I } = \{ \pmb { h } ^ { c } , \pmb { h } _ { 1 } ^ { v } , . . . , \pmb { h } _ { m } ^ { v } \}
$$

# Modal Fusion

The text and image of multimodal meme have the problem of weak correlation, and directly fusing the text and image may result in incorrect meme understanding. A good fusion solution should extract and integrate sufficient information from multimodal sequences while preserving the independence of each modality. Therefore, we propose a novel global-local cross-modal interaction model that not only considers interactions between modalities but also emphasizes the importance of each modality itself to enhance multimodal fusion at multiple granularities. Specifically, we devise an efficient mechanism called Cross-modal Attention Promotion (CAP) that leverages symmetric cross-modal attention to explore the inherent correlations between the two input feature sequences, promoting the exchange of beneficial information across the sequences. CAP utilizes self-attention to model the temporal dependencies within each feature sequence, enabling the integration of more information. The mechanism takes sequences $\mathbf { \delta } _ { h ^ { T } }$ and $\boldsymbol { h } ^ { I }$ as inputs and generates their mutually reinforcing information $h _ { T  I }$ and $h _ { I  T }$ . The computation of $C A P _ { T  I } ( h ^ { T } , h ^ { I } )$ is as follows:

$$
\begin{array} { r } { \pmb { h } _ { T  I } ^ { \prime } = M C A ( L N ( \pmb { h } _ { T } ) , L N ( \pmb { h } _ { I } ) ) + \pmb { h } _ { T } } \\ { \pmb { h } _ { T  I } ^ { \prime \prime } = M S A ( L N ( \pmb { h } _ { T  I } ^ { \prime } ) ) + \pmb { h } _ { T  I } ^ { \prime } } \\ { \pmb { h } _ { T  I } = F N ( L N ( \pmb { h } _ { T  I } ^ { \prime \prime } ) ) + \pmb { h } _ { T  I } ^ { \prime \prime } } \end{array}
$$

where LN denotes layer normalization and FN is the feedforward neural network. $M S A ( \cdot )$ refers to the output of the multi-head self-attention mechanism computation. $M C A ( \cdot , \cdot )$ represents the result of the multiple crossattention mechanism calculation. Similarly, we can obtain $C A P _ { I  T } ( h _ { I } , h _ { T } )$ .

The traditional cross-attention interaction requires two updates during the modal interaction process to achieve modal enhancement, which is inefficient and introduces redundant features into the sequence. Based on the historical experience from large-scale pretraining, it has been observed that a single token can represent the entire sequence, further improving the efficiency of modal interaction. Motivated by this observation, we propose a global-local cross-modal interaction model with linear computational cost to enhance efficiency. The discourse-level representation of each modality replaces the standard information and interacts with local unimodal features within a global multimodal context. This means that the representation of each modality not only relies on local features but also takes into account the influence of global context. This global-local interaction model reduces the introduction of redundant features and improves modal interaction effectiveness while maintaining efficiency.

We establish the global multimodal context information denoted as $\pmb { g } ^ { i } = c o n c a t ( \pmb { h } _ { T } ^ { i } , \pmb { h } _ { I } ^ { i } )$ by concatenating the representations of each modality at each layer of global-local interaction, where i represents the number of layers of global local interaction. By integrating the global context information and local modal information, and learning modal consistency and specificity, we ensure effective interaction and capture relevant information from both the global and local perspectives. The entire interaction process is as follows:

$$
h _ { T } ^ { ( i + 1 ) } , g _ { T \right. G } ^ { ( i ) } = C A P _ { T \left. G } ^ { ( i ) } ( h _ { T } ^ { ( i ) } , g ^ { ( i ) } )
$$

$$
h _ { I } ^ { ( i + 1 ) } , g _ { I \right. G } ^ { ( i ) } = C A P _ { I \left. G } ^ { ( i ) } ( h _ { I } ^ { ( i ) } , g ^ { ( i ) } )
$$

By stacking multiple layers, the global multimodal context and local unimodal features can mutually reinforce and progressively refine each other. We hierarchically handle the entire learning process, with each layer capturing different features corresponding to the model’s main stages. The model initially learns shallow interaction features, gradually progressing to acquire higher-order semantic features in later stages. This hierarchical learning method successfully integrates information from various modalities by ingeniously designed aggregation blocks, providing the model with a more comprehensive and enriched representation of multimodal features. Through the model’s interactions, information from different modalities can be combined in a deeper and more effective manner, enabling the acquisition of more advanced feature representations in subsequent hierarchical learning. Subsequently, we aggregate the features from both unimodal and multimodal sources to facilitate subsequent task predictions.

$y _ { m } = s o f t m a x ( W _ { m } M S A ( [ { \pmb h } _ { T } ^ { ( L ) } , { \pmb h } _ { I } ^ { ( L ) } , { \pmb g } ^ { ( L ) } ] ) + b )$ (6) where $y _ { m }$ is the feature output distribution after multimodal fusion, $W _ { m }$ and $b$ are trainable parameters.

Our approach focuses not only on the interactions between modalities but also on the individual feature representations of each modality. We separately use the unimodal features obtained from text and image encoders to predict subsequent tasks. This allows to capture the unique characteristics and information within each modality, thereby improving the accuracy and effectiveness of MMU.

$$
\begin{array} { r } { y _ { T } = s o f t m a x ( W _ { t } h _ { T } + b ) } \\ { y _ { I } = s o f t m a x ( W _ { i } h _ { I } + b ) } \end{array}
$$

Given the $y _ { M } , y _ { T }$ , and $y _ { I }$ , we obtain the final prediction y:

$$
{ \pmb y } = { \pmb y } _ { M } + { \pmb y } _ { T } + { \pmb y } _ { I }
$$

where y can be considered as a comprehensive feature set encompassing multi-granular features, including text, image, and image-text modalities.

<html><body><table><tr><td rowspan="2">Method</td><td colspan="3"></td><td colspan="3">Chirese</td><td colspan="3">Enprish</td><td colspan="3">Chirese</td></tr><tr><td>Acc</td><td>Engrish</td><td>Rec</td><td>Acc</td><td></td><td>Rec</td><td>Acc</td><td></td><td>Rec Intention Detection</td><td>Acc</td><td></td><td>Rec</td></tr><tr><td colspan="10">Sentiment Analysis</td><td colspan="3"></td></tr><tr><td>VGG16</td><td>20.57</td><td>20.84</td><td>24.22</td><td>29.94</td><td>26.04</td><td>29.20</td><td>37.19</td><td>38.71</td><td>38.15</td><td>47.48</td><td>49.21</td><td>47.81</td></tr><tr><td>DenseNet-161</td><td>21.88</td><td>21.71</td><td>25.65</td><td>29.45</td><td>27.50</td><td>29.36</td><td>38.10</td><td>39.31</td><td>37.89</td><td>47.23</td><td>39.24</td><td>47.06</td></tr><tr><td>ResNet-50</td><td>21.74</td><td>18.63</td><td>21.35</td><td>29.36</td><td>27.50</td><td>29.28</td><td>39.19</td><td>37.12</td><td>40.10</td><td>47.15</td><td>39.23</td><td>47.06</td></tr><tr><td>Multi-BERT_EfficientNet</td><td>28.52</td><td>24.52</td><td>29.04</td><td>33.50</td><td>35.29</td><td>33.42</td><td>43.10</td><td>41.54</td><td>42.19</td><td>51.03</td><td>43.06</td><td>51.03</td></tr><tr><td>Multi-BERT_ViT</td><td>24.43</td><td>23.41</td><td>23.96</td><td>33.25</td><td>27.33</td><td>32.84</td><td>41.28</td><td>40.13</td><td>40.62</td><td>50.62</td><td>41.32</td><td>50.62</td></tr><tr><td>Multi-BERT_PiT</td><td>25.00</td><td>27.82</td><td>28.12</td><td>33.66</td><td>33.58</td><td>33.09</td><td>42.23</td><td>41.09</td><td>41.02</td><td>50.21</td><td>50.00</td><td>50.04</td></tr><tr><td>MET_add</td><td>24.65</td><td>24.52</td><td>25.26</td><td>32.50</td><td>32.62</td><td>33.50</td><td>40.32</td><td>40.39</td><td>41.28</td><td>52.93</td><td>52.68</td><td>54.01</td></tr><tr><td>MET_cat</td><td>27.68</td><td>28.41</td><td>29.82</td><td>33.42</td><td>34.33</td><td>33.91</td><td>38.56</td><td>39.19</td><td>39.84</td><td>51.58</td><td>51.48</td><td>52.85</td></tr><tr><td>M3F_add</td><td>30.47</td><td>33.45</td><td>30.34</td><td>39.95</td><td>41.80</td><td>39.87</td><td>44.40</td><td>41.89</td><td>44.32</td><td>55.25</td><td>54.57</td><td>55.00</td></tr><tr><td>M3F_cat</td><td>29.82</td><td>34.18</td><td>30.73</td><td>37.22</td><td>39.55</td><td>37.97</td><td>44.10</td><td>44.56</td><td>43.53</td><td>53.52</td><td>54.72</td><td>54.52</td></tr><tr><td>Ours</td><td>34.36</td><td>37.77</td><td>34.38</td><td>42.11 (+2.16%)</td><td>47.59</td><td>42.02</td><td>47.92</td><td>47.53</td><td>47.06 (+2.74%)</td><td>58.56</td><td>57.99</td><td>58.32 (+3.32%)</td></tr><tr><td>(+3.89%)</td><td>(+3.52%)</td><td colspan="3">(+3.65%)</td><td>(+5.79%) (+2.15%)</td><td></td><td colspan="3">(+3.52%) (+2.97%)</td><td colspan="3">(+3.31%) (+3.27%)</td></tr><tr><td></td><td></td><td></td><td>Offensiveness Detection</td><td></td><td></td><td></td><td></td><td></td><td></td><td>Metaphor Recognition</td><td></td><td></td></tr><tr><td>VGG16</td><td>67.10</td><td>63.42</td><td>72.53</td><td>70.07</td><td>64.11</td><td>72.07</td><td>78.39</td><td>79.73</td><td>79.95</td><td>67.00</td><td>67.24</td><td>67.82</td></tr><tr><td>DenseNet-161</td><td>69.66</td><td>62.07</td><td>69.98</td><td>71.43</td><td>70.82</td><td>75.43</td><td>80.08</td><td>80.23</td><td>80.47</td><td>67.16</td><td>67.91</td><td>67.99</td></tr><tr><td>ResNet-50</td><td>69.21</td><td>64.62</td><td>72.57</td><td>73.24</td><td>69.62</td><td>75.74</td><td>80.34</td><td>81.22</td><td>80.86</td><td>67.74</td><td>67.63</td><td>67.66</td></tr><tr><td>Multi-BERT_EfficientNet</td><td>73.78</td><td>67.98</td><td>74.56</td><td>78.15</td><td>72.11</td><td>79.98</td><td>82.46</td><td>84.39</td><td>83.11</td><td>74.19</td><td>71.26</td><td>74.28</td></tr><tr><td>Multi-BERT_ViT</td><td>71.22</td><td>62.96</td><td>72.66</td><td>76.92</td><td>67.31</td><td>78.74</td><td>81.90</td><td>82.01</td><td>82.46</td><td>73.28</td><td>72.55</td><td>73.70</td></tr><tr><td>Multi-BERT_PiT</td><td>72.79</td><td>66.69</td><td>74.26</td><td>77.17</td><td>70.16</td><td>79.05</td><td>82.07</td><td>83.05</td><td>82.98</td><td>75.10</td><td>73.15</td><td>74.28</td></tr><tr><td>MET_add</td><td>68.39</td><td>66.21</td><td>72.14</td><td>76.01</td><td>74.76</td><td>78.16</td><td>81.33</td><td>81.49</td><td>82.29</td><td>74.04</td><td>74.51</td><td>74.96</td></tr><tr><td>MET_cat M3F_add</td><td>67.25</td><td>66.15</td><td>74.48</td><td>73.19</td><td>71.59</td><td>79.49</td><td>82.39</td><td>82.69</td><td>83.33</td><td>72.90</td><td>72.80</td><td>75.67</td></tr><tr><td>M3F_cat</td><td>76.17 74.09</td><td>69.45 69.59</td><td>76.19</td><td>80.81</td><td>76.00</td><td>80.73</td><td>83.98</td><td>85.86</td><td>84.38</td><td>77.01</td><td>72.94</td><td>82.68</td></tr><tr><td>Ours</td><td></td><td></td><td>76.15</td><td>80.07</td><td>76.20</td><td>80.62</td><td>83.20</td><td>85.97</td><td>85.81</td><td>76.18</td><td>73.02</td><td>80.00</td></tr><tr><td></td><td>78.11 (+1.94%)</td><td>77.73 (+8.14%)</td><td>78.32 (+2.13%)</td><td>82.15 (+1.34%)</td><td>81.80 (+5.6%)</td><td>82.02 (+1.29%)</td><td>87.51 (+3.53%)</td><td>88.58 (+2.61%)</td><td>88.89 (+3.08%)</td><td>78.39 (+1.38%)</td><td>78.16 (+3.65%)</td><td>83.92 (+1.24%)</td></tr></table></body></html>

Table 1: Experimental results on the MET-MEME dataset of four tasks.

# Training

For each task, we train the model using the standard gradient descent algorithm to minimize the cross-entropy loss.

$$
\operatorname* { m i n } _ { \theta } \mathcal { L } _ { * } = - \sum _ { i = 1 } ^ { N } y _ { * } ^ { i } l o g \hat { y } _ { * } ^ { i } + \lambda _ { * } \left. \theta _ { * } \right. ^ { 2 }
$$

where $*$ stands for the representation of different tasks. N is the training data size. $y ^ { i }$ and $\hat { y } ^ { i }$ respectively represent the ground-truth and estimated label distribution of instance i. $\theta$ denotes all trainable parameters of the model, $\lambda$ represents the coefficient of L2-regularization.

Dual-semantic Guided Loss. In addition to task-specific losses, we devise a dual-semantic guided loss to effectively leverage cross-modal information, enhancing the model’s comprehension and alignment of multimodal representations in the semantic space. The context-aware multimodal representations ${ \mathbf { \bar { \mathbf { h } } } } T _ { i }$ and $h I _ { I }$ ) contain contextually relevant information associated with specific memes. By bringing related image-text pairs closer in the forward direction and pushing unrelated pairs apart in the reverse direction, these representations are aligned in the same semantic space, effectively utilizing cross-modal information. Specifically, we contrast the multimodal representation of a specific meme sample (i.e., $h _ { T _ { i } } )$ , with another multimodal representation $( h _ { I _ { i } } )$ from within the same batch of sampled memes. By comparing the similarities and differences between these representations, the model learns how to better differentiate and capture the semantic information among different meme samples.

$$
\mathcal { L } _ { d g } = - l o g \frac { e x p ( s i m ( h _ { T _ { i } } , h _ { I _ { i } } ) / \tau ) } { \sum _ { k = 1 [ k \neq i ] } ^ { 2 N } e x p ( s i m ( h _ { T _ { k } } , h _ { I _ { k } } ) / \tau ) ) }
$$

where sim is the cosine-similarity, $\mathrm { \bf N }$ is the batch size, and $\tau$ is the temperature to scale the logits.

By minimizing task-specific losses for each individual task and incorporating a contrastive loss, our overall loss function is defined as follows:

$$
\mathcal { L } = \mathcal { L } _ { m r } + \mathcal { L } _ { s a } + \mathcal { L } _ { i d } + \mathcal { L } _ { o d } + \mathcal { L } _ { d g }
$$

# Experiments

# Experimental Setting

Datasets. We assess the efficacy of our model on the widely used MET-MEME bilingual dataset (Xu et al. 2022), which consists of both English and Chinese memes. The English meme dataset is sourced from MEMOTION and Google search, comprising 4,000 text-image pairs, comprising 4,000 text-image pairs. The Chinese meme dataset consists of 6,045 text-image pairs, covering six different categories, including animals, scenery, animations, films, dolls, and humans.

Evaluation Metrics. In terms of evaluation metrics, we align with Wang et al. (2024) and use three metrics, namely accuracy (Acc), weighted precision (Pre), and recall (Rec), to assess the performance. All our scores are the average over 5 runnings with random seeds.

# Baseline Systems

To validate the effectiveness of our model, we compare it against the following state-of-the-art baselines. (1) Unimodal models that solely utilize image modality information, such as VGG16 (Simonyan and Zisserman 2014), DenseNet-161 (Huang et al. 2017), and ResNet-50 (He et al. 2016). (2) Multimodal models that incorporate both text and image modalities. These include Multi-BERT-EfficientNet (Tan and Le 2019), Multi-BERT-ViT (Dosovitskiy et al. 2020), MultiBERT-PiT (Heo et al. 2021), MET (Xu et al. 2022) employ a straightforward concatenation or element-wise addition to

<html><body><table><tr><td rowspan="2">Method</td><td colspan="3">SentimentAnalysis</td><td colspan="3">IntentionDetection</td></tr><tr><td>Acc</td><td>Pre</td><td>Rec</td><td>Acc</td><td>Pre</td><td>Rec</td></tr><tr><td>Ours</td><td>34.36</td><td>37.77</td><td>34.38</td><td>47.92</td><td>47.53</td><td>47.06</td></tr><tr><td>w/o OM</td><td>32.47</td><td>36.16</td><td>32.39</td><td>46.27</td><td>46.17</td><td>45.89</td></tr><tr><td>w/o UP</td><td>32.85</td><td>36.53</td><td>32.76</td><td>46.73</td><td>46.59</td><td>46.11</td></tr><tr><td>W/o GL</td><td>31.76</td><td>35.49</td><td>31.83</td><td>45.82</td><td>45.41</td><td>45.47</td></tr><tr><td>w/o DG</td><td>33.59</td><td>36.94</td><td>33.52</td><td>47.05</td><td>46.94</td><td>46.63</td></tr><tr><td rowspan="2"></td><td colspan="3">OffensivenessDetection</td><td colspan="3">Metaphor Recognition</td></tr><tr><td>Acc</td><td>Pre</td><td>Rec</td><td>Acc</td><td>Pre</td><td>Rec</td></tr><tr><td>Ours</td><td>78.11</td><td>77.73</td><td>78.32</td><td>87.51</td><td>88.58</td><td>88.89</td></tr><tr><td>w/o OM</td><td>77.28</td><td>73.41</td><td>77.22</td><td>85.42</td><td>87.45</td><td>87.46</td></tr><tr><td>w/o UP</td><td>77.53</td><td>74.66</td><td>77.68</td><td>85.81</td><td>87.64</td><td>87.74</td></tr><tr><td>W/o GL</td><td>76.89</td><td>72.57</td><td>76.94</td><td>84.98</td><td>86.77</td><td>86.79</td></tr><tr><td>W/o DG</td><td>77.74</td><td>75.39</td><td>77.93</td><td>86.43</td><td>87.93</td><td>88.02</td></tr></table></body></html>

Table 2: Ablation results on the MET-MEME English dataset of four tasks. “OM” means object-level semantic mining, “UP” means unimodal prediction, “GL” means global-local interaction, “DG” means dual-semantic guided strategy.

merge feature vectors for meme understanding, and M3F (Wang et al. 2024) devise attention mechanisms to capture the interaction between text and images.

# Main Results

We conduct a comprehensive comparison of our MGMCF with SoTA unimodal and multimodal models on the METMEME dataset. Table 1 presents the results for four tasks: sentiment analysis (SA), intention detection (ID), offensiveness detection (OD), and metaphor recognition (MR). The results highlight that our approach outperforms SoTA baselines across all evaluation metrics for the four tasks, revealing several key findings. Firstly, compared to unimodal models, multimodal models demonstrate superior performance by leveraging additional visual-textual features. However, it is crucial to thoroughly exploit visual information and deeply fuse multimodal features. Otherwise, they only yield marginal improvements over unimodal models or even underperform them (e.g., in the OD task, the MET method performs worse than DenseNet-161 and ResNet-50). By creating cross-modal attention, M3F achieves the current SoTA results. Notably, our model significantly surpasses the SoTA techniques. On the English meme dataset, our precision improves by $8 . 1 4 \%$ in the OD task, and accuracy improves by $3 . 5 3 \%$ , $3 . 8 9 \%$ , and $3 . 5 2 \%$ in MR, SA, and ID, respectively. On the Chinese dataset, our precision improves by $3 . 6 5 \%$ , $5 . 7 9 \%$ , $3 . 2 7 \%$ , and $5 . 6 \%$ in MR, SA, ID, and OD, respectively. Furthermore, our approach exhibits significant enhancements compared to MET. On the English MEME dataset, improvements range from $6 . 1 8 \%$ to $1 3 . 2 5 \%$ , and on the Chinese MEME dataset, enhancements range from $3 . 8 6 \%$ to $1 4 . 9 7 \%$ . These findings indicate that by focusing on object-level fine-grained image details and the intrinsic unimodal clues, and integrating multi-granular clues, we achieve significant performance improvements in the MMU.

# Ablation Study

We perform ablation experiments to evaluate the contribution of each component in our model. As depicted in Table 2, no variant matches the full model’s performance, highlighting the indispensability of each component. Specifically, when the global-local interaction is not utilized, three evaluation metric scores for all four tasks suffer the most significant decline. In particular, the precision score for the OD task drops by $5 . 1 6 \%$ , and for the SA task drops by $2 . 2 8 \%$ . This indicates that the global-local interaction successfully integrates information from different modalities, providing a more comprehensive multimodal feature representation. To validate the necessity of object-level semantic mining module, we remove this module, and the decline in results signifies its indispensable impact on MMU. This finding suggests that mining fine-grained information from images can offer more detailed insights into image content. Furthermore, removing unimodal predictions leads to a performance drop, indicating that unimodal predictions contribute to the final predictions and effectively address the issue of weak correlation between image and text. Additionally, performance declines when dual-semantic guided strategy is removed, demonstrating its crucial role in enhancing semantic alignment and reducing modalities’ inconsistencies.

![](images/9e0a66cdd57306a103f27f7d07efae92cca51547130c7df29b48cacf85069f73.jpg)  
Figure 3: Comparative results between global-local and locallocal interaction on the MET-MEME English dataset.

# Deep Analyses on The Proposed Methods

To further investigate the effectiveness of our method, we conduct in-depth analyses to answer the following questions, aiming to mine the intuition and analyze implicit phenomena. Q1: What are the advantages of the global-local interaction? To further validate the effectiveness of our proposed global-local interaction model, we conduct a comparative analysis between our global-local interaction method and the local-local interaction method. The local-local interaction method refers to performing pairwise local interactions within each modality. As shown in Figure 3, the results consistently demonstrate the superiority of the global-local interaction across all evaluation metrics for the four tasks. This finding indicates that by incorporating a higher-level global context that encompasses the entire modality, the global-local interaction method achieves more comprehensive and effective interactions between modalities. In contrast, the locallocal interaction method solely focuses on intra-modality local feature interactions and fails to fully leverage the holistic information across modalities. By leveraging the global context, we capture a broader range of semantic information, enabling a deeper understanding of the interdependence between modalities and effectively addressing inconsistencies and differences between modalities, thereby enhancing the performance of the MMU task.

![](images/615a6e63dd60f73c19e47e5f6316a380cd2c24752fa0c6b5ed391d70952189fb.jpg)  
Figure 4: Visualization of a typical example.

![](images/620238c0ef86dedcfe4f1721b43b7150ae5d71232f981aa4f5e2bd761f61799b.jpg)  
Figure 5: Influence of unimodal prediction on the METMEME English dataset.

Q2: How can fine-grained visual features help improve model performance? We conduct a visualization in Figure 4 to better illustrate the outstanding performance of the fine-grained visual enhancement module. By visualizing the attention distribution of the model, we observe that the module exhibits higher attention towards specific object parts in the image when the fine-grained visual enhancement module is utilized. Focusing on specific objects rather than the entire image allows for more accurate capture of crucial visual details. This confirms the effectiveness of the fine-grained visual enhancement module in improving the model’s attention and understanding of key visual information.

Q3: What are the advantages of unimodal prediction in multimodal meme understanding? In order to validate the effectiveness of incorporating unimodal prediction in multimodal meme understanding, we conduct a comparative analysis of the performance of combining unimodal prediction with separately removing text modal prediction and image modal prediction. As illustrated in Figure 5, the results consistently show that the combined unimodal prediction outperform the scenarios where any unimodal prediction is removed across all evaluation metrics in the four tasks. This indicates that by focusing on the crucial information within each modality, we can achieve a more comprehensive and accurate understanding of the visual and textual elements within memes. Furthermore, we observe that removing the image modality prediction has a larger impact on the performance compared to removing the text modality prediction. This finding emphasizes the importance of enhancing visual information. By capturing fine-grained visual details, the model can better leverage the rich visual cues and context present in memes. These findings highlight the value of unimodal prediction and underscore the significance of considering the specific characteristics of each modality in MMU.

![](images/8230395f7a394f0caf5dacb119e222a7fef2db9ea1e75e2538f1614d90d3adda.jpg)  
Figure 6: Influence of CLIP in MMU on the MET-MEME English dataset.

Q4: What impact does a large language model have on MMU? Considering the extensive usage of large language models (Wu et al. 2024a,b; Fei et al. 2024c,b), we investigate their influence on MMU. Figure 6 displays the results achieved by employing the standalone CLIP model and by integrating the CLIP model with our proposed method. Notably, employing the CLIP model alone yields impressive performance, attesting to its adeptness in comprehending multimodal memes. Moreover, the integration of the CLIP model with our method results in additional performance improvements, underscoring the efficacy of our approach.

# Conclusion

In this paper, we explore two major challenges in the MMU task: the loss of fine-grained metaphorical visual clues and the neglect of weak correlation between multimodal text and images, proposing a solution named MGMCF. MGMCF enhances the complexity and diversity of images by capturing object-level fine-grained visual clues, and resolves the weak correlation between text and images through a novel globallocal cross-modal interaction for multi-granular clue fusion. Extensive experiments on the MET-MEME bilingual dataset demonstrate the effectiveness of all our proposed innovative methods and hypotheses, achieving SoTA performance.