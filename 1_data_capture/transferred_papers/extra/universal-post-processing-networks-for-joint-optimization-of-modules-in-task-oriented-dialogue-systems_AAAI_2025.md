# Universal Post-Processing Networks for Joint Optimization of Modules in Task-Oriented Dialogue Systems

Atsumoto Ohashi, Ryuichiro Higashinaka

Graduate School of Informatics, Nagoya University, Japan ohashi.atsumoto. $\operatorname { c 0 } \ @$ s.mail.nagoya-u.ac.jp, higashinaka@i.nagoya-u.ac.jp

# Abstract

Post-processing networks (PPNs) are components that modify the outputs of arbitrary modules in task-oriented dialogue systems and are optimized using reinforcement learning (RL) to improve the overall task completion capability of the system. However, previous PPN-based approaches have been limited to handling only a subset of modules within a system, which poses a significant limitation in improving the system performance. In this study, we propose a joint optimization method for post-processing the outputs of all modules using universal post-processing networks (UniPPNs), which are language-model-based networks that can modify the outputs of arbitrary modules in a system as a sequence-transformation task. Moreover, our RL algorithm, which employs a modulelevel Markov decision process, enables fine-grained value and advantage estimation for each module, thereby stabilizing joint learning for post-processing the outputs of all modules. Through both simulation-based and human evaluation experiments using the MultiWOZ dataset, we demonstrated that UniPPN outperforms conventional PPNs in the task completion capability of task-oriented dialogue systems.

Code — https://github.com/nu-dialogue/UniPPN

# 1 Introduction

Typical task-oriented dialogue systems process a single user input through multiple subtasks to produce a final response. These subtasks include (1) natural language understanding (NLU), which estimates the user’s intent from the input; (2) dialogue state tracking (DST), which accumulates the user’s requests up to the current turn as a dialogue state; (3) dialogue policy (policy), which determines the next action that the system should take as dialogue acts (DAs); and (4) natural language generation (NLG), which converts these DAs into a final system response. Recent research has moved beyond optimizing dedicated modules for each subtask individually. It has explored the use of reinforcement learning (RL) to train several modules based on actual dialogue experiences, thereby optimizing the overall task completion capability of the system (Ni et al. 2022; Kwan et al. 2023).

Recently, a new approach using post-processing networks (PPNs) was proposed to optimize the task completion capability of dialogue systems without directly training the modules (Ohashi and Higashinaka 2022). In this approach, instead of training the modules directly, PPNs, which are components with trainable parameters that modify their outputs, are trained using RL. For example, Ohashi and Higashinaka (2022) implemented PPNs that post-process outputs from NLU, DST, and policy as multi-binary classification tasks using multi-layer perceptrons (referred to as BinPPN), and demonstrated that this optimization improved the overall task completion capability of the systems. In addition, a large language model (LLM)-based generative PPN (GenPPN) was proposed to post-process the natural language output from NLG, and its effectiveness has been demonstrated (Ohashi and Higashinaka 2023).

![](images/b91c05661b5f392a260e6b5022425a5510554812945147d5bf826360ac93eb63.jpg)  
Figure 1: Diagram of UniPPN. UniPPN modifies the output $\mathrm { o u t } _ { m }$ of $\mathbf { M o d u l e } _ { m }$ to $\mathrm { o u t } _ { m } ^ { + }$ , which serves as the input for $\mathbf { M o d u l e } _ { m + 1 }$

However, conventional PPN-based methods have two major limitations. First, BinPPN and GenPPN cannot be optimized jointly because of their different model architectures and training algorithms. Although it is possible to post-process the outputs of all modules by combining disjointly trained BinPPNs and GenPPNs, using multiple networks that are not jointly optimized may fail to achieve sufficient performance improvement. The second limitation is the narrow applicability of GenPPN. GenPPN aims to generate utterances that are easily understood by users and relies on a reward function that requires feedback on whether the DAs output from the policy are correctly conveyed to the user. This approach cannot be applied to an end-to-end dialogue system, such as the LLM-powered model proposed by Hudecˇek and Dusek (2023), which does not explicitly output DAs, imposing significant limitations on GenPPN’s applicability.

In this study, we propose a universal post-processing network (UniPPN) and an optimization method that combines the strengths of two conventional PPNs. UniPPN can jointly optimize the post-processing of outputs from all modules in task-oriented dialogue systems (Figure 1). Our proposed method involves a single-language model-based UniPPN that post-processes the outputs from all modules as a sequence-transformation task. Additionally, we introduce a newly designed module-level Markov decision process (MDP) that extends the standard MDP paradigm and incorporates it into UniPPN’s optimization algorithm. This approach enables fine-grained value and advantage estimation for each module, even with sparse feedback obtained only at the end of multi-turn dialogues, thus ensuring stable joint optimization. Because UniPPN does not require as dense feedback about DAs as GenPPN, it can be applied to a wide range of systems, including end-to-end systems that do not output DAs.

To verify the effectiveness of UniPPN across various dialogue systems, we conducted experiments using dialogue simulations based on the MultiWOZ dataset (Budzianowski et al. 2018). Specifically, we compared the task completion capabilities of dialogue systems equipped with conventional PPNs to those using UniPPN. The results demonstrated that UniPPN significantly outperformed conventional PPNs in terms of task completion capability. Additionally, through dialogue experiments with human users, we demonstrated that dialogue systems using UniPPN outperformed those using conventional PPNs.

# 2 Related Work

Modularity of Task-Oriented Dialogue Systems In typical pipeline task-oriented dialogue systems, dedicated modules for each subtask, such as NLU, DST, policy, and NLG, have been individually developed and optimized (Zhang et al. 2020). However, in recent years, methods addressing multiple subtasks using a single model have become common (Ni et al. 2022). For example, word-level DST (Wu et al. 2019; Zhao et al. 2022) estimates the dialogue state directly from the dialogue history without requiring user intent estimation using NLU. Similarly, a word-level policy (Lubis et al. 2020; Wang et al. 2020) generates system responses directly from the dialogue state without requiring conversion from DA to system utterances by NLG. Furthermore, endto-end dialogue systems (Hosseini-Asl et al. 2020; He et al. 2022; Wu et al. 2023), which learn all subtasks using a single model, are becoming popular. Because these end-to-end systems maintain modularity by sequentially executing each subtask, they are often referred to as modularly end-to-end systems (Qin et al. 2023).

Online RL for Task Completion In response generation for task-oriented dialogue systems, it is crucial not only to maximize the probability of reference tokens in corpora but also to maximize task completion capability in actual multiturn dialogues (Kwan et al. 2023). Some studies (Liu et al. 2018; Tseng et al. 2021) employed online RL frameworks to train dialogue systems based on experiences obtained online from interactions with users. For example, research has focused on optimizing DST (Chen et al. 2022) or policy (Li et al. 2020; Deng et al. 2024) within pipeline systems. Additionally, Zhao and Eskenazi (2016) demonstrated that jointly optimizing DST and policy with shared parameters outperforms systems in which the DST and policy are trained separately. Our proposed method also utilizes an online RL framework to optimize dialogue systems. However, contrary to previous studies that focused on learning the modules, we concentrate on learning to modify the outputs of these modules.

Post-Processing Networks Methods that train modules via RL cannot be applied to dialogue systems with nontrainable modules, such as rule-based or API-based modules, as expected in real-world scenarios. To address this issue, Ohashi and Higashinaka (2022) proposed optimizing BinPPNs instead of the modules. BinPPNs modify the outputs of NLU, DST, and policy, and are optimized using online RL. Specifically, BinPPNs perform post-processing on the set of slot-value pairs output by each module through binary classification to determine whether to delete (0) or maintain (1) each pair. To handle post-processing for NLG, which outputs natural language rather than a set of slot-value pairs, Ohashi and Higashinaka (2023) introduced GenPPN, which uses LLMs to paraphrase the system utterance output by NLG, to improve task completion by generating system utterances that can be easily understood by users. It is optimized through RL based on feedback regarding whether DAs are correctly conveyed to users.

Our proposed UniPPN can process arbitrary sequences as both input and output. Therefore, contrary to BinPPN, which is limited to binary decisions of deletes/maintenance, it allows for more flexible post-processing. In addition, contrary to GenPPN, which is optimized using detailed feedback on DAs, UniPPN is optimized solely using the task success/failure signal obtained at the end of the dialogue. This makes it applicable to a wide range of systems, including word-level policies and end-to-end systems.

# 3 Preliminary

The problem of learning capabilities for multi-turn taskoriented dialogue is often formulated as an MDP and optimized through RL. An MDP is defined by tuple $( \mathbf { \dot { \mathcal { S } } } , \mathbf { \mathcal { A } } , \mathcal { P } , R , \gamma )$ . Essentially, $s$ and $\mathcal { A }$ represent all possible dialogue histories and system response sentences, respectively. $\mathcal { P } ( \boldsymbol { s } ^ { \prime } | \boldsymbol { s } , a )$ represents the transition model, $s \times \mathcal { A } \times$ $S  [ 0 , \dot { 1 } ]$ defines the dialogue environment containing the user, and $\textstyle { \dot { R } } ( s , a )$ represents the immediate reward function $s \times \mathcal { A } \to \mathbb { R }$ . $\gamma$ denotes the discount factor. At each turn $t$ , the policy $\mathcal { F } : \dot { S }  \mathcal { A }$ (i.e., the dialogue system) samples an action (i.e., the system response) $\boldsymbol { a } _ { t } \breve { \sim \mathcal { F } ( \dot { \boldsymbol { a } } _ { t } | \boldsymbol { s } _ { t } ) }$ . Until the final state at turn $T$ is reached, the next state $s _ { t } ^ { \prime } \sim P ( s _ { t } ^ { \prime } | s _ { t } , a _ { t } )$ and the immediate reward $r _ { t } = R ( s _ { t } , a _ { t } )$ are obtained. The goal of RL is to train $\mathcal { F }$ to maximize the value function $V$ , which is the expected cumulative discounted reward as follows.

$$
V ^ { \mathcal { F } } ( s ) : = \mathbb { E } \left[ \sum _ { t = 0 } ^ { T } \gamma ^ { t } r _ { t } | s _ { 0 } = s \right]
$$

Numerous studies targeted only a part of $\mathcal { F }$ , such as the policy module, rather than the entire $\mathcal { F }$ .

In complex problems, such as task-oriented dialogues, directly obtaining a policy that maximizes Eq. (1) is challenging. One effective method is the policy-gradient-based approach (Sutton et al. 1999), which directly improves the policy network $\mathcal { F } _ { \theta }$ parameterized by $\theta$ . According to the policy gradient theorem, gradient $\nabla _ { \mathcal { F } } \dot { J } ( \boldsymbol { \theta } )$ is expressed as follows:

![](images/d6bf1377d9073ff9514d9cf9995140551ffa7e083e93afaca48f44bb817d30ce.jpg)  
Step 1. Generate dialogues

![](images/e3a2c62a6d3ea44b210f82e18c42ebe9215ac6fb9b3a693398e2ca9643da8078.jpg)  
Step 2. Create pseudo-post-processing demonstrations containing pairs of positive and negative outputs   
Figure 2: Procedure for creating pseudo-post-processing demonstration data. First, we generate dialogues between the dialogue system and the user simulator. Subsequently, we create pairs of positive and negative outputs, where the output $\mathrm { o u t } _ { t }$ of module $m$ for context $s _ { t }$ at turn $t$ is positive and the output $\mathrm { o u t } _ { u }$ at another turn $u$ is negative (i.e., $\mathrm { o u t } _ { t } ^ { - }$ ). In imitation learning stage, the reconstruction from $\mathrm { o u t } _ { t } ^ { - }$ to $\mathrm { o u t } _ { t }$ is learned as pseudo-post-processing.

$$
\nabla _ { \mathcal { F } } J ( \theta ) = \mathbb { E } \left[ \sum _ { t = 0 } ^ { T } \Psi _ { t } \nabla _ { \theta } \log \mathcal { F } _ { \theta } ( a _ { t } \vert s _ { t } ) \right]
$$

The specific definition of $\Psi _ { t }$ varies depending on the implementation of the RL algorithm, such as the sum of the rewards obtained over all turns or advantage estimates (Schulman et al. 2015).

# 4 Proposed Method

In this section, we explain the problem formulation of our study, the proposed UniPPN, imitation learning (IL) and RL, which together constitute our optimization procedure for UniPPN.

# Problem Formulation

Here, we formulate the optimization problem for the dialogue system $\mathcal { F }$ through post-processing. We assume that $\mathcal { F }$ has a modularity consisting of $M$ modules: Module $^ { 1 }$ , ..., Module $M$ . At each turn $t$ , each module Module $\mathbf { \Sigma } _ { m }$ takes the output $\mathrm { o u t } _ { ( t , m - 1 ) }$ of the previous $\mathbf { M o d u l e } _ { m - 1 }$ as its input $\mathrm { i n } _ { ( t , m ) }$ , outputting its processing result $\mathrm { \ o u t } _ { ( t , m ) } \sim$ $\mathbf { M o d u l e } _ { m } \bigl ( \mathrm { i n } _ { ( t , m ) } \bigr )$ . Some modules may use the dialogue history $s _ { t }$ as additional input. The post-processing network $\mathrm { P P N } _ { m }$ for $\mathbf { M o d u l e } _ { m }$ modifies $\mathrm { o u t } _ { ( t , m ) }$ and the modified $\mathbf { o u t } _ { ( t , m ) } ^ { + } \sim \mathrm { P P N } _ { m } \big ( s _ { t } , \mathrm { i n } _ { ( t , m ) } , \mathbf { o u t } _ { ( t , m ) } \big )$ becomes the input for Module $m { + } 1$ . For the optimization of , we train instead of Module $\mathbf { \Sigma } _ { m }$ .

# UniPPN

In our proposed method, the post-processing of the outputs from all $M$ modules is performed by a single network, UniPPN $\pi$ (Figure 1). Specifically, UniPPN modifies the output of any Module $m$ : $\begin{array} { r } { \operatorname { o u t } _ { ( t , m ) } ^ { + } \sim \operatorname { U n i P P N } ( s _ { t } , \operatorname { i n } _ { ( t , m ) } . } \end{array}$ $\mathrm { o u t } _ { ( t , m ) }$ , pref $\mathbf { \boldsymbol { x } } _ { m }$ ). Here, prefi $\zeta _ { m }$ is an indicator that specifies that the module to be post-processed is Module $\mathrm { \Sigma } _ { m }$ . The input and output formats of UniPPN are text sequences, with post-processing executed as a sequence-transformation task. For the tokenized sequences $\pmb { x } \overset { \cdot } { = } \left( x _ { 1 } , . . . , x _ { k } \right)$ and $y =$ $( y _ { 1 } , . . . , y _ { l } )$ , representing $\left( s _ { t } , \operatorname { i n } _ { ( t , m ) } , \operatorname { o u t } _ { ( t , m ) } , \operatorname { p r e f i x } _ { m } \right)$ and t(+t,m) respectively, the following conditional probability is modeled:

$$
\pi _ { \boldsymbol { \theta } } ( \pmb { y } | \pmb { x } ) = \prod _ { i = 1 } ^ { l } \pi _ { \boldsymbol { \theta } } ( y _ { i } | \pmb { x } , \pmb { y } _ { < i } )
$$

where $\pi _ { \boldsymbol { \theta } }$ represents a pre-trained language model parameterized by $\theta$ . By treating not only the post-processing of natural language, such as the output of NLG modules but also structural data, such as the output of NLU or DST as a sequence-transformation task (Raffel et al. 2020; Liang et al. 2020), UniPPN can uniformly perform post-processing across all modules.

# Imitation Learning of Post-Processing

Pre-trained language models are typically trained on web text and may not sufficiently possess the ability to modify the outputs of modules in task-oriented dialogue systems. Therefore, we conduct additional pre-training to teach the model $\pi _ { \boldsymbol { \theta } }$ the formats of input $\left( s _ { t } , \operatorname { i n } _ { ( t , m ) } , \operatorname { o u t } _ { ( t , m ) } , \operatorname { p r e f i x } _ { m } \right)$ and output ut(+t,m) through supervised fine-tuning. In the general RL paradigm, $\mathrm { I L }$ conducted before online RL uses demonstration data, which consist of the action history of experts, such as humans. However, in our problem setting, demonstration data for post-processing the outputs of each module in the dialogue system $\mathcal { F }$ do not exist. Therefore, we automatically generate post-processing demonstration data and use them for supervised fine-tuning.

Using the procedure shown in Figure 2, we create pseudopost-processing demonstration data for each $\mathbf { M o d u l e } _ { m }$ . This process involves sampling dialogues by repeating interactions between $\mathcal { F }$ and the environment $\mathcal { P }$ to generate the input-output history $h _ { ( t , m ) } = \left( s _ { t } , \mathrm { i n } _ { ( t , m ) } , \mathrm { o u t } _ { ( t , m ) } \right)$ for each $\mathbf { M o d u l e } _ { m }$ at each turn $t$ , resulting in $H _ { m } = \{ h _ { ( 1 , m ) } \}$ , ..., $h _ { \left( \mid H _ { m } \mid , m \right) } \}$ . We now demonstrate the modification of $\mathrm { o u t } _ { ( t , m ) }$ . Here, the label $\mathrm { o u t } _ { ( t , m ) } ^ { + }$ , which represents the correct modification of $\mathrm { o u t } _ { ( t , m ) }$ , cannot be created automatically. Under the assumption that the output $\mathrm { o u t } _ { ( t , m ) }$ of $\mathbf { M o d u l e } _ { m }$ is reasonably valid, we consider $\mathrm { o u t } _ { ( t , m ) }$ to be the target output after post-processing; we use $\mathrm { o u t } _ { ( t , m ) } ^ { - }$ , randomly sampled from another turn $u$ (which may be from the same or a different dialogue) as the negative output that should be post-processed. This creates one demonstration instance $d _ { ( t , m ) } = \{ ( s _ { t } , \operatorname { i n } _ { ( t , m ) }$ , $\mathrm { o u t } _ { ( t , m ) } ^ { - }$ , prefixm), $\mathrm { o u t } _ { ( t , m ) } \}$ , representing the modification from $\mathrm { o u t } _ { ( t , m ) } ^ { - }$ to $\operatorname { o u t } _ { ( t , m ) } .$ We applied this pseudo-data creation process to all samples in $H _ { m }$ , resulting in the final demonstration dataset $D _ { m } = \{ d _ { ( 1 , m ) } , . . . , d _ { ( | H _ { m } | , m ) } \}$ . In the following section, we describe the two techniques used to create $D _ { m }$ .

Sampling Realistic out− If we sample a turn $u$ that is completely irrelevant to the context of $t$ , it could introduce noise, causing $\pi$ to potentially learn to ignore $\mathrm { o u t } _ { ( t , m ) } ^ { - }$ rather than to modify it appropriately. To ensure that the mistakes are reasonable, we sample turns with contexts similar to $t$ . Specifically, from the entire history excluding $h _ { ( t , m ) }$ (i.e., $H _ { m } \backslash \{ h _ { ( t , m ) } \} )$ , we extract the top few turns with the highest cosine similarity to the vector representation of the context $s _ { t }$ in $h _ { ( t , m ) }$ , and randomly sample $h _ { ( u , m ) }$ from the extracted turns. We use a general-purpose embedding model, such as E5 (Wang et al. 2022) to vectorize the context.

Learning to Copy In post-processing, it is not always necessary to modify the outputs; outputs without issues should be “copied” without modification. To reflect this, during the $\mathrm { I L }$ phase, we input the original $\mathrm { o u t } _ { ( t , m ) }$ into $\pi$ to ensure that modifications are not always required. In these cases, the target output is only the special token “copy”. Specifically, demonstrations of such cases are $d _ { ( t , m ) } = \{ ( s _ { t } ,$ $\mathrm { i n } _ { ( t , m ) }$ , $\mathbf { o u t } _ { ( t , m ) }$ , $\mathrm { p r e f i x } _ { m } .$ ), $\boldsymbol { \mathrm { c o p y } } \}$ . This approach allows the model to explicitly learn whether post-processing is necessary, while also reducing the generation costs when postprocessing is unnecessary. Whether each instance becomes a copy instance is determined randomly using copy ratio $\alpha \in [ 0 , 1 ]$ , which is a hyperparameter.

We update $\theta$ based on the maximum likelihood objective using the final dataset $D _ { 1 : M } = [ D _ { 1 } ; . . . ; D _ { M } ]$ , which combines the pseudo-post-processing data for all $M$ modules. The optimized parameters in this $\mathrm { I L }$ step are denoted by $\phi$ .

# Optimization with Reinforcement Learning

In the RL phase, we install the UniPPN $\pi _ { \phi }$ obtained from the IL step into the dialogue system $\mathcal { F }$ . Subsequently, let $\mathcal { F }$ interact repeatedly with the environment $\mathcal { P }$ over multiple turns and update $\phi$ based on these experiences using a policy-gradient-based approach. In typical task-oriented dialogue systems using online RL, only a single policy network (e.g., a policy module) operates per turn, and it is updated according to Eq. (2). In contrast, our study involves a policy $\pi$ that acts $M$ times per turn, and outputs the system response as action $a$ . Although each of the $M$ actions should have different gradients based on their individual advantages, Eq. (2) treats them as having the same contributions. This can result in coarse rewards and learning instability.

Therefore, we extend the standard MDP described in Section 3 and introduce a module-level MDP, where the unit of time step is the “post-processing of one module by $\pi$ ” rather than the “one turn response by $\mathcal { F } ^ { \prime \prime }$ . Specifically, the value function to be maximized and policy gradient of $\pi _ { \phi }$ are as follows:

$$
V ^ { \pi } ( \pmb { x } ) : = \mathbb { E } \left[ \sum _ { t = 0 } ^ { T } \sum _ { m = 1 } ^ { M } \gamma ^ { ( t + 1 ) ( m - 1 ) } r _ { ( t , m ) } | \pmb { x } _ { ( 0 , 1 ) } = \pmb { x } \right]
$$

$$
\nabla _ { \pi } J ( \phi ) = \mathbb { E } \left[ \sum _ { t = 0 } ^ { T } \sum _ { m = 1 } ^ { M } \Psi _ { ( t , m ) } \nabla _ { \phi } \log \pi _ { \phi } ( \pmb { y } _ { ( t , m ) } | \pmb { x } _ { ( t , m ) } ) \right]
$$

Here, $r _ { ( t , m ) }$ represents the immediate reward for postprocessing the output of $\mathbf { M o d u l e } _ { m }$ at turn $t$ . As in previous studies using online RL (Hou et al. 2021), a small negative fixed value is assigned continuously until the end of the dialogue. $\pmb { x } _ { ( t , m ) }$ and $\pmb { y } _ { ( t , m ) }$ are the tokenized sequences of the input text $( s _ { t } , \mathrm { i n } _ { ( t , m ) }$ , $\mathrm { o u t } _ { ( t , m ) }$ , prefixm) and output text of UniPPN, respectively. Eq. (5) shows that the gradients can be computed in $M$ gradient accumulation steps. Note that, in Eq. (4), the number of calculations for the value function in the module-level MDP is $T \times M$ , resulting in a possible exponential increase in the computational cost. However, because $M$ in a typical task-oriented dialogue system is four at most, this is not a problem in practice.

To implement $\Psi _ { ( t , m ) }$ , we adopt a generalized advantage estimation (Schulman et al. 2015). Specifically, we compute the advantage estimate $\hat { A } _ { ( t , m ) }$ based on the value $V _ { \psi } ( \pmb { x } _ { ( t , m ) } )$ of $\pmb { x } _ { ( t , m ) }$ estimated using another language model $V _ { \psi }$ parameterized by $\psi$ as a critic network:

$$
\begin{array} { r l } & { \hat { \boldsymbol { A } } _ { ( t , m ) } = \delta _ { ( t , m ) } + \gamma \lambda \hat { \boldsymbol { A } } _ { ( t , m ) ^ { \prime } } , } \\ & { \delta _ { ( t , m ) } = \boldsymbol { r } _ { ( t , m ) } + \gamma V _ { \psi } ( \pmb { x } _ { ( t , m ) ^ { \prime } } ) - V _ { \psi } \big ( \pmb { x } _ { ( t , m ) } \big ) } \end{array}
$$

$$
( t , m ) ^ { \prime } = { \left\{ \begin{array} { l l } { ( t , m + 1 ) } & { { \mathrm { i f } } m < M } \\ { ( t + 1 , 1 ) } & { { \mathrm { i f } } m = M } \end{array} \right. }
$$

Here, $\delta _ { ( t , m ) }$ represents the TD residual, and the hyperparameter $\lambda \in [ 0 , 1 ]$ controls the trade-off between utilizing actual long-term rewards and the estimated values. Because $V _ { \psi }$ estimates the state value for each Module $m$ at each turn $t$ , fine-grained advantage estimation according to the contribution of each module is possible even in settings with sparse rewards across multi-turn dialogues. An advantage of this algorithm is that it does not require a high-cost manual reward design for each module, as required in previous studies. $V _ { \psi }$ is trained to minimize the mean squared error with respect to the cumulative reward and $\pi _ { \phi }$ is optimized using a clipped surrogate objective with proximal policy optimization (PPO) (Schulman et al. 2017). For a detailed implementation of the RL algorithm, refer to Appendix A.

# 5 Experiments

In this evaluation experiment, we demonstrate that joint optimization using UniPPN is more effective than disjoint optimization combining conventional BinPPN and GenPPN for post-processing outputs from all modules to improve taskoriented dialogue systems.

# Experimental Setup

We conducted evaluation experiments using the MultiWOZ (Budzianowski et al. 2018) dataset, which contains a multi-domain task-oriented dialogue on travel information between customers and clerks. We applied UniPPN to various dialogue systems developed for MultiWOZ and assessed their task completion capabilities. For the user simulation, we used the agenda-based user simulator (Schatzmann et al. 2007) provided by ConvLab-2 (Zhu et al. 2020), which is an evaluation toolkit for task-oriented dialogue systems.

The dialogue systems used in our experiments included both pipeline and end-to-end systems. For the modules constituting the pipeline systems, we selected relatively recently proposed models that ranked high on the MultiWOZ benchmark1 and had publicly available implementations. The models adopted for each module in the pipeline system are as follows:

NLU BERT NLU (Chen, Zhuo, and Wang 2019), a classification model based on BERT (Devlin et al. 2019).   
DST Rule-based DST and D3ST (Zhao et al. 2022), a stateof-the-art word-level DST based on T5 (Raffel et al. 2020).   
Policy Rule-based policy and PPO policy fine-tuned using PPO (Schulman et al. 2017). We also used LAVA (Lubis et al. 2020), a word-level policy.   
NLG Template-based NLG and SC-GPT (Peng et al. 2020), which is based on GPT-2 (Radford et al. 2019).

For end-to-end systems, we adopted two representative models: PPTOD (Su et al. 2022) and an LLM-based model (Hudecˇek and Dusek 2023). PPTOD is a T5-based dialogue model that is fine-tuned using MultiWOZ. The LLMbased model performs word-level DST and a word-level policy based on in-context learning with few-shot examples retrieved from MultiWOZ. For the LLM, we used GPT-4o mini provided by OpenAI’s API.2

# Evaluation Metrics

In the evaluation, each dialogue system interacted with the user simulator 1,024 times, and each of the 1,024 different user goals for testing was set in each dialogue. We reported the average score of 1,024 dialogues as the final score.

As evaluation metrics, we used the average turns across all dialogues, which represents the number of turns required to achieve a task, with lower values indicating better efficiency. Each turn comprised a pair of a user utterance and the corresponding system response. We also used Inform Recall/Precision/F1. These metrics assess whether the system responds adequately to the information requested by the user during a dialogue. In addition, we used the Goal Match Rate to assess whether the conditions of the entity (specific facilities, such as hotels) presented by the system matched the user’s goal. Similarly, we also assessed the conditions of the entity booked by the system using the Book Rate. We set the maximum number of turns in one dialogue to 20. A task was only considered Success if the Inform Recall, Match Rate, and Book Rate all reached 1.0 within 20 turns.

# UniPPN Implementation

We used a medium-sized GPT-2 (Radford et al. 2019) with 355M parameters as the backbone model for UniPPN. We chose this parameter size because of its superior balance between the computational cost and performance, which was confirmed through preliminary experiments.

Imitation Learning To construct the post-processing demonstration data $D _ { 1 : M }$ for each dialogue system, we sampled 10,000 turns of interaction between the dialogue system and user simulator.3 To sample turns with similar contexts in the construction of $D _ { 1 : M }$ , we adopted GTE-base (Li et al. 2023) as the embedding model and used the latest three utterances as the context. In addition, we set the copy ratio $\alpha$ to 0.1 throughout the experiment because, in preliminary experiments, we examined 9 levels from 0.1 to 0.9 for $\alpha$ and found that 0.1 yielded the highest reward.

Reinforcement Learning As an approximator for the value function, we used GPT-2 of 124M parameters, with an additional feedforward network outputting a scalar value. For the reward function $R ( t , m )$ , we set a small negative value of $R ( t , m ) = - 0 . 1$ until the end of the dialogue and $R ( T , M ) = 2 \times M$ for the final step if the task was achieved. We trained for 200 iterations, and in each iteration, we sampled 1,024 turns as training data. We used the checkpoints from the final iteration for testing.

For detailed implementations and hyperparameters of the learning process, please refer to Appendix B.

# Baselines

As baselines for this experiment, we used two methods: the original dialogue system without post-processing and a method that post-processes the outputs of all modules by combining conventional BinPPN and GenPPN (called BinPPN&GenPPN). Because BinPPN and GenPPN cannot be trained jointly, RL was used to train the two types of PPNs (BinPPN and GenPPN) separately. Specifically, we used RL to optimize the post-processing of the three modules (NLU, DST, and policy) using BinPPN. Thereafter, while installing these three BinPPNs in the system and freezing their parameters, we attached GenPPN to NLG and trained it again using RL. The BinPPN was optimized first because NLU, DST, and policy, whose output BinPPN post-processes, precede NLG, whose output GenPPN post-processes; therefore, this order of optimization is considered appropriate. Note that, contrary to UniPPN, BinPPN&GenPPN require two RL phases. For the implementation and hyperparameters of BinPPN and GenPPN, we used those published and reported in previous studies (Ohashi and Higashinaka 2022, 2023). However, for the backbone model of GenPPN, we used Llama 3.1

Table 1: Test results for each dialogue system and when post-processing is applied to all of their modules using BinPPN and GenPPN (+BinPPN&GenPPN) or using UniPPN $( + \mathrm { U n i P P N } )$ . The ✓under each module in the row indicates whether PPN is applied to that module. Note that UniPPNs applied to the same system are the same network. The superscript “word” for DST and policy indicates that they are word-level DST and word-level policy, respectively.   

<html><body><table><tr><td rowspan="2">System</td><td colspan="4">Module combination</td><td>Success</td><td colspan="3">Inform</td><td>Book</td><td rowspan="2">Match</td><td rowspan="2">Turns↓</td></tr><tr><td>NLU</td><td>DST</td><td>Policy</td><td>NLG</td><td>Rate</td><td>Recall</td><td>Prec.</td><td>F1 Rate</td><td>Rate</td></tr><tr><td>Pipeline System</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>SYSRULE</td><td>BERT</td><td>Rule</td><td>Rule</td><td>Template</td><td>83.69</td><td>93.72</td><td>81.38</td><td>85.08</td><td>91.19</td><td>91.63</td><td>5.92</td></tr><tr><td>+BinPPN&GenPPN</td><td>√</td><td>√</td><td>√</td><td>√</td><td>85.25</td><td>93.96</td><td>83.53</td><td>86.54</td><td>91.18</td><td>92.68</td><td>5.67</td></tr><tr><td>+UniPPN</td><td>√</td><td>√</td><td>√</td><td>√</td><td>90.62</td><td>96.46</td><td>82.86</td><td>87.19</td><td>97.13</td><td>94.81</td><td>5.77</td></tr><tr><td>SYSD3ST</td><td>1</td><td>D3STword</td><td>Rule</td><td>Template</td><td>58.50</td><td>72.41</td><td>67.36</td><td>66.62</td><td>62.22</td><td>77.12</td><td>6.78</td></tr><tr><td>+BinPPN&GenPPN</td><td></td><td>√</td><td>√</td><td>√</td><td>67.58</td><td>82.24</td><td>68.02</td><td>71.89</td><td>75.31</td><td>81.59</td><td>6.36</td></tr><tr><td>+UniPPN</td><td>1</td><td>√</td><td>√</td><td>√</td><td>85.06</td><td>93.63</td><td>71.82</td><td>78.56</td><td>89.25</td><td>92.15</td><td>5.78</td></tr><tr><td>SYSPPO</td><td>BERT</td><td>Rule</td><td>PPO</td><td>Template</td><td>69.24</td><td>86.90</td><td>66.99</td><td>72.90</td><td>80.74</td><td>83.79</td><td>8.55</td></tr><tr><td>+BinPPN&GenPPN</td><td>√</td><td>√</td><td>√</td><td>√</td><td>70.61</td><td>86.59</td><td>66.08</td><td>72.46</td><td>80.71</td><td>84.42</td><td>8.46</td></tr><tr><td>+UniPPN</td><td>√</td><td>√</td><td>√</td><td>√</td><td>89.36</td><td>96.72</td><td>68.37</td><td>77.68</td><td>94.73</td><td>94.82</td><td>5.24</td></tr><tr><td>SYSsGPT</td><td>BERT</td><td>Rule</td><td>Rule</td><td>SC-GPT</td><td>75.29</td><td>93.73</td><td>79.49</td><td>83.92</td><td>68.53</td><td>92.06</td><td>6.12</td></tr><tr><td>+BinPPN&GenPPN</td><td>√</td><td>√</td><td>√</td><td>√</td><td>86.72</td><td>94.66</td><td>82.79</td><td>86.32</td><td>92.53</td><td>92.84</td><td>5.68</td></tr><tr><td>+UniPPN</td><td>√</td><td>√</td><td>√</td><td>√</td><td>90.14</td><td>97.19</td><td>84.14</td><td>88.38</td><td>95.89</td><td>95.69</td><td>6.10</td></tr><tr><td>SYSLAVA</td><td>BERT</td><td>Rule</td><td>LAVA word</td><td>1</td><td>64.36</td><td>82.87</td><td>55.74</td><td>64.04</td><td>72.96</td><td>80.34</td><td>10.24</td></tr><tr><td>+UniPPN</td><td>√</td><td>√</td><td>√</td><td>1</td><td>79.39</td><td>98.10</td><td>64.41</td><td>75.21</td><td>88.66</td><td>89.91</td><td>5.84</td></tr><tr><td>End-to-End System</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>SYSPPTOD</td><td>1</td><td>PPTOD Word</td><td>PPTODWord</td><td></td><td>61.04</td><td>82.19</td><td>75.12</td><td>76.07</td><td>50.80</td><td>83.61</td><td>8.15</td></tr><tr><td>+UniPPN</td><td></td><td>√</td><td>√</td><td>一</td><td>80.37</td><td>92.71</td><td>75.83</td><td>81.00</td><td>83.08</td><td>90.01</td><td>6.97</td></tr><tr><td>SYSLLM (GPT-4o mini)</td><td></td><td>LLMword</td><td>LLMword</td><td>1</td><td>62.30</td><td>77.66</td><td>58.64</td><td>62.85</td><td>62.37</td><td>79.26</td><td>7.66</td></tr><tr><td>+UniPPN</td><td></td><td>√</td><td>√</td><td>1</td><td>88.28</td><td>94.80</td><td>65.01</td><td>74.31</td><td>90.81</td><td>93.57</td><td>4.66</td></tr></table></body></html>

8B (Dubey et al. 2024) instead of Llama 7B (Touvron et al.   
2023) as described in the previous study.

# Main Results

Table 1 shows the test results when post-processing was applied to all modules of each system. We evaluated two cases for the application of post-processing to pipeline systems consisting of multiple modules: $^ +$ BinPPN&GenPPN and +UniPPN. Because BinPPN&GenPPN cannot be applied to ${ \mathrm { S Y S } } _ { \mathrm { L A V A } }$ or end-to-end systems that do not output DAs, only UniPPN was applied to these systems.

In the pipeline systems, $+ \mathrm { U n i P P N }$ significantly outperformed +BinPPN&GenPPN. For example, even for systems, such as ${ \mathrm { S Y S } } _ { \mathrm { R U L E } }$ and $\mathrm { \Delta S Y S _ { P P O } }$ , where performance improvement by +BinPPN&GenPPN was limited, UniPPN improved the performance. In particular, it is noteworthy that UniPPN enhanced ${ \mathrm { S Y S } } _ { \mathrm { R U L E } }$ to 90.62 points, considering that ${ \mathrm { S Y S } } _ { \mathrm { R U L E } }$ comprises high-performance modules that are carefully crafted by hand-written rules, and its original success rate is high at approximately 84 points. Furthermore, we applied UniPPN to the system, including the word-level policy and end-to-end systems, to which conventional BinPPN or GenPPN could not be applied. UniPPN significantly improved the original performance of ${ \mathrm { S Y S } } _ { \mathrm { L A V A } }$ , SYSPPTOD, and $\mathrm { S Y S } _ { \mathrm { L L M } }$ for all evaluation metrics.

From these results, we demonstrated that the joint optimization of post-processing the outputs from all modules using UniPPN is more effective than the disjoint optimization of multiple PPNs. Furthermore, its efficacy was demonstrated, and it was shown that it can be applied to any dialogue system, regardless of the trainability of each module, including rule- or API-based modules. We believe that UniPPN is more practical considering the high training cost of BinPPN&GenPPN and the complexity of installing multiple networks in the systems.

# Comparison with either BinPPN or GenPPN

To clarify the fundamental performance difference between UniPPN and BinPPN, we compared the performance when either BinPPN or UniPPN was applied to NLU, DST, and policy, which BinPPN can be applied, for each system. Table 2 presents the results. We can observe that for most systems, the performance improvement by UniPPN exceeds that of BinPPN. This performance difference may be due to limited post-processing capabilities of BinPPN, which is limited to basic binary operations, leaving little room for improvement, whereas UniPPN can flexibly generate various types of information. Similarly, we compared the performance when either GenPPN or UniPPN was applied to the NLG of each system. Table 3 shows the results. For the turn metric, GenPPN consistently outperformed UniPPN. This could be because GenPPN learns to generate responses, such that DAs are easily understood by the user simulator, thereby reducing the increase in turns caused by users asking back.

Table 2: Test results when applying either BinPPN or UniPPN to the three modules of NLU, DST, and policy in each system. For ${ \mathrm { S Y S } } _ { \mathrm { L A V A } }$ , which uses word-level policy, BinPPN cannot be applied, therefore, either BinPPN or UniPPN is applied only to NLU and DST.   

<html><body><table><tr><td>System</td><td>PPN</td><td>Success</td><td>Inf.F1</td><td>Book</td><td>Match</td><td>Turns ↓</td></tr><tr><td>SYSRULE</td><td>Bin</td><td>83.40</td><td>86.07</td><td>91.13</td><td>91.47</td><td>6.04</td></tr><tr><td></td><td>Uni</td><td>87.11</td><td>83.81</td><td>96.08</td><td>93.13</td><td>5.80</td></tr><tr><td>SYSD3ST</td><td>Bin</td><td>62.89</td><td>70.78</td><td>70.71</td><td>79.02</td><td>7.21</td></tr><tr><td></td><td>Uni</td><td>77.93</td><td>76.97</td><td>79.50</td><td>88.66</td><td>5.83</td></tr><tr><td>SYSPPO</td><td>Bin</td><td>69.82</td><td>72.80</td><td>80.78</td><td>83.64</td><td>8.63</td></tr><tr><td></td><td>Uni</td><td>85.84</td><td>76.37</td><td>94.05</td><td>92.50</td><td>5.40</td></tr><tr><td>SYSscGPT</td><td>Bin</td><td>73.44</td><td>83.60</td><td>63.06</td><td>91.81</td><td>6.08</td></tr><tr><td></td><td>Uni</td><td>77.44</td><td>85.06</td><td>69.34</td><td>93.62</td><td>6.10</td></tr><tr><td>SYSLAVA</td><td>Bin</td><td>64.06</td><td>69.56</td><td>80.99</td><td>79.98</td><td>8.72</td></tr><tr><td></td><td>Uni</td><td>65.72</td><td>67.55</td><td>76.48</td><td>80.83</td><td>9.45</td></tr></table></body></html>

Table 3: Test results when applying either GenPPN or UniPPN only to the NLG of each system.   

<html><body><table><tr><td>System</td><td>PPN</td><td>Success</td><td>Inf.F1</td><td>Book</td><td>Match</td><td>Turns↓</td></tr><tr><td>SYSRULE</td><td>Gen</td><td>85.06</td><td>85.08</td><td>90.90</td><td>92.35</td><td>5.61</td></tr><tr><td></td><td>Uni</td><td>85.06</td><td>85.08</td><td>91.58</td><td>92.29</td><td>5.81</td></tr><tr><td>SYSSCGPT</td><td>Gen</td><td>84.08</td><td>85.40</td><td>91.62</td><td>91.65</td><td>5.67</td></tr><tr><td></td><td>Uni</td><td>84.77</td><td>81.66</td><td>90.50</td><td>92.56</td><td>6.15</td></tr></table></body></html>

However, for other metrics, such as success rate, there was no significant difference between the two methods. Considering that the training algorithm of GenPPN requires the internal DAs of the system and feedback on whether the user understood those DAs, UniPPN, which only requires the final dialogue outcome as a reward, is promising.

Based on these results, we demonstrated that UniPPN addresses the multiple challenges of conventional BinPPN and GenPPN, resulting in improved task completion capability, reduced computational cost, and broader applicability.

# 6 Human Evaluation

We verified whether the performance improvement of dialogue systems by UniPPN is also effective for human users. Specifically, for $\mathrm { \Delta S Y S _ { P P O } }$ in Table 1, we had human users interact with three types of systems (i.e., the original system, +BinPPN&GenPPN, and +UniPPN) and evaluated their performance. We chose $\mathrm { \Delta { S Y S } _ { \mathrm { P P O } } }$ because it achieved the lowest task success rate among the systems containing all four modules. We believe that the impact of post-processing on the performance is most apparent in this case. We recruited more than 40 workers for each system on Amazon Mechanical Turk (AMT) and had them interact with one of the three systems to achieve dialogue goals randomly created for each dialogue. After the dialogue, the workers were requested to subjectively evaluate the system’s language understanding (LU), response appropriateness (RA), and overall satisfaction (OS) with the dialogue on a 5-point Likert scale. Ethical approval was obtained from our institution before the experiment. For detailed experimental settings, please refer to Appendix C.

Table 4: Results of human evaluation. $N$ indicates the number of subjects who conversed with each system. LU, RA, and OS indicate evaluations of the system’s language understanding, response appropriateness, and overall satisfaction, respectively. $+$ indicates that there was a significant tendency with $p < 0 . 1$ according to the Mann-Whitney U test in the difference between the scores of +BinPPN&GenPPN and $+ \mathrm { U n i P P N }$ .   

<html><body><table><tr><td>System</td><td>N</td><td>Success</td><td>Turns↓</td><td>LU</td><td>RA</td><td>OS</td></tr><tr><td>SYSPPO</td><td>43</td><td>46.51</td><td>8.79</td><td>2.70</td><td>2.70</td><td>2.63</td></tr><tr><td>+BinPPN&GenPPN</td><td>40</td><td>42.50</td><td>9.80</td><td>2.70</td><td>2.85</td><td>2.38</td></tr><tr><td>+UniPPN</td><td>48</td><td>54.17</td><td>8.06+</td><td>2.81</td><td>2.90</td><td>2.56</td></tr></table></body></html>

Table 4 lists the results of the human evaluation metrics for the task completion and subjective assessments. For the evaluation metrics related to task completion capability, such as success rate and number of turns, $+ \mathrm { U n i P P N }$ outperformed both the original $\operatorname { s Y S } _ { \operatorname { P P O } }$ and +BinPPN&GenPPN. Notably, the difference in the number of turns showed a statistically significant tendency, highlighting the effectiveness of UniPPN, which can jointly learn to post-process the outputs of all modules, compared with disjointly trained conventional PPNs. By contrast, +BinPPN&GenPPN performed worse than the original $\operatorname { S Y S } _ { \operatorname { P P O } }$ for most metrics. This may be because the post-processing for each module is trained disjointly, preventing coordination between PPNs and deteriorating the overall system performance. Regarding the subjective evaluation metrics of LU, RA, and OS, there were no significant differences between $\operatorname { S Y S } _ { \operatorname { P P O } }$ and +UniPPN. This was expected, considering that UniPPN’s reward signals were only related to task completion and did not include signals related to dialogue satisfaction.

# 7 Conclusion

In this study, we proposed UniPPN, a method that jointly learns the post-processing of outputs from all modules in task-oriented dialogue systems. Using simulation experiments based on the MultiWOZ dataset, we applied UniPPN to various pipeline systems with recent high-performance modules and end-to-end systems, including a GPT-4o minipowered system. Our results confirm that UniPPN significantly outperforms conventional PPN-based methods in terms of task completion performance of dialogue systems. Furthermore, human evaluation experiments demonstrated that UniPPN, optimized in a simulation environment, is effective in real-world scenarios.

In future studies, we aim to reduce the overall training cost of UniPPN. This involves reducing the number of dialogue experiences required for learning convergence and optimizing the model size for efficiency. In addition, we plan to extend beyond text dialogue to support the post-processing of outputs from modules in multimodal dialogue systems.