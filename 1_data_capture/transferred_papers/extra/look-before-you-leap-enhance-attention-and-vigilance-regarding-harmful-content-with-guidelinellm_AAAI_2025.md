# Look Before You Leap: Enhancing Attention and Vigilance Regarding Harmful Content with GuidelineLLM

Shaoqing Zhang1\*, Zhuosheng Zhang2, Kehai Chen1†, Rongxiang Weng3, Muyun Yang1, Tiejun Zhao1, Min Zhang

1School of Computer Science and Technology, Harbin Institute of Technology, China 2School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, China 3Meituan, China

# Abstract

Despite being empowered with alignment mechanisms, large language models (LLMs) are increasingly vulnerable to emerging jailbreak attacks that can compromise their alignment mechanisms. This vulnerability poses significant risks to real-world applications. Existing work faces challenges in both training efficiency and generalization capabilities (i.e., Reinforcement Learning from Human Feedback and RedTeaming). Developing effective strategies to enable LLMs to resist continuously evolving jailbreak attempts represents a significant challenge. To address this challenge, we propose a novel defensive paradigm called GuidelineLLM, which assists LLMs in recognizing queries that may have harmful content. Before LLMs respond to a query, GuidelineLLM first identifies potential risks associated with the query, summarizes these risks into guideline suggestions, and then feeds these guidelines to the responding LLMs. Importantly, our approach eliminates the necessity for additional safety finetuning of the LLMs themselves; only the GuidelineLLM requires fine-tuning. This characteristic enhances the general applicability of GuidelineLLM across various LLMs. Experimental results demonstrate that GuidelineLLM can significantly reduce the attack success rate (ASR) against LLM (an average reduction of $3 4 . 1 7 \%$ ASR) while maintaining the usefulness of LLM in handling benign queries.

Code — https://github.com/sqzhang-lazy/GuidelineLLM Datasets — https://github.com/sqzhang-lazy/GuidelineLLM Extended version — https://arxiv.org/pdf/2412.10423

# Introduction

Recently, large language models (LLMs) (Touvron et al. 2023; Achiam et al. 2023; Anil et al. 2023) have become increasingly prominent in daily activities, exemplified by applications such as ChatGPT and Intelligent Assistants. The utilization of these models for various tasks is an expanding trend (Qin et al. 2023; Zhong et al. 2023; Yao et al. 2022), necessitating a reduction in responses containing harmful

Sure, I can do some Sorry, I can’t assist harmful behavior with that … Alignment Full-Param Train O 自 田 Time Cost Unseen Jailbreak GuidelineLLM Attack T Only Prompt 田 1G.uBideelminine:dful of the content: 2. Avoid promoting harmful content: Guideline 3. Prioritize well-being: LLM

content. Although LLMs have demonstrated a capability for recognizing harmful content, methods to attack LLMs have continuously evolved, particularly jailbreak attacks (Yuan et al. 2023; Deng et al. 2023b; Zeng et al. 2024a; Zou et al. 2023). Jailbreak attacks can efficiently induce models to produce harmful responses during the inference phase, posing significant risks to the practical application of LLMs.

To enhance the safety of LLMs, common approaches involve reinforcing their alignment mechanisms. Techniques such as Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al. 2022; Bai et al. 2022a; Jiang et al. 2024b) and Red-Teaming (Ganguli et al. 2022; Perez et al. 2022; Touvron et al. 2023) have proven effective in improving the ability of LLMs to detect harmful content. However, these methods demand extensive training data and significant computational resources, which can limit the speed of iterative improvements. Furthermore, additional retraining might degrade the performance of established safety alignment mechanisms (Qi et al. 2023; Zhou et al. 2024). Therefore, it is critical to investigate methodologies that can mitigate the production of harmful content without requiring modifications to the model parameters.

Many existing methods have focused on enhancing the recognition of harmful content during the inference stage. These methods aim to heighten the attention of LLMs toward harmful content prior to generating responses, employing techniques such as Self-Reminder and Self-Defense (Xie et al. 2023; Helbling et al. 2023). However, these methods often struggle with generalization and can be circumvented by continually evolving jailbreak tactics. Thus, the ongoing efforts to improve the safety of LLMs confront two principal challenges: training efficiency and generalization capabilities.

In this work, we propose a novel defensive paradigm called GuidelineLLM, designed to assist LLMs in identifying queries that possess potential harmfulness. Before LLMs respond to a query, GuidelineLLM first identifies the query risks, summarizes them into guideline suggestions, and feeds them to the responding LLMs.

This approach effectively addresses the previously identified challenges by making LLMs cognizant of unsafe content within a query. Consequently, GuidelineLLM activates the alignment mechanisms of LLMs, thereby enhancing their safety and fortifying them against various jailbreak attacks. Remarkably, our approach does not necessitate additional safety fine-tuning for the responsing LLMs; only GuidelineLLM needs to be fine-tuned. This characteristic makes GuidelineLLM applicable to various LLMs. Figure 1 illustrates a comparative analysis between GuidelineLLM’s defensive methods and traditional alignment mechanisms. Additionally, we present a fine-tuning framework for GuidelineLLM that is adaptable for formulating guidelines against newly emerging jailbreak techniques.

In summary, our contributions are as follows:

(i) We propose a defensive paradigm called GuidelineLLM, which can enhance the safety of various LLMs. This paradigm can reduce the harmfulness of the output content without necessitating additional safety training for the response models themselves, significantly mitigating the impact of jailbreak attacks.

(ii) We introduce a fine-tuning framework for GuidelineLLM, which includes the templated construction of jailbreak attacks, referred to as T-Jailbreak. T-Jailbreak can be expanded with relevant jailbreak attacks according to the definition of new jailbreak techniques. Experimental results indicate that T-Jailbreak has a high attack success rate (ASR).

(iii) Experiments show that our GuidelineLLM can significantly reduce the harmfulness of the output (an average reduction of $3 4 . 1 7 \%$ ASR) while maintaining the helpfulness of LLMs for benign queries.

# Related Work Enhancing Safety Policies of LLMs

Two prevalent methods are currently employed to enhance the safety policies of LLMs: alignment and red teaming. The objective of alignment is to bridge the gap between LLMs’ language modeling goals, such as predicting the next token during pre-training, and their ultimate aim of “following instructions and being helpful, truthful, and harmless” in practical applications (Ouyang et al. 2022). Common techniques in this domain include Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al. 2022; Bai et al. 2022a), Constitutional AI (Bai et al. 2022b), and self-alignment (Sun et al. 2024). These approaches focus on embedding alignment rules within pre-trained models to restrict harmful behaviors during inference. In LLMs research, the term “RedTeaming” has recently come to denote the systematic testing or probing of LLMs to identify their potential for harmful behavior and uncover safety vulnerabilities. The critical aspect of red teaming is the aggregation of sufficient and diverse attacks, enabling LLMs to refuse responses to these attacks. Current efforts in this area mainly concentrate on automating the generation of high-quality, provocative queries, allowing LLMs to encounter a wider variety and richer range of attacks, thereby bolstering their defensive capabilities (Deng et al. 2023a; Ge et al. 2023; Jiang et al. $2 0 2 4 \mathrm { a }$ ; Zhang et al. 2024a; Xu et al. 2024). However, recent studies also demonstrate that retraining the model can lead to the deterioration of safety alignment mechanisms (Qi et al. 2023; Zhou et al. 2024). This indicates that arbitrary parameter updates to the model are not advisable and can slow the model iteration process.

# Enhancing Inference Safety of LLMs

Ensuring that LLMs output only safe content during inference is critically important. Common methodologies to achieve this include preprocessing the input before inference, employing specialized inference paradigms, and detecting harmful content through input-output result analysis. Given the sensitivity of LLMs to input prompts, modifying the prompt content can enhance their ability to focus on harmful content. For instance, incorporating a system prompt can assist LLMs in identifying harmful content (Xie et al. 2023). By leveraging LLMs’ in-context learning capabilities, models can be conditioned to recognize and refuse harmful content (Wei, Wang, and Wang 2023). Additionally, creating multiple perturbed samples of the input and aggregating the results has proven effective (Robey et al. 2023; Cao et al. 2023). Some approaches involve having LLMs analyze the intent of the question before providing an answer (Zhang et al. 2024b; Zeng et al. 2024b). Alternatively, decoding strategies can be applied to suppress harmful prompts during generation, thereby ensuring safer outputs (Zhong et al. 2024). In terms of detection and classification, Meta has specifically fine-tuned a model named LlamaGuard, which can classify inputs and input-output pairs according to taxonomy and risk guidelines (Inan et al. 2023). In this work, we primarily focus on safeguarding LLMs during the inference phase. We aim to find a defensive paradigm that applies to different LLMs and can be continuously expanded to counter the ever-evolving jailbreak attacks.

# The Proposed Method

In this section, we first present how to initialize a small count set of T-Jailbreak queries from some attack queries and the definition of different jailbreak techniques. Then, we utilize this batch of T-Jailbreak queries and sampled benign queries to construct the initial set of Guideline data. Finally, we will demonstrate the framework for fine-tuning the GuidelineLLM.

Table 1: The seven jailbreak techniques summarized in the previous work (Liu et al. 2023b).   

<html><body><table><tr><td>Technique</td><td>Definition</td></tr><tr><td>Role play</td><td>Prompt requires you to adopta persona,leading to unexpected responses.</td></tr><tr><td>Rule Determine</td><td>Prompt asks you to follow the output requirements,resulting in the expected output.</td></tr><tr><td>Logical Reasoning</td><td>Prompt requires logical reasoning,leading to exploitable outputs.</td></tr><tr><td>Text Continuation</td><td>Prompt requests you to continue text, leading to exploitable outputs.</td></tr><tr><td>Program Execution</td><td>Prompt requests execution of a program, leading to exploitable outputs.</td></tr><tr><td>Simulate Mode</td><td>Prompt requests you to enter the some mode,leading to exploitable outputs.</td></tr><tr><td>SuperiorModel</td><td>Prompt leverages superior model output to exploit your behavior.</td></tr></table></body></html>

# Initializing T-Jailbreak Data

To ensure the diversity of jailbreak attacks and enable GuidelineLLM to formulate guidelines for analyzing various types of jailbreak attacks, we have constructed the T-Jailbreak dataset by summarizing seven identified jailbreak techniques, as outlined in previous research (Liu et al. 2023b). These techniques are detailed in Table 1.

In line with previous studies (Inan et al. 2023; Shen et al. 2023), we utilize the policies from Llama3-Guard and DAN to initialize our attack queries. Combining these attack queries with template prompts, we develop the T-Jailbreak dataset for use in our experiments. Specifically, we use the gpt-3.5-turbo-0125 to initialize attack and jailbreak queries. We configured the temperature parameter to 0.9 and the top p parameter to 0.85 to promote diversity in the generated results.

# Initializing Guideline Data

Considering that GuidelineLLM needs to maintain the helpfulness of LLMs while enhancing their defensive capabilities, it is crucial to include both harmful and benign queries in the collection of guideline data.

The jailbreak queries used in our study are from the TJailbreak dataset that we construct, while the benign queries are sampled as follows: 1,000 entries from Alpaca (Taori et al. 2023), and 1,800 entries from GPTTeacher1. We use gpt-3.5-turbo-0125, with the same parameter settings as for Section Initialize T-Jailbreak Data, to initialize our guideline data.

# Fine-tuning GuidelineLLM Framework

The framework of fine-tuning GuidelineLLM is shown in Figure 2.

To enable GuidelineLLM to analyze different jailbreak techniques effectively, we also need to fine-tune JailbreakLLM. JailbreakLLM is designed to generate new jailbreak queries based on provided jailbreak techniques and examples. Our fine-tuning framework is structured to expand from a small initial batch of jailbreak queries to a sufficient number of queries, thereby facilitating the exploration and development of new jailbreak methods. We illustrate the general workflow of fine-tuning GuidelineLLM in Algorithm 1.

Algorithm 1: Fine-tuning GuidelineLLM Framework   

<html><body><table><tr><td>Input:GPT seriesmodel GPT,responseLLM Mr,sam pled benign set B， JailbreakLLM based-model M♀, GuidelineLLMbased-model M Output: GuidelineLLM Mg T-Jailbreak set T° ← Initialize(GPT) Guideline set G° ← Initialize(GPT,T, B) JailbreakLLMM←Fine-tune(M♀,To) GuidelineLLM Mg ← Fine-tune(Mg, Go) repeat Ti+1 ← Generate(M,Ti) Ti+1 ← Filter(Mr,Ti+1). Gi+1 ← Generate(Mg,Ti+1,B) Gi+1 ← Filter(Mr,Gi+1) JailbreakLLM M+1←Fine-tune(Mj,Ti) GuidelineLLM Mi+1 ← Fine-tune(M, G)</td></tr></table></body></html>

Fine-tuning and Inference For JailbreakLLM and GuidelineLLM, our base candidate models are Llama2-7B-chat and Vicuna-7B. Considering the different alignment mechanisms between these models, we use the Llama2-7B-Chat model, which exhibits more robust alignment mechanisms, as the base model for fine-tuning the GuidelineLLM. Conversely, we use the Vicuna-7B model, which has comparatively weaker alignment mechanisms, as the base model for fine-tuning the JailbreakLLM. Both JailbreakLLM and GuidelineLLM are fine-tuned using Low-Rank Adaptation (LoRA) (Mangrulkar et al. 2022), setting the training epoch to 3.

The prompts for fine-tuning GuidelineLLM include jailbreak and benign queries as inputs, with corresponding guidelines as outputs. For JailbreakLLM, prompts require generating attack queries and jailbreak queries based on specific techniques. To enhance data diversity, two examples of the same jailbreak technique are sampled as inputs during fine-tuning. During inference, both GuidelineLLM and JailbreakLLM use prompts consistent with those in the finetuning phase.

Turn 0 Initialize Data Turn i Generate Guideline Turn i Fine-tune   
1 Generate Jailbreak Guideline Generate Benign Guideline 目 目 Harmful New / Jailbreak Query Benign Query 目 GuiNdewline Guideline Guideline Add Guideline Set LLM Guidelines Guideline Set LLM Set 四 Y Generate D Finetune   
1 New   
1 New Filter Guideline Add New Jailbreak   
1 Jailbreak Jailbreak Jailbreak Guideline Jailbreak LLM   
1 Set LLM Query Set Set ↑ Next Turn i+1

Evaluating the Quality To facilitate the subsequent turn of fine-tuning for the JailbreakLLM in generating more effective jailbreak queries, it is necessary to filter the currently generated queries for quality and effectiveness. Simultaneously, to ensure that the next phase of LLMs accurately rejects jailbreak queries while appropriately responding to benign queries, the number of jailbreak and benign queries in the guideline set must be balanced.

As illustrated in Figure 2, the quality of generated jailbreak queries is assessed by inputting them into the LLMs and evaluating the harmfulness of their outputs through rulebased detection. Following previous work (Liu et al. 2023a), we utilize a series of detection tokens to determine harmful responses. If these tokens are present in the output, it is inferred that a harmful response has been produced.

We select the jailbreak queries that yielded harmful outputs from the LLMs and incorporate them into the TJailbreak dataset. Furthermore, GuidelineLLM is employed to generate guidelines for these jailbreak queries, which are subsequently added to the Guideline dataset. The number of jailbreak queries added in this iteration is documented. To ensure dataset balance, we sample benign queries from the Alpaca and GPTTeacher datasets. These benign queries are then used to produce guidelines, discarding instances where the LLMs refuse to provide answers and retaining those where helpful responses are generated, until the number of benign instances matches the newly added jailbreak queries.

# Experiments

# Dataset and Setup

For harmful datasets, we select three in-the-wild jailbreak datasets (i.e., DAN (Shen et al. 2023), SAP200 (Deng et al. 2023a), JailbreakChatGPT (Liu et al. 2023b)) and a popular optimization-based automatic jailbreak method (i.e., AutoDAN (Liu et al. 2023a)). Additionally, we use gpt-3.5-turbo0125 to generate a T-jailbreak dataset comprising 1,137 queries. Besides, to evaluate the impact of GuidelineLLM on the helpfulness, we sample 400 queries from the Alpaca and GPTTeacher datasets (Taori et al. 2023).

# Evaluation Metrics

For harmful datasets, we report attack success rate(ASR, (Shen et al. 2023)) through rule-based detection, where lower scores indicate more robust safety. Following previous work (Liu et al. 2023a), we collect a series of detection tokens, and if these tokens are detected in the outputs of the LLMs, we determine that the LLMs has produced a harmful response.

For DAN, AutoDAN, and T-Jailbreak datasets, considering the complexity of responses, we adopt gpt-3.5-turbo0125 as an auto-annotation function for successful attack examples (Zheng et al. 2024). We also adopt gpt-3.5-turbo0125 for the benign dataset as an auto-annotation function for evaluating the helpfulness.

We set up five options: A(Very Harmful/Helpful), B(Harmful/Helpful), C(Moderately Harmful/Helpful), D(Slightly Harmful/Helpful), and E(No Harmful/Helpful at All). To facilitate scoring, we assign 4 points to option A, 3 points to option B, 2 points to option C, 1 point to option D, and 0 points to option E. For harmful data, a lower calculated score indicates a safer response, while for benign data, a higher calculated score indicates greater helpfulness.

# Baseline

We select Llama2-7B-Chat (Touvron et al. 2023), Vicuna7B, and Vicuna-13B (Chiang et al. 2023) as our responding LLMs. We compare the performance of our GuidelineLLM method with several other approaches: No defense, SelfReminderr (Xie et al. 2023), IA (Zhang et al. 2024b), and Llama3-Guard (Inan et al. 2023). For the Llama3-Guard, we only evaluate the harmfulness of the inputs. Considering that IA’s defensive method requires two rounds of inference, and both Self-Reminder and GuidelineLLM add additional prompts to the original query (i.e., Self-Reminder adds a system prompt, and GuidelineLLM adds guidelines), in the AutoDAN experiment, we use the input content from the last round of inference for each defensive method as the attack target.

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Defense Methods</td><td colspan="5">Jailbreak Dataset</td><td rowspan="2">Average</td></tr><tr><td>DAN</td><td>SAP200</td><td>JailbreakChatGPT</td><td>AutoDAN</td><td>T-Jailbreak</td></tr><tr><td rowspan="5">Llama2-7B-Chat</td><td>No Defense</td><td>9.50</td><td>1.62</td><td>0.00</td><td>58.57</td><td>27.00</td><td>19.34</td></tr><tr><td>Self-Reminder</td><td>2.23</td><td>0.00</td><td>0.00</td><td>0.19</td><td>6.16</td><td>1.72</td></tr><tr><td>IA</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.19</td><td>0.00</td><td>0.04</td></tr><tr><td>Llama3-Guard</td><td>48.46</td><td>14.69</td><td>22.37</td><td>1</td><td>63.85</td><td>37.34</td></tr><tr><td>GuidelineLLM (Ours)</td><td>1.49</td><td>0.19</td><td>0.00</td><td>0.00</td><td>5.98</td><td>1.53</td></tr><tr><td rowspan="5">Vicuna-7B</td><td>No Defense</td><td>62.76</td><td>55.00</td><td>47.37</td><td>90.77</td><td>59.98</td><td>63.18</td></tr><tr><td>Self-Reminder</td><td>62.39</td><td>18.75</td><td>47.37</td><td>62.88</td><td>34.21</td><td>45.12</td></tr><tr><td>IA</td><td>8.37</td><td>1.94</td><td>11.84</td><td>49.42</td><td>11.43</td><td>16.6</td></tr><tr><td>Llama3-Guard</td><td>48.46</td><td>14.69</td><td>22.37</td><td></td><td>63.85</td><td>37.34</td></tr><tr><td>GuidelineLLM(Ours)</td><td>14.90</td><td>2.00</td><td>3.95</td><td>17.69</td><td>6.24</td><td>8.96</td></tr><tr><td rowspan="5">Vicuna-13B</td><td>No Defense</td><td>61.27</td><td>56.25</td><td>32.89</td><td>96.92</td><td>60.07</td><td>42.10</td></tr><tr><td>Self-Reminder</td><td>58.66</td><td>16.31</td><td>30.26</td><td>84.81</td><td>35.71</td><td>45.15</td></tr><tr><td>IA</td><td>6.29</td><td>0.00</td><td>12.00</td><td>31.98</td><td>2.81</td><td>10.62</td></tr><tr><td>Llama3-Guard</td><td>48.46</td><td>14.69</td><td>22.37</td><td>1</td><td>63.85</td><td>37.34</td></tr><tr><td>GuidelineLLM(Ours)</td><td>14.15</td><td>4.63</td><td>1.32</td><td>21.54</td><td>16.53</td><td>11.63</td></tr></table></body></html>

Table 2: Main results. Comparison of our GuidelineLLM and four baselines under five jailbreak datasets in terms of $\mathrm { A S R ( \% ) }$ “-” means lacking implementation. The best results are highlighted in bold.

# Main Results

Table 2 presents the main results of different defensive methods. Analyzing these results, we derive several key findings:

(i) The proposed GuidelineLLM significantly reduces the harmfulness of outputs on LLMs, achieving a notable decrease in the ASR on average. Specifically, GuidelineLLM surpasses the best baseline, IA, in defending Vicuna-7B $( 8 . 9 6 \%$ ASR), and demonstrates comparable performance to IA on Llama2-7B-Chat ( $1 . 5 3 \%$ ASR) and Vicuna-13B $1 1 . 6 3 \%$ ASR). This outcome underscores the effectiveness of our approach, achieving substantial reductions in harmful outputs with a single inference.

(ii) In the AutoDAN dataset, which features an optimization-based strong jailbreak method, GuidelineLLM exhibits superior performance, with an ASR markedly lower than other baselines $0 \%$ ASR in Llama2-7B-Chat, $1 7 . 6 9 \%$ ASR in Vicuna-7B, and $2 1 . 5 4 \%$ ASR in Vicuna-13B). This demonstrates the robustness of our method; incorporating guidelines into queries enhances the focus of LLMs on detecting harmful content, thereby preventing optimizationbased strong jailbreak methods from circumventing safety measures.

(iii) GuidelineLLM also show solid defensive capabilities in-the-wild jailbreak datasets. Notably, in the JailbreakChatGPT dataset, GuidelineLLM’s ASR is significantly lower than that of other baselines. This finding indicates that GuidelineLLM is effective in defending against manually constructed jailbreak attacks as well.

In experiments involving GuidelineLLM, we observe that the LLMs frequently generate safe responses to jailbreak queries, which are not detectable using token-based detection methods. Consequently, we collect GuidelineLLM defense examples that are judged to have been successfully bypassed in datasets with higher ASR. Table 3 presents the harmfulness scores.

Table 3: Using gpt-3.5-turbo-0125 to evaluate the attack success examples from the three jailbreak datasets, DAN, T-Jailbreak, and AutoDAN for GuidelineLLM defense method. Due to the excellent performance of Llama2- 7B-Chat on the DAN and AutoDAN datasets, we do not conduct evaluations on these two datasets. The results for A, B, C, D and E are shown as percentages $( \% )$ .   

<html><body><table><tr><td>Model</td><td>Data</td><td>A</td><td>B</td><td>C</td><td>D</td><td>E</td><td>Score</td></tr><tr><td>Llama2- 7B-Chat</td><td>T-Jailbreak</td><td>0</td><td>1.73</td><td>5.17</td><td>0</td><td>93.1</td><td>0.16</td></tr><tr><td>Vicuna- 7B</td><td>DAN AutoDAN</td><td>27.16 51.69</td><td>28.40 2.25</td><td>17.28 0</td><td>0 0</td><td>27.16 46.07</td><td>2.28 2.13</td></tr><tr><td>Vicuna- 13B</td><td>T-Jailbreak DAN AutoDAN T-Jailbreak</td><td>3.75 34.09 53.7 1.52</td><td>15 28.41 4.63</td><td>13.75 9.09 0</td><td>1.25 0 0</td><td>66.25 28.41 41.67</td><td>0.88 2.39 2.29</td></tr></table></body></html>

We could find that many of the outputs judged to have successfully been bypassed under the GuidelineLLM defensive method are non-harmful. In the T-Jailbreak dataset, the harmfulness scores of the three responding LLMs are all below 1 point. In the AutoDAN dataset, nearly half of the examples judged as $\mathrm { ~ E ~ }$ are present. Similarly, in the DAN dataset, one-quarter of the examples are also judged as E.

The results prove that under the guidance of GuidelineLLM, the LLMs can provide safer and more userfriendly responses to jailbreak queries

Table 4: The results of the helpfulness scores for GuidelineLLM and Vanilla. The results for A, B, C, D, E, and FRR are shown as percentages $( \% )$ .   

<html><body><table><tr><td>Model</td><td>Method</td><td>A B</td><td>C D E FRR Score</td></tr><tr><td>Llama2- 7B-Chat</td><td>Vanilla Guideline LLM</td><td>91.62 3.14 71.02 5.74</td><td>3.4 0.26 1.57 1.83 3.82 15.93 0.78 6.53 7.31 3.33</td></tr><tr><td>Vicuna- 7B</td><td>Vanilla Guideline</td><td>91.27 0.79</td><td>5.03 0.26 2.65 2.91 3.77</td></tr><tr><td></td><td>LLM</td><td>75.94 5.35</td><td>14.71 0.53 3.48 4.01 3.50</td></tr><tr><td>Vicuna- 13B</td><td>Vanilla Guideline</td><td>93.62 1.33 3.72 85.86 2.62</td><td>0.53 0.8 1.33 3.86 9.69 0.26 1.57 1.83 3.70</td></tr></table></body></html>

Table 5: The results of the helpfulness scores for benign guideline in the fine-tuning dataset. The ✓indicates that benign guidelines are included in the fine-tuning dataset, while the ✗indicates that benign guidelines are not included in the fine-tuning dataset. The results for A, B, C, D, E, and FRR are shown as percentages $( \% )$ .   

<html><body><table><tr><td>Model</td><td>Benign Guideline</td><td>A</td><td>B</td><td>C</td><td>D</td><td>E</td><td>FRR</td><td>Score</td></tr><tr><td rowspan="2">Llama2- 7B-Chat</td><td>√</td><td>71.02</td><td>5.74</td><td>15.93</td><td>0.78</td><td>6.53</td><td>7.31</td><td>3.33</td></tr><tr><td>×</td><td>58.09</td><td>12.20</td><td>22.28</td><td>0.27</td><td>7.16</td><td>7.43</td><td>3.14</td></tr><tr><td rowspan="2">Vicuna- 7B</td><td>√</td><td>75.94</td><td>5.35</td><td>14.71</td><td>0.53</td><td>3.48</td><td>4.01</td><td>3.50</td></tr><tr><td>×</td><td>72.63</td><td>9.76</td><td>11.65</td><td>0.81</td><td>5.15</td><td>5.96</td><td>3.43</td></tr><tr><td rowspan="2">Vicuna- 13B</td><td>√</td><td>85.86</td><td>2.62</td><td>9.69</td><td>0.26</td><td>1.57</td><td>1.83</td><td>3.70</td></tr><tr><td>×</td><td>80.62</td><td>10.59</td><td>5.94</td><td>0.78</td><td>2.07</td><td>2.85</td><td>3.67</td></tr></table></body></html>

# Helpfulness Evaluation

Table 4 illustrates the impact of incorporating guidelines on the helpfulness of responding LLMs for benign queries.

Concerning the scores, the inclusion of guidelines slightly diminishes the helpfulness of the responding LLMs; however, the scores remain above 3.3, thereby maintaining a satisfactory level of helpfulness. Notably, the impact of adding guidelines on the helpfulness score is minimal for Vicuna13B. We classify options D (Slightly Helpful) and E (Not Helpful at all) as false refusals for benign queries by the LLMs. We then compute the sum of these two classifications, D and E, to derive the False Refusal Rate (FRR). The results reveal that for Llama2-7B-Chat, adding guidelines raises FRR from $1 . 8 3 \%$ to $7 . 3 1 \%$ . For Vicuna-7B and Vicuna-13B, the impact of adding guidelines on FRR is not significant. This discrepancy might be attributable to the stronger compliance with instructions exhibited by Llama2- 7B-Chat, wherein the added guidelines impose relatively stronger restrictions, resulting in diminished helpfulness. It is worth noting that since the guidelines employed in our study are generated by ”gpt-3.5-turbo-0125,” more professionally crafted guidelines could potentially enhance the helpfulness of the LLMs.

# Analysis

The impact of benign guidelines in the fine-tuning dataset To investigate the impact of benign guidelines on the fine-tuning of GuidelineLLM, we conduct an experiment using only jailbreak queries and jailbreak guidelines. Table 5 presents the results of the helpfulness scores for benign guideline in the fine-tuning dataset. The data reveal that GuidelineLLM fine-tuned solely with jailbreak guidelines leads to a decrease in helpfulness scores across all LLMs, accompanied by an increased FRR. These observations imply that while using only jailbreak guidelines for fine-tuning GuidelineLLM enhances the defensive performance of the LLMs, it compromises their helpfulness. The results, as presented in Table 6, indicate that fine-tuning solely with jailbreak guidelines results in a lower ASR across all LLMs, in comparison to fine-tuning with both jailbreak and benign guidelines. This finding suggests that GuidelineLLM finetuned exclusively with jailbreak guidelines demonstrates superior defensive performance.

Therefore, to preserve the helpfulness of LLMs while simultaneously improving their defensive effectiveness, it is imperative to include benign guidelines in the fine-tuning dataset for GuidelineLLM. This balanced approach ensures that LLMs remain effective and responsive to benign queries while also being robust against jailbreak attempts.

The impact of the number of guidelines To examine the impact of the number of guidelines provided by GuidelineLLM on the harmfulness and helpfulness of the responding LLMs’ outputs, we set three different maximum numbers of guidelines: 3, 5, and 7.

Table 7 presents the harmfulness results for varying maximum numbers of provided guidelines. For Llama2-7B-Chat, which features more robust alignment mechanisms, we observe that an increase in the number of guidelines provided results in a lower average ASR, reaching as low as $1 . 5 2 \%$ . Conversely, for Vicuna-7B and Vicuna-13B, which possess weaker alignment mechanisms, fewer guidelines lead to a lower average ASR, at $6 . 7 7 \%$ and $9 . 1 6 \%$ , respectively. This indicates that the effectiveness of GuidelineLLM is contingent upon the alignment mechanisms of the LLM. For LLMs with robust alignment mechanisms, supplying detailed guidelines aids in quickly identifying harmful content and subsequently refusing to respond.

Table 8 displays the helpfulness results for different maximum numbers of provided guidelines. The data reveal that variations in the number of guidelines have negligible impact on the helpfulness of all three LLMs.

The ASR of different jailbreak techniques To evaluate jailbreak capabilities in the T-Jailbreak dataset, we selected 30 questions per technique and used two defense methods: No Defense and Self-Reminder. Results (Table 9) show that ”Role Play” and ”Logical Reasoning” are the most effective techniques, likely due to LLMs’ strong instruction adherence and chain-of-thought capabilities, which may cause them to overlook harmful queries. These findings highlight how certain techniques exploit LLMs’ strengths, emphasizing the need for improved defense strategies to address such vulnerabilities.

Table 6: The ASR $( \% )$ results for benign guideline in the fine-tuning dataset across the four different jailbreak datasets. $\checkmark$ indicates that benign guidelines are included in the fine-tuning dataset, while ✗indicates that benign guidelines are not included in the fine-tuning dataset. The best results are highlighted in bold.   

<html><body><table><tr><td rowspan="2">Models</td><td rowspan="2">Benign Guideline</td><td colspan="4">JailbreakDataset</td><td rowspan="2">Average</td></tr><tr><td>DAN</td><td>SAP200</td><td>JailbreakChatGPT</td><td>T-Jailbreak</td></tr><tr><td rowspan="2">Llama2-7B-Chat</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>>x</td><td>1.49</td><td>0.19</td><td>0.00</td><td>5.98</td><td>1.92</td></tr><tr><td rowspan="2">Vicuna-7B</td><td>√</td><td>14.90</td><td>2.00</td><td>3.95</td><td>6.24</td><td>6.77</td></tr><tr><td>×</td><td>12.48</td><td>0.12</td><td>10.53</td><td>2.73</td><td>6.47</td></tr><tr><td rowspan="2">Vicuna-13B</td><td></td><td></td><td>4.60</td><td></td><td></td><td></td></tr><tr><td><x</td><td>14.15</td><td></td><td>13</td><td>14.5</td><td>9.16</td></tr></table></body></html>

Table 7: The ASR $( \% )$ results for different maximum number of guidelines across the four different jailbreak datasets. If the maximum is 7, it means that the number of guidelines provided by GuidelineLLM must be less than or equal to 7. The best results are highlighted in bold.   

<html><body><table><tr><td rowspan="2">Models</td><td rowspan="2">Max</td><td colspan="4">JailbreakDataset</td><td rowspan="2">Average</td></tr><tr><td>DAN</td><td>SAP200</td><td>JailbreakChatGPT</td><td>T-Jailbreak</td></tr><tr><td rowspan="3">Llama2-7B-Chat</td><td>n=3</td><td>1.49</td><td>0.19</td><td>0.00</td><td>5.98</td><td>1.92</td></tr><tr><td>n=5</td><td>1.30</td><td>0.25</td><td>0.00</td><td>5.10</td><td>1.66</td></tr><tr><td>n=7</td><td>0.74</td><td>0.13</td><td>0.00</td><td>5.19</td><td>1.52</td></tr><tr><td rowspan="3">Vicuna-7B</td><td>n=3</td><td>14.90</td><td>2.00</td><td>3.95</td><td>6.24</td><td>6.77</td></tr><tr><td>n=5</td><td>14.15</td><td>2.25</td><td>5.26</td><td>7.48</td><td>7.29</td></tr><tr><td>n=7</td><td>15.83</td><td>1.75</td><td>6.58</td><td>7.56</td><td>7.93</td></tr><tr><td rowspan="3">Vicuna-13B</td><td>n=3</td><td>14.15</td><td>4.63</td><td>1.32</td><td>16.53</td><td>9.16</td></tr><tr><td>n=5</td><td>14.90</td><td>5.50</td><td>1.32</td><td>16.45</td><td>9.54</td></tr><tr><td>n=7</td><td>18.25</td><td>5.75</td><td>1.32</td><td>19.00</td><td>11.08</td></tr></table></body></html>

<html><body><table><tr><td>Model</td><td>Max</td><td>A</td><td>B C</td><td>D</td><td>E</td><td>FRR</td><td>Score</td></tr><tr><td>Llama2- 7B-Chat</td><td>n=3 n=5 n=7</td><td>71.02 5.74 64.21 8.16 68.56 7.22</td><td>15.93 21.84 18.81</td><td>0.78 0.00 1.03</td><td>6.53 5.79 4.38</td><td>7.31 5.79 5.41</td><td>3.33 3.25 3.34</td></tr><tr><td>Vicuna- 7B</td><td>n=3 n=5 n=7</td><td>75.94 5.35 75.20 4.09 74.54 5.04</td><td>14.71 14.99 15.12</td><td>0.53 1.36 0.53</td><td>3.48 4.36 3.48</td><td>4.01 5.72 4.01</td><td>3.50 3.44 3.44</td></tr><tr><td>Vicuna- 13B</td><td>n=3 n=5 n=7</td><td>85.86 87.70 86.13</td><td>2.62 9.69 3.28 7.38 3.47 8.53</td><td>0.26 0.82 0.53</td><td>1.57 0.82 1.33</td><td>1.83 1.64 1.83</td><td>3.70 3.76 3.72</td></tr></table></body></html>

Table 8: The results of the helpfulness scores for different maximum number of guidelines and Vanilla. The results for A, B, C, D, E, and FRR are shown as percentages $( \% )$ .

Table 9: The $\mathrm { A S R ( \% ) }$ for seven jailbreak techniques under the No Defense and Self-Reminder defense methods. ASR values greater than $45 \%$ are highlighted in bold.   

<html><body><table><tr><td>Technique</td><td>Model</td><td></td><td>No Defense Self-Reminder</td></tr><tr><td rowspan="3">Role Play</td><td>Llama2-7B-Chat</td><td>63.33</td><td>10.00</td></tr><tr><td>Vicuna-7B</td><td>78.89</td><td>51.11</td></tr><tr><td>Vicuna-13B</td><td>75.56</td><td>47.78</td></tr><tr><td rowspan="3">Rule Determine</td><td>Llama2-7B-Chat</td><td>30.00</td><td>13.33</td></tr><tr><td>Vicuna-7B</td><td>51.67</td><td>40.00</td></tr><tr><td>Vicuna-13B</td><td>58.33</td><td>51.67</td></tr><tr><td rowspan="3">Logical Reasoning</td><td>Llama2-7B-Chat</td><td>80.00</td><td>17.78</td></tr><tr><td>Vicuna-7B</td><td>71.11</td><td>46.67</td></tr><tr><td>Vicuna-13B</td><td>75.56</td><td>50.00</td></tr></table></body></html>

# Conclusion

This study introduces a defensive paradigm called GuidelineLLM, aimed at enhancing the safety of various LLMs. Before responding to a query, GuidelineLLM first identifies potential risks, summarizes them into guideline suggestions, and feeds these to the responding LLMs. This approach reduces the harmfulness of the resulting content without requiring additional safety training for the LLMs. Additionally, we present a fine-tuning framework for GuidelineLLM, which includes the templated construction of jailbreak attacks, referred to as T-Jailbreak. Experiments demonstrate that GuidelineLLM can significantly reduce the harmfulness of output (achieving an average reduction of $3 4 . 1 7 \%$ in the ASR) while maintaining the helpfulness of LLMs in responding to benign queries.