# CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt Optimization for Text Generation

Han $\mathbf { H } \mathbf { e } ^ { * }$ , Qianchu Liu\*, Lei $\mathbf { X } \mathbf { u } ^ { * }$ , Chaitanya Shivade, Yi Zhang, Sundararajan Srinivasan, Katrin Kirchhoff

Amazon AWS AI Labs hankcs, liufqian, leixx, shivadc, yizhngn, sundarsr, katrinki $@$ amazon.com

# Abstract

Existing automatic prompt engineering methods are typically designed for discriminative tasks, where new task prompts are iteratively refined with limited feedback from a single metric reflecting a single aspect. However, these approaches are suboptimal for generative tasks, which require more nuanced guidance beyond a single numeric metric to improve the prompt and optimize multiple aspects of the generated text. To address these challenges, we propose a novel multi-aspect Critique-Suggestion-guided automatic Prompt Optimization (CriSPO) approach. CriSPO introduces a critique-suggestion module as its core component. This module spontaneously discovers aspects, and compares generated and reference texts across these aspects, providing specific suggestions for prompt modification. These clear critiques and actionable suggestions guide a receptive optimizer module to make more substantial changes, exploring a broader and more effective search space. To further improve CriSPO with multi-metric optimization, we introduce an Automatic Suffix Tuning (AST) extension to enhance the performance of task prompts across multiple metrics. We evaluate CriSPO on 4 state-of-the-art Large Language Models (LLMs) across 4 summarization and 5 Question Answering (QA) datasets. Extensive experiments show $3 \%$ ROUGE score improvement on summarization and substantial improvement of various metrics on QA.

# Code — https://github.com/amazon-science/CriSPO Extended version — https://arxiv.org/abs/2410.02748

# 1 Introduction

LLMs have emerged as powerful tools for various natural language processing tasks, including text generation (Brown et al. 2020). To fully leverage their capabilities, a critical step is to design a precise task prompt which specifies the desired behavior of the LLM to solve a task. Manual prompt engineering is often laborious, skill-intensive and sub-optimal, motivating the need for automatic prompt engineering techniques which automatically tune the task prompt.

Recent research has made notable progress in automatic prompt engineering for discriminative tasks, such as text classification (Zhou et al. 2022; Yang et al. 2023; Pryzant et al. 2023; Sordoni et al. 2024). These methods focus on optimizing task prompts for a single metric on a single aspect. The process typically involves instructing an LLM optimizer with a meta-prompt to generate new task prompts based on previously sampled task prompts and their corresponding scores. By iteratively exploring candidates and selecting the task prompt with the highest score, performance on the target metric improves over numerous iterations. However, applying these methods directly to text generation tasks, such as summarization, is sub-optimal due to challenges in obtaining effective optimization signals. Unlike classification tasks, where metrics are straightforward (eg. accuracy), automatic metrics for text generation, like ROUGE (Lin 2004), provides limited guidance for prompt refinement. For example, a lower ROUGE score may result from aspects such as mismatched length, differences in word choice due to formality, or varying writing formats, making it difficult to guide LLMs in prompt modification without fine-grained feedback targeting these individual aspects. Furthermore, evaluating text generation involves multiple metrics (Fabbri et al. 2021; Gao and Wan 2022; Elangovan et al. 2024). In addition to reference similarity, other metrics such as factual consistency, which can be assessed using metrics like AlignScore (Zha et al. 2023), are also important. Balancing or utilizing these multiple metrics is not fully addressed by existing prompt engineering methods that focus on optimizing a single metric.

To address these challenges, we introduce CriSPO, a multi-aspect Critique-Suggestion-guided automatic Prompt Optimization (CriSPO) approach. Overall, our approach employs LLMs to automatically identifies multi-aspect prompt revision suggestions, based on which prompts are automatically designed and refined (Table 8 in Appendix shows a working example of how a prompt gets revised in CriSPO). Inspired by recent self-reflection studies, where LLMs generate verbal feedback to aid in self-improvement (Gero et al. 2023; Shinn et al. 2023; Madaan et al. 2024), we designed the first key component of CriSPO: the multi-aspect critiquesuggestion meta-prompt. It automatically discovers proper aspects to compare generated text with reference, write critiques of flaws (Pryzant et al. 2023) and suggestions to improve the task prompt (Figure 2 shows a word cloud of aspects identified by CriSPO, including number of words, style, and precision). Both critiques and suggestions, written in natural language, are more helpful for prompt improvement than a single ROUGE score. We then create a receptive optimizer meta-prompt that generates new prompts. In addition to conditioning on previous high-score task prompts and scores, this optimizer also reviews the past critiques and suggestions. It then generates an overall suggestion and an improved task prompt candidate in a Chain-of-Thought (CoT) (Wei et al. 2022) manner. Our approach iteratively optimizes the task prompt using LLMs similar to previous work like Optimization by PROmpting (OPRO) (Yang et al. 2023), but it enriches the training signal with multi-aspect critiques and suggestions to better optimize a text generation metric. To further enhance performance by allowing the prompt to access external data, we design the task prompt template that contains placeholders for In-Context Learning (ICL) examples or retrieved contexts. The receptive optimizer meta-prompt generates these templates directly, so it can flexibly move components in task prompt for better organization.

While CriSPO offers multi-aspect guidance for optimizing text generation through critiques and suggestions, we further enhance this guidance by incorporating multiple metrics as additional teaching signals. To this end, we propose a novel Automatic Suffix Tuning (AST) extension which divides prompts into chunks conquering different metrics. Through multi-objective learning, we improve each new metric with little to no drop in existing metrics.

We test CriSPO on state-of-the-art LLMs, including Claude (Anthropic 2023, 2024), Mistral (Jiang et al. 2023) and Llama3 (MetaAI 2024), across 9 heterogeneous datasets. These include 4 summarization datasets spanning various abstractiveness, formats, and domains, as well as 5 QA datasets. Extensive experiments demonstrate that CriSPO significantly improves prompt quality and task performance over strong baselines as verified by human evaluation. We also conduct ablation study to assess the effectiveness of key ingredients.

# Our contributions are summarized below:

1) We propose CriSPO, an automatic prompt engineering approach tailored for generative tasks. It discovers aspects to critique generated text and write suggestions for more effective prompt revision.

2) We conduct comprehensive experiments across multiple LLMs and datasets, demonstrating the effectiveness and robustness of our method. We show an overall $3 \%$ improvement on ROUGE scores with qualitative verification from human evaluation. CriSPO also obtained consistent improvements on various QA tasks.

3) We propose AST that enables prompt tuning for multiple metrics. We show that CriSPO with AST can jointly optimize AlignScore (Zha et al. 2023) for faithfulness and ROUGE for reference similarity.

# 2 Related Work

There is an increasing effort in the literature to explore gradient-free automatic prompt engineering methods with off-the-shelf LLMs. The focus of these approaches is to find a good search algorithm for better prompt candidates to solve discriminitive tasks. Earlier studies have employed conventional paraphrasing methods for prompt generation through editing phrases (Prasad et al. 2023) or back translation $\mathrm { \Delta X u }$ et al. 2022). More recently, LLMs themselves have been used to sample prompt candidates. Zhou et al. (2022) proposed Automatic Prompt Engineering (APE) which iteratively prompts an LLM to generate semantically similar variations of the locally best prompt. Pryzant et al. (2023) add verbal feedback based on error examples to propose better prompts in terms of accuracy. Concurrently, Sordoni et al. (2024) learn prompts with variational inference by considering their outputs as latent variables. Later on, Yang et al. (2023) propose OPRO to improve over them by incorporating the history of past prompts with their scores which stabilizes optimization. More structured prompts have also been explored by imposing expert-level planning (Wang et al. 2023). In a parallel thread, Fernando et al. (2023) and Guo et al. (2023) were inspired by evolutionary algorithms to perform mutation operations for prompt generation. All of the existing approaches have mostly been designed to target classification tasks using a single metric. Comparing to the existing studies, our proposed method specifically targets the unique challenges in text generation and approaches the prompt optimization problem in a multi-aspect and multi-metric fashion. For practitioners, Khattab et al. (2023) design DSPy framework to build and optimize complex LLM pipelines in a programmatic fashion. TextGrad (Yuksekgonul et al. 2024) further generalizes optimization to text beyond prompt. Our CriSPO can be used as a powerful optimizer in these frameworks.

Our approach is also inspired by recent studies on using LLMs to automatically correct its output (Pan et al. 2023; Madaan et al. 2024). Gero et al. (2023) apply multiple selfreflection steps to improve the performance of information extraction. Yan et al. (2024) use CoT to generate structured comparison and preferences for two model outputs. Shinn et al. (2023) argue the importance of the self-reflection history and propose reflexion agent to provide verbal feedback on past trials for better decision in the next trials. It is important to notice that these self-reflection studies are strictly speaking not automatic prompt engineering approaches as these studies optimize output revision rather than directly on the prompts. CriSPO, however, automatically reflects on the design of the prompt and uses these past reflections to revise the prompts.

# 3 Method

Problem Formulation: In a text generation task, let $\mathcal { D } _ { \mathrm { t r n } } =$ $\{ ( x _ { i } , y _ { i } ) \} _ { i = 1 \dots n }$ be the training set, with a development set $\mathcal { D } _ { \mathrm { d e v } }$ and a test set $\mathcal { D } _ { \mathrm { t s t } }$ . Here, $x$ represents the input data, and $y$ is the corresponding ground truth reference. A task prompt $p$ comprises instructions that, when filled with input $x$ , are fed to a black-box API $\mathrm { L L M _ { t a s k } } ^ { 1 }$ to generate a completion $\hat { y } = \mathrm { L L M } _ { \mathrm { t a s k } } ( p , x )$ . The goal is to optimize $p$ using $\mathcal { D } _ { \mathtt { t r n } }$ and $\mathcal { D } _ { \mathrm { d e v } }$ to identify an optimal prompt $p ^ { * }$ that maximizes performance on one or more evaluation metrics on $\mathcal { D } _ { \mathrm { t s t } }$ .

CriSPO Overview: CriSPO is an automatic prompt optimization algorithm designed to iteratively refine a task prompt $p$ from an initial seed prompt $p _ { 0 }$ to the optimum: $p ^ { * } \gets \mathcal { F } ( p _ { 0 } )$ . In each iteration $t$ , we conduct the following steps:

Multi-Aspect Critique- M Candidate Task Prompt p Receptive Optimizer M。   
Suggestion Meta-Prompt C Here are some examples: Meta-Prompt INSERT_EXAMPLES_HERE. Candidate task prompt Now summarize an article: Task Description INSERT_INPUT_HERE. Input Reference Generation Try to follow a similar style. Top-K task Scores Critiques and {xi} {yi} {} prompts {Sk suggestions Step 4: Sample new   
Instructions: Step 1: Inference candidate prompt Instructions:   
1. Identify aspects to critique. on LLMtask 1. Synthesize all suggestions to   
2. Critique on these aspects. improvement ideas.   
3. Write suggestions. 2. Follow the ideas, and write a Multi-Aspect Receptive task prompt using placeholders. Critique-Suggestion Optimizer   
Response LLMcrit LLMopti Multiple Aspects Response Critiques prPoremvpitosu,s ctaosrkes, Step 3: Pick top-K Ideatshoent ahsokwptro imptrove Ct Step 2: Generate Suggestions critiques and critiques and task prompts Candidate task prompt suggestions suggestions for next iteration

• Evaluate on $\underline { { \mathcal { D } _ { \mathtt { t r n } } } }$ : Apply the candidate prompt $p _ { t }$ on $\overline { { \mathcal { D } _ { \mathtt { t r n } } } }$ , call $\mathtt { L L M } _ { \mathtt { L a s k } }$ to generate outputs $\{ \hat { y } _ { i } ^ { t } \} _ { i = 1 \dots n }$ and compute a primary metric $s _ { t }$ , which can be a single metric or an aggregation of multiple metrics. • Generate Critiques and Suggestions: Apply the multiaspect critique-suggestion meta-prompt $M _ { c }$ and call $\mathtt { L L M } _ { \mathtt { c r i t } }$ to compare $\{ \hat { y } _ { i } ^ { t } \} _ { i = 1 \dots n }$ and $\{ y _ { i } ^ { t } \} _ { i = 1 \dots n }$ and generate critiques and suggestions $c _ { t }$ (see Section 3.1). • Generate a Candidate Task Prompt: Select the top- $K$ task prompts from previous iterations based on the primary metric, and insert the corresponding $K$ triples $\{ ( p _ { k } , s _ { k } , c _ { k } ) \}$ into the receptive optimizer meta-prompt $M _ { o }$ . Then call $\mathbf { L L M } _ { \mathrm { o p t i } }$ to generate the next candidate prompt $p _ { t + 1 }$ (see Section 3.2).

We evaluate the current prompt $p _ { t }$ on $\mathcal { D } _ { \mathrm { d e v } }$ and select $p ^ { * }$ based on the primary metric. Upon reaching the maximum number of iterations, we apply an optional AST to enhance performance on secondary metrics on $p ^ { * }$ (see Section 3.3). Figure 1 demonstrates the workflow of CriSPO on summarization tasks. Table 8 (in Appendix) shows a concrete working example of CriSPO.

# 3.1 Multi-Aspect Critiques and Suggestions

Given a prompt $p _ { t }$ and its outputs $\{ \hat { y } _ { i } ^ { t } \}$ on $\mathcal { D } _ { \mathtt { t r n } }$ , we design a multi-aspect critique-suggestion meta-prompt $M _ { c }$ to identify critiques – flaws of the generated outputs across multiple aspects, and suggestions – specific edits on the task prompt to rectify each flaw.

Constructive critiques with spontaneous dimension discovery: In $M _ { c }$ , we first instruct $\mathbf { L L M } _ { \mathrm { c r i t } }$ to generate several task-specific and iteration-specific aspects for a given batch of outputs from the current $p _ { t }$ . This approach ensures that as task prompts evolve across iterations, the focus remains on relevant aspects, addressing specific issues that arise. Figure 2 illustrates the aspects discovered during optimization. For each aspect, $M _ { c }$ instructs $\mathbf { L L M } _ { \mathrm { c r i t } }$ to generates a critique highlighting potential problems of the outputs generated with $p _ { t }$ on the batch.

i Length

Multi-aspect suggestions: In line with each critique, a corresponding suggestion is made by $\mathbf { L L M } _ { \mathrm { c r i t } }$ to edit $p _ { t }$ . As opposed to Pryzant et al. (2023), we decoupled the edit suggestion module from the new prompt generation process. Rather than generating a new prompt with each suggestion, we pack a history of critiques and suggestions into the receptive optimizer for generating the next prompt, enabling more stable optimization over the infinite search space.

Our $M _ { c }$ is implemented in a single CoT meta-prompt which generates, dimensions, critiques and suggestions in one single LLM call, specifically

$$
c _ { t } = \mathrm { L L M } _ { \mathrm { c r i t } } \left( M _ { c } , p _ { t } , ( x _ { i } , y _ { i } , \hat { y } _ { i } ^ { t } ) _ { i = 1 \dots n } \right) .
$$

$M _ { c }$ for different LLMs and tasks are shown in Appendix $\mathrm { ~ H ~ }$ .

# 3.2 Receptive Prompt Optimizer

Our receptive prompt optimizer meta-prompt $M _ { o }$ improves over the OPRO optimizer meta-prompt (Yang et al. 2023) by enriching its optimization trajectory $\{ ( p _ { k } , \bar { s _ { k } } ) \}$ with past critiques and suggestions $c _ { k }$ . Thus, ours samples candidate prompts for the next iteration conditioned on an enriched optimization trajectory:

$$
p _ { t + 1 } = \mathrm { L L M } _ { \mathrm { o p t i } } \left( M _ { o } , \{ ( p _ { k } , s _ { k } , c _ { k } ) \} \right) .
$$

Specifically, we enhance the OPRO optimizer module with the following three improvements to better utilize critiques and suggestions for achieving stronger guidance and better exploration. See Appendix I for all $M _ { o }$ by LLMs and tasks.

Enriched optimization trajectory: The critiques and suggestions generated in Section 3.1 are used in an enriched optimization trajectory to propose new prompts via an OPROstyle optimizer. Specifically, our enriched optimization trajectory includes the top- $K$ best-performing past prompts $\{ \bar { p } _ { k } \}$ , their scores $\left\{ s _ { k } \right\}$ , critiques and suggestions $\left\{ c _ { k } \right\}$ , sorted in the ascending order by scores. Including critiques and suggestions in the optimization trajectory allows the LLM to avoid common limitations and identify common strengths from the past prompts for stable optimization.

Chain-of-thought: After enriching the optimization trajectory, we also apply CoT to the optimization process. Specifically, $\mathbf { L L M } _ { \mathrm { o p t i } }$ is explicitly asked to first compare high-score prompts to low-score ones, and then elicit general ideas and learnings, and finally draft a new and better prompt. CoT further ensures the optimizer to harness collective strength from the history and identify a promising path through comparing the divergent past prompts.

Flexible task prompt template: Instead of only tuning instruction text and fixing the input position as in existing approaches such as OPRO, CriSPO optimizes the task prompt structure using a template that can freely and naturally move around input and instruction in the prompt. It uses placeholders for the input and any external data. For example, we instruct LLM to generate example placeholder INSERT EXAMPLES HERE to indicate the position of ICL examples. In Retrieval-Augmented Generation (RAG) settings, we introduce a context placeholder INSERT CONTEXT HERE which will be replaced by the retrieved context for each question. When filled the placeholders with proper data, the task prompt clearly organized all the information to help $\mathtt { L L M } _ { \mathtt { L a s k } }$ better solve the task.

# 3.3 Multi-Metric Automatic Suffix Tuning

Using components in Section 3.1 and 3.2, CriSPO is ready to optimize a primary metric. To benefit from more teaching signals, e.g., completeness and faithfulness, here we extend CriSPO to multi-metric optimization by proposing a novel multi-metric learning extension named as AST.

In AST, we propose to optimize a suffix postscript $\sigma$ appended to $p ^ { * }$ , which has already been trained on certain metrics. $p ^ { * }$ will remain fixed throughout the whole tuning process for a new metric to preserve most of its performance on existing metrics. $p ^ { * }$ is extended with an additional suffix $\sigma ^ { * }  { \mathcal { F } } ( \sigma _ { 0 } )$ , which serves as a postscript to steer the LLM toward the new metric and remedy any potential regression in performance on existing metrics. Specifically, we provide both the main prompt $p ^ { * }$ and each suffix $\sigma _ { t }$ in the meta-prompts while asking the LLM to critique or refine only the suffix. To ensure we maintain existing metrics while improving on the additional metric, we take inspirations from the balance terms of loss functions in multi-task learning (He and Choi 2021) and compute an aggregated score across the multiple metrics. Since the score of each metric is on different scales and hard to estimate before training, we propose to use the average ranking of each metric as the ultimate basis to score prompt candidates in the meta-prompt.

# 4 Main Experiments

# 4.1 Experiment Setup

Datasets We select a diverse range of 4 summarization tasks including conventional document summarization tasks such as CNN daily mail (Hermann et al. 2015) (news headline summarization), and also conversation summarization tasks such as SAMSum (Gliwa et al. 2019), MeetingBank (Hu et al. 2023). In addition, we test on a medical-domain clinical note summarization task, ACI-Bench (Yim et al. 2023). Detailed data setup can be found in Appendix C. These tasks cover various lengths, domains and styles as summarized in Table 9. We report ROUGE-1/2/L F-measure (Lin 2004)2 to measure output similarity to the references.

LLMs and Baselines We test our approach on state-of-theart LLMs including proprietary models: Claude Instant (Anthropic 2023), Claude3 Sonnet (Anthropic 2024), and opensource LLMs: Mistral 7B (Jiang et al. 2023) and Llama3 8B (MetaAI 2024). We use the same LLM for all the 3 CriSPO modules: task inference, critique-suggestion and receptive optimization, apart from the Llama3 setup. Specific hyperparameters with ablations are detailed in Appendix D.

Our baseline methods include manual prompts with zero/few-shot ICL. These manual prompts are carefully tuned for each task to incorporate length constraints and task guidelines, and therefore establish a high bar of performance from manual prompt engineering (Appendix J). Given there are no existing automatic prompting results for text generation, we adapted OPRO (Yang et al. 2023), a competitive established approach, on our selected tasks. We use the same hyperparameter setup in OPRO and CriSPO for fair comparison.

# 4.2 Main Results

As shown in Table 1, across all the tasks and LLMs, CriSPO consistently improves over 0-shot manual prompt and OPRO baselines. Overall, there are approximately 3-4 point improvements for all LLMs. Even the strong state-of-the-art Claude3 Sonnet model can still greatly benefit from CriSPO. The consistent improvement shows CriSPO is a more effective search method than existing method (OPRO) to unlock the full potential of these LLMs, and offers an alternative solution to the more labour-intensive manual prompt engineering.

Table 1: Comparing CriSPO with manual prompts and OPRO on representative summarization benchmarks. Averaged R1/R2/RL (i.e. ROUGE-1/2/L) are reported across 3 runs. 3-shot\*: 3-shot ICL with example selection. Claude3: Claude3 Sonnet. Claude In.: Claude Instant. Llama3 results marked with (#) are using Claude3 Sonnet as the optimizer. 3-shot\* results are empty for Llama3 due to its limited context window. Standard deviation and SOTA results are in Appendix M. Claude3 Sonnet achieves new SOTA ROUGE-1 performance on ACI-Bench.   

<html><body><table><tr><td></td><td></td><td colspan="6">Manual</td><td colspan="8">Automatic Prompt Engineering</td></tr><tr><td></td><td></td><td colspan="2">0-shot</td><td colspan="4"></td><td colspan="3">OPRO</td><td colspan="3">CriSPO</td><td colspan="3">CriSPO 3-shot*</td></tr><tr><td>Dataset</td><td>LLM</td><td>R1</td><td>R2</td><td>RL</td><td>R1</td><td>R2</td><td>RL</td><td>R1 R2</td><td></td><td>RL</td><td>R1 R2</td><td></td><td>RL</td><td>R1 R2</td><td>RL</td></tr><tr><td>CNN</td><td>Claude In.</td><td>37.5</td><td>12.5</td><td>22.6</td><td>40.4</td><td>14.8</td><td>24.8</td><td>39.5</td><td>14.3 24.5</td><td></td><td>40.1 15.7</td><td>26.1</td><td>42.1</td><td>17.0</td><td>27.4</td></tr><tr><td></td><td>Claude3</td><td>38.8</td><td>14.4</td><td>24.0</td><td>40.3</td><td>15.4 25.2</td><td>39.7</td><td>15.1</td><td>5.1</td><td>42.2</td><td>17.3</td><td>27.9</td><td>41.6</td><td>16.3</td><td>27.1</td></tr><tr><td></td><td>Mistral 7B</td><td>30.9</td><td>11.0</td><td>20.4</td><td>30.7</td><td>10.6 20.1</td><td>36.5</td><td>14.4</td><td>23.0</td><td>38.5</td><td>14.3</td><td>23.9</td><td>38.5</td><td>14.3</td><td>24.1</td></tr><tr><td></td><td>Llama3 8B</td><td>37.9</td><td>14.4</td><td>23.8</td><td></td><td></td><td></td><td>39.1 15.2</td><td>24.6#</td><td></td><td>41.5 16.3</td><td>26.5#</td><td></td><td></td><td></td></tr><tr><td>MeetingBank Claude In.</td><td></td><td>30.7</td><td>11.6</td><td>20.5</td><td>34.2</td><td>17.3</td><td>25.5</td><td>39.0 20.3</td><td>29.7</td><td></td><td>41.4 23.7</td><td>33.1</td><td>50.1</td><td>35.4</td><td>44.4</td></tr><tr><td></td><td>Claude3</td><td>31.2</td><td>14.2</td><td>22.3</td><td>37.5</td><td>22.0 29.5</td><td>41.5</td><td>21.8</td><td>32.0</td><td>47.4</td><td>32.5</td><td>40.9</td><td>58.5</td><td>46.5</td><td>54.1</td></tr><tr><td></td><td>Mistral 7B</td><td>26.0</td><td>11.5</td><td>18.5</td><td>31.3</td><td>14.8 22.7</td><td>33.9</td><td>15.4</td><td>24.2</td><td>39.1</td><td>19.5</td><td>29.3</td><td>35.2</td><td>16.7</td><td>26.1</td></tr><tr><td></td><td>Llama3 8B</td><td>31.4</td><td>14.6</td><td>22.6</td><td></td><td></td><td></td><td>40.2 22.3</td><td>31.5#</td><td>44.7</td><td>27.6</td><td>36.8#</td><td></td><td></td><td></td></tr><tr><td>SAMSum</td><td>Claude In.</td><td>33.9</td><td>11.7</td><td>25.6</td><td>37.8</td><td>14.3</td><td>28.8</td><td>38.1</td><td>13.4</td><td>28.7</td><td>44.4 16.9</td><td>34.3</td><td>45.7</td><td>18.7</td><td>36.2</td></tr><tr><td></td><td>Claude3</td><td>35.8</td><td>12.7</td><td>27.0</td><td>41.1</td><td>16.6 31.3</td><td></td><td>39.0 14.7</td><td>30.1</td><td></td><td>43.4 17.1</td><td>34.3</td><td>47.2</td><td>20.8</td><td>38.2</td></tr><tr><td></td><td>Mistral 7B</td><td>32.0</td><td>10.2</td><td>24.1</td><td>39.5</td><td>14.1 30.3</td><td>37.9</td><td>13.6</td><td>29.0</td><td>37.6</td><td>12.4</td><td>28.4</td><td>40.0</td><td>14.2</td><td>30.8</td></tr><tr><td></td><td>Llama3 8B</td><td>35.7</td><td>12.3</td><td>27.1</td><td></td><td></td><td>39.3</td><td>14.7</td><td>30.0#</td><td>44.8</td><td>18.8</td><td>35.4#</td><td></td><td></td><td></td></tr><tr><td>ACI-Bench</td><td>Claude In.</td><td>43.8</td><td>16.9</td><td>26.1</td><td>51.5</td><td>23.6</td><td>33.5</td><td>45.2 16.3</td><td>25.5</td><td></td><td>53.0 19.7</td><td>26.8</td><td>58.2</td><td>26.7</td><td>35.3</td></tr><tr><td></td><td>Claude3</td><td>47.3</td><td>20.3</td><td>29.3</td><td>59.1</td><td>30.1 38.6</td><td>48.8</td><td>20.1</td><td>29.5</td><td>54.0</td><td>21.4</td><td>30.3</td><td>63.1</td><td>32.5</td><td>41.0</td></tr><tr><td></td><td>Mistral 7B</td><td>47.8</td><td>17.7</td><td>25.4</td><td>48.4</td><td>19.2 28.1</td><td>45.1</td><td>17.0</td><td>25.2</td><td>50.2</td><td>18.2</td><td>25.6</td><td>50.3</td><td>18.7</td><td>26.2</td></tr><tr><td></td><td>Llama3 8B</td><td>50.5</td><td>19.8</td><td>27.7</td><td></td><td></td><td>54.2</td><td>22.0</td><td>29.3#</td><td>56.2</td><td>22.8</td><td>29.9#</td><td></td><td></td><td></td></tr><tr><td>Average</td><td>Claude In.</td><td>36.5</td><td>13.2</td><td>23.7</td><td>41.0</td><td>17.5</td><td>28.2</td><td>40.4</td><td>16.1 27.1</td><td></td><td>44.7 19.0</td><td>30.1</td><td>49.0</td><td>24.4</td><td>35.8</td></tr><tr><td></td><td>Claude3</td><td>38.3</td><td>15.4</td><td>25.6</td><td>44.5</td><td>21.0 31.2</td><td>42.2</td><td>17.9</td><td>29.2</td><td>46.8</td><td>22.1</td><td>33.3</td><td>52.6</td><td>29.0</td><td>40.1</td></tr><tr><td></td><td>Mistral 7B</td><td>34.2</td><td>12.6</td><td>22.1</td><td>37.5</td><td>14.7 25.3</td><td>38.4</td><td>15.1</td><td>25.4</td><td>41.4</td><td>16.1</td><td>26.8</td><td>41.0</td><td>16.0</td><td>26.8</td></tr><tr><td></td><td>Llama3 8B</td><td>38.9</td><td>15.3</td><td>25.3</td><td></td><td></td><td>43.2</td><td>18.6</td><td>28.8</td><td>46.8</td><td>21.4</td><td>32.2</td><td></td><td></td><td></td></tr></table></body></html>

Additionally, we found examples to be helpful as adding 3-shot ICL significantly improves the performance. Owning to the versatile template in CriSPO, we can easily integrate examples and we show CriSPO 3-shot can further boost performance over CriSPO and achieves the best performance in most setups. It is also worth noticing that the vanilla CriSPO w/o ICL can match or even outperform manual prompt with 3-shot in most datasets and setups, reducing latency and cost.

# 4.3 Ablating Key Ingredients

Table 2 shows the ablation results of CriSPO with Claude Instant on SAMSum dataset. We observed that the three key components in our approach, including flexible template, critique and step-by-step CoT optimization, are essential for achieving optimal performance. Removing any of these components leads to a decrease in performance. Removing critique-suggestion module and CoT optimization altogether leads to a 5 point decrease, similar to OPRO performance. This indicates these two elements are essential to the success of CriSPO and flexible template is only effective when being added on top of these two elements.

The key novelty in our proposed novel critique-suggestion strategy in CriSPO is that it has multi-aspect: i.e. the LLM will generate multi-aspect comparison without enforcing predefined aspects. To understand the effect of the multi-aspect critique-suggestion, we provide two alternative baselines: 1. no multi-aspect: we ask LLM to compare predictions and references in general with no explicit requirement for generating critique and suggestions along multiple dimensions/aspects. This is in line with the approach adopted by Pryzant et al. (2023). 2. predefined aspects: we carefully design dimensions potentially helpful for the summarization task and include verbosity, comprehensiveness, precision and style along with their definitions (Appendix L). The no multi-aspect critiquesuggestion baseline performs significantly worse, lacking critical and targeted suggestions due to its tendency to be too general. The predefined multi-aspect approach is as effective as CriSPO but we see no significant improvement from explicit definitions of dimensions. This is because the critique LLM in CriSPO is already able to identify relevant dimensions (such as completeness, verbosity etc. as in Table 8) for each iteration without explicit guidance.

Table 2: Ablation studies of CriSPO on the SAMSum dataset with Claude Instant. We report average and std deviation of the ROUGE-1 F results across 3 runs.   

<html><body><table><tr><td>Method</td><td>Crit-Sugg</td><td>CoT</td><td>Template</td><td>avg</td><td>std</td></tr><tr><td>CriSPO</td><td>√</td><td>√</td><td>√</td><td>44.4</td><td>1.9</td></tr><tr><td></td><td>X</td><td>√</td><td>√</td><td>42.8</td><td>0.8</td></tr><tr><td></td><td>√</td><td>×</td><td>√</td><td>43.9</td><td>0.3</td></tr><tr><td></td><td>√</td><td>√</td><td>X</td><td>42.2</td><td>1.6</td></tr><tr><td></td><td>X</td><td>×</td><td>√</td><td>37.4</td><td>3.4</td></tr><tr><td>OPRO</td><td>X</td><td>X</td><td>X</td><td>38.1</td><td>1.3</td></tr><tr><td>Method</td><td colspan="5">Changing multi-aspect</td></tr><tr><td>CriSPO</td><td colspan="3">free multi-aspects</td><td>44.4</td><td>1.9</td></tr><tr><td></td><td colspan="3">no multi-aspects</td><td>41.1</td><td>0.9</td></tr><tr><td></td><td colspan="3">pre-defined multi-aspects</td><td>44.5</td><td>0.7</td></tr></table></body></html>

# 4.4 Qualitative Analysis and Human Evaluation

To qualitatively compare CriSPO outputs with the baselines, we conducted human evaluation on 20 examples from the SAMSum testset. We follow the procedure from Liu et al. (2023) where the reference summaries are split into atomic content units and annotators mark them as either present or missing in the prediction summary. In total, we collected 300 annotations (100 annotations $\times 3$ annotators). A final normalized recall score is computed with a length penalty, which indicates how similar the prediction summary is to the reference summary. In our experiment, we asked three annotators with postgraduate degrees to independently annotate the summaries with blinded setup. The inter-annotator agreement is “almost perfect” (0.8679 Fleiss kappa). We then took the majority vote, and calculated the final normalized recall score (human rating) using a de-correlated length penalty.

As shown in Table 3, CriSPO achieves the highest rating according to our human evaluation. Table 4 shows qualitative examples where prompts found by CriSPO better capture the style of the reference summaries in terms of length, what to focus on and what to skip. CriSPO outputs also look the most similar to the references, especially in terms of being as concise as the reference while covering all the key details.

Table 3: Human evaluation on sampled SAMSum test.   

<html><body><table><tr><td></td><td>Manual</td><td>OPRO</td><td>CriSPO</td></tr><tr><td>Human Rating</td><td>0.58</td><td>0.59</td><td>0.63</td></tr></table></body></html>

# 4.5 Quantitative Analysis of Prompt Diversity

To verify that our design in Section 3 leads to a better exploration of the solution space, we quantitatively analyze the diversity of prompts found by CriSPO and OPRO (same hyperparameters, Section 4.1) on the summarization datasets. We measure 4 aggregated properties on all task prompts explored by each method during optimization: length (number of words), vocabulary size (number of unique words used), and pairwise ROUGE-L/semantic similarity. For pairwise semantic similarity, we employ Sentence Transformers (Reimers and Gurevych 2019) to obtain their embeddings and cosine distances.

As shown in Table 5, CriSPO prompts demonstrate larger variations in length and vocabulary while being less similar in lexicons and semantics, indicating its strength in exploring a larger space. We also provide a visualization of the prompts found by OPRO and CriSPO in Appendix F.

Table 4: Qualitatively comparing prompts found by OPRO and CriSPO on SAMSum. CriSPO is able to find a prompt to generate output more similar to the reference’s concise style.   

<html><body><table><tr><td>OPRO [Best Prompt]: Generate a one to two sentence sum- mary within the {summary〉tags that concisely describes the key details of the conversation and any conclusions reached. INPUT_DOC CriSPO [Best Prompt]: The text below contains a discussion ex- pressing several key facts and events. Your concise1-sentence summary should relate only the 2 most important pieces of information stated, without assumptions or extra context. IN-</td></tr><tr><td>PUT_DOC Write the summary within (summary) tags. OPRO [Example Output]: Ralph asked Andrew if he heard a Polish joke,then told a joke about sinkinga Polish battleship by putting itin water.Andrewresponded that the joke was terrible and so unfunny that it made his mouth dry,requiring a sip of</td></tr><tr><td>Water. CriSPO [Example Output]:Ralph tells Andrew a Polish bat- tleship joke that Andrew finds unfunny. [Reference]: Ralph told Andrew a joke.</td></tr></table></body></html>

Table 5: Prompt diversity on 4 summarization datasets.   

<html><body><table><tr><td>Dataset</td><td>Length↑</td><td>Vocab↑</td><td>ROUGE-L↓</td><td>Cosine↓</td></tr><tr><td>CNN</td><td></td><td></td><td></td><td></td></tr><tr><td>OPRO</td><td>41±6</td><td>36±5</td><td>57.5</td><td>0.93</td></tr><tr><td>CriSPO</td><td>149±24</td><td>96±12</td><td>50.3</td><td>0.90</td></tr><tr><td>MeetingBank</td><td></td><td></td><td></td><td></td></tr><tr><td>OPRO</td><td>31±5</td><td>28±4</td><td>44.9</td><td>0.84</td></tr><tr><td>CriSPO</td><td>216±41</td><td>135±19</td><td>39.7</td><td>0.80</td></tr><tr><td>SAMSum</td><td></td><td></td><td></td><td></td></tr><tr><td>OPRO</td><td>34±6</td><td>30±5</td><td>57.0</td><td>0.94</td></tr><tr><td>CriSPO</td><td>172±22</td><td>112±12</td><td>46.0</td><td>0.88</td></tr><tr><td>ACI-Bench</td><td></td><td></td><td></td><td></td></tr><tr><td>OPRO</td><td>58±11</td><td>46±8</td><td>62.7</td><td>0.95</td></tr><tr><td>CriSPO</td><td>247±40</td><td>117±13</td><td>54.3</td><td>0.93</td></tr></table></body></html>

# 5 Extension with Multi-Metric Optimization

AST Setup In this experiment, we extend CriSPO with our proposed AST to optimize multiple metrics simultaneously. Specifically, we take the best prompts optimized for ROUGE$^ { 1 \mathrm { F } }$ -measure from CriSPO with Claude Instant as the seed main prompt $p ^ { * }$ . We employ AST to optimize AlignScore (Zha et al. 2023) starting from a simple seed suffix $\sigma _ { 0 }$ : “Every word of your summary must be faithful to the input/conversation” across all datasets. The AlignScore between the input text and the output summary is used as a signal reflecting the faithfulness. With regard to baselines, we report the initial performance in ROUGE-1 F-measure and AlignScore of the seed main prompt w/ and w/o the seed suffix. We also provide a strong baseline to tune both the main prompt and its suffix together (full tuning) rather than only the suffix in AST.

Results The results for multi-metric optimization are presented in Table 6. On all datasets, our AST is able to optimize the new metric AlignScore with a negligible or zero regression on the existing metric ROUGE, meaning that AST can reduce LLM hallucination while maintaining relevancy in the output. In particular, AST dramatically improves AlignScore by 11.7 points on CNN. Across tasks, AST is the most effective approach to improve AlignScore while maintaining ROUGE. Among all methods, AST is the only one that brings consistent improvement on AlignScore for every task, and achieves the best average overall improvement (by 4.3). The main prompt w/ suffix seed prompt slightly improves AlignScore (by 1.2) and the full-tuning baseline only meaningfully improves AlignScore on CNN and the overall improvement is marginal (by 0.7). The superiority of AST shows that it can robustly optimize multiple metrics across various domains.

Table 6: Claude Instant multi-metric results on summarization tasks. In seed block, main and w/ suffix refers to the ROUGE-1-optimized prompt $p ^ { * }$ and its concatenation with the manual suffix $\sigma _ { 0 }$ respectively. In CriSPO block, full and w/ AST refers to the full-prompt tuning baseline and AST in Section 3.3 respectively. We show in the parentheses the absolute differences with main $p ^ { * }$ only when $> 0 . 5$ . Here we use CriSPO $\mathcal { F } ( \cdot )$ to optimize an aggregation of two metrics.

generation by eliciting reasoning before the final answer. Following the conventions, we report Exact Match for Natural Questions and TriviaQA, F1 for Squad, ROUGE-L for NarrativeQA, accuracy for MedMCQA. For efficiency, we only used a small fraction of the train and dev set for the experiments. The specific data settings are listed in Appendix C.

Table 7: Comparing CriSPO with manual prompts and competitive automatic prompt engineering baseline OPRO on representative QA benchmarks. We report Exact matching for NQ (Natural Questions) and TQA (TrivialQA). We report F1 for Squad, Rouge-L for NarQA (NarrativeQA), and accuracy for MedMCQA. $k ^ { * }$ : $k$ -shot ICL with example selection. Standard deviations can be found in Table 17.   

<html><body><table><tr><td></td><td colspan="2">Seed</td><td colspan="2">CriSPO: F(·)</td></tr><tr><td>Dataset</td><td>main p*</td><td>w/ suffix p*+00</td><td>full F(p*+σ0)</td><td>w/ AST p*+F(σo)</td></tr><tr><td>CNN</td><td></td><td>40.6</td><td></td><td></td></tr><tr><td>ROUGE-1 AlignScore</td><td>40.7 66.5</td><td>69.5(↑3.0)</td><td>40.6 69.6(↑3.1)</td><td>40.4 78.1(↑11.7)</td></tr><tr><td>MeetingBank</td><td></td><td></td><td></td><td></td></tr><tr><td>ROUGE-1</td><td>39.6</td><td>39.9</td><td>39.4</td><td>39.7</td></tr><tr><td>AlignScore</td><td>43.6</td><td>43.7</td><td>43.8</td><td>44.4(↑0.9)</td></tr><tr><td>SAMSum</td><td></td><td></td><td></td><td></td></tr><tr><td>ROUGE-1</td><td>45.5</td><td>45.9</td><td>45.8</td><td>45.1</td></tr><tr><td>AlignScore</td><td>87.2</td><td>86.6(↓0.6)</td><td>86.6(↓0.6)</td><td>88.6(↑1.4)</td></tr><tr><td>ACI-Bench ROUGE-1</td><td>54.4</td><td></td><td></td><td></td></tr><tr><td>AlignScore</td><td>66.7</td><td>55.2(↑0.8) 69.0(↑2.3)</td><td>54.5 66.5</td><td>54.3 70.0(↑3.4)</td></tr><tr><td>Average</td><td></td><td></td><td></td><td></td></tr><tr><td>ROUGE-1</td><td>45.1</td><td>45.4</td><td>45.1</td><td>44.9</td></tr><tr><td>AlignScore</td><td>66.0</td><td>67.2(↑1.2)</td><td>66.7(10.7)</td><td>70.3(↑4.3)</td></tr></table></body></html>

Results Similar to summarization tasks, we observe CriSPO significantly outperforms the manual prompt and OPRO baseline in various QA datasets as shown in Table 7. For NarrativeQA, CriSPO brings massive improvement $_ { + 1 0 }$ ROUGE-L) compared with baselines, achieving the new SOTA performance. For Natural Questions and TrivialQA, CriSPO has no issue incorporating the RAG setup and achieving consistent improvement over the manual prompt and OPRO. Surprisingly, CriSPO even outperforms OPRO on MedMCQA despite it is not designed for classification tasks.

<html><body><table><tr><td colspan="2"></td><td colspan="2">Manual</td><td colspan="3">Automatic Prompt Engineering</td></tr><tr><td>Task</td><td>Claude</td><td>0-shot</td><td>64*</td><td>OPRO</td><td>CriSPO CriSPO 64*</td><td></td></tr><tr><td>NQ</td><td>Instant</td><td>34.0</td><td>33.4</td><td>8.0</td><td>36.5</td><td>37.8</td></tr><tr><td rowspan="3">T-QA</td><td>Sonnet</td><td>26.6</td><td>32.0</td><td>6.7</td><td>38.3</td><td>38.7</td></tr><tr><td>Instant</td><td>58.6</td><td>59.2</td><td>53.7</td><td>66.3</td><td>67.5</td></tr><tr><td>Sonnet</td><td>58.4</td><td>65.0</td><td>41.8</td><td>70.6</td><td>72.1</td></tr><tr><td></td><td></td><td>0-shot</td><td>5*</td><td>OPRO</td><td>CriSPO</td><td>CriSPO 5*</td></tr><tr><td rowspan="2">Squad</td><td>Instant</td><td>79.5</td><td>82.5</td><td>78.5</td><td>87.8</td><td>89.4</td></tr><tr><td>Sonnet</td><td>76.1</td><td>83.2</td><td>76.4</td><td>85.3</td><td>87.9</td></tr><tr><td rowspan="2">NarQA</td><td>Instant</td><td>64.2</td><td>67.0</td><td>59.4</td><td>75.1</td><td>76.1</td></tr><tr><td>Sonnet</td><td>64.0</td><td>66.7</td><td>58.6</td><td>76.2</td><td>75.2</td></tr><tr><td>Med-</td><td>Instant</td><td>49.2</td><td>53.8</td><td>50.5</td><td>52.3</td><td>54.4</td></tr><tr><td>MCQA</td><td>Sonnet</td><td>49.8</td><td>54.4</td><td>57.7</td><td>57.9</td><td>57.4</td></tr></table></body></html>

# 7 Conclusion

# 6 Generalization to Other Tasks

To confirm its generalizability, in this section, we apply CriSPO to extractive, abstractive and multi-choice QA tasks.

Datasets We benchmark CriSPO on 5 commonly used QA datasets, including 1) Wikipedia-based QA: Natural Questions (Kwiatkowski et al. 2019), TriviaQA (Joshi et al. 2017), Squad (Rajpurkar et al. 2016) 2) story-based abstractive reading comprehension: NarrativeQA (Koˇcisk\`y et al. 2018) and 3) medical domain multiple-choice QA: MedMCQA (Pal, Umapathi, and Sankarasubbu 2022) . For Natural Questions and TrivialQA, we also incorporate the RAG setup to optimize the prompt template with inserted pre-retrieved contexts from each dataset. We retrieved the Wikipedia pages following Izacard and Grave (2021). For NarrativeQA, we use summaries as contexts. For MedMCQA, we cast it to text

In this paper, we tackle the challenging problem of automatic prompt engineering for text generation. We propose CriSPO, a multi-aspect critique-suggestion guided optimizer augmented with enriched trajectory, CoT and flexible template. Our experiments show multi-aspect critique-suggestion is critical for finding good task prompts. Overall, CriSPO achieves $3 \%$ ROUGE score improvement and $4 \%$ human rating increase compared to baseline methods for summarization, and significant improvement for QA. We also show that CriSPO can effectively optimize multiple metrics through a novel suffix tuning extension AST, and incorporate ICL and RAG with flexible prompt templates. Ablation studies confirm the effectiveness of all CriSPO components. Human evaluation and quantitative analysis show CriSPO encourages more effective prompt exploration and the optimized prompts can better capture task requirements.