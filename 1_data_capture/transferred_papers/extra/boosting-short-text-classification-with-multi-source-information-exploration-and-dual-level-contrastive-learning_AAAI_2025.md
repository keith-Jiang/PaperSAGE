# Boosting Short Text Classification with Multi-Source Information Exploration and Dual-Level Contrastive Learning

Yonghao Liu1\*, Mengyu $\mathbf { L i } ^ { 1 \ast }$ , Wei $\mathbf { P a n g } ^ { 2 }$ , Fausto Giunchiglia3, Lan Huang1, Xiaoyue Feng1†, Renchu Guan1†

$^ 1$ Key Laboratory of Symbolic Computation and Knowledge Engineering of the Ministry of Education, College of Computer Science and Technology, Jilin University 2Mathematical and Computer Sciences, Heriot-Watt University 3University of Trento   
{yonghao20, mengyul21} $@$ mails.jlu.edu.cn, w.pang@hw.ac.uk, fausto.giunchiglia@unitn.it {huanglan, fengxy, guanrenchu $\} \ @ .$ jlu.edu.cn

# Abstract

Short text classification, as a research subtopic in natural language processing, is more challenging due to its semantic sparsity and insufficient labeled samples in practical scenarios. We propose a novel model named MI-DELIGHT for short text classification in this work. Specifically, it first performs multi-source information (i.e., statistical information, linguistic information, and factual information) exploration to alleviate the sparsity issues. Then, the graph learning approach is adopted to learn the representation of short texts, which are presented in graph forms. Moreover, we introduce a dual-level (i.e., instance-level and cluster-level) contrastive learning auxiliary task to effectively capture different-grained contrastive information within massive unlabeled data. Meanwhile, previous models merely perform the main task and auxiliary tasks in parallel, without considering the relationship among tasks. Therefore, we introduce a hierarchical architecture to explicitly model the correlations between tasks. We conduct extensive experiments across various benchmark datasets, demonstrating that MI-DELIGHT significantly surpasses previous competitive models. It even outperforms popular large language models on several datasets.

# Introduction

Text classification is a fundamental task in natural language processing (NLP). As a special form of text, short texts often appear in our daily life in the form of tweets, queries, and news feeds (Phan, Nguyen, and Horiguchi 2008). To deal with these short texts, short text classification (STC), as a subtask of text classification, has attracted extensive attention from the research community. It is widely used in various practical scenarios, such as news classification (Dilrukshi, De Zoysa, and Caldera 2013), sentiment analysis (Chen et al. 2019), and query intent classification (Wang et al. 2017). It is worth noting that compared to traditional text classification, STC is particularly nontrivial, which is mainly attributed to its two well-known challenges, i.e., semantic sparsity and limited labeled texts (Hu et al. 2019).

For the challenge of semantic sparsity, short texts typically contain only one or two sentences with few words, which have limited available contextual information (Tang, Qu, and Mei 2015). Such severe semantic sparsity often leads to vagueness and ambiguity, thus hindering the accurate understanding of short texts. An effective solution is to explore multi-source information to enrich the context for short texts. On the one hand, we can collect the statistical information and linguistic information contained within short texts. Statistical information is related to the statistics of words that constitute texts, and is often obtained by modeling word co-occurrence and word distribution probabilities in the text (Thilakaratne, Falkner, and Atapattu 2019; Liu et al. 2024d). For example, by analyzing the word statistics in the text, Latent Dirichlet Allocation (LDA) can uncover the topic structure that can enrich the information in short texts. Linguistic information is implicit in the syntax and semantics of texts (Liu et al. 2019). For instance, we can obtain the part-of-speech (POS) information of words to determine their syntactic roles in the text. On the other hand, auxiliary factual information can also be injected to compensate for the missing contextual information (Liu et al. 2023b,a). In this paper, factual information mainly refers to those textrelated entities existing in common sense knowledge graphs (Chen et al. 2019). With such enriched auxiliary information, the learned model can naturally understand the meanings of short texts better.

When faced with the challenge of limited labeled texts in real-world applications, it is often the case that there is a vast amount of easily accessible short texts, but only a small number of labeled data are available (Kenter and De Rijke 2015). In addition, the proportion of unlabeled data is much higher compared to that of long texts. Consequently, deep learning models that rely on large-scale labeled data for training are susceptible to overfitting issues, leading to unsatisfactory performance outcomes. To cope with such issue, on the one hand, some works (Hu et al. 2019; Yang et al. 2021) are mainly devoted to fully utilizing limited labeled short texts. They perform supervised graph learning on the constructed corpus-level graph to learn the textual representations. Nevertheless, the performance of these models is largely influenced by the limited labeled data, as they only provide relatively restricted information. On the other hand, some works (Liu, Qiu, and Huang 2016, 2017) attempt to introduce auxiliary tasks to alleviate the inefficient data problem. They typically design auxiliary tasks and then jointly train these tasks, aiming to enable the knowledge contained in the tasks to be utilized by other tasks, thereby improving the model generalization ability. However, the reliability of auxiliary tasks are questionable, and unreliable auxiliary ones can impair the model performance.

Recently, contrastive learning (CL) has attracted tremendous attention due to its effectiveness in extracting features from unlabeled data and simple mechanism. Using CL for auxiliary feature learning appears to be a promising approach, as it enables the extraction of self-supervised contrastive information from a large corpus of unlabeled texts. Moreover, CL has been extensively demonstrated to function as a dependable auxiliary task for extracting discriminative information (Chen et al. 2022; Pan et al. 2022). By this way, we can simultaneously handle the limitations of the two aforementioned types of models. However, typically, only instance-level contrastive learning (ICL) is previously used for auxiliary feature learning, which regards each instance as a distinct class. The unique positive pair originates from the same instance, and other instances sharing similar underlying semantics are considered negative pairs and pushed apart. Therefore, it is not sufficient to use such an unsupervised ICL approach from a fine-grained perspective alone. We also need to introduce a coarse-grained CL as an auxiliary task, such as cluster-level supervised CL (CCL), which can further enable the aggregation of samples that share intrinsically similar signals from a coarse-grained perspective. Furthermore, previous models that incorporate CL simply combine the losses of the main task and the auxiliary task, performing them in parallel, without adequately considering the significance of a well-structured architecture that facilitates connections among multiple tasks. This approach is deemed unreasonable, as there exists a causal relationship established through the learned features across tasks. In other words, as we progress from ICL to CCL to classification, the growing complexity of the tasks necessitates the acquisition of increasingly sophisticated features, enabling a transition from rudimentary to abstract characteristics.

In this paper, we introduce a novel model called MIDELIGHT that leverages Multi-source Information and Dual-level contrastivE LearnIng for sHort Text classification. On the one hand, graphs, as a basic data structure, possess the desirable characteristics of flexibility and simplicity. As such, we adopt graphs as the uniform representation form for texts with injected information. On the other hand, graph neural networks (GNNs) have a natural advantage in learning from graph data. Meanwhile, in numerous NLP tasks, GNNs have demonstrated superior performance in capturing non-consecutive and long-range word interactions, as well as their powerful representation capabilities for modeling texts. Therefore, we first construct a word graph and a POS graph to explore the statistical and linguistic information contained in short texts. Additionally, we also build an entity graph to introduce supplementary factual information. After obtaining all the information mentioned above by GNNs and extracting rudimentary text features, we design a dual-level CL auxiliary task to assist in obtaining improved text features in a more comprehensive manner. Importantly, we introduce a hierarchical structure to leverage the casual relationships among tasks and extract abstract features step by step. Specifically, we first employ ICL based on the elementary text features to capture the fine-grained contrastive information. Then, we perform CCL based on advanced text features obtained during the ICL process to capture the coarse-grained contrastive information. Finally, we classify high-level text features obtained during the CCL process. In summary, our contributions are as follows:

(1) We propose a novel model, namely MI-DELIGHT, which is capable of modeling short texts and resolving existing semantic sparsity and inefficient labeled samples. (2) We build three types of graphs to explore statistical, linguistic and factual information to compensate for critical context. Moreover, we design a hierarchical dual-level CL auxiliary tasks, including CCL and ICL, to effectively capture multi-grained contrastive information. (3) We conduct diverse experiments, and MI-DELIGHT consistently surpasses other competitive models, including some popular large language models, across several benchmark datasets.

# Related Work

Text Classification: Traditional text classification methods typically first use hand-crafted lexical features (Li et al. 2022), such as BoW and TF-IDF, to represent text and then adopt SVM or Naive Bayes classifiers. With the development of neural networks, deep learning models without feature engineering, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), have become mainstream approaches in this area. Moreover, recent studies (Guan et al. 2021; Liu et al. 2022) have demonstrated the successful application of GNNs to text classification tasks. These models capture word interactions using graph structures and have shown promising results. One line of work involves building corpus graphs, treating both text and words as nodes, and performing classification in a semi-supervised manner (Yao, Mao, and Luo 2019; Liu et al. 2020). Another line of research focuses on constructing a graph for each text and deriving document representations through graph learning on word-word edges (Ding et al. 2020; Liu et al. 2021).

Short Text Classification: STC is a challenging task in which irregular word orders and missing function words hinder the proper understanding of short texts. Some existing and popular approaches attempt to introduce additional information, such as entities or latent topics, to assist with text understanding (Zeng et al. 2018). Some studies (Ye et al. 2020; Wang et al. 2021) have conducted label propagation via graph structures of constructed heterogeneous graphs and yielded notable gains. Further, several models (Su et al. 2022; Liu et al. 2024b) propose leveraging CL on the corpus-level graph for STC and achieves promising results. However, these models only engage in instance-level CL, thus disregarding cluster-level features. Recent popular large language models (LLMs), such as GPT-3.5 (Ouyang et al. 2022) and Llama (Touvron et al. 2023) have been pretrained on massive high-quality data, thus exhibiting excellent understanding of general texts. However, their performance on domain-specific (e.g, medical or legal domains) texts is not as expected (Chang et al. 2023).

Contrastive Learning: CL approaches learn representations by contrasting positive pairs against negative pairs, and have been highly successful in various fields such as NLP (Gao, Yao, and Chen 2021; Wu et al. 2022) and graphs (Liu et al. 2024c,a). Initially, many CL approaches focus on instance discrimination tasks in an unsupervised manner (Caron et al. 2020; Tian, Krishnan, and Isola 2020). The following studies (Khosla et al. 2020; Liu et al. 2024c; Li et al. 2024) explore fully supervised CL, which can explicitly leverage label information, enabling the extraction of more task-relevant information. Some recent studies (Zheng et al. 2021; Huynh et al. 2022) consider similar samples as positive pairs and aim to pull them together. However, these methods mostly focus on unsupervised tasks and do not take advantage of the handful of available instance labels.

# Preliminary

Most modern GNN models follow a recursive neighborhood aggregation scheme, where the representation of a node is iteratively updated by aggregating the features of its neighbors. A classic model is the graph convolutional network (GCN) (Kipf and Welling 2017), which can be defined formally as follows:

$$
\mathbf { H } ^ { ( \ell + 1 ) } = \sigma ( \hat { \mathbf { D } } ^ { - \frac { 1 } { 2 } } \hat { \mathbf { A } } \hat { \mathbf { D } } ^ { - \frac { 1 } { 2 } } \mathbf { H } ^ { ( \ell ) } \mathbf { W } ^ { ( \ell ) } ) ,
$$

where $\mathbf { H } ^ { \left( \ell \right) }$ is the $\ell$ -th output node representation and $\mathbf { H } ^ { ( 0 ) } = \mathbf { X }$ is an initial node embedding. $\bar { \hat { \mathbf { A } } } = \mathbf { A } + \mathbf { I }$ is an adjacency matrix with added self-loops, and $\begin{array} { r } { \hat { \bf D } _ { i i } = \sum _ { j } \hat { \bf A } _ { i j } } \end{array}$ is the corresponding diagonal degree matrix. $\sigma ( \cdot )$ is an activation function such as ReLU and $\mathbf { W } ^ { ( \ell ) }$ is a layer-specific trainable matrix.

# Method

In this section, we present the proposed MI-DELIGHT model for STC. The overall architecture is shown in Fig. 1. We then proceed to elaborate on the key components.

# Multi-Source Information Exploration

Our goal is to develop a model that can efficiently predict the labels of numerous unlabeled texts when trained on a given short text dataset $\mathcal { D }$ with limited labeled samples. To alleviate the issue of semantic sparsity in short texts, we aim to perform multi-source information exploration to maximally utilize statistical and linguistic information from the text itself, as well as factual information from outside.

Statistical Information: As mentioned before, statistical information is related to word statistics in the text. To capture this information, we construct a word graph $\mathcal { G } _ { w } \ =$ $\{ \gamma _ { w } , { \bf X } _ { w } , { \bf A } _ { w } \}$ , where $\mathcal { V } _ { w }$ is the set of word nodes and $\mathbf { A } _ { w } ~ \in ~ \mathbb { R } ^ { | \mathcal { V } _ { w } | \times | \mathcal { V } _ { w } | }$ is the corresponding adjacency matrix determined by point-wise mutual information (PMI), i.e., $\mathbf { A } _ { w , i j } = \operatorname* { m a x } ( \mathrm { P M I } ( v _ { i } , v _ { j } ) , 0 )$ , where $v _ { i } , v _ { j } \in \mathcal { V } _ { w }$ , which is a popular way to measure the word co-occurrence relationship. $\mathbf { X } _ { w } \in \dot { \mathbb { R } } ^ { | \mathcal { V } _ { w } | \times f _ { w } }$ is the feature matrix of all words with $f _ { w }$ -dimensional features. We initialize $\mathbf { X } _ { w }$ as pretrained word embeddings generated by the GloVe method, which explicitly utilizes the global co-occurrence information of words and captures the underlying statistical information. Then, we feed the word graph $\mathcal { G } _ { w }$ into the GCN using Eq.1 to obtain updated node embeddings $\mathbf { H } _ { w }$ with statistical information.

Linguistic Information: This information is necessary for comprehensively understanding short texts, including semantic and syntactic structure. Here, we acquire linguistic information by identifying the syntactic roles of each word in a given text, such as adjectives or adverbs, which helps to eliminate syntactic word ambiguity. To this end, we construct a POS graph ${ \mathcal G } _ { p } ~ = ~ \{ { \mathcal V } _ { p } , { \bf X } _ { p } , { \bf A } _ { p } \}$ , where $\nu _ { p }$ is the formed POS tag node set and $\mathbf { A } _ { p }$ denotes the POS adjacency matrix calculated by PMI, i.e., $\begin{array} { r l } { \mathbf { A } _ { p , i j } } & { { } = } \end{array}$ $\operatorname* { m a x } ( \mathrm { P \tilde { M } I } ( v _ { i } , \tilde { v } _ { j } ) , 0 )$ , where $v _ { i } , v _ { j } \in \ \mathcal { V } _ { p }$ . We initialize the node features $\dot { \mathbf X } _ { t } \in \mathbb R ^ { | \mathcal V _ { t } | \times f _ { t } }$ as one-hot vectors. Similarly, we obtain updated POS tag features $\mathbf { H } _ { p }$ by performing Eq.1. Factual Information: Additional factual information can help supplement the contextual knowledge required for short texts to enhance the classification ability of subsequent models. Therefore, we extract the entities in the short text that are resided in the knowledge graph and construct an entity graph ${ \mathcal G } _ { e } ~ = ~ \{ { \mathcal V } _ { e } , { \bf X } _ { e } , { \bf A } _ { e } \}$ . Here, we utilize the TAGME tool for entity linking on the NELL (Carlson et al. 2010) knowledge graph. $\nu _ { e }$ is the entity node set. The entities embeddings $\mathbf { \bar { X } } _ { e } \in \mathbb { R } ^ { | \mathcal { V } _ { e } | \times f _ { e } }$ are initialized by TransE (Bordes et al. 2013). $\mathbf { A } _ { e }$ is the entity adjacency matrix derived by the cosine similarity of each entity pair, i.e., $\mathbf { A } _ { e , i j } ~ =$ $\operatorname* { m a x } ( \cos ( \mathbf { X } _ { e , i } , \mathbf { X } _ { e , j } ) , 0 )$ . The updated entity node embeddings ${ \bf { H } } _ { e }$ are also obtained by performing Eq.1.

# Text Representation Learning

Given three types of graphs $\mathcal { G } = \{ \mathcal { G } _ { \pi } \} , \pi \in \{ w , e , p \}$ , to obtain text embeddings, we employ the following information aggregation strategy:

$$
\begin{array} { r l } & { \mathbf { Z } _ { \pi } = \mathbf { P } _ { \pi } \mathbf { H } _ { \pi } , } \\ & { \mathbf { Z } _ { \pi } = \mathbf { Z } _ { \pi } / | | \mathbf { Z } _ { \pi } | | _ { 2 } , \pi \in \{ w , e , p \} , } \end{array}
$$

where $\mathbf { H } _ { \pi }$ denotes the updated node embeddings of $\mathcal { G } _ { \pi }$ obtained via a 2-layer GCN. We set $\mathbf { P } _ { \pi } ~ \in ~ \mathbb { R } ^ { N \times | \mathcal { V } _ { \pi } | }$ as the TF-IDF value between each text and word or POS tag of the corpus when $\pi \in \{ w , p \} . \ N$ denotes the number of short texts. Moreover, when $\pi = e$ , we make ${ \bf P } _ { e , i j } = 1$ if the $i$ -th text contains the $j$ -th entity and 0 otherwise. After the normalized text-relevant features $\mathbf { Z } _ { w }$ , $\mathbf { Z } _ { e }$ , and $\mathbf { Z } _ { p }$ are derived, we concatenate them to obtain the text embeddings, i.e., $\mathbf { Z } = \mathbf { Z } _ { w } | | \mathbf { Z } _ { e } | | \mathbf { Z } _ { p }$ .

# Data Augmentation

A key step for applying CL to NLP is to construct positive sample pairs. A typical approach for generating positive samples is a data augmentation technique, such as backtranslation (Edunov et al. 2018), random noise injection (Xie

![](images/afb90d196276321b6643deef76a2f86605eeb69205eeda53e759a7855af6ec14.jpg)  
Figure 1: The overall architecture of MI-DELIGHT. We first generate augmented samples for the input texts. Then, the original corpus Dorg = diorg	iN= 1 and the augmented corpus Daug = diaug	iN=1 are used to construct a word graph $\mathcal { G } _ { w }$ , a POS graph $\mathcal { G } _ { p }$ and an entity graph $\mathcal { G } _ { e }$ , and the text embeddings are obtained via the text representation learning module. Finally, these embeddings are mapped through different projection heads into different hidden spaces to which ICL, CCL, and cross-entropy (CE) are applied in a certain hierarchical order. From ICL to CCL and then to CE, the task complexity keeps increasing, and the features keep more abstract. Here, feature specificity represents the abstraction level of features.

et al. 2017), and word substitution (Wei and Zou 2019). Here, we augment the original data by replacing its words with WordNet synonyms. Formally, for each text $d _ { i } ^ { \mathrm { { o r g } } }$ in the original corpus Dorg = $\mathcal { D } _ { \mathrm { o r g } } ~ = ~ \left\{ d _ { i } ^ { \mathrm { o r g } } \right\} _ { i = 1 } ^ { N }$ augmented text $d _ { i } ^ { \mathrm { a u g } } \ = \ \mathrm { a u g } \left( d _ { i } ^ { \mathrm { o r g } } \right)$ and augmented corpus Daug = diaug	iN=1. We denote the overall corpus and the corresponding text embeddings as $\mathcal { D } = \mathcal { D } _ { \mathrm { o r g } } \cup \mathcal { D } _ { \mathrm { a u g } }$ and $\mathbf { Z } = \mathbf { Z } ^ { \mathrm { o r g } } \cup \mathbf { Z } ^ { \mathrm { a u g } }$ , respectively. Note that our model is feasible for data augmentations, and we explore the impacts of different data augmentations on the model in the experiment section.

# Hierarchical Structure among Tasks

In contrast to prior models, we have implemented a hierarchical structure to explicitly account for the relationship established through distinct stages of learned text features among the primary classification task and the auxiliary CL tasks. First, we utilize the rudimentary features acquired during the multi-source information exploration stage to perform ICL, enabling us to capture fine-grained contrastive information. Then, based on the intermediate features obtained at the ICL stage, we perform CCL to capture coarse-grained contrastive information. Finally, leveraging abstract features obtained at the CCL stage, we carry out the ultimate classification task.

Instance-Level Contrastive Learning: First, based on the rudimentary text features $\mathbf { z }$ , we leverage ICL to perform instance discrimination tasks to explore fine-grained contrastive information. Typically, two texts from the same source data exhibit similar meanings. Their encoded textlevel embeddings should be as similar as possible in the latent space. We refer to $d _ { i }$ and the augmented version $d _ { j }$ as a pair of positive samples while treating the other $2 ( N - 1 )$

texts in $\mathcal { D }$ as negative samples to this positive pair, which should be far away from the positive samples. With the obtained text embeddings $\mathbf { Z }$ , we perform the normalization operation on them, i.e., $\tilde { \textbf { Z } } = \textbf { Z } / | | \textbf { Z } | | _ { 2 }$ . Notably, we do not map $\mathbf { Z }$ to a hidden space through a projection head as in traditional CL. Due to the fine-grained information required by ICL, introducing a projection head would not only compromise the semantics but also introduce more parameters. Therefore, we avoid using a projection head in this stage. The objective function for a positive pair of examples $( d _ { i } , d _ { j } )$ is defined as follows:

$$
\mathcal { L } _ { i } ^ { \mathrm { I C L } } = - \log \frac { \exp ( ( \tilde { \mathbf { Z } } _ { i } \cdot \tilde { \mathbf { Z } } _ { j } ) / \tau ) } { \sum _ { k = 1 } ^ { 2 N } \mathbb { I } _ { k \neq i } \exp ( ( \tilde { \mathbf { Z } } _ { i } \cdot \tilde { \mathbf { Z } } _ { k } ) / \tau ) } ,
$$

where $\tilde { \mathbf { Z } } _ { i }$ and $\tilde { \mathbf { Z } } _ { j }$ denote the output embeddings of the $i$ -th text and its augmented text, respectively. ${ \mathbb I } _ { k \neq i }$ is an indicator function set to 1 if $k \neq i$ , and $\tau$ denotes the temperature parameter.

The ICL loss is computed by averaging over all positive pairs on $\mathcal { D }$ , which is expressed as:

$$
\mathcal { L } ^ { \mathrm { I C L } } = \frac { 1 } { 2 N } \sum _ { i = 1 } ^ { 2 N } \mathcal { L } _ { i } ^ { \mathrm { I C L } } .
$$

Cluster-Level Contrastive Learning: Next, based on the intermediate text features $\tilde { \mathbf { Z } }$ derived in the ICL stage corresponding to the corpus $\mathcal { D }$ , we perform CCL. We expect to assign pseudo-cluster labels to the data in the corpus to explore their similarities from a cluster perspective and exploit their high-level feature clustering information such that similar instances can be pulled together. For ease of the presentation, we denote $\tilde { \mathbf { Z } } ^ { * }$ as the original or augmented text features from $\mathcal { D } _ { * }$ , where $*$ stands for “org” or “aug”. We leverage the scores computed by the cosine similarity function to build relations between different texts. Next, we define a text $d _ { j } ^ { * }$ as the nearest neighbor of text $d _ { i } ^ { * }$ when Eq.5 is satisfied.

$$
\begin{array} { r } { \mathrm { n e a r } ( d _ { i } ^ { * } ) = \arg \operatorname* { m a x } _ { { d _ { j } ^ { * } } } \cos ( \tilde { \mathbf { Z } } _ { i } ^ { * } , \tilde { \mathbf { Z } } _ { j } ^ { * } ) } \\ { s . t . \quad \forall d _ { j } ^ { * } \in \mathcal { D } _ { * } \wedge j \neq i . } \end{array}
$$

The text $d _ { i } ^ { * }$ is connected with the text $d _ { j } ^ { * }$ if near $\mathbf { \left( \boldsymbol { d } _ { i } ^ { * } \right) _ { \tau } } =$ $d _ { j } ^ { * }$ or nea $\mathfrak { r } ( d _ { j } ^ { * } ) ~ = ~ d _ { i } ^ { * }$ . After performing the above operation, we can acquire the symmetric connections within $\mathcal { D } _ { * }$ . Then, we utilize the connected component labeling algorithm (Di Stefano and Bulgarelli 1999) to assign the corresponding pseudo-cluster label for each derived component. Since any two instances in a component can be connected by paths, we treat its internal instances as similar. Subsequently, we can obtain the label matrix $\mathbf { Y ^ { * } }$ . $\mathbf { Y } _ { i j } ^ { * }$ is set to 1 if $d _ { i } ^ { * }$ and $d _ { j } ^ { * }$ are in the same component.

Subsequently, we adopt a projection head $\Phi ( \cdot )$ , which maps the representations $\tilde { \mathbf { Z } }$ to a hidden space where the CCL loss is applied, i.e., ${ \bf U } = \Phi ( { \tilde { \bf Z } } )$ . Due to the different target granularities, there may be potential conflicts in the feature space between CCL and ICL, thus requiring a projection head. Here, the dimension of U is half the dimension of $\tilde { \mathbf { Z } }$ . Next, we normalize the output into a unit form, i.e., $\tilde { \textbf { U } } = \textbf { U } / | | \textbf { U } | | _ { 2 }$ . Moreover, intuitively, since $d _ { i } ^ { \mathrm { { o r g } } }$ and $d _ { i } ^ { \mathrm { a u g } }$ have the sa||me||meanings, the labels $\mathbf { Y } _ { i } ^ { \mathrm { { o r g } } }$ and $\mathbf { Y } _ { i } ^ { \mathrm { a u g } }$ should ibe consistent. We can swap supervision signals between them. Another crucial reason for swapping supervision is that the dot product value of the positive samples in the same class may be large, which leads to a potentially small ${ \mathcal { L } } _ { i } ^ { \mathrm { C C L } }$ under standard CL settings, thus affecting model optimization. The detailed CCL loss with swapped supervision is defined as follows:

$$
\begin{array} { r l } & { \mathcal { L } ^ { \mathrm { C C L } } = \displaystyle \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \mathcal { L } _ { i } ^ { \mathrm { C C L } } , } \\ & { \mathcal { L } _ { i } ^ { \mathrm { C C L } } = \mathcal { L } _ { i } ^ { \mathrm { s w a p } } ( \mathbf { U } _ { \mathrm { o r g } , i } , \mathbf { Y } _ { i } ^ { \mathrm { a u g } } ) + \mathcal { L } _ { i } ^ { \mathrm { s w a p } } ( \mathbf { U } _ { \mathrm { a u g } , i } , \mathbf { Y } _ { i } ^ { \mathrm { o r g } } ) , } \\ & { \mathcal { L } _ { i } ^ { \mathrm { s w a p } } = - \displaystyle \sum _ { j } ^ { N } \mathbb { I } _ { \mathbf { Y } _ { i j } = 1 } \log \frac { \exp ( \mathbf { U } _ { i } \cdot \mathbf { U } _ { j } / \tau ) } { \sum _ { k = 1 } ^ { N } \mathbb { I } _ { k \neq i } \exp ( \mathbf { U } _ { i } \cdot \mathbf { U } _ { k } / \tau ) } , } \end{array}
$$

where $\tau$ symbolizes the temperature parameter and “ ” denotes the dot product operator. $\mathbb { I } _ { \mathbf { Y } _ { i j } = 1 }$ aims to find texts with the same label as that of the $i$ -th text.

In these two applied components, ICL can provide beneficial information for the subsequent CCL task, while CCL can offer further guidance for the final classification task. Moreover, they can form a complementary relationship: ICL can provide a certain degree of variance in the obtained features, which can prevent the feature variability collapse. CCL has the capacity to mitigate class collision to a certain extent.

Classification Task: Finally, leveraging the abstract text features $\tilde { \textbf { U } }$ derived from the CCL process, we perform the final classification task. We adopt an extra projection head $\Psi ( \cdot )$ with the same structure as that of $\Phi ( \cdot )$ to map the text embeddings $\tilde { \textbf { U } }$ to another latent space, i.e., ${ \bf Q } \ = \ \Psi ( \tilde { \bf U } )$ . Here, the dimension of $\mathbf { Q }$ is the number of classes. Then, we make predictions of these labeled data by performing a linear transformation followed by a ReLU activation on their hidden features. We specify the loss function as the commonly used cross-entropy function. Formally, the above operations can be expressed as:

$$
\mathcal { L } ^ { \mathrm { C E } } = - \frac { 1 } { \vert \mathcal { D } _ { \mathrm { t r a i n } } \vert } \sum _ { i \in \mathcal { D } _ { \mathrm { t r a i n } } } \sum _ { j } ^ { c } \mathcal { V } _ { i j } \log \mathbf { Q } _ { i j }
$$

where $\mathcal { D } _ { \mathrm { t r a i n } }$ is the set of training data from ${ \mathcal { D } } _ { \mathrm { o r g } }$ and $y$ is the one-hot vector of the ground-truth label of the training data. $c$ is the number of classes.

The adopted hierarchical architecture of tasks is similar to multi-task learning (Zhang and Yang 2021), which offers several advantages. First, by gradually progressing from ICL to CCL and then to classification, the model can extract more abstract and high-level features, which helps improve its generalization ability. Second, through step-by-step learning, the model can better utilize the data, as the results of the previous stage can provide better initialization and guidance for the subsequent stage. This inter-task correlation is beneficial for the model learning.

# Model Optimation

Overall, the final loss function of our proposed model is the combination of the classification loss $\mathcal { L } ^ { \mathrm { \bar { C E } } }$ , ICL loss ${ \mathcal { L } } ^ { \mathrm { I C L } }$ , and CCL loss ${ \mathcal { L } } ^ { \mathrm { C C L } }$ , which is formulated as follows:

$$
\begin{array} { r } { \mathcal { L } = \mathcal { L } ^ { \mathrm { C E } } + \eta \mathcal { L } ^ { \mathrm { I C L } } + \zeta \mathcal { L } ^ { \mathrm { C C L } } , } \end{array}
$$

where $\eta$ and $\zeta$ are hyperparameters that control the proportions of different losses.

During the model inference, we feed the obtained test text embeddings to the classification head $\Upsilon ( \cdot )$ to evaluate the model performance.

# Experiment

Datasets: We perform experiments on real-world STC datasets employed in earlier studies (Hu et al. 2019; Wang et al. 2021), i.e., Twitter, MR (Pang and Lee 2005), Snippets (Phan, Nguyen, and Horiguchi 2008), Ohsumed (Hersh et al. 1994), and TagMyNews (Vitale, Ferragina, and Scaiella 2012). The statistics of these datasets are summarized in Table 1.

The preprocessing for these datasets is consistent with previous studies, including the removal of non-English characters, stop words, and infrequent words with counts of less than five. Following previous studies (Hu et al. 2019), we randomly sample 40 labeled short documents per class, half of which form the training set, and the other half form the validation set. The remaining data constitute the test set, and their labels are invisible during training.

Baselines: We select four types of baseline models for comparison. (1) Traditional models include $\mathbf { T F - I D F + S V M }$ and PTE (Tang, Qu, and Mei 2015). (2) Deep learning models contain CNNs (Kim 2014), LSTM (Liu et al. 2015) and BERT (Devlin et al. 2019). Here, BERT-avg and BERT-cls denote the text embeddings represented by the average word embeddings and the token CLS embedding, respectively.

<html><body><table><tr><td>Dataset</td><td>#Docs</td><td>#Train (ratio)</td><td>#Words</td><td>#Entities</td><td>#Tags</td><td>Avg.Len</td><td>#Classes</td></tr><tr><td>Twitter</td><td>10,000</td><td>40 (0.40%)</td><td>21,065</td><td>5,837</td><td>41</td><td>3.5</td><td>2</td></tr><tr><td>MR</td><td>10,662</td><td>40 (0.38%)</td><td>18,764</td><td>6,415</td><td>41</td><td>7.6</td><td>2</td></tr><tr><td>Snippets</td><td>12,340</td><td>160 (1.30%)</td><td>29,040</td><td>9,737</td><td>34</td><td>14.5</td><td>8</td></tr><tr><td>Ohsumed</td><td>7,400</td><td>460 (6.22%)</td><td>11,764</td><td>4,507</td><td>38</td><td>6.8</td><td>23</td></tr><tr><td>TagMyNews</td><td>32.549</td><td>140 (0.43%)</td><td>38.629</td><td>14,734</td><td>42</td><td>5.1</td><td>7</td></tr></table></body></html>

Table 1: Summary statistics of the evaluation datasets.

(3) GNN-based models consist of TLGNN (Huang et al. 2019), HyperGAT (Ding et al. 2020), TextING (Zhang et al. 2020), DADGNN (Liu et al. 2021), and TextGCN (Yao, Mao, and Luo 2019). (4) Deep short text models include STCKA (Chen et al. 2019), HGAT (Hu et al. 2019), STGCN (Ye et al. 2020), SHINE (Wang et al. 2021), NCHGAT (Su et al. 2022), and GIFT (Liu et al. 2024b). Notably, we also provide several large language models, containing GPT-3.5 (Ouyang et al. 2022), Bloom-7.1B (Scao et al. 2022), Llama2-7B (Touvron et al. 2023), and Llama3- 8B (AI $@$ Meta 2024). Due to computational resource constraints, we only fine-tune approximately 7B LLMs through some GPU reduction techniques.

Evaluation Metric: We use the accuracy (ACC) and macroF1 score (F1) to evaluate the model performance, which are widely adopted by previous studies (Hu et al. 2019). All experiments are repeated ten times to obtain average metrics.

# Result

Model Performance: Table 2 indicates that MI-DELIGHT achieves competitive performance across several datasets by a large margin in terms of accuracy and macro-F1 score. A key factor contributing to the remarkable superiority of MIDELIGHT over other competing models lies in its deliberate design of a dual CL auxiliary task. This task serves the purpose of acquiring informative text representations and effectively capturing contrastive information at various levels. Specifically, the ICL and CCL within this framework enable the model to discern fine-grained details while also considering broader patterns and contexts. Moreover, the introduced hierarchical concept can help the model learn step by step, while fully utilizing the inter-task correlations, thereby enhancing the overall model performance. Moreover, we construct three types of graphs, including a word graph, a POS graph, and an entity graph, to incorporate statistical, linguistic and factual knowledge, which exploits semantic and syntactic information from the text and additional information from outside. All of the above operations are beneficial for better identifying the correct meanings of short texts.

We find that MI-DELIGHT basically achieves greater improvements on the Snippets, Ohsumed, and TagMyNews, which even considerably surpasses LLMs on these datasets. We attribute this phenomenon to the fact that the more unlabeled texts exist, the better MI-DELIGHT can extract useful self-supervised signals from them. LLMs have limited understanding in specific-domain (e.g., medical domain) texts. However, LLMs achieve satisfactory performance in general texts such as Twitter and MR. This is because they are pretrained on vast amounts of high-quality data and have a large number of parameters. Moreover, they may have already encountered part of the test data.

Model Variants: To assess the effectiveness of each part of MI-DELIGHT, we design the following model variants to perform ablation experiments. (1) w/o word graph: We remove the word graph that introduces statistical knowledge. (2) w/o POS graph: We exclude the POS graph that incorporates linguistic knowledge. (3) w/o entity graph: We delete the entity graph that contains factual knowledge. (4) w/o CCL and ICL: We remove CCL and ICL simultaneously, leaving the text representation learning module which is combined with the cross-entropy loss for optimization. (5) w/o CCL: We eliminate the CCL module to demonstrate the role of the ICL module. (6) w/o ICL: We exclude the ICL module to confirm the efficiency of the CCL module. (7) parallel: We simply add projection heads for all tasks and perform them in parallel. We obtain several findings by observing the results presented in the first seven rows shown in Table 3. First, when we delete any part of the model, the performance of MI-DELIGHT decreases significantly, illustrating that each part plays an essential role in our model. Second, three constructed graphs used to enrich the short text information bring different types of information that play an indispensable role. Since the word graph can provide the most fundamental semantic information, removing it would significantly reduce the model’s performance. Third, both the CCL and ICL modules are designed to allow the model to learn more discriminative text representations. Finally, our proposed hierarchical architecture is superior to the parallel version, since it fully utilizes the inter-task correlations.

Moreover, we explore the impacts of different approaches for generating positive sample pairs in the model. (1) MIDELIGHT (deletion): It randomly deletes a fraction of the words in a given sentence to generate an enhanced positive sample. (2) MI-DELIGHT (context): It leverages pretrained large-scale language models (e.g., BERT) to find a portion of the input text with suitable words for substitution. (3) MI-DELIGHT (WordNet): It generates augmented positive pairs by replacing words of an input text with WordNet synonyms, which is the default experimental setting. The empirical results are shown in the last three rows of Table 3. As expected, the relevant metrics obtained across all the datasets drastically decrease when we adopt the deletion method for augmenting original texts. A plausible reason is deleting keywords from the original sentence changes its semantic information.

Table 2: Results $( \% )$ of several the Accuracy and Macro-F1 score on several short text datasets. We highlight the best performance in bold excluding the LLMs based on the pairwise t-test with $9 5 \%$ confidence.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="2">Twitter</td><td colspan="2">MR</td><td colspan="2">Snippets</td><td colspan="2">Ohsumed</td><td colspan="2">TagMyNews</td></tr><tr><td>ACC</td><td>F1</td><td>ACC</td><td>F1</td><td>ACC</td><td>F1</td><td>ACC</td><td>F1</td><td>ACC</td><td>F1</td></tr><tr><td>TF-IDF+SVM</td><td>53.62</td><td>52.46</td><td>54.29 55.02</td><td>48.13</td><td>64.70</td><td>59.17</td><td>39.02</td><td>24.78</td><td>39.91</td><td>32.05</td></tr><tr><td>PTE CNN</td><td>54.24</td><td>53.17 56.02</td><td>59.06</td><td>52.62 59.01</td><td>63.10</td><td>59.11 69.28</td><td>38.29</td><td>22.27 12.06</td><td>40.39 57.12</td><td>34.12 45.37</td></tr><tr><td rowspan="3">LSTM BERT-avg</td><td>57.29</td><td></td><td>60.89</td><td>60.70</td><td>77.09</td><td></td><td>32.92</td><td>7.20</td><td>57.32</td><td>45.56</td></tr><tr><td>60.28</td><td>60.22</td><td>51.69</td><td></td><td>75.89</td><td>67.72</td><td>28.86</td><td>5.65</td><td></td><td></td></tr><tr><td>54.92 52.06</td><td>51.16 43.41</td><td>53.50</td><td>50.65 47.02</td><td>79.31 81.55</td><td>78.47 79.06</td><td>24.29 22.26</td><td>5.50</td><td>55.11 58.19</td><td>44.31 42.35</td></tr><tr><td>BERT-cls TLGNN</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HyperGAT</td><td>59.02</td><td>54.56</td><td>59.22</td><td>59.36</td><td>70.25</td><td>63.29</td><td>35.76</td><td>13.12</td><td>45.25</td><td>33.52</td></tr><tr><td>TextING</td><td>59.15</td><td>55.19</td><td>58.65</td><td>58.62</td><td>70.89</td><td>63.42</td><td>36.60</td><td>20.02</td><td>45.60</td><td>31.51</td></tr><tr><td>DADGNN</td><td>59.62</td><td>59.22</td><td>58.89</td><td>58.76</td><td>71.10</td><td>70.65</td><td>38.26</td><td>21.35</td><td>52.10</td><td>39.99</td></tr><tr><td>TextGCN</td><td>59.51 60.15</td><td>55.32 59.82</td><td>58.92 59.12</td><td>58.86 58.98</td><td>71.65 77.82</td><td>70.66 71.95</td><td>37.65 41.56</td><td>22.16 27.43</td><td>47.96 54.28</td><td>39.25</td></tr><tr><td>STCKA</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>46.01</td></tr><tr><td>HGAT</td><td>57.56 63.21</td><td>57.02 62.48</td><td>53.25 62.75</td><td>51.19 62.36</td><td>68.96 82.36</td><td>61.27 74.44</td><td>32.20 42.68</td><td>12.25 24.82</td><td>32.15</td><td>23.26</td></tr><tr><td>STGCN</td><td>64.33</td><td>64.29</td><td>58.25</td><td>58.22</td><td></td><td></td><td></td><td></td><td>61.72</td><td>53.81</td></tr><tr><td>SHINE</td><td>72.54</td><td>72.19</td><td>64.58</td><td>63.89</td><td>70.01 82.39</td><td>69.93 81.62</td><td>35.22 45.57</td><td>28.30 30.98</td><td>35.65</td><td>35.16</td></tr><tr><td>NC-HGAT</td><td>63.76</td><td></td><td>62.46</td><td>62.14</td><td></td><td></td><td></td><td></td><td>62.50</td><td>56.21</td></tr><tr><td>GIFT</td><td>73.16</td><td>62.94 73.16</td><td>65.21</td><td>65.21</td><td>82.42 83.73</td><td>74.62 82.35</td><td>43.27 45.62</td><td>27.98 31.25</td><td>62.15 63.26</td><td>55.02 56.92</td></tr><tr><td>Ours</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>75.11</td><td>75.06</td><td>66.49</td><td>66.47</td><td>87.90</td><td>86.84</td><td>48.56</td><td>33.20</td><td>69.72</td><td>65.94</td></tr><tr><td>GPT-3.5</td><td>81.23</td><td>80.02</td><td>87.43</td><td>86.62</td><td>66.52</td><td>63.48</td><td>47.98</td><td>32.49</td><td>61.43</td><td>54.79</td></tr><tr><td>Bloom-7.1B</td><td>87.52</td><td>86.56</td><td>87.03</td><td>86.96</td><td>71.39</td><td>60.76</td><td>37.46</td><td>30.12</td><td>66.13</td><td>62.13</td></tr><tr><td>Llama2-7B</td><td>87.45</td><td>86.43</td><td>87.26</td><td>86.69</td><td>73.05</td><td>68.11</td><td>42.16</td><td>30.19</td><td>67.31</td><td>64.33</td></tr><tr><td>Llama3-8B</td><td>89.50</td><td>89.47</td><td>84.38</td><td>84.16</td><td>61.68</td><td>60.62</td><td>38.76</td><td>22.93</td><td>65.80</td><td>60.69</td></tr></table></body></html>

<html><body><table><tr><td rowspan="2">Model</td><td colspan="2">Twitter</td><td colspan="2">MR</td><td colspan="2">Snippets</td><td colspan="2">Ohsumed</td><td colspan="2">TagMyNews</td></tr><tr><td>ACC</td><td>F1</td><td>ACC</td><td>F1</td><td>ACC</td><td>F1</td><td>ACC</td><td>F1</td><td>ACC</td><td>F1</td></tr><tr><td>w/o word graph</td><td>62.60</td><td>62.10</td><td>55.02</td><td>54.96</td><td>76.15</td><td>74.80</td><td>29.08</td><td>21.26</td><td>62.52</td><td>57.19</td></tr><tr><td>w/o POS graph</td><td>72.40</td><td>71.65</td><td>65.46</td><td>65.25</td><td>85.86</td><td>85.19</td><td>46.96</td><td>28.19</td><td>66.39</td><td>59.10</td></tr><tr><td>w/o entity graph</td><td>70.42</td><td>70.36</td><td>64.76</td><td>64.79</td><td>85.95</td><td>85.12</td><td>47.55</td><td>29.98</td><td>67.28</td><td>62.90</td></tr><tr><td>w/o CCL and ICL</td><td>72.57</td><td>72.26</td><td>62.33</td><td>62.32</td><td>86.12</td><td>83.89</td><td>45.98</td><td>27.53</td><td>66.38</td><td>62.10</td></tr><tr><td>w/o CCL</td><td>72.19</td><td>72.18</td><td>66.02</td><td>65.92</td><td>85.34</td><td>83.99</td><td>48.38</td><td>31.59</td><td>66.56</td><td>62.45</td></tr><tr><td>w/o ICL</td><td>73.91</td><td>73.60</td><td>65.43</td><td>65.29</td><td>85.79</td><td>83.48</td><td>46.29</td><td>28.96</td><td>66.41</td><td>62.34</td></tr><tr><td>parallel</td><td>73.74</td><td>73.72</td><td>65.32</td><td>65.30</td><td>84.54</td><td>83.82</td><td>48.51</td><td>31.82</td><td>68.72</td><td>64.94</td></tr><tr><td>MI-DELIGHT(deletion)</td><td>72.04</td><td>71.92</td><td>63.52</td><td>63.50</td><td>83.98</td><td>82.26</td><td>44.59</td><td>27.92</td><td>67.44</td><td>62.67</td></tr><tr><td>MI-DELIGHT (context)</td><td>75.66</td><td>75.56</td><td>65.68</td><td>65.62</td><td>84.01</td><td>82.66</td><td>44.29</td><td>28.25</td><td>67.72</td><td>63.28</td></tr><tr><td>MI-DELIGHT (WordNet)</td><td>75.11</td><td>75.06</td><td>66.49</td><td>66.47</td><td>87.90</td><td>86.84</td><td>48.56</td><td>32.20</td><td>69.72</td><td>65.94</td></tr></table></body></html>

Table 3: The ablation and different text augmentation results $( \% )$ of various experimental settings.

# Conclusion

In this work, we propose a novel model named MIDELIGHT for STC. We build three types of graphs to introduce the statistical, linguistic, and factual information for enriching short texts. Also, we design a dual-level CL auxiliary tasks to capture multi-grained contrastive information. Moreover, we leverage a hierarchical structure to capture inter-task correlations. The empirical results reveal that MIDELIGHT consistently outperforms other baselines, including some popular LLMs, across several datasets.

# Code — https://github.com/KEAML-JLU/MI-DELIGHT