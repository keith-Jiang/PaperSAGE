# Multi-Granularity Open Intent Classification via Adaptive Granular-Ball Decision Boundary

Yanhua Li1,2, Xiaocao Ouyang1,2, Chaofan Pan1,2, Jie Zhang1,2, Sen Zhao3, Shuyin Xia3, Xin $\mathbf { Y a n g ^ { 1 , 2 * } }$ , Guoyin $\mathbf { W a n g ^ { 4 } }$ , Tianrui Li5

1School of Computing and Artificial Intelligence, Southwestern University of Finance and Economics, Chengdu 611130, China 2Engineering Research Center of Intelligent Finance, Ministry of Education, Chengdu 611130, China 3Key Laboratory of Big Data Intelligent Computing, Chongqing Key Laboratory of Computational Intelligence, Chongqing University of Posts and Telecommunications, Chongqing 400065, China 4National Center for Applied Mathematics in Chongqing, Chongqing Normal University, Chongqing 401331, China 5School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu 611756, China yyhhliyanhua@163.com, oyxiaocao $@$ swufe.edu.cn, pan.chaofan $@$ foxmail.com, jiezhanglx $@$ gmail.com, zhaosen@cqupt.edu.cn, xiasy $@$ cqupt.edu.cn, yangxin $@$ swufe.edu.cn, wanggy $@$ cqnu.edu.cn, trli@swjtu.edu.cn

# Abstract

Open intent classification is critical for the development of dialogue systems, aiming to accurately classify known intents into their corresponding classes while identifying unknown intents. Prior boundary-based methods assumed known intents fit within compact spherical regions, focusing on coarsegrained representation and precise spherical decision boundaries. However, these assumptions are often violated in practical scenarios, making it difficult to distinguish known intent classes from unknowns using a single spherical boundary. To tackle these issues, we propose a Multi-granularity Open intent classification method via adaptive Granular-Ball decision boundary (MOGB). Our MOGB method consists of two modules: representation learning and decision boundary acquiring. To effectively represent the intent distribution, we design a hierarchical representation learning method. This involves iteratively alternating between adaptive granular-ball clustering and nearest sub-centroid classification to capture fine-grained semantic structures within known intent classes. Furthermore, multi-granularity decision boundaries are constructed for open intent classification by employing granularballs with varying centroids and radii. Extensive experiments conducted on three public datasets demonstrate the effectiveness of our proposed method.

Code ‚Äî https://github.com/Liyanhuaa/MOGB

# Introduction

The identification of user intent is critical for dialogue systems, especially as individuals increasingly rely on these systems to support both daily activities and professional tasks. In real-world applications, dialogue systems often encounter intents from unknown (open) classes that they have not seen during training, resulting in their inability to handle these intents effectively (Zheng, Chen, and Huang 2020). To address this, open intent classification aims to classify known intents while identifying unknown intents (Shu, Xu, and Liu 2017).

![](images/a481eb2dfa5d35b4361e313a5e684803849bdad142d8870aec774056236e9819.jpg)  
Figure 1: (a) Previous boundary-based methods struggle to learn a boundary for open intent classification by balancing open space risk and empirical risk. (b) Our proposed Multigranularity decision boundary can effectively eliminate both two risks.

Recent researches endeavor to develop effective methods to tackle the issue of open intent classification. Early methods (Jain, Scheirer, and Boult 2014; Hendrycks and Gimpel 2022) focused on establishing a $K$ -class classifier and determining whether a sample belongs to unknowns by checking if its maximum probability exceeds a certain threshold. Due to the overfitting of deep learning approaches (Guo et al. 2017), $K$ -class classifiers tend to overconfidence about known intent classes and are prone to misclassifying unknown intents as known ones. To overcome this limitation, some studies train a $K { + } 1$ classifier directly, leveraging pseudo-unknown data to represent the open intents (Zheng, Chen, and Huang 2020; Cheng et al. 2022).

In recent years, boundary-based methods (Zhang, Xu, and Lin 2021; Zhang et al. 2023a; Yang et al. 2022) have been proposed to learn specific decision boundaries between known intents and unknowns. These methods often involve two stages, i.e. representation learning and boundary learning. They mainly focused on learning more accurate decision boundaries to balance open space risk (classify unknowns as known) and empirical risk (classify known as unknowns). Initially, Zhang et al. (Zhang, Xu, and Lin 2021) acquired feature inputs through softmax-based representation learning and obtained adaptive decision boundary (ADB) for each known intent class. Following this work, Liu et al. (Liu et al. 2023) designed a K-center contrastive learning algorithm to learn discriminative intent representations and proposed adjustable boundary learning by expanding or shrinking the initial decision boundary. Zhang et al. (Zhang et al. 2023a) learned distance-aware intent representations and tight adaptive decision boundaries to improve open intent detection.

Existing boundary-based methods (Zhang, Xu, and Lin 2021; Zhang et al. 2023a) implicitly assumed the features of known intents distributed in a compact spherical region, while intents lying outside this distribution are regarded as open intents. It means that open intents do not exist within the distribution of a single known intent class and only appear between different known intent classes. However, as illustrated in Figure 1 (a), in real-world situations, known intents do not consistently adhere to a compact spherical distribution and open intents can exist between or within the distribution of known intents (named inter-open intents and intra-open intents). The investigation of Gaussian Hypothesis Testing concerning known intents distribution, as presented by Zhou et al. (Zhou, Liu, and Qiu 2022), revealed that merely $57 \%$ of the known intents within the CLINC-FULL dataset conform to the Gaussian assumption. This finding further implies that the assumption of a spherical feature distribution is unreasonable. When dealing with non-spherical intent distributions, it is difficult to distinguish known intents from unknowns using just one decision boundary. Furthermore, the open space risk resulting from intra-open intents can never be eliminated, regardless of how to adjust the boundary, as shown in Figure 1 (a).

In this paper, we propose a new straightforward but powerful solution: exploring the patterns within each known intent class, dividing intents into multiple sub-parts, and establishing decision boundaries for each sub-part, as illustrated in Figure 1 (b). This solution offers two benefits over earlier boundary-based methods. On the one hand, having several sub-parts can more accurately reflect the actual distribution of known intent classes. On the other hand, it creates fine-grained decision boundaries that can more efficiently exclude intra-open intents, reducing the open space risk.

In light of this, the following three challenges come to this paper: (1) How to divide each known intent class into multiple sub-parts that accurately reflect the intent distribution? (2) How to learn discriminative representations that capture different semantic aspects within class, facilitating the division of known classes into sub-parts? (3) How to obtain decision boundaries for these sub-parts within each known intent class?

To solve these challenges, we propose a Multi-granularity Open intent classification method via adaptive Granular Ball decision boundary (MOGB). Our MOGB method consists of a hierarchical representation learning module and a decision boundary acquiring module. For challenge (1), inspired by granular-ball computing (Xia et al. 2023), we propose a hierarchical representation approach through adaptive granular-ball clustering, which reflects intent distribution using multiple granular-balls with varying sizes. For challenge (2), the nearest sub-centroid classifier is proposed to learn discriminative representations, with the loss function encouraging each intent to move closer to its semantically closest sub-centroid. In the training of hierarchical representation learning, the adaptive granular-ball clustering and nearest sub-centroid classification are iteratively conducted to mutually complement each other, facilitating the acquisition of discriminative representations. For challenge (3), we propose multi-granularity decision boundaries through granular-balls with varying radii and sub-centroids to distinguish the known and open intents. Our contributions are summarized as follows:

‚Ä¢ We propose the hierarchical representation learning method, which adaptively reflects intra-class structure and intent distribution via granular-ball clustering. ‚Ä¢ We propose the nearest sub-centroid classifier to acquire discriminative representations by gathering intents to its semantically nearest sub-centroid. ‚Ä¢ We design multi-granularity decision boundaries for each known intent class to facilitate open intent classification, which provides a new way to solve this question.

# Related Work

# Open Intent Classification

Recognizing the intent behind user interactions is crucial in dialogue systems, as they aim to understand users‚Äô potential requirements. Most classification models currently work under the closed-world assumption, which may not align with real-world systems functioning in open settings. Such systems often face queries that fall outside the supported intents, referred to as out-of-domain queries. With the prevalent of dialogue systems, there has been an increasing emphasis on identifying open intents in recent times (Zhou, Liu, and Chen 2021; Zhan et al. 2021; Parmar et al. 2023; Yan et al. 2020; Zhang et al. 2021).

Existing open intent classification methods can be categorized into two primary groups. The first group involves directly training a classifier with $K { + } 1$ classes, with an extra class representing the open intent (Zheng, Chen, and Huang 2020). The second group of methodologies focused on outlier detection algorithms, which can be categorized into two main subtypes: probability-based and decision boundarybased approaches. Probability-based methods use a predefined threshold to determine if a sample represents an open intent (Shu, Xu, and Liu 2017). Boundary-based approaches (Zhang, Xu, and Lin 2021; Liu et al. 2023; Zhang et al. 2023a) have been developed to overcome the limitations of probability-based methods, which struggle to establish a distinct boundary between known classes and unknowns.

However, current boundary-based approaches assume that the feature distribution of known intents is within a compact spherical region, but this is not always the case in reality. In addition, the single decision boundary they established includes intra-open intents, increasing the open space risks.

# Granular-ball Computing

Building on the human cognitive mechanisms of ‚Äúmacro first‚Äù (Chen 1982) and multi-granularity cognitive computation (Wang 2017), Xia et al. (Xia et al. 2019) proposed granular-ball computing, which is efficient, robust, and interpretable. It adheres to a hierarchical rule of progressing from coarse-grained to fine-grained. In granular-ball computing, varying sizes of granular-balls are adaptively generated to depict data distribution. Each granular-ball is defined by a centroid and a radius, with the centroid representing the mean of all object vectors within the ball and the radius determined by the average distance of objects from the centroid. The effectiveness of this self-supervised data-covering method has been empirically validated (Xie et al. 2024c), and it has demonstrated successful applications in addressing diverse challenges within the field of AI (Liu et al. 2024; Wang et al. 2024a,b).

Current granular-ball computing methods encompass granular-ball classification (Xia et al. 2024b; Quadir and Tanveer 2024), granular-ball clustering (Xie et al. 2024a,b), granular-ball three-way decision (Yang et al. 2024; Xia et al. 2024a), and granular-ball rough sets (Xia et al. 2020, 2023; Zhang et al. 2023b). Furthermore, several applications have demonstrated the effectiveness of granular-ball representation, such as granular-ball representation in text adversarial defense (Wang et al. 2024a), label noise combating (Wang et al. 2024b), feature selection (Cao et al. 2024), and deep reinforcement learning (Liu et al. 2024).

In summary, granular-ball computing is an effective way to discover the structure of arbitrary data distribution, with the radius of the granular-ball serving as a natural decision boundary for open classification. Despite this, there remains a notable absence of research applying granular-ball computing to the specific domain of open intent classification. Our MOGB method will employ granular-ball clustering to achieve hierarchical representation and establish multigranularity adaptive decision boundaries.

# Our Methodology

# Problem Statement and Model Overview

Problem Statement Given a dataset $S ~ = ~ \{ ( \mu _ { i } , y _ { i } ) \} _ { i = 1 } ^ { N }$ consisting of $N$ intents, where each pair $( \mu _ { i } , y _ { i } )$ represents an intent $\mu _ { i }$ and its corresponding label $y _ { i }$ . The labels $y _ { i } \in \{ 1 , \cdot \cdot \cdot , \dot { K } , K + 1 \}$ can be divided into two categories: $I _ { \mathrm { K n o w n } } = \{ 1 , \cdots , K \}$ denotes a set of known intent labels with $K$ representing the number of known intent classes, while $I _ { \mathrm { U n k n o w n } } = K + 1$ is used to label unknown (open) intents. During training and validation, only samples associated with known intents are utilized. In contrast, the testing dataset includes both known and open intents, and we need to classify the known intents into their respective classes and identify open intents as unknowns.

Model Overview Our proposed MOGB method consists of two modules: hierarchical representation learning and decision boundary acquiring. In module 1, the hierarchical representation of each known intent class is obtained via adaptive granular-ball clustering, as depicted in Figure 2 (a). Subsequently, we learn discriminative representation by the nearest sub-centroid classifier, as shown in Figure 2 (b). The iterative process of granular-ball clustering and nearest sub-centroid classification during training mutually reinforce each other to obtain discriminative representations. In module 2, multi-granularity decision boundaries are constructed for open intent classification, as illustrated in Figure 2 (c).

# Hierarchical Representation Learning

In this section, the BERT pre-trained model is employed for feature extraction, followed by adaptive granular-ball clustering and the nearest sub-centroid classifier.

Feature Extraction Utilizing the pre-trained BERT model (Kenton and Toutanova 2019), we extract token embeddings $[ C L S , T _ { 1 } , T _ { 2 } , \ldots , T _ { M } ] \in \mathbb { R } ^ { ( M + 1 ) \times H }$ from the final hidden layer to represent each intent $\mu _ { i }$ . Here, $M$ represents the sequence length and $H$ denotes the embedding dimension. Following previous studies (Zhang, $\mathrm { X u }$ , and Lin 2021), a mean pooling operation is applied to these embeddings in order to construct a semantic representation of each sentence denoted as $x _ { i } \in \mathbb { R } ^ { H }$ :

$$
x _ { i } = \mathtt { M e a n P o o l i n g } ( [ C L S , T _ { 1 } , T _ { 2 } , \ldots , T _ { M } ] ) ,
$$

where $C L S$ acts as a classification token. To further enhance the feature extraction process, $x _ { i }$ is subsequently passed through a dense layer $h$ , transforming the representation to zi RD:

$$
z _ { i } = h ( x _ { i } ) = \mathrm { R e L U } ( W _ { h } x _ { i } + b _ { h } ) ,
$$

where $D$ denotes the dimension of the intent representation, $W _ { h } \in \mathbb { R } ^ { H \times D }$ and $b _ { h } \in \mathbb { R } ^ { D }$ represent the weight and bias parameters of the dense layer $h$ , respectively.

Adaptive Granular-ball Clustering In order to accurately represent the true distribution of the known intent class and mitigate open space risk associated with intraopen intents, we need to group the known intent class into sub-classes that reflect inherent patterns within the class. Although conventional class-wise clustering methods can partially reveal data structure, their predefined cluster number setting limits the exploration of diverse patterns within each class. Granular-ball clustering is an efficient and adaptive clustering method to represent the true data distribution (Xia et al. 2021). Motivated by it, we represent the intent distribution using granular-balls generated through adaptive granular-ball clustering, as shown in Figure 2 (a), which eliminates the necessity for specifying cluster numbers.

The basic definitions of the granular-ball are introduced as following (Xia et al. 2021):

Definition 1. Given a dataset of intents $Z = \{ ( z _ { i } , y _ { i } ) \mid i =$ $1 , 2 , \ldots , N \}$ consisting of $N$ feature representations $z _ { i }$ and their corresponding labels $y _ { i }$ , we have a set of granularballs denoted as $G \ = \ \left\{ g b _ { 1 } , g b _ { 2 } , \ldots , g b _ { m } \right\}$ generated on the dataset, where $m$ denotes the number of granular-balls. Each granular-ball $g b _ { j } = \{ ( z _ { i } , y _ { i } ) \mid i = 1 , \bar { 2 } , \ldots , n _ { j } \}$ represents a distinct subset containing $n _ { j }$ intents. The fundamental components of each granular-ball include sample

Module 1: Hierarchical Representation Learning Module 2: Decision Boundary Acquiring   
(a) Adaptive Granular-Ball Clustering (b) Nearest Sub-centroid Classifier (c) Multi-Granularity Decision Boundary Forget password 01 Known intents (class 1) Open intents Password not work Transfer monùëÇe‡¨∂y d ‡µè d ùëÇ‡¨∂‡¨µ How to set new password?   
Coarse-grained ùëÇ‡¨∑‡¨µ 7 ùíÖùíéùíäùíè Send moneyùëÇ‡¨∂ Reset password Gathering similar semantics . Two steps alternate iteratively d ‡µè   
Fine-grained Known intents (class 2)

count $n _ { j }$ , centroid $O _ { j }$ , radius $r _ { j }$ , label $l _ { j }$ , and purity $p _ { j }$ . The centroid of $g b _ { j }$ is computed as the average representation $\begin{array} { r } { O _ { j } \ = \ \frac { 1 } { n _ { j } } \sum _ { i = 1 } ^ { n _ { j } } z _ { i } } \end{array}$ . The radius of $g b _ { j }$ is determined by calculating the average distance between all samples and the centroid: $\begin{array} { r } { \dot { r } _ { j } = \frac { 1 } { n _ { j } } \sum _ { i = 1 } ^ { n _ { j } } | | z _ { i } - O _ { j } | | } \end{array}$ , where $| | z _ { i } - O _ { j } | |$ denotes the Euclidean distance between $z _ { i }$ and $O _ { j }$ . The label $l _ { j }$ assigned to granular-ball $g b _ { j }$ corresponds to the class that contains the highest number of samples within it. Purity $p _ { j }$ indicates the proportion of samples labeled with $l _ { j }$ in granular-ball $g b _ { j }$ .

The input of the clustering includes all representations $Z ~ = ~ \{ ( \bar { z } _ { i } , y _ { i } ) ~ | ~ i ~ = ~ 1 , 2 , \bar { . ~ . ~ . ~ } , N \}$ from different known intent classes. Adaptive granular-ball clustering is an iterative process to split granular-balls. First, the representations $Z$ are initialized as a single granular-ball. Subsequently, the granular-balls that satisfy the split conditions undergo splitting (conditions described below). The process terminates when all granular-balls can no longer be further split.

To control the splitting process effectively, we set the condition of purity limit $p _ { l }$ and sample count limit $n _ { l }$ . Specifically, for a granular-ball $g b _ { j }$ with purity $p _ { j }$ and sample count $n _ { j }$ , if $p _ { j } ~ < ~ p _ { l }$ and $n _ { j } ~ > ~ n _ { l }$ , the granular-ball will be split further until either $p _ { j } ~ \geq ~ p _ { l }$ or $n _ { j } ~ \leq ~ n _ { l }$ . During the splitting phase, the granular-ball with a unique class label set $L _ { j }$ will be split into $| L _ { j } |$ new granular-balls, where $L _ { j } = \mathrm { u n i q u e } ( \{ y _ { i } | ( z _ { i } , y _ { i } ) \in g \bar { b _ { j } } \} )$ . To achieve this division accurately, one intent from each unique class within the original granular-ball is randomly selected as a pseudo centroid for creating a new granular-ball. Subsequently, other intents are assigned to their closest respective new granular-balls based on their Euclidean distances from pseudo centroids.

The adaptive granular-ball clustering algorithm produces a set of granular-balls denoted as $G = \mathcal { \bar { \{ g b _ { 1 } , g b _ { 2 } , \dots , g b _ { m } \} } }$ , where each granular-ball $g b _ { j } \ ( j \ = \ 1 , \cdot \cdot \ , m )$ is assigned a label $l _ { j } \ \in \ I _ { \mathrm { K n o w n } } \ = \ \{ 1 , \dot { \cdot } \cdot \cdot , K \}$ . To eliminate outliers and prevent overfitting, we select high-quality granular-balls with purity $p _ { j }$ above $p _ { t }$ and sample count $n _ { j }$ more than $n _ { t }$ to represent known intents. Each known intent class $c \in I _ { \mathrm { K n o w n } }$ corresponds to a number of $n _ { c }$ granular-balls labeled with c, where nc = count(lj = c), and cK= $\textstyle \sum _ { c = 1 } ^ { K } n _ { c } \ = \ m$ granular-balls representing class $c$ are dPenot1ed as $\{ g b _ { s } ^ { c } \} _ { s = 1 } ^ { n _ { c } }$ , where $g b _ { s } ^ { c }$ represents the $s$ -th granular-ball with label $c$ . The collection of all these granular-balls {gbcs}cK,s,n=c1 reveals the detailed structure of the distributions for all known intents.

Nearest Sub-Centroid Classifier The effectiveness of adaptive granular-ball clustering is highly dependent on the quality of representation. Therefore, it is necessary to obtain discriminative representations that reflect the semantic structure of known intent classes. However, previous crossentropy loss-based (Zhang, Xu, and Lin 2021) or contrastive loss-based (Liu et al. 2023) representation learning methods treat each class as an entity, failing to explore patterns within the class. To this end, we propose the nearest sub-centroid representation learning method to gather similar semantics and separate dissimilar semantics, as shown in Figure 2 (b).

According to the granular-balls $\{ g b _ { s } ^ { c } \} _ { c , s = 1 } ^ { K , n _ { c } }$ , we compute their centroids as $\{ O _ { s } ^ { c } \} _ { c , s = 1 } ^ { K , n _ { c } }$ , where $O _ { s } ^ { c }$ denotes the $s$ -th sub$c$ mative properties of known intents. In the classification of representation learning, the distances from test intent to all sub-centroids are calculated as follows:

$$
d i s \left. z _ { i } , O _ { s } ^ { c } \right. = \frac { z _ { i } ^ { \top } O _ { s } ^ { c } } { \| z _ { i } \| \| O _ { s } ^ { c } \| } , i = \{ 1 , \cdots , N \} ,
$$

where $\left\| z _ { i } \right\|$ and $\| O _ { s } ^ { c } \|$ denote the Euclidean norm vectors of $z _ { i }$ and $O _ { s } ^ { c }$ . We assign the label of test intent $z _ { i }$ corresponding to the label of the sub-centroid closest to $z _ { i }$ . Concretely, if di $s \left. z _ { i } , O _ { s ^ { * } } ^ { c ^ { * } } \right.$ denotes the minimum distance among dis $\langle z _ { i } , O _ { s } ^ { c } \rangle$ , the predicted label of $z _ { i }$ is $c ^ { * }$ . The classifier rule is elaborated as follows:

$$
\hat { y } _ { i } = c ^ { * } , ( c ^ { * } , s ^ { * } ) = \underset { c \in \{ 1 , \dots , K \} , s \in \{ 1 , \dots , n _ { c } \} } { \arg \operatorname* { m i n } } d i s \langle z _ { i } , O _ { s } ^ { c } \rangle .
$$

Based on the classifier provided, the training loss associated with the nearest sub-centroid representation learning is

shown as follows:

$$
\mathcal { L } _ { g b } = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } - \log p ( y _ { i } | z _ { i } ) ,
$$

$$
p ( y | z ) = \frac { \exp \left( - \operatorname* { m i n } \left( \left\{ d i s \left. z , O _ { s } ^ { y } \right. \right\} _ { s = 1 } ^ { n _ { y } } \right) \right) } { \sum _ { c = 1 } ^ { K } \exp \left( - \operatorname* { m i n } \left( \left\{ d i s \left. z , O _ { s } ^ { c } \right. \right\} _ { s = 1 } ^ { n _ { c } } \right) \right) } .
$$

During hierarchical representation learning, adaptive granular-ball clustering produces informative sub-centroids and the nearest sub-centroid classifier optimizes the representation by adjusting the arrangement between intents and sub-centroids. The improved representation subsequently facilitates the identification of more informative sub-centroids. In summary, the two steps iteratively complement each other, eventually benefiting the open intent classification.

Multi-Granularity Decision Boundary in Inference In our MOGB method, pairs of centroids and radii Oc, rc K,nc generated from training data of known intents are directly employed to construct multi-granularity decision boundaries for each class.

In inference, the distances between test sample $z _ { i }$ and all sub-centroids $\{ O _ { s } ^ { c } \} _ { c , s = 1 } ^ { K , n _ { c } }$ are first calculated as $d i s \langle z _ { i } , O _ { s } ^ { c } \rangle$ Then, we compare each $d i s \langle z _ { i } , O _ { s } ^ { c } \rangle$ with its corresponding rpaled if $\{ r _ { s } ^ { c } \} _ { c , s = 1 } ^ { K , n _ { c } }$ etoalml amkeulotip-egrnacnlualsasriiftiycatbiouin.dIafritehse, ewset scalamssify it as unknown. Otherwise, the sample is predicted as the same class as the nearest sub-centroid that satisfies the distance condition. The inference rule is formulated as:

$$
\hat { y } _ { i } = \left\{ \begin{array} { l l } { \mathrm { u n k n o w n , i f } d i s \langle z _ { i } , O _ { s } ^ { c } \rangle > r _ { s } ^ { c } , \forall c \in I _ { \mathrm { K n o w n } } ; } \\ { \quad \quad \mathrm { a r g m i n } d i s \langle z _ { i } , O _ { s } ^ { c } \rangle , \mathrm { O t h e r w i s e } . } \\ { c \in I _ { \mathrm { K n o w n } } , d i s \langle z _ { i } , O _ { s } ^ { c } \rangle \le r _ { s } ^ { c } } \end{array} \right.
$$

Intuitively, compared to existing boundary-based methods that include two training processes, our MOGB method can simultaneously learn representation and acquire decision boundaries with just one training.

# Experiments

# Datasets

In accordance with prior studies (Zhang, Xu, and Lin 2021; Zhang et al. 2023a), we conduct experiments on three commonly used datasets: StackOverflow (Xu et al. 2015) is a dataset containing 3,370,528 programming question titles across 20 categories. Consistent with prior studies, a subset of 1,000 examples from each category was randomly selected for analysis. SNIPS (Coucke et al. 2018) is composed of 14,484 spoken utterances with 7 distinct categories of intent classes. BANKING (Casanueva et al. 2020) is an online banking inquiry with 13,080 instances and 77 intents classes. Table 1 presents detailed statistics of these datasets.

# Baselines

We compare our MOGB method with the following open intent classification methods: MSP (Hendrycks and Gimpel

Table 1: Statistics of datasets.   

<html><body><table><tr><td>Dataset</td><td>#Class</td><td>#Train/Valid/Test</td><td>Length</td></tr><tr><td>StackOverflow</td><td>20</td><td>12,000/2,000/6.000</td><td>9.18</td></tr><tr><td>SNIPS</td><td>7</td><td>13,084/700/700</td><td>9.05</td></tr><tr><td>BANKING</td><td>77</td><td>9,003/1,000/3,080</td><td>11.91</td></tr></table></body></html>

2022) classifies samples based on their maximum softmax probabilities; DOC (Shu, Xu, and Liu 2017) conducts open intent classification by establishing probability thresholds through Gaussian fitting. OpenMax (Bendale and Boult 2016) implements open set recognition by transforming the softmax layer in a neural network into an OpenMax layer. DeepUnk (Lin and $\mathrm { X u } 2 0 1 9 _ { , }$ ) utilizes margin loss that minimizes intra-class variance and maximizes inter-class variance to learn enhanced representation. ADB (Zhang, Xu, and Lin 2021) acquires feature representation by softmax loss and learns a single decision boundary for each class. DA-ADB (Zhang et al. 2023a) learns distance-aware intent representations to obtain appropriate decision boundary.

# Evaluation Metrics

Following previous work (Zhang, Xu, and Lin 2021), we utilize F1-score and accuracy metrics to assess different approaches. Additionally, we also compute the fine-grained macro F1-score for both known classes and unknowns.

# Experimental Settings

Following the experimental setting of prior study (Zhang, $\mathrm { { X u } }$ , and Lin 2021), a predetermined percentage of classes are randomly designated as known classes, while the remaining classes are considered unknown (open). During testing, all unknown classes are collectively treated as a single additional class. Training exclusively utilizes samples from the known classes, with samples from the unknown classes excluded from the training dataset. The test dataset comprises both known and unknown samples. The experimental results are presented based on three datasets, with known class ratios set at $2 5 \%$ , $50 \%$ , and $7 5 \%$ , respectively.

During representation learning, we use the pre-trained BERT model (Kenton and Toutanova 2019), keeping all parameters fixed except for those in the final layer. The training batch size is set to 128, with a learning rate of 2e-5 to finetune the final layer. We control the adaptive granular-ball clustering by two attributes of the granular-ball: purity limit $p _ { l }$ and sample count limit $n _ { l }$ . We set $p _ { l } = 0 . 9$ for all datasets and varying $n _ { l }$ for different datasets depending on the number of samples within each class. In addition, granular-balls with high quality are selected to represent the distribution by new purity limit $p _ { t }$ and sample count limit $n _ { t }$ . We set $p _ { t } = 1$ for all datasets and set varying $n _ { l }$ based on the dataset.

# Main Results

Table 2 displays the results of open intent classification on three datasets (StackOverflow, SNIPS, and BANKING) at different proportions of known classes $2 5 \%$ , $50 \%$ , and $7 5 \%$ ). We report the accuracy of all intent classes (denoted as ‚ÄúAcc‚Äù) and F1-score for all intent classes, unknown, and known classes (denoted as ‚ÄúF1-All‚Äù, ‚ÄúF1-U‚Äù, and ‚ÄúF1-K‚Äù, respectively). The most optimal performance is emphasized through the blod values, while the underline values indicate a suboptimal performance of our MOGB.

<html><body><table><tr><td rowspan="2">Methods</td><td rowspan="2"></td><td colspan="4">StackOverflow</td><td colspan="4">SNIPS</td><td colspan="4">BANKING</td></tr><tr><td>Acc</td><td>F1-All</td><td>F1-U</td><td>F1-K</td><td>Acc</td><td>F1-All</td><td>F1-U</td><td>F1-K</td><td>Acc</td><td>F1-All</td><td>F1-U</td><td>F1-K</td></tr><tr><td rowspan="7">25%</td><td>MSP+</td><td>28.67</td><td>37.85</td><td>13.03</td><td>42.82</td><td>28.57</td><td>37.65</td><td>0.00</td><td>56.48</td><td>43.67</td><td>50.09</td><td>41.43</td><td>50.55</td></tr><tr><td>DOCt</td><td>42.74</td><td>47.73</td><td>41.25</td><td>49.02</td><td>45.63</td><td>55.10</td><td>34.67</td><td>65.32</td><td>56.99</td><td>58.03</td><td>61.42</td><td>57.85</td></tr><tr><td>OpenMaxt</td><td>40.28</td><td>45.98</td><td>49.29</td><td>52.60</td><td>59.57</td><td>49.68</td><td>61.92</td><td>43.56</td><td>49.94</td><td>54.14</td><td>51.32</td><td>54.23</td></tr><tr><td>DeepUnk*</td><td>70.68</td><td>65.57</td><td>36.87</td><td>47.39</td><td>53.42</td><td>59.67</td><td>49.32</td><td>64.85</td><td>70.68</td><td>65.57</td><td>76.98</td><td>64.97</td></tr><tr><td>ADB‚Ä†</td><td>86.72</td><td>80.83</td><td>90.88</td><td>78.82</td><td>58.57</td><td>65.72</td><td>58.92</td><td>69.12</td><td>78.85</td><td>71.62</td><td>84.56</td><td>70.94</td></tr><tr><td>DA-ADB*</td><td>87.01</td><td>80.05</td><td>91.16</td><td>77.82</td><td>47.09</td><td>52.75</td><td>44.38</td><td>56.93</td><td>78.39</td><td>70.67</td><td>84.27</td><td>69.95</td></tr><tr><td>MOGB</td><td>91.48</td><td>84.43</td><td>94.42</td><td>87.80</td><td>68.00</td><td>72.20</td><td>71.36</td><td>72.63</td><td>83.08</td><td>75.19</td><td>88.29</td><td>74.50</td></tr><tr><td rowspan="7">50%</td><td>MSPt</td><td>52.42</td><td>63.01</td><td>23.99</td><td>66.91</td><td>57.97</td><td>63.34</td><td>7.12</td><td>77.40</td><td>59.73</td><td>63.60</td><td>41.19</td><td>71.97</td></tr><tr><td>DOC+</td><td>52.53</td><td>62.84</td><td>25.44</td><td>66.58</td><td>72.50</td><td>78.15</td><td>54.89</td><td>83.97</td><td>64.81</td><td>73.12</td><td>55.14</td><td>73.59</td></tr><tr><td>OpenMax‚Ä†</td><td>60.35</td><td>68.18</td><td>45.00</td><td>70.49</td><td>59.82</td><td>65.50</td><td>14.23</td><td>78.31</td><td>65.31</td><td>74.24</td><td>54.33</td><td>74.76</td></tr><tr><td>DeepUnk*</td><td>71.01</td><td>75.41</td><td>35.80</td><td>67.67</td><td>69.53</td><td>75.10</td><td>49.67</td><td>81.45</td><td>71.01</td><td>75.41</td><td>67.80</td><td>75.61</td></tr><tr><td>ADB‚Ä†</td><td>86.40</td><td>85.83</td><td>87.34</td><td>85.68</td><td>73.57</td><td>78.82</td><td>62.17</td><td>82.98</td><td>78.86</td><td>80.90</td><td>78.44</td><td>80.96</td></tr><tr><td>DA-ADB*</td><td>86.02</td><td>85.21</td><td>87.21</td><td>85.01</td><td>73.69</td><td>77.35</td><td>66.58</td><td>80.05</td><td>79.09</td><td>81.03</td><td>78.78</td><td>81.09</td></tr><tr><td>MOGB</td><td>88.67</td><td>87.49</td><td>89.71</td><td>87.27</td><td>73.86</td><td>78.91</td><td>62.42</td><td>83.03</td><td>80.58</td><td>81.52</td><td>81.04</td><td>81.53</td></tr><tr><td rowspan="6">75%</td><td>MSPt</td><td>72.17</td><td>77.95</td><td>33.96</td><td>80.88</td><td>71.49</td><td>72.56</td><td>6.52</td><td>85.77</td><td>75.89</td><td>83.60</td><td>39.23</td><td>84.36</td></tr><tr><td>DOCt</td><td>68.91</td><td>75.06</td><td>16.76</td><td>78.95</td><td>79.96</td><td>83.11</td><td>51.67</td><td>89.40</td><td>76.77</td><td>83.34</td><td>50.60</td><td>83.91</td></tr><tr><td>OpenMax‚Ä†</td><td>74.42</td><td>79.78</td><td>44.87</td><td>82.11</td><td>72.20</td><td>73.63</td><td>11.03</td><td>86.15</td><td>77.45</td><td>84.07</td><td>50.85</td><td>84.64</td></tr><tr><td>DeepUnk*</td><td>71.56</td><td>77.63</td><td>34.38</td><td>77.63</td><td>78.50</td><td>81.74</td><td>52.05</td><td>87.68</td><td>74.73</td><td>81.12</td><td>50.57</td><td>81.65</td></tr><tr><td>ADB‚Ä†</td><td>82.78</td><td>85.99</td><td>73.86</td><td>86.80</td><td>86.71</td><td>88.61</td><td>73.35</td><td>91.66</td><td>81.08</td><td>85.96</td><td>66.47</td><td>86.09</td></tr><tr><td>DA-ADB*</td><td>82.89</td><td>86.11</td><td>74.06</td><td>86.92</td><td>81.57</td><td>84.29</td><td>68.62</td><td>87.42</td><td>81.31</td><td>86.01</td><td>67.22</td><td>86.36</td></tr><tr><td></td><td>MOGB</td><td>84.37</td><td>87.37</td><td>75.52</td><td>88.16</td><td>87.71</td><td>89.39</td><td>73.62</td><td>92.55</td><td>82.69</td><td>86.82</td><td>71.27</td><td>87.09</td></tr></table></body></html>

Table 2: Results of MOGB across StackOverflow, SNIPS, and BANKING with different known class ratios $2 5 \%$ , $50 \%$ , and $7 5 \%$ ). $^ *$ means the results are not provided in the original paper and we get the results by running its released codes. The results with $\dagger$ are from (Zhang, Xu, and Lin 2021).

According to Acc and F1-All, our MOGB methodology exhibits superior performance across all three datasets, consistently surpassing all other baseline methods. Moreover, we observed that our MOGB approach surpasses ADB and DA-ADB by a greater margin when the known class ratio is low. Conversely, when the known class ratio is high, the enhancement in performance of our method is somewhat minimal. Specifically, in cases where the proportion of the known class is low $( 2 5 \% )$ , compared with the strong baseline ADB, our MOGB improves four metrics (‚ÄúAcc‚Äù / ‚ÄúF1-All‚Äù / ‚ÄúF1- U‚Äù / ‚ÄúF1-K‚Äù) by $. 7 6 \% / 3 . 6 0 \% / 3 . 5 4 \% / 8 . 9 8 \%$ on StackOverflow, by $9 . 4 3 \%$ / $5 . 4 8 \% / \ 1 2 . 4 4 \% \ / \ 3 . 5 1 \%$ on SNIPS, and by $4 . 2 3 \% / 3 . 5 7 \% / 3 . 7 3 \% / 3 . 5 6 \%$ on BANKING. Conversely, when the known class ratio is higher $( 7 5 \% )$ , compared with ADB, our MOGB method enhances four metrics by $1 . 5 9 \%$ $\textit { / } 1 . 3 8 \% / \ 1 . 6 6 \% \ / \ 1 . 3 6 \%$ on StackOverflow, by $. 0 0 \% / 0 . 7 8 \% / 0 . 2 7 \% / 0 . 8 9 \%$ on SNIPS and by $1 . 6 1 \% /$ $0 . 8 6 \% / 4 . 8 0 \% / 1 . 0 0 \%$ on BANKING.

These above results indicate that our MOGB is more effective in situations with a high proportion of unknown intents. This can be ascribed to the fact that the test dataset includes more open intents under a low known class ratio. Compared with the single decision boundary methods (ADB and DA-ADB), the multi-granularity decision boundaries in MOGB allocate more space for open intents. Therefore, the open space risk associated with intra-open intents is reduced effectively, resulting in enhanced performance.

# Ablation Study

In this subsection, we examine the impact of two components of MOGB on three datasets with a known class ratio of $2 5 \%$ . From Table 3, we observed that removing any component will lead to performance degradation, emphasizing the essence of each independent component. Specifically, (1) w/o HRL refers to removing the Hierarchical Representation Learning (HRL) module and learning representation by cross-entropy loss; (2) w/o MB refers to removing the Multigranularity decision Boundary (MB) and classification by a single boundary of each class; (3) w/o HRL $+ \mathbf { M B }$ indicates the exclusion of both Hierarchical Representation Learning and Multi-granularity decision Boundary. The combination of HRL and MB can achieve the best performance.

# Discussion

In the MOGB method, the multi-granularity decision boundaries are established based on granular-balls derived from the training dataset. Consequently, the quality of these granular-balls has a significant impact on the classification performance. The quality of a granular-ball largely depends on its purity and the sample count. We use granular-balls with purity exceeding $p _ { t }$ and sample count more than $n _ { t }$ to build the decision boundaries. Next, we report the effect of $p _ { t }$ and $n _ { t }$ .

Table 3: Ablation study on StackOverflow, SNIPS, and BANKING with a known class ratio of $2 5 \%$ .   

<html><body><table><tr><td>Dataset</td><td>Method</td><td>Acc</td><td>F1-All</td></tr><tr><td>StackOverflow</td><td>MOGB W/o HRL w/o MB W/o HRL+MB</td><td>91.48 85.70 82.38 86.30</td><td>84.43 79.72 75.82 80.06</td></tr><tr><td>SNIPS</td><td>MOGB W/o HRL w/o MB W/o HRL+MB</td><td>68.00 62.57 62.57 60.29</td><td>72.20 68.94 68.88 67.54</td></tr><tr><td>BANKING</td><td>MOGB W/o HRL w/o MB w/o HRL+MB</td><td>83.08 77.40 81.14 78.80</td><td>75.19 71.55 74.25 72.20</td></tr></table></body></html>

Table 4: Effect of $p _ { t }$ on StackOverflow with known class ratio of $7 5 \%$ .   

<html><body><table><tr><td>pt</td><td>Acc</td><td>F1-All F1-U F1-K</td></tr><tr><td>0.80</td><td>83.10</td><td>86.41 72.70 87.32</td></tr><tr><td>0.85</td><td>83.27</td><td>86.50 73.15 87.39</td></tr><tr><td>0.90</td><td>84.37</td><td>87.37 75.52 88.16</td></tr><tr><td>0.93</td><td>84.45</td><td>87.41 75.78 88.19</td></tr><tr><td>0.95</td><td>84.23 87.31</td><td>75.40 88.10</td></tr><tr><td>0.97</td><td>84.23 87.31</td><td>75.40 88.10</td></tr></table></body></html>

# Effect of the Purity Limit $p _ { t }$

A higher $p _ { t }$ results in a small number of trustworthy granular-balls being used to represent the dataset and fewer decision boundaries being constructed, reducing the known intent space and increasing the chance of misclassifying known intents as unknowns. Conversely, a lower $p _ { t }$ expands the known space, reducing open space but increasing the risk of misclassifying unknown intents as known classes.

Table 4 presents the Acc, F1-All, F1-U, and F1-K as the purity threshold $p _ { t }$ increases from 0.80 to 0.97 on the StackOverflow dataset, with a known class ratio of $7 5 \%$ . Overall, the performance of our method in open classification remains relatively stable, highlighting its robustness. However, a slight trend in performance variation can be observed: as $p _ { t }$ increases, classification performance initially improves, peaking at $p _ { t } = 0 . 9 3$ . This improvement occurs because high-purity granular-balls better capture the distribution of known classes by reducing uncertainty. Beyond this point, further increasing $p _ { t }$ to 0.95 causes a performance decline. The stricter purity requirement excludes certain decision boundaries that represent known classes, leading to portions of known class spaces being misclassified as unknowns and increasing open space risk. This outcome is consistent with our expectations.

# Effect of the Sample Count Limit $n _ { t }$

Granular-balls containing samples exceeding a specified threshold $n _ { t }$ are chosen to represent data distribution and establish decision boundaries for the open classification. Figure 3 shows the performance of MOGB on dataset BANKING with a known class ratio of $50 \%$ as $n _ { t }$ changes, along with the number of established decision boundaries at different $n _ { t }$ values. There are 39 known classes under a known class ratio of $50 \%$ on the BANKING dataset. From Figure 3, we observed that the number of decision boundaries for open classification decreases as $n _ { t }$ increases. In particular, when $n _ { t } = 1$ , a total of 314 decision boundaries are established, signifying that each known intent class is characterized by several distinct boundaries. Optimal performance is achieved at $n _ { t } = 5$ , where 104 decision boundaries are constructed. However, at $n _ { t } = 1 9$ , only 43 decision boundaries are constructed, with most classes represented by a single boundary, degrading the performance of four metrics.

![](images/3cc881b4417a142b47dbf7f3d8c1921e4e2a4277b344d795f39a7d8d1c7112c7.jpg)  
Figure 3: Effect of $n _ { t }$ on BANKING with $50 \%$ known class. X-axis represents the value of $n _ { t }$ , the left Y-axis denotes the values of four metrics, and the right ${ \mathrm { Y } } .$ -axis indicates the number of established decision boundaries for all known classes.

The observed phenomenon aligns with our expectations. A strict $n _ { t }$ limit reduces decision boundaries, leading to incomplete feature space coverage and higher empirical risk. In contrast, a looser $n _ { t }$ limit introduces more boundaries that better fit the true distribution but risk overcovering, ultimately reducing performance. Although the performance of MOGB varies with changes in $n _ { t }$ , the overall fluctuation remains small, demonstrating the robustness of our method.

# Conclusions

This paper proposes a multi-granularity open intent classification method via hierarchical representation learning and multi-granularity decision boundary (MOGB). Specifically, the granular-balls of diverse sizes are generated by adaptive granular-ball clustering to represent the known intent space during hierarchical representation learning. Additionally, the nearest sub-centroid classifier equips for learning fine-grained representation that reflects semantic structures within known intent classes. Furthermore, multi-granularity decision boundaries for each known intent class are constructed for open intent classification, which reduces both empirical and open-space risk. Finally, extensive experiments demonstrate the superiority of the MOGB method.