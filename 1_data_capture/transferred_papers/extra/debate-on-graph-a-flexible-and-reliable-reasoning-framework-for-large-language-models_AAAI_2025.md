# Debate on Graph: A Flexible and Reliable Reasoning Framework for Large Language Models

Jie $\mathbf { M } \mathbf { a } ^ { 1 * }$ , Zhitao Gao1, Qi Chai2, Wangchun $\mathbf { S u n } ^ { 1 }$ , Pinghui Wang1\*, Hongbin Pei1, Jing Tao1, Lingyun Song3, Jun Liu1, Chen Zhang4, Lizhen Cui5

1 MOE KLINNS Lab, Xi’an Jiaotong University 2 The Hong Kong University of Science and Technology (Guangzhou) 3 Northwestern Polytechnical University 4 Zhejiang Createlink Technology 5 Shandong University {jiema, phwang}@xjtu.edu.cn

# Abstract

Large Language Models (LLMs) may suffer from hallucinations in real-world applications due to the lack of relevant knowledge. In contrast, knowledge graphs encompass extensive, multi-relational structures that store a vast array of symbolic facts. Consequently, integrating LLMs with knowledge graphs has been extensively explored, with Knowledge Graph Question Answering (KGQA) serving as a critical touchstone for the integration. This task requires LLMs to answer natural language questions by retrieving relevant triples from knowledge graphs. However, existing methods face two significant challenges: excessively long reasoning paths distracting from the answer generation, and false-positive relations hindering the path refinement. In this paper, we propose an iterative interactive KGQA framework that leverages the interactive learning capabilities of LLMs to perform reasoning and Debating over Graphs (DoG). Specifically, DoG employs a subgraph-focusing mechanism, allowing LLMs to perform answer trying after each reasoning step, thereby mitigating the impact of lengthy reasoning paths. On the other hand, DoG utilizes a multi-role debate team to gradually simplify complex questions, reducing the influence of false-positive relations. This debate mechanism ensures the reliability of the reasoning process. Experimental results on five public datasets demonstrate the effectiveness and superiority of our architecture. Notably, DoG outperforms the state-of-the-art method ToG by $2 3 . 7 \%$ and $9 . 1 \%$ in accuracy on WebQuestions and GrailQA, respectively. Furthermore, the integration experiments with various LLMs on the mentioned datasets highlight the flexibility of DoG.

Code — https://github.com/reml-group/DoG

# Introduction

Large Language Models (LLMs), characterized by their substantial parameter amount (Zhang et al. 2023) and training on extensive, diverse, and unlabeled data (Rawte, Sheth, and Das 2023), exhibit remarkable proficiency in a wide range of natural language understanding and generation tasks (Lin et al. 2023; Liu et al. 2024). For example, GPT-4 (Achiam

What are the languages spoken in the films starred by [Albert Nobbs] actors? starred_by Albert Nobbs Glenn Close Gray Oldman ① excessively long paths starred Air Force One starred_by ② false positive relations in_language Russian   
X According to the path information, since Glenn Close is an American actor, we can speculate that the language of the movie she starred in is English. X   
What are the languages spoken in the films starred by [Albert Nobbs] actors? ①   
What are the languages spoken in the film starred by [Glenn Close]? ②   
What are the languages spoken in the [Air Force One]? 山 D Albert Nobbs starred_by Glenn Close ② ① istarred Air Force One in_language Russian   
B According to the triple [Air Force One, in_language, Russian], the answer of the question is Russion.

et al. 2023) demonstrates human-level performance across a majority of professional and academic exams originally intended for humans. However, recent studies (Guan et al. 2024; Waldendorf, Haddow, and Birch 2024; Gunjal, Yin, and Bas 2024; Ma et al. 2023) have revealed that they may suffer from hallucinations in real-world applications due to a deficiency in relevant knowledge.

Knowledge graphs (Wang et al. 2024) are large-scale, multi-relational structures housing a plethora of symbolic facts, such as the triple <The Eiffel Tower, locatedIn, Paris>. The incorporation of these structured facts may tackle the aforementioned issue of hallucinations in LLMs (Guan et al. 2024; Quintero-Narvaez and Monroy 2024; Shi et al. 2023). One approach to evaluating the integration of knowledge graphs with LLMs is through Knowledge Graph Question Answering (KGQA) (Ji et al. 2022), which requires machines to answer natural language questions by retrieving relevant facts from knowledge graphs. Recent works (Li et al. 2024; Toroghi et al. 2024; Nie et al. 2024) primarily follow an iterative inference paradigm, consisting of two steps: (1) identifying the initial entity in the question, and (2) retrieving and refining the inference path iteratively until reaching the answer or obtaining sufficient evidence to answer the question. Although they have achieved significant success, they still suffer from excessively long paths and false-positive relations.

Challenge 1: excessively long paths distracting from the answer generation. Existing methods (Ye et al. 2022; Guo et al. 2023a; Kim et al. 2023) usually feed a lengthy evidence path like {<Albert Nobbs, starred by, Glenn Close>, , <Air Force One, starred by, Gray Oldman>, $\cdots \}$ at the top of Fig. 1 into LLMs to perform answer generation in a single step, which may make it challenging for LLMs to discern the key points in the path. For instance, LLMs may focus on the tail entity Glenn Close and employ their internal prior knowledge to generate answers. This will result in answers that appear reasonable but are incorrect.

Challenge 2: false-positive relations hindering the path refinement. Current methods (Bai et al. 2023; Hu et al. 2024; Li et al. 2024) typically focus on identifying relations within graphs that closely match or have the same meaning as those in the questions, even if the relations have already been identified in previous reasoning steps. For example, at the top of Fig. 1, these methods may select starred by, which was used in the previous reasoning step and is mentioned in the question, to expand paths rather than choosing in language when dealing with the entity Air Force One. This will lead to incomplete evidence paths.

To address these challenges, we propose an iterative interactive KGQA framework that leverages the interactive learning capabilities of LLMs to perform reasoning and Debating over Graphs, dubbed DoG. Unlike existing approaches (Jiang et al. 2023a; Luo et al. 2023; Sun et al. 2024) that seek to construct a complete evidence chain before answering questions, our architecture employs a subgraphfocusing mechanism that allows LLMs to perform answer trying after each reasoning step. For each filtered triple, DoG uses LLMs to assess whether sufficient information is available to answer the current question. In this way, the triple in each reasoning step, such as <Glenn Close, starred, AirForce One> in the bottom of Fig. 1, can be deeply pondered by LLMs. If the triple does not support answering the current question, DoG employs a multirole LLM team to debate and simplify the question based on the triple. The iterative process allows complex multihop questions to be gradually transformed into single-hop questions, which enables LLMs not to be disturbed by the relation that is retrieved in the previous reasoning step. For example, the relation starred by that is linked with Air Force One will not disturb reasoning after the simplification procedure $\textcircled{2}$ . This is inspired by the human brain in tackling complex questions, which guides LLMs to reason on graphs through chain-of-thought (Wei et al. 2022). The simplification process can also enhance the transparency of the reasoning process.

To verify the effectiveness and superiority of our architecture, we conduct thorough experiments on five public KGQA datasets: MetaQA (Zhang et al. 2018), WebQSP (Yih et al. 2016), CWQ (Talmor and Berant 2018), WebQuestions (Berant et al. 2013), and GrailQA (Gu et al. 2021). Our findings show that DoG achieves state-of-the-art results on all datasets, except for the 2-hop and 3-hop questions within MetaQA. Notably, DoG outperforms the strong baseline ToG (Sun et al. 2024) by $2 3 . 7 \%$ and $9 . 1 \%$ in accuracy on WebQuestions and GrailQA, respectively. In summary, our contributions are threefold.

• We propose a flexible and reliable reasoning framework, DoG, which enables LLMs to reason and debate over knowledge graphs and answer questions after thorough deliberation.   
• We introduce a strategy, which transforms questions from complex to easy through the interactive learning of a multi-role LLM team, for handling complex reasoning on knowledge graphs. This guides LLMs to engage in step-by-step reasoning, thereby enhancing the reliability of the reasoning process.   
• Extensive experiments and ablation studies are carried out on five public datasets to demonstrate the effectiveness and superiority of our architecture. Furthermore, we also conduct integration experiments with various LLMs to verify the flexibility of DoG.

# Related Work

The methods of LLM reasoning over knowledge graphs can be classified into batch triple recalling, and reasoning path refining from the perspective of evidence gathering.

Batch triple recalling. Knowledge graphs typically store an extensive amount of facts (Cui et al. 2023). For instance, Freebase (Bollacker et al. 2008) contains over 1.9 billion triples, and even the smaller non-open-domain MetaQA (Zhang et al. 2018) includes over 130,000 triples. The number of relevant triples can be substantial even when constrained by the entities in a given question. Injecting all these triples into the context window of LLMs to perform reasoning not only incurs a high encoding cost but also introduces significant noise (Wei et al. 2023). To address this issue, previous studies (Shu et al. 2022; Ye et al. 2022; Guo et al. 2023a) focus on how to filter suitable facts. For instance, KAPING (Baek, Aji, and Saffari 2023) projects questions and triples into the same space to obtain relevant knowledge by semantic similarity. KG-GPT (Kim et al. 2023) further focuses on fine-grained question representations, decomposing multi-hop questions into sub-questions and matching the relations associated with entities in those sub-questions, then selecting the top-k relevant relations to form evidence triples. Similarly, KGR (Guan et al. 2024) splits the retrieved triples into several chunks and utilizes LLM to distinguish the critical triple relevant with questions.

Reasoning path refining. The paradigm of this kind of method (Gu, Deng, and Su 2023; Jiang et al. 2023a; Liu et al. 2023; Luo et al. 2023; Sun et al. 2024; Guo et al. 2023b) is first to identify the initial entity in the question, then to iteratively retrieve and refine the reasoning path until reaching the answer or obtaining sufficient evidence to answer the question, and finally to employ LLMs to generate answers based on the refined path. For example, Jiang et al. (2023a) proposed an iterative reading-reasoning approach, which iterates an invoking-linearization-generation procedure. It utilizes LLMs to perform reasoning on the interface that is specifically designed for reading structured data until deriving the final answer. Similarly, Sun et al. (2024) introduced a deep and responsible reasoning framework, which first conducts a beam search on a graph from the entity within questions and then acquires multiple reasoning paths as evidence for answer generation. It is noteworthy that these methods all treat the LLM as a tool for accomplishing specific tasks, conceptualizing it as function executors, and relying on incontext learning (Dong et al. 2022) or fine-tuning to refine its outputs (Jiang et al. 2024). However, some studies (Zhao et al. 2024; Zhang, Xu, and Deng 2023; Schumann et al. 2024) have demonstrated that LLMs can be induced to exhibit human personality traits and role distinctions to undertake complex reasoning tasks.

Communicative Agents. The primary objective of agents is to collaboratively address complex tasks in a productive and efficient manner through autonomous communication and negotiation (Chan et al. 2023; Liang et al. 2023; Yang et al. 2023; Kirk et al. 2024). LLMs such as ChatGPT and Vicuna (Chiang et al. 2023) are frequently employed as these communicative agents. Recently, numerous studies have investigated the application of these agents in various domains, including AI societies (Li et al. 2023a), software development (Qian et al. 2023), translation (Liang et al. 2023), arithmetic problem-solving (Du et al. 2023), dialogue response generation (Chan et al. 2023), and strategic planning among robots (Mandi, Jain, and Song 2023). Specifically, Wang et al. (2023) guided ChatGPT to emulate expert system reviewers, thereby improving the quality of its literature retrieval queries. Kong et al. (2023) introduced a strategically designed role-playing prompt method to enhance reasoning abilities by assigning appropriate expert roles for tasks. Additionally, Shen et al. (2024) assessed the changes in decision-making abilities when LLM assumes different personality traits. Inspired by these studies, we explore the benefit of multi-agent role differentiation and debates for complex reasoning on knowledge graphs.

# Method

# Task Formulation

Given a knowledge graph $\mathcal { G }$ consisting of $N$ triples, represented as $\{ ( e _ { i } ^ { l } , r _ { l } , e _ { i + 1 } ^ { l } ) | e _ { i } \in \mathcal { E } , r _ { l } \in \mathcal { R } , i \in [ 1 , I ] , l \in$ $[ 1 , L ] \}$ , where $e _ { i } ^ { l }$ and $e _ { i + 1 } ^ { l }$ denote the head and tail entity, respectively, $I$ is the number of entities, $L$ denotes the number of relations, and $r _ { l }$ is the relation between entities, KGQA requires machines to answer natural language questions $q$ based on retrieved evidence paths $P = \{ p _ { j } \} _ { j = 1 } ^ { m }$ with $p _ { j }$ representing a triple and $m$ denoting the number of triples. In this paper, we leverage LLMs to reason over $P$ and generate answers $\hat { a }$ word by word.

# Overview

As depicted in Fig. 2, given a $K$ -hop question $q$ and the initial topic entity ${ \bf { \bar { \rho } } } _ { e _ { i } ^ { l } }$ within $q$ , our framework first invokes knowledge graphs to retrieve the set of candidate relations $R$ linked to $\mathbf { \bar { \rho } } _ { e _ { i } ^ { l } }$ . Then, it enables LLMs to filter out the most relevant relation $\hat { r } _ { l }$ from $R$ based on in-context learning. Subsequently, the knowledge graph is invoked again to complete the triple information from $( e _ { i } ^ { l } , \hat { r } _ { l } , ? )$ to $( \bar { e } _ { i } ^ { l } , \hat { r } _ { l } , e _ { i + 1 } ^ { l } )$ . Fourthly, DoG focuses on the current reasoning state and employs LLMs to decide on the subsequent action based on the completed triple: providing a direct answer to the question or performing deep thinking with further iterations. In the latter scenario, a multi-role LLM team leverages the mentioned triple to transform the $K$ -hop question to a $K \cdot 1$ hop (slightly easier) one through debate, with the tail entity $e _ { i + 1 } ^ { l }$ being the subsequent topic entity for the simplified question in the next iteration. All of these debate steps are autonomously executed by the LLM team. The iteration will be ended until LLMs generate answers in the fourth step.

# Knowledge Graph Invoking

Reasoning on graphs requires LLMs first to identify relevant knowledge triples. To facilitate this, we have designed two interactive interfaces specifically tailored to retrieve these triples from knowledge graphs. The interfaces are invoked as needed, depending on the requirements.

• get relations $( e _ { i } ^ { l } )$ : This interface is designed to retrieve the candidate relation set $R$ associated with the entity $e _ { i } ^ { l }$ . For example, in Fig. 2, it is invoked to retrieve the candidate relation set of Joe Anderson. • triple filling $( e _ { i } ^ { l } , \hat { r } _ { l } )$ : This interface is responsible for obtaining the tail entity ${ < e _ { i } ^ { l } , \hat { r } _ { l } , \mathord { \left. \right.} >  }$ given the head entity and the filtered relation. We will introduce relation filtering in the next subsection.

The underlying mechanisms of these interfaces are implemented through either SPARQL (for Freebase queries) or specific matching (for questions in MetaQA). To facilitate comprehension and generation by LLMs, all entities and relationships above the interfaces are expressed in natural language, with the conversion between a Machine ID (MID) and a corresponding friendly name carried out exclusively within the interfaces. The MID facilitates efficient access to comprehensive details related to the entity. More specifically, in Freebase, the MID is a unique identifier assigned to each entity, allowing for straightforward retrieval of entityspecific information. The friendly name of the MID is a natural language descriptor. For example, the MID of the friendly name Jamaican is $m . 0 3 _ { - } r 3$ .

# Relation Filtering

Through get relations $( e _ { i } ^ { l } )$ , we obtain a candidate relation set $R$ associated with the initial entity in the question. Subsequently, DoG selects the optimal relation $\hat { r } _ { l }$ from this set through in-context learning. The prompt and in-context examples are detailed in the In-context Learning subsection of the appendix. Specifically, DoG first utilizes LLMs to identify the first-hop problem to be solved in the given question $q$ . Then, it allows LLMs to choose the optimal relation according to the mentioned sub-question. This serves as a guiding principle for relation selection, avoiding the constant reliance on the complete multi-hop question throughout the entire reasoning stage, as seen in previous studies (Jiang et al. 2023a; Sun et al. 2024). We believe this short-sighted greedy strategy can guide a correct progression on the graph, alleviating the need to account for future inferential information regarding the multi-hop question. For example, for the question in Fig. 2 “In what year

Question : In what year was the movie [Joe Anderson] starring in released? KG Invoking Question Simplifying def get_relations(el) 8 Current question: In what year was the movie [Joe Anderson] Subgraph Focus: Iteration details 王 a return R 8 starring in released? 1 £ Relation Filtering KPlneoawsne iwnrfiotremfuarttiohne:r[JqouesAtinodnesr.son, \~starred_actors, High Life] ITnarwgheattQyueeasrtiwoans: the movie Joe Anderson starring in released? current task: Which film [expert]: I think further question that need to be addressed 1 业 8 acted by Joe Anderson? should be “What are some notable films in which Joe Anderson When did High Ligh release? rel_12:\~sdtiarrercetde_da_cbtyo;rs; hwaosrkaactned c?”arineeorr.der to gain further insight into his body of X rel_3:\~written_by Subgraph 1 8 ”n\~esetdaerdr!ed_actors” is CR 香 [sicrmitpilicf]y: tIhteh nprkowblheamt  ahnedegxeptetrthesapirdoibs ewmrosntgh.atHnerereedstuol sbedisdolnvoetd in the next step.I think the new question should be “When did Joe Anderson \~directed_by Subgraph Focus KG Invoking High Life  which was starred by Joe Anderson release?” \~starred_by </ deftriplefilinge,rt) [linguist]: Critics' improvements are effective, but the final 8 \~written_by retumn[e,r,eit1] 品 question still contains redundant information, which may X High Life introducing noise, so a more appropriate new question is has_tag Answer Trying “When did [High Life] release?” ? release_year written_by pUmrneoatbrblyletmototdoeacnsisdolewvewr.hnaLetext Able 当 Answ“e2r0G0en9e”rating 品 吕 Iterative Reasoning 2009 Subgraph 2

was the movie Joe Anderson starring in released”, the firsthop question to be addressed is “Which film starred Joe Anderson?”. The linearized relation set is $\{ \sim$ directed by; starred actors; written by (“ ” represents a passive relationship), from which the optimal relation starred actors can be easily selected.

# Answer Trying

After obtaining the optimal relation, our architecture invokes the triple-filling interface triple filling $( e _ { i } ^ { l } , \ \hat { r } _ { l } )$ to acquire a complete triple, such as <Joe Anderson, starred actors, High Life> in Fig. 2. Then, DoG utilizes LLMs to determine whether the retrieved triple can sufficiently support answering the question. If the triple is insufficient, DoG prompts LLMs to deeply contemplate the current question based on the provided triple. This allows DoG to generate answers based on a single triple, thus avoiding excessively long and potentially confusing paths composed of multiple triples. The prompt and in-context examples are detailed in the In-context Learning subsection of the appendix. Notably, if the maximum iteration limit is reached without successfully generating an answer, the parameterized knowledge of LLMs is utilized to respond.

# Question Simplifying

Once LLMs determine that a question is unanswerable with the current retrieved triple, it represents that further exploration is required. Inspired by how humans tackle complex questions, our architecture employs a question-simplifying strategy to transform questions from $K$ hop to $K \cdot 1$ hop based on the retrieved triple. Specifically, DoG utilizes a team of agents with distinct roles to engage in debate, ensuring the reliability of the reasoning process. The debate team consists of three roles.

• Question simplifying expert (R1). This expert provides initial simplifications for questions, which may contain apparent errors. For example, the original question in Fig. 2 is initially simplified as “What are some notable films in which Joe Anderson has acted?”. This is far from the intention of the original question.   
• Critic (R2). The critic examines the simplification efforts of the above expert and offers suggestions for modifications. For instance, the above question is modified into “When did High Life which was starred by Joe Anderson release?”.   
• Linguist (R3). This role ensures that the simplified question is not only semantically correct but also free from redundant information of previously resolved subquestions. For example, the mentioned question is further refined to “When did [High Life] release?”.

Due to the interdependency and progressive nature of the roles played by the three agents, DoG employs a one-by-one discussion strategy (Chan et al. 2023). Each agent, implemented by ChatGPT, takes turns contributing to the ongoing optimization of the simplified question, with the statements made by other agents serving as references for guiding subsequent remarks generation. After simplification, we obtain a slightly easier $K$ -1 hop question, prompting LLMs to undergo iteration once again. In this way, the relation in the first-hop sub-question is removed in the simplified question, effectively avoiding the impact of false positive relations. The iteration process, from knowledge graph invocation to question simplification, continues until LLMs make an answerable decision in the answer-trying module. The prompt and in-context examples are shown in the In-context Learning subsection of the appendix.

<html><body><table><tr><td rowspan="2">Method</td><td rowspan="2">一 Class</td><td rowspan="2">LM</td><td colspan="3">MetaQA</td><td rowspan="2">WebQSP|CWQ丨WebQ|GrailQA</td><td rowspan="2"></td><td rowspan="2"></td><td rowspan="2"></td></tr><tr><td>1-hop</td><td>2-hop</td><td>3-hop</td></tr><tr><td>KV-Mem</td><td rowspan="6">SL</td><td></td><td>96.2</td><td>82.7</td><td>48.9</td><td>46.7</td><td>18.4</td><td></td><td>、</td></tr><tr><td>GraftNet</td><td>=</td><td>97.0</td><td>94.8</td><td>77.7</td><td>66.4</td><td>36.8</td><td></td><td>1</td></tr><tr><td>PullNet</td><td></td><td>97.0</td><td>99.9</td><td>91.4</td><td>68.1</td><td>45.9</td><td></td><td></td></tr><tr><td></td><td>RoBERTa</td><td>97.5</td><td>98.8</td><td>94.8</td><td>66.6</td><td></td><td>=</td><td>=</td></tr><tr><td>EmbedKGQA NSM</td><td></td><td>97.1</td><td>99.9</td><td>98.9</td><td>68.7</td><td>47.6</td><td>=</td><td></td></tr><tr><td>TransferNet</td><td>BERT</td><td>97.5</td><td>100.0</td><td>100.0</td><td>71.4</td><td>48.6</td><td></td><td>= =</td></tr><tr><td>UniKGQA</td><td rowspan="4"></td><td>RoBERTa</td><td>98.0</td><td>99.9</td><td>99.9</td><td>77.2</td><td>51.2</td><td></td><td></td></tr><tr><td>StructGPT</td><td>GPT-3.5-Turbo</td><td>97.1</td><td>97.3</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>KG-GPT</td><td>GPT-3.5-Turbo</td><td>96.3</td><td>94.4</td><td>87.0 94.0</td><td>72.6</td><td>1</td><td>- =</td><td>=</td></tr><tr><td>KB-BINDER</td><td>Codex</td><td>93.5</td><td>99.9</td><td>99.5</td><td>74.4</td><td></td><td></td><td>58.5</td></tr><tr><td>ToG</td><td></td><td>GPT-3.5-Turbo</td><td></td><td></td><td></td><td>76.2</td><td>57.1</td><td>54.5</td><td>68.7</td></tr><tr><td>DoG</td><td></td><td>GPT-3.5-Turbo</td><td>98.6</td><td>96.6</td><td>90.9</td><td>88.6</td><td>58.2</td><td>78.2</td><td>77.8</td></tr><tr><td>DoG</td><td>ICL</td><td>Qwen-14B</td><td>99.5</td><td>92.4</td><td>79.8</td><td>83.2</td><td>48.1</td><td>65.6</td><td>74.6</td></tr><tr><td>DoG</td><td></td><td>LIama-3-8B</td><td>99.8</td><td>91.0</td><td>84.8</td><td>90.2</td><td>55.9</td><td>70.8</td><td>74.8</td></tr><tr><td>DoG</td><td></td><td>GPT-4</td><td>100.0</td><td>99.0</td><td>96.0</td><td>91.0</td><td>56.0</td><td>80.0</td><td>80.0</td></tr></table></body></html>

Table 1: Comparison with previous state-of-the-art Supervised Learning (SL) and In-Context Learning based methods. The best results for SL and ICL methods are marked in bold, and the second-best results are underlined. WebQ denotes the WebQuestions dataset. The ToG measurement on WebQSP is based on the F1 score rather than EM (Hits $\ @ 1$ ).

# Experiments Dataset and Evaluation

We select five public datasets to evaluate the reasoning ability over knowledge graphs: MetaQA (Zhang et al. 2018), WebQSP (Yih et al. 2016), CWQ (Talmor and Berant 2018), WebQuestions (Berant et al. 2013), and GrailQA (Gu et al. 2021). MetaQA comprises a movie ontology derived from the WikiMovies dataset (Miller et al. 2016) and contains three sets of natural language question-answer pairs: 1-hop, 2-hop, and 3-hop. WebQSP contains questions sourced from the WebQuestions dataset, which are answerable using Freebase. CWQ is designed for answering complex questions that require reasoning over multiple web snippets. GraiQA, which tests three-level generalizations including i.i.d., compositional, and zero-shot, covers 3,720 relations and 86 domains from Freebase. Following (Xiong, Bao, and Zhao 2024; Sun et al. 2024), we uniformly sample 500 instances per type for the mentioned five datasets to reduce computational cost. We use exact match accuracy $( \mathrm { H i t s } @ 1 )$ to evaluate the reasoning performance of our framework and baselines following previous works (Jiang et al. $2 0 2 3 \mathrm { a }$ ; Xiong, Bao, and Zhao 2024; Sun et al. 2024; Baek, Aji, and Saffari 2023). For the experiment of integrating DoG with GPT-4, we uniformly sample only 100 instances per type from the mentioned datasets to reduce costs.

# Implementation Settings

We preprocess the MetaQA dataset to construct a structured knowledge graph, facilitating subsequent query and retrieval operations. A local Virtuoso server is deployed for datasets derived from the Freebase. We utilize the OpenAI API to call ChatGPT (gpt-3.5-turbo-0125) and GPT-4 (gpt-4-0613). Additionally, we employ Qwen-14B and Llama-3-8B, running on 8 V100 GPUs, to verify the flexibility of DoG. The maximum number of debate rounds for the multi-agent team is limited to three, with only the best unique relation being recalled. We implement in-context learning across multiple modules: specifically, 10 exemplars for Relation Filtering and Answer Trying, and one exemplar for Question Simplifying.

# Baselines

Inspired by (Jiang et al. 2023a), we compare DoG with previous state-of-the-art supervised learning and in-context learning-based methods, to verify its effectiveness and superiority. Supervised learning: KV-Mem (Miller et al. 2016), GraftNet (Sun et al. 2018), PullNet (Sun, Bedrax-Weiss, and Cohen 2019), EmbedKGQA (Saxena, Tripathi, and Talukdar 2020), NSM (He et al. 2021), TransferNet (Shi et al. 2021), UniKGQA (Jiang et al. 2023b). In-context learning: StructGPT (Jiang et al. 2023a), KG-GPT (Kim et al. 2023), KB-BINDER (Li et al. 2023b), ToG (Sun et al. 2024). The baselines are detailed in the Baseline Introduction subsection of the appendix.

# Reasoning on Knowledge Graphs

Main Result Table 1 presents a comparison across five public datasets. Taking GPT-3.5 as an example, we observe that DoG enables it to achieve competitive results on MetaQA and the best results on the other four datasets compared with baselines. Specifically, DoG outperforms the best-supervised method, UniKGQA, by $1 1 . 4 \%$ on WebQSP. Additionally, it surpasses the best in-context learning method, ToG, by $2 3 . 7 \%$ and $9 . 1 \%$ on WebQuestions and GrailQA, respectively. These datasets comprise complex and compositional questions. Therefore, these results not only highlight the effectiveness and superiority of DoG but also confirm its capability for complex reasoning.

Flexibility Verification We conduct experiments on the aforementioned datasets to explore whether DoG enables other LLMs, including QWen, Llama, and GPT-4, to achieve complex reasoning on knowledge graphs. Experimental results in Table 1 show that DoG facilitates improvements in some cases compared to GPT-3.5. Specifically, DoG with Llama achieves a $1 . 6 \%$ improvement on WebQSP. It also allows GPT-4 to achieve the most significant improvement on the mentioned datasets. These results clearly demonstrate the flexibility and effectiveness of our architecture. We observe that the performance of DoG with Qwen is slightly lower than with other LLMs. This could be attributed to its marginally weaker complex reasoning capabilities compared to other LLMs.

Table 2: Ablation results. MetaQA# denotes the #-hop split of this dataset. SF and QS refer to the subgraph focusing and question simplifying, respectively. R1, R2, and R3 are the different experts in QS. QS’ indicates that the tasks of the mentioned three roles are fused into a single agent. Avg. represents the average performance increase across the datasets.   

<html><body><table><tr><td>Num.</td><td>Settings</td><td>MetaQA²</td><td>MetaQA³</td><td>WebQSP</td><td>CWQ</td><td>WebQ</td><td>GrailQA</td><td>Avg.</td></tr><tr><td>1</td><td>w/o SF and QS</td><td>76.6</td><td>38.8</td><td>77.4</td><td>43.0</td><td>67.8</td><td>69.3</td><td></td></tr><tr><td>2</td><td>w/ SF and R1</td><td>91.4+14.8</td><td>83.4+44.6</td><td>81.0+3.6</td><td>50.0+7.0</td><td>69.8+2.0</td><td>75.2+5.9</td><td>+13.0</td></tr><tr><td>3</td><td>w/ SF,R1 and R2</td><td>90.6+14.0</td><td>85.2+46.6</td><td>83.6+6.2</td><td>52.2+9.2</td><td>72.2+4.4</td><td>78.2+8.9</td><td>+14.9</td></tr><tr><td>4</td><td>w/ SF,R1,R2,and R3</td><td>96.6+20.0</td><td>90.9+52.1</td><td>88.6+11.2</td><td>58.2+15.2</td><td>78.2+10.4</td><td>77.8+8.5</td><td>+19.6</td></tr><tr><td>5</td><td>w/ SF and QS'</td><td>86.6+10.0</td><td>68.2+30.6</td><td>81.4+4.0</td><td>46.8+3.8</td><td>71.2+3.4</td><td>71.8+2.5</td><td>+9.3</td></tr></table></body></html>

![](images/0b4f09e71c09ff4f3cc29463a3cc6c35367d33b25f8c60c0afd8a66d6451c103.jpg)  
Figure 3: Impact of debate rounds for LLM reasoning on knowledge graphs. It is unnecessary to simplify the question for the 1-hop question within MetaQA.

# Ablation Studies

We conduct ablation experiments on the aforementioned datasets to analyze the contribution of each component of DoG. The ablation results for DoG with GPT-3.5 are presented in Table 2. We perform experiments on the 2-hop and 3-hop splits of MetaQA, as the 1-hop questions do not require complex reasoning. Row 1 shows the results without the subgraph-focusing and question-simplifying components. In other words, this configuration allows LLMs to answer complex questions directly after collecting the whole set of evidence triples, rather than reasoning step by step. We observe a significant performance decrease compared to the results in Row 4, strongly demonstrating the effectiveness of the mentioned modules. Rows 2 and 3 aim to verify the contribution of the expert role in the debate team. The results show consistent improvements across five public datasets, suggesting that each agent plays a critical role in simplifying questions. This also highlights the importance of transforming complex questions into simpler ones for LLMs step-by-step reasoning on knowledge graphs. Row 5 aims to verify the necessity of the debating process, where the tasks of the three roles are performed by a single agent. The average result decreases by $10 . 3 \%$ compared to Row 4, strongly supporting the effectiveness of the debating mechanism.

# Analyses for Debate Rounds

We conduct experiments to explore how the number of debate rounds affects LLM reasoning on knowledge graphs. Fig. 3 shows the performance trend of DoG with GPT-3.5 as the number of debate rounds increases across the five datasets mentioned. We observe that DoG achieves the best results on the majority of datasets with just a single round of debates. Additionally, increasing the number of debate rounds leads to a performance decrease in some datasets. DoG utilizes a one-by-one discussion strategy, which makes each agent aware of the historical debate record. This makes the agents more susceptible to being influenced by the views of others, potentially leading to inaccurate decisions for question simplifications. We may also conclude that the agent is sufficiently strong to achieve the goal of instructions without needing iterative debates.

# Exemplar Impacts

DoG leverages in-context learning to guide LLMs in performing relation filtering, question simplification, and answer trying during iterative reasoning. Specifically, DoG provides instructions and exemplars to help LLMs achieve these objectives. We conduct experiments on five public datasets to explore the impact of the number of exemplars on LLM reasoning. Fig. 4 shows the analyses for the mentioned three modules. In Relation Filtering, we observe that reasoning performance improves as the number of exemplars increases in the majority of datasets. However, reasoning errors caused by relation filtering account for a large proportion, which we will discuss in the next subsection. In Question Simplifying, the performance improvement is not significant with the increase in the number of exemplars, likely due to the complexity of this task. Converting questions from complex to simple step-by-step may be challenging for LLMs, and they may not be able to infer strategies for addressing this issue from exemplars. In Answer Trying, we see that reasoning performance improves with the increase in the number of exemplars in most cases. In summary, the number of exemplars plays a critical role in decision-making, especially for less complex tasks. In contrast, for more complex tasks, detailed instructions may have a greater impact on LLM reasoning.

# Error Analyses

To analyze the deficiency of DoG, we randomly select 50 failure cases from each dataset, including MetaQA, WebQSP, and GrailQA, for manual inspection. Fig. 5 shows the proportion of factors contributing to these errors. We observe that relation filtering errors are quite common. This may be caused by too many relations linked to the entities in questions, making it difficult for LLMs to accurately filter the most relevant relation. Iteration stopping errors denote LLMs make inaccurate decisions in the answer-trying module, either terminating the iterative reasoning too early or too late. This type of error is particularly prevalent in GrailQA cases. Answer aliasing errors mean the generated answers do not have the same description or wording as the annotations, even though they are semantically consistent. This error can be mitigated by introducing a rich collection of aliases. Answer generation errors refer to that LLMs provide incorrect answers based on accurately retrieved triples and simplified questions. Question simplifying errors represent that LLMs fail to transform questions from complex to easy. Additionally, other errors account for $4 \%$ of the failure cases in each dataset. This type of error often occurs due to API access issues, an excessively long context, or exceeding the token limit per minute. More details can be found in the Failure Cases subsection of the appendix.

![](images/886886e69681a68bebe3cf4c5d5911a81c93c778bda3bcdc6d15ce42f2252b13.jpg)  
Figure 4: Impacts of the number of exemplars on the performance of LLM reasoning. It is unnecessary to perform question simplifying for the 1-hop question within MetaQA. DoG does not utilize LLMs to generate answers for questions within MetaQA. Instead, it provides answers based on the last retrieved triple after iterative reasoning.

![](images/47de2cc41ce79549c86b5422d16707a81320bd9feda718631a7dcf4228d72f88.jpg)  
Figure 5: Analysis of 50 sampled failure cases per dataset. We visualize the proportion of factors contributing to errors. We do not perform manual inspection for the failure cases in CWQ and WebQ due to the lack of annotations, such as those for the ground-truth relations.

# Conclusion and Future Work

This paper proposes an iterative interactive framework, DoG, for knowledge graph question answering. It leverages the interactive learning and reasoning capabilities of LLMs to perform debating on knowledge graphs. Specifically, it employs a team of multi-role agents to transform questions from complex to simple, enabling LLMs to perform reliable step-by-step reasoning based on the retrieved knowledge triples. Extensive experiments across five public datasets demonstrate the effectiveness and superiority of DoG in the few-shot setting, outperforming nearly all incontext and supervised learning-based baselines. Additionally, the integration results with different LLMs verify its flexibility. In the future, we will explore enhancing relation filtering performance from knowledge graphs given the entity of questions.