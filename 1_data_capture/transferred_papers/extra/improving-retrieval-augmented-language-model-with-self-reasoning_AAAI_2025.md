# Improving Retrieval Augmented Language Model with Self-Reasoning

Yuan $\mathbf { X _ { i } } \mathbf { a } ^ { 1 }$ , Jingbo Zhou2,\*, Zhenhui $\mathbf { S h i ^ { \mathrm { 1 } } }$ , Jun Chen1, Haifeng Huang1

1Baidu Inc., China 2Baidu Research, China {xiayuan,zhoujingbo,shizhenhui,chenjun22,huanghaifeng}@baidu.com

# Abstract

The Retrieval-Augmented Language Model (RALM) has demonstrated remarkable performance on knowledgeintensive tasks by integrating external knowledge during inference, which mitigates the factual hallucinations inherited in large language models (LLMs). Despite these advancements, challenges persist in the implementation of RALMs, particularly in terms of reliability and traceability. Specifically, the irrelevant document retrieval may result in unhelpful responses or even deteriorate the performance of LLMs, while the lack of appropriate citations in outputs complicates efforts to verify the trustworthiness of the models. To this end, we propose a novel self-reasoning framework aimed at improving the reliability and traceability of RALMs, whose core idea is to leverage reasoning trajectories generated by the LLM itself. The framework involves constructing self-reasoning trajectories through three processes: a relevance-aware process, an evidenceaware selective process, and a trajectory analysis process. We evaluated our framework across four public datasets (two short-form QA datasets, one long-form QA dataset, and one fact verification dataset) to demonstrate its superiority. Our method can outperform existing state-of-the-art models and achieve performance comparable with GPT-4, using only 2,000 training samples.

Documents Question Who painted the ceiling of the Florence Cathedral? Relevant-Aware Process The provide documents are relevant with question Evidence-Aware Selective Process Document 1: … painted surface, was started in 1568 by Giorgio Vasari and Federico Zuccari and would last till 1579. The upper portion, near the lantern, representing The 24 Elders of Apocalypse was finished by Vasari before his death SELF-REASONING This piece indicates that the painting of the Florence Cathedral was started in 1568 by Giorgio Vasari and Federico Zuccari, which directly answers the question. Trajectory Analysis Process The ceiling of the Florence Cathedral was painted by Federico Zuccari and his collaborators, beginning in 1568 and completing in 1579. The work was started by Giorgio Vasari and later continued by Federico Zuccari, with contributions from various artists, including Domenico Cresti [1][3] 3 Final Answer Giorgio Vasari, Federico Zuccari

# Introduction

The Retrieval-Augmented Language Model (RALM), also known as Retrieval-Augmented Generation (RAG), has become a crucial enhancement for Large Language Models (LLMs) by integrating external knowledge during inference. Despite their advanced capabilities in language understanding and generation (Brown et al. 2020; Touvron et al. 2023), LLMs are prone to producing hallucinated and inaccurate content, especially in knowledge-intensive tasks (Ji et al. 2023). Augmenting LLMs with relevant information obtained from external sources like Wikipedia and search engines has proven effective in reducing these inaccuracies (Guu et al. 2020; Lewis et al. 2020; Borgeaud et al. 2022; Izacard et al. 2022; Asai et al. 2024). This approach has proven effective in mitigating the factual hallucinations that

are inherent in LLMs (Kwiatkowski et al. 2019; Petroni et al.   
2021; Ram et al. 2023).

Nevertheless, there are still limitations associated with RALMs, particularly concerning reliability and traceability. Firstly, the reliability of the retrieved information remains a substantial concern. Previous studies have shown that noisy retrieval can adversely affect the performance of an LLM (Menick et al. 2022; Li et al. 2023), as irrelevant data can lead to misguided responses and disturb the model’s ability to leverage its intrinsic knowledge effectively. Secondly, the interpretability and traceability of outputs generated by RALMs need to be improved. Although RALMs incorporate retrieved documents during both the training and inference phases, they may fail to explicitly cite these documents, thus complicating the process of tracing and verifying the claims made by LLMs. To improve the retrieval robustness, recent studies have explored incorporating external tools such as natural language inference (NLI) models (Honovich et al. 2022) and document summarization models during inference (Yoran et al. 2023; Xu et al. 2024). However, the effectiveness of these external tools largely influences the overall performance of RALMs. Additionally, training and optimizing these auxiliary models require additional costs. Consequently, identifying the most appropriate training and selection methods for NLI and summarization models remains a critical challenge in leveraging these approaches.

To address the above limitations, we propose a novel endto-end SELF-REASONING framework to improve the performance of RALMs. For convenience, we will also refer to this framework as SELF-REASONING RAG and use the terms interchangeably. Our intuition is that the explicit self-reasoning trajectory crafted by LLMs can improve both the retrieval robustness and accuracy in question answering. During the pre-training phase, while an LLM primarily focuses on knowledge acquisition, it does not learn to reason from retrieved documents to generate answers. To address this, a feasible approach is to incorporate reasoning trajectories into a post-training phase. Such an approach could potentially teach the model to reason and distinguish relevant and irrelevant documents, thereby enhancing its query response accuracy. An example of how our SELF-REASONING framework generates reasoning trajectories is illustrated in Figure 1. In contrast, as shown in the middle part of Figure 2, the conventional RALM methods gather all documents in a non-selective manner, leading to the distraction of the LLM by irrelevant content and consequently resulting in the generation of erroneous answers.

Our framework constructs self-reasoning trajectories comprising three processes: 1) a Relevance-Aware Process (RAP), which instructs the LLM to judge the relevance between the retrieved documents and the question, 2) an Evidence-Aware Selective Process (EAP), which directs the LLM to choose and cite relevant documents, and then automatically select snippets of key sentences as evidence from the cited documents, 3) a Trajectory Analysis Process (TAP), which requires the LLM to synthesize a concise analysis based on all gathered self-reasoning trajectories generated by previous two processes and subsequently provide the final inferred answer. Furthermore, we propose a gradual training method by employing stage-wise masking strategies to enhance the performance of our framework. We summarize our contributions as follows:

• We propose a novel end-to-end SELF-REASONING framework that improves the robustness of RALMs by leveraging reasoning trajectories generated by the LLM itself, without the need for external tools. • We carefully design three processes to enhance the interpretability and traceability of RALMs by requiring LLMs to explicitly generate snippets and citations from documents, and further explain the reason why cited documents can help answer the question.

• We evaluate our framework on four public datasets (two short-form QA, one long-form QA, and one fact verification), demonstrating that our method surpasses existing state-of-the-art models in performance using only 2,000 training samples.

# Related Work

# Retrieval-augmented LMs

Many studies have investigated augmenting the performance of LLMs with externally retrieved information (Izacard et al. 2022; Guu et al. 2020; Borgeaud et al. 2022) and some of them pre-train language models with retrieved passages. For works focusing on RALMs with citations, Menick et al. (2022); Nakano et al. (2021) instruct or train an LLM to answer questions with retrieved documents while providing citations. Gao et al. (2023b) proposes an end-to-end system to retrieve supporting evidence and generate answers with citations, while only focusing on prompting without updating their model weights. Other works instruct or fine-tune LLMs to use external tools to retrieve dynamically (Schick et al. 2023; Yao et al. 2023; Jiang et al. 2023), which offers an adaptive method of when and what to search. Gao et al. (2023a) improves the attribution and factuality of language models by taking outputs of LLMs and applying a post-process retrieve-and-edit approach.

# Robustness for RALMs

To improve the robustness of RALMs, previous works can be divided into two categories. The first category utilizes retrieved documents to enhance the Chain of Thought (CoT). For example, IRCoT (Trivedi et al. 2023) iteratively uses retrieved documents to generate CoT, which is then used to retrieve further documents in subsequent steps. ReAct (Yao et al. 2023) introduces an iterative CoT paradigm that integrates reasoning with search results. However, irrelevant retrievals may produce misguided CoT, adversely affecting LLM performance (Menick et al. 2022; Li et al. 2023).

To address the issue of irrelevant retrieval information, the second category proposes using external modules to process retrieved documents during inference. For instance, Yoran et al. (2023) utilize a natural language inference model to filter out irrelevant documents, Yan et al. (2024) employ a retrieval evaluator to classify documents based on their quality, and $\mathrm { { X u } }$ et al. (2024) and Yu et al. (2023) apply models to filter out or compress retrieved documents. Baek et al. (2023) deploy a separate small language model as a verifier to detect and correct errors in LLMs during retrieval. A method presented by Asai et al. (2024), which appears most similar to our approach, develops a technique that instructs models to retrieve information using specifically designed reflection tokens. However, this approach needs to train extra critic models and generator models to predict the reflection tokens, which requires tens of thousands of extra training samples.

Unlike the second group of works, which rely on external tools or additional modules to eliminate irrelevant information, the SELF-REASONING RAG method integrates selfreasoning directly into the model’s architecture, thereby enhancing the performance of LLMs and providing a more efficient and scalable solution. Further related works on LLMs for reasoning are discussed in the Appendix.

![](images/0501660182b615e0e8fc25a655240e0e782f397450346ac498f3f248fdedfc14.jpg)  
Figure 2: An illustration of the SELF-REASONING framework. The upper is the basic LLMs which answer the question by inherent knowledge. The middle is the standard retrieval augmented LMs, which use retrieved documents to help answer the question. The bottom is our SELF-REASONING framework which uses self-generated reason trajectories to output answers.

# Preliminary

We formally define the problem of retrieval augmented generation with self-reasoning. Given a query $q$ and a corpus of documents $\mathcal { D }$ , an LLM-generated answer with $m$ statements and $n$ tokens can be defined as $y = ( s _ { 1 } , s _ { 2 } , \cdot \cdot \cdot , s _ { m } ) =$ $( w _ { 1 } , w _ { 2 } , \cdot \cdot \cdot , w _ { n } )$ , where $s _ { i }$ is the $i$ -th statement and $w _ { j }$ is the $j$ -th token in the generated answer. In addition, for longform QA settings, each statement $s _ { i }$ should cite a list of documents $C _ { i } = \bar { \{ c _ { i } ^ { ( 1 ) } , c _ { i } ^ { ( 2 ) } , \ldots \} }$ , where ci(k) ∈ D. In our work, we train an LLM (e.g. LLaMA2) to first generate reasoning trajectories $\tau$ through self-reasoning and then to generate answers $y ^ { \ast }$ (short-form answers) on condition of $\tau$ . The model output is $y = \operatorname { c o n c a t } ( \tau , y ^ { * } )$ , which is the concatenation of $\tau$ and $y ^ { * }$ . Note that the generations of $\tau$ and $y ^ { \ast }$ are done in a single pass within the SELF-REASONING framework.

# Method

Here we provide a detailed implementation of the selfreasoning process which involves three processes: 1) a Relevance-Aware Process (RAP), 2) an Evidence-Aware Selective Process (EAP), and 3) a Trajectory Analysis Process (TAP). An illustration of our SELF-REASONING framework is shown in Figure 2. Additionally, we outline the process of data generation and quality control, and present the specifics of model training.

# Relevance-Aware Process

In this work, we choose DPR (Karpukhin et al. 2020) and Contriever (Izacard et al. 2021) as default retrievers $R$ to recall the top- $k$ relevant documents. When presented with a question and a set of documents, people can determine whether the question is relevant to the retrieved documents. Therefore, we first instruct the model to judge the relevance between the retrieved documents $\mathcal { D }$ and the given question $q$ . We further request the model to explicitly generate reasons explaining why given documents are identified as relevant. The output should include two fields as relevant and relevant reason, as depicted in Figure 2. If all of the retrieved documents are irrelevant, the model should provide an answer based on the internal knowledge acquired during its pre-training phase. We define the self-reasoning trajectories generated by RAP as $\tau _ { r }$ .

# Evidence-Aware Selective Process

When answering a question, people generally first identify the crucial sentences from the provided documents and then cite or highlight them as key points. This process of citing the document facilitates reading comprehension and can serve as a technique for combining multiple short answers to address various aspects. While people may carry out this selective process and citation instantaneously, LLMs need to formulate the self-reasoning trajectories explicitly.

In our work, we require the LLM to explicitly state the reason why the selected sentence is supportive and plausible in answering the question. We define the selected sentence as evidence in our paper. Specifically, after retrieving the top- $k$ documents, the self-reasoning method for Evidence

Aware Selective Process can be formulated as follows: First, we instruct the LLM to choose relevant documents and automatically select snippets of key sentences for the selected documents. Then, we request the LLM to output the reason why the selected snippets can answer the question. The intermediate output is a list containing multiple contents, each content should include two fields, as cite content and reason for cite, which is illustrated in Figure 2. We define the self-reasoning trajectories generated by EAP as $\tau _ { e }$ .

# Trajectory Analysis Process

Finally, we consolidate all the self-reasoning trajectories ${ \bf \Xi } _ { \mathcal { T } _ { r } }$ and $\tau _ { e }$ ) in the previous processes together to form a chain of reasoning snippets, thereby enhancing the overall performance of the retrieval augmentation generation. Specifically, we ask the LLM to analyze the reasoning trajectories within itself and ultimately to output a concise analysis and a short answer. We instruct the LLM to output content with two fields as analysis and answer, which is shown in Figure 2. We define the self-reasoning trajectories generated by TAP as $\tau _ { a }$ . In this work, the analysis output is defined as a longform answer, and the answer output is defined as a shortform answer. In the experiment section, we further explored the performance of long-form and short-form QA settings.

# Data Generation and Quality Control

Training Data Generation. For the Relevance-Aware Process data generation, as manually labeling the relevant and irrelevant documents is label-intensive, we request GPT4 (OpenAI 2023) to generate answers as ground truth. Specifically, we instruct GPT-4 to generate labels regarding irrelevant fields, and further to output the reasons why the given documents cannot answer the question. We concatenate the given question and the retrieved documents as positive samples. For negative samples, we randomly select a different question from the training set and retrieve the top- $k$ documents related to it. These documents are then concatenated with the initial question to form negative samples. To avoid order bias in the training data, we shuffle the order of the documents.

For the EAP and TAP data generation, manually annotating the citation and writing the self-reasoning process for each question is not feasible in practice. Therefore, we follow a similar process to RAP, we first instruct GPT-4 to generate a snippet of selected documents and subsequently output the reasoning process as trajectories. The method for constructing the EAP training data is the same as RAP except that the instructions given to GPT-4 are different. The details of the instructions are shown in the Appendix.

Data Quality Control. For training data generation, correct and comprehensive reasoning trajectories are very important. When training an LLM, the quality of the training samples is more important than the quantity (Zhou et al. 2023). As we cannot guarantee the correctness of selfreasoning trajectories and citations by GPT-4, we develop two efficient methods to control the quality of data generation: 1) The first method is to use the off-the-shelf tools Gao et al. (2023b) to automatically verify the performance of data generation for document citations. We calculate the citation precision and recall score for each training sample and filter out scores lower than our pre-defined thresholds $\delta _ { p }$ and $\delta _ { r }$ , for citation precision and recall, respectively. 2) Second, though the validation of self-reasoning trajectories and citations generated by GPT-4 is challenging, verifying the correctness of the final answer is straightforward. Therefore, we filter out the trajectories that lead to the incorrect answers and only keep the correct ones. We totally generate 10,000 training samples by GPT-4, after the filtering strategy by quality control, we finally keep 2,000 training samples with high quality. More details and pseudo-codes can be found in the Appendix.

# Model Training

We train the self-reasoning RAG model $\phi$ by our constructed corpus which is augmented with self-reasoning trajectories $\tau$ using the standard language modeling objective, maximizing likelihood:

$$
\operatorname* { m a x } _ { \phi } \mathbb { E } _ { ( q , \tau , y ) \sim \mathcal { D } _ { s r } } \log p _ { \phi } ( y \mid \tau , q ) p _ { \phi } ( \tau \mid q )
$$

where $\tau = \tau _ { r } \oplus \tau _ { e } \oplus \tau _ { a }$ are the self-reasoning trajectories, $\oplus$ is a concatenation operator, $\tau _ { r } , \tau _ { e } , \tau _ { a }$ are trajectories generated by above three processes respectively. $q$ is the provided question, and $y$ is the model output, including the intermediate reason trajectories and the final answer. $\mathcal { D } _ { s r }$ is the training corpus augmented with self-reasoning trajectories.

During training, we observed that it is more challenging to ensure the correctness of an LLM with 13B parameters when generating long reasoning trajectories than short ones. We hypothesize that an LLM’s effective reasoning length is limited and exceeding this limit might lead to error accumulation during the inference stage. Therefore, we propose a gradual training method by employing stage-wise masking strategies to gradually learn to generate long trajectories.

Specifically, we propose a stage-wise training process while we train the LLM stage by stage. In the first stage, we mask the trajectories produced by the next two stages (EAP and TAP) and train the model with a learning rate $\boldsymbol { r } _ { a }$ . Then in the second stage, we only mask the trajectories generated by TAP and train the model with a learning rate $r _ { b }$ . Finally, we concatenate the reasoning trajectories from all stages and put them into a self-reasoning LLM for end-to-end training with a learning rate $r _ { c }$ . Hyper-parameters for training are described in the Appendix.

# Experiments

# Datasets and Settings

To demonstrate the effectiveness of our proposed SELFREASONING framework, we conduct an extensive experimental evaluation on two short-form QA datasets (NaturalQuestion (Kwiatkowski et al. 2019) and PopQA (Mallen et al. 2023)), one long-form QA dataset (ASQA (Stelmakh et al. 2022)), and one fact verification dataset (FEVER (Thorne et al. 2018)). Detailed descriptions of the datasets can be found in the Appendix. We explore off-the-shelf retrievers. We use DPR (Karpukhin et al. 2020) and

Table 1: Performance comparisons with different baseline models on two short-form QA datasets, a long-form QA dataset, and a fact verification dataset. The numbers with bold black represent the best results excluding GPT-4. The results are averaged over five runs, and presented with standard variance values omitted (all $\leq 2 \%$ ).   

<html><body><table><tr><td rowspan="2">Models</td><td>NaturalQuestion</td><td>PopQA</td><td>FEVER</td><td colspan="3">ASQA</td></tr><tr><td>(acc)</td><td>(acc)</td><td>(acc)</td><td>(em-recall)</td><td>(precision)</td><td>(recall)</td></tr><tr><td colspan="7">Baselines without retrieval</td></tr><tr><td>LLaMA27B</td><td>19.2</td><td>18.4</td><td>23.2</td><td>10.2</td><td></td><td></td></tr><tr><td>LLaMA213B</td><td>24.0</td><td>22.6</td><td>25.3</td><td>15.3</td><td>-</td><td></td></tr><tr><td>LLaMA273B-ht </td><td></td><td></td><td>265</td><td>163</td><td></td><td></td></tr><tr><td></td><td>20.2</td><td>21.5</td><td></td><td></td><td>--</td><td></td></tr><tr><td colspan="7">Baselineswith retrieval</td></tr><tr><td>LLaMA27B</td><td>27.8</td><td>47.8</td><td>39.8</td><td>28.5</td><td>13.6</td><td>9.59</td></tr><tr><td>LLaMA213B</td><td>34.0</td><td>48.1</td><td>35.2</td><td>26.8</td><td>21.8</td><td>16.3</td></tr><tr><td>LLaMA27B-chat</td><td>27.4</td><td>52.9</td><td>43.4</td><td>25.3</td><td>34.5</td><td>33.2</td></tr><tr><td>LLaMA213B-chat</td><td>32.7</td><td>53.5</td><td>53.4</td><td>26.4</td><td>39.4</td><td>38.4</td></tr><tr><td>Vicuna7B (Chiang et al. 2023)</td><td>28.0</td><td>55.2</td><td>62.4</td><td>24.3</td><td>45.7</td><td>40.8</td></tr><tr><td>Vicunai3B (Chiang et al. 2023)</td><td>35.4</td><td>56.1</td><td>60.6</td><td>27.3</td><td>51.3</td><td>50.2</td></tr><tr><td>LLaMA2-FT7B</td><td>36.8</td><td>54.4</td><td>67.5</td><td>28.5</td><td>47.2</td><td>45.4</td></tr><tr><td>ReCotMP (xt l 2023)24)</td><td>38.4</td><td>_-</td><td>64.6</td><td>__</td><td>--</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Self-RAG7B (Asai et al.2024)</td><td>37.2</td><td>54.9</td><td>70.2</td><td>30.0</td><td>66.9</td><td>67.8</td></tr><tr><td>Self-RAG13B (Asai et al. 2024)</td><td>38.8</td><td>55.8</td><td>72.1</td><td>31.7</td><td>70.3</td><td>71.3</td></tr><tr><td>SELF-REASONING7B</td><td>38.0</td><td>54.2</td><td>78.6</td><td>33.9</td><td>66.3</td><td>70.8</td></tr><tr><td>SELF-REASONING13B</td><td>41.4</td><td>57.3</td><td>83.9</td><td>35.2</td><td>71.2</td><td>72.3</td></tr><tr><td>GPT-4</td><td>46.6</td><td>62.5</td><td>87.7</td><td>41.3</td><td>75.6</td><td>68.5</td></tr></table></body></html>

Contriever-MS MARCO (Izacard et al. 2021) to retrieve the top five documents from Wikipedia.

By default, we use DPR as a retriever for the NQ, as DPR has been fine-tuned on the high-quality NQ data. On the PopQA, where question and answer pairs are created based on Wikipedia in 2022, therefore, for the PopQA, we use the December 2020 preprocessed Wikipedia corpus provided by (Izacard et al. 2022) and use Contriever as a retriever. For the ASQA dataset, we use GTR (Ni et al. 2022) as a retrieval that corresponds to the experimental settings in (Gao et al. 2023b). More settings can be found in the Appendix.

# Evaluation Metrics

We use different evaluation metrics for short-form QA, longform QA, and fact verification tasks.

Short-form QA metrics. We report accuracy for shortform QA tasks, which is based on whether ground-truth answers are included in the model predictions instead of strictly requiring exact matching, following Mallen et al. (2023); Schick et al. (2023).

Long-form QA metrics. For long-form QA tasks, we report the EM recall as a correctness metric, and the citation recall and the citation precision for citation quality, which are the same as the metrics in (Gao et al. 2023b).

Fact verification metrics. For the fact verification task, we report the accuracy as a metric, which is a three-class classification accuracy, following Thorne et al. (2018).

# Baseline Models

Baseline models without retrieval. We evaluate strong open-source pre-trained LLMs as baseline models. For basic LLMs, we test LLaMA2-7B, LLaMA2-13B (Touvron et al. 2023) and its instruction-tuned chat version LLaMA2-Chat7B, LLaMA2-Chat-13B.

Baseline models with retrieval. First, we benchmark the models using the LLaMA2 and the Vicuna (Chiang et al. 2023) series models for baselines. Additionally, for a fair comparison, we also include LLaMA2-FT, where LLaMA2 is fine-tuned on all the training samples generated by GPT4 except the self-reasoning trajectories. To establish strong baselines, we compare our method against RECOMP (Xu et al. 2024), ReAct (Yao et al. 2023), and Self-RAG (Asai et al. 2024), all of which are trained with extra GPT-4 generated samples or external tools. We also compare our framework with GPT-4 (OpenAI 2023). We include categorical comparisons with the baseline models in the Appendix.

# Main Results

Table 1 shows the performance comparisons with different methods on the four public datasets. For short-form QA evaluations, the performance of LLMs with augmented retrieval is consistently better than that of basic ones, affirming the effectiveness of the augmented approach. Notably, under the same order of magnitude parameters, our SELFREASONING framework outperforms most of the strong baseline LLMs. Specifically, compared to the Self-RAG, our framework is an end-to-end system trained with only 2,000 self-reasoning trajectory samples. In contrast, the Self-RAG requires training additional critic LMs to predict reflection tokens using an additional 46,000 instances generated by GPT-4. This efficiency not only simplifies the training process but also significantly reduces resource consumption.

![](images/68525fe0862b7d5221dfceb2645188eb0215fafb92cb499f466a32c4ac70b622.jpg)  
Figure 3: Noise robustness experiment results on three different datasets: (a) On the left is the NQ dataset, (b) in the middle is the PopQA dataset, and (c) on the right is the FEVER dataset. The Self-RAG and Vicuna are 13B parameter size models.

In the context of long-form QA evaluations, for the metrics of EM recall, it needs to comprehend multiple documents and merge answers. The EAP and TAP are specifically designed for multi-document reading comprehension, enabling our performance to surpass other baselines. In terms of citation evaluation metrics, the SELF-REASONING RAG can achieve better results than GPT-4 in ASQA citation recall metrics (72.3 vs. 68.5). This is largely due to the reasoning trajectories generated in the EAP, which can enhance the recall and precision of citation evaluation, leading to more interpretable and traceable generations.

For fact verification evaluations, we observed that SELFSEASONING is dominantly superior to all baseline models. Our method achieves a much higher accuracy rate than the Self-RAG model (83.9 vs. 72.1). The RAP in our framework is designed to judge the relevance between the retrieved documents and the question, which leads to a notable enhancement in accuracy for this fact verification task.

To clearly demonstrate the practical applications and benefits of our SELF-REASONING framework, we provide a case study for a more in-depth analysis in Appendix, which illustrates how our framework operates in real-world scenarios.

# Analysis

# Ablation Study

We conduct an ablation study on two short-form QA datasets and a fact verification dataset to analyze the individual contributions of each process within our proposed SELFREASONING framework. We further explore the effectiveness of the gradual learning (GL) method and the quality control (QC) of data generation (a detailed analysis described in the Appendix). The main ablation study results are shown in Table 2 and Table 3.

Table 2: The ablation study on two short-form QA datasets and a fact verification dataset with 13B parameter size models. In the table, the ORIGIN represents our self-reasoning model enhanced with self-generated trajectories.   

<html><body><table><tr><td rowspan="2">Models</td><td>NQ</td><td>PopQA</td><td>FEVER</td></tr><tr><td>(acc)</td><td>(acc)</td><td>(acc)</td></tr><tr><td>ORIGIN</td><td>41.4</td><td>57.3</td><td>83.9</td></tr><tr><td>w/o (RAP)</td><td>39.9</td><td>54.3</td><td>72.2</td></tr><tr><td>w/o (EAP)</td><td>37.2</td><td>53.2</td><td>78.4</td></tr><tr><td>w/o (TAP)</td><td>38.2</td><td>53.4</td><td>81.2</td></tr><tr><td>w/o (GL)</td><td>39.5</td><td>55.3</td><td>81.2</td></tr><tr><td>w/o (QC)</td><td>37.7</td><td>54.2</td><td>80.8</td></tr></table></body></html>

Table 3: The analysis on the effectiveness of self-reasoning trajectories with 13B parameter size models. In the table, the $+$ trajectory indicates the result of the baseline model is enhanced with self-generated trajectories by our framework.   

<html><body><table><tr><td rowspan="2">Models</td><td>NQ</td><td>PopQA</td><td>FEVER</td></tr><tr><td>(acc)</td><td>(acc)</td><td>(acc)</td></tr><tr><td>LLaMA2</td><td>32.7</td><td>53.5</td><td>53.4</td></tr><tr><td>+_trajectory___38.3</td><td></td><td>54.2</td><td>79.2</td></tr><tr><td>Vicuna</td><td>35.4</td><td>56.1</td><td>60.6</td></tr><tr><td>+ trajectory</td><td>38.5</td><td>56.4</td><td>79.6</td></tr></table></body></html>

Effectiveness of RAP. First, we evaluate the effect of the RAP. The removal of the RAP causes the overall performance to drop in two short-form QA datasets and a fact verification dataset, suggesting that preliminary consideration of the relevance between questions and retrieved documents can help improve performance. We notice that the performance declines most significantly in the FEVER dataset. Detecting irrelevant documents is critical in the factverification task. Our model will immediately output NotEnoughInfo if it detects that all documents are irrelevant.

Effectiveness of EAP. Then we evaluate the effect of the EAP. Removing the EAP causes the overall performance of the average accuracy to decline from 60.9 to 56.3 in three short-form QA datasets, which indicates that snippets of key sentences and document citations generated through selfreasoning are instrumental in boosting accuracy.

Effectiveness of TAP. Finally, we evaluate the effect of the TAP. When excluding the TAP, we can observe a performance decline on all three datasets, demonstrating that self-analysis based on two previous processes generated trajectories can also improve the performance of LLMs. Note that the analysis content generated by TAP is indispensable for the long-form QA evaluation.

Effectiveness of Self-Reasoning Trajectory. To verify whether the trajectories generated by the self-reasoning framework are truly effective, we put the trajectories generated by our SELF-REASONING framework into the original baseline models as input prompts, and then use the baseline models to regenerate the answers. We observe that incorporating self-generated trajectories can significantly enhance performance in short QA tasks and fact verification tasks.

# Retrieval Robustness Analysis

Retrievers are not perfect and past work has shown that noisy retrieval can have negative effects on the performance of LLMs (Petroni et al. 2020; Li et al. 2023). In this section, we design two kinds of settings to validate the robustness of RALMs. In the first setting, we test whether the order of the retrieved documents will affect the performance of the RALMs. Specifically, after retrieving the top- $k$ documents using retrievals with a descending relevance score, we randomly shuffle the order of the retrieved documents and then input them to an LLM. In the second setting, we test how noisy documents impact the performance of LLMs. When retrieving the top- $k$ documents from the given question, we randomly replace $50 \%$ of the retrieved documents with other documents sampled from a different question in the dataset.

Figure 3 shows the noise robustness experiment results on three datasets. Our SELF-REASONING framework consistently outperforms the Self-RAG and Vicuna models. We observe that random shuffling of retrieved documents has a minimal impact on the performance of RALMs. If the provided documents are supportive, it is trivial for a RALM to determine the correct answer. However, when presented with noisy documents, all models experience a decline in performance. The performance drop in our self-reasoning framework is relatively minimal, demonstrating the robustness of our method even when handling noisy documents.

# Citation Analysis

As the automatic evaluation by the NLI model cannot detect partially supported citations, we discuss the analysis of citations with human evaluation in this section. Similarly to Liu, Zhang, and Liang (2023), we conduct a human evaluation on two dimensions: 1) citation recall: annotators are given a statement and all documents that the statement refers to and are asked to judge whether the documents fully support the given statement; 2) citation precision: given a statement and one of its citations, annotators are asked to validate whether the citation fully supports, partially supports or does not support the statement. As shown in Figure 4, the relative rankings by human evaluation align well with those from the automatic evaluation, and the human evaluation often yields a closely higher score when compared with the automatic evaluation. Details of human annotation can be found in the Appendix.

![](images/814a6091615d898c720869449d750c8d3b378e0f3b48f4416e2296985c56e7a4.jpg)  
Figure 4: Human citation quality evaluation vs. automatic citation evaluation on the long-form ASQA dataset.

# Latency Analysis

We also compared the inference latency of SELFREASONING RAG with that of Self-RAG and GPT-4. The results show that our method maintains comparable latency to Self-RAG while delivering substantial performance gains. Detailed results are available in the Appendix.

# Conclusion

RALMs can effectively enhance the performance of LLMs in handling knowledge-intensive tasks. Despite their effectiveness, notable concerns about their reliability and traceability persist. To address these limitations, we propose a novel SELF-REASONING framework to improve the performance of RALMs by using reasoning trajectories generated by the LLM itself. It is comprised of a relevance-aware process, an evidence-aware selective process, and a trajectory analysis process. We conduct extensive experiments on four public datasets to demonstrate the superiority of our framework over existing state-of-the-art models.