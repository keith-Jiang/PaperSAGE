# Thought-Path Contrastive Learning via Premise-Oriented Data Augmentation for Logical Reading Comprehension

Chenxu Wang1, Ping Jian\*1,2, Zhen Yang1

1School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China 2Beijing Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications, Beijing Institute of Technology, Beijing, China {wangchenxu, pjian, bityangzhen}@bit.edu.cn

# Abstract

Logical reading comprehension is a challenging task that involves understanding the underlying semantics of text and applying reasoning to deduce the correct answer. Prior researches have primarily focused on enhancing logical reasoning capabilities through Chain-of-Thought (CoT) or data augmentation. However, previous work constructing chainof-thought rationales concentrates solely on analyzing correct options, neglecting the incorrect alternatives. Addtionally, earlier efforts on data augmentation by altering contexts rely on rule-based methods, which result in generated contexts that lack diversity and coherence. To address these issues, we propose a Premise-Oriented Data Augmentation (PODA) framework. This framework can generate CoT rationales including analyses for both correct and incorrect options, while constructing diverse and high-quality counterfactual contexts from incorrect candidate options. We integrate summarizing premises and identifying premises for each option into rationales. Subsequently, we employ multistep prompts with identified premises to construct counterfactual context. To facilitate the model’s capabilities to better differentiate the reasoning process associated with each option, we introduce a novel thought-path contrastive learning method that compares reasoning paths between the original and counterfactual samples. Experimental results on three representative LLMs demonstrate that our method can improve the baselines substantially across two challenging logical reasoning benchmarks (ReClor and LogiQA 2.0).

# Code — https://github.com/lalalamdbf/TPReasoner

# 1 Introduction

Logical reasoning is a fundamental component of human cognition, essential for comprehending text and applying reasoning to deduce appropriate conclusions. Recently, challenging logical reasoning benchmarks have been proposed through machine reading comprehension (MRC) tasks (Yu et al. 2020; Liu et al. 2023a), which require models to derive the correct answer based on the given context, question and options. With the advent of large language models (LLMs), enhancing their capabilities in logical reasoning is a crucial step toward achieving strong artificial intelligence (Chollet 2019). Especially, the highly advanced

Question: The conclusion of the salesperson‘s argument is most strongly supported if which one of the following is assumed? Original Context: Salesperson: Counterfactual Context: If your vacuuming needs are Salesperson: For those small, limited to small areas, … and uncarpeted areas that need, … works on wood and tile floors. and tile floors remain spotless. (a) The only types of floor surfaces (b) If your household cleaning needs that most consumers encounter are include … , it is likely that you will carpet, wood, and tile. $\smile$ need a vacuum cleaner. × model, GPT-4 (Achiam et al. 2023), has exhibited remarkable abilities to handle such tasks. However, a broad spectrum of open LLMs, including LLaMA2 (Touvron et al. 2023), Mistral (Jiang et al. 2023) and LLaMA3 (AI@Meta 2024), still fall short in logical reasoning, significantly trailing behind GPT-4. Consequently, improving the logical reasoning capabilities of community models has increasingly attracted the attention of many researchers (Liu et al. 2023b; Jiang et al. 2023).

For logical MRC tasks, LogiCoT (Liu et al. 2023b) constructs instruction-tuning data with Chain-of-Thought (CoT) rationales. Nevertheless, these rationales only provide analyses for the correct options, neglecting the incorrect alternatives. This oversight limits the model’s ability to fully understand why certain answers are wrong, which is crucial for enhancing its reasoning capabilities and overall performance in distinguishing between similar options. In addition, previous studies typically create counterfactual contexts based on rule-based data augmentation. For instance, LReasoner (Wang et al. 2022) generates logically nonequivalent sentences by utilizing templates and syntax parsing. AMR-LDA (Bao et al. 2023) constructs counterfactual sentences based on Abstract Meaning Representation (AMR, Banarescu et al., 2013) graph and logical laws. These methods rely on complex principles and make minimal changes to the text, that cannot ensure the diversity of generated content and accurate modifications to its underlying logic. Additionally, they directly modify the context without considering its relationship with the options, which leads to a mismatch between the counterfactual context and options.

Table 1: A logical reasoning example. The CoT rationale is annotated by GPT-3.5 or GPT-4. Due to space constraints, we refer to the specific reasoning process as [thought-path].   

<html><body><table><tr><td>Context:[Content of the context]</td></tr><tr><td>Question:[Content of the question]</td></tr><tr><td>Options: [Content of the options]</td></tr><tr><td>Summarize Premises: 1.[Premise 1] 2.[Premise 2] 3.[Premise 3] Analyze Options: (a)[Thought-path1] Identify Premises :Unrelated to the premises. (b)[Thought-path 2] Identify Premises: Supported by premises 2 and 3. (c)[Thought-path 3] Identify Premises:Unrelated to the premises. (d)[Thought-path 4] Identify Premises: Contradicted by premise 1. [A summary of thought-paths]. Therefore, the optimal correct answer is (b).</td></tr></table></body></html>

In view of above challenges, we propose a premisesoriented data augmentation (PODA) framework. As shown in Figure 1 and Table 1, the objective of PODA is to generate CoT rationales that include analyses for both correct and incorrect options, while also constructing counterfactual contexts based on incorrect candidate options. In Table 1, analyses for both correct and incorrect options are presented in Analyze options. Besides, we incorporate summarizing premises and identifying premises for each option into rationales. Each option has a specific relationship with these premises—either supported, contradicted, or unrelated. PODA will create high-quality and diverse counterfactual contexts using multi-step prompts based on these premises and relationships. Furthermore, since supervised fine-tuning (SFT) focuses solely on individual instances, it lacks the comparison between different samples. For original and counterfactual samples, there are thought-paths that indicate similar and dissimilar reasoning processes associated with options. Therefore, we propose a thoughtpath contrastive learning approach, which specifically compares thought-paths across different samples, facilitating the model’s capabilities to better distinguish diverse reasoning paths. The main contributions of this paper are summarized as follows:

• We propose a premise-oriented data augmentation framework, which can generate CoT rationales involving analyses for both correct and incorrect options, while automatically constructing diverse and high-quality counterfactual data from incorrect candidate options. • We introduce a thought-path contrastive learning approach, facilitating models to distinguish different reasoning paths between original and counterfactual samples.

• Experimental results conducted on representative open LLMs (LLaMA2-7B, Mistral-7B and LLaMA3-8B) demonstrate that our method achieves superior performance on two logical MRC benchmarks.

# 2 Related Work

# 2.1 Chain-of-Thought Prompting

LLMs are capable of performing complex reasoning to derive the final answer by generating intermediate reasoning steps through a process called Chain-of-Thought (CoT). Zero-shot-CoT (Kojima et al. 2022) showcases impressive reasoning performance only using a single instruction ”Let’s think step by step”. Few-shot-CoT (Zhang et al. 2022; Wang et al. 2023) further boosts the reasoning abilities of LLMs by incorporating several CoT demonstrations. In addition, by offering carefully-crafted CoT demonstrations, LLMs can be encouraged to develop the similar reasoning skills and deliver responses in a uniform format. To ensure obtained CoTs are well-structured, we adopt Few-shot-CoT for data collection using GPT-3.5 and GPT-4. Recently, Liu et al. (2023c) also utilized GPT-4 to annotate the intermediate steps of correct options for logical MRC tasks. In contrast, our study expands the analysis to include incorrect options and focuses on mining information from CoT rationales to generate new logical MRC data.

# 2.2 Logical Reasoning

Leveraging logical reasoning capabilities embodies a comprehensive approach to natural language understanding (NLU). Previous studies have primarily focused on integrating logical knowledge into language models. For example, Huang et al. (2021) exploited a logic graph to model semantic relationships. Wang et al. (2022) and Bao et al. (2023) constructed equivalent/nonequivalent instances through intricate logic rules and entity replacement. These techniques, however, are constrained by their reliance on manually designed rules, which struggles to reliably identify complex logical relationships in diverse texts. Thus, our work shifts away from annotating logical relationships. Instead, we decompose and construct contexts using premises as the foundational units. Moreover, our contrastive learning approach improves LLMs’ logical reasoning capabilities by enabling them to distinguish various thought-paths.

# 3 Methodology

Figure 2 shows the overall architecture of our method (PODA-TPCL). It consists of two key components: PremiseOriented Data Augmentation (PODA) and Thought-Path Contrastive Learning (TPCL). The former module is aimed at generating CoT rationales that comprise analyses for correct and incorrect options, while constructing diverse and high-quality counterfactual logical reasoning data from incorrect candidate options. The latter one enhances the reasoning capabilities by comparing thought-paths between the original and counterfactual samples.

![](images/c7950976401ce2b0716c9167368b379a1c889256863c2dea5659f7f12a15a9b1.jpg)  
Figure 2: The overall architecture of our method. (1) PODA annotates Chain-of-Thought (CoT) rationales and generates counterfactual logical reasoning data. (2) The original and counterfactual samples are used for thought-path contrastive learning.

# 3.1 Premise-Oriented Data Augmentation

PODA initially creates analyses by forming thoughtpaths for both correct and incorrect options. Summarizing premises and identifying premises for each option are incorporated into CoT rationales, which are essential for generating new data. The core idea of it is to prompt a large language model through in-context learning to generate counterfactual data that can reverse the current answer to a new answer. A context can be divided into Premises (the known information from the text), which have specific relationships with the options. The relationships are categorized into three types: supported, contradicted and unrelated. Utilizing the premise as a foundational unit, we can construct counterfactual samples based on these relationships.

CoT Rationale Annotation As illustrated in Table 1, we design a structural CoT consisting of three steps: (1) Summarize Premises: Extract supporting statements from the context to serve as premises. (2) Analyze Options: Conduct a thorough evaluation of each option, clarifying the specific relationships between the options and premises, referred to as thought-path in our work. (3) Derive answer: Combine all thought-paths and determine the final answer. To guarantee a well-structured CoT, we utilize Few-shot-CoT for data collection.1

Premises Generation To prompt GPT-4 for new premises generation, we use a masked natural language inference (NLI) format to build the prompt. Let $P _ { a }$ represent the premises, associated with the question $Q$ and the current answer $a$ . We replace $P _ { a }$ with a mask token [blank], and then $P _ { a }$ serves as the output of the in-context exemplar to satisfy the question and answer. Given a new answer $a ^ { \prime }$ we want to flip to, we ask the model to complete [blank] with creative premises $P _ { a ^ { \prime } }$ that align with $a ^ { \prime }$ . This approach enables the model to generate counterfactual premises that are logically consistent with the new answer.

Context Generation To create new contexts, we preserve the origin premises $P _ { \neg a }$ that are irrelevant to the current answer $a$ , and introduce counterfactual premises $P _ { a ^ { \prime } }$ corresponding to the new answer $a ^ { \prime }$ . Let origin context $C$ serve as the output of the in-context exemplar, which is consistent with all origin premises $P = \{ P _ { a } , P _ { \neg a } \}$ . According to the reorganized premises $P ^ { \prime } = \{ P _ { a ^ { \prime } } , P _ { \neg a } \}$ , we ask the model to craft a creative context $C ^ { \prime }$ that develops the ideas and scenarios presented. This newly crafted context integrates the counterfactual premises while maintaining coherence and expanding upon the original narrative structure.

Correctness Verification Upon a combination of the initial three stages, we then implement correctness verification using Few-shot-CoT to filter out incorrect samples. The prompt and output format of this stage align with CoT Rationale Annotation. The potential mistakes of samples primarily stem from the following three aspects:

1. Some options are excessively absolute in their wording (e.g., using must or can’t), which conflicts with the nature of the question, making them unsuitable as correct answers.

2. Several options in the original samples are deliberately designed as incorrect choices that violate common sense. Thus, it is inappropriate to create new contexts based on these options.

3. Given the complexity of logical reasoning, it is challenging to ensure that the generated premises align perfectly with the expected answers for particularly difficult samples.

# 3.2 Thought-Path Contrastive Learning

Supervised fine-tuning (SFT) can notably improve the model’s performance. However, SFT only focuses on single instances, which results in its lack of the comparison between different samples. For logical MRC tasks, PODA annotates Chain-of-Thought (CoT) rationales, offering analyses for both correct and incorrect options, while also generating counterfactual samples. It can be observed that thought-paths exhibit similar and dissimilar reasoning processes associated with options in original and counterfactual samples.

In light of such motivation, we propose a thought-path contrastive learning approach. As depicted in the part (2) of Figure 2, the original/counterfactual sample has four thought-paths, with each corresponding to one option. Three thought-paths indicate that the corresponding options are incorrect, while one thought-path suggests it is correct. Therefore, for the original and counterfactual sample pair, the reasoning processes of thought-paths 2 and $2 ^ { \prime }$ (thought-paths 4 and $4 ^ { \prime }$ ) are analogous, whereas those of thought-paths 1 and $1 ^ { \prime }$ (thought-paths 3 and $3 ^ { \prime }$ ) are different. The goal of our method is to pull similar thought-paths closer while pushing different ones far apart. Simultaneously, we seek to enhance the model’s capabilities to precisely distinguish between pairs of thought-paths (e.g., similarity(thought-paths $\phantom { } 4 , 4 ^ { \prime } ) \dot { \gg }$ similarity(thought-paths $2 , 2 ^ { \prime } )$ ).

Inspired by recent advances in learning to preference optimization algorithms such as RLHF (Ouyang et al. 2022) and DPO (Rafailov et al. 2023), our objective is to present a simple approach for comparing the similarity of thoughtpath pairs. To achieve this objective, we employ the BradleyTerry preference model (Bradley and Terry 1952) to construct the loss function for the similarity comparison. Given the input pair $\pi _ { 0 } = ( x _ { 1 } , x _ { 2 } )$ , the Bradley-Terry model calculates the likelihood of similarity comparison over thoughtpath pairs, denoted as $( p _ { s } , p _ { s } ^ { \prime } ) > ( p _ { d } , p _ { d } ^ { \prime } ) | \pi _ { 0 }$ , where $\bar { ( p _ { s } , p _ { s } ^ { \prime } ) }$ and $( p _ { d } , p _ { d } ^ { \prime } )$ represent the similar and different thought-path pairs, respectively. In our work, we simply choose the reward function $\begin{array} { r } { r ^ { * } \stackrel { \mathbf { \theta } } { = } \frac { 1 } { \tau } \mathrm { s i m } ( \cdot ) } \end{array}$ to measure similarity using cosine distance, where $\tau$ is the temperature coefficient controlling the sharpness of the similarity distribution. Under the Bradley-Terry model, we can derive a streamlined probability measure for pairwise similarity comparison:

$$
\begin{array} { l } { { p ^ { * } ( ( p _ { s } , p _ { s } ^ { \prime } ) > ( p _ { d } , p _ { d } ^ { \prime } ) \mid \pi _ { 0 } ) } } \\ { { \displaystyle ~ = \sigma ( r ( p _ { s } , p _ { s } ^ { \prime } ) - r ( p _ { d } , p _ { d } ^ { \prime } ) ) } } \\ { { \displaystyle ~ = \frac { 1 } { 1 + \exp [ \frac { 1 } { \tau } \sin ( p _ { d } , p _ { d } ^ { \prime } ) - \frac { 1 } { \tau } \sin ( p _ { s } , p _ { s } ^ { \prime } ) ] } } } \end{array}
$$

where $\pi _ { 0 }$ is omitted as it doesn’t directly contribute to the calculation. Then we formulate the problem as binary classification using the negative log-likelihood loss:

$$
\begin{array} { r l } & { \mathcal { L } ( ( p _ { s } , p _ { s } ^ { \prime } ) , ( p _ { d } , p _ { d } ^ { \prime } ) , \pi _ { 0 } ) ) } \\ & { = - \mathbb { E } [ \log ( \sigma ( r ( p _ { s } , p _ { s } ^ { \prime } ) - r ( p _ { d } , p _ { d } ^ { \prime } ) ) ) ] } \end{array}
$$

The objective of this loss function is to decrease the distance between dissimilar pair $( p _ { d } , p _ { d } ^ { \prime } )$ while increasing the distance between similar pair $( p _ { s } , p _ { s } ^ { \prime } )$ . Additionally, the model learns to differentiate between pairs of thought-pairs based on preference optimization. Due to the presence of multiple groups of similar and dissimilar thought-path pairs for input $\pi _ { 0 }$ , we simply calculate the average loss as follows:

$$
\begin{array} { r l } {  { \mathcal { L } _ { \mathrm { T P C L } } ( ( p _ { s } , p _ { s } ^ { \prime } ) , ( p _ { d } , p _ { d } ^ { \prime } ) , \pi _ { 0 } ) ) = } } \\ & { - \mathbb { E } [ \frac { 1 } { N * M } \sum _ { i = 1 } ^ { N } \sum _ { j = 1 } ^ { M } \log ( \sigma ( r ( p _ { s j } , p _ { s j } ^ { \prime } ) } \\ & { - r ( p _ { d i } , p _ { d j } ^ { \prime } ) ) ) ] } \end{array}
$$

where $N$ and $M$ represent the number of similar and dissimilar thought-path pairs respectively. Both $N$ and $M$ are set to 2 in our work. We also add a cross-entropy loss consistent with SFT to ensure the model does not deviate from the data distribution. Given an input sequence $x$ , the average likelihood of generating the output sequence $y$ , consisting of $m$ tokens, is computed as follows:

$$
\mathcal { L } _ { \mathrm { S F T } } = \frac { 1 } { m } \sum _ { t = 1 } ^ { m } \log P ( y _ { t } \mid x , y _ { < t } )
$$

The overall training goal is the combination of TPCL loss and SFT loss:

$$
\begin{array} { r } { \mathcal { L } = \mathcal { L } _ { \mathrm { T P C L } } + \mathcal { L } _ { \mathrm { S F T } } } \end{array}
$$

Table 2: Experimental results (Accuracy $\%$ ) of our method compared with baseline models on ReClor and LogiQA 2.0 benchmarks. Segment-1: Discriminative language models; Segment-2: Instruction-tuned LLMs; Segment-3: API-based LLMs (3- shot-CoT); Segment-4: TPReasoner (our method). Test-E and Test-H denote Test-Easy and Test-Hard respectively. The best and second best results are marked in bold and underlined (comparisons do not include API-based LLMs).   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="4">ReClor</td><td colspan="2">LogiQA 2.0</td></tr><tr><td>Dev</td><td>Test</td><td>Test-E</td><td>Test-H</td><td>Dev</td><td>Test</td></tr><tr><td colspan="7">Discriminative Language Models</td></tr><tr><td>RoBERTa-Large (Liu et al. 2019)</td><td>62.60</td><td>55.60</td><td>75.50</td><td>40.00</td><td></td><td></td></tr><tr><td>DGAN (Huang et al. 2021)</td><td>65.80</td><td>58.30</td><td>75.91</td><td>44.46</td><td></td><td></td></tr><tr><td>LReasoner (Wang et al. 2022)</td><td>66.20</td><td>62.40</td><td>81.40</td><td>47.50</td><td></td><td></td></tr><tr><td>AMR-LDA (Bao et al. 2023)</td><td>65.26</td><td>56.86</td><td>77.34</td><td>40.77</td><td></td><td></td></tr><tr><td>FocalReasoner (Ouyang, Zhang,and Zhao 2024)</td><td>66.80</td><td>58.90</td><td>77.10</td><td>44.60</td><td></td><td></td></tr><tr><td colspan="7">Instruction-tunedLLMs</td></tr><tr><td>LLaMA2-7B-logicot (Liu et al. 2023b)</td><td>49.20</td><td>50.50</td><td>59.06</td><td>43.75</td><td>45.06</td><td>43.19</td></tr><tr><td>LLaMA3-8B-logicot</td><td>61.50</td><td>62.65</td><td>71.25</td><td>55.89</td><td>54.11</td><td>54.07</td></tr><tr><td>Mistral-7B-logicot</td><td>63.00</td><td>61.90</td><td>70.45</td><td>55.18</td><td>55.83</td><td>54.25</td></tr><tr><td colspan="7">API-based LLMs (3-shot-CoT)</td></tr><tr><td>GPT-3.5 (gpt-3.5-turbo-0613)</td><td>56.00</td><td>58.20</td><td>61.82</td><td>55.36</td><td>55.07</td><td>51.15</td></tr><tr><td>GPT-4 (gpt-40)</td><td>87.20</td><td>89.30</td><td>90.45</td><td>88.39</td><td>76.32</td><td>74.81</td></tr><tr><td colspan="7">TPReasoner</td></tr><tr><td>LLaMA2-7B</td><td>58.00</td><td>58.63</td><td>66.14</td><td>52.74</td><td>49.71</td><td>49.75</td></tr><tr><td>LLaMA3-8B</td><td>67.60</td><td>70.97</td><td>77.27</td><td>66.01</td><td>60.29</td><td>58.78</td></tr><tr><td>Mistral-7B</td><td>69.73</td><td>71.17</td><td>78.79</td><td>65.18</td><td>61.22</td><td>60.28</td></tr></table></body></html>

# 4 Experiment

# 4.1 Datasets

ReClor (Yu et al. 2020) comprises 6,138 questionanswering samples collected from standardized exams including GMAT and LSAT, which are split into train / dev / test sets with $4 , 6 3 8 \mathrm { ~ / ~ } 5 0 0 \mathrm { ~ / ~ } 1 , 0 0 0$ samples respectively. To evaluate the difficulty of the questions, the test set is further divided into Test-E and Test-H. The instances on Test-E are easy and biased that can be solved without knowing contexts and questions. The other harder and unbiased ones are taken as the Test-H set.

LogiQA 2.0 (Liu et al. 2023a) is an updated and reannotated version of LogiQA (Liu et al. 2020). There are 15,708 instances derived from the Chinese Civil Service Examination, meticulously translated into English by experts. The dataset is randomly split into train / dev / test sets with $1 2 , 5 6 7 / 1 , 5 6 9 / 1 , 5 7 2$ samples respectively.

Synthetic Data is generated by our PODA framework. We construct 5,075 and 13,477 counterfactual samples based on the train sets of ReClor and LogiQA 2.0, respectively. To perform thought-path contrastive learning, each counterfactual sample is paired with its corresponding origin one.

# 4.2 Implementation Settings

In PODA framework, gpt-3.5-turbo-0613 and gpt-4-0613 are utilized for CoT Rationale Annotation. Subsequently, gpt-4-0125-preview is employed for Premises and Context Generation. In the end, gpt-4-0613 is used for Correctness

Verification.2 We set the sampling temperature of 0.75 and the top probability of 0.9, ensuring the generated text maintains both diversity and high quality. The detailed prompts are provided in Appendix C.2.

During the training process, we adopt LLaMA2-7B, Mistral-7B and LLaMA3-8B as baselines. In order to accelerate training, we employ LoRA (Hu et al. 2021) to fine-tune the model. The AdamW optimizer (Loshchilov and Hutter 2017) is used with a learning rate warmup of 0.03. Due to the absence of corresponding counterfactual samples for some original samples as illustrated in Correctness Verification, we implement a two-stage training strategy. The implementation of our code refers to Llamafactory (Zheng et al. 2024). All hyper-parameters of training are listed in Appendix A.

# 4.3 Baselines

In this paper, we compare our method with two types of baselines: discriminative language models and large language models (LLMs). The discriminative language model baselines include RoBERTa-Large (Liu et al. 2019), DGAN (Huang et al. 2021), LReasoner (Wang et al. 2022), AMRLDA (Bao et al. 2023) and FocalReasoner (Ouyang, Zhang, and Zhao 2024). The LLM baselines encompass instructiontuned method such as LogiCoT (Liu et al. 2023b), as well as API-based LLMs like GPT-3.5 (gpt-3.5-turbo-0613) and GPT-4 (gpt-4o) by utilizing 3-shot-CoT. More details can be found in Appendix B.

<html><body><table><tr><td rowspan="2">Model</td><td colspan="4">ReClor</td><td colspan="2">LogiQA 2.0</td></tr><tr><td>Dev</td><td>Test</td><td>Test-E</td><td>Test-H</td><td>Dev</td><td>Test</td></tr><tr><td>LLaMA2-7B</td><td>58.00</td><td>58.63</td><td>66.14</td><td>52.74</td><td>49.71</td><td>49.75</td></tr><tr><td> - w/o TPCL</td><td>55.27</td><td>56.73</td><td>63.48</td><td>51.43</td><td>48.25</td><td>48.60</td></tr><tr><td>- W/o TPCL + CD</td><td>53.20</td><td>52.17</td><td>59.16</td><td>46.67</td><td>46.15</td><td>45.48</td></tr><tr><td>- W/o TPCL + CD +WOA</td><td>51.40</td><td>50.27</td><td>56.21</td><td>45.58</td><td>44.89</td><td>44.47</td></tr><tr><td>LLaMA3-8B</td><td>67.60</td><td>70.97</td><td>77.27</td><td>66.01</td><td>60.29</td><td>58.78</td></tr><tr><td> - w/o TPCL</td><td>66.00</td><td>69.30</td><td>74.09</td><td>65.54</td><td>57.11</td><td>57.06</td></tr><tr><td>- w/o TPCL + CD</td><td>63.07</td><td>66.10</td><td>73.11</td><td>60.59</td><td>55.19</td><td>54.83</td></tr><tr><td>- W/o TPCL + CD + WOA</td><td>61.17</td><td>64.03</td><td>71.53</td><td>58.11</td><td>53.51</td><td>53.07</td></tr><tr><td>Mistral-7B</td><td>69.73</td><td>71.17</td><td>78.79</td><td>65.18</td><td>61.22</td><td>60.28</td></tr><tr><td> - w/o TPCL</td><td>67.07</td><td>69.57</td><td>78.03</td><td>64.40</td><td>59.91</td><td>58.63</td></tr><tr><td>- W/o TPCL + CD</td><td>65.20</td><td>67.37</td><td>75.68</td><td>62.92</td><td>58.24</td><td>56.97</td></tr><tr><td>- W/o TPCL + CD + WOA</td><td>63.10</td><td>64.83</td><td>73.02</td><td>58.37</td><td>56.04</td><td>55.19</td></tr></table></body></html>

Table 3: Ablation study of our method. TPCL stands for thought-path contrastive learning approach. CD refers to the utilizatio of counterfactual data. WOA signifies that CoT rationales involve the analyses for wrong options.

# 5 Result and Analysis

# 5.1 Overall Results

Table 2 presents the primary experimental results of our method and other baselines on ReClor and LogiQA 2.0 benchmarks, in terms of accuracy. Our method employs the CoT rationales, which is appropriate for generative LLMs. Hence, we did not conduct experiments on discriminative language models. Compared to these baselines, TPReasoner exhibits superior performance except for GPT-4.

On ReClor dataset, LLaMA3-8B and Mistral-7B, based on our approach, significantly outperform all discriminative model methods. Compared with LReasoner, PODA-TPCL achieves improvements of $1 . 4 \AA { - } 3 . 5 \%$ and $8 \%$ on the dev and test sets, respectively. Although LLaMA2-7B, when using our method, does not surpass all discriminative model baselines, the results on Test-H demonstrate that it exhibits stronger robustness and generalization for data distribution. There is a substantial disparity (exceeding $3 0 \%$ ) between the performances on Test-E and Test-H for discriminative models, indicating that these models tend to take shortcuts for simpler and biased samples rather than genuinely comprehending them. In contrast, our method achieves a gap of less than $1 5 \%$ between Test-E and Test-H, with the performance on Test-H clearly surpassing that of the discriminative models. This demonstrates the great potential of leveraging CoT rationales to solve complex logical reasoning tasks.

Compared to the instruction-tuned LLMs based on LogiCoT, our models achieve superior performance on ReClor and LogiQA 2.0 datasets. This improvement is attributed to the PODA framework, which offers additional analyses of incorrect options and constructs high-quality, diverse counterfactual instances. Additionally, TPCL boosts the model’s reasoning capabilities by facilitating the learning of similar and distinct thought-paths between different samples. Furthermore, our model demonstrates competitive performance comparable to GPT-3.5, trailing only behind GPT-4.

# 5.2 Ablation Study

An ablation study is conducted to investigate the efficacy of three key components, thought-path contrastive learning (TPCL), counterfactual data (CD) and wrong options analyses (WOA), as presented in Table 3. For w/o TPCL, we eliminate TPCL and only employ SFT to train the model. There is a noticeable decline in performance, with a drop of $1 - 3 \%$ across the two datasets. These results convincingly demonstrate that TPCL significantly boosts the model’s reasoning capabilities by comparing reasoning paths between original and counterfactual samples. For w/o TPCL $+ \mathrm { C D }$ , we additionally exclude the counterfactual samples generated by PODA and solely utilize the original data. It can be observed that the models without CD have severe performance degradation. This suggests that the counterfactual samples are beneficial for LLMs to conduct logical reasoning. Furthermore, it demonstrates that the data synthesized by our framework is of high-quality, automatically generated without requiring human interventions. For w/o TPCL $+ \mathbf { C D } + \mathbf { W O A }$ , we further omit the analyses of wrong options in CoT rationales. As a result, the models’ performance decreases by approximately $2 \%$ . It indicates that incorporating reasoning processes for incorrect options enables the model to analyze problems more thoroughly, thereby improving its reasoning abilities. Overall, PODA-TPCL achieves a performance improvement of $5 - 7 \%$ across three models on ReClor and LogiQA 2.0 datasets, underscoring its exceptional robustness and generalization capabilities.

# 5.3 Evaluation of Data Quality

Accuracy Evaluation of Counterfactual Data A primary concern is whether the synthetic data accurately matches the correctly labeled option. In order to evaluate this, we choose five outstanding LLMs, including Mixtral- $\mathbf { \delta } \cdot 8 \times 7 \mathbf { B }$ - Instruct, GPT-3.5 (gpt-3.5-turo-0613), LLaMA2-70B-chat, LLaMA3-70B-Instruct and GPT-4 (gpt-4o-2024-05-13). Non-GPT series models evaluate all counterfactual data. Due to budget constraints, a random subset of 200 samples from the generated dataset are evaluated by GPT-3.5 and

<html><body><table><tr><td>Model</td><td>Accuracy</td></tr><tr><td>Mixtral-8×7B-Instruct</td><td>74.05</td></tr><tr><td>GPT-3.5 (gpt-3.5-turo-0613) LLaMA2-70B-chat</td><td>75.50</td></tr><tr><td>LLaMA3-70B-Instruct</td><td>79.39</td></tr><tr><td>GPT-4 (gpt-4o-2024-05-13)</td><td>82.19 93.00</td></tr><tr><td></td><td></td></tr><tr><td>Human Performance</td><td>90.00</td></tr></table></body></html>

Table 4: Evaluation of accuracy $( \% )$ for counterfactual data.

GPT-4. We utilize 3-shot CoTs to evaluate the accuracy. As illustrated in Table 4, the accuracy assessed by LLaMA3- 70B-Instruct and GPT-4 can reach $8 2 . 1 9 \%$ and $93 \%$ respectively, indicating PODA can generate high-quality counterfactual data. Moreover, accuracies for other models vary between approximately $7 5 \%$ and $7 9 \%$ , reflecting the significant challenges and complexities presented by these data. Overall, the generated data can have applicability in both training and evaluation domains.

Comparison for Counterfactual Data We compare PODA with a rule-based method, LReasoner (Wang et al. 2022), which constructs counterfactual contexts by modifying logical expressions according to logical laws. Two random subsets of 200 counterfactual samples were selected respectively. We ensure that the two subsets are derived from the same set of original samples for a fair comparison. These instances are evaluated by GPT-4 (gpt-4o-2024-05-13) using four key metrics: Coherence (Is the context well-connected and logically consistent?), Clarity (Is the context clear and easy to understand?), Relevance (Does the context relate to the question and options?), and Diversity (How does the counterfactual context differ from the original one?). Each context was rated on a scale from 1 (poor) to 5 (excellent) for each metric. Our method attains average scores of 4.61 for Coherence, 4.36 for Clarity, 4.71 for Relevance, 3.18 for Diversity, and 4.22 Overall. In contrast, LReasoner achieves average scores of 2.98 for Coherence, 2.96 for Clarity, 4.63 for Relevance, 1.08 for Diversity, and 2.91 Overall. This comparison clearly demonstrates that the contexts generated by our method significantly outperform those produced by the rule-based method in both quality and diversity.

Comparison for CoT Rationales We compare PODA with LogiCoT, which also utilizes GPT-4 for CoT rationales but focuses solely on analyzing the correct option. Similarly, two random subsets of CoT rationales were selected from the same contexts. These rationales are assessed by GPT-4 (gpt-4o-2024-05-13) using four key metrics: Coherence (Is the CoT rationale logically consistent?), Completeness (Does it offer a thorough explanation for the reasoning?), Relevance (Does it directly and effectively addresses the context, question and options?), and Faithfulness (Is it factually correct and free from fabricated details?). Each rationale was rated on a scale from 1 (poor) to 5 (excellent) for each metric. PODA surpasses LogiCoT across four metrics, especially in Completeness. This suggests that incorporating the analysis of incorrect options into the rationales can

Table 5: Evaluating counterfactual data on Coherence (Coh), Clarity (Clar), Relevance (Rel), and Diversity (Div).   

<html><body><table><tr><td>Method</td><td>Coh</td><td>Clar</td><td>Rel</td><td>Div</td><td>Overall</td></tr><tr><td>LReasoner</td><td>2.98</td><td>2.96</td><td>4.63</td><td>1.08</td><td>2.91</td></tr><tr><td>PODA</td><td>4.61</td><td>4.36</td><td>4.71</td><td>3.18</td><td>4.22</td></tr></table></body></html>

Table 6: Evaluating rationales on Coherence (Coh), Completeness (Comp), Relevance (Rel), and Faithfulness (Faith).   

<html><body><table><tr><td>Method</td><td>Coh</td><td>Comp</td><td>Rel</td><td>Faith</td><td>Overall</td></tr><tr><td>LogiCoT</td><td>4.82</td><td>3.38</td><td>4.91</td><td>4.80</td><td>4.48</td></tr><tr><td>PODA</td><td>4.86</td><td>4.46</td><td>4.99</td><td>4.86</td><td>4.79</td></tr></table></body></html>

enhance the quality of CoTs.

# 5.4 What Does TPCL Update Do?

We analyze the gradient of the loss function ${ \mathcal { L } } _ { \mathrm { T P C L } }$ (considering a group of similar and dissimilar thought-path pairs), whose gradient can be written as:

$$
\begin{array} { r l } {  { \nabla \mathcal { L } _ { \mathrm { T P C L } } ( ( p _ { s } , p _ { s } ^ { \prime } ) , ( p _ { d } , p _ { d } ^ { \prime } ) , \pi _ { 0 } ) ) = } } \\ & { - \frac { 1 } { \tau } \mathbb { E } [ \sigma ( r ( p _ { d } , p _ { d } ^ { \prime } ) - r ( p _ { s } , p _ { s } ^ { \prime } ) ) } \\ & { * [ \nabla \mathrm { s i m } ( p _ { s } , p _ { s } ^ { \prime } ) - \nabla \mathrm { s i m } ( p _ { d } , p _ { d } ^ { \prime } ) ] ] } \end{array}
$$

Intuitively, the gradient of ${ \mathcal { L } } _ { \mathrm { T P C L } }$ increases the similarity of the similar thought-paths $( ( p _ { s } , p _ { s } ^ { \prime } )$ and decreases the similarity of the dissimilar thought-paths $( ( p _ { d } , p _ { d } ^ { \prime } )$ . Meanwhile, $\sigma ( r ( \bar { p } _ { d } , p _ { d } ^ { \prime } ) \ : - \ : r ( p _ { s } , p _ { s } ^ { \prime } ) )$ serves as an adjustable weight for the similarity reward estimate, assigning greater weight when $r ( p _ { d } , p _ { d } ^ { \prime } )$ is approximately equal to or greater than $r ( p _ { s } , p _ { s } ^ { \prime } )$ . This mechanism accelerates the convergence of the loss function. Overall, the gradient update aligns with our objective to pull similar thought-paths closer while pushing dissimilar ones further apart, which enhances the model’s reasoning capabilities by comparing thought-paths between the original and counterfactual samples.

# 6 Conclusion

In this paper, we propose a premise-oriented data augmentation framework that generates CoT rationales, providing analyses for both correct and incorrect options. Additionally, the framework automatically constructs diverse and high-quality counterfactual data from incorrect candidate options. We also introduce a thought-path contrastive learning method to effectively leverage pairs of original and counterfactual samples, enabling models to distinguish between different reasoning paths. Extensive experiments conducted on three open LLMs demonstrate that our approach achieves competitive performance on two logical reasoning benchmarks.