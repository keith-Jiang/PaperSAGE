# A Video-grounded Dialogue Dataset and Metric for Event-driven Activities

Wiradee Imrattanatrai1\*, Masaki Asada1\*, Kimihiro Hasegawa2, Zhi-Qi Cheng2, Ken Fukuda1, Teruko Mitamura2

1National Institute of Advanced Industrial Science and Technology (AIST) 2Language Technologies Institute, Carnegie Mellon University wiradee.imrattanatrai $@$ aist.go.jp, masaki.asada $@$ aist.go.jp

# Abstract

This paper presents VDAct, a dataset for a Video-grounded Dialogue on Event-driven Activities, alongside VDEval, a session-based context evaluation metric specially designed for the task. Unlike existing datasets, VDAct includes longer and more complex video sequences that depict a variety of event-driven activities that require advanced contextual understanding for accurate response generation. The dataset comprises 3,000 dialogues with over 30,000 question-andanswer pairs, derived from 1,000 videos with diverse activity scenarios. VDAct displays a notably challenging characteristic due to its broad spectrum of activity scenarios and wide range of question types. Empirical studies on state-of-the-art vision foundation models highlight their limitations in addressing certain question types on our dataset. Furthermore, VDEval, which integrates dialogue session history and video content summaries extracted from our supplementary Knowledge Graphs to evaluate individual responses, demonstrates a significantly higher correlation with human assessments on the VDAct dataset than existing evaluation metrics that rely solely on the context of single dialogue turns.

Resources — https://github.com/aistairc/VDAct

# Introduction

The video-grounded dialogue task involves generating responses to user utterances based on the video content. This task poses significant challenges particularly when dealing with videos presenting compound activities where multiple related events occur in sequence. With these event-driven activities, advanced system capabilities in multimodal understanding, temporal reasoning, and contextual interpretation are required to align visual cues with conversational context and handle dynamic changes in the video. Although several datasets exist for video-based reasoning through question answering, only a few benchmark datasets are available for the video-grounded dialogue task (Alamri et al. 2019; Pasunuru and Bansal 2018). These datasets mostly feature short videos depicting simple activities and a limited range of question types, while in real-world scenarios, dialogue discussions often center around multifaceted activities about how various events with different associated actions are temporally and contextually related. Thus, exposure to dialogues on event-driven activities would expand the system’s ability to handle complex interactions and improve its capacity to generate accurate and relevant responses.

Thus, to advance research and development of videogrounded dialogue systems on event-driven activities, we introduce a new dataset named “Video-grounded Dialogue on Event-driven Activities” (VDAct). This dataset includes dialogues based on daily scenarios where each involves multiple activities with long sequences of events. We opted to utilize virtual simulation videos that allow a variety of activity combinations. Unlike existing datasets which primarily focus on descriptive questions, VDAct includes several other categories to capture a broader range of interactions as shown in Table 1. In addition to the descriptive questions which aim to obtain factual information about the activities, we incorporate three other main categories including temporal questions, which focus on temporal aspects, such as timing, duration, and sequence of events or activities; explanatory questions which explore the reasons or causes behind events or activities; and quantitative questions, which seeks numerical or quantitative data. Additionally, VDAct incorporates questions related to the video and dialogue attributes, as well as open-ended and subjective questions.

In addition to videos and dialogues, we enrich our dataset with Knowledge Graphs (KGs) as supplementary information. Given that our target videos represent compound activities involving multiple events, KGs could be useful for both system development and evaluation as they offer detailed information that links visual cues to the structured information. This information includes event sequences, transitions between events, action-object interactions, and changes in agent and object states, as illustrated in Figure 1.

Furthermore, existing evaluation metrics, such as those designed for text generation (Papineni et al. 2002; Banerjee and Lavie 2005; Lin 2004), and QA tasks (Man˜as, Krojer, and Agrawal 2024; Chan et al. 2023; Wada et al. 2024), are insufficient for assessing the quality of generated responses in the context of dialogues. These metrics usually assess system-generated text by comparing it to a reference text, but they often overlook cases where multiple responses can be equally valid. For example, in response to Q7 in Figure 1, both “He grabbed the plates from the kitchen ta

[QNT]   
Q1: How many tasks does the man do in the kitchen?   
A1: The man has three tasks in the kitchen.

# [QNT]

Q2: How many cleaning chores does he do in the kitchen? A2: Two cleaning chores, one tidying one.

# [D-OBJ] [T-SEQ]

Q3: What is the first thing he cleans? A3: He cleans the television set.

# [D-INT]

Q4: Does he clean under the TV? A4: No he seems pretty focused on the screen part.

# [D-INT] [T-SEQ]

Q5: Does he clean another thing after the TV or tidy first? A5: He tidies first, then does more cleaning.

![](images/d58b5bba7c5ff067e4a19e3e6cf2a559138bed098221f6a6e6ce85d5d84eb0c1.jpg)  
Figure 1: VDAct with example dialogue (turns 1-7) for an activity scenario video with corresponding KG elements for the events. Each input utterance (i.e., question) is labeled with the relevant question types. Note that the object IDs are omitted from the KG illustration.

# [D-OBJ]

Q6: What does he tidy up? A6: He clears two dirty dishes.

# [D-LOC] [REF]

Q7: Where did he get them from? A7: From the kitchen table.

ble.” and “They were on the rug, which was on top of the kitchen table.” are valid when considering additional context from dialogue history and KGs. Although neither Q7 nor A7 mentions what objects they are referring to, the first response is deemed correct because it aligns with prior dialogue turns. Additionally, the KG information for the related event (i.e., Event3 of Activity2) shows that the plate was on the rug, making the second response also accurate. However, because the existing metrics only compare responses to the reference and do not consider this additional context, they cannot fully capture the correctness of both responses and as a result, evaluate them as partially or completely inaccurate. Thus, we propose a new LLM-based evaluation metric that integrates session-based context including a summary of KG information and dialogue history, rather than relying on turn-based context. This inclusion allows the evaluation model to verify the responses more comprehensively.

To this end, our main contributions are three-fold. First, we introduce a new video-grounded dialogue dataset focused on event-driven activities and provide accompanying KGs for the activity scenarios. Second, we propose a new evaluation metric, specifically for the video-grounded dialogue task, to address the unique aspects of dialogues where multiple responses are possible and inference from summary and dialogue history are required. Finally, we selected Vision LMs (VLMs) to evaluate their performance on our newly constructed dataset and evaluate the correlation between the results and our new proposed metric.

# Related Works

This section reviews the existing datasets and the existing evaluation metrics for the video-grounded dialogue task.

Video-grounded Dialogue Datasets In terms of video understanding, several video-grounded QA datasets have been introduced. Among them, a few provide KGs such as STAR (Wu et al. 2021) which presents multi-choice QA samples along with situation hypergraphs based on realworld videos, and EgoTaskQA (Jia et al. 2022) which introduced a QA dataset with annotations of object status, human-object and multi-agent relationships, and causal dependency structures between actions, all derived from egocentric videos. However, while QA tasks treat each question independently, dialogue tasks build on prior interactions of questions and answers. This offers a key advantage for system development as the sequential nature of dialogue enables the system to maintain continuity and deliver context-aware responses by referencing previous turns. For the video-grounded dialogue datasets, VisDial (Das et al. 2017) treated the problem of visual dialogue as a multiturn QA where the system is expected to answer questions given dialogue history and corresponding images. Audio Visual Scene-aware Dialog (AVSD) (Alamri et al. 2019) extended the work from VisDial to include additional modalities including videos with audio signals. Twitch-FIFA (Pasunuru and Bansal 2018) introduced a video-context dialogue dataset based on live-broadcast soccer games and chat from Twitch.tv. Video-grounded Scene and Topic AwaRe dialogue (VSTAR) (Wang et al. 2023) introduced a large-scale benchmark dataset for understanding the dialogue between characters in a TV series. These datasets usually contain short videos with simple activities and focus mainly on descriptive questions. This constraint limits the system’s ability to learn from more complex scenarios that involve multiple related events occurring in sequence. Thus, we propose a new dataset with longer videos depicting complex activities, and dialogues presenting a variety of question types.

Table 1: Categories and types of questions involve in VDAct.   

<html><body><table><tr><td>Category</td><td></td><td>Description</td></tr><tr><td colspan="3">(1) Descriptive</td></tr><tr><td>- Agents</td><td>D-AGT</td><td>Questions about characteristics or states of the agents</td></tr><tr><td>- Actions</td><td>D-ACT</td><td>General questions related to actions without specifying objects</td></tr><tr><td>- Objects</td><td>D-OBJ</td><td>Questions about objects involved in actions,those upon which actions are performed,orthe states of objects</td></tr><tr><td>- Interactions</td><td>D-INT</td><td>Questions about whether actions were performed on specific objects</td></tr><tr><td>-Locations</td><td>D-LOC</td><td>Questions concerning the whereabouts of agents,objects,or where the actions were performed</td></tr><tr><td colspan="3">(2) Temporal</td></tr><tr><td>- Sequence</td><td>T-SEQ</td><td>Questions related to the temporal sequence of actions w/wo objects or questions related to specific points in time</td></tr><tr><td>- Frequency</td><td>T-FRQ</td><td>Questions about the frequency or duration of actions performed w/wo objects</td></tr><tr><td>(3) Explanatory</td><td>EXP</td><td>Questions seeking explanations for how and why actions were carried out</td></tr><tr><td>(4) Quantitative</td><td>QNT</td><td>Questions related to quantity or number of agents or objects</td></tr><tr><td colspan="3">(5) Other</td></tr><tr><td>- Reference</td><td>REF</td><td>Questions that reference previous dialogue turns or require answers based on the dialogue history</td></tr><tr><td>- Supplementary</td><td>SUP</td><td>Questions seeking further details or information about the activities</td></tr><tr><td>- Video Attributes</td><td>VID</td><td>Questions about attributes such as audio,quality,length of the video,orthe language spoken within the video</td></tr><tr><td>- Opinions</td><td>OPI</td><td>Questions asking opinions or facts about theagents,actions,and objects thatare subjective or cannot be verified</td></tr></table></body></html>

Evaluation Metrics for Video-grounded Dialgoue Developing an effective evaluation metric for the videogrounded dialogue task presents a significant challenge. Previous studies reported that classic evaluation metrics such as BLEU, METEOR, and ROUGE showed low correlation with human evaluations for the video-grounded dialogue task (Liu et al. 2016; Alamri et al. 2019). Considering this point, AVSD employs ranking-based evaluation metrics and a discriminative ranking task setting where the model prediction is selected from candidate answers.

Learning-based metrics such as PAC-S (Sarto et al. 2023) and Polos (Wada et al. 2024) have been proposed and showed a high correlation with human evaluation for the image captioning task. However, these metrics require vast amounts of human scores to train the metrics. This makes it difficult to apply the metrics on the video modality. LLM-based evaluation metrics, such as CLAIR (Chan et al. 2023) for image captioning, and LAVE (Man˜as, Krojer, and Agrawal 2024) and LLM-Acc/Rel (Maaz et al. 2024) for VQA have been gaining attention in recent years. However, these metrics only compare the generated text with the reference of a single QA turn without considering additional contextual information that is beneficial for the evaluation.

In this study, we exploit the advantage of the video context being directly linked to event-centric structured KGs and dialogue history to propose a new LLM-based evaluation metric for the video-grounded dialogue task.

# VDAct Dataset

# Data Collection

We prepare target scenario videos for dialogue generation, followed by employing crowdsourced workers to create dialogues discussing the event-driven activities depicted in the videos. To support the video-grounded dialogue task, we additionally include scenario KGs and their summaries. The following subsections detail the collection process for each data component.

Preparation of Scenario Videos To gather video data representing daily living activities, we leverage the VirtualHome2KG dataset (Egami et al. 2023)1, which integrates KGs with video data for tasks such as activity recognition. VirtualHome2KG contains simulation videos depicting various daily activities performed by a single agent in a 3D virtual space using the VirtualHome platform (Puig et al. 2018). Each video captures an activity in a unique home environment, varying in room layouts and camera angles. VirtualHome2KG relies on activities of 11 classes defined by HomeOntology (Vassiliades et al. 2020) such as BedTimeSleep, EatingDrinking, FoodPreparation, HouseArrangement, and others, as well as one additional class, Abnormal. Each activity is associated with a program representing a sequence of events involving actions and objects. For example, an event of “[WALK] television (297)” where the number denotes the object ID. The dataset consists of 3,530 videos covering 706 activities across 12 categories, 7 environment setups, and 6 viewpoints, including an indoor camera switching view, character rear views, and fixed camera angles.

As the target for our dialogue is to analyze the daily scenario consisting of multiple activities, we combined 2- 5 available activities within VirtualHome2KG as an activity scenario. Following are the constraints we imposed to obtain high-quality videos. First, we need to ensure the seamless connectivity of the activity videos as scenario videos. Thus, we chose to combine the activities where each occurs in the same environment setup and that the agent starts and ends the activity in the same room as its previous and succeeding activity, respectively. As there can be too many possible activity combinations, we do not allow the same activity to be in the same scenario, as well as limit the scenario video to 1 to 5 minutes of the combined activity videos. Out of six viewpoints, we selected activity videos from either an indoor camera switching view or a fixed view at a room corner as they show the clearest depiction of activities for a scenario.

With a set of candidate scenarios, we further filtered out scenarios with some criteria to diversify the activity combination. Firstly, we set the limit for the number of individual activities appearing as the first activity as 8. Secondly, we excluded scenarios that have more than half of their activities duplicated across other scenarios. Lastly, we limit each adjacent activity pair to appear in no more than two scenarios. Thus, we obtained 3,021 unique activity combinations as scenarios. From this list, we randomly sample 1,000 scenarios for preparing the dialogues.

Dialogue Data Creation For the creation of dialogue data, we hired six crowdsourced workers through a reputable third-party company specialized in creating and collecting language data for NLP research and development. The goal of this data collection step is to obtain dialogue sessions demonstrating the information exchange of the person’s activity scenario between a pair of annotators. For each given scenario, the two annotators were assigned different roles to have a formal discussion about the scenario. One annotator was assigned to act as the investigator with a responsibility to investigate the person’s behavior and figure out how the person performs the activities by asking questions to the corresponding annotator. The investigator was not allowed to watch the videos but was given an unordered list of activities to provide some ideas about the scenario. The other annotator, acting as the correspondent, was given a list of activities, and was assigned to watch the videos to provide accurate answers to the investigator.

To cover multiple types of questions, we chose to provide examples from four main categories (i.e., descriptive, temporal, explanatory, and quantitative) that can be incorporated into the dialogues. Moreover, we also provided a few example dialogues to the annotators to have a clear picture of the task. We do not set strict minimum or maximum limits on the number of each question type per dialogue, as such a limitation could negatively impact the natural flow of the conversation. Instead, we instructed annotators to include as many question types within each dialogue to ensure type coverage.

For 1,000 scenarios from the previous data collection step, we formed three different annotator pairs. In half of the scenarios, one annotator was the investigator, while the other was the correspondent. The role of the pair switches for the latter half of the scenarios. This approach ensured that each annotator had the opportunity to play both roles, potentially leading to more diverse and comprehensive dialogues. Moreover, to ensure that the annotators fully understand the task instruction, we manually reviewed the initial dialogues created by each annotator pair before allowing them to proceed with the rest of the process. In total, we obtained 3,000 distinct dialogues with over 30,000 turns (i.e., question-answer pairs) through this process.

Table 2: Example triplets and the corresponding sentences for template-based video summaries from KGs.   

<html><body><table><tr><td>Triplet</td><td>Sentence</td></tr><tr><td>(evento,from,bedroom75) (eventO,action,walk) (eventO,mainObject,bathroom11)</td><td>The person is in the bedroom. He walks to the bathroom.</td></tr><tr><td>(door53,inside,bathroom) (door53,state,OPEN) (eventl,action,walk) (eventl,mainObject,door53)</td><td>The door is inside the bathroom. The door is OPEN. He walks to the door.</td></tr><tr><td>(event2,action,close) (event2,mainObject,door53)</td><td>He closes the door.</td></tr><tr><td>(door53, state,CLOSED) (toilet,inside,bathroom11) (event3,action,walk) (event3,mainObject,toilet46)</td><td>The door is CLOSED. The toilet is inside the bathroom. He walks to the toilet.</td></tr><tr><td>(toilet46,close,character1) (characterl,inside,bathroom11) (event4,action,sit) (event4,mainObject,toilet46)</td><td>The toilet is next to the person. The person is inside the bathroom He sits on the toilet.</td></tr></table></body></html>

Knowledge Graph Collection for Scenarios As supplementary information for the dataset, we collected KGs for scenarios by relying on KGs for activities provided by VirtualHome2KG. Each activity KG consists of 9 main node types including Activity, Event, Action, Situation, Object, State, Attribute, StateVal, and Shape. The Activity such as “Drink wine while watching television3” links to Event nodes such as “event0” and “event1” which indicate events happening in sequence. Each Event node connects to the main Object node (e.g., wine465) and the Action node (e.g., grab) associated with the event. Additionally, Event nodes are connected to Situation nodes that describe the state of each object (i.e., a State node) in the environment before and after the event using the StateVal nodes (e.g., ON, OFF, CLEAN and DIRTY). Meanwhile, Shape nodes represent the 3D coordinates of agents and object states and are linked from State nodes by bbox relations and to each other through spatial relations (e.g., inside, on, and close).

Since the average number of triplets for a single activity in VirtualHome2KG is over 29,237, we selectively curated and combined only the most relevant triplets within activity KGs as a scenario KG to suit the purpose of our dataset for the task. Instead of including all objects present in the environment, we focused on the main and target objects directly involved in the events of each activity. This approach aligns with our goal of constructing dialogues centered around activities, as most dialogue turns typically pertain to the main and target objects on which actions are performed. Additionally, to enhance the temporal coherence of the scenario, we introduced additional triplets with the nextActivity relationship to link adjacent activities in chronological order.

Table 3: Comparison of our dataset with the existing datasets.   

<html><body><table><tr><td>Dataset</td><td>#Videos</td><td>#Dialogues</td><td>#QA Pairs</td><td>Video Source</td><td>Avg. Video Length</td><td>Avg. Question Length</td><td>Avg. Answer Length</td><td>KG</td></tr><tr><td>VisDial</td><td>120k (images)</td><td>120k</td><td>1.2M</td><td>-</td><td></td><td>5.1</td><td>8.2</td><td>X</td></tr><tr><td>Twitch-FIFA</td><td>49</td><td>15.083</td><td>15.083</td><td>Soccer match</td><td>30 secs</td><td>68</td><td>6.3</td><td>×</td></tr><tr><td>AVSD</td><td>11,816</td><td>11,816</td><td>118,160</td><td>Crowdsourced</td><td>30 secs</td><td>7.9</td><td>9.4</td><td>×</td></tr><tr><td>VDAct</td><td>1,000</td><td>3,000</td><td>30.095</td><td>VirtualHome</td><td>248 secs</td><td>7.8</td><td>10.2</td><td>√</td></tr></table></body></html>

Preparation of Scenario Video Summaries We generated video summaries for each scenario by translating triplets from the scenario KGs using a template-based approach. The examples are shown in Table 2. After generating the linearized KG-to-text summaries, we further refined the text using a commercial LLM, specifically GPT-4o-mini, with the prompt: “Please summarize the following text without adding any extra information: $\{ \mathrm { t e x t } \}$ ,” where $\{ \mathrm { t e x t } \}$ is the placeholder for the linearized summary. This additional step is necessary to remove redundant sentences that might arise from triplets where object states remain unchanged throughout the event sequence. For example, the sentence “the stove is inside the kitchen.” might appear repeatedly for multiple events, so this step helps eliminate such redundancies. After the refinement, we checked the quality of a few summaries whether they contained necessary information and were free from fabricated details.

# Dataset Analysis

Table 3 compares the existing datasets for the visual dialogue task with ours. While our dataset includes a smaller number of videos and dialogues, it presents target videos with a longer average duration and represents a diverse combination of activities. Additionally, unlike existing datasets, our dataset includes KGs that provide structured, eventcentric information linking activities, events, and other relevant details. These KGs are a valuable resource for developing and evaluating video-grounded dialogue systems.

Activities for Scenarios We analyzed the diversity of activity combinations within scenarios by examining the distribution of activities across categories, as shown in Figure 2a. The analysis indicates a balanced distribution of activities among $1 ^ { \mathrm { s t } }$ to $4 ^ { \mathrm { t h } }$ placement within scenarios, categorized by their respective types. Although the HouseArrangement category shows a noticeably higher number of activities in scenarios compared to other categories, this is justified by the fact that it contains nearly twice as many distinct activities (45) as the second-highest category (23).

Question Types for Dialogues In Figure 2b, we conducted a comprehensive analysis of the question types used in the dataset by categorizing them into 13 pre-defined types based on their distinct characteristics, as detailed in Table 1. To compare our dataset with the closely related AVSD dataset, we randomly selected 60 dialogues from each and labeled each question with one or more of our question types. It is important to note that each question can be assigned multiple types. Our analysis revealed that the AVSD dataset primarily features the question types D-ACT, D-INT,

Abnormal (23)   
PhysicalActivity (6) BedTimeSleep (3) Work (5) EatingDrinking (11) Social Food   
Interaction Preparation (1) NW (8) g 5 Other (2) HouseArrangement (45) Leisure (14) HouseCleaning (13) HygieneStyling (6) 1st 2nd 3rd 4th 5th

(a) The occurrence of activities by categories based on their placements in scenarios. The number in parentheses indicates the total count of activities within that category.

![](images/7d8c8be881f190b120c76336e6c3a1163eb579505a3e2a11d04820c3ab57a5b6.jpg)  
(b) Comparison of the number of question types.   
Figure 2: Statistics on activities as scenarios in the VDAct dataset and percentages of different questions types for sample dialogues in comparison with the AVSD dataset.

and D-AGT, whereas our dataset highlights T-SEQ, D-INT, REF, and D-ACT as the top types. This difference emphasizes the realistic nature of our dialogues, which frequently reference dialogue history in a natural conversational style. Additionally, our dataset emphasizes discussions about activities involving actions and interactions between actions and objects, including temporal inferences. Particularly, it contains a significantly higher number of questions classified as D-OBJ, D-LOC, T-SEQ, EXP, and REF compared to the AVSD dataset. Meanwhile, as opposed to our dataset, the AVSD dataset includes a significantly higher number of questions related to D-AGT and VID types. This is because the AVSD dataset employs crowdsourced videos with varying agents, leading to a greater focus on questions concerning agent states such as ages, appearances, and emotions and the video attributes such as audio and language spoken.

Table 4: Comparison of Scenario and Activity KGs by the average number of different components.   

<html><body><table><tr><td></td><td>Activity</td><td>Scenario</td></tr><tr><td>Events</td><td>10.2</td><td>40.26</td></tr><tr><td>Situations</td><td>11.2</td><td>44.56</td></tr><tr><td>States</td><td>14.92</td><td>60.36</td></tr><tr><td>StateVals</td><td>10.84</td><td>44.04</td></tr><tr><td>Shapes</td><td>86.56</td><td>360.17</td></tr><tr><td>Main Objects</td><td>4.08</td><td>13.56</td></tr><tr><td>Target Objects</td><td>0.78</td><td>2.57</td></tr><tr><td>Triplets</td><td>320.78</td><td>1,317.51</td></tr></table></body></html>

Scenario KGs Table 4 shows the statistics of our scenario KGs after merging the activity KGs. From this table, it suggests that each scenario KG contains more number of Events, Situations, States, and Objects nodes than the activity KGs. This increase highlights the greater complexity and detail of our dataset in providing compound activities with multiple related events for the dialogue task.

# VDEval Metric

We introduce a new evaluation metric for the videogrounded dialogue task to overcome the shortcomings of the existing metrics. Particularly due to the lack of sufficient context, the existing metrics failed to verify the content in generated responses as they are only presented with the user utterances (i.e., questions) and references. Typically, in the context of dialogue, different responses can be considered accurate, even if they do not precisely match the references, as long as they align with the video context and dialogue history. In addition, in some cases, responses may include extra information that needs to be assessed for relevance.

Thus, for our proposed metric, we extended the existing LLM-based metric which demonstrates a high correlation with humans for the VQA task, LAVE (Man˜as, Krojer, and Agrawal 2024). LAVE uses rationales and a scaled score rating of 1-3 to assess the generated answer (i.e., response) given the question and the reference answer. Our new metric introduces two key improvements over LAVE. First, instead of evaluating the generated response using the individual turn-based context, we include the entire dialogue history, which provides context from the previous question-answer pairs with evaluated scores and rationale. Second, we add a video summary from scenario KGs that offers a high-level overview of the video content for evaluation. Our approach to incorporating session-based dialogue context is based on the idea that this additional context provides details and information that are difficult to capture with just the question and reference alone. By including the full dialogue history and video content summary, LLMs can use this richer context to accurately assess the correctness of the current response. Figure 3 illustrates how our enhanced metric compares to the existing one.

Turn-based Context Session-based Context Instruction: Instruction: dialogue 𝑑# dialogue 𝑑! dialogue 𝑑! Summary: D turn 𝑡 Summary: 1 turn 𝑡1 + dialogue 𝑑" turn 𝑡1 turn 𝑡2 turn 𝑡 turn 𝑡𝑁 turn 𝑡\$%!   
turn 𝒕𝒊   
Question: Where did he get the book from?