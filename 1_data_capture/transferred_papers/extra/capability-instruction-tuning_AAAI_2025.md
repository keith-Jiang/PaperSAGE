# Capability Instruction Tuning

Yi-Kai Zhang, De-Chuan Zhan, Han-Jia Ye\*

School of Artificial Intelligence, Nanjing University National Key Laboratory for Novel Software Technology, Nanjing University {zhangyk, zhandc, yehj} $@$ lamda.nju.edu.cn

# Abstract

Large Language Models (LLMs) have demonstrated humanlike instruction-following abilities, particularly those exceeding 100 billion parameters. The combined capability of some smaller, resource-friendly LLMs can address most of the instructions that larger LLMs excel at. In this work, we explore how to route the best-performing LLM for each instruction to achieve better overall performance. We develop a new paradigm, constructing capability instructions with model capability representation, user instruction, and performance inquiry prompts to assess the performance. To learn from capability instructions, we introduce a new end-to-end framework called Model Selection with Aptitude Test (MODEL-SAT), which generates positive and negative samples based on what different models perform well or struggle with. MODEL-SAT uses a model capability encoder that extends its model representation to a lightweight LLM. Our experiments show that MODEL-SAT understands the performance dimensions of candidate models and provides the probabilities of their capability to handle various instructions. Additionally, during deployment, a new model can quickly infer its aptitude test results across 50 tasks, each with 20 shots. MODEL-SAT performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios. The code is available at https://github.com/Now-Join-Us/CIT-LLM-Routing.

: Test instruction : GPT-4o’s correct : GPT-4o’s incorrect : Following 5 LLMs’ correct instructions, respectively   
2023/06 2023/06 2023/08 2023/11 2024/01 Phi-1 +ChatGLM2 +Zephyr +Yi +InternLM Coverage Ratio of GPT-4o Corrects 27% 45% 80% 88% 90% Release Time   
-1+ChatGLM2+Zephyr) covers $80 \%$ of the GPT-4o’s correct instruction

# Introduction

Large Language Models (LLMs) (OpenAI 2022; Du et al. 2022; Touvron et al. 2023a; Chiang et al. 2023; Jiang et al. 2023) rapidly evolve, demonstrating near-human general capabilities, especially in understanding, reasoning, and creative tasks related to instruction-response scenarios. Recent advancements have even enabled these LLMs to be trained in multilingual (Yang et al. 2024; Dubey et al. 2024), multidomain (Zhou et al. 2024; Yang et al. 2024), and multimodal (Chen et al. 2015, 2023; Reid et al. 2024) environments, allowing them to tackle complex instructions such as “What is the relationship between Fourier series and Hilbert space?” or to interpret images by identifying, “What are the basis vectors of the Hilbert space?”

The rise of LLMs and their extensions has incredibly energized community applications. However, achieving more comprehensive capabilities often requires LLMs of a larger scale. According to the Open LLM Leaderboard (Aidar Myrzakhan 2024), $60 \%$ of the top 50 LLMs have around 70 billion (B) parameters or more, with only three LLMs under 10B. Additionally, some closed-source LLMs consistently dominate performance rankings over extended periods. Consequently, optimizing LLM applications often hinges on substantial computational resources or costly token purchases. A natural idea arises: Can we utilize multiple smaller LLMs, which are more resource-friendly and have below one-tenth of the parameters of their larger counterparts, to achieve performance comparable to gigantic LLMs while maintaining low inference costs?

In the experiments, we find that the combined capability of some smaller-scale LLMs, despite their lower overall performance, can address most of the instructions that larger LLMs excel at. As shown in Figure 1, on the Massive Multitask Language Understanding (MMLU) (Hendrycks et al. 2020) benchmark, the Phi-1 LLM with 1.3B performs nearly $50 \%$ worse than GPT-4o. However, it exhibits similar effectiveness to GPT-4o in the high school mathematics category. Moreover, we create an early-access LLM zoo that includes Phi-1 (Gunasekar et al. 2023) and four 7B LLMs, which were released a year earlier than GPT-4o and exhibit an approximately $30 \%$ performance gap compared to GPT-4o. However, the combined accurate responses from this zoo cover $90 \%$ of which GPT-4o handles correctly and address nearly $80 \%$ with which GPT-4o struggles. By strategically assigning instructions to the suitable LLM in the zoo, there is potential to exceed GPT-4o’s performance by $1 5 \%$ . From this phenomenon, the model routing for each instruction enhances performance with seamless LLM transitions and minimal inference costs, all without user awareness.

The key to the proposed instruction-level model routing is to efficiently identify the optimal model from a vast pool of options, without prior access to the potential candidates’ inference outputs (Tan et al. 2023; Xiao et al. 2023) or the target task’s ground truth (You et al. 2022; Pándy et al. 2022). In this paper, we introduce MODEL-SAT: Model Selection with Aptitude Test. Our approach leverages 50 core 20-shot tasks, where the model test result represents model capability. By learning the generalization relationships between the capability representations of the candidate models and the instructions to be assigned, we can select the most suitable model across various repositories and target instructions.

Driven by the model capability representation, the MODELSAT framework establishes a novel paradigm, denoted as capability instruction tuning. Capability instructions consist of a capability representation, a user instruction, and a prompt to probe whether the model can perform that instruction. Using extensive historical performance data, capability instruction tuning learns an implicit relationship between core capability representations and unseen instructions. Moreover, it delves deeper into understanding the mapping between the capabilities’ performance and the instructions’ semantic distribution. This intuition comes from the observation that individuals who perform well in the mathematical sections of the college admission SAT in the United States often pursue careers that involve logical reasoning. Capability instruction tuning aims to equip the model with a lightweight standardized guide to assess its effectiveness in handling future instructions.

Specifically, we combine model capability representation with positive and negative training instructions regarding current model performance, yielding statements like, “The model achieves accuracy $8 5 \%$ on the task of ’Mathematics, Geometry, ...’. Instruction: ..., Predict whether the model can handle ...”. To align the performance distribution inherent in model representation to the instruction semantic, we are the first to incorporate a capability encoder and extend the input of a lightweight LLM to include capability representation.

The end-to-end MODEL-SAT functions as a model router that outputs the probabilities indicating which models will likely excel at specific instructions.

Additionally, we establish several comprehensive benchmarks for model routing of LLMs and their extensions. Our benchmarks cover a range of model zoos, such as (1) smallerscale, weaker ones, (2) mixed-scale options, and (3) highperformance larger-scale LLMs. Furthermore, we expand the model routing to include multimodal LLM-instruction settings. MODEL-SAT achieve significantly improved overall performance across model zoos without incurring any inference overhead, comparable to the performance levels of larger-scale LLMs. Notably, the capability instruction tuning maintains the model representation generalization to unseen data. The new LLM can quickly develop effective model representations after just a few inferences (only on $5 0 \mathrm { ~ x ~ } 2 0$ -shot tasks). In light of practical routing scenarios with the emergence of new-version LLMs, we establish 60 incremental routing scenarios that impose higher routing speed and overhead requirements. Throughout these settings, MODEL-SAT consistently demonstrates superior performance.

In summary, our contributions are:

• A novel paradigm: capability instruction tuning, where model representation with efficient aptitude tests and instructions create capability instructions for highperformance-driven instruction-level model routing.   
• MODEL-SAT framework, features a model capability encoder and a lightweight LLM to end-to-end learn the router via various model capability representations.   
• Comprehensive model routing benchmarks for LLMs and their extensions, covering five LLM zoo setups with multimodal scenarios, as well as simulating 60 incrementalreleased model routers to ensure quick adaptation to unseen data and new LLMs.   
• An open-source, deployable model routing toolkit that applies model routing techniques to any model zoo, enhancing performance while remaining unaware of users with acceptable routing delays.

# Preliminary

We begin by discussing the key elements and the pipeline of model selection, followed by the evolution of related works.

# Instruction, Output, and Answer

Consider a test instruction dataset $\mathcal { D } _ { \mathrm { t e s t } } = \{ ( \mathbf { x } _ { i } , \mathbf { a } _ { i } ) \} _ { i = 1 } ^ { N }$ with $N$ labeled samples. The $\mathbf { x } _ { i }$ and $\mathbf { a } _ { i }$ represent the instruction and its corresponding answer, respectively. Given an LLM or its extension, represented as $f$ , the output generated for instruction $\mathbf { x } _ { i }$ is denoted as $\mathbf { o } _ { i }$ , i.e., $f ( \mathbf { x } _ { i } ) = \mathbf { o } _ { i }$ . There are no restrictions on the language, domain, or modality of $\mathbf { x } _ { i }$ ; In this paper, we focus on decoder-only text generation models, which means that ${ \bf a } _ { i }$ is typically presented in text form. For the model $f$ to excel at instruction $\mathbf { x } _ { i }$ , it is equivalent to obtaining a high score on the evaluation eval $\left( \mathbf { o } _ { i } , \mathbf { a } _ { i } \right)$ .

# Pipeline of Model Routing

Consider a candidate model zoo composed of many trained LLMs, M = {f m}mM=1. Model routing involves selecting a

User Instruction $\mathbf { x } _ { i }$ Response In a cage, there are a total of 10 heads and 28 legs. 6 chickens and 4 rabbits. Calculate how many chickens and rabbits        are? 全 Our Implementation Overhead of Selection (FLOPS) Re-ranking Based (BGE) How Do Re-ranking Based Methods Work Capability Representation 全 MODEL-SAT First Inference on All Models from Aptitude Test Previous Method 2 chickens and 8 rabbits. 75% on Psychology, 55% on … 𝐨 6 chickens and 4 rabbits. 80% on Math, 35% on Histor… 6 chickens and 8 rabbits. 65% on Medicine, 65% on Bi… Growth of the Candidate Model Library Predict Re-ranking Score ? B B 8 B .4B 70B Z 2 3 8 专 𝐨!" 0.52 𝐜" 0.52 2 B Lian × 0.76 V Meta 3 久 𝐱! 𝐨 0.58 业 𝐱! 1 phi 𝐨 0.61 𝐜𝟑 0.61 (b) Comparative Analysis of Online-generated 𝐨!" Select the Fixed 𝐜! Only let the selected model Re-ranking Based Method Pink one answer the instruction! v.s. Our Deployment Cost: Efficiency Gains from

model from the zoo for each instruction $\mathbf { x } _ { i }$ in the test dataset ${ \mathcal { D } } _ { \mathrm { t e s t } }$ . Specifically, the sequence of selected models is formalized as ${ \pmb f } = ( \bar { f _ { 1 } } , f _ { 2 } , \ldots \bar { , } f _ { N } )$ , where $f _ { i } \in \mathcal { M }$ . We define the optimal model $\hat { f }$ for instruction $\mathbf { x } _ { i }$ as the model that maximizes the score: eva $\mathrm { l } \left( { \hat { f } } ( \mathbf { x } _ { i } ) , \mathbf { a } _ { i } \right)$ . The objective of the instruction-level model routing is:

$$
\hat { \pmb { f } } = \left( \underset { { \displaystyle f ^ { m } \in \mathcal { M } } } { \arg \operatorname* { m i n } } \ \ell \left( f ^ { m } \left( \mathbf { x } _ { i } \right) , \ \mathbf { a } _ { i } \right) \right) _ { i = 1 } ^ { N } \ ,
$$

where $\ell \left( \cdot \right)$ represents the loss function associated with the metrics between $\mathbf { o } _ { i } ^ { m } = f ^ { m } ( \mathbf { x } _ { i } )$ and the ground truth $\mathbf { a } _ { i }$ The model routing bottleneck arises from the number of instructions on which no model in the zoo performs well.

# Revisit from Requirement, Target, and Key Inputs

Routing target of parameter initialization or models with zero-shot capabilities: Early model router (Tran, Nguyen, and Hassner 2019; Nguyen et al. 2020; Tan, Li, and Huang 2021; Ding et al. 2022; Tianhao et al. 2024) efforts primarily focus on identifying a good training initialization that facilitates fine-tuning downstream tasks to achieve optimal performance. In this context, candidate models likely required additional training to adapt to the target task. Recently, guided by scaling laws, foundational models like LLMs have experienced remarkable advancements in their zero-shot capabilities (Touvron et al. 2023b; Wei et al. 2022; Team et al. 2023). Extended models have demonstrated considerable potential in multilingual, multi-domain, and multimodal applications. For instance, Llama 3.1 (Dubey et al. 2024) serves as a multilingual agent, Qwen2-Math (Yang et al. 2024) tackles several Olympiad-level problems, and GPT-4o (OpenAI 2023) processes information from multiple sources.

Routing requirements with target instruction annotation, backpropagation delay, or candidate output: Some works (Bao et al. 2019; Li et al. 2021; You et al. 2021; Deshpande et al. 2021; Pándy et al. 2022) design the proxy metric of transferability, which approximates the lower bound of fine-tuned performance. These works often rely on certain source clues, labeled instructions, or backpropagation steps to assess the transferability from the source pre-trained model to the target dataset. Additionally, some re-ranking-based works (Tan et al. 2023; Xiao et al. 2023; Zhang et al. 2024a) train an extra model to learn the contrastive relationships between the instruction and the candidate inference outputs im}mM=1, routing the optimal one linked to model f m. However, obtaining all inferences may introduce significant delays when the number of models $M$ in the repository becomes excessively large (Shnitzer et al. 2023; Lu et al. 2023; Hu et al. 2024). Our MODEL-SAT aims to route models without annotation or inference requirements, considering candidates as

Capability Instruction: $\mathrm { c } ^ { m } + { \pmb x } _ { i } + { \mathsf p }$   
User Instruction $\pmb { x } _ { i }$ : "10 heads and 28 legs, How many $\boxtimes$ and ?"   
Capability Instruction: The model achieve $7 5 \%$ on Psychology, $5 5 \%$ on Math, 35% on Medicine …… “ 2180 lhegasds, How many and ${ \ ? } ^ { \prime \prime }$ . Predict whether the model can handle 𝒙! Capability Representation 𝐜𝒎 User Instruction 𝒙𝒊 Performance Inquiry Prompt p

![](images/ba4722a730d6e4262c70f9c06962a76fc9147f3385f295dfbd26d4bcf284b52d.jpg)  
Figure 3: One example of a Capability Instruction. It is an instruction for model routing that inquires whether a model can handle a specific user instruction. It comprises three components: the capability representation $\mathtt { c } ^ { m }$ based on the streamlined aptitude test, the user instruction $\mathbf { x } _ { i }$ to be assigned, and a performance inquiry prompt p. This instruction is inputted into the MODEL-SAT Capability LLM, which outputs the probability that the model can perform the user instruction well.   
Figure 4: The Architecture of MODEL-SAT.

black boxes. A central feature is constructing model representations for each model and learning the adjusted relationship between it and the target instructions.

Key input – model representation for model routing: When routing a model for instruction, the router requires the key representation that captures the model’s characteristics. We followed the concept of learnware (Zhou 2016), leveraging a small amount of model-proficient data to construct shared specifications (Zhou and Tan 2024; Tan et al. 2024). Other relevant methods leverage forward behavior or results on target as model representation, which inevitably introduces inference delays. Recently, some approaches (Lu et al. 2024; Srivatsa, Maurya, and Kochmar 2024; Ding et al. 2024; Feng, Shen, and You 2024) have started to utilize learnable parameters as model representations. For instance, some introduce a surrogate scorer as the corresponding model representation, learning the mapping from the task to the accuracy of candidate model outputs. Model Spider (Zhang et al. 2023b) takes this concept by encoding the model representation into a learnable vector, which acts as the input token for a Transformer-based router. However, learnable representation face challenges when new models are introduced, as they require extensive historical performance for costly pre-training of the router. Our solution uses text-only descriptions of capabilities. New models can create representations by inferring 50 quick tasks, each with 20 shots.

# MODEL-SAT: Model Routing with Aptitude Test

In this section, we start by building the model representation and progress to the details of MODEL-SAT, training data, and optimization process. Finally, we outline an efficient deployment framework for model routing.

# Capability Instructions

The capability instruction mainly comprises the capability representation of the candidate model $f ^ { m }$ , user instruction $\mathbf { x } _ { i }$ and performance inquiry prompt. Specifically, the model’s capability representation is formed from 50 distinct tasks across various categories from the MMLU dataset, with each task being 20-shot. We provide a concise description of five keywords for each task. Next, we evaluate the candidate models across these 50 tasks and describe the results in natural language, i.e., model representation. Furthermore, the advantage of representing in natural language is that it helps to include extra expert knowledge, such as mentioning which languages a model supports. The easy-to-obtain representations serve as an aptitude test for the models, indicating their potential capabilities across various dimensions.

To assess how well the candidate model can follow a single or a set of instructions $\mathbf { x } _ { i }$ , we introduce the training instructions that were executed correctly versus those incorrectly. These will be paired with the performance inquiry prompt p to form the capability instruction, denoted as $\mathbf { z } _ { i }$ , which drives the router to predict adaptation scores. As illustrated in Figure 3, it combines the capability representation $\mathtt { C } ^ { m }$ for candidate $m$ , the instruction $\mathbf { x } _ { i }$ , and a inquiry prompt $\mathtt { p }$ . Core task sampling: We sample instructions of core tasks with the highest distinguishability, avoiding those where most models perform correctly or incorrectly. In the model zoo of training, samples for which half of the models make mistakes while the other half are correct carry greater weight.

# Architecture

Motivation: Although LLMs demonstrate strong instructionfollowing abilities, a gap exists between performance and the semantic distribution in capability instructions, particularly in understanding combinations of performance dimensions. For example, if a candidate model achieves $80 \%$ in mathematics and $9 5 \%$ in legal principles, the model may possess legal reasoning skills. To address this, we propose extending a capability encoder E5-Large ${ \sim } 0 . 5 \mathrm { B }$ $( \psi )$ before a Phi-3-Mini 3.8B LLM $( \varphi )$ to align the candidate performance with the instructions. The architecture is illustrated in Figure 4.

Structure: The capability instruction comprises the capability representation $\mathtt { c } ^ { m }$ for model $m$ , the instruction $\mathbf { x } _ { i }$ , and the query prompt $\mathtt { p }$ . We first align the model representation, mapped by the capability encoder, into an embedded feature of LLM inputs. This is achieved using a single-layer MLP, which acts as a connector to adapt the dimensions.

Consequently, we derive the aligned model capability vector:

$$
\mathbf { e } _ { \scriptscriptstyle \subset ^ { m } } = \mathbf { W } \cdot \boldsymbol { \psi } \left( \mathsf { c } ^ { m } \right) ,
$$

where $\mathbf { e } _ { \scriptscriptstyle \mathrm { C } ^ { m } }$ is combined with the input embeddings of $\mathbf { x } _ { i }$ and $\mathtt { p }$ to form the capability instruction $\mathbf { z } _ { i }$ , i.e.,

$$
\mathbf { z } _ { i } = [ \mathbf { e } _ { \mathrm { c } ^ { m } } , \mathbf { e } _ { \mathbf { x } _ { i } } , \mathbf { e } _ { \mathrm { p } } ] = [ \mathbf { e } _ { 1 } , \mathbf { e } _ { 2 } , \ldots , \mathbf { e } _ { s } ] ,
$$

where $s$ denotes the length of the concatenated capability instruction sequence. Our alignment module operates at a natural language level, allowing for a streamlined design. In the following Section , we also explore alternative approaches, including training without the alignment module.

# Tuning Recipe

Forward Process of the Prediction Score: As shown in Figure 3, the query prompt $\mathtt { p }$ in the capability instruction includes keywords related to positive terms that the model excels at. For example, “Yes” serves as the key response in the prompt “predict whether the model can handle test instruction by indicating ‘Yes’ or ‘No’.” In this context, the model routing prediction score is:

$$
\operatorname* { P r } \left( { ^ { \ast } } \mathrm { Y e s } ^ { \prime } \mid \mathbf { z } _ { i } \right) = \prod _ { t = 1 } ^ { s } \mathbf { 1 } _ { \left( \mid \mathbf { c } ^ { m } \mid , s \right) } \cdot \varphi \left( \mathbf { e } _ { t } \mid \left[ \mathbf { e } _ { 1 } , \cdot \cdot \cdot , \mathbf { e } _ { t - 1 } \right] \right) ,
$$

where we omit the input embedding layer for the $\operatorname { L L M } \varphi$ .

Positive and Negative Instructions for Training: We apply Homogeneous In-Batch Negative Sampling (Karpukhin et al. 2020; Zhang et al. 2023a) for each capability representation $\mathtt { c } ^ { m }$ with its well-performed and poorly-performed instructions to enhance the discriminative during training. Typically, a $k$ -shot training batch $\mathbf { Z } = \left\{ \mathbf { z } _ { i } \right\} _ { i = 0 } ^ { k }$ contains 1 positive instruction and $k - 1$ negative ones.

Loss Design: We denote the position of the positive instruction in the training batch $\mathbf { Z }$ as $y _ { \mathrm { p o s } }$ , and the remaining ones are $k - 1$ negative instructions. Our objective is to enhance the prediction score for the positive ones as the candidate performs better on this instruction. We employ the cross-entropy loss to optimize this in one batch $\mathbf { Z }$ :

$$
\mathcal { L } _ { \mathrm { C E } } = \mathbb { E } _ { \mathbf { Z } \in \mathcal { D } _ { \mathrm { t e s t } } } [ - \log \operatorname* { P r } ( \pmb { h } _ { \varphi ,  { \mathrm { \ : Y e s } } ^ { \prime } } ( \mathbf { Z } ) = y _ { \mathrm { p o s } }  \mathbf { Z } ) ] ,
$$

where $h _ { \varphi , \mathrm { ' Y e s } } , ( \mathbf { Z } ) \ = \ \arg \operatorname* { m a x } _ { \mathbf { z } _ { i } \in \mathbf { Z } } \ \mathrm { P r } \left( \mathbf { \tilde { \Delta } Y e s } ^ { , } | \mathbf { z } _ { i } \right)$ is the LLM $\varphi$ to identify which instruction $\mathbf { z } _ { i } \in \mathbf { Z }$ can be done well (positive) and which cannot (negative).

Learning Strategy: The model representation is derived from the capability distribution on MMLU. Similarly, we develop both in-domain and out-of-domain learning environments for MODEL-SAT. In the first stage, we collect in-domain positive and negative training instructions, primarily sourced from the same category as the MMLU dataset. We only fine-tune the connector between the capability encoder $\psi$ and the LLM $\varphi$ , establishing an initial capabilityto-instruction mapping. In the second stage, we fine-tune all model parameters. We apply a larger learning rate on the encoder and connector to enhance capability alignment with instruction semantics.

Data Refinement: We further address noise in whether the candidate model can accurately perform the instructions, influencing whether a capability instruction is a positive or negative training sample. For those difficult instructions that only a few models handle correctly, we implement a circle test by rotating the sequence of options to prevent lucky guesses. Furthermore, we prioritize higher-ranked candidates in the training data by sampling with increased weight.

# Efficient Deployment

MODEL-SAT provides the routing prediction for the candidate model applied to the target instruction. These scores are generated by the same model, rendering them comparable. In this paper, we propose an open-source and comprehensive model routing toolkit, MODEL-SAT. This toolkit offers a viable solution for dynamic model routing within communities such as HuggingFace, harnessing the repository to boost performance on target tasks.

MODEL-SAT exhibits remarkable generalization capabilities for unseen data, which can be directly concatenated into the capability instruction. Similarly, the incremental extension to new models proves highly efficient, requiring only inference on 50 core tasks for the model representation. As later addressed in the experiments, MODEL-SAT exhibits zero-shot model routing abilities, facilitating the streamlined development of capability instructions in broader contexts.

# Experiments

This section begins by detailing the construction of training and test instructions in capability instructions tuning. It then presents different zoo setups for testing and concludes with an analysis of results and ablation studies.

# Training and Test Instructions

As mentioned earlier, the capability instruction consists of model representation $\mathtt { C } ^ { m }$ , instructions $\mathbf { x } _ { i }$ to assign, and performance inquiry prompts $\mathtt { p }$ .

Candidate Model Representations $\mathtt { C } ^ { m }$ for candidate $m$ We introduced 66 open-source LLMs of varying scales. This includes 60 models under 10B, 15 ones between 10B and 20B, and 5 ones around 60B. We sample 50 categories from the MMLU dataset, with 20 distinguishing instructions from each. Different candidate models share core tasks to ensure stability in capability demonstration.

Instructions $\mathbf { x } _ { i }$ Pending to Assign: We consider more than 20 datasets that include areas such as language, analysis, understanding, and reasoning in general evaluations, as well as specialized fields like mathematics and medicine. For each dataset, we sample sets of positive and negative instructions where the model performed well or poorly, with sampling on stronger models assigned greater weight. Each dataset contains about 100 instructions on average.

Performance Inquiry Prompts p: We explore different approaches for the probability of model routing. For capability instructions, we design the performance inquiry prompt, such as “predict whether the model can handle test instruction by indicating ‘Yes’ or ‘No’.” In this context, a response of ‘Yes’ signifies that the model is well-performed to the instruction. We also experiment with integrating a regression linear layer onto the next token embedding.

Table 1: A Comprehensive Performance Evaluation: Covering smaller-scale, high-performance giant LLMs, and a mixed LLM zoo of small, medium, and large levels. Model-SAT performs instruction-level model selection, consistently maintaining efficient and precise results that outperform the optimal one in the LLM zoo. Bold is the best, and underlined is the second-best.   

<html><body><table><tr><td rowspan="2">Method</td><td rowspan="2">#Params</td><td colspan="2">In-Domain WinoG.</td><td colspan="4">Out-of-Domain</td><td rowspan="2">MNLI</td><td rowspan="2">Mean</td></tr><tr><td>MMLU</td><td></td><td>ARC-C</td><td>BoolQ</td><td>TruthfulQAMRPC</td><td></td></tr><tr><td colspan="10">Smaller-scale LLMs (<10B)</td></tr><tr><td rowspan="9">InternLM2.5 (Cai et al. 2024) Meta-Llama-3 Instruct (Touvron et al. 2023b) Qwen2 Instruct (Yang et al. 2024)</td><td>7.7B</td><td>69.88</td><td>81.22</td><td>60.75</td><td>70.43</td><td>54.56</td><td>68.38</td><td>60.68</td><td>66.89</td></tr><tr><td>8.0B</td><td>65.59</td><td>75.45</td><td>62.12</td><td>76.76</td><td>51.63</td><td>68.38</td><td>55.82</td><td>65.62</td></tr><tr><td>7.6B</td><td>69.13</td><td>74.11</td><td>61.43</td><td>82.57</td><td>55.49</td><td>78.92</td><td>54.96</td><td>68.19</td></tr><tr><td>9.4B</td><td>69.28</td><td>80.82</td><td>66.13</td><td>84.77</td><td>59.32</td><td>78.92</td><td>40.73</td><td>68.65</td></tr><tr><td>7.4B xB</td><td>75.90</td><td>77.11</td><td>71.08</td><td>86.70</td><td>64.62</td><td>75.98 78.92</td><td>46.82</td><td>71.38</td></tr><tr><td>75.90 81.22</td><td></td><td></td><td>71.08</td><td>86.70</td><td>64.62</td><td></td><td>60.68</td><td>74.16</td></tr><tr><td>LLM Selection on Smaller-scale LLMs (5 models)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>xB</td><td></td><td>70.11</td><td>77.66</td><td>64.59</td><td>79.94</td><td>57.58</td><td>72.79</td><td>51.54</td></tr><tr><td>Cappy (Tan et al. 2023)</td><td>M·xB</td><td>69.53</td><td>78.06</td><td>63.99</td><td>81.10</td><td>57.77</td><td>74.75</td><td>53.17</td></tr><tr><td>BGE Large (Xiao et al. 2023)</td><td>(0.3+M) xB</td><td>71.68</td><td>78.53</td><td>66.38</td><td>82.48</td><td>61.44</td><td>73.28</td><td>55.39</td><td>68.34 69.88</td></tr><tr><td>GTE Large (Zhang et al. 2024a) MODEL-SAT (Ours)</td><td>(1.8+M) xB M·4.3B+xB</td><td>72.02</td><td>79.32</td><td>68.09</td><td>83.73</td><td>59.61</td><td>75.74</td><td>56.16</td><td>70.67</td></tr><tr><td></td><td></td><td>79.86</td><td>82.24</td><td>72.53</td><td>86.73</td><td>65.12</td><td>79.66</td><td>60.83</td><td>75.28</td></tr><tr><td colspan="10">14B 34B</td></tr><tr><td colspan="2">Phi-3 Medium-128k (Abdin et al. 2024)</td><td>76.63</td><td>74.35</td><td>66.49</td><td></td><td></td><td>Larger-Scale LLMs (10B~50B)</td><td></td><td></td></tr><tr><td colspan="2">Yi-1.5 chat (Young et al. 2024)</td><td></td><td>81.47</td><td>70.62</td><td>86.30 87.84</td><td>54.54</td><td>78.92 80.88</td><td>59.42</td><td>70.95 74.50</td></tr><tr><td colspan="2">Meta-Llama-3 Instruct (Touvron et al. 2023b)</td><td>77.15 79.89</td><td>82.62</td><td>71.67</td><td>93.61</td><td>62.02 61.83</td><td>83.58</td><td>61.56</td><td>76.90</td></tr><tr><td colspan="2">Qwen2 Instruct (Yang et al. 2024)</td><td>83.79</td><td>84.41</td><td>68.62</td><td>94.90</td><td>54.85</td><td>84.31</td><td>65.07</td><td>76.83</td></tr><tr><td colspan="2">Mixtral-8x22B Instruct-v0.1 (Jiang et al.2024)</td><td>77.63</td><td>85.25</td><td>72.68</td><td>92.71</td><td>68.19</td><td>81.13</td><td>66.95 67.70</td><td>77.90</td></tr><tr><td colspan="10"></td></tr><tr><td colspan="10">Smaller-Scale LLM Zoo</td></tr><tr><td rowspan="6"></td><td></td><td>79.86</td><td>82.24</td><td>72.53</td><td>86.73</td><td>65.12</td><td>79.66</td><td>60.83</td><td>75.28</td></tr><tr><td>Smaller-Mixed LLM Zoo</td><td>78.60</td><td>82.08</td><td>72.01</td><td>86.48</td><td>64.50</td><td>78.19</td><td>60.72</td><td>74.65</td></tr><tr><td>Ours Middle-Mixed LLM Zoo M?4.3B+xB</td><td>79.97</td><td>83.03</td><td>72.69</td><td>87.80</td><td>64.87</td><td>83.82</td><td>61.80</td><td>76.28</td></tr><tr><td>Larger-Mixed LLM Zoo</td><td>84.16</td><td>86.27</td><td>73.21</td><td>93.94</td><td>69.16</td><td>85.54</td><td>67.92</td><td>80.03</td></tr><tr><td>High-Performance LLM Zoo</td><td>85.64</td><td>87.85</td><td>73.63</td><td>95.02</td><td>69.40</td><td>88.24</td><td>68.39</td><td>81.17</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

Table 2: Performance Comparisons of Other Learning Strategies for Capability Instructions. The capability encoder of Model-SAT learns the mapping of performance to semantics, demonstrating strong model selection abilities.   

<html><body><table><tr><td>Method</td><td>MMLU</td><td>ARC-C</td><td>TruthfulQA</td><td>Mean</td></tr><tr><td>Random Selection</td><td>70.11</td><td>64.59</td><td>57.58</td><td>64.09</td></tr><tr><td>Best-Perfoming</td><td>75.90</td><td>71.08</td><td>64.62</td><td>70.53</td></tr><tr><td>RoBERTa+MLP</td><td>71.25</td><td>64.33</td><td>59.12</td><td>64.90</td></tr><tr><td>+kNearest Neighbors</td><td>70.02</td><td>65.02</td><td>58.51</td><td>64.52</td></tr><tr><td>+ Random Forest</td><td>73.75</td><td>68.05</td><td>61.44</td><td>67.75</td></tr><tr><td>Phi-3 Mini-128K</td><td>74.71</td><td>70.58</td><td>61.70</td><td>68.83</td></tr><tr><td>MODEL-SAT</td><td>79.86</td><td>72.53</td><td>65.12</td><td>72.50</td></tr></table></body></html>

The capability instruction for the test $\mathbf { z } _ { i }$ similarly consists of the model representation $\mathtt { c } ^ { m }$ , the target instruction $\mathbf { x } _ { i }$ to be assigned, and the performance inquiry prompt $\mathrm { ^ p }$ . To ensure test stability, we conduct a perturbation evaluation on model representation. Specifically, we randomly alter the ranking of the aptitude test results in capability representation twice and then calculate the average routing scores $\operatorname* { P r } \left( \mathbf { \hat { \pi } } \mathbf { Y } \mathbf { e s } ^ { , } \mid \mathbf { z } _ { i } \right)$ . The response on this instruction $\mathbf { x } _ { i }$ is provided by the candidate model with the highest routing score.

Table 3: Performance Comparisons in Selection of Multimodal LLM. Model-SAT maintains excellent average performance on the above three popular evaluation benchmarks.   

<html><body><table><tr><td>Dataset</td><td>Phi-3</td><td>InternLM</td><td>MiniCPM</td><td>Random</td><td>MODEL-SAT</td></tr><tr><td>MMMU VAL</td><td>41.86</td><td>41.06</td><td>43.10</td><td>42.19</td><td>43.21</td></tr><tr><td>AI2D TEST</td><td>78.40</td><td>79.44</td><td>77.33</td><td>78.24</td><td>80.38</td></tr><tr><td>CCBench</td><td>37.60</td><td>56.62</td><td>57.16</td><td>50.49</td><td>56.96</td></tr></table></body></html>

# Benchmarks of LLM Routing

In this section, we outline benchmarks with various LLMs and their extension zoos, featuring detailed settings.

Smaller-Scale LLM do Better: As demonstrated in the Table 1, the smaller-scale zoo contains InternLM2.5 (7.7B), Meta-Llama-3-Instruct (8.0B), Qwen2-Instruct (7.6B), GLM4 (9.4B), and Phi-3-Small-128K (7.4B). The smallermixed zoo includes the smaller-scale zoo and Phi-1 (1.3B), BLOOMZ (3B), and Zephyr-Alpha (7.2B). These LLMs have fewer than 10B parameters and low deployment costs. In Figure 1, we show that the union of correct responses can cover a set of instructions that only larger-scale ones can manage.

General LLM Zoo Settings. 1) Middle-Mixed and Larger-Mixed LLM Zoo: The middle-mixed zoo includes the smaller-scale zoo and Phi-3-Medium-128K (14B), and Yi-1.5-Chat (34B). The larger-mixed zoo consists of middlemixed ones, Meta-Llama-3-Instruct (70B), Qwen2-Instruct (72B), and Mixtral- $\cdot 8 \mathrm { x } 2 2 \mathrm { B }$ -Instruct-v0.1 (140B). The mixed zoo can validate the routing method across different capabilities. 2) High-Performance LLM Zoo: We select from larger-scale LLMs to boost performance further. The model zoo contains only three models above with over 70B parameters. 3) Multimodal LLM Zoo: To verify the generality of capability instruction tuning, we construct a multimodal LLM zoo that includes MiniCPM-Llama3-V 2.5, Phi-3-Vision$1 2 8 \mathrm { k \Omega }$ -Instruct, and InternLM-XComposer2-VL-7B.

Instructions for Model Routing Evaluation: The test capability instructions differ from the training ones of model routers and consist of seven evaluation datasets. Datasets including MMLU (Hendrycks et al. 2021) (5-shot) and WinoGrande (Sakaguchi et al. 2020) (5-shot) cover a broad range and are involved in the training part as the in-domain evaluation. On the other hand, datasets such as ARCChallenge (Bhakthavatsalam et al. 2021) (25-shot), TruthfulQA (6-shot), and BoolQ (Clark et al. 2019) (1-shot) with MRPC (1-shot) and MNLI (1-shot) in GLUE (Wang et al. 2019) benchmark focus on specific capabilities, serving as the unseen out-of-domain evaluation of the model routers. We consider the evaluation datasets MMMU-VAL (Hendrycks et al. 2020), AI2D-TEST (Kembhavi et al. 2016), and CCBench (Liu et al. 2024) in the multimodal scenario.

Real-world Model Routing with Unseen Datasets & Latest LLMs: 1) In Table 1, In-Domain and Out-of-Domain indicate whether the dataset is included in the training set for LLM routing. 2) We also design a novel model routing setting that, with the release of 60 LLMs, we update the existing zoo after each new model with the top 5 historically best and the latest one. With the continual increment of unseen LLMs, this dynamic environment tests whether methods can maintain a compelling performance.

# Toward Comprehensive and Effective Routing

Performance Analysis in Various Model Zoos. Table 1 demonstrates that MODEL-SAT performs impressively across five comprehensive LLM Zoos. (a) Smaller-Scale: routing of LLMs under 10B achieve performance comparable to the $\mathord { \sim } 7 0 \mathrm { B }$ LLMs. MODEL-SAT’s average score of $7 5 . 2 8 \%$ closely matches Meta-Llama-3-Instruct-70B’s $7 6 . 9 0 \%$ , and outperforms it on the ARC-C and TruthfulQA benchmarks. Furthermore, MODEL-SAT selects the optimal model for each instruction, surpassing the best-performing models in the Smaller-Scale Zoo. (b) Smaller-Mixed: We add three earlier released, weaker, and smaller LLMs. MODEL-SAT maintained stable performance, with a slight decrease of about $1 \%$ compared to row (a), while performance on ARC-C, BoolQ, and MNLI benchmarks remained nearly identical. (c) MiddleMixed: Row (c) includes two medium-scale models (10B to 70B), resulting in improved performance for MODEL-SAT compared to row (a). Its average performance now closely matches that of the 70B model. (d) Larger-Mixed: Incorporating three 70B models in row (d) showed that MODEL-SAT remains robust despite significant performance variances in the LLM zoo, with improvements of nearly $5 \%$ on MMLU, BoolQ, and so on. (e) High-Performance: Row (e) features a routing of only three 70B models, revealing that the capabilities of gigantic LLMs are further unleashed, achieving a state-of-the-art score of $8 5 . 6 4 \%$ on MMLU and $7 3 . 6 3 \%$ on the ARC-Challenge.

Comparative Analysis of Routing Delay. In Table 1, the selected model parameter-scale is denoted as $\mathbf { \nabla } _ { \mathbf { X } } \mathbf { B }$ . The overhead associated with the re-ranking method is related to the $M$ candidates of $\mathbf { \nabla } _ { \mathbf { X } } \mathbf { B }$ . Figure 2 illustrates that the reranking requires obtaining all inference results from the zoo first, while MODEL-SAT processes model representations all at once and utilizes them throughout its lifetime. As the scale of models in the zoo grows, MODEL-SAT’s routing cost remains unaffected by inference ones, ensuring efficient model routing.

Comparison Ranking-based Methods: We evaluate reranking methods such as Cappy, BGE-Large, and GTE-Large. Although they have access to the outputs of each candidate, re-ranking often struggles to find optimal results, potentially because it primarily focuses on retrieving different semantics rather than optimizing performance across similar outputs.

Detailed Comparative Analysis: Furthermore, we explore various learning strategies within the capability instruction tuning framework. Using features extracted from RoBERTa, we train MLP (classification-based), $k$ Nearest Neighbors (clustering-based), and Random Forest (tree-based). We observed that, except for tree models that are suitable for handling capability representation as similar tabular data, other learning strategies fail to capture performance distribution mappings. Additionally, we analyze MODEL-SAT without the capability encoder. Since the model representation is expressed at the natural language level, Phi-3 in Table 2 can also learn some LLM-Instruction mappings, but its performance remains inferior to that of capability-encoder-based ones.

Ablation Studies: Explore Generalization in Multimodal Scenarios. Most multimodal LLMs (MLLMs) are derived from input-extending LLMs. Multimodal MODELSAT is built on the Wings (Zhang et al. 2024b) training architecture, integrating model representation embeddings with visual ones. It maintains strong performance in multimodal scenarios, achieving optimal average performance across MMMU-VAL, AI2D-TEST, and CCBench datasets.

# Conclusion

This paper proposes a novel model routing paradigm called capability instruction tuning with instruction-level model routing. It constructs a capability instruction comprising capabilities, instructions, and inquiry prompts to select the most suitable one. We present MODEL-SAT, featuring a capability encoder and lightweight LLM. It selects models without inference overhead of candidates and quickly adapts to new models. Model-SAT performs optimally across five proposed LLM routing settings and its multimodal extension.