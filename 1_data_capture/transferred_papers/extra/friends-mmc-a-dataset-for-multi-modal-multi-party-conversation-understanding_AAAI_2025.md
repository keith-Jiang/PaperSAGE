# Friends-MMC: A Dataset for Multi-modal Multi-party Conversation Understanding

Yueqian Wang1, Xiaojun Meng2, Yuxuan Wang3, Jianxin Liang1, Qun Liu2, Dongyan Zhao1,4

1Wangxuan Institute of Computer Technology, Peking University 2Huawei Noah’s Ark Lab 3Beijing Institute for General Artificial Intelligence 4National Key Laboratory of General Artificial Intelligence {wangyueqian,liangjx,zhaodongyan}@pku.edu.cn, {xiaojun.meng, qun.liu}@huawei.com, wangyuxuan1 $@$ bigai.ai

# Abstract

Multi-modal multi-party conversation (MMC) is a less studied yet important topic of research due to that it well fits real-world scenarios and thus potentially has more widelyused applications. Compared with the traditional multi-modal conversations, MMC requires stronger character-centered understanding abilities as there are many interlocutors appearing in both the visual and textual context. To facilitate the study of this problem, we present Friends-MMC in this paper, an MMC dataset that contains $^ { 2 4 , 0 0 0 + }$ unique utterances paired with video context. To explore the character-centered understanding of the dialogue, we also annotate the speaker of each utterance, the names and bounding bboxes of faces that appear in the video. Based on this Friends-MMC dataset, we further study two fundamental MMC tasks: conversation speaker identification and conversation response prediction, both of which have the multi-party nature with the video or image as visual context. For conversation speaker identification, we demonstrate the inefficiencies of existing methods such as pre-trained models, and propose a simple yet effective baseline method that leverages an optimization solver to utilize the context of two modalities to achieve better performance. For conversation response prediction, we fine-tune generative dialogue models on Friend-MMC, and analyze the benefits of speaker information. The code and dataset will be publicly available, and thus we call for more attention on modelling speaker information when understanding conversations.

# Datasets —

https://github.com/yellow-binary-tree/Friends-MMC

# Introduction

Multi-modal dialogue systems have attracted extensive attention in recent studies (Zang et al. 2021; Zheng et al. 2022; Feng et al. 2023; Zhu et al. 2023; Liu et al. 2023; Li et al. 2023a; Maaz et al. 2023; Li et al. 2023b), especially with the rapid development of multi-modal large language models. However, there are two main deficiencies of existing works: (1) As most multi-modal datasets are collected from human annotations, LLM generated contents or chat history from social media, these dialogues are mostly presented in question-answer format (AlAmri et al. 2019) between a human and a system, instead of among several human interlocutors; (2) the speakers are bystanders (Das et al. 2016) and discuss the given visual content such as an image or a video, instead of really being situated in the context.

In contrast, in real-world face-to-face conversations, the interlocutors are situated in the visual & audio context 1, which means that the conversation and the visual context influence each other rather than interlocutors discussing about a given fixed visual context. Also, there can be more than 2 interlocutors involved, which means that modelling speaker information is crucial for comprehending the conversation. Understanding face-to-face conversations is an important foundation for achieving embodied artificial intelligence (Zhou et al. 2023; Xu et al. 2023; Zhang et al. 2023). Therefore, we emphasize that multi-modal multi-party conversation is a very important field for real industrial applications yet a less studied topic.

To overcome the shortcomings of existing work on multimodal conversation, in this paper we extend its scopes and propose a new field of research: multi-modal multi-party conversation (MMC). Compared with traditional multimodal conversations, MMC differs in the following aspects: (1) The conversation is not only between a user and an assistant, but among an arbitrary number of interlocutors, therefore the information of the speaker should be explicitly modeled; (2) Instead of chatting on images / videos as bystanders, the interlocutors are situated within the visual context, or in other words, the visual context provides rich information about the interlocutors, such as their identities, expressions and actions.

To foster the study of MMC, we build Friends-MMC, a multi-modal multi-party conversation dataset collected from the famous TV series Friends. 2 An overview of Friends

![](images/8dba190f9cfe1612ec9243474b39b779bb1b806214c962355db067287cf758ac.jpg)  
Figure 1: An example of multi-modal multi-party conversation. The task of conversation speaker identification is to infer the dotted arrows pointing from characters to utterances, and the task of conversation response prediction is to infer the last utterance in the dotted rectangular. Only one frame of the video is shown as the visual context to avoid clutter.

MMC is shown in Figure 1. A session in Friends-MMC consists of several turns, each paired with a video clip as visual context, in which the face tracks are detected and classified by character‘. Compared to the existing multi-modal dialogue and multi-party conversation datasets, our proposed Friends-MMC have some traits worth emphasizing:

a) Modalities of available data are multiple, including: textual information of each utterance3, visual & audio context in forms of frames and videos, face bounding boxes and face character names. Utilizing all of these modalities can be challenging for existing models; b) Conversations are taken from daily life such as TV series, which are more natural and diverse compared to existing multi-party conversation datasets (Ouchi and Tsuboi 2016; Hu et al. 2019) that are mostly collected from chats on the topic of computers. c) Reasoning can be very complex. For example, in terms of conversation speaker identification, the speaker may not appear in its corresponding video or frame. Therefore, the preceding or succeeding textual and visual context, as well as their temporal relations, should be taken into account, which is quite difficult to solve for even humans in our experiments.

We focus on two fundamental MMC tasks: Conversation Speaker Identification and Conversation Response Prediction. The goal of conversation speaker identification is to link the speaker of every turn to the faces in the corresponding visual context. It requires a system to not only infer the speaker of each turn from the textual content, but also understand the visual context in which the dialogue happens. Existing works (Gu et al. 2021; Meng, Mou, and Jin 2018) of speaker identification mostly focus on text-only multi-party conversation, where only the speaker of the last one or few turns should be predicted given the speaker label of previous turns. Our paper introduces the multi-modal information of the interlocutors to make this task more challenging and better conform to real-world scenarios where all speaker labels are not available in the dialogue history. To accomplish this task, we present our task-specific baseline method including three modality-specific modules which is further introduced later in the “Conversation Speaker Identification” Section. With our task-specific designs, this system can achieve competitive accuracy on conversation speaker identification, far exceeding recent multi-modal pre-trained models, while takes a small amount of computation and a enjoys a high degree of flexibility.

The other task is conversation response prediction, one of the most popular tasks for dialogue modelling. It requires a system to predict the last utterance with respect to the context. Compared with existing response generation task in multi-modal dialogues, the context of MMC is very heterogeneous in modality. We fine-tune text-only and multi-modal dialogue models on Friends-MMC with different sources of speaker information, to validate that the speaker information is critical to the response prediction of MMCs. In other words, the ability of correctly identifying the conversation speaker can benefit response prediction, which is an issue that has been ignored in existing multimodal dialogue researches where two speakers simply take turns to speak to each other.

In summary, our contributions are three-fold: (1) We formally propose multi-modal multi-party conversation, a new and valuable field of research, and study two sub-tasks of it: conversation speaker identification and conversation response prediction; (2) We build and release Friends-MMC, a dataset to facilitate the study of multi-modal multi-party conversation; (3) We design a baseline for conversation speaker identification, validate its performance on FriendsMMC, and analyze the benefits of speaker information on conversation response prediction.

# Friends-MMC Dataset

In this section, we describe the dataset collection and annotation procedure for constructing the Friends-MMC dataset, which covers all the 220 episodes from 10 seasons of the TV show Friends. The reasons we use Friends are: (1) it is a sitcom series, which has numerous conversations that contain diverse topics of daily life; (2) Though having as many as 220 episodes, it has a relatively small number of main characters, which is convenient for automatic face labelling and data cleaning; and (3) It’s easy to get publicly available resources like high-quality subtitles that are often manually revised and paired perfectly with the video by a large group of TV fans, which greatly reduces manual labour during the data construction process as well as guarantees the data quality.

The content and speaker of each turn are extracted from transcripts and subtitles4. Faces and their character names in each frame are detected and labelled automatically for the train set (Season 1, 2, and from 4 to 10), and manually for the test set (Season 3) to ensure its accuracy.

# Construction Process

Figure 2 shows the overall construction process of the dataset. Now we introduce every step in details:

Video Preprocessing. We crop a clip from the video according to the start and end timestamp in the subtitle. We use an off-the-shelf face detector (Zhang et al. 2017) to detect faces for each frame in the clip. Following (Kalogeiton and Zisserman 2020), we merge the faces in adjacent frames into face tracks and thus remove the faces that are not in any track to clean out false positive faces.

Face Prototype Construction. C1C (Kalogeiton and Zisserman 2020) is a dataset with human-labelled face tracks for season 3 of Friends. We choose a set of 18 main characters, manually select 20 faces in different viewing angles per character, and encode them using Facenet-512 (Schroff, Kalenichenko, and Philbin 2015) to get face feature vectors in the embedding space of Facenet-512 as prototypes for each character.

Automatic Face Labelling. We automatically label the detected faces tracks with character names by finding their nearest neighbour in the mentioned embedding space. In particular, for each detected face track, we encode each frame in the track with Facenet-512 and calculate the cosine similarity between their feature and all prototypes. If the mean value of the largest 5 cosine similarities is greater than a threshold $t = 0 . 6$ (which is set to maximize the validation accuracy described in the following paragraph), we label this face track with the corresponding character name, otherwise we think this face does not belong to any of the main characters and discard it.

To validate the accuracy of automatic face labelling, we use the same process to detect and label faces for season 3 and compare the results with human-annotated ones from C1C. The verification follows the rule that if the IoU of bounding boxes of an automatically labelled face and a human-annotated face is greater than 0.5, we identify them as a pair of identical faces. Given this threshold, $9 5 \%$ of all pairs of identical faces are labelled with correct names, which verifies the effectiveness of our automatic face labelling method.

Test & Test-noisy Set. For the test set, we directly use the human-annotated faces in C1C to guarantee the accuracy of face labelling, thus serving as high-quality ground-truths for this test set. Moreover, in order to align with the fact of imperfect face recognition in real-world scenarios and be consistent with the train set, we also create a more challenging test-noisy set by randomly removing $20 \%$ labelled face tracks.

Image Frame Selection. As many recent multi-modal pre-trained models accept images instead of videos as input, we also provide an alternative option of visual context where each turn is paired with only 1 frame. In such case, we select the frame with the most detected faces from each video clip, as well as its face bounding boxes and face names, as the paired visual context of this turn.

Session Selection with Sliding Windows. We use a sliding window of size $m$ to select $m$ adjacent turns as a dialogue session if the following conditions are met: (1) all speakers are from the main characters; (2) the time intervals between all adjacent turns are shorter than 8 seconds, which is a heuristic rule to prevent selecting turns from different scenes. Therefore, we use $m = \{ 5 , 8 \}$ to create two types of sessions with different context lengths (5 turns and 8 turns). Note that different dialogue sessions may contain the same turn, as it belongs to different contexts and thus the preceding or succeeding textual and visual contents differ.

# Dataset Statistics

Dataset statistics are shown in Table 3. Apart from the basic statistics, we also count the proportion of speakers whose faces are not detected in the current clip or frame, or even not appear in all the $m$ clips or frames within this entire session.

As table 3 shows, the test-noisy set includes a significantly larger number of speakers not in the current clips (or frames for image as input) than the test set. Therefore, this case is more challenging for speaker identification task, as the candidate model needs to really understand the conversation and find out more clues from the context rather than only the current clips or frames to infer who is the real speaker.

# Conversation Speaker Identification Task Introduction

Conversation speaker identification requires models to identify the speaker of each turn given the textual and visual context. Existing works on speaker identification mostly focuses on the text-only multi-party conversation. It often asks models to predict the speaker of the last few turns, given the dialogue history and the speaker of previous turns. However, in real-world scenarios, speaker labels are usually either available for all turns or not available for any turn. To simulate it, when we extend the task to multi-modality, we provide the basic visual information of interlocutors so that the speaker of each turn can be predicted.

start end speaker content C1C Chandler Embedding Space start end speaker content faces 1196 1272 Monica There's ... Monica 1196 1272 Monica There's ... [box1, Monica], [...] ProFtaocteype hoebe 1272 1375 Joey C'mon, ... 1272 1375 Joey C'mon, ... [box1, Joey] Construction 1392 1461 Chand... So, does... 1392 1461 Chand. So, does... [box1, Joey], [...] Video Chand Preprocessing (disca Mon Automatic · SeswsitohnSliedleinctgion Face Labeling Windows

Table 1: Dataset Statistics of Friends-MMC. We provide a train set, a test set and a more challenging test-noisy set.   

<html><body><table><tr><td rowspan="2"></td><td colspan="3">5 turns</td><td colspan="3">8 turns</td></tr><tr><td>train</td><td>test</td><td>test-noisy</td><td>train</td><td>test</td><td>test-noisy</td></tr><tr><td># sessions # unique turns</td><td>13584 21092</td><td>2017 3069</td><td>2017 3069</td><td>8730 16990</td><td>1325</td><td>1325 2480</td></tr><tr><td># words in utterance # speakers in each session</td><td>18.87 2.83</td><td>20.28 2.85</td><td>20.28 2.85</td><td>18.71 3.43</td><td>2480 20.42 3.47</td><td>20.42 3.47</td></tr><tr><td># face tracks per clip avg.secsper face track</td><td>2.41 2.31</td><td>3.12 2.71</td><td>2.50 2.72</td><td>2.39</td><td>3.14</td><td>2.52</td></tr><tr><td>% speakers not in current clip</td><td>13.43</td><td></td><td></td><td>2.30</td><td>2.74</td><td>2.73</td></tr><tr><td></td><td></td><td>1.03</td><td>19.26</td><td>13.51</td><td>1.10</td><td>18.93</td></tr><tr><td>% speakersnot in all clips</td><td>6.13</td><td>0.17</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td>1.13</td><td>5.57</td><td>0.14</td><td>0.44</td></tr><tr><td># faces per frame</td><td>1.61</td><td>2.20</td><td>1.76</td><td>1.60</td><td>2.21</td><td>1.78</td></tr><tr><td>% speakers not in current frame</td><td>24.05</td><td>6.52</td><td>25.64</td><td>24.15</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>6.42</td><td>25.30</td></tr><tr><td>% speakers not in all frames</td><td>9.53</td><td>1.01</td><td>3.32</td><td>7.45</td><td>0.42</td><td>1.37</td></tr></table></body></html>

However, too many decisive clues for identifying the speaker, such as voice characteristics and facial movements, can also be provided by the video and audio. This will cause models to tend to only rely on the rich video information and ignore the dialogue context, which deviates from our original motivation of promoting researches on conversation systems. Therefore, we also propose an alternative setting: providing only one frame and no audio as the visual context. In this setting, the basic information of interlocutors such as identities, expressions, and the scene they are in, can also be provided by this frame, and it leaks less decisive clues and shortcuts for directly identifying the speaker.

# Baseline Method

To effectively utilize all the modalities of the proposed benchmark dataset, including visual, audio, textual and face tracks, we propose a baseline method, which consists of three modules: 1) a visual model $M _ { 1 }$ to recognize speaking faces, 2) a text model $M _ { 2 }$ to analyse multi-speaker relations based on dialogue contexts, and 3) a quadratic binary optimization problem solver to combine their results and thus identify the speaker of each turn. This modular design makes our system enjoy a high degree of flexibility, as one can easily change the visual model or the text model with alternative ones when different contextual information (e.g., using image or video as visual input) is provided. Figure 3 shows the overview of our proposed method, and we introduce each module in the following paragraphs.

Visual Model We use a visual model to predict the probability of each face belonging to the current speaker individually: $p _ { f a c e } = M _ { 1 } \bar { ( f a c e ) { \bf \Psi } } \in { \bf \Psi } ( 0 , 1 )$ . There are two different visual contexts: 1) If the provided visual context is an image frame, we use an inception model (Szegedy et al. 2014) pre-trained on VGGFace2 (Cao et al. 2017) and finetuned on the train set of Friends-MMC as the visual model $M _ { 1 }$ , and $f a c e$ is an image of the face cropped using bounding box $b$ . When fine-tuning on Friends-MMC, the speaking label of a face $y _ { f a c e }$ is set to 1 if the character name $c$ of this face is identical to the speaker name $y$ , and 0 otherwise: $y _ { f a c e } = \mathbf { 1 } [ c = y ]$ . We use the cross-entropy classification loss as the training objective. 2) If the provided visual context is a video, we use TalkNet (Tao et al. 2021), a state-ofthe-art active speaker detection model, as the visual model $M _ { 1 }$ , and face is a video of the face cropped using bounding box sequence $b$ , accompanied by the audio of the same period. Note that using TalkNet in a zero-shot manner is good enough, we try to fine-tune it on the train set of FriendsMMC as like the CNN model, but it does not result in better performance.

Text Model We use a DeBERTa-v3 (He, Gao, and Chen 2021) first trained on Ubuntu Dialogue Corpus (Hu et al. 2019) and then fine-tuned on the train set of Friends-MMC as $M _ { 2 }$ to predict whether every two utterances in a dialogue session are spoken by the same speaker. The intuitive reason behind it is that for some utterances, it is hard to identify the speaker solely by feeding the video or frame into $M _ { 1 }$ . We thus try to conjecture its speaker by finding whether it likely shares the same speaker with another utterance, for which we have confidences or prior knowledge to infer its speaker. In addition, $M _ { 2 }$ helps to achieve a relatively global optimum speaker inference by considering all turns together.

Given a dialogue session consists of $m$ utterances, we prepend an ${ < } \mathrm { b } \mathrm { o } s >$ token to each utterance as the input of $M _ { 2 }$ as like:

$< \mathrm { b } \odot \mathsf { s } > u _ { 1 } \cdot \cdot \cdot < \mathrm { b } \odot \mathsf { s } > u _ { m }$ . We use the last hidden state of each ${ < } \mathrm { b } \circ \mathsf { s } > h _ { i }$ as the representation of each utterance, and use a head layer to calculate the similarity of every two representations:

$$
p _ { s i m } ^ { i j } = \sigma ( W _ { 2 } { \mathrm { G e L U } } ( W _ { 1 } [ h _ { i } ; h _ { j } ; | h _ { i } - h _ { j } | ] + b _ { 1 } ) + b _ { 2 } )
$$

where $i , j = 1 , \cdots , m$ , and $( W _ { 1 } , b _ { 1 } , W _ { 2 } , b _ { 2 } )$ are learnable parameters. $\sigma$ is the sigmoid activation function, and pisjim ∈ (0, 1) is a scalar that denotes the probability of two utterances spoken by the same person. The loss function is defined as:

$$
\mathcal { L } _ { M _ { 2 } } = M S E ( p _ { s i m } , y _ { s i m } ) + M S E ( p _ { s i m } , p _ { s i m } ^ { T } )
$$

where $y _ { s i m } \in \{ 0 , 1 \} ^ { m \times m }$ is the ground truth label of whether any two utterances are from the same speaker, and $M S E$ denotes mean squared error loss.

Quadratic Binary Optimization Problem Solver This module aims to integrate the outputs of both visual $( M _ { 1 } )$ and textual $( M _ { 2 } )$ models for speaker identification. In particular, we design two matrices using the outputs of these two models, and address it as the quadratic binary optimization.

For each dialogue session, we first obtain a candidate speaker set by recording all faces appeared in every frame $/$ video: $\mathbf { C } = \{ c _ { 1 } , \cdot \cdot \cdot , c _ { l } \}$ . We construct a vision reward matrix $\mathbf { B } \in R ^ { l \times m }$ of selecting a character $c _ { i }$ as the speaker of the turn $u _ { j }$ . If the face of $c _ { i }$ appears in the frame $/$ video $v _ { j } , b _ { i j }$ is set to the probability of that face as a speaking face predicted by $M _ { 1 }$ , otherwise $b _ { i j } ~ = ~ 0$ . Apparently, B can only express those situations that the speaker appears in the corresponding frame $/$ video. To address those problems, we construct another text reward matrix $\mathbf { A } \in R ^ { m \times m }$ to reveal the reward for assigning the same speaker to two turns $u _ { i }$ and $u _ { j }$ . We first use $M _ { 2 }$ described in the previous subsection and get a similarity matrix $p _ { s i m }$ . However, if we simply use this similarity matrix $p _ { s i m }$ as the reward matrix A, since all elements in $p _ { s i m }$ are larger than 0, the optimization solver tends to assign the same speaker to every turn to get the maximum rewards. To avoid which, we subtract the similarity matrix with the mean value of its elements as the reward matrix, i.e., $\mathbf { A } = p _ { s i m } - \mathrm { m e a n } ( p _ { s i m } )$

As Figure 3 shows, with $\mathbf { A }$ and $\mathbf { B }$ in hand, the task of multi-modal multi-party speaker identification can be represented by a quadratic binary optimization problem:

$$
\begin{array} { c l } { \mathrm { M a x i m i z e } } & { f ( X ) = ( 1 - \alpha ) X ^ { T } A X + \alpha X B } \\ { \mathrm { s . t . } } & { X \in \{ 0 , 1 \} ^ { m \times l } , } \\ & { \displaystyle \sum _ { j = 1 } ^ { l } X _ { i j } = 1 , \quad i = 1 , 2 , . . . , m } \end{array}
$$

where $\alpha$ is a hyperparameter to control the weight of two rewards and is selected according to the performance on a validation set held-out from the train set. We use 0.8 for frame as visual context, 0.7 for video as visual context, and 0.2 when ground truth labels of the text model are provided $( M _ { 2 } ^ { \dagger } )$ . By now, this problem can be easily solved using optimization problem solvers like (Gurobi Optimization, LLC 2023), which adaptively makes decisions based on the output of $M _ { 1 }$ and $M _ { 2 }$ . The reason we use an optimization solver instead of an end-to-end pre-trained model is that this task of identifying speakers still remains challenging to use the general attention mechanism of pre-trained models to fuse different modalities.

![](images/3c8cc702d1b05426dec2c7950560b25924cd27d886d254e19735a452508c340e.jpg)  
Figure 3: Model overview of the three modules in different colors: the visual $( M _ { 1 } )$ is yellow, the textual $( M _ { 2 } )$ is green, and the optimization solver taking vision and text reward matrix as input is blue.

# Experiment Results

Implementations We conduct experiments in five settings with information of different modalities used: (1) image (frame) only; (2) video only; (3) text only; (4) image and text; and (5) video and text. In image only setting (1), we use the model $M _ { 1 }$ CNN to predict one face from all detected faces from the frame as the speaker. If there are no faces, we randomly choose a character from the candidate speaker list. The video only (2) is almost the same as (1) but uses $M _ { 1 }$ TalkNet to predict one face from the video. In the text only setting (3), we compare $M _ { 2 }$ (DeBERTa-v3) with 3-shot GPT 3.5 using its in-context learning ability to perform this task. For $M _ { 2 }$ , although it is good at judging whether two utterances are said by the same speaker, it is not trained to identify the speaker for a single utterance. Therefore, it can only make guesses according to the relations between sentences. GPT 3.5, however, possesses some ability to understand the candidate speaker list and identifying names from utterances, so it can make full use of the textual context to achieve more accurate reasoning. In image-text setting (4), the jointed $M _ { 1 }$ $\small \begin{array} { r l } { J _ { 1 } \ \mathrm { ( C N N ) } + M _ { 2 } } \end{array}$ model is used together with a quadratic binary optimization solver, and we also try to replace the output of $M _ { 2 }$ with ground truth labels (denoted by $M _ { 2 } ^ { \dagger }$ ) to explore bottlenecks and possible improvement directions. We also test GPT4-o (OpenAI 2024) under this setting. Video-text setting (5) is almost the same as (4), but again CNN is replaced by TalkNet for $M _ { 1 }$ .

We also fine-tune three popular and powerful multi-modal pre-trained models: Violet $\mathrm { F u }$ et al. 2021), LLaVA (Liu et al. 2023) and Emu (Sun et al. 2023). As LLaVA only accepts one image at a time during pre-training, we also use one single turn (one utterance and one frame) during fine-tuning and testing, neglecting information from adjacent turns. Violet and Emu, on the other hand, are pre-trained with videos or image-text interleaved data, so we can directly use all 5 or 8 turns as input.

Table 2: Accuracy on the test and test-noisy set of FriendsMMC. $M _ { 1 }$ and $M _ { 2 }$ denote the visual and textual model of our baseline method, respectively. For $M _ { 1 }$ , we use CNN or TalkNet to take image or video as input. $\dagger$ indicates that we use ground truths instead of textual model outputs $( M _ { 2 } )$ to serve as upper bounds.   

<html><body><table><tr><td></td><td>5 turns noisy</td><td>8 turns</td><td>noisy</td></tr><tr><td>0</td><td>random (std.dev.)</td><td>31.82 32.61 (0.25)(0.47)(0.49)(0.27)</td><td>28.5429.03</td></tr><tr><td colspan="4">Frame Only</td></tr><tr><td>1</td><td>M1(CNN)</td><td>72.88 63.7272.90</td><td>62.51</td></tr><tr><td colspan="4">VideoOnly</td></tr><tr><td>2 M1(TalkNet)</td><td></td><td>80.89 70.9181.00</td><td>70.50</td></tr><tr><td colspan="4">Text Only</td></tr><tr><td>3 4</td><td>M2 GPT3.5 (3-shot)</td><td>33.24 33.85 37.21 37.24</td><td>29.09 29.33 33.35 32.81</td></tr><tr><td colspan="4">Useimageand textmodality Violet</td></tr><tr><td>6 LLaVAv1.5-13B 7 Emu-14B 8 M1(CNN)+M2 9 M(CNN)+M 10 GPT-4o (0-shot)</td><td>46.30 61.76 75.81 84.90 66.36</td><td>42.39 58.23 68.61 78.01 65.60</td><td>45.73 41.41 60.96 56.46 74.53 67.21 90.80 83.93 63.64 61.02 84.49</td></tr><tr><td colspan="4">Usevideo and textmodality 12 Mi(TalkNet)+M2 83.21</td></tr><tr><td>13Mi(TalkNet)+M</td><td>90.88</td><td>74.12 83.09</td><td>83.60 75.00 95.10 89.69</td></tr></table></body></html>

To check how much useful information can be provided by using frame as the visual context, we also report the human performance of this task. we randomly sample 80 dialogue sessions from each (5 turns $_ { / 8 }$ turns) test set, provide dialogue contents, frames, face bounding boxes & names to participants, and ask them to select a speaker for each turn from the candidate speaker set (i.e., the characters that appear in all frames). This process requires intensive efforts from humans, according to their post-interview, as the task of selecting speakers requires careful observation and a thorough understanding of the dialogue contents. We thus only perform the human studies on the test set, since we believe the human performance on the test-noisy set should be apparently worse. See appendix for details of all the baselines mentioned above.

Main Results Results can be found in Table 2: (1) visual context acquired by the vision model $M _ { 1 }$ , including which face appears in the frame and looks like a speaking face, serves as the most critical clues, shown by the performance of $M _ { 1 }$ (line 1, 2). We can conclude that this speaker identification task is still vision dominant, especially when videos are provided. (2) Speaker relations acquired by the text model $M _ { 2 }$ also play a vital supporting role to bring a significant improvement of $2 \% \sim 5 \%$ from $M _ { 1 }$ to $M _ { 1 } + M _ { 2 }$ (line $8 \nu s$ . line 1, line $1 2 \nu s$ . line 2). The textual context benefits this task not only by providing dialogue contents, but also aims for more real scenarios where the speaker does not appear in the scene. What’s more, the benefit is greater when the paired visual model is more accurate. It makes sense that one has to accurately identify speakers of some turns before it is able to identify other turns using this speaker relation information. (3) Directly fine-tuning a multi-modal pretrained model (line 5, 6, 7) or using proprietary multi-modal models (line 10) is not a good option despite we have tried different models with various parameter sizes. We believe the key aspects that are essential to solve this problem remain difficult to be understood by these candidate models, as this task differs a lot from the original training objectives. It also may be due to the reason that this speaker identification task is difficult to be formatted as the proper and shorten input of the model and thus to be easily learned. (4) Comparing the line 8-9 and 12-13, our current method of fine-tuning $M _ { 2 }$ model to predict whether two utterances are spoken by the same speaker still has a lot of room for improvement. This strong upper bound result also evidences that our designed objective for the textual module is meaningful.

The influence of the hyper-parameter alpha to the results is listed in the appendix.

# Conversation Response Prediction

# Task Introduction

Conversation response prediction aims to predict the textual content of the last turn given the visual context of all turns and the textual content of all previous turns. In this study, we demonstrate that speaker information is critical to the response prediction task of multi-modal multi-party conversations. Though predicting the next utterance in multi-modal dialogues has been a widely studied topic, none of the previous work focuses on the specific nature of multi-party dialogues, i.e., taking the speaker information into account.

We hypothesize that the potential advantages come from speaker information in two aspects: (1) The model may learn the speaking style of each speaker. When the speaker information like name is available, it can be used as a global speaking style indicator for generating better responses; (2) Speaker information can be a local context indicator to infer the response is from which person’s point of view, i.e., should be consistent with which utterance in the local conversation.

To validate these two aspects, we constructed 3 test sets: (a) a test set with random speaker names: speakers of all utterances are randomly assigned. Here the potential advantages of (1) and (2) are both broken. (b) a test set with random history speaker names: similar to (a) but we keep the speaker name of the utterance we need to predict unchanged. Here only the potential advantage of (2) is broken. (c) a test set with shuffled names: we replace all names in both speaker information and utterance content with another name according to a predefined random shuffle mapping, e.g., replace “Ross” with “Joey”, and “Joey” with “Chandler”. Here only the potential advantage of (1) is broken. Note that in all cases the speaker information of the train set is not modified.

Our experiments are in the following settings: train and test without speaker name; train with ground truth speaker information and test with ground truth or random or shuffled or our identified speaker name $( M _ { 1 } + M _ { 2 } )$ . Note that when using our $M _ { 2 }$ model, the last utterance to predict is removed from the input, i.e., the speaker of top $m - 1$ turns is predicted using both visual and textual context, and the speaker of the last turn is predicted using visual context. In addition, we finetune Emu-14B as a dialogue model with visual input, and Llama2-7B as a dialogue model without visual input. As mentioned in (Wang, Wang, and Zhao 2023), it is often hard to predict the next utterance solely by the dialogue history, hence the text generation metrics (e.g., rouge) may not be very valid when evaluating on our movie dialogue dataset. Therefore, following (Wang, Wang, and Zhao 2023), we build a response selection task to evaluate models instead of using generative metrics. We randomly select 9 other utterances from the test set as negative responses, and ask the model to select the one with the lowest perplexity from totally 10 (plus ground truth) candidates as output.

Table 3: Accuracy of conversation response prediction by selecting one from a set of ten utterances as candidates.   

<html><body><table><tr><td>Model</td><td>Speaker</td><td>5 turns</td><td>8 turns</td></tr><tr><td>Llama2-7B</td><td>No Random Random History Shuffled Ground truth Mi(CNN)+ M2 Mi(TalkNet)+M2</td><td>30.69 31.23 31.63 35.20 36.89 34.16 34.56</td><td>36.98 43.32 43.40 48.60 49.36 45.81 46.64</td></tr><tr><td>Emu-14B</td><td>No Random Random History Shuffled Ground truth Mi(CNN)+ M2 Mi(TalkNet) + M2</td><td>30.49 29.35 29.45 33.02 34.06 31.98 32.97</td><td>31.09 31.55 31.25 35.17 36.30 33.89 34.64</td></tr></table></body></html>

# Experiment Results

As shown in Table 3, regardless of which pre-trained model is used, or the length of dialogue context, in all cases adding speaker information always improves the performance. Comparing to using the random speakers and random history speakers, the performance with shuffled speakers is much closer to the one with ground truth speakers. Therefore, in terms of our hypothesis on the two potential advantages that come from speaker information, this result indicates that speaker information mainly works by indicating the local context instead of serving as a global speaking style indicator. Overall, our baseline method of speaker identification also benefits the response prediction task as it achieves a significant better performance than the ones with no or random speakers.

# Related Works

# Multi-party Conversations

Multi-party conversations (MPC), as opposed to two-party conversations, is a more practical and challenging scenario which studies conversations that involve more than two interlocutors. Research on MPC often includes three subtopics: speaker prediction, utterance prediction, and addressee prediction. (Ouchi and Tsuboi 2016; Hu et al. 2019) built MPC datasets from Ubuntu technical dialogues and proposed baseline models for MPC tasks. Recent studies on MPC usually train and evaluate models jointly on those three objectives. (Gu et al. 2021) propose MPC-BERT, which finetunes BERT (Devlin et al. 2019) on several self-supervised tasks, and achieve state-of-the-art results on the above MPC tasks. (Su and Zhou 2022) identify speakers of utterances by clustering them with pairwise relations encoded by a dialogue content encoder. GIFT (Gu et al. 2023) revises the model structure of transformer encoders to make the selfattention layer be aware of the information flow of MPC. Details regarding MPC can be found in these recent surveys (Gu, Tao, and Ling 2022; Ganesh, Palmer, and Kann 2023).

# Multi-Modal Dialogue Datasets

There have been a number of works on building multi-modal dialogue datasets (Das et al. 2016; Mostafazadeh et al. 2017; Shuster et al. 2020; AlAmri et al. 2019; Zheng et al. 2022; Zang et al. 2021; Feng et al. 2023). Despite the diversity in modality (image or video) and the position of the visual information in the dialogue history, all above datasets are limited as the interlocutors are outside the visual contexts rather than “situated” inside them, and only includes two interlocutors instead of being “multi-party”.

Dialogue in movie/TV series is a typical data source of multi-party conversation with “situated” visual context. Recent large-scale movie dialogue datasets include OpenViDial (Meng et al. 2020; Wang et al. 2021) and VSTAR (Wang et al. 2023). However, these datasets do not have any kind of speaker information, which hinders a deeper-level understanding of the conversation. Perhaps the dataset most similar to ours is MELD (Poria et al. 2018), which is also a speaker-aware multi-modal multi-party dialogue dataset collected from Friends but focuses only on emotion recognition, and does not annotate faces in the visual context.

# Conclusion

We work on a new field of research, multi-modal multi-party conversation (MMC), to bridge the gap between existing researches on multi-modal dialogue and real-world applications such as face-to-face conversations or meetings. We build Friends-MMC, an MMC dataset in which each utterance is paired with video context, speaker, face bounding box and face name annotation. We propose two new tasks of MMC, namely conversation speaker identification and conversation response prediction, and design a baseline to solve both tasks. Our future directions include collecting more diverse data rather than movies, and exploring ways to make better use of speaker information for response generation.