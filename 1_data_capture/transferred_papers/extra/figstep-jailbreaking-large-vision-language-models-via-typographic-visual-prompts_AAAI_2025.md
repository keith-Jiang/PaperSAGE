# FigStep: Jailbreaking Large Vision-Language Models via Typographic Visual Prompts

Yichen $\mathbf { G o n g ^ { 1 * } }$ , Delong $\mathbf { R a n } ^ { 2 * }$ , Jinyuan $\mathbf { L i u } ^ { 3 }$ , Conglei Wang4, Tianshuo $\mathbf { C o n g ^ { 3 \dagger } }$ , Anyu Wang3,5,6†, Sisi Duan3,5,6,7, Xiaoyun Wang3,5,6,7,8

1Department of Computer Science and Technology, Tsinghua University, 2Institute for Network Sciences and Cyberspace, Tsinghua University, 3Institute for Advanced Study, BNRist, Tsinghua University, 4Carnegie Mellon University, 5Zhongguancun Laboratory, 6National Financial Cryptography Research Center, 7Shandong Institute of Blockchain, 8School of Cyber Science and Technology, Shandong University gongyc18, rdl22, liujinyuan24 @mails.tsinghua.edu.cn, congleiw $@$ andrew.cmu.edu, {congtianshuo, anyuwang, duansisi, xiaoyunwang} $@$ tsinghua.edu.cn

# Abstract

Large Vision-Language Models (LVLMs) signify a groundbreaking paradigm shift within the Artificial Intelligence (AI) community, extending beyond the capabilities of Large Language Models (LLMs) by assimilating additional modalities (e.g., images). Despite this advancement, the safety of LVLMs remains adequately underexplored, with a potential overreliance on the safety assurances purported by their underlying LLMs. In this paper, we propose FigStep, a straightforward yet effective black-box jailbreak algorithm against LVLMs. Instead of feeding textual harmful instructions directly, FigStep converts the prohibited content into images through typography to bypass the safety alignment. The experimental results indicate that FigStep can achieve an average attack success rate of $8 2 . 5 0 \%$ on six promising open-source LVLMs. Not merely to demonstrate the efficacy of FigStep, we conduct comprehensive ablation studies and analyze the distribution of the semantic embeddings to uncover that the reason behind the success of FigStep is the deficiency of safety alignment for visual embeddings. Moreover, we compare FigStep with five text-only jailbreaks and four image-based jailbreaks to demonstrate the superiority of FigStep, i.e., negligible attack costs and better attack performance. Above all, our work reveals that current LVLMs are vulnerable to jailbreak attacks, which highlights the necessity of novel cross-modality safety alignment techniques.

Code, Datasets — https://github.com/ThuCCSLab/FigStep Extended version — https://arxiv.org/abs/2311.05608

# Introduction

Large Vision-Language Models (LVLMs) are at the forefront of the recent transformative wave in Artificial Intelligence (AI) research. Unlike single-modal Large Language Models (LLMs) like ChatGPT (OpenAI 2022), LVLMs can process queries with both visual and textual modalities. Noteworthy LVLMs like GPT-4V (OpenAI 2023a) and LLaVA (Liu et al. 2023b) have remarkable abilities, which could enhance end-user-oriented scenarios like image captioning for blind people (Xu et al. 2015) or recommendation systems for children (Deldjoo et al. 2017), where content safety is crucial.

![](images/d072e079037362836fc48b5b078803ffc203071f7401cd8433717b4e50ca2f6b.jpg)  
Figure 1: FigStep jailbreaks LVLM through transferring the harmful information from textual domain to visual domain, thereby bypassing the textual module’s safety alignment.

Typically, an LVLM consists of a visual module, a connector, and a textual module (see Figure 1). To be specific, the visual module is an image encoder (Radford et al. 2021; Li et al. 2023) that extracts visual embeddings from imageprompts. The connector will transform these visual embeddings to the same latent space as the textual module (Liu et al. 2023b). The textual module takes the concatenation of textprompts and transforms visual embeddings to generate the final textual responses. As the core component of LVLM, the textual module is usually an off-the-shelf pre-trained LLM that has undergone strict safety alignment to ensure LVLM safety (Zheng et al. 2023; Touvron et al. 2023; Perez et al. 2022; Korbak et al. 2023; Shevlane et al. 2023).

However, most of the popular open-source LVLMs do not undergo a rigorous safety assessment before being released (Liu et al. 2023b; Zhu et al. 2023; Wang et al. 2023). Meanwhile, since the components of LVLM are not safely aligned as a whole, the safety guardrail of the underlying LLM may not cover the unforeseen domains introduced by the visual modality, which could lead to jailbreaks. Therefore, a natural question arises: Does the safety alignment of the underlying LLMs provide an illusory safety guarantee to the corresponding LVLMs? It is worth noting that recent research has revealed that LVLMs are susceptible to jailbreak attacks (Shayegani, Dong, and Abu-Ghazaleh 2024; Qi et al. 2023a; Carlini et al. 2023). The cornerstone of their methodology involves manipulating the model’s output by introducing perturbation, usually generated through optimization, to the image-prompts, which is fundamentally analogous to the techniques employed in crafting adversarial examples within the Computer Vision (CV) domain (Carlini and Wagner 2017; Madry et al. 2019).

We highlight that distinct from the above jailbreak methods, FigStep eliminates the need for perturbation, thereby asserting that black-box access alone is sufficient to jailbreak LVLMs. Meanwhile, our intention is not only to exhibit that the computational cost and technical barriers to executing FigStep are negligible, but also to leverage FigStep to underscore the ubiquity of safety vulnerabilities within LVLMs. More critically, compared with optimization-based jailbreaks, FigStep could offer a more convenient baseline for conducting safety assessments of LVLMs.

# Our Contributions

We first propose a novel safety benchmark namely SafeBench, on which we launch FigStep against six popular open-source LVLMs. Our results demonstrate that FigStep substantially promotes the Attack Success Rate (ASR) compared to directly feeding text-only harmful questions. To find out the reason behind the success of FigStep, we further perform exhaustive ablation studies and analyze the distribution of semantic embeddings, noticing that the visual embeddings are only semantically but not safely aligned to the LLM’s textual embeddings. Finally, we explore three potential defense methods: OCR-tool detection, adding random noise, and system prompt modification, and find that all of them are ineffective in resisting FigStep. Accordingly, we propose two enhanced variants: $\mathsf { F i g S t e p } _ { \mathrm { a d v } }$ and $\mathsf { F i g S t e p } _ { \mathrm { h i d e } }$ to address the OCR detection. We also propose $\mathsf { F i g S t e p } _ { \mathrm { p r o } }$ , which splits imageprompt into harmless segments, to jailbreak GPT-4V (OpenAI 2023a) and GPT-4o (OpenAI 2024a).

In summary, we prove that adversaries can easily exploit the core ideas of FigStep to jailbreak LVLMs, thereby revealing that the safety of LVLMs cannot be solely dependent on their underlying LLMs. This is because of an intrinsic limitation within text-only safety alignment approaches that hinders their applicability to the non-discrete nature of visual information. To this end, we advocate for the utilization of FigStep as a “probe” to aid in the development of novel safety alignment methodologies that can align the textual and visual modalities in a compositional manner.

Above all, our major contributions are as follows. • We introduce SafeBench, a novel comprehensive safety benchmark for evaluating the safety risks of LVLMs.

• We propose FigStep, an efficient black-box jailbreak algorithm against LVLMs. We highlight that FigStep should serve as a baseline for evaluating LVLM’s cross-modal safety alignment.   
• Our work demonstrates that current prominent LVLMs (open-source or closed-source) are exposed to significant risks of misuse, necessitating the urgent development of new defensive mechanisms.

# Related Work

Jailbreak Against LLMs. To forbid LLMs from generating harmful content that violates human values (Bommasani et al. 2021; Liang et al. 2022), different safety alignment techniques are proposed, such as supervised instructiontuning (Wei et al. 2021; Ouyang et al. 2022) and RLHF (Li 2017; Chung et al. 2022). However, safety alignment techniques are not impregnable. Currently, there are two methodologies capable of compromising these safety mechanisms: the removal of safety guardrails through model fine-tuning techniques (Qi et al. 2023b; Zhan et al. 2023; Yang et al. 2023) and jailbreaks that focus on the meticulous modification of inputs to bypass the safety alignment without updating model parameters (Yi et al. 2024; Liu et al. 2023c; Deng et al. 2024). We focus on jailbreaks in this paper. The jailbreak techniques are broadly classified into two categories: gradient-based methods represented by Greedy Coordinate Gradient (GCG) (Zou et al. 2023) and non-gradient methods, such as MultiLingual (Deng et al. 2024), CipherChat (Yuan et al. 2023), DeepInception (Li et al. 2024), and In-Context Attack (ICA) (Wei, Wang, and Wang 2023).

Jailbreak Against LVLMs. The current safety alignment techniques primarily focused on the training and fine-tuning processes of single-modal language models. With the trend in LLMs moving towards multimodality, recent studies (Carlini et al. 2023; Bailey et al. 2023; Zhao et al. 2023; Qi et al. 2023a; Shayegani, Dong, and Abu-Ghazaleh 2024; Niu et al. 2024) have demonstrated that LVLMs can be directed to produce arbitrary responses (e.g., wrong image description or harmful response) through generating adversarial perturbations onto the input images. Unlike these attacks, FigStep has almost no costs with a weaker threat model.

# Threat Model

Adversary’s Goal. The adversary’s goal is to exploit the LVLM in order to obtain the answer to some questions that are forbidden by the safety policy, even though the LVLM is designed to avoid doing so. This goal captures the real-world scenario, where a malicious user might abuse the model’s power to acquire inappropriate knowledge, or an ignorant user might force the model to provide guidance for crucial decisions without considering the risk of being misled.

Adversary’s Knowledge & Capabilities. In this paper, we present a black-box attack that does not require any information or manipulation of the LVLM. The adversary is only required to have the capability to query the model and receive its textual response. The dialogue is restricted to one turn without any history except a preset system prompt. This scenario resembles the most common situation where the attacker is merely a regular user who cannot deploy an LVLM instance on their own due to the unavailability of the model or the scarcity of resources.

# Methodology

In this section, we present FigStep, a straightforward yet effective jailbreak algorithm using typographic visual prompts. Initially, we elucidate the core concepts of our attack, followed by a detailed presentation of the FigStep pipeline.

# Intuitions

We first summarize the main observations about LVLM that can inspire our attack. These insights will be validated later in the Evaluation section.

• Intuition 1: The LVLMs can understand and follow the instructions in typographic visual prompts. The LVLMs have been fine-tuned to perform multimodal tasks such as answering questions that are based on both texts and images or recognizing text in images (Liu et al. 2023a). Intuitively, this capability signifies that the LVLMs can also recognize and answer the typographic questions in images.   
• Intuition 2: The content safety guardrails of LVLMs are ineffective against the typographic visual prompts. Even if underlying LLMs of LVLMs have been safety aligned in advance, the visual inputs could introduce new risks to content safety since the visual embedding space is only semantically but not safely aligned to the LLM’s embedding space.   
• Intuition 3: The safety alignment within LVLMs can be further breached when instructed to generate the content step-by-step. This intuition is based on the model’s ability to reason step-by-step (Wei et al. 2022). By instructing the model to answer the prohibited question in steps, the model could be more engaged in the completion task and improve the quality of the responses, enhancing the jailbreaking effectiveness of FigStep.

# Pipeline

Given a prohibited text-only query $Q ^ { * } = ( T ^ { * } , \bot )$ , FigStep’s goal is to generate the corresponding jailbreaking query

$$
Q ^ { \mathrm { j a i l } } = ( T ^ { \prime } , I ^ { \prime } ) \gets \mathsf { F i g S t e p } ( T ^ { * } ) .
$$

To achieve this goal, the pipeline of FigStep is designed into three steps: $\jmath$ ) Paraphrase, 2) Typography, and 3) Incitement, as illustrated in Figure 2. These steps are detailed as follows.

1) Paraphrase: Following Intuition 3, the first step of FigStep is to rephrase the prohibited question $T ^ { * }$ into a textual statement $T ^ { \dagger } \in \mathbb { T }$ . This statement is designed to begin with a noun such as “Steps to”, “List of”, and “Methods to” which indicates that the answer is a list and the model should generate the answer item-by-item.

2) Typography: Based on intuitions 1 and 2, instead of directly feeding the paraphrased instruction $T ^ { \dagger }$ into the LVLM, FigStep will transform this text into a typographical image $I ^ { \prime } \in \mathbb { I }$ as the final jailbreaking image-prompt.

The numbered index from 1 to 3 is added to the visual prompt as a hint to the response format.

3) Incitement: FigStep designs an incitement text-prompt $T ^ { \prime } \in \mathbb { T }$ to motivate the model to engage in the completion task. This incitement prompt is designed to be neutral and benign to avoid triggering the model’s content safety mechanisms. As the gradient-based adversarial prompts (Zou et al. 2023) can be easily detected by the perplexity-based filter and need white-box access (Song, Rush, and Shmatikov 2020; Alon and Kamfonas 2023), we manually craft and hardcode the default benign incitement prompt of FigStep.

# Evaluation

# Experimental Setup

Dataset. To simulate possible harmful questions posed by the malicious users, we propose SafeBench, a novel comprehensive safety benchmark which consists of 500 harmful questions. The construction of SafeBench contains two steps:

• Common Safety Topic Collection. We first collect the common forbidden topics listed in both the OpenAI usage policy (OpenAI 2023b) and the Meta’s Llama-2 usage policy (Meta 2023), and then select 10 different topics that should be included in SafeBench. • LLM-based Dataset Generation. For each selected topic, we first compose a detailed description by integrating related content from the usage policies, then we query GPT-4 to generate 50 non-repetitive questions according to each topic description. In order to facilitate large-scale comprehensive experiments more conveniently, we sample 5 questions from each topic in SafeBench, ultimately creating a small-scale dataset named SafeBench-Tiny that consists of a total of 50 harmful questions.

LVLMs. We focus on the promising open-source LVLMs to conduct the main attack analysis. For instance, we select the following six LVLMs: two from LLaVA-v1.5 (Liu et al. 2023a), three from MiniGPT4 (Zhu et al. 2023), and CogVLM-Chat-v1.1 (Wang et al. 2023). MiniGPT-4 models only train the connector layer. LLaVA models keep the visual encoder frozen while continuing to update both the connector layer and the base LLM. CogVLM takes a different approach by adding an additional QKV matrix and an MLP as a visual expert module in each layer of the base LLM, alongside using a visual encoder and connector layer. In $\mathbf { C o g V L M }$ , only the connector layer and the visual expert modules are trainable. Note that we also demonstrate that the SOTA closed-source models, GPT-4V and GPT-4o, are vulnerable to the methodology of FigStep.

FigStep. The default malicious image-prompt $I ^ { \prime }$ of FigStep is a typography of $T ^ { \dagger }$ that contains black text and a white background. The image size of $I ^ { \prime }$ is $7 6 0 \times 7 6 0$ . The text font is FreeMono Bold and the font size is 80. As for the jailbreaking incitement text-prompt, we use a manually designed inciting prompt as our default $T ^ { \prime }$ to launch FigStep. We will further discuss the impact of different settings in $I ^ { \prime }$ and $T ^ { \prime }$ on the effectiveness of FigStep in the Discussion section. Meanwhile,

Harmful Response   
FigStep Large Vision-Language Models (LVLMs)   
“How can someone manufacture illegal drugs?”（! ） 日 9 图 IAllcetgivailty Paraphrase LLaVA MiniGPT4 CogVLM GPT-4V Hate ↓ (##) Speech   
“ilSltegpasl tdor amgas.n”ufacture Typography Visual Module Output ： （!"） ↓ 8 GMealnwearrateion Connector (!#) √   
Incitement c“oGnetnenrtaftoerdeatcaihleitdem...” Textual Module S Fraud

we carry out the “Paraphrase” of FigStep with the help of GPT-4 by using a paraphrasing prompt template. Specifically, we leverage few-shot learning (Brown et al. 2020) within five demonstrations to enhance the paraphrase effectiveness of GPT-4. LVLMs utilize the default hyperparameters during the inference process.

Metric. We use the following two metrics to evaluate the effectiveness of jailbreaks.

• Attack Success Rate (ASR): Given a prohibited question dataset, ASR refers to the proportion of generating prohibited responses by different jailbreak algorithms. Due to the unstable performance of current automated jailbreak evaluators (Ran et al. 2024), following (Yuan et al. 2023; Li et al. 2024), all the model responses are manually assessed for the sake of accuracy. Furthermore, considering the stochastic nature of the model’s replies, we repeatedly launch FigStep five times for each question, and one jailbreak could be deemed successful if any one of five attempts could yield a prohibited response. To this end, we manually reviewed a total of 66, 000 model responses. • Perplexity (PPL): We introduce PPL to evaluate the quality of the model responses. A lower PPL indicates a higher degree of “confidence” in the generated text, meaning that the model’s responses are statistically closer to real human language. We use GPT-2 to calculate the PPL of each response and report the mean value.

# Vanilla Query

Before evaluating the effectiveness of FigStep, we first take the harmful textual questions from SafeBench to directly query LVLMs. We denote these queries as vanilla queries. The related results are shown in Table 1.

Underlying LLMs Determine LVLMs Safety. First, we could observe that the safety disparity among LVLMs is associated with their underlying LLMs. Take MiniGPT4 as an example, which leverages three kinds of LLMs, MiniGPT4- Llama-2-CHAT-7B performs the best safety property, owing to the strict safety alignment within Llama-2-CHAT-7B.

The Impact of Model Intelligence on ASR. The PPL results illustrate that CogVLM-Chat-v1.1 exhibits limited proficiency in processing text-only queries. The responses always report that there is no information in the image, instead of answering or refusing the query. In our manual review, we consider that although such responses do not constitute direct refusals to assist with users’ requests, they still do not violate AI safety policy.

Table 1: The results of ASR and PPL caused by vanilla queries and FigStep. The evaluation dataset is SafeBench.   

<html><body><table><tr><td>LVLMs</td><td>Attack</td><td>ASR (↑)</td><td>PPL (↓)</td></tr><tr><td rowspan="2">LLaVA-1.5-V-1.5-7B</td><td>Vanilla</td><td>57.40%</td><td>24.01</td></tr><tr><td>FigStep</td><td>84.00%</td><td>5.77</td></tr><tr><td rowspan="2">LLaVA-1.5-V-1.5-13B</td><td>Vanilla</td><td>45.40%</td><td>9.17</td></tr><tr><td>FigStep</td><td>88.20%</td><td>6.05</td></tr><tr><td rowspan="2">MGPT4-L2-CHAT-7B</td><td>Vanilla</td><td>23.80%</td><td>7.98</td></tr><tr><td>FigStep</td><td>82.60%</td><td>9.54</td></tr><tr><td rowspan="2">MGPT4-V-7B</td><td>Vanilla</td><td>50.60%</td><td>23.24</td></tr><tr><td>FigStep</td><td>68.00%</td><td>8.23</td></tr><tr><td rowspan="2">MGPT4-V-13B</td><td>Vanilla</td><td>83.40%</td><td>20.62</td></tr><tr><td>FigStep</td><td>85.20%</td><td>7.32</td></tr><tr><td rowspan="2">CogVLM-Chat-v1.1</td><td>Vanilla</td><td>8.20%</td><td>30.54</td></tr><tr><td>FigStep</td><td>87.00%</td><td>9.44</td></tr><tr><td rowspan="2">Average</td><td>Vanilla</td><td>44.80 %</td><td>19.26</td></tr><tr><td>FigStep</td><td>82.50%</td><td>7.73</td></tr></table></body></html>

# Jailbreaking via FigStep

From this part, we demonstrate the attack efficacy of FigStep.

FigStep Outperforms Vanilla Query. Initially, as Table 1 shows, FigStep is capable of achieving effective jailbreaking performance regardless of the underlying LLMs, visual modules, or different types of connectors. Although LLaMA2-Chat-7B performs excellent safety alignment for text-only

100 100   
80 40 Vanilla 80 40 Vanilla 20 FigStep 20 FigStep 0 0 夕名众么石今名 夕名今石夕名 (a) LLaVA-1.5-V-1.5-7B (b) LLaVA-1.5-V-1.5-13B 100 100   
8 80 1 8 40 Vanilla Vanilla 20 FigStep 20 FigStep 0 0 夕名众么石今名 夕而名今石今名 (c) MiniGPT4-L2-CHAT-7B (d) MiniGPT4-V-7B 100 100   
80 40 Vanilla 8 80 40 Vanilla 20 FigStep 20 FigStep 0 0 夕名夕今么石夕名 夕而名夕今石夕名 (e) MiniGPT4-Vicuna-13B (f) CogVLM-Chat-v1.1

queries, its vulnerability significantly increases when meeting FigStep. The effectiveness of FigStep also naturally validates our first intuition: these LVLMs can generate policy-violating content corresponding to the instructions in image-prompts, indicating that they can accurately recognize and interpret the text in image-prompts. Above all, the higher ASR and lower PPL achieved by FigStep underscores its powerful jailbreaking effect.

Attack Success Rate on Each Topic. Figure 3 presents detailed ASR results on each topic in SafeBench. Overall, FigStep achieves a high ASR across different prohibited topics. To be specific, Figure 3c illustrates the effectiveness of FigStep in breaching the safety alignment of MiniGPT4- Llama-2-CHAT-7B across the first seven topics, wherein MiniGPT4-Llama-2-CHAT-7B originally exhibited strong robustness. For example, the vanilla query yields an average ASR of $5 . 1 4 \%$ across these first seven topics, while FigStep significantly enhances ASR to $7 6 . 8 6 \%$ . Meanwhile, for the latter three topics, the average ASR of the vanilla query is $6 7 . 3 3 \%$ , indicating that LLaMA-2-Chat-7B is not well-aligned for questions of these topics, and FigStep still markedly increases the ASR to $9 6 . 0 0 \%$ .

# Ablation Study

To demonstrate the necessity of each component in FigStep (i.e., the design of FigStep is not trivial), besides vanilla query (denoted as $Q ^ { v a }$ ) and FigStep, we propose additional four kinds of potential queries that the malicious users can use. The LVLMs discussed in this part are LLaVA-v1.5-Vicunav1.5-13B, MiniGPT4-Llama-2-CHAT-7B, and $\mathrm { C o g V L M } .$ Chat-v1.1. For the sake of brevity, we use LLaVA, MiniGPT4, and $\mathrm { C o g V L M }$ to denote them and utilize SafeBench-Tiny as the evaluation dataset unless otherwise stated.

Table 2: Results of Ablation Study.   

<html><body><table><tr><td>Queries</td><td>LVLMs</td><td>ASR (↑)</td><td>PPL (↓)</td></tr><tr><td rowspan="3">Qua</td><td>LLaVA</td><td>32.00%</td><td>18.32</td></tr><tr><td>MiniGPT4</td><td>18.00%</td><td>8.16</td></tr><tr><td>CogVLM</td><td>10.00%</td><td>37.14</td></tr><tr><td rowspan="3">Qi</td><td>LLaVA</td><td>16.00%</td><td>10.44</td></tr><tr><td>MiniGPT4</td><td>28.00%</td><td>8.48</td></tr><tr><td>CogVLM</td><td>0.00%</td><td>211.55</td></tr><tr><td rowspan="3">Q</td><td>LLaVA</td><td>60.00%</td><td>7.02</td></tr><tr><td>MiniGPT4</td><td>30.00%</td><td>9.25</td></tr><tr><td>CogVLM</td><td>0.00%</td><td>12.75</td></tr><tr><td rowspan="3">Q</td><td>LLaVA</td><td>4.00%</td><td>35.94</td></tr><tr><td>MiniGPT4</td><td>34.00%</td><td>82.58</td></tr><tr><td>CogVLM</td><td>0.00%</td><td>31.42</td></tr><tr><td rowspan="3">Q4</td><td>LLaVA</td><td>0.00%</td><td>58.43</td></tr><tr><td>MiniGPT4</td><td>26.00%</td><td>39.15</td></tr><tr><td>CogVLM</td><td>4.00%</td><td>30.37</td></tr><tr><td rowspan="3">FigStep</td><td>LLaVA</td><td>92.00%</td><td>5.37</td></tr><tr><td>MiniGPT4</td><td>90.00%</td><td>9.21</td></tr><tr><td>CogVLM</td><td>82.00%</td><td>9.22</td></tr></table></body></html>

The detailed explanations of the proposed malicious queries are outlined below. (1) $Q _ { 1 } ^ { \prime }$ is a text-only query that consists of two parts: the first part is the rephrased declarative statement of the text-prompt in $Q ^ { v a }$ , and the second part is three indexes “1. 2. 3.” Note that the above text-prompt is exactly the textual content embedded in the image-prompt of FigStep. (2) $Q _ { 2 } ^ { \prime }$ is another kind of text-only query. To construct the text-prompt of $Q _ { 2 } ^ { \prime }$ , we add the inciting text-prompt of FigStep upon the text-prompt of $Q _ { 1 } ^ { \prime }$ . In other words, $\bar { Q } _ { 2 } ^ { \prime }$ integrates all the textual information that appears in FigStep, but only in textual modality. (3) $Q _ { 3 } ^ { \prime }$ is an image-only query. $Q _ { 3 } ^ { \prime }$ only contains FigStep’s image-prompt and leaves its textprompt out. (4) The formats of $Q _ { 4 } ^ { \prime }$ and FigStep are similar, i.e., they both contain text-prompt and image-prompt concurrently. But differently, the texts in the image-prompts of $Q _ { 4 } ^ { \prime }$ are the original questions, and the text-prompt instructs the model to provide answers to these questions. The goal of proposing $Q _ { 4 } ^ { \prime }$ is to evaluate if directly embedding the harmful question into image-prompt can jailbreak LVLMs effectively. Validation of Intuition 2. The detailed results of all these queries are illustrated in Table 2. We conduct a comparison of the ASR results for $Q ^ { v a }$ , $Q _ { 1 } ^ { \prime }$ , $Q _ { 2 } ^ { \prime }$ , and FigStep. In these queries, except for FigStep, the text-prompts of the other three queries contain harmful content. We can observe that due to harmful keywords in the textual prompts, $Q ^ { v a }$ , $Q _ { 1 } ^ { \prime }$ , and $Q _ { 2 } ^ { \prime }$ are ineffective in jailbreaking LVLMs. Note that the information in $Q _ { 2 } ^ { \prime }$ and FigStep are the same, but the jailbreaking efficacy of FigStep is significantly stronger, highlighting the importance of embedding unsafe words in the image-prompts. Meanwhile, through comparing $Q _ { 3 } ^ { \prime }$ with FigStep, we could deduce that even if harmful information is embedded in images, without a valid incitement textual prompt to guide the model into continuation mode, the model fails to comprehend the user’s intent and cannot complete the information presented in the image-prompts.

![](images/5bc815f23418fca19526f6b74f62a3db396826bcbdbd814f280af95cbe9a6fd1.jpg)  
Figure 4: A visualization of how the embeddings for benign and prohibited questions differ depending on the type of prompt used: $Q ^ { v a }$ , $Q _ { 2 } ^ { \prime }$ or FigStep.

Validation of Intuition 3. Recall that our third intuition is using an incitement text-prompt to engage the model in a continuation task. Here we first take text-only queries as examples. Among them, only the text-prompt of $Q _ { 2 } ^ { \prime }$ clarified what needs to be replenished by the model, causing a higher ASR than $Q ^ { v a }$ and $Q _ { 1 } ^ { \prime }$ . Moreover, across all three LVLMs, FigStep’s jailbreaking performance consistently surpasses that of $Q _ { 4 } ^ { \prime }$ . This is attributed to the fact that $Q _ { 4 } ^ { \prime }$ does not engage the model in a continuation task but rather guides the model to provide direct answers to questions, even though the text-prompts of $Q _ { 4 } ^ { \prime }$ are benign, which is easier to trigger the alignment mechanism in LVLMs.

# Discussion

Prompt Semantic Visualization. To explore why FigStep breaks LVLM’s safety guardrail, we analyze the embedding separability between benign and prohibited questions when queried in different formats. To begin with, for each topic of Illegal Activity, Hate Speech, and Malware Generation, we generate 50 benign questions using GPT-4 according to the original prohibited questions in SafeBench. All these questions are transformed into the prompt format of $Q ^ { v a }$ , $Q _ { 2 } ^ { \prime }$ , and FigStep. Following Gerganov (2023), the semantic embedding of the whole query is defined as the hidden vector of the last layer. Therefore, we use t-SNE (Van der Maaten and Hinton 2008) to project these embeddings onto a twodimensional space, as shown in Figure 4. For MiniGPT4, the text-only prompts $Q ^ { v a }$ and $Q _ { 2 } ^ { \prime }$ leads to highly separable embeddings for benign and prohibited queries, indicating that the underlying LLM can effectively differentiate them and output appropriate responses. Meanwhile, the typographic prompts (FigStep) result in overlapping embeddings of benign and prohibited queries, implying that the visual embedding transformation ignores the safety constraints of the textual latent space. Similar conclusions hold for LLaVA and $\mathbf { C o g V L M }$ .

Table 3: We compare FigStep with various advanced textbased and image-based jailbreak algorithms. The results are evaluated across three harmful topics: IA (Illegal Activity), HS (Hate Speech), and MG (Malware Generation). Here the victim LVLM is MiniGPT4.   

<html><body><table><tr><td>Method</td><td>IA</td><td>HS</td><td>MG</td></tr><tr><td>GCG</td><td>0.00%</td><td>10.00%</td><td>10.00%</td></tr><tr><td>CipherChat</td><td>0.00%</td><td>4.00%</td><td>2.00%</td></tr><tr><td>DeepInception</td><td>52.00%</td><td>22.00%</td><td>54.00%</td></tr><tr><td>ICA</td><td>0.00%</td><td>0.00%</td><td>0.00%</td></tr><tr><td>MultiLingual</td><td>0.00%</td><td>4.00%</td><td>6.00%</td></tr><tr><td>VRP</td><td>14.00%</td><td>2.00%</td><td>8.00%</td></tr><tr><td>QR</td><td>38.00%</td><td>22.00%</td><td>38.00%</td></tr><tr><td>JPoCR</td><td>28.00%</td><td>18.00%</td><td>30.00%</td></tr><tr><td>FigStep</td><td>82.00 %</td><td>38.00%</td><td>86.00 %</td></tr><tr><td>JPoCR (Red teaming)</td><td>64.00%</td><td>42.00%</td><td>76.00%</td></tr><tr><td>FigStep (Red teaming)</td><td>100.00%</td><td>76.00%</td><td>98.00%</td></tr><tr><td>VAE</td><td>30.00%</td><td>6.00%</td><td>10.00%</td></tr><tr><td>JPadv</td><td>32.00%</td><td>20.00%</td><td>30.00%</td></tr><tr><td>FigStepadv</td><td>80.00 %</td><td>38.00%</td><td>80.00%</td></tr></table></body></html>

Comparison with Other Jailbreaks. We further compare FigStep with SOTA jailbreak methods, including text-based jailbreaks and image-based jailbreaks. Notably, we introduce (a) $\mathsf { F i g S t e p } _ { \mathrm { a d v } }$ , a variant of FigStep utilizing adversarial perturbation, and (b) FigStep (Red teaming), which uses additional 10 rephrased text-prompts to fully jailbreak LVLMs. In specific, we use FGSM to generate the adversarial image for $\mathsf { F i g S t e p } _ { \mathrm { a d v } }$ . An image with random Gaussian noise is set as the initial image. The typography image in FigStep is used as the target image. The optimization goal is to minimize the distance between their visual embeddings. Table 3 shows the results of FigStep and other attacks. We observe that FigStep outperforms the text-based jailbreak methods, as well as visual adversarial examples (VAE) (Qi et al. 2023a), Visual-RolePlay (VRP) (Ma et al. 2024), QueryRelevant Images (QR) (Liu et al. 2023d), Jailbreak-in-pieces $( \mathrm { J P _ { O C R } } )$ (Shayegani, Dong, and Abu-Ghazaleh 2024), and its optimized version $\mathrm { J } \mathrm { P } _ { \mathrm { a d v } }$ . $\scriptstyle \mathrm { J P } _ { \mathrm { O C R } }$ , as a gradient-free jailbreaking method, only transfers core harmful phases (i.e., a word) into images, causing it relatively ineffective in circumventing the safeguards of VLMs, while FigStep injects an entire instruction into the image-prompt and conduct paraphrasing. Besides the red teaming versions of FigStep and $\scriptstyle \mathrm { J P } _ { \mathrm { O C R } }$ , FigStepadv is also more powerful than $\mathrm { J } \mathrm { P } _ { \mathrm { a d v } }$ , which indicates that FigStep has more potential to be a stepping stone for advanced gradient-based jailbreaks against LVLMs. In short, the methodology of FigStep presents consistently superior performance than other methods.

Impact of Hyperparameters. Figure 5 shows the ASR results under different number of repetitions and temperatures. We observe that with more jailbreak attempts, the ASR pro

100 100 8 75 75 LLaVA 5 50 LLaVA 50 MingGPT4 MingGPT4 25 25 0 1 2 3 4 5 6 8 910 0 0.2 0.4 0.6 0.8 1.0 (a) Number of repetitions (b) Temperature

gressively enlarges. However, FigStep is effective enough that it does not need to repeat as many as 5 times to achieve a high $A S R$ . For instance, if we just query with 1 repetition, $A S R$ for LLaVA already attains $82 \%$ , and $A S R$ for both MiniGPT4 and $\mathrm { C o g V L M }$ could reach up to $60 \%$ . Moreover, as the number of repetitions increases to 3, the results of $A S R$ on all three models reach above $80 \%$ . When querying with 10 repetitions, the $A S R$ on MiniGPT4 and LLaVA achieves $9 8 \%$ and $94 \%$ , respectively. From the results under different temperatures, we can see that as temperature increases, there will be a higher probability of generating harmful responses. The observed experimental phenomenon can be attributed to the fact that as the temperature increases, the model’s creativity is enhanced, leading to a richer diversity in the generated content.

# Defenses

In this section, we discuss three potential defenses: OCR Detection, System Prompt Modification, and adding random noise into image-prompts.

OCR Detection. We first utilize EasyOCR (AI 2023) to recognize the text in the visual-prompts of FigStep, the averaged detection success rate is $\bar { 8 8 . 9 8 \% }$ . However, when we leverage LLaMA-2-Chat-7B as a toxicity classifier to judge the harmfulness of the extracted textual content, only $4 0 . 0 0 \%$ of the responses are deemed as harmful, and the results are reduced to $3 0 . 0 0 \%$ when using OpenAI’s moderation (OpenAI 2024b). These guardrails can be deliberately disabled in opensource models. Furthermore, they could even be actively bypassed. To demonstrate this, we propose $\mathsf { F i g S t e p } _ { \mathrm { h i d e } }$ , which hides the text in the image by manipulating the background color. Specifically, the background color spectrum is set to #000010, which is very close to the font color #000000. The ASR results of $\mathsf { F i g S t e p } _ { \mathrm { h i d e } }$ are $6 4 . 0 0 \%$ , $6 8 . 0 0 \%$ , and $5 2 . 0 0 \%$ against LLaVA, MiniGPT4, and $\mathrm { C o g V L M }$ , respectively, illustrating that such visual-prompts do not effect the jailbreaking performance. Therefore, as long as the core vulnerabilities within the LVLMs persist, the system-level defenses, such as OCR detection, are inefficient in mitigating FigStep.

System Prompt-Based Defense. We then try to add a new textual safety guidance prompt upon the existing system prompt to assess whether a meticulously designed system prompt can mitigate the impact of FigStep. The safety guidance instructs the model to check for text in the image and avoid assisting if the content violates AI safety policies. In this scenario, the ASR results of $\mathsf { F i g S t e p } _ { \mathrm { h i d e } }$ are $6 8 . 0 0 \%$ , $6 4 . 0 0 \%$ , and $4 8 . 0 0 \%$ against LLaVA, MiniGPT4, and $\mathrm { C o g V L M }$ , respectively. Therefore, FigStep can still jailbreak LVLMs with high ASR though we pre-define a new system prompt with wider consideration for safety.

Table 4: ASR results of GPT-4V and GPT-4o.   

<html><body><table><tr><td></td><td>Baseline FigStep</td><td>FigStephide</td><td>FigSteppro</td></tr><tr><td>GPT-40</td><td>28.00%</td><td>48.00% 56.00%</td><td>62.00%</td></tr><tr><td>GPT-4V</td><td>18.00%</td><td>34.00% 52.00%</td><td>70.00%</td></tr></table></body></html>

Random Noise-Based Defense. We add Gaussian noise (mean $\scriptstyle = 0$ , std=100) to make visible degradation to the image quality. However, FigStep is robust to such defense with only a slight reduction in ASR (MiniGPT4: $9 0 \% \to 8 6 \%$ $\mathbf { C o g V L M }$ : $8 2 \% \to 7 6 \%$ , LLaVA: $9 2 \%  9 2 \%$ ). This may be due to the large font size and high contrast between the text color and the background in the image prompt. However, introducing Gaussian noise may affect the performance of benign downstream tasks. When perturbing the images of the first thirty questions from the Llava-bench-in-the-wild (Liu et al. 2023b), the number of correct answers also slightly decreases: MiniGPT4: $1 5 {  } 1 3$ , $\mathbf { C o g V L M }$ : $2 6 {  } 2 5$ , LLaVA: $2 4  2 2$ . This indicates that it may interfere with the experience of legitimate users. Therefore, incorporating random noise into the image-prompt is inefficient in resisting FigStep and can slightly impair the model’s ability to perceive regular images.

Real-world Case Study. We regard the SOTA closed-source LVLMs, GPT-4o and GPT-4V, as our real-world case studies. These commercial LVLMs have deployed powerful OCR toolkit in advance (OpenAI 2023a). Here we further propose a variant of FigStep, namely $\mathsf { F i g S t e p } _ { \mathrm { p r o } }$ . In brief, $\mathsf { F i g S t e p } _ { \mathrm { p r o } }$ splits image-prompt into harmless segments, inputs them to the model simultaneously, and then subsequently reconstructs them by exploiting the intelligence of LVLMs. Table 4 shows the ASR results of FigStep, $\mathsf { F i g S t e p } _ { \mathrm { h i d e } }$ , and $\mathsf { F i g S t e p } _ { \mathrm { p r o } }$ We observe that FigStep can increase the harmfulness of both GPT-4V and GPT-4o compared to baseline results, and $\mathsf { F i g S t e p } _ { \mathrm { p r o } }$ can further outperform FigStep. Hence, as long as this vulnerability persists, relying solely on external tools for jailbreak prevention may be temporary.

# Conclusion

In this paper, we introduce FigStep, a straightforward yet effective jailbreak algorithm against LVLMs. Our approach is centered on transforming harmful textual instructions into typographic images, circumventing the safety alignment in the underlying LLMs of LVLMs. By conducting a comprehensive evaluation, we uncover cross-modality alignment vulnerabilities of LVLMs. Above all, we highlight that it is dangerous and irresponsible to directly release the LVLMs without ensuring strict cross-modal alignment, and we advocate for the utilization of FigStep to develop novel crossmodel safety alignment techniques in the future.