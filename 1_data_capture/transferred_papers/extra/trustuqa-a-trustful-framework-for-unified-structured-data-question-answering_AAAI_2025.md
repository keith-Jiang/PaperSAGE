# TrustUQA: A Trustful Framework for Unified Structured Data Question Answering Wen Zhang1,4, Long $\mathbf { J i n } ^ { 1 }$ , Yushan $\mathbf { Z } \mathbf { h } \mathbf { u } ^ { 1 }$ , Jiaoyan Chen2, Zhiwei Huang1, Junjie Wang1, Yin Hua1 , Lei Liang3, Huajun Chen1,4,5\*

1Zhejiang University 2University of Manchester 3Ant Group 4ZJU-Ant Group Joint Lab of Knowledge Graph 5Zhejiang Key Laboratory of Big Data Intelligent Computing {zhang.wen, longjin, yushanzhu, huajunsir} $@$ zju.edu.cn

# Abstract

Natural language question answering (QA) over structured data sources such as tables and knowledge graphs have been widely investigated, especially with Large Language Models (LLMs) in recent years. The main solutions include question to formal query parsing and retrieval-based answer generation. However, current methods of the former often suffer from weak generalization, failing to dealing with multi-types of sources, while the later is limited in trustfulness. In this paper, we propose TrustUQA, a trustful QA framework that can simultaneously support multiple types of structured data in a unified way. To this end, it adopts an LLM-friendly and unified knowledge representation method called Condition Graph (CG), and uses an LLM and demonstration-based twolevel method for CG querying. For enhancement, it is also equipped with dynamic demonstration retrieval. We have evaluated TrustUQA with 5 benchmarks covering 3 types of structured data. It outperforms 2 existing unified structured data QA methods. In comparison with the baselines that are specific to one data type, it achieves state-of-the-art on 2 of the datasets. Further more, we have demonstrated the potential of our method for more general QA tasks, QA over mixed structured data and QA across structured data.

# Code — https://github.com/zjukg/TrustUQA Extended version — https://arxiv.org/abs/2406.18916

# 1 Introduction

Question answering (QA) seeking answers for a natural language question from structured data has attracted increasing attention in the past decade (2023a; 2023), leading to a few directions including QA over tables (2023), QA over knowledge graph (i.e., KGQA) (2018), QA over temporal KG (2023b) and so on. A straightforward and widely studied solution is to parse questions into formal queries that can be executed on data storage and reasoning engines (a.k.a. NL2Query). According to the data source types, these works are divided into NL2SQL (2020) for relational databases, NL2SPARQL (2020) for KGs, etc. However, current NL2Query methods are specifically developed for one query language corresponding to one data type.This significantly limits their generality and usage in real-world scenarios, especially when it is unknown in which data resource the answer lies, or the answer relies on multiple data sources.

With the development of LLMs and Retrieval-augmented Generation (RAG), another solution, which first retrieves relevant evidences of the question from data sources and then generates the answer, has become more and more popular (2023a; 2024). Using different retrieval techniques for different structured data, this solution can lead to more general methods. For example, StructGPT(2023a) iteratively retrieves evidences and feeds them into an LLM for answer generation, supporting KG, table and relational database. However, this solution suffers from some trustfulness issues: (i) the generated answer may be inconsistent with the original structured data due to the hallucination of LLMs (2023b), the insistence of LLMs’ parametric knowledge (2023) and the irrelevant evidences that are retrieved; (ii) a list of evidences are exposed to the LLM, potentially leaking private data if a third-party LLM is used; (iii) the evidences can provide some plausible citations to the answer but it is hard to provide high quality explanations. In contrast, NL2Query can mostly avoid the issue of $( i )$ and $( i i )$ as only some meta data and prototypes need to be exposed to the LLM to generate the query instead of the answer, and the issue of (iii) since the query and its reasoning procedure are both accessible, providing logically rigorous explanation to the answer. Therefore, in this work, we adopt NL2Query, and propose a trustful and unified framework named TrustUQA supporting different types of structured data simultaneously.

There are two desiderata for TrustUQA. (1) It is expected to represent diverse structured data. Thus we propose Condition Graph (CG) and corresponding techniques for translating tables, KGs and temporal KGs into a CG. (2) It can support effective querying over the data representation. Thus we propose a method called Lwo-layer Function-based CG Query, which firstly uses LLMs to write basic queries (i.e., LLM queries) based on the question, and then uses predefined rules to transform these LLM queries to queries that can be executed on the CG (i.e., execution queries) for the eventual answer. Functions of the LLM query are designed with simple vocabularies that are more understandable by LLMs, and thus this method can get higher accuracy using no fine-tuning but few-shot prompting. We also propose a dynamic demonstration retrieving method to further improve the prompt quality for higher accuracy.

To evaluate the effectiveness of TrustUQA, we experiment it on 5 benchmarks covering 3 kinds of structured data, including WikiSQL (2017) and WTQ (2015) for table QA, WebQSP (2016) and MetaQA (2018) for KG, and CronQuestion (2021) for temporal KG. We compare its results with existing RAG-based unified QA methods (2023a; 2024). Results show that TrustUQA outperforms these RAG-based methods on WikiSQL and WebQSP. It also achieves state-of-the-art on WebQSP and CronQuestions compared to the models that support only one type of structured data.

Besides the comprehensive model analysis, we further demonstrate the generality of TrustUQA by questions whose answers potentially rely on different data sources — tables, KG and temporal KG. It includes two tasks: QA over mixed structured data where each answer relies on one of the given data sources but which source it relies on is not specified, and $Q A$ across structured data where answering the question relies on data from more than one sources. These tasks are close to real-world scenarios, but have not been explored.

In summary, our contributions lie in three aspects: (a) We propose a trustful framework TrustUQA for unified QA over multiple kinds of structured data. (b) We conduct comprehensives experiments over 5 benchmarks covering 3 types of structured data, proving the effectiveness of TrustUQA. (c) We demonstrate the generality of TrustUQA on two practical tasks that have never been explored before — QA over mixed structured data and QA across structured data.

# 2 Related Works

NL2Query. For table QA, the query language is SQL and the task is also known as NL2SQL. NL2SQL methods include schema-based approaches that build the query based on the schema of the database and the data indices (2016; 2011), parsing-based approaches that parse questions through grammatical structures (2017; 2017), and neural-machine translation-based approaches that model NL2SQL as a language translation problem (2019; 2020). Methods based on pre-trained language models (PLMs) fine-tune PLMs to generate SQL queries (2022; 2023a), complete the SQL query sketch (2023), address the sub-queries (2023). We refer (2023) for a comprehensive survey on NL2SQL.

For KGQA, the formal query language is SPARQL, thus the task is also called NL2SPARQL. The methods usually include three steps: question understanding, linking and filtering (2023). In question understanding, some works (2018; 2021) use natural language parsers such as dependency parsing while the others (2023; 2021) use sequence to sequence generation to get graph patterns. The linking step maps the mentions in questions to the entities and relations in the KG, where mapping dictionary (2012), indexing system (2019; 2018), embedding generation system (2020) are usually applied. In filtering step, the type constraint is applied to filter answers. LLMs are also applied for this task, in which generate-then-retrieve paradigm are usually applied that firstly generate the logical form and then bind the mentions to elements in KGs (2023b; 2023).

These NL2Query methods are developed towards one specific query language or even for one specific dataset, lacking generality for supporting different types of structured data.

NL2Answer. NL2Answer methods directly generate the answer, skipping the formal query. For example, KGQA can be modeled as multi-hop reasoning (2018). Some works (2023c; 2018) trains a text encoder to encode the question and a reasoning module to perform multi-hop reasoning. The others (2023b) use a unified model for text encoding and reasoning. Since these models need to be trained, they are often only applicable to some specific datasets after training. To address this problem, more general pre-training solutions that work across datasets are proposed. For example, TableGPT (2023) is a unified framework that enables LLMs to understand and operate on tables following natural languages. It pre-trains a general table encoder encoding tables into embeddings, and prompt-tuning the LLM with table embedding as input over a vast corpus. These methods are applicable to multiple datasets but cannot support different types of structured data.

Unified QA. With LLMs, some unified QA frameworks that can support different types of structured data have been proposed. StructGPT (2023a) is an iterative reading-thenreasoning framework. In reading, it collects evidence from the structured data through specialized functions for different kinds of structured data. In reasoning, it uses LLMs to generate the answer or the next reasoning step based on the collected evidence. Readi (2024) is a reasoning-path-editing framework. Given a question, it first generates a reasoning path, and edits the path according to the feedback from instantiating the path over the structured data. It collects the KG evidence based on the edited reasoning paths and uses LLMs to generate the answer based on the evidence and the question. Though these two methods could be applied to different types of structured data, they are not unified enough, because the functions in StructGPT and the reasoning paths in Readi are specific to data types. More importantly, they generate the answer based on evidence retrieval, and thus are not as trustful as NL2Query methods, as we discussed in Introduction.

# 3 Methodology

TrustUQA uses Condition Graph (CG) for general and expressive data representation. It can represent simple relationships, complex temporal facts, and rules. For composing execution queries over CG, we design execution query function e.g., search_node() and search_condition(). As directly writing execution queries by LLMs is challenging due to their complexity and novelty, we propose a querying method called Two-layer Function-based CG Query. In the first layer, it uses LLMs to write simplified functions like get_inf ormation(), whose vocabularies (head entity, tail entity, relation, key and value) are general and more understandable to LLMs, and applies such LLM functions for composing LLM queries. In the second layer, it translates each LLM query into an execution query according to predefined rules. To augment LLMs for writing LLM queries, we apply few-shot prompting with a few demonstrations of question and LLM query pairs, and

Query Executor ( = )\*+(,)\*), ""#) LLMs .++, = 112(3, 43) Execution Query ,)\*) ↑ Condition Graph ↑ Question-specific data !'( Prompt 02 Condition Graph .Q-u.e-ry=T/ra/n(s.la00to1r) Init Prompt Translator Dynamic Demon ""# = %\$%("&) LLM Query ,++, -stration Retriever Structured Data !! Question 5 Condition Graph Two-layer CG Query Dynamic Demonstration Representation Functions Retriever

propose a dynamic demonstration retriever, since the optimal demonstration varies with each question.

Briefly, TrustUQA consists of 3 main modules, as shown in Figure 1: (1) Condition Graph Translator $( T _ { c g } )$ transforming input structured data into a CG, (2) Query Translator $( T _ { q } )$ translating LLM queries to execution queries over the CG, (3) Dynamic Demonstration Retriever $( R )$ selecting the most similar examples from the training data as demonstrations for writing LLM queries.

# 3.1 Condition Graph Translator $T _ { c g }$

Given structured data $\mathcal { D } _ { s }$ , ${ T _ { c g } }$ translates it into a condition graph which is formally defined below.

Definition 1. Condition Graph is a labeled directed graph represented as $\mathcal { C G } = \{ \mathcal { N } , \mathcal { T } \}$ . $\mathcal { N }$ is the set of nodes in CG. Each node is combined with a string to represent the semantic meaning, which could be entities such as Earth, relationships such as has friends, properties such as time, or numerical values such as 2024. $\mathcal { T } =$ $\{ ( n o d e _ { 1 } , n o d e _ { 2 }$ , condition) $| n o d e _ { 1 } \in \mathcal { N } , n o d e _ { 2 } \in \mathcal { N } \}$ is a collection of condition triples representing an edge that $n o d e _ { 1 }$ is connected to node2 under the condition, where condit $i o n = [ n o d e _ { a } , n o d e _ { b } , . . . ]$ is a list of nodes in N . The condition can be empty $( I J )$ indicating that node $_ 1$ is connected to node2 without any conditions.

Condition graph is expressive. The triple in the CG can represent (1) simple relationships such as (Born In, Ulm, [Einstein]), (2) complex facts such as (date, 14 March 1879, [Einstein, Born In, Ulm]), (3) common rules such as (Person, Has Parents, []), meaning that each person has parents.

Condition graph is a unified representation. Next, we introduce how to translate three typical structured data — tables, KGs, and temporal KGs, into a CG.

s.whicA ctaobnltea as $\mathcal { D } _ { s }$ wis $T a b l e = \{ \bar { R } , C , V \}$ $m$ $R \bar { = } \{ r o w _ { i } \} _ { i = 1 } ^ { m }$ and $n$ columns $C = \{ c o l _ { j } \} _ { j = 1 } ^ { n }$ . The header $r o w _ { 1 }$ records the semantic meaning of each column. $V = \{ v _ { i , j } | i \in [ 1 , m ] , j \in$ $[ 1 , n ] \}$ represents the cell values of the table. For example, a table with name | born in city | Time as the header and Einstein | Ulm | 14 March 1879 as the record. During transformation, the translator $T _ { c g }$ first inserts a column $c _ { 0 }$ in front of the first column that $C : = \{ c _ { 0 } , c o l _ { 1 } , . . . , c o l _ { n } \} .$ . The values of the inserted column cells are strings indicating the order of the corresponding row, i.e. for the ith row, the inserted value is $[ l i n e \_ i ]$ . For example, after insertion, the previous example table becomes [line_1]| name | born in city | Time as the header and [line_ $_ { - 2 l }$ | Einstein | Ulm | 14 March 1879 as the record. After inserting, starting from the second row and the second column, the translator will generate two condition triples for each cell. For example, for $v _ { 2 , 2 } =$ Einstein in the example table, there will be ( $[ l i n e \_ { 2 } ]$ , name, []) represents the entity [line_2] has name, and (name, Einstein, $[ l i n e \_ { 2 } ] ;$ represents the name of entity $[ l i n e \_ { 2 } ]$ is Einstein. Formally, after translation, $\mathcal { D } _ { c g } ^ { t a b l e } \overset { \cdot } { = } \left\{ \mathcal { N } _ { t a b l e } , \mathcal { T } _ { t a b l e } \right\}$ where $\mathcal { N } _ { t a b l e } = V \cup \{ l i n e \_ 1 , l i n e \_ 2 , . . . , l i n e \_ m \}$ , $\mathcal { T } _ { t a b l e } =$ $\{ ( [ l i n e \_ i ] , v _ { 1 , j } , \dotsc ] ) , ( v _ { 1 , j } , v _ { i , j } , [ l i n e \_ i ] ) | i \in [ 2 , m ] , j \quad \notin \{ 1 , \ldots , m \} \} .$ $[ 2 , n + 1 ] \}$ .

Translating Knowledge Graph. A KG as $\mathcal { D } _ { s }$ is represented as $\mathcal { K G } = \{ \mathcal { E } , \mathcal { R } , \mathcal { F } \}$ , where $\varepsilon , \mathcal { R }$ and $\mathcal { F }$ are the set of entities, relations and facts. $\mathcal { F } = \{ ( h , r , t ) | \{ h , t \} \in \mathcal { E } , r \in$ $\mathcal { R } \}$ is a set of triples representing the relations between entities. Translator $T _ { c g }$ transforms a $\kappa g$ by generating two condition triples for each knowledge graph fact. For example, given a triple (Einstein, born in city, Ulm), one condition triple (Einstein, born in, []) denotes Einstein has the property born in, another condition triple (born In, Ulm, [Einstein]) means if it is for Einstein, the value of born in is Ulm. Thus after translation, $\mathcal { D } _ { c g } ^ { k g } = \{ \mathcal { N } _ { k g } , \mathcal { T } _ { k g } \}$ , where $\mathcal { N } _ { k g } = \mathcal { E } \cup \mathcal { R }$ , and $\mathcal { T } _ { k g } = \{ ( h , r , \mathbb { I } ) , \bar { ( } r , t , [ h ] ) | ( h , r , t ) \in \mathcal { F } \}$ .

Translating Temporal Knowledge Graph. A temporal KG as the $\mathcal { D } _ { s }$ is represented as $\mathcal { T } \kappa \mathcal { G } \mathrm { ~ \bar { ~ } { ~ = ~ } ~ } \{ \mathcal { E } , \mathcal { R } , \bar { \tau } , \mathcal { Q } \}$ , where $\varepsilon , \mathcal { R }$ and $\tau$ are the entity, relation, and time sets, and $\mathcal { Q } = \{ ( h , r , t , \tau _ { s } , \tau _ { e } ) \}$ is the quintuple set where $h , t \in \mathcal { E }$ , $\textit { r } \in \textit { \textbf { R } }$ , and $\tau _ { s } , \tau _ { e } ~ \in ~ \tau$ denote the start time and end time of the fact $( h , r , t )$ , respectively. After translation, $\mathcal { T } _ { t k g }$ collects all condition triples translated from each TKG triple. Specifically, for each $( h , r , t , \tau _ { s } , \tau _ { e } )$ in $\mathcal { Q }$ , following condition triples are generated $( h , r , [ ] )$ , $( \dot { r } , t , [ h ] )$ , (start time, $\tau _ { s } , [ h , r , t ] )$ , (end time, $\tau _ { e }$ , $[ h , r , t ] )$ , $\{ ( \pm \mathrm { i } \mathrm { m e } , \tau ^ { \prime } , [ h , r , t ] ) | \tau ^ { \prime } \in \{ \tau _ { s } , \tau _ { s } + 1 , \tau _ { s } + 2 , . . . , \tau _ { e } \} \}$ , where start time, end time and time are three built-in terms introduced during translation and $\tau ^ { \prime }$ are the integer time stamps between the start time and the end time. Thus after translation, $\mathcal { D } _ { c g } ^ { t k g } = \{ \mathcal { N } _ { t k g } , \mathcal { T } _ { t k g } \}$ . $\mathcal { N } _ { t k g } =$ $\mathcal { E } \cup \mathcal { R } \cup \tau \cup \left\{ \begin{array} { r l } \end{array} \right.$ start time, end time, $\mathsf { t i m e } \} \cup \{ \tau ^ { \prime } \}$ .

# 3.2 Query Translator $T _ { Q }$

The query translator $T _ { Q }$ translates the LLM query $\mathcal { Q } _ { l l m }$ into the execution query $\mathcal { Q } _ { e x e }$ that can be executed over the CG.

LLM Query $\mathcal { Q } _ { l l m }$ . The LLM query is generated by an LLM corresponding to the question. LLMs know the common vocabularies of head entity, relation, tail entity from KGs, as well as key, value used in tables and property graphs well. Thus we design the following searching function to search information from the graph:

$$
\begin{array} { r } { g e t \_ i n f o r m a t i o n ( h e a d \_ e n t i t y , r e l a t i o n , } \\ { t a i l \_ e n t i t y , k e y , v a l u e ) } \end{array}
$$

Table 1: Rules for translating LLM query functions to execution query functions. $g i , h , r , t , k ,$ $\boldsymbol { v }$ denote get_information, head_entity, relation, tail_entity, key, and value, sn, sc, $n 1$ , $n 2$ , $c$ , and $s$ denote search_node, searh_condition, $n o d e _ { 1 }$ , $n o d e _ { 2 }$ , condition and scope. For the parameters $t$ and $v$ of get_information, op is one of $\bar { \{ > , < , = , \geq , \bar { \leq } \} }$ and is $\stackrel { 6 6 } { = } \stackrel { , }$ in the table as example.   

<html><body><table><tr><td>LLM Query Function</td><td>Execution Query Function</td></tr><tr><td>gi(h=H)</td><td>sn(nl=H)</td></tr><tr><td>gi(r = R)</td><td>sn(nl=R)</td></tr><tr><td>gi(k=K)</td><td>sn(n1=K)</td></tr><tr><td>gi(h=H,r=R)</td><td>sn(n1 =R,c=H</td></tr><tr><td>gi(h=H,k=K)</td><td>sn(n1=K,c=H)</td></tr><tr><td>gi(r=R,t"="T)</td><td>sc(n1=R,n2=T,op=“=")</td></tr><tr><td>gi(k= K,v"="V)</td><td>sc(n1=K,n2=V,op=“=")</td></tr><tr><td>gi(r=R,t“="T,k=K,v“="V)</td><td>output_of_queryi = sc(n1 = R,n2= T,op="=" output_of_query2 = sc(nl = K,n2= V,op ="=")</td></tr><tr><td>gi(r = R,t"="T,k = K)</td><td>set_interaction(set1 = output_of_query1,set2 = output_of_query2) output_of_queryi= sc(n1=R,n2=T,op="=") sn(nl = K,s=output_of_query1)</td></tr><tr><td>gi(r = R,k =K,v"="V)</td><td>output_of_query1 = sc(n1 = K,n2= V,op ="=") sn(n1= R,s = output_of_queryi)</td></tr></table></body></html>

where the variables head_entity, relation, tail_entity, key and value are set to None by default. This function is able to represent complex queries. For example, get_information(head_entity=None, relation=Won, tail_entity=Nobel Prize, key=Year, value $> 2 0 0 0$ ) represents "Who are the Nobel Prize Winners after 2000?" get_information(head_entity=Einstein, relation $\ c =$ Won, tail_entity=Nobel Prize, key=Year, value=None) corresponds to "In which year did Einstein won the Nobel Prize?"

Apart from get_information(), we also design a set of reasoning functions as follows:

• Set operations: set_intersection $( s e t _ { 1 } , s e t _ { 2 } )$ , set_union $( s e t _ { 1 } , s e t _ { 2 } )$ , set_difference $( s e t _ { 1 }$ , set2), set_negation $( s e t _ { 1 } , s e t _ { 2 } )$ , and $k e e p ( s e t , v a l u e ) ^ { 1 }$ • Simple calculations: m $e a n ( ) , m a x ( ) , m i n ( ) , c o u n t ( ) .$ .

With these functions, multiple (nested) searching and reasoning steps can be expressed by LLM queries. We give examples of these reasoning functions in Appendix.

Execution Query $\mathcal { Q } _ { e x e }$ . The execution query is executed over the CG, and is composed of a set of execution query functions which include searching functions and reasoning functions. The reasoning functions are the same as the LLM reasoning functions. The search functions are used to obtain information from the CG following constrains:

• search_node $( n o d e _ { 1 }$ , condition, scope): return node2 of the condition triple $( n o d e _ { 1 } , ? , [ c o n d i t i o n ] )$ ) from the condition triple set denoted as scope. condition $\approx$ None means condition list is empty. scope=all by default means including all the condition triples in $\mathcal { D } _ { c g }$ . For example search_node( $n o d e 1 { = } \mathrm { B o r n }$ In, condition= Einstein, scope=all) means getting the born-in information of Einstein based on all condition triples.

• search_condition $( n o d e _ { 1 }$ , node2_value, op): return condition in the condition triple $( n o d e _ { 1 } , n o d e _ { 2 } , ? )$ such that $n o d e _ { 2 }$ satisfies the operation op with respect to node2_value. For example, if $o p$ is $\mathbf { \partial } ^ { \bullet } > ^ { \bullet }$ , the function return condition where $n o d e _ { 2 } ~ > ~ n o d e _ { 2 } { \bf { \sigma } } _ { - } v a l u e$ . Specifically, search_condition $\scriptstyle \overbrace { n o d e _ { 1 } = \mathrm { B o r n } }$ , $n o d e _ { 2 } { \it - v a l u e } { = } 2 0 2 0$ , $o p { = } ^ { \prime } > ^ { \prime } )$ means searching entities borning after 2020. To determine if the op constrain meets, compar $\hat { \mathbf { \Omega } } ( o p , v a l _ { 1 } , v a l _ { 2 } )$ will be called.

• compar $\cdot ( o p , v a l _ { 1 } , v a l _ { 2 } )$ : return ture if $v a l _ { 1 } \ \{ o p \} \ v a l _ { 2 }$ , otherwise return $f a l s e$ , where $o p$ is one of the comparison symbols $\{ > , < , = , \geq , \leq \}$ . For example, compare $\bar { ( o p = ^ { \prime } > ^ { \prime } }$ , $v a l _ { 1 } { = } 2 0 2 4$ , $v a l _ { 2 } { = } 2 0 2 0$ ) will return true.

Given a CG $\mathcal { D } _ { c g }$ and complex execution queries based on these functions $\mathcal { Q } _ { e x e }$ , the query executor $E x e$ can automatically execute $\mathcal { Q } _ { e x e }$ over $\mathcal { D } _ { c g }$ to get the answer.

Translating $\mathcal { Q } _ { l l m }$ to $\mathcal { Q } _ { e x e }$ . There are semantic mapping and syntax mapping steps for $T _ { Q }$ to translate $\mathcal { Q } _ { l l m }$ to $\mathcal { Q } _ { e x e }$ (1) Semantic mapping: mapping the variable values in $\mathcal { Q } _ { l l m }$ to the nodes in the $\mathcal { D } _ { c g }$ . To achieve this, we use a dense text encoder $E$ , such as SentenceBert (Reimers and Gurevych 2019), to encode the nodes and variable values into vectors and map each value to the most similar node. We replace the value by the string of the mapped node in the function, resulting $\mathcal { Q } _ { l l m } ^ { \prime }$ . (2) Syntax mapping: translating $\mathcal { Q } _ { l l m } ^ { \prime }$ to $\mathcal { Q } _ { e x e }$ following a fixed set of rules. We summarize the translation rules in Table 1. There is a chance that the generated LLM queries are composed of undefined LLM query functions. For example, LLM might generate compare(A’s score, $B$ ’s score) for question “For A and $B$ , who has a larger score?”. For such LLM query, the translator $T _ { Q }$ keeps the query as it is during translation. And during execution, the LLM function is called to use LLM to get the output. Specifically, we include the function name and parameters as input to LLM to generate output. The LLM function prompt is shown in Appendix.

# 3.3 Dynamic Demonstration Retriever $R$

Given a question $q$ , we use an LLM to generate the LLM query, denoted as $\mathcal { Q } _ { l l m } = L L M ( q , p _ { q } )$ where $p _ { q }$ is another input acting as the question-specific prompt. $p _ { q }$ includes a few question-query examples, called demonstrations. The initial demonstrations are manually crafted with a focus on representativeness and diversity, shown in Appendix.

However, the optimal demonstration for different questions varies. For example, for question “What is the most common language in Norway?”, the LLM query of the question “What is the major language spoken in Canada?” is more informative than “Where the queen of Denmark lives?”. Thus we propose a dynamic demonstration retriever to retrieve $k$ most similar questions of $q$ from the training dataset $D _ { t r a i n } { = } \{ ( q _ { t r a i n } , a _ { t r a i n } ) \}$ as the demonstrations. Specifically, given a question $q$ , we use a text encoder $E$ to encode $q$ and training question $q _ { t r a i n }$ into vectors. Then we calculate the similarity of the question vectors and select the $m$ most similar training questions (denoted as $s$ ) where $m > k$ . Then we iteratively generate the LLM query $\scriptstyle { \mathcal { Q } } _ { l l m } ^ { q _ { i } } = L L M ( q _ { i } , p _ { q _ { i } } )$ from the most to the least similar $q _ { i } \in S$ . If the result from $E x e ( T _ { Q } ( \mathcal { Q } _ { l l m } ^ { q _ { i } } ) , \mathcal { D } _ { c g } )$ exactly matches to the labeled answer $a _ { t r a i n }$ , we regard $( q _ { i } , \mathcal { Q } _ { l l m } ^ { q _ { i } } )$ as a question-query pair demonstration. We repeat this step until $k$ demonstrations are collected. If $k$ demonstrations are not collected, we supplement the remaining ones by the initial demonstrations.

# 4 Experiments

We adopt 5 datasets covering 3 data types: WikiSQL (2017) and WTQ (2015) for table, WebQuestionsSP(WebQSP) (2016) and MetaQA (2018) for KG, and CronQuestions (2021) for temporal KG. Their statistics and demonstrations are shown in the Appendix.We use GPT-3.5 (gpt-3.5-turbo0613) as the LLM with self-consistency strategy of 5 times, and SentenceBERT (2019) as the dense text encoder. If the answer is “None” due to mismatched entity-relation pairs and key-value inconsistencies etc., we implement the retry mechanism with 3 times trials. We set the number of retrieves $m = 1 5$ and the number of demonstrations $k = 8$ .

4.1 Table QA Experiment   
Table 2: Denotation accuracy of Table QA.   

<html><body><table><tr><td colspan="3">Methods WikiSQL WTQ</td></tr><tr><td colspan="3">Data Type Specific Models</td></tr><tr><td>MAPO (2018) TAPAS (2020) UnifiedSKG (2022) TAPEX (2022)</td><td>72.6 83.6 86.0 89.5</td><td>43.8 48.8 49.3 57.5 65.9</td></tr><tr><td>DATER(2023) Unified Models (with GPT3.5 as the LLM)</td><td>=</td><td></td></tr><tr><td>StructGPT (2023a) Readi (2024) TrustUQA(ours)</td><td>65.6 66.2 85.9</td><td>52.2 66.7 44.2</td></tr></table></body></html>

Experiment Setting We adopt denotation accuracy (2023a) to assess whether the predicted answer matches the labeled answer based on set-level equivalence. We write 8 initial demonstrations for both WTQ and WikiSQL. We add the table’s column names and one randomly selected record under each column in a linearized format behind the question.

Result Analysis As shown in Table 2, on WikiSQL, TrustUQA surpasses MAPO and TAPAS and approaches UnifiedSKG with an accuracy of $8 5 . 9 \%$ . Compared to unified models, it achieves a nearly $20 \%$ improvement.

On WTQ, TrustUQA achieves an accuracy of $4 4 . 2 \%$ , surpassing MAPO. However, there remains a gap compared to the other methods. We analyze the cases and find three reasons. Firstly, some questions have more than one correct answer, like “name a player that had more than 5 league goals but no other goals", but only one is included in the labeled answer. Secondly, some tables are presented in a none standard format, such as the table for “how many countries won a gold medal" whose last row is the total statistics of the previous rows. Thirdly, there are limitations of TrustUQA during element mapping. For the question “What are the number of times a race was held in August?" the term “August" could refer to either “August 1st" or “August 4th", but the translator only retrieves one instance.

Table 3: $\operatorname { H i t } @ 1$ results of KG QA. Digits with ∗ are set comparison accuracy.   

<html><body><table><tr><td>Methods</td><td colspan="3">MetaQA 1 hop 2 hop 3 hop</td></tr><tr><td colspan="3">Data Type Specific Models</td></tr><tr><td>KV-Mem (2016) GraftNet (2018) EmbedKGQA(2020) NSM(2021)</td><td>96.2 82.7 97.0 94.8 97.5 98.8 97.1 99.9 99.0</td><td>48.9 77.7 94.8 98.9 99.1 75.1</td><td>46.7 66.4 66.6 68.7</td></tr><tr><td>UniKGQA (2023c) 97.5 Davinvi-003 (2022) 52.1</td></tr><tr><td>KB-BINDER (2023b) 93.5 KB-BINDER-R 92.9</td><td>25.3 42.5 99.6 96.4 99.9 99.5</td><td>48.3</td></tr><tr><td colspan="3">Unified Models (with GPT3.5 as the LLM)</td></tr><tr><td>StructGPT (2023a) Readi (2024) TrustUQA(ours)</td><td>97.1 97.3 98.4 99.9 97.1 97.9 99.94* 99.99* 99.99*</td><td>87.0 69.6 99.4 74.3 98.4 83.5</td></tr></table></body></html>

# 4.2 Knowledge Graph QA

Experiment Setting For MetaQA, we construct 13, 15, and 11 demonstrations for 1-hop, 2-hop, and 3-hop, respectively. Include description of CG data, prompt also lists relations.

For WebQSP, we used a processed version of the official WebQSP Dataset.For each question, a relevant subset of the KG (a set of triples) is retrieved from Freebase. Some relations and entities in WebQSP represented by IDs without entity names lack clear semantic meaning. Thus we divide the questions in WebQSP into two types, questions regarding semantic-clear and semantic-unclear relations which can be found in Appendix. The initial demonstrations include topic entity, first step relations (relations have topic entity) and second step relations for WebQSP.

Table 5: Ablation Study. The result of MetaQA is the average of 1,2,3 hops.   

<html><body><table><tr><td>Methods</td><td>All</td><td>Question Type|Answer Type Com</td><td>Sim</td><td>Ent</td><td>Tim</td></tr><tr><td colspan="6">Data Type Specific Models</td></tr><tr><td>BERT (2019)</td><td>24.3</td><td>23.9</td><td>24.9</td><td>27.7</td><td>17.9</td></tr><tr><td>RoBERTa (2019)</td><td>22.5</td><td>21.7</td><td>23.7</td><td>25.1</td><td>17.7</td></tr><tr><td>EmbedKGQA(2020)</td><td>28.8</td><td>28.6</td><td>29.0</td><td>41.1</td><td>05.7</td></tr><tr><td>EaE (2020)</td><td>28.8</td><td>25.7</td><td>32.9</td><td>31.8</td><td>23.1</td></tr><tr><td>CronKGQA(2021)</td><td>64.7</td><td>39.2</td><td>98.7</td><td>69.9</td><td>54.9</td></tr><tr><td>TempoQR-S(2022)</td><td>79.9</td><td>65.5</td><td>99.0</td><td>87.6</td><td>65.3</td></tr><tr><td>TempoQR-H(2022)</td><td>91.8</td><td>86.4</td><td>99.0</td><td>92.6</td><td>90.3</td></tr><tr><td>TSQA (2022)</td><td>83.1</td><td>71.3</td><td>98.7</td><td>82.9</td><td>83.6</td></tr><tr><td>TMA (2023a)</td><td>78.4</td><td>63.2</td><td>98.7</td><td>79.2</td><td>74.3</td></tr><tr><td>CTRN(2023)</td><td>92.0 96.9</td><td>86.9 94.5</td><td>99.0 99.2</td><td>92.1 96.2</td><td>91.7 96.6</td></tr><tr><td colspan="6">LGQA (2023b)</td></tr><tr><td>Unified Models (with GPT3.5 as the LLM) TrustUQA(ours)</td><td>97.2</td><td>95.4</td><td>99.5</td><td>96.1</td><td>99.1</td></tr></table></body></html>

Table 4: Hits $\ @ 1$ of temporal KG QA on CronQuestion.

# Result Analysis We report the $\mathrm { H i t } @ 1$ results in Table 3.

For MetaQA, TrustUQA performs competitively in comparison with existing unified methods. During our experiments, we discover name ambiguity issues in MetaQA. For instance, two movies might share the same name, making it unclear which one is referenced in a question. Consequently, the gold answer might only represent a subset of the true answers. Based on this analysis, higher Hit $@ 1$ results may not indicate better QA performance on MetaQA, especially when the $\operatorname { H i t } @ 1$ results exceed $9 7 \%$ , due to the incompleteness of labeled answers. Therefore, we report the accuracy of set comparison between predicted and labeled answers (last row in Table 3), where predicted answers are considered correct if they include all labeled answers. TrustUQA achieves over $9 9 . 9 \%$ accuracy across all three types of questions. Figure 2(a) shows the frequency of numbers of answers predicted by TrustUQA and answers labeled. It shows the two frequency distributions are nearly identical, and thus we can conclude the TrustUQA performs perfectly on MetaQA and this is not due to giving a large number of answers.

For WebQSP, TrustUQA achieves a $\operatorname { H i t } @ 1$ score of $8 3 . 5 \%$ , surpassing all the baselines and achieves the state-of-the-art.

# 4.3 Temporal Knowledge Graph QA

Experiment Setting We use Hits $\ @ 1$ for evaluation.We design 9 demonstrations, each including the question, relation, and annotation. Entities that are not originally presented in the annotation but exist in the entity list are appended at the end. We use the question and answer type of each question as metadata during retrieving.

Result Analysis As Table 4 shows, TrustUQA is the only unified model for this task and achieves the state-of-the-art.

Since TrustUQA and LGQA could achieve high performance on CronQuestion, more challenging QA datasets about temporal KG are expected to be created.

# 4.4 Model Analysis

Ablation Study As shown in Table 5, -dynamic means static initial demonstration and -two-layer means making LLM directly generate the execution query, the performance of TrustUQA decrease in majority of the datasets.

<html><body><table><tr><td></td><td>Wiki.</td><td>WTQ</td><td>MetaQA</td><td>Web.</td><td>CronQ.</td></tr><tr><td>TrustUQA</td><td>85.9</td><td>44.2</td><td>99.97*</td><td>83.5</td><td>97.2</td></tr><tr><td>-dynamic</td><td>84.5</td><td>43.7</td><td>99.87*</td><td>75.1</td><td>96.1</td></tr><tr><td>-two-layer</td><td>84.4</td><td>44.2</td><td>99.87*</td><td>83.4</td><td>87.6</td></tr></table></body></html>

Using Different LLMs and Text Encoders We have tested GPT-4 and other three text encoders on two Table QA datasets. The results can be seen in Table 6. The results show that TrustUQA performs slightly differently with various text encoders, and consistently achieves better performance with the more advanced LLM GPT-4 compared to GPT-3.5.

Table 6: TrustUQA with different LLMs (GPT-3.5/GPT-4) and text encoders.   

<html><body><table><tr><td></td><td>SentB.</td><td>DPR</td><td>ANCE</td><td>M3E</td></tr><tr><td></td><td></td><td></td><td>WikiSQL|85.9/87.4 86.0/87.4 85.5/87.4 86.1/87.7</td><td></td></tr><tr><td>WTQ</td><td></td><td></td><td>44.2/52.144.5/51.445.3/51.2</td><td>44.5/50.8</td></tr></table></body></html>

Efficiency Analysis We evaluate the time cost of each step of our method on the MetaQA dataset with 100 random samples2. Steps include (1) dynamic demo retrieval, (2) generating $\mathcal { Q } _ { l l m }$ , (3) translating $\mathcal { Q } _ { l l m }$ to $\mathcal { Q } _ { e x e }$ and (4) execute the query. Results are in Figure 2(e). We can see that most of the time are cost for $\mathcal { Q } _ { l l m }$ generation. The dynamic demonstration retrieval, query translation and execution are quite efficient which takes less than $20 \%$ of the whole time.

Hyperparameter Analysis In Figure 2 (b)-(d), we show that more demonstrations and more re-try leads to slightly better results. And generating answers in the self-consistency strategy by 5 times achieves the best results. Considering the tradeoff between time/computation cost and the performance, we set a moderate number for demonstrations and retry, which are 8 and 3 respectively.

Error Analysis We recognized three types of errors.(1) The most frequent type is query generation error, such as logical error where the generated query is logically wrong, unclear

10123 1hop lab. 90 gpt-3.5 gpt-4 gpt-3.5 gpt-4 gpt-3.5 gpt-4 24 (1) dreytnriaemvincg 1hop pre. e. 85 (2) generating 23hop  lparb. 80 (34) terxaencsluatitiong   
10 3hop pre. 75 0 2 4 8 10 1 3 5 1 3 5 7   
(a) Number of Answer (b) Demostration Size (c) Number of retries (d) Number of SC (e) Time cost

Table 7: The calling rate and results of the LLM function.   

<html><body><table><tr><td></td><td>Wiki.</td><td>WTQ MetaQA</td><td>Web.</td><td>CronQ.</td></tr><tr><td>Calling Rate</td><td>4.0% 30.7%</td><td><1%</td><td><1%</td><td>2.0%</td></tr><tr><td>Results</td><td>38.9% 11.4%</td><td>0.0%</td><td>2.2%</td><td>20.4%</td></tr></table></body></html>

logic in query with steps either missing or unnecessary, and grammar error in query. (2) The second type is mapping error including errors in relation and entity mapping during the parameter-value mapping step. (3) The third type is LLM function error. When calling LLM function, though enough data are provided, LLM still generates wrong answers.

Calling Rate and Result of LLM function. Results are shown in Table 7. We observe the following: (1) Except for WTQ, the usage rates under each dataset are generally low, with most being less than $1 \%$ . (2) The accuracy is low. Compared to the experimental results on each dataset, the results of the LLM function are lower. Thus calling the execution functions we design in TrustUQA is more trustful.

Table 8: Results of QA over mixed structured data.   

<html><body><table><tr><td></td><td>Wiki. WTQ MetaQA Web. CronQ.</td></tr><tr><td>Dmix + Sunknown</td><td>56.0 8.0 93.0 37.0 51%</td><td>72.0</td></tr><tr><td>dynamic Acc</td><td>96% 42% 100%</td><td>85%</td></tr><tr><td>Dunmix + Sunknown</td><td>79.0 25.0 97.0</td><td>46.0 84.0</td></tr><tr><td>Dunmix + Sknown</td><td>85.0 42.0 99.0</td><td>84.0 95.0</td></tr></table></body></html>

Question: When does a director nominated for the 11th Korea Musical Awards work win the Chlotrudis Award for Best Actor? Answer: 2002 query1 $\mathbf { \tau } = \mathbf { \tau }$ gi(relation $\scriptstyle { \frac { - 1 } { . } }$ Award', tail_entity $\mathbf { \Psi } = \mathbf { \Psi } ^ { \dagger } ]$ 11th Korea Musical Awards') query2 $\ O =$ gi(relation $\scriptstyle 1 = 1$ Nominated work', head_entit $\scriptstyle \mathbf { \Gamma } = \mathbf { \Gamma }$ output_of_query1') query3 $\ O =$ gi(relation $\scriptstyle 1 = 1$ directed_by', head_entity='output_of_query2') rqeuleartiyo4 $\mathbf { \Sigma } = \mathbf { \Sigma }$ gwii(nhneeard',_teanilti_teyn=t'itCyh=l'otruutpdiust_Aofw_aqrudefroyr3',Beksety=A'tcitomre',') !!"! query1, query2 in Table $$ query3 in KG query4 in TKG

# 4.5 Potentials of TrustUQA

QA over Mixed Structured Data In this task, each question relies on one of the mixed multiple data sources but which data source it relies on is not specified, which we call mixed data $( D _ { m i x e d } ) +$ answer source unknown $( S _ { u n k n o w n } )$ setting. In contrast, the setting of the evaluation tasks in Section $4 . 1 \textrm { - } 4 . 3$ is unmixed data $( D _ { u n m i x } ) +$ answer source known $( S _ { k n o w n } )$ . To simulate this scenario, we randomly extract 100 data from each of WikiSQL, WTQ, MetaQA-1hop, WebQSP, and CronQuestion, and experiment on these 500 test questions. During experiments, for $D _ { \mathrm { m i x } }$ setting, we translate data from 5 sources into CG form and store them together, and we store them independently for $D _ { \mathrm { u n m i x } }$ setting. For $S _ { \mathrm { u n k n o w n } }$ setting, we use a unified prompt with the demonstration’s format only including the question, and for $S _ { \mathrm { k n o w n } }$ setting, we use the same prompt for each data source as before.

Results are shown in Table 8, in which we report the results using the same evaluation metric on questions from each dataset. The results in the first-row show that TrustUQA could answer questions for different data sources and thus has the potential for QA over mixed structured data. But compared to the original results, i.e. the $D _ { \mathrm { u n m i x } } + D _ { \mathrm { k n o w n } }$ setting, performance drop occurs because the number of candidates for node-value mapping is significantly larger. Comparing the last two rows, we observe a significant performance drop.

This is because the results are positively correlated with the dynamic demonstration retrieval accuracy, whether the dynamic demonstrations are retrieved from the correct data source, as shown in the second row in Table 8. We refer the reader to Appendixfor more details of this experiment.

QA across Structured Data In this task, questions can be answered relying on more than one data source. We construct a case from 3 types of structure datasets: WikiSQL, MetaQA and CronQuestion. This case is shown in Figure 3. For question When does a director nominated for the 11st Korea Musical Awards work win the Chlotrudis Award for Best Actor? $Q _ { e x e }$ first gets Hedwig and the Angry Inch from WikiSQL, then gets John Cameron Mitchell from MetaQA, and finally gets the correct answer 2002 from the CronQuestion. This case shows the potential of TrustUQA for QA across different types of structured data. We refer the reader to Appendixfor more details of this experiment.

# 5 Discussion and Conclusion

We introduce a trustful framework for unified structured data QA, called TrustUQA, which based on Condition Graph and two-layer query. We experimentally prove its effectiveness and the potential of dealing with more challenging scenarios.