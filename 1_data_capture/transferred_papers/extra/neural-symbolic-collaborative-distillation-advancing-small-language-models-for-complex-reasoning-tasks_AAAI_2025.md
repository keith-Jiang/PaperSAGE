# Neural-Symbolic Collaborative Distillation: Advancing Small Language Models for Complex Reasoning Tasks

Huanxuan Liao1,2, Shizhu $\mathbf { H e } ^ { 1 , 2 * }$ , Yao ${ \bf X } { \bf u } ^ { 1 , 2 }$ , Yuanzhe Zhang4, Kang Liu1,2,3, Jun Zhao1,2

1 The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China 2 School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China 3 Shanghai Artificial Intelligence Laboratory, Shanghai, China 4 National Science Library, Chinese Academy of Sciences, Beijing, China liaohuanxuan $2 0 2 3 @$ ia.ac.cn {yao.xu, shizhu.he, kliu, jzhao}@nlpr.ia.ac.cn

# Abstract

In this paper, we propose Neural-Symbolic Collaborative Distillation (NesyCD), a novel knowledge distillation method for learning the complex reasoning abilities of Large Language Models (LLMs, e.g., ${ > } 1 3 \mathbf { B }$ ). We argue that complex reasoning tasks are difficult for Small Language Models (SLMs, e.g., $\leq 7 \mathrm { B }$ ), as these tasks demand not only general cognitive abilities but also specialized knowledge, which is often sparse and difficult for these neural-based SLMs to effectively capture. Therefore, NesyCD distills the general capabilities and specialized knowledge in LLMs in different ways. On the one hand, we distill only general abilities from teacher LLMs into the student SLMs of parameterized neural networks. On the other hand, for the specialized abilities and uncommon knowledge of a complex reasoning task, we employ a symbolic knowledge distillation approach to obtain and store the specialized knowledge within a symbolic knowledge base (KB). By decoupling general and specialized capabilities, the proposed NesyCD can achieve superior performance cost-effectively, utilizing smaller models and blending parameterized neural networks with symbolic KB. Moreover, the specialized KB generalizes well and is comprehended and manipulated by humans. Our experiments show that NesyCD significantly boosts SLMs’ complex reasoning performance on in-domain (BBH, GSM8K) and outof-domain (AGIEval, ARC) datasets. Notably, our approach enabled the LLaMA3-8B and Qwen2-7B to surpass GPT-3.5- turbo in performance and come close to matching LLaMA3- 70B, despite the latter having $9 \times$ more parameters.

Code — https://github.com/Xnhyacinth/NesyCD Extended version — https://arxiv.org/abs/2409.13203

# 1 Introduction

Large Language Models (LLMs) (Yang et al. 2024) excel in various complex reasoning tasks such as mathematical (Yu et al. 2023a), commonsense (Zhao, Lee, and Hsu 2023) and symbolic reasoning (Suzgun et al. 2022) with In-Context Learning (ICL) (Ye et al. 2023) and Chain-ofThought (CoT) Prompting (Kaplan et al. 2020). Due to the high computational costs and expensive API calls required for LLMs (e.g., ChatGPT and LLaMA3-70B), enhancing Small Language Models (SLMs, e.g., $\leq 7 \mathrm { B }$ ) to handle complex reasoning efficiently is more practical and crucial for large-scale deployment.

To meet the practical needs mentioned above, many research efforts (Magister et al. 2023; Hsieh et al. 2023; Li et al. 2024b) in recent years have proposed the transfer of reasoning capabilities from teacher LLMs to student SLMs through CoT distillation (shown in the middle of Figure 1). Specifically, LLMs generate high-quality rationales, which are then utilized to fine-tune SLMs. This CoT distillation enhances the performance of SLMs in many tasks that require complex reasoning abilities such as arithmetic (Cobbe et al. 2021) and symbolic reasoning (Wei et al. 2022).

Despite some progress in CoT distillation, significant challenges persist that limit the performance of SLMs in complex reasoning tasks: 1) Inconsistency in Capabilities between Teacher and Student Models: Existing methods often ignore the gap between the knowledge modeling and complex reasoning capabilities of LLMs and SLMs. Due to fewer parameters, student models struggle to acquire the comprehensive knowledge necessary for complex reasoning tasks (Kang et al. 2023). As illustrated in Figure 1, an SLM trained with traditional CoT distillation fails to solve hard questions that the teacher model can handle effectively. 2) Difficulty in Modeling Sparse Specialized Knowledge: Neural knowledge distillation faces challenges in representing sparse and specialized knowledge due to the limited parameter space of distilled SLMs. In complex reasoning tasks, particularly with hard questions, specialized knowledge may contradict general knowledge (e.g., velocity superposition in relative motion). This sparse and specialized knowledge is hard to model in small-scale models, significantly impacting the SLM’s performance on challenging questions and deteriorating its ability to handle unseen tasks.

In this paper, we argue that complex reasoning tasks require both general knowledge (e.g., numerical addition) and specialized knowledge (e.g., relative displacement). It can be simply understood as follows: the former refers to what SLMs can effectively model and is primarily used for answering high-frequency, easy questions, while the latter in

Two old ladies spent the afternoon walking through Park. The first Mary and Jimmy start running from the same spot, but in opposite Easy four miles, what is the combined total distance the two ladies walked? lady walked twice as far as the second lady. If the second lady walked miles per hour. What is the distance between them after 1 hour? directions. Mary runs at 5 miles per hour and Jimmy runs at 4 Hard Question Question Previous CoT Distillation ！ Let's denote the distance walked by the second lady as D_2=4 miles 1. Determine the individual distances covered by each in 1 hour The first lady walked twice as far as the second lady. Thus, the … 2. The total distance between them after 1 hour is the sum of the … The first lady walked $2 ^ { * } 4 { = } { < } { < } 2 ^ { * } 4 { = } 8 { > } { > } 8$ miles. j 1 hour is 60 minutes. Mary moves 5 miles/hou in 1 hour. Jimmy moves 4 miles/hour\*60 minutes=240 miles in 1 hour. $= 8 0 0$ minutes=300 miles In total, the two ladies walked $4 + 8 = < < 4 + 8 = 1 2 > > 1 2 \ .$ miles. The distance between them is 300 miles-240 miles=60miles. Correct Cases Error Cases Neural-Symbolic General Task Model Collaborative Let me analyze the error and provide the learning Uncertain Answer Certain Answer Distillation summary and  supplementary knowledge!   
1 hour is 60 minutes. In 1 hour, Mary Mary moves… Will have traveled… ！ LearningSpumecmiaray:lized Knowledge Base   
1. Determine the Direction: Based on the question, decide Δ = 74.2 Δ = 92.8 whether to subtract distances for the same direction or Retrieve? add them for opposite directions. Yes Supplementary Knowledge: Specialized Δ confidence Neural Model Symbolic KB 1. Speed and Distance Relationships: Know the formula KRneotrwilevdagle Distillation Distillation tohbejescutsmmofovtihneigrisnpoeepdpsosmitueltdiiprleicet $\mathbf { \Psi } = \mathbf { \Psi }$ nys,ththee $\times$ tea.l d…istance is Answer uncertain certain

volves aspects that SLMs find challenging to model and are essential for addressing low-frequency, hard questions. For instance, as shown in Figure 1, answering the right hard question demands domain-specific knowledge and advanced reasoning skills, like applying physics formulas to calculate displacement and understanding relative displacement.

To tackle the aforementioned challenges in existing CoT distillation methods, we propose a novel Neural-Symbolic Collaborative Distillation (NesyCD) which transfers the general reasoning capabilities and common knowledge of LLMs to SLMs. Unlike using only neural-based models to build students before, we use a symbolic knowledge base (KB) to model and store relatively sparse specialized knowledge. Firstly, we gather correct and error cases made by SLMs fine-tuned with CoT distillation. Next, LLMs analyze error cases, extracting specialized knowledge through elaborate prompts and storing them in a symbolic KB. Finally, we fine-tune the SLMs with specialized knowledge augmented distillation, enhancing SLMs’ abilities for hard questions. Moreover, to enhance the student SLMs’ robustness against potentially noisy retrieved knowledge, we incorporate novel auxiliary tasks like augmented distillation (AD), answer prediction (AP) and direct CoT (DC) for multi-task learning to effectively utilize specialized knowledge.

To validate the effectiveness of NesyCD, we empirically demonstrate that it significantly improves the baseline performance of several open-source SLMs, such as TinyL

LaMA (Zhang et al. 2024a) and LLaMA2-7B (Touvron et al. 2023), across various benchmarks including GSM8k (Cobbe et al. 2021) for mathematical reasoning, BBH (Suzgun et al. 2022) and AGIEval (Zhong et al. 2023) for general reasoning, and ARC (Clark et al. 2018) for factual knowledge. Additionally, our extensive analysis shows that NesyCD is efficient regarding training data and model size. Specifically, the NesyCD-enhanced 1.1B TinyLLaMA outperforms the finetuned LLaMA2-7B and achieves superior results using only a quarter of the full training data compared to other strong baselines. Our findings and contributions are as follows:

• We propose a neural-symbolic collaborative distillation method, which, to our knowledge, is the first approach to leverage a co-distillation framework that integrates neural-based models with symbolic knowledge bases for learning the complex reasoning capabilities of LLMs. • We distinguish complex reasoning into general and specialized abilities through SLMs’ error analysis. General abilities are modeled by a neural network, while specialized abilities are captured by a symbolic KB. Integrating these components enhances SLMs’ complex reasoning, leading to more efficient models and reduced costs. • The experimental results demonstrate that the proposed NesyCD significantly enhances the performance of SLMs across wide benchmarks for knowledge, mathematical, symbolic and other complex reasoning tasks both in-domain and out-of-domain.

# 2 Related Work

# 2.1 CoT Distillation from LLMs

The Chain-of-Thought (CoT) reasoning ability of LLMs, characterized by step-by-step question solving, is known as an emergent ability to improve performance in various reasoning tasks. Recent works (Ho, Schmid, and Yun 2022; Fu et al. 2023) endeavor to transfer the CoT reasoning capabilities of LLMs to SLMs. Std-CoT (Magister et al. 2023) involves fine-tuning SLMs directly using CoTs extracted from teacher LLMs. Subsequent studies (Hsieh et al. 2023; Li et al. 2024b) have proposed treating the learning of rationales and answers as separate optimization objectives. CasCoD (Dai et al. 2024) takes a different approach by decomposing the traditional single-step learning process into two cascaded steps. However, the performance of these methods is hindered by the limited knowledge and capabilities of SLMs with fewer parameters (Ho, Schmid, and Yun 2022; Kang et al. 2023). This deficiency is particularly detrimental in complex reasoning tasks that require specialized knowledge and sophisticated reasoning skills. To address this issue, we propose to enhance SLMs by integrating knowledge retrieved from the specialized knowledge base (KB) generated by teacher LLMs.

# 2.2 Knowledge-Augmented LMs

Knowledge-augmented LMs (KALMs) enhance their reasoning by utilizing external KBs. A common approach involves retrieving relevant passages from sources like Wikipedia based on questions (Chen et al. 2017). KARD (Kang et al. 2023) applies KALMs to knowledge-intensive tasks and finds it crucial for accurate answers and factual rationales. However, challenges like chunk indexing and independent encoding of documents can hinder KALMs’ effectiveness in using external KB. To address this, we propose harnessing the world knowledge and reasoning capabilities of LLMs to generate specialized knowledge for KALMs, including learning summaries and supplementary knowledge to boost complex reasoning abilities.

# 2.3 Learning from Errors

Humans learn from their errors to avoid repeating them, and this capability has inspired efforts to enhance LLMs (Li et al. 2023; Wang et al. 2024). LLM2LLM (Lee et al. 2024) employs an instructor model to help target models learn from their errors. TRAN (Tong et al. 2024) uses a rule-based system to prevent past errors, while LEAP (Zhang et al. 2024b) extracts and integrates principles from LLMs’ errors into prompts. However, these approaches have not been adapted to improve SLMs, and the principles used in reasoning remain static rather than dynamically tailored based on the model’s capabilities.

# 3 Methods

We propose Neural-Symbolic Collaborative Distillation (NesyCD), which consists of four learning processes (illustrated in Figure 2): 1) General Distillation (§3.1), where a large language model (LLM) serves as a teacher model (General Teaching) $\mathcal { T } _ { G }$ to generate rationales. Subsequently, a small language model (SLM) is fine-tuned to generate these rationales and provide answers to given questions, resulting in the Student Model (Primary Learning) $ { \boldsymbol { S } } _ { P }$ ; 2) Demonstration Collection (§3.2), where we evaluate the performance of the $ { \boldsymbol { S } } _ { P }$ on the specific task and dataset, and collect the correct and error cases for the following steps; 3) Symbolic Knowledge Distillation (§3.3), where the previous teacher model, acting as the Targeted Teaching model $\mathcal { T } _ { T }$ , analyzes and generates specialized knowledge aimed at the errors made by $ { \boldsymbol { S } } _ { P }$ . This knowledge assists $ { \boldsymbol { S } } _ { P }$ in addressing similar tasks in the future and is then stored in a specialized knowledge base (KB). 4) Symbolic KB Augmented Neural Distillation $( \ S 3 . 4 )$ , where the student model (Enhanced Learning) $\scriptstyle { \mathcal { S } } _ { E }$ is initialized with the fine-tuned $ { \boldsymbol { S } } _ { P }$ . Using multi-task learning, fine-tune $\scriptstyle { \mathcal { S } } _ { E }$ to generate rationales and answers based on both questions and retrieved specialized knowledge, as well as on the questions alone.

![](images/63d38fdf7765013c888e5105e57ced73a67f620a1c8c3ed6a13ccec2ae1368fe.jpg)  
Figure 2: Overview of NesyCD. 1) General Distillation (§3.1): Fine-tune the student $ { \boldsymbol { S } } _ { P }$ to generate rationales obtained from the teacher $\mathcal { T } _ { G }$ and answers. 2) Demonstration Collection (§3.2): Evaluate $ { \boldsymbol { S } } _ { P }$ and collect correct and error cases addressed by $ { \boldsymbol { S } } _ { P }$ . 3) Symbolic Knowledge Distillation (§3.3): The teacher $\mathcal { T } _ { T }$ analyzes errors and generate specialized KB. 4) Symbolic KB Augmented Neural Distillation (§3.4): Use multi-task learning to fine-tune $\scriptstyle { \mathcal { S } } _ { E }$ , enabling it to effectively utilize retrieved specialized knowledge.

# 3.1 General Distillation

Rationale Generation with LLMs: The ability to generate high-quality rationales is known as the emergent ability of LLMs (Ho, Schmid, and Yun 2022). Our objective is to transfer this capability to SLMs through CoT distillation. First, we employ CoT prompts (Wei et al. 2022) to guide the $\mathcal { T } _ { G }$ in generating CoT. We generate rationales for each training data point $\mathcal { D } _ { \mathrm { t r a i n } } = \{ ( \boldsymbol { q } _ { i } , \boldsymbol { a } _ { i } ) \} _ { i = 1 } ^ { n }$ , where $\pmb q _ { i }$ is a question and $\mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ } \mathrm { ~ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ } \mathrm { ~ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ } \mathrm { ~ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ \mathrm { ~ } \ } \mathrm { ~ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ \mathrm { ~ } \ } \mathrm { ~ \ \ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ \mathrm { ~ \alpha ~ } \mathrm { ~ } \mathrm { ~ } \bf { ~ \beta } } \mathrm  ~ \mathrm { ~ \alpha ~ } _ { \mathbf { \beta } } \mathrm { ~ \alpha _ { \beta } \mathrm } \mathrm { ~ \mathrm { ~ \alpha } ~ \mathrm { ~ } \mathrm } \mathrm { ~ } \mathrm \mathrm  ~ \bf { ~ \alpha ~ } \alpha _ { \beta } \mathrm \mathrm { ~ \alpha } \mathrm { ~ \alpha ~ \alpha ~ \alpha ~ \beta } \mathrm \mathrm { ~ } \mathrm \mathrm { ~ \alpha } \mathrm \mathrm { ~ \alpha ~ \alpha \alpha ~ \alpha ~ \alpha \mathrm } \mathrm { ~ \alpha \mathrm \mathrm } \mathrm  ~ \alpha \mathrm \mathrm { ~ ~ } \mathrm \mathrm \mathrm \alpha \mathrm  ~ \alpha ~ \alpha \alpha ~ \alpha \alpha ~ \alpha \mathrm \mathrm \alpha ~ \alpha \mathrm \mathrm \alpha \mathrm  ~ ~ \alpha \alpha ~ \alpha \alpha ~ \alpha \alpha \mathrm \alpha ~ \alpha \mathrm \mathrm \alpha \mathrm \mathrm \mathrm \mathrm  ~ ~ \alpha \alpha \alpha ~ \alpha \alpha \alpha \alpha \mathrm \mathrm \alpha \mathrm \mathrm \mathrm \alpha \mathrm \mathrm \mathrm \alpha \mathrm \mathrm \mathrm \mathrm  ~ \alpha \alpha \alpha \alpha \alpha \mathrm \mathrm \alpha \mathrm \mathrm \mathrm \alpha \mathrm \mathrm \mathrm \mathrm \mathrm \alpha \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm  \alpha \alpha \mathrm \alpha \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \alpha \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm  \alpha \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm \mathrm$ is an answer, retaining only those that align with the correct answers in the dataset (Dai et al. 2024).

$$
{ \pmb r } _ { i j } = \mathcal { T } _ { G } ( { \pmb p } , { \pmb q } _ { i } , { \pmb a } _ { i } )
$$

where $\boldsymbol { r }$ are generated rationales, $j \in \{ 1 , . . . , l \}$ and $\pmb { p }$ is the CoT prompt which is shown in Appendix F.1.

Fine-tuning SLMs with Rationales: We initially conduct Std-CoT (Magister et al. 2023) to develop the $ { \boldsymbol { S } } _ { P }$ . Given a question $\pmb q _ { i }$ , we fine-tune the $ { \boldsymbol { S } } _ { P }$ with trainable parameters $\theta$ , to generate the rationale $\boldsymbol { r } _ { i j }$ derived from the $\mathcal { T } _ { G }$ and answer $\mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ } \mathrm { ~ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ } \mathrm { ~ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ } \mathrm { ~ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ \mathrm { ~ } \ \ } \mathrm { ~ \ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ \mathrm { ~ \alpha ~ } \mathbf { \beta } } \mathrm  ~ \mathrm { ~ \alpha ~ } _ { \mathbf { \beta } } \mathrm { ~ \alpha _ { \mathbf { \beta } } \mathrm \mathrm { ~ \alpha } \mathrm { ~ \alpha } \mathrm _ { \mathbf { \beta \alpha } } \mathrm \mathrm { ~ \alpha \beta } \mathrm } \mathrm { ~ ~ \mathrm } \mathrm  ~ \mathrm { ~ \alpha ~ } \mathrm \mathrm { ~ } \mathrm \mathrm { ~ } \mathrm \mathrm { ~ \alpha ~ } \mathrm { ~ \alpha ~ \alpha ~ \beta ~ \alpha \alpha ~ \beta ~ \alpha \alpha ~ \beta ~ \alpha \alpha ~ \beta \alpha ~ \alpha \mathrm } \mathrm \mathrm { ~ \alpha \mathrm } \mathrm  ~ \alpha \mathrm ~ \alpha \alpha \mathrm \mathrm { ~ ~ \alpha \alpha ~ \alpha \beta \alpha \beta ~ \alpha \alpha \beta \mathrm \alpha \mathrm \mathrm \mathrm } \mathrm  \mathrm \mathrm \mathrm  ~ ~ \alpha \alpha \alpha \alpha \alpha \delta \delta \delta \delta \delta \delta \delta ~ \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta $ . We aim to minimize the negative log-likelihood of the sequence comprising the rationale $\boldsymbol { r } _ { i j }$ and the answer $\mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ \textit ~ { ~ a ~ } ~ } _ { i }$ , ensuring that the rationale precedes the answer.

$$
\mathcal { L } _ { \mathrm { S t d - C o T } } ( \theta ) = - \frac { 1 } { n \cdot l } \sum _ { i = 1 } ^ { n } \sum _ { j = 1 } ^ { l } \log p _ { \theta } ( \pmb { r } _ { i j } , \pmb { a } _ { i } \mid \pmb { q } _ { i } )
$$

Rationales offer a deeper understanding of the reasoning behind answers, helping SLMs to respond more accurately (Hsieh et al. 2023). However, SLMs with limited parameters may struggle to retain all training data and complex reasoning capabilities, which can affect the quality of rationale generation (Kang et al. 2023). Furthermore, this implicit learning may cause SLMs to focus on answering questions directly after reading, potentially impairing generalization in reasoning (Dai et al. 2024). Therefore, it is essential to assess the knowledge and capabilities that SLMs fail to acquire and have teacher LLMs generate specialized knowledge. This approach enables SLMs to retrieve and utilize knowledge effectively, enhancing their ability to produce high-quality rationales and perform complex reasoning when needed.

# 3.2 Demonstration Collection

The $ { \boldsymbol { S } } _ { P }$ analyzes $\mathcal { D } _ { \mathrm { t r a i n } }$ to generate predicted rationales $\hat { \pmb { r } }$ and answers $\hat { \bf { a } }$ . Incorrect solutions are identified by comparing each $\hat { a _ { i } }$ with the actual answer $\mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ } \mathrm { ~ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ } \mathrm { ~ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ \mathrm { ~ } \ \ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ \mathrm { ~ } \ \ } \mathrm { ~ \ \ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ \mathrm { ~ \ } \ } \mathrm { ~ \ \ } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm \mathrm { ~ \alpha } _ { \mathbf { \beta } } \mathrm \mathrm  ~ \mathrm { ~ \ \ } \mathrm { ~ \ } \alpha _ { \mathbf { \beta } } \mathrm \mathrm { ~ \alpha } \mathrm _ { \mathbf { \beta } } \mathrm \mathrm { ~ \alpha } \mathrm _ { \mathbf { \beta \alpha } } \mathrm \mathrm { ~ \alpha } \mathrm _ { \beta \alpha } \mathrm \mathrm { ~ \alpha \beta } \mathrm \mathrm { ~ \alpha \alpha _ { \beta \beta } \mathrm \mathrm } \mathrm  ~ \mathrm \alpha \alpha _ { \beta \beta } \mathrm \mathrm \mathrm { ~ \alpha } \mathrm \mathrm { ~ \alpha \alpha } \mathrm \mathrm { ~ \alpha \alpha \beta } \mathrm \mathrm \mathrm { ~ \alpha \alpha \alpha \beta \mathrm } \mathrm \mathrm { \alpha \mathrm \mathrm \alpha \mathrm } \mathrm  \mathrm \alpha \mathrm \mathrm  ~ \alpha \alpha \alpha \alpha \beta \alpha \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta \delta $ . The collected errors, $\mathcal { D } _ { \mathrm { n e g } }$ , reveal the weaknesses of the student model.

$$
{ \mathcal { D } } _ { \mathrm { n e g } } = \{ ( \pmb { q } _ { i } , \hat { \pmb { r } } _ { i } , \pmb { a } _ { i } ) \ | \ \hat { \pmb { a } } _ { i } \neq \pmb { a } _ { i } , ( \pmb { q } _ { i } , \pmb { a } _ { i } ) \in { \mathcal { D } } _ { \mathrm { t r a i n } } \}
$$

In our view, correct cases are those manageable by conventional distillation models using general knowledge. In contrast, error cases are difficult for small-scale neural models to handle effectively, requiring additional specialized knowledge and advanced reasoning capabilities. This specialized knowledge is sparse, making it more cost-effective to represent through a symbolic KB, as neural networks would need a larger parameter scale to capture it.

# 3.3 Symbolic Knowledge Distillation

The $\mathcal { T } _ { T }$ examines each error $\hat { \pmb { r } } _ { i }$ in ${ \mathcal { D } } _ { \mathrm { n e g } }$ and generates specialized knowledge $\mathbf { \lambda } _ { k _ { i } }$ , including generalized learning summaries $\pmb { k } _ { i } ^ { m }$ and supplemental knowledge $k _ { i } ^ { p }$ . This addresses the issues of insufficient knowledge and lack of reasoning ability in the SLM. For each question $\pmb q _ { i }$ in ${ \mathcal { D } } _ { \mathrm { n e g } }$ , we construct the teacher model’s prompt1 $p ^ { \prime }$ that incorporates the student’s incorrect rationale $\hat { \boldsymbol { r _ { i } } }$ , and the correct answer $\mathbf { \alpha } _ { \mathbf { \alpha } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \alpha } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } \mathrm { ~ \textit ~ { ~ a ~ } ~ } _ { i }$ which constructs a specialized KB, represented as $\mathcal { D } _ { \mathrm { k } } ~ =$ $\{ ( \mathbf { q } _ { i } , \mathbf { k } _ { i } ) \ | \ ( \mathbf { q } _ { i } , \hat { r _ { i } } , \mathbf { a } _ { i } ) \ \in \ \mathcal { D } _ { \mathrm { n e g } }$ . The process is primarily driven by the identification and correction of errors.

$$
\pmb { k } _ { i } = \mathcal { T } _ { T } ( \pmb { p } ^ { \prime } , \pmb { q } _ { i } , \hat { \pmb { r } } _ { i } , \pmb { a } _ { i } )
$$

# 3.4 Symbolic KB Augmented Neural Distillation

Inspired by knowledge augmentation (Kang et al. 2023), we propose retrieving relevant specialized knowledge from the specialized KB which is generated through error analysis of the SLM to support its memory and reasoning capabilities. Acquiring specialized knowledge is crucial for training the SLM to produce high-quality rationales, subsequently leading to the correct answers to given questions. In alignment with prior knowledge-intensive tasks, we employ a dense retriever Contriever (Izacard et al. 2021) to retrieve a set of relevant questions for each question: $\mathcal { Q } _ { i } =$ $\mathrm { t o p k } ( \rho ( \pmb q | \pmb q _ { i } ; \mathcal { D } _ { \mathrm { k } } ) , m )$ , where $\rho$ scores the questions $\pmb q \in \mathcal { D } _ { \mathbf { k } }$ based on their relevance to the question $\pmb q _ { i }$ , and topk selects the top $m$ questions with the highest relevance scores. Then we can get the specialized knowledge:

$$
{ \mathcal { K } } _ { i } = \{ { \pmb k } _ { j } \ | \ ( { \pmb q } _ { j } , { \pmb k } _ { j } ) \in { \mathcal { D } } _ { \mathbf { k } } , { \pmb q } _ { j } \in { \mathcal { Q } } _ { i } \}
$$

Finally, we fine-tune the student model $s _ { E }$ , initialized with the fine-tuned student $ { \boldsymbol { S } } _ { P }$ , using the retrieved specialized knowledge $\textstyle { \mathcal { K } } _ { i }$ to generate the rationale $\boldsymbol { r } _ { i j }$ and the answer $\mathbf { \alpha } _ { \mathbf { \beta } } \mathbf { \alpha } _ { \mathbf { \beta } } a _ { i }$ for the question $\pmb q _ { i }$ .

$$
{ \mathcal { L } } _ { \mathrm { N e s y C D } } ( \theta ) = - { \frac { 1 } { n \cdot l } } \sum _ { i = 1 } ^ { n } \sum _ { j = 1 } ^ { l } \log p _ { \theta } ( \pmb { r } _ { i j } , \pmb { a } _ { i } \mid \pmb { q } _ { i } , { \mathcal { K } } _ { i } )
$$

where the rationale and answer are sequentially generated as we did in $\ S 3 . 1$ . Beyond fine-tuning the SLM with augmented distillation (AD), we propose two auxiliary tasks in multi-task learning to enhance reasoning capabilities. These tasks aim to improve the SLM’s ability to integrate and apply specialized knowledge effectively: 1) Answer Prediction (AP), which generates answers directly, aiming to help SLMs internalize the reasoning required for direct questions that do not necessitate a CoT; 2) Direct CoT (DC), which relies solely on the SLM’s intrinsic knowledge (i.e., $\textstyle \mathcal { K } _ { i }$ is empty) to address relatively easy questions.

During the inference stage, as illustrated at the bottom of Figure 1, we determine the necessity of retrieval based on the model confidence (Wang and Zhou 2024). For instances requiring retrieval, we extract the most specialized knowledge from the specialized knowledge base relevant to the question to assist in generating the rationale and answer.

$$
\Delta _ { \mathrm { { a n s w e r } } } = { \frac { 1 } { | { \mathrm { a n s w e r } } | } } \sum _ { x _ { t } \in { \mathrm { a n s w e r } } } p ( x _ { t } ^ { 1 } \mid x _ { < t } ) - p ( x _ { t } ^ { 2 } \mid x _ { < t } )
$$

Here, $\boldsymbol { x } _ { t } ^ { 1 }$ and $x _ { t } ^ { 2 }$ represent the top two tokens at the $t$ -th decoding step. These tokens are selected based on their highest post-softmax probabilities from the vocabulary, given that $\boldsymbol { x } _ { t }$ is part of the answer tokens. More analysis about $\Delta _ { \mathrm { { a n s w e r } } }$ can be seen in Appendix B.1.

# 4 Experiments

In this section, we conduct extensive experiments and comprehensive analysis to evaluate the effectiveness of NesyCD on both in-domain (ID) and out-of-domain (OOD) datasets.

Table 1: Performance $( \% )$ of LLaMA2-7B (Touvron et al. 2023) and TinyLLaMA-1.1B (Zhang et al. 2024a) with different methods across seven selected datasets. Bold indicates the best in each setting. We report the mean and standard deviation of accuracy with 3 different runs for CoT distillation methods. We provide a systematic case study in Appendix E.   

<html><body><table><tr><td rowspan="2">Methods</td><td colspan="3">In-Domain</td><td colspan="5">Out-Of-Domain</td><td rowspan="2">Average</td></tr><tr><td>BBH-test</td><td>GSM8K</td><td>BB-sub</td><td>AGIEval</td><td>GSM8K-PLUS</td><td>ARC-E</td><td>ARC-C</td></tr><tr><td colspan="10">#Closed-source model and Open-source models (Zero-shot-CoT)</td></tr><tr><td>GPT-3.5-turbo (Teacher)</td><td>43.2</td><td>72.6</td><td>44.0</td><td>50.5</td><td>55.9</td><td>91.8</td><td>84.1</td><td></td></tr><tr><td>LLaMA-3-70B-Instruct</td><td>62.6</td><td>89.2</td><td>51.0</td><td>66.3</td><td>72.9</td><td>97.6</td><td>93.2</td><td>63.2 76.1</td></tr><tr><td colspan="10">#TinyLLaMA-1.1Bbased</td></tr><tr><td>Zero-shot (Radford et al.2019)</td><td>14.0</td><td>2.0</td><td>17.7</td><td>17.8</td><td>1.5</td><td>19.4</td><td>15.0</td><td>12.5</td></tr><tr><td>Zero-shot-CoT (Kojima et al. 2022)</td><td>13.5</td><td>1.4</td><td>17.7</td><td>10.4</td><td>1.3</td><td>16.0</td><td>13.4</td><td>10.5</td></tr><tr><td>Fine-tuning</td><td>48.8</td><td>3.5</td><td>26.0</td><td>21.2</td><td>3.7</td><td>28.0</td><td>24.6</td><td>22.3</td></tr><tr><td>Knowledge-Augmented Fine-tuning</td><td>49.3</td><td>3.7</td><td>27.4</td><td>21.9</td><td>3.3</td><td>29.4</td><td>25.3</td><td>22.9</td></tr><tr><td>Std-CoT (Magister et al. 2023)</td><td>47.8±.43</td><td>7.9±.27</td><td>27.6±.31</td><td>21.5±.56</td><td>4.3±.62</td><td>28.2±.69</td><td>25.0±.48</td><td>23.2</td></tr><tr><td>MT-CoT (Li etal.2024b)</td><td>44.1±.78</td><td>4.1±.35</td><td>25.0±.45</td><td>21.4±.64</td><td>2.8±.83</td><td>33.5±.52</td><td>25.1±.59</td><td>22.3</td></tr><tr><td>Step-by-step (Hsieh et al.2023)</td><td>42.4±.56</td><td>4.3±.47</td><td>26.2±.38</td><td>21.1±.72</td><td>3.1±.54</td><td>29.6±.61</td><td>25.9±.66</td><td>21.8</td></tr><tr><td>KARD (BM25) (Kang et al. 2023)</td><td>49.5±.61</td><td>7.6±.40</td><td>26.9±.43</td><td>20.2±.48</td><td>4.0±.77</td><td>28.2±.85</td><td>26.5±.91</td><td>23.3</td></tr><tr><td>CasCoD (Dai et al. 2024)</td><td>48.1±.49</td><td>6.8±.39</td><td>23.1±.64</td><td>19.4±.73</td><td>4.8±.48</td><td>29.0±.63</td><td>27.1±.42</td><td>22.6</td></tr><tr><td>NesyCD (ours)</td><td>66.3±.42</td><td>11.8±.83</td><td>30.6±.27</td><td>23.1±.41</td><td>7.2±.93</td><td>36.2±.76</td><td>29.0±.58</td><td>29.3</td></tr><tr><td colspan="9">#LLaMA2-7Bbased</td></tr><tr><td>Zero-shot (Radford et al. 2019)</td><td>17.3</td><td>2.7</td><td>18.6</td><td>19.2</td><td>2.4</td><td>25.2</td><td>20.6</td><td>17.0</td></tr><tr><td>Zero-shot-CoT(Kojima et al.2022)</td><td>13.5</td><td>3.1</td><td>12.2</td><td>10.3</td><td>2.1</td><td>29.1</td><td>20.2</td><td>12.9</td></tr><tr><td>Fine-tuning</td><td>57.8</td><td>5.8</td><td>33.3</td><td>31.0</td><td>5.8</td><td>73.3</td><td>56.3</td><td>37.6</td></tr><tr><td>Knowledge-Augmented Fine-tuning</td><td>58.7</td><td>6.3</td><td>34.2</td><td>31.8</td><td>6.1</td><td>75.1</td><td>57.0</td><td>38.5</td></tr><tr><td>Std-CoT (Magister et al. 2023)</td><td>58.1±.74</td><td>20.5±.71</td><td>30.7±.48</td><td>23.6±.65</td><td>12.0±.26</td><td>73.4±.81</td><td>55.9±.78</td><td>39.2</td></tr><tr><td>MT-CoT (Li et al. 2024b)</td><td>46.4±.52</td><td>7.5±.48</td><td>28.1±.55</td><td>32.1±.53</td><td>5.8±.39</td><td>70.3±.67</td><td>55.7±.45</td><td>35.1</td></tr><tr><td>Step-by-step (Hsieh et al. 2023)</td><td>53.9±.69</td><td>8.3±.57</td><td>32.3±.33</td><td>32.4±.40</td><td>5.6±.41</td><td>74.9±.52</td><td>60.0±.56</td><td>38.2</td></tr><tr><td>KARD (BM25) (Kang et al.2023)</td><td>59.2±.93</td><td>23.5±.62</td><td>30.8±.66</td><td>29.2±.79</td><td>15.2±.54</td><td>70.2±.71</td><td>55.4±.48</td><td>40.5</td></tr><tr><td>CasCoD (Dai et al.2024)</td><td>59.6±.78</td><td>23.6±.87</td><td>32.2±.71</td><td>28.8±.63</td><td>14.5±.68</td><td>72.6±.49</td><td>56.7±.83</td><td>41.1</td></tr><tr><td>NesyCD (ours)</td><td>75.5±.69</td><td>32.4±.53</td><td>36.9±.38</td><td>33.6±.71</td><td>24.1±.47</td><td>77.5±.89</td><td>60.8±.56</td><td>48.7</td></tr></table></body></html>

# 4.1 Datasets

Following (Wang et al. $2 0 2 3 \mathrm { a }$ ; Ying et al. 2024), we focus on three practical abilities: factual, mathematical, and general reasoning. For each ability, we select a relevant public dataset, integrate its training data into the target dataset $\mathcal { D } _ { \mathrm { t r a i n } }$ for mixed training, and combine its test data into the evaluation dataset $\mathcal { D } _ { \mathrm { e v a l } }$ . Additionally, each ability includes an OOD dataset in $\mathcal { D } _ { \mathrm { e v a l } }$ . This setup allows us to evaluate the model’s ability to generalize and enhance performance beyond the ID training environment.

Factual Reasoning: We select the Multitask Language Understanding (MMLU) (Hendrycks et al. 2021a) as the ID dataset, which includes multiple-choice questions across 57 subjects. For OOD evaluation, we use the ARC (Clark et al. 2018), comprising both Easy and Challenge segments.

Mathematical Reasoning: We select MetaMathQA (Yu et al. 2023a) as the ID dataset, which includes a high-quality collection of forward and reverse mathematical reasoning question-answer pairs, derived from GSM8K (Cobbe et al. 2021) and MATH (Hendrycks et al. 2021b). For OOD evaluation, we use GSM8K and ${ \mathrm { G S M 8 K + } }$ (Li et al. 2024a).

General Complex Reasoning: We chose BIG-Bench Hard (BBH) (Suzgun et al. 2022) as the ID dataset, which includes 27 challenging tasks spanning arithmetic, symbolic reasoning, and more, derived from BIG-Bench (BB) (Srivastava et al. 2022). Most of the data consists of multiple-choice questions. For OOD evaluation, we use BB-Sub filtered by CasCoD, and AGIEval (Zhong et al. 2023) subtasks about English multiple-choice questions.

# 4.2 Baselines

We compare our method with the following baselines: 1) Teacher & Vanilla Student in Zero-shot (Radford et al. 2019), Zero-shot-CoT (Kojima et al. 2022). 2) Fine-tuning involves fine-tuning a model to generate answers given only questions. The performance of the baselines above illustrates the capability of SLMs to solve tasks using only training data, without external guidance or additional knowledge. 3) CoT distillation includes Std-CoT (Magister et al. 2023) which is the standard CoT distillation method, enabling direct fine-tuning of the student model with CoT data; Stepby-step (Hsieh et al. 2023) is a multi-task method that extracts rationales and answers separately; MT-CoT (Li et al. 2024b) is another multi-task method that optimizes both answer prediction and CoT generation simultaneously; CasCoD (Dai et al. 2024) decomposes the traditional singlestep learning process into two cascaded learning steps. 4) Knowledge-Augmentation involves attaching retrieved passages to the question during both training and inference. This includes Knowledge-Augmented Fine-tuning focuses on generating answers only, and KARD (Kang et al. 2023) emphasizes learning the generation of rationales.

![](images/dee3520688a0b836b27c8374fbbcbd3f640b8e94857b7997c0c3dab9896a0e40.jpg)  
Figure 3: Efficiency on training data and model size. The backbone model for the data size variation is Qwen2-1.5B

Table 2: Results for different retrievers.   

<html><body><table><tr><td>Retriever</td><td>BBH</td><td>BB</td><td>AGIEval</td><td>GSM8K+ ARC-E</td><td></td></tr><tr><td>Contriever</td><td>75.49</td><td>36.92</td><td>33.63</td><td>24.11</td><td>77.53</td></tr><tr><td>DPR</td><td>74.47</td><td>36.84</td><td>32.87</td><td>23.43</td><td>77.48</td></tr><tr><td>BM25</td><td>75.55</td><td>37.11</td><td>31.44</td><td>23.57</td><td>77.10</td></tr></table></body></html>

Table 3: Results for different $m$ .   

<html><body><table><tr><td>#Knowledge</td><td>BBH</td><td>BB</td><td>AGIEval</td><td>GSM8K+</td><td>ARC-E</td></tr><tr><td>m=1</td><td>75.49</td><td>36.92</td><td>33.63</td><td>24.11</td><td>77.53</td></tr><tr><td>m=2</td><td>72.41</td><td>35.36</td><td>32.57</td><td>21.28</td><td>77.48</td></tr><tr><td>m=3</td><td>73.28</td><td>34.71</td><td>31.83</td><td>20.92</td><td>76.98</td></tr></table></body></html>

# 4.3 Implementations

For all experiments, we use the LLaMA2-7B (Touvron et al. 2023) and TinyLLaMA-1.1B (Zhang et al. 2024a) as the student SLM. We query the teacher model GPT-3.5-turbo to annotate the CoTs data with the manual prompt (Suzgun et al. 2022). Unless otherwise specified, $m$ is set to 1 (§4.7) and $\Delta _ { \mathrm { t h r e s h o l d } }$ is set to 0.68 $( \ S 4 . 8 )$ . We follow the standard metrics and datasets statics elaborated in Appendix A.1.

We employ LoRA (Hu et al. 2022) for parameter-efficient fine-tuning of the student SLMs. All experiments are conducted on 2 A100 GPUs with 80GB. During the inference stage, we utilize vLLM (Kwon et al. 2023) to accelerate inference. Detailed information about training and hyperparameters is provided in Appendix A.2.

# 4.4 Main Results

Table 1 shows that the NesyCD has achieved significant improvements on both ID and OOD datasets using two weaker $\mathrm { S L M s } ^ { 2 }$ . Specifically, LLaMA2-7B and TinyLLaMA-1.1B demonstrated an average improvement of $8 . 4 \%$ and $5 . 9 \%$ respectively, consistently outperforming all existing baselines. For an analysis of model size, please refer to $\ S 4 . 5$ . The impact of NesyCD decreases as the model size (and hence capability) increases because larger models can retain knowledge better during pre-training and fine-tuning.

Compared to Zeroshot and Zeroshot-CoT, CoT distillation has significantly improved the performance of SLM. While fine-tuning methods can significantly enhance factual and general reasoning abilities, fine-tuning’s impact on mathematical reasoning remains minimal. CoT distillation aids SLM in generating rationales that clarify intermediate steps, thereby enhancing mathematical reasoning capabilities. Among various CoT distillation methods, NesyCD not only boosts symbolic knowledge learning to rectify SLM errors but also adaptively retrieves specialized knowledge based on confidence during tests to assist in producing high-quality rationales, thus improving overall performance. In comparison to KARD (Kang et al. 2023), our specialized KB is more relevant to the capabilities of the SLM and the question at hand. Additionally, it generates a distribution more aligned with the SLM through the LLM, significantly enhancing effectiveness (Yu et al. 2023b). Our NesyCD significantly enhances the SLM’s performance, demonstrating the effectiveness of neural-symbolic knowledge integration in complex reasoning tasks.

# 4.5 Efficiency on Dataset and Model Sizes

To evaluate the efficiency of NesyCD in terms of training data and model size, we measured test accuracy using Qwen2’s (Yang et al. 2024) 0.5B, 1.5B, and 7B models across various methods while varying the amount of training data and model size. As shown at the bottom of Figure 3, NesyCD successfully transfers the reasoning capabilities of the teacher LLM by generating symbolic specialized knowledge, even with minimal training data. As the training data decreases, the performance gap between NesyCD and other baselines widens, demonstrating NesyCD’s superior robustness and sample efficiency. This suggests that NesyCD performs better with fewer samples and that its effectiveness can be further enhanced by increasing the

![](images/09a8925fa836f631d7d7fddd0e2046b0db8843fdff4a5eabe2c7d02cdf8d644f.jpg)  
Figure 4: Performance variation trend on $\Delta _ { \mathrm { t h r e s h o l d } }$ . The results are reported by ID-Avg and OOD-Avg which respectively denote average accuracy on ID and OOD datasets.

# training data, allowing for more effective distillation.

Regarding model size efficiency, as shown at the top of Figure 3, NesyCD outperforms other baselines across various model scales. Notably, NesyCD enables Qwen2-7B to surpass the teacher GPT-3.5 Turbo in both ID and OOD performance, despite having over $1 0 \times$ fewer parameters. These results highlight NesyCD’s significant practical benefits in resource-constrained environments, as it reduces the computational cost required for SLMs while achieving performance levels that exceed those of larger LLMs. This further illustrates that SLMs cannot fully utilize the CoT reasoning generated by LLMs, thereby necessitating the implementation of our proposed NesyCD.

# 4.6 Performances with Different Retrievers

We examined the impact of various retrievers including DPR (Karpukhin et al. 2020), Contriever (Izacard et al. 2021), and BM25 (Robertson and Zaragoza 2009) on the performance of NesyCD using LLaMA2-7B. As shown in Table 2, the performance differences among these retrievers are minimal, with Contriever performing slightly better. This finding suggests that NesyCD can attain improved benefits as the quality of the retriever advances, effectively retrieving more relevant specialized knowledge and enhancing the capability to solve complex reasoning tasks.

# 4.7 The Number of Knowledge Used for Inference

Even LLMs can be easily distracted by irrelevant background information (Shi et al. 2023) or extended context (Liu et al. 2023). Therefore, simply adding more knowledge during the inference process does not necessarily enhance performance unless the relevant knowledge is selected. Table 3 illustrates the impact of the number of knowledge used during inference in the NesyCD ( $\dot { m }$ in $\ S 4 . 7$ ) on LLaMA2- 7B. We observe that performance decreases as $m$ increases which implies that including additional knowledge does not always enhance reasoning and can interfere with the model’s judgment, corroborating previous research findings.

Table 4: Ablation studies on different components.   

<html><body><table><tr><td>Methods</td><td>BBH</td><td>BB</td><td>AGIEval</td><td>GSM8K+</td><td>ARC-E</td></tr><tr><td>NesyCD</td><td>75.49</td><td>36.92</td><td>33.63</td><td>24.11</td><td>77.53</td></tr><tr><td>w/o km</td><td>68.48</td><td>35.32</td><td>31.75</td><td>20.93</td><td>76.86</td></tr><tr><td>w/o kp</td><td>68.86</td><td>35.64</td><td>31.59</td><td>22.35</td><td>77.06</td></tr><tr><td>w/o AD</td><td>64.34</td><td>32.23</td><td>28.85</td><td>20.43</td><td>74.79</td></tr><tr><td>w/o AP&DC</td><td>66.51</td><td>35.12</td><td>29.83</td><td>21.88</td><td>77.41</td></tr><tr><td>Std-CoT</td><td>58.13</td><td>30.68</td><td>23.61</td><td>12.02</td><td>73.36</td></tr><tr><td>wk</td><td>61.24</td><td>31.33</td><td>26.41</td><td>13.75</td><td>74.32</td></tr><tr><td>Zero-shot-CoT</td><td>13.51</td><td>12.19</td><td>10.32</td><td>2.08</td><td>29.13</td></tr><tr><td>wk</td><td>14.57</td><td>13.23</td><td>11.68</td><td>2.92</td><td>30.68</td></tr></table></body></html>

# 4.8 Impact of Confidence Threshold

We investigated the effect of varying the confidence threshold $\Delta _ { \mathrm { t h r e s h o l d } }$ on performance across both ID and OOD datasets using LLaMA2-7B, as shown in Figure 4. A higher confidence threshold means the model requires greater certainty to trust its output. In extreme cases, $\Delta _ { \mathrm { t h r e s h o l d } } ~ = ~ 0$ means no retrieval is performed, while $\Delta _ { \mathrm { t h r e s h o l d } } ~ = ~ 1 0 0$ means retrieval is performed for all cases. Both extremes lead to significant performance degradation, underscoring the need for adaptive retrieval. For easy tasks, the model might produce hallucinations and incorrect answers with additional knowledge, while for complex tasks lacking external guidance, the model cannot rely solely on internal parameters. Despite these challenges, our method consistently outperforms other baselines, even in extreme scenarios.

# 4.9 Ablation Studies

To demonstrate the effectiveness of NesyCD, we created four variants by individually removing the learning summary $\pmb { k } ^ { m }$ , the supplementary knowledge $k ^ { p }$ , the augmented distillation (AD), and the multi-task learning (AP & DC, $\ S 3 . 4 )$ respectively. Specialized knowledge $\boldsymbol { k }$ is composed of $\pmb { k } ^ { m }$ and $k ^ { p }$ (§3.2). We employed LLaMA2-7B as the SLM for ablation studies, and the results are presented in Table 4. We can observe that performance diminishes with the exclusion of any single component, underscoring the significance of each element. Additionally, specialized knowledge exhibits orthogonality and universality, enhancing Zero-shotCoT and other CoT distillation methods (w $k$ ) which confirms the importance of refining symbolic knowledge.

# 5 Conclusion

In this work, we introduce Neural-Symbolic Collaborative Distillation (NesyCD), a method aimed at enhancing the capabilities of Small Language Models (SLMs) for complex reasoning tasks that require additional knowledge and advanced reasoning skills. NesyCD uses Large Language Models (LLMs) to analyze SLM errors and generate specialized knowledge, including learning summaries and supplementary knowledge, organized into an external knowledge base. By integrating parameter updates with retrieving specialized knowledge, NesyCD improves both rationale generation and answer accuracy for SLMs. Empirical experiments show that NesyCD surpasses fine-tuning and CoT distillation baselines in in- and out-of-domain scenarios.