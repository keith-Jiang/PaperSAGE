# XCOT: Cross-lingual Instruction Tuning for Cross-lingual Chain-of-Thought Reasoning

Linzheng Chai1, Jian Yang1\*, Tao $\mathbf { S u n } ^ { 1 }$ , Hongcheng $\mathbf { G u o } ^ { 1 }$ , Jiaheng ${ { \bf { L i u } } ^ { 1 } }$ , Bing Wang1, Xinnian Liang1, Jiaqi $\mathbf { B } \mathbf { a } \mathbf { i } ^ { 2 }$ , Tongliang $\mathbf { L i } ^ { 3 }$ , Qiyao Peng4, Zhoujun Li1

1The State Key Laboratory of Complex & Critical Software Environment, Beihang University 2Cyberspace Institute of Advanced Technology, Guangzhou University 3Beijing Information Science and Technology University 4School of New Media and Communication, Tianjin University {chailinzheng, jiaya}@buaa.edu.cn

# Abstract

Chain-of-thought (CoT) has emerged as a powerful technique to elicit reasoning in large language models and improve a variety of downstream tasks. CoT mainly demonstrates excellent performance in English, but its usage in low-resource languages is constrained due to poor language generalization. To bridge the gap among different languages, we propose a cross-lingual instruction fine-tuning framework (XCOT) to transfer knowledge from high-resource languages to low-resource languages. Specifically, the multilingual instruction training data (XCOT-INSTRUCT) is created to encourage the semantic alignment of multiple languages. We introduce cross-lingual in-context few-shot learning (xICL) to accelerate multilingual agreement in instruction tuning, where some fragments of source languages in examples are randomly substituted by their counterpart translations of target languages. During multilingual instruction tuning, we adopt the randomly online CoT strategy to enhance the multilingual reasoning ability of the large language model by first translating the query to another language and then answering in English. To further facilitate the language transfer, we leverage the high-resource CoT to supervise the training of low-resource languages with cross-lingual distillation. Experimental results demonstrate the superior performance of XCOT in reducing the gap among different languages, highlighting its potential to reduce the cross-lingual gap.

# Introduction

Recent advancements in Large Language Models (LLMs) (Touvron et al. 2023; Touvron and Louis Martin 2023; Patel et al. 2023; OpenAI 2023; Bai et al. 2023; Hui et al. 2024) in natural language processing (NLP) have intensively engaged the interests of researchers. LLMs (Wei et al. 2022c; Zhang et al. 2023; Kojima et al. 2022) are further equipped with the chain-of-thought (CoT) technique to gain impressive performance in complex reasoning tasks, where LLMs first produce intermediate reasoning steps and infer the final answer.

However, existing studies related to the CoT methods are mainly constrained in high-resource languages (e.g. English) and deliver little consideration into multilingual scenarios. Recent works (Shi et al. 2023; Qin et al. 2023; Chen et al. 2023) endeavor to simply use prompt engineering to improve the language generalization ability of the model without any fine-tuning. These prompt-based methods ignore the potential of representation-based crosslingual alignment derived from the cross-lingual supervised fine-tuning (cross-lingual SFT). Supervised fine-tuning has been shown to perform at a satisfactory level across various tasks, such as FLAN (Wei et al. 2022a) and InstructGPT (Ouyang et al. 2022). Therefore, how to encourage crosslingual alignment in supervised fine-tuning still requires further exploration.

![](images/9b35d91a4b0d092bc4f4ff5f03a8d4feb4ec531a5aae68f5e1d4b6cae54523d6.jpg)  
Figure 1: Illustration of XCOT. The cross-lingual instruction tuning is used to align representations of different languages.

To minimize the gap among different languages, we propose a Cross-lingual Chain-of-Thought reasoning (XCOT) framework using cross-lingual supervised instruction finetuning. Specifically, we first construct the multilingual instruction training data (XCOT-INSTRUCT) by translating English to other languages. Then, we randomly substitute some fragments of source languages in examples by their counterpart translations of target languages. To transfer high-resource languages to low-resource languages, we mix the tokens of the source and target language in the same query to enable the LLMs to handle different languages. The code-switched examples and the query can be applied to cross-lingual in-context learning in supervised instruction tuning. During multilingual instruction tuning, we adopt

Framework of Cross-lingual CoT En 目 English: John writes 20 pages a day. How long will it take Code-switched Corpora Translation hGiemrmtoanw:rJitoeh3n sbcohorkesibttha2t0aSreit4e0n0apmagTeasge.acWhi?e lange wird er 目 brauchen, um drei Bücher mit jeweils 400 Seiten zu schreiben? 《 Code-switch: John write 20 Seiten am Tag. Wie lange wird er Zh Fr De brauchen, um drei Bücher mit are 400 pages each? Multilingual Rejection Multilingual Corpora Span-level Codeswitch Code-switched Context Sampling English Question Y Code-switched English CoT SFT Multilingual SFT Answer G T 三 G D Cross-lingual Code-switched Context Distillation Large Language Random Online CoT Chinese Question Enhanced Model English CoT Code-switched Answer Corpora

the randomly online CoT strategy to enhance the multilingual reasoning ability of the large language model by first translating the query to another language and then answering in English. To further facilitate the language transfer, we leverage the high-resource CoT to supervise the training of low-resource languages with cross-lingual distillation. Experimental results on previous benchmarks demonstrate the superior performance of XCOT in reducing the gap among different languages, highlighting its potential to reduce the cross-lingual gap.

Extensive experiments of XCOT are evaluated on multilingual benchmarks MGSM of 11 languages and MSVAMP of 10 languages. The results demonstrate that our proposed method consistently achieves state-of-the-art performance across all languages, notably surpassing strong baseline by an average margin of $1 5 \%$ . The contributions in this work are summarized as follows: (1) We construct the multilingual instruction data to transfer knowledge of high-resource languages into low-resource languages. The training data is further augmented by cross-lingual in-context learning, where a piece of code-switched demonstration context and the current query are concatenated as the input for LLM. (2) During training, we propose the random online CoT (Random-CoT), which first randomly translates the query into other languages and then answers in English. (3) To align the representations of different languages, we propose cross-lingual knowledge to align the output distribution given the queries of different languages using Kullback–Leibler divergence.

# Cross-lingual CoT Reasoning

Given the query $q = ( q _ { 1 } \ldots , q _ { m } ) $ of language $\mathbf { { { L } } } _ { i }$ , the large language model (LLM) $\mathcal { M }$ outputs the corresponding answer $a \ = \ ( a _ { 1 } , \ldots , a _ { n } )$ of language $L _ { j }$ , where $m$ and $n$ $L _ { j }$ laerengstohusrcofe parnodmtpatrgaentdlangsuwaegrei,nwahsearem $\mathsf { \bar { L } } _ { a l l } = \mathsf { \bar { \{ L _ { k } \} } } _ { k = 1 } ^ { K }$ $( q , a )$ $L _ { i }$ and is number of languages. LLM further enhances the task performance by chain-of-thought reasoning, where the chain-of-thought examples of sequences $c = ( c _ { 1 } , \ldots , c _ { t } ) $ are added into the exemplars of prompting. The high-quality rationales $c$ comprised of a series of intermediate natural language reasoning steps provide helpful suggestions for the final output. Given multiple chain-of-thought examples as demonstrations and the original prompt $q$ of the target language as a whole, the problem definition of cross-lingual CoT is described as:

$$
P ( a | q , c ) = \prod _ { j = 1 } ^ { n } P ( a _ { j } | a _ { < j } ; q , c , \mathcal { M } )
$$

where $q$ (question) and $c$ (corresponding exemplars) are concatenated as a whole $p$ to predict the answer denoted as $P ( a | p )$ . Driven by the CoT demonstrations $c$ , the LLM first generates the intermediate steps and then outputs the final answer $a$ .

# XCOT

# Model Overview

Figure 2 describes the overall framework of our method XCOT. Specifically, cross-lingual in-context few-shot learning (xICL) encourages multilingual alignment in instruction tuning, where the query in the example is mixed with different language tokens. During multilingual instruction tuning, the randomly online CoT strategy (Random-CoT) is used to promote the multilingual reasoning ability of LLM and then answer in English. Finally, we leverage the highresource CoT to supervise the training of low-resource languages with cross-lingual distillation.

Algorithm 1: Random Online CoT   

<html><body><table><tr><td>Multilingual LLM: M; Batch size: B;</td><td>Input: Multilingual Instruction Dataset: D; Maximum supervised fine-tuning step: T; Target language set: Lall ={Lk}=1;</td></tr><tr><td>1t←0</td><td>Output: Fine-tuned LLM: M</td></tr><tr><td></td><td>2 while t≤T;do</td></tr><tr><td>3</td><td>Random sampled batchB∈ D</td></tr><tr><td>4</td><td>for k ←1 to B; do</td></tr><tr><td>5</td><td>(cLi,j,qLi,aLə)←B</td></tr><tr><td>6</td><td>Lk ~U(Lau) (Lk ≠Lj)</td></tr><tr><td>7</td><td>qLk←M([cLi,j,qLi])</td></tr><tr><td>8</td><td>// Translate qLi →qLκ a'Lj ←M([cLi,j,qL²,qLk])</td></tr><tr><td></td><td>// Answer in language Lj</td></tr><tr><td>9</td><td>B ←BU(qLκ,qLκ,a'Lj) Optimize M with B</td></tr><tr><td>10</td><td>t↑t+1</td></tr><tr><td>11</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>12returnM</td><td></td></tr></table></body></html>

# XCOT-INSTRUCT

Data Construction We create a new multilingual instruction dataset (XCOT-INSTRUCT) for cross-lingual chain-ofthought reasoning, which can be used as the training corpora for multilingual benchmarks, such as MGSM (Shi et al. 2023) and MSVAMP (Chen et al. 2023). We use the multilingual translator to expand the English instruction data GSM8K into other 10 languages, including German, French, Spanish, Russian, Chinese, Japanese, Thai, Telugu, Bengali, and Swahili. The instruction dataset of each language contains 7.4K samples, where we only translate the query into other languages and retain the response in English to facilitate the cross-lingual transfer. Finally, we obtain the multilingual instruction data $D = \{ D ^ { L _ { k } } \} _ { k = 1 } ^ { K }$ and $( q ^ { L _ { i } } , c ^ { L _ { i } } , a ^ { L _ { j } } ) \in D ^ { L _ { i } }$ , where $D ^ { L _ { i } }$ is the SFT training data of language $L _ { i }$ and the number of the languages is $K$ . $D ^ { L _ { i } }$ contains query $q ^ { L _ { i } }$ and response $a ^ { L _ { j } }$ with the corresponding context $c ^ { \hat { L } _ { i } }$ . $q ^ { L _ { i } }$ is the query of source language and $a ^ { L _ { j } }$ is the response of the high-resource language $\cdot L _ { j }$ is English in our work). cLi = {qb , ab }bB=1 is the context demonstration comprised of $B$ queries of language $\mathbf { } L _ { i }$ and the responses of $L _ { j }$ . For each language, we construct about 22K data context demonstration samples.

Cross-lingual Instruction Tuning Given the crosslingual instruction corpora $D = \{ D ^ { \check { L } _ { k } } \} _ { k = 1 } ^ { K }$ }kK=1, where D contains $K$ languages and $L _ { a l l } = \{ L _ { k } \} _ { k = 1 } ^ { K }$ . The LLM is jointly trained on the union of the multilingual corpora :

$$
\mathcal { L } _ { x } = - \sum _ { i = 1 } ^ { K } \mathbb { E } _ { { } _ { c } L _ { i } , { q } ^ { L _ { i } } , { a } ^ { L _ { j } } \sim D _ { L _ { i } } } \left[ \log P ( { a } ^ { L _ { j } } | { q } ^ { L _ { i } } , { c } ^ { L _ { i } } ; \mathcal { M } ) \right]
$$

where $q ^ { L _ { i } }$ is the query of the language $L _ { i }$ and $a ^ { L _ { j } }$ is the response of language $L _ { j }$ .

# Cross-lingual In-context Learning (xICL)

To encourage cross-lingual alignment across different languages, we construct the code-switched query by replacing the spans of the source query with the counterparts of the target language.

Given a bilingual query $( q ^ { L _ { i } } , q ^ { L _ { j } } )$ with the source language query $q ^ { L _ { i } } = \{ q _ { 1 } ^ { L _ { i } } , \ldots , q _ { m } ^ { L _ { i } } \}$ of $m$ tokens and the target translation $q ^ { L _ { j } } = \{ y _ { 1 } ^ { L _ { j } } , \dots , y _ { n } ^ { L _ { j } } \}$ of $n$ tokens, we create the code-switched sequence $q ^ { L _ { i , j } }$ by substituing the phrase $q _ { u _ { 1 } : u _ { 2 } } ^ { L _ { i } }$ with counterpart translation $q _ { v _ { 1 } : v _ { 2 } } ^ { L _ { j } }$ , where $\left| { { L } _ { j } } \right|$ is the target translation of source piece quL1i:u2 . quL1i:u2 denotes the phrase in $q ^ { L _ { i } }$ from the $u _ { 1 }$ -th token to the $u _ { 2 }$ -th token $\mathrm { ~ \textmu ~ } _ { 1 } \mathrm { ~ \leq ~ }$ $u _ { 1 } \leq u _ { 2 } \leq m )$ and $q _ { v _ { 1 } : v _ { 2 } } ^ { L _ { j } }$ denotes the phrase in $q ^ { L _ { j } }$ from the $\boldsymbol { v } _ { 1 }$ -th token to the $v _ { 2 }$ -th token $( 1 \leq v _ { 1 } \leq v _ { 2 } \leq n )$ . For each phrase in code-switched in $q ^ { L _ { i , j } }$ , it comes from source phrase $q _ { u _ { 1 } : u _ { 2 } } ^ { L _ { i } }$ or target phrase $q _ { v _ { 1 } : v _ { 2 } } ^ { L _ { j } }$ . The proportion of the source words in the code-switched sequence $q ^ { L _ { i , j } }$ is denoted as $\alpha$ . $q _ { i , j } ^ { L }$ contains $q ^ { L _ { i / j } }$ (source sentence with target tokens) and $q ^ { L _ { j / i } }$ (target sentence with source tokens).

Specifically, the code-switched sequence can be created in two ways: (1) $q ^ { L _ { i / j } }$ (source sentence with target tokens): most tokens in $q ^ { L _ { i / j } }$ derive from $q ^ { L _ { i } }$ , where some source phrases quL1i:u2 are substituted by their target counterpart phrases $q _ { v _ { 1 } : v _ { 2 } } ^ { L _ { j } }$ $\mathit { \check { \alpha } } \geq \ 0 . 5 )$ . (2) $q ^ { L _ { j / i } }$ (target sentence with source tokens): most tokens in $q ^ { L _ { j , i } }$ derive from $q ^ { L _ { j } }$ where some target phrases $q _ { v _ { 1 } , v _ { 2 } } ^ { L _ { j } }$ are substituted by their source counterpart phrases $q _ { u _ { 1 } , u _ { 2 } } ^ { L _ { i } }$ $( \alpha < 0 . 5 )$ .

# Random Online CoT(Random-CoT)

To force the model to understand the multilingual queries, we introduce the random online CoT, which first prompts the LLM to translate the query $q ^ { L _ { i } }$ to another language query $q ^ { L _ { k } }$ and then answer in $a ^ { \check { L _ { j } } }$ during the LLM tuning. Algorithm 1 describes the detail of Random-CoT, where given the training instance $( c ^ { L _ { i , j } } , q ^ { L _ { i } } , a ^ { L _ { j } } ) \in D$ , we uniformly sample an intermediate language $L _ { k }$ $\boldsymbol { \mathbf { \mathit { L } } } _ { k } \neq \boldsymbol { \mathit { L } } _ { i } )$ and prompt LLM first to translate $q ^ { L _ { i } }$ to $q ^ { L _ { k } ^ { - } }$ . Although $L _ { i }$ may belong to low-resource languages and the quality of $q ^ { L _ { k } }$ may be poor initially, our method still benefits from the translation signal of $q ^ { L _ { i } } \to q ^ { L _ { k } }$ by aligning the representations of different languages.

# Cross-lingual Distillation (xDistill)

mSampling To further augment the cross-lingual instruction tuning, we refer to the RFT(Yuan et al. 2023), use the fine-tuned LLM $\mathcal { M }$ to generate the synthetic response of the multilingual queries and then select correct reasoning paths as the augmented dataset $D ^ { \prime }$ . Finally, our model is trained on the original and augmented dataset $D \cup D ^ { \prime }$ .

From High-Resource to Low-Resource We use the highresource samples to supervise the low-resource samples. Given the parallel high-resource sample $( c ^ { L _ { i , j } } , q ^ { L _ { i } } , \dot { a } ^ { L _ { j } } )$

<html><body><table><tr><td>Method</td><td>Base Model</td><td>En</td><td>De</td><td>Fr</td><td>Es</td><td>Ru</td><td>Zh</td><td>Ja</td><td>Th</td><td>Te</td><td>Bn</td><td>Sw</td><td>Avg.</td></tr><tr><td>Closed-Source Models</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Native-CoTt</td><td>GPT-3.5</td><td>67.2</td><td>62.0</td><td>59.2</td><td>61.2</td><td>50.4</td><td>52.8</td><td>46.8</td><td>15.6</td><td>1</td><td>7.6</td><td>40.0</td><td>46.3</td></tr><tr><td>Native-CoT†</td><td>GPT-4</td><td>80.0</td><td>73.6</td><td>72.0</td><td>71.2</td><td>64.0</td><td>70.0</td><td>71.6</td><td>40.4</td><td>1</td><td>17.6</td><td>64.4</td><td>62.5</td></tr><tr><td>Open-Source Models(7B)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Llama-2†(Touvron and Louis Martin 2023)</td><td>Llama-2</td><td>43.2</td><td>37.2</td><td>34.4</td><td>32.4</td><td>28.0</td><td>22.4</td><td>15.2</td><td>4.8</td><td>1</td><td>3.2</td><td>5.2</td><td>22.6</td></tr><tr><td>RFT† (Yuan et al.2023)</td><td>Llama-2</td><td>44.8</td><td>33.6</td><td>34.0</td><td>34.0</td><td>29.2</td><td>16.8</td><td>6.8</td><td>2.0</td><td>1</td><td>2.4</td><td>2.8</td><td>20.6</td></tr><tr><td>MAmmoTH† (Yue et al.2023)</td><td>Llama-2</td><td>49.6</td><td>33.2</td><td>32.8</td><td>32.4</td><td>26.0</td><td>17.2</td><td>10.8</td><td>4.8</td><td>1</td><td>3.6</td><td>2.4</td><td>21.3</td></tr><tr><td>WizardMath† (Luo et al.2023)</td><td>Llama-2</td><td>47.6</td><td>30.4</td><td>30.4</td><td>34.8</td><td>30.8</td><td>22.4</td><td>24.0</td><td>4.0</td><td>1</td><td>2.0</td><td>3.4</td><td>23.0</td></tr><tr><td>MathOctopus† (Chen et al.2023)</td><td>Llama-2</td><td>54.8</td><td>43.6</td><td>38.0</td><td>45.2</td><td>48.4</td><td>45.2</td><td>35.6</td><td>36.4</td><td>1</td><td>33.2</td><td>38.4</td><td>41.9</td></tr><tr><td>XCoT</td><td>Bloom</td><td>30.0</td><td>30.4</td><td>28.8</td><td>32.4</td><td>33.6</td><td>30.0</td><td>29.6</td><td>28.4</td><td>28.4</td><td>33.2</td><td>26.8</td><td>30.1</td></tr><tr><td>XCoT</td><td>Llama-2</td><td>48.4</td><td>47.2</td><td>49.6</td><td>48.8</td><td>50.0</td><td>50.0</td><td>50.0</td><td>49.2</td><td>42.8</td><td>40.4</td><td>48.4</td><td>47.7</td></tr><tr><td>Open-Source Models (13B)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LLama-2†(Touvron and Louis Martin 2023)</td><td>Llama-2</td><td>50.4</td><td>42.8</td><td>40.8</td><td>45.2</td><td>39.2</td><td>32.8</td><td>25.2</td><td>6.8</td><td>1</td><td>6.0</td><td>7.6</td><td>29.7</td></tr><tr><td>RFT† (Yuan et al.2023)</td><td>Llama-2</td><td>52.0</td><td>38.4</td><td>44.8</td><td>46.8 41.6</td><td></td><td>33.6</td><td>26.4</td><td>4.4</td><td>1</td><td>3.2</td><td>3.6</td><td>29.5</td></tr><tr><td>MAmmoth† (Yue et al.2023)</td><td>Llama-2</td><td>56.4</td><td>45.6</td><td>39.6</td><td>50.0</td><td>36.8</td><td>31.2</td><td>19.2</td><td>5.2</td><td>1</td><td>3.6</td><td>1.6</td><td>28.9</td></tr><tr><td>WizardMATH† (Luo et al.2023)</td><td>Llama-2</td><td>52.8</td><td>40.4</td><td>42.0</td><td>45.6</td><td>34.4</td><td>28.0</td><td>22.0</td><td>5.6</td><td>1</td><td>6.4</td><td>5.6</td><td>28.4</td></tr><tr><td>MathOctopus† (Chen et al.2023)</td><td>Llama-2</td><td>51.6</td><td>49.2</td><td>49.6</td><td>53.2</td><td>47.6</td><td>51.2</td><td>39.6</td><td>46.0</td><td>1</td><td>42.0</td><td>46.0</td><td>47.6</td></tr><tr><td>XCoT</td><td>Llama-2</td><td>54.4</td><td>52.4</td><td>46.4</td><td>54.8</td><td>56.8</td><td>54.0</td><td>49.6</td><td>50.0</td><td>47.2</td><td>50.0</td><td>51.6</td><td>51.5</td></tr></table></body></html>

Table 1: Multilingual evaluation results on the MGSM benchmark. †: Results from (Chen et al. 2023), for MathOctopus, we uniformly report the performance under xRFT and parallel-training settings.   
Table 2: Multilingual evaluation results on the MSVAMP benchmark. †: Results from (Chen et al. 2023), for MathOctopus, we uniformly report the performance under xRFT and parallel-training settings.   

<html><body><table><tr><td>Method</td><td>Base Model</td><td>En</td><td>De</td><td>Fr</td><td>Es</td><td>Ru</td><td>Zh</td><td>Ja</td><td>Th</td><td>Bn</td><td>Sw</td><td>Avg.</td></tr><tr><td>Closed-Source Models</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Native-CoTt</td><td>GPT-3.5</td><td>81.2</td><td>73.9</td><td>78.2</td><td>74.6</td><td>70.9</td><td>78.4</td><td>74.0</td><td>46.0</td><td>14.4</td><td>68.4</td><td>66.0</td></tr><tr><td>Native-CoTt</td><td>GPT-4</td><td>80.1</td><td>78.1</td><td>83.9</td><td>81.5</td><td>77.9</td><td>78.9</td><td>74.8</td><td>68.1</td><td>31.2</td><td>75.7</td><td>73.0</td></tr><tr><td>Open-Source Models (7B)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Llama-2†(Touvron and Louis Martin 2023)</td><td>Llama-2</td><td>38.8</td><td>39.0</td><td>39.1</td><td>39.2</td><td>39.1</td><td>35.2</td><td>31.6</td><td>18.2</td><td>11.5</td><td>17.2</td><td>30.9</td></tr><tr><td>RFTt (Yuan et al. 2023)</td><td>Llama-2</td><td>42.7</td><td>40.8</td><td>41.5</td><td>42.5</td><td>39.5</td><td>34.9</td><td>33.9</td><td>16.9</td><td>7.7</td><td>14.9</td><td>31.5</td></tr><tr><td>MAmmoTH† (Yue et al. 2023)</td><td>Llama-2</td><td>45.1</td><td>39.6</td><td>39.9</td><td>42.9</td><td>33.7</td><td>26.8</td><td>26.7</td><td>6.3</td><td>4.3</td><td>4.2</td><td>27.0</td></tr><tr><td>WizardMath† (Luo et al. 2023)</td><td>Llama-2</td><td>48.5</td><td>39.2</td><td>37.7</td><td>44.8</td><td>37.4</td><td>36.3</td><td>37.9</td><td>17.0</td><td>16.1</td><td>10.3</td><td>32.5</td></tr><tr><td>MathOctopus† (Chen et al.2023)</td><td>Llama-2</td><td>46.8</td><td>43.1</td><td>45.3</td><td>44.5</td><td>42.1</td><td>43.2</td><td>43.2</td><td>40.5</td><td>32.8</td><td>42.3</td><td>42.4</td></tr><tr><td>XCoT</td><td>Llama-2</td><td>47.3</td><td>44.3</td><td>42.8</td><td>44.0</td><td>43.9</td><td>43.4</td><td>43.7</td><td>39.8</td><td>38.0</td><td>41.7</td><td>42.9</td></tr></table></body></html>

and low-resource sample $( c ^ { L _ { k , j } } , q ^ { L _ { k } } , a ^ { L _ { j } } )$ , the model separately predict the target distribution $\begin{array} { r l } { P _ { h i g h } } & { { } = } \end{array}$ $p ( a ^ { L _ { j } } | _ { \mathbf { c } } c ^ { L _ { i , j } } , q ^ { L _ { i } } )$ and $P _ { l o w } = p ( a ^ { L _ { j } } | c ^ { L _ { k , j } } , q ^ { L _ { k } } )$ . Since $q ^ { L _ { i } }$ and $q ^ { L _ { k } }$ are semantically equal, we can use distribution $P _ { h i g h }$ to supervise $P _ { l o w }$ in token level:

$$
\mathcal { L } _ { d } = - \frac { 1 } { n } \sum _ { t = 1 } ^ { n } \left[ P _ { h i g h } ^ { t } \log P _ { l o w } ^ { t } \right]
$$

the English question, we randomly select another language as the target language.

We implement our model based on Llama-2-7B, Llama2-13B, and Bloom-7b1. We finetune these models with 3 epochs and use a cosine scheduler with a learning rate of 2e-5 and set $3 \%$ warm up. For cross-lingual distillation, the weight $\beta$ of the distillation loss is set to 0.3.

where $P _ { h i g h } ^ { t }$ and $P _ { l o w } ^ { t }$ is the $t$ -token distribution in answer. Through the token-level cross-lingual supervision, we transfer the knowledge to low-resource languages.

# Experiments Cross-lingual Supervised Fine-tuning

For each question in the dataset, we randomly select 2 other questions and corresponding answers as the context. We set $0 ~ < ~ \alpha ~ < ~ 1$ with a 0.8 replacement threshold to perform the code-switch operation on the question in context. For the selection of language in the codeswitch operation, we use English as the source language, and the language corresponding to the question is used as the target language. For

# Evaluation

To comprehensively assess the cross-lingual proficiency of XCOT, we evaluate the method using the MGSM (Shi et al. 2023) benchmark, which extends the English GSM8K (Cobbe et al. 2021) dataset into ten typologically varied languages through the manual translation of problems. To conduct a thorough and wide-ranging evaluation of the multilingual mathematical problem-solving skills, we also evaluate our method on MSVAMP (Chen et al. 2023), originating from the SVAMP (Patel, Bhattamishra, and Goyal 2021) dataset. This dataset incorporates mathematical problems in 10 different languages, initially translated using machine translation and subsequently refined through careful

<html><body><table><tr><td></td><td></td><td colspan="5">Llama-2-7B</td><td colspan="5">Llama-2-13B</td></tr><tr><td>k</td><td>De</td><td>Fr</td><td>Es</td><td>Ru</td><td>Zh</td><td>De</td><td>Fr</td><td>Es</td><td>Ru</td><td>Zh</td><td>Avg.</td></tr><tr><td>10</td><td>1.68</td><td>1.67</td><td>1.68</td><td>1.80</td><td>1.69</td><td>1.98</td><td>1.96</td><td>1.98</td><td>1.98</td><td>1.96</td><td>7.22</td></tr><tr><td>20</td><td>2.58</td><td>2.56</td><td>2.59</td><td>2.66</td><td>2.59</td><td>2.95</td><td>2.95</td><td>2.97</td><td>2.97</td><td>2.94</td><td>11.02</td></tr><tr><td>30</td><td>3.21</td><td>3.24</td><td>3.21</td><td>3.35</td><td>3.22</td><td>3.70</td><td>3.71</td><td>3.70</td><td>3.73</td><td>3.69</td><td>13.93</td></tr><tr><td>50</td><td>4.32</td><td>4.29</td><td>4.34</td><td>4.47</td><td>4.35</td><td>4.93</td><td>4.93</td><td>4.91</td><td>4.72</td><td>4.91</td><td>18.43</td></tr></table></body></html>

Table 3: Distinct reasoning paths of each language with different sampling times. Different reasoning paths per question are generated by different SFT models with different $k$ .

human review and correction for accuracy and nuance. Our method is evaluated on MGSM and MSVAMP with the accuracy metric. In the experiments, we report the accuracy of all methods.

# Baselines

XCOT are mainly compared with: (1) close-source LLM GPT-3.5, GPT-4; (2) open-source models Llama-2 and Bloom. XCOT primarily conduct experiments based on Llama-2 and compare with other Llama-2-based methods RFT, MathOctopus, MAmmoTH, WizardMath, etc. Furthermore, we select Bloom as the base model to explore the performance of XCOT when combined with multilingual LLM.

# Main Results

MGSM Table 1 presents the results of our method and previous baselines on MGSM of 11 languages, including En, De, En, Fr, Es, Ru, Zh, Ja, Th, Te, Bn, Sw. Compared to the open-source baseline Llama-2, MAmmoTH (Yue et al. 2023) trained with a union of chain-of-thought (CoT) and program-of-thought (PoT) gains strong improvement. Our method significantly outperforms the previous strong baseline MAmmoTH by an average of points. It can prove that our method can leverage cross-lingual in-context learning (xICL), random online CoT (Random-CoT), and crosslingual distillation (xDistill) to encourage alignment across different languages.

MSVAMP Table 2 compares the performance of our method with previous relevant methods on MSVAMP of 10 languages. The recent strong multilingual baseline MathOctopus beats the previous baselines MAmmoth and WizardMath with the help of the multilingual instruction dataset MGSM8KInstruct. Further, our proposed method gains the best performance of 42.9 points in 7B level across all languages, demonstrating that our proposed framework strengthens transferability from the high-resource languages to all other languages.

# Analysis

Ablation Study To verify the effectiveness of each module in our method, we conduct an ablation study by gradually adding modules. In addition, Llama- $2  { - } 7  { \mathbf { B } } +  { \mathbf { \ " } }$ Translate serves as the baseline model, fine-tuned only on GSM8K. At the time of testing, MGSM data was first translated into English using Google Translate. The multilingual LLM Llama-7B is first trained on the multilingual corpora XCOT-INSTRUCT, where the model is denoted as $\textcircled{1}$ . Compared to the initial model $\textcircled{1}$ , the model $\textcircled{2}$ with the code-switched context in multilingual tuning gains the improvement of $+ 4 . 7$ points on average, which shows the usage of xICL in encouraging alignment across different languages. Then, the model $\textcircled{3}$ is further enhanced with mSampling by a large margin $+ 5 . 8$ points, where the model generates the multilingual responses and chooses correct reasoning paths as the augmented dataset. During multilingual tuning, our method adopts Random-CoT to first translate the query to another language and then answer in English. For the output distribution, the high-resource distribution is used to supervise the low-resource distribution (xDistill). Putting them all together, we obtain the final model XCOT $\textcircled{5}$ with 47.7 points. Table 4 summarizes the results of the ablation study of cross-lingual transfer in different parts, which emphasizes the effectiveness of cross-lingual transfer that can gradually improve performance in different aspects. Compared with the Google Translate method, XCOT has significant improvements.

Cross-lingual Prompting To trigger the cross-lingual potential capability of LLM, we introduce xICL to force the model to understand the multilingual queries and align their representations. To advance multilingual agreement in instruction tuning, we randomly replace some fragments of source languages in examples with their counterpart translations of target languages for cross-lingual in-context fewshot learning (xICL). Table 6 shows the results of XCOT with English context, native context, and code-switched context on different backbones. The query mixed with different language tokens brings significant improvement in various languages.

Cross-lingual Reasoning Path Our multilingual instruction data is augmented by multilingual sampling, where the fine-tuned LLM generates the response and selects the correct path. Table 3 shows that different languages have a similar number of reasoning paths, which proves that using the cross-lingual CoT successfully transfers reasoning patterns from one language to another language. XCOT can accumulate all reasoning paths to improve the model performance.

Multilingual Representations We randomly select 250 parallel queries with their 2-shot examples of each language in XCOT-INSTRUCT and visualize their representations (Maaten and Hinton 2008) of the last Llama decoder layer in Figure 3 using our multilingual model fine-tuned on XCOT-INSTRUCT and the multilingual baseline. The first hidden state of the layers is adopted as the sentence representation. Compared to Figure 3(a) of the baseline, different languages become closer and more likely to overlap with each other in Figure 3(b) of our method, demonstrating that our method effectively aligns representations of different languages to the shared space.

Understanding and Reasoning in English After the cross-lingual SFT with Random-CoT, XCOT chooses the high-resource language (English) as the auxiliary language to understand and answer the non-English question. In Figure 4, our method uses “Let’s think step by step in En

Table 4: Ablation study on MGSM based on Llama-2-7b. $\textcircled{5}$ is the final model of our method(XCOT) .   

<html><body><table><tr><td>ID</td><td>Method</td><td>En</td><td>De</td><td>Fr</td><td>Es</td><td>Ru</td><td>Zh</td><td>Ja</td><td>Th</td><td>Te</td><td>Bn</td><td>Sw</td><td>AVG</td></tr><tr><td></td><td>Llama-2-7B+ Translate</td><td>42.4</td><td>39.2</td><td>36.8</td><td>39.2</td><td>40.0</td><td>39.6</td><td>38.0</td><td>32.0</td><td>36.0</td><td>39.2</td><td>40.4|</td><td>38.4</td></tr><tr><td>①</td><td>Llama-2-7B</td><td>38.4</td><td>37.2</td><td>37.6</td><td>39.2</td><td>37.6</td><td>30.4</td><td>29.6</td><td>26.8</td><td>13.2</td><td>23.2</td><td>18.4</td><td>30.1</td></tr><tr><td>②</td><td>+ xICL</td><td>45.6</td><td>38.4</td><td>33.6</td><td>36.4</td><td>40.4</td><td>37.2</td><td>32.0</td><td>32.0</td><td>26.0</td><td>30.0</td><td>32.0</td><td>34.8</td></tr><tr><td>③</td><td>+mSampling</td><td>50.8</td><td>41.2</td><td>42.0</td><td>44.4</td><td>42.0</td><td>40.4</td><td>38.0</td><td>40.8</td><td>35.6</td><td>34.8</td><td>37.6</td><td>40.6</td></tr><tr><td>④</td><td>+Random-CoT</td><td>48.0</td><td>50.4</td><td>46.0</td><td>48.4</td><td>46.8</td><td>51.2</td><td>46.4</td><td>47.2</td><td>41.6</td><td>41.2</td><td>46.8</td><td>46.7</td></tr><tr><td>⑤</td><td>+ xDistill</td><td>48.4</td><td>47.2</td><td>49.6</td><td>48.8</td><td>50.0</td><td>50.0</td><td>50.0</td><td>49.2</td><td>42.8</td><td>40.4</td><td>48.4</td><td>47.7</td></tr></table></body></html>

<html><body><table><tr><td>Random-CoTDirection</td><td>En</td><td>De</td><td>Fr</td><td>Es</td><td>Ru</td><td>Zh</td><td>Ja</td><td>Th</td><td>Te</td><td>Bn</td><td>Sw</td><td>Avg.</td></tr><tr><td>La𝑙→En</td><td>50.8</td><td>41.2</td><td>42.0</td><td>44.4</td><td>42.0</td><td>40.4</td><td>38.0</td><td>40.8</td><td>35.6</td><td>34.8</td><td>37.6</td><td>40.6</td></tr><tr><td>Llow →EnULhigh →Lhigh</td><td>50.8</td><td>45.6</td><td>49.6</td><td>46.4</td><td>49.6</td><td>41.2</td><td>42.4</td><td>42.8</td><td>37.6</td><td>38.8</td><td>42.8</td><td>44.3</td></tr><tr><td>Lau→Lall</td><td>46.0</td><td>46.4</td><td>45.2</td><td>50.0</td><td>48.0</td><td>50.4</td><td>47.6</td><td>43.2</td><td>40.8</td><td>40.4</td><td>45.2</td><td>45.7</td></tr><tr><td>Lall→Lhigh</td><td>48.4</td><td>47.2</td><td>49.6</td><td>48.8</td><td>50.0</td><td>50.0</td><td>50.0</td><td>49.2</td><td>42.8</td><td>40.4</td><td>48.4</td><td>47.7</td></tr></table></body></html>

Table 5: Study on translation direction of Random-CoT. $L _ { a l l }$ , $L _ { h i g h }$ and $L _ { l o w }$ denote all languages, high-resource languages, and low-resource languages, respectively.

![](images/1e0486d47c6ba567c5f7f2a97bc63750e8ba44ed74f90ed8066c1aaf34f99771.jpg)  
Figure 3: (a) and (b) are representations of Llama-7B and our method from the last decoder layer. Each color denotes one language (11 languages in MGSM).   
Figure 4: The prompt of Random-CoT.

glish” to answer the English question. For the non-English question, we adopt “Let’s think the question in $\{ L a n g u a g e \}$ and then think step by step in English”, where $\{ L a n g u a g e \}$ can be high-resource languages in SFT tuning but we set $\{ L a n g u a g e \}$ as English during inference stage. To effectively transfer knowledge from high-resource to lowresource languages, we force LLM to understand the query in English and then think in English.

Analysis in Random-CoT To facilitate the alignment among different languages, the question of language $L _ { i _ { 1 } }$ is first translated into another language $L _ { i _ { 2 } }$ in SFT tuning. Given the query of language $L _ { i _ { 1 } }$ , we can translate another language $L _ { i _ { 2 } }$ ( $L _ { i _ { 1 } } \neq L _ { i _ { 2 } } { \mathrm { ~ } }$ ). $L _ { a l l }$ denotes all languages; $L _ { h i g h }$ contains En, De, Fr, Es, Ru, Zh, Ja; $L _ { l o w }$ contains Th, Te, Bn, Sw. For example, the strategy $\cdot { { L } _ { a l l } } \to { { L } _ { h i g h } } ^ { , , }$ denotes that the $L _ { i _ { 1 } } \in L _ { a l l } \land L _ { i _ { 2 } } \in L _ { h i g h } .$ Table 5 shows the results of our method with different Random-CoT strategies and the strategy $\cdot { \cal L } _ { a l l } \to { \cal L } _ { h i g h } \cdot$ ” gets the best performance, which can be attributed the language transfer from high-resource to low-resource languages.

Data Efficiency Figure 5 plots the multilingual evaluation results of XCOT with different SFT data sizes. We observe that our method with nearly $20 \%$ SFT data can still beat the strong baseline Llama-2-7B, which can attributed to the mutual reinforcement of multiple languages.

# (a) Prompt for English Query

Below is an instruction that describes a task. Write a   
response that appropriately completes the request.   
### Instruction:   
{Question}   
### Response:   
Let's think step by step in English.   
(b) Prompt for Other Language Query   
Below is an instruction that describes a task. Write a response that appropriately completes the request.   
### Instruction:   
{Question}   
### Response:   
Let's think the question in {Language} and then think step by step in English.

Cases Study Given the queries of different languages, our method prompts LLM to first consider the multilingual query in English and then answer in English. Table 7 shows examples of the Spanish and German baseline. We observe that Llama-2 tends to generate incorrect answers for nonEnglish queries. For the German Question “Jimmy hat 2 $\$ 5$ mehr als doppelt so viel Geld wie Ethel. Wenn Ethal 8 $\$ 5$ hat, wie viel Geld hat dann Jimmy im Moment?”, our method first thinks the non-English query in English “Question: Jimmy has $\$ 2$ more than twice the money Ethel has. If Ethel has $\$ 8$ , how much money does Jimmy have now?”. and then answer in English. It proves that our method can align both query and response across different languages.

# Related Work

# Large Language Models

Large language models (LLMs) have shown great power in NLP tasks, and as the scale of the model gets larger, LLMs emerge with surprising capabilities (Touvron and

Table 6: The results of XCOT with different context prompting strategies.   

<html><body><table><tr><td>Base Model</td><td>CoT</td><td>En</td><td>De</td><td>Fr</td><td>Es</td><td>Ru</td><td>Zh</td><td>Ja</td><td>Th</td><td>Te</td><td>Bn</td><td>Sw</td><td>AVG</td></tr><tr><td rowspan="3">Bloom-7B</td><td>En-Context</td><td>32.8</td><td>31.2</td><td>30.4</td><td>30.0</td><td>34.0</td><td>29.6</td><td>28.0</td><td>27.6</td><td>27.2</td><td>30.4</td><td>28.4</td><td>29.9</td></tr><tr><td>Native-Context</td><td>32.8</td><td>30.0</td><td>25.6</td><td>32.0</td><td>31.2</td><td>30.0</td><td>25.2</td><td>28.8</td><td>25.6</td><td>30.0</td><td>30.4</td><td>29.2</td></tr><tr><td>Codeswitch-Context</td><td>30.0</td><td>30.4</td><td>28.8</td><td>32.4</td><td>33.6</td><td>30.0</td><td>29.6</td><td>28.4</td><td>28.4</td><td>33.2</td><td>26.8</td><td>30.1</td></tr><tr><td rowspan="3">Llama-2-7B</td><td>En-Context</td><td>50.0</td><td>46.4</td><td>50.0</td><td>47.2</td><td>47.6</td><td>50.4</td><td>47.6</td><td>44.8</td><td>42.0</td><td>39.2</td><td>48.4</td><td>46.7</td></tr><tr><td>Native-Context</td><td>50.0</td><td>48.4</td><td>44.0</td><td>48.8</td><td>47.6</td><td>46.4</td><td>45.6</td><td>45.6</td><td>42.0</td><td>41.2</td><td>44.4</td><td>45.8</td></tr><tr><td>Codeswitch-Context</td><td>48.4</td><td>47.2</td><td>49.6</td><td>48.8</td><td>50.0</td><td>50.0</td><td>50.0</td><td>49.2</td><td>42.8</td><td>40.4</td><td>48.4</td><td>47.7</td></tr><tr><td rowspan="3">Llama-2-13B</td><td>En-Context</td><td>50.8</td><td>50.4</td><td>48.0</td><td>50.4</td><td>55.6</td><td>48.4</td><td>48.0</td><td>54.0</td><td>44.8</td><td>45.6</td><td>49.2</td><td>49.5</td></tr><tr><td>Native-Context</td><td>50.8</td><td>52.0</td><td>49.2</td><td>49.2</td><td>56.0</td><td>50.0</td><td>49.6</td><td>52.8</td><td>46.0</td><td>45.2</td><td>48.8</td><td>49.9</td></tr><tr><td>Codeswitch-Context</td><td>50.8</td><td>51.6</td><td>49.2</td><td>50.8</td><td>56.0</td><td>50.0</td><td>48.8</td><td>54.0</td><td>44.8</td><td>46.8</td><td>50.0</td><td>50.2</td></tr></table></body></html>

Table 7: Cases study on Llama-2 and our proposed method within 7B-level on MGSM.   

<html><body><table><tr><td></td><td>Spanish QuestionDan planta 3rosales.Cada rosal tiene 25 rosas.Cada rosa tiene 8espinas.iCuantas espinas hay en total?</td></tr><tr><td rowspan="2">Llama-2</td><td>There are 3x 25=75 roses.There are 75 x8= 600 thorns.</td></tr><tr><td>There are 600 x 2=1200 forks.The answer is 1200.×</td></tr><tr><td rowspan="3">XCoT</td><td>Question:Dan plants 3 rose bushes.Each rose bush has 25 roses.Each rose has 8 thorns.</td></tr><tr><td>How many thorns are there in total?Let's think step by step in English.</td></tr><tr><td>There are 25*3=i25*3=75i75 roses So there are 75*8=j75*8=600i600 thorns.The answer is 600.</td></tr><tr><td></td><td>GermanQuestionJimmyhat2$mehralsdopeltsovielGeldwieEthel.WennEthal8$hat,wievielGeldhatdannJimmyimMoment?</td></tr><tr><td>Llama-2</td><td>Jimmy's money is 2* $8= $16.The answer is 16.X</td></tr><tr><td>xCoT</td><td>Question:Jimmy has $2 more than twice the money Ethel has.IfEthel has $8,how much money does Jimmyhave now? Let's think step by step in English.Twice the money Ethelhas is 2 *8=i2*8=1616 dollars.Jimmy has 2 more than twice the money Ethel has,which means he has16+2=ii16+2=18ii18 dollars.The answer is 18.</td></tr></table></body></html>

![](images/f59a87611107870c4197cb2f4dcc786c243b91b4c1cc25c8d5f2663c5880c409.jpg)  
Figure 5: Multilingual evaluation results on MGSM with different SFT data size.

Louis Martin 2023; Wei et al. 2022b; Du et al. 2022; Guo et al. 2023), such as following human instructions, incontextual learning, and reasoning complex tasks. Wei et al. (2022c); Kojima et al. (2022) found that LLM can solve complex problems efficiently by chain-of-thought prompting strategy. Some works (Yuan et al. 2023; Shao et al. 2024; Yue et al. 2023; Yu et al. 2023) further improve the LLM’s reasoning capabilities by fine-tuning. In this paper, we focus on using cross-lingual corpus to enhance the cross-lingual reasoning capabilities of LLM.

# Cross-lingual Transfer

Cross-lingual transfer pertains to utilizing labeled data from a resource language to address the challenge of insufficient labeled data in the target language. Previous works (Conneau and Lample 2019; Conneau et al. 2020; Yang et al. 2020; Ma et al. 2020; Yang et al. 2023) demonstrate that pretrained models trained on multi-lingual data proficiently perform cross-lingual transfer tasks. These multi-lingual pretrained models have found extensive application across various downstream NLP tasks, such as translation (Tan et al. 2019; Yang et al. 2022b; Gaschi et al. 2023; Yang et al. 2022c), summarization (Bhattacharjee et al. 2023; Wang et al. 2023), information extraction (Zhou et al. 2022; Yang et al. 2022a; Li et al. 2023; Wu et al. 2020), spell checking (Bexte et al. 2022; Sharma et al. 2023; Li et al. 2022, 2024). Many LLMs (Scao et al. 2022; Muennighoff et al. 2022) are trained on multilingual data, endowing them with strong cross-linguistic abilities.

# Conclusion

We propose a cross-lingual instruction fine-tuning framework (XCOT) to address the disparity by encouraging alignment among different languages. A cross-lingual instruction dataset is created to align reasoning capability across various languages semantically. Our method incorporates crosslingual in-context Learning to trigger the cross-lingual alignment. During instruction tuning, we adopt random online CoT, which prompts LLM to translate the query into different languages and subsequently provide an English response. To further promote language transfer, we leverage a high-resource CoT to guide low-resource CoT training with cross-lingual distillation. Comprehensive evaluations demonstrate XCOT effectively reduces the multilingual linguistic gap, highlighting its potential as a robust solution.