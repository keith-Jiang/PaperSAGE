# Audio Entailment: Assessing Deductive Reasoning for Audio Understanding

Soham Deshmukh1,2, Shuo $\mathbf { H a n } ^ { 1 }$ , Hazim Bukhari1, Benjamin Elizalde2, Hannes Gamper3, Rita Singh1, Bhiksha Raj1

1Carnegie Mellon University 2Microsoft 3Microsoft Research sdeshmuk, shuohan, hbukhari @andrew.cmu.edu

# Abstract

Recent literature uses language to build foundation models for audio. These Audio–Language Models (ALMs) are trained on a vast number of audio–text pairs and show remarkable performance in tasks including Text-to-Audio Retrieval, Captioning, and Question Answering. However, their ability to engage in more complex open-ended tasks, like Interactive Question-Answering, requires proficiency in logical reasoning—a skill not yet benchmarked. We introduce the novel task of Audio Entailment to evaluate an ALM’s deductive reasoning ability. This task assesses whether a text description (hypothesis) of audio content can be deduced from an audio recording (premise), with potential conclusions being entailment, neutral, or contradiction, depending on the sufficiency of the evidence. We create two datasets for this task with audio recordings sourced from two audio captioning datasets—AudioCaps and Clotho—and hypotheses generated using Large Language Models (LLMs). We benchmark state-of-the-art ALMs and find deficiencies in logical reasoning with both zero-shot and linear probe evaluations. Finally, we propose “caption-before-reason”, an intermediate step of captioning that improves the Zero-Shot and linear-probe performance of ALMs by an absolute $6 \%$ and $3 \%$ , respectively.

Datasets — https://github.com/microsoft/AudioEntailment

# 1 Introduction

Recent literature uses language to build foundation models for audio. These models, referred to as Audio–Language Models, are trained on millions of audio–text pairs using either Contrastive Learning (e.g., CLAP (Elizalde et al. 2023; Wu et al. 2023)) or Next-Token Prediction (e.g., Pengi (Deshmukh et al. 2023), Qwen-Audio (Chu et al. 2023)). Once trained, ALMs can perform multiple tasks grounded in audio and user-provided instructions, for example text-toaudio retrieval, captioning, question-answering, and text-toaudio generation. Owing to their performance, support for various tasks, and inherent ease-of-use, ALMs are being extensively used across various scenarios.

ALMs have achieved state-of-the-art (SoTA) performance on close-ended tasks like Classification and Retrieval, beating Self-Supervised Learning (SSL) models as well as Supervised models. The latest ALMs efforts (Chu et al. 2023;

Gong et al. 2023a; Tang et al. 2024) focus on improving open-ended text generation. The task (Deshmukh et al. 2023) consists of generating free-form text, given an audio and a text input, and has flexibility in the correctness of the output. For instance, an audio recording labeled as “dog barking” can be identified by the ALMs as “canine barking” and still be marked as correct. The open-ended text generation for ALMs usually takes the form of interactive question-answering with the user. From a Machine Learning perspective, one can think of a model performing different tasks of Audio Captioning, Audio Question Answering, Audio Dialogues, and Reasoning, to enable interactive Question-Answering. To generate natural and accurate responses, the ALMs should have learned to think step-bystep, utilize the learned real-world knowledge, and have the ability to ask follow-up questions for clarifications about the acoustic content. ALMs are evaluated on such abilities through Audio Question Answering tasks. Although the performance has been promising, ALMs do not perform well on interactive Question-Answering. Hence, we introduce a new direction to evaluate a specific type of reasoning of ALMs called Logical Reasoning.

Logical Reasoning (Copi, Cohen, and McMahon 2016) can be defined in the context of a premise and a hypothesis. To perform Logical Reasoning, one needs a comprehension of premises, the relationships among premises, and then use rigorous methods to infer conclusions that are implied by the premises and relations. Deductive reasoning, a form of Logical Reasoning, is useful where the premises are known to be true, as it allows for drawing specific conclusions from general principles. Deductive reasoning in audio perception involves a “top-down” approach, where one begins with hearing an audio and then determines if a logical conclusion can be drawn. For instance, an audio contains a dog barking and children playing. The hypothesis is “children playing in the park with a dog barking nearby.” Thus, we can conclude the hypothesis is plausible, as parks can be associated with these sounds. Evaluating deductive reasoning also helps in identifying audio hallucinations. They may manifest in two ways: (1) Inferred Cues: The model generates cues not present in the audio input, such as introducing audio events that were neither mentioned nor implied. (2) Contextual Events: The model relies on contextual assumptions rather than audio evidence, for example, interpreting a sound as “dog barking”

Zero-Shot performance of Audio-Language Models 1010 parameters trainingpairs 0.4 108 Gegf 0.2 106 0.0 A GA GA M GU M AudioEntailmenttask 0.70 Hypothesis (text) 0.65 Proposed approach   
→ 0.60 Entailment   
Premise ALM Contradiction 0.55   
(audio) Neutral 0.50 0.45 ACCP RF1

because the word “dog” is usually followed by “barking”, while the audio more accurately suggests “whimpering” or other actions. By benchmarking ALMs for deductive reasoning, we can uncover audio hallucinations.

In this work, we study Logical Reasoning for ALMs. Our contributions are:

• We introduce the task of Audio Entailment to test the Deductive Reasoning ability of ALMs. The task determines if a textual hypothesis $\mathcal { H }$ can be concluded from an audio premise $\mathcal { P }$ . The conclusion can be entailment, neutral, or contradiction based on the evidence. We created two datasets, ACE and CLE, where Hypotheses were first generated by GPT-4 and then verified and corrected by human annotators. This two-step process enhances the quality of the datasets, which will be publicly released.   
• We benchmark SoTA ALMs, showing they have limited deductive reasoning. We test both contrastive and nexttoken prediction ALMs in Zero-Shot and linear-probe setups and highlight ways to enhance audio-grounded reasoning.   
• Based on our findings, we propose “caption-beforereason” which performs intermediate captioning before reasoning, improving zero-shot and linear-probe performance by an absolute $6 \%$ and $3 \%$ , respectively.

# 2 Related Work

Audio-Language Models. The early models focused on close-ended tasks. For example, CLAP (Elizalde, Deshmukh, and Wang 2024; Wu et al. 2023; Dhamyal et al. 2024) is contrastively trained on millions of audio-text pairs and learns multimodal representations that can be used for closeended tasks like Zero-Shot classification and retrieval. With the success of CLAP, later ALMs focused on tackling openended tasks, like Audio Captioning or Audio Question Answering (AQA). For example, Pengi (Deshmukh et al. 2023) and LTU (Gong et al. 2023b) concurrently framed all audio tasks as audio-and-text input to text output tasks. In terms of architecture, Pengi and LTU jointly train an audio encoder with a frozen or near-frozen LLM. Each is capable of producing text based on audio inputs and text prompts. The subsequent generation of ALMs focus on performing joint speech–audio understanding and utilize larger training data and LLMs. For example, Qwen-Audio (Chu et al. 2023), LTU-AS (Gong et al. 2023a), GAMA (Ghosh et al. 2024b), AudioFlamingo (Kong et al. 2024) and SALMONN (Tang et al. 2024) beat existing ALMs on 30 different tasks, each showcasing unique strengths and weaknesses.

Audio Question Answering (AQA). The task involves analyzing an audio signal and a question to provide accurate answers. There are two AQA datasets in the literature to train and test ALMs. (1) ClothoAQA (Lipping et al. 2022) is a crowdsourced dataset consisting of 1991 audio files, selected from the Clotho dataset (Drossos, Lipping, and Virtanen 2020). It includes a set of six different questions and corresponding answers for each audio file, which were collected through crowdsourcing using Amazon Mechanical Turk. (2) OpenAQA (Gong et al. 2023b) combines 5 different datasets from the literature and converts them into a triplet format of: audio input, text prompt, and text output. It includes 1.9M close-ended questions and 3.7M open-ended questions generated with the help of GPT-3.5-Turbo (Brown et al. 2020). However, neither dataset evaluates deductive Reasoning.

Text and Visual Entailment. Natural Language Inference (MacCartney 2009; Dagan, Glickman, and Magnini 2005), also known as Textual Entailment, is a concept in Natural Language Processing that involves determining the relationship between two text fragments. The relationship is directional and holds whenever the truth of one text fragment (the premise) follows from another text (the hypothesis). For example, if the premise is “The cat sat on the mat”, and the hypothesis is “There is a cat on a mat”, then we can infer that the hypothesis is true given the premise. Visual Entailment (Xie et al. 2019; Do et al. 2020) extends this to the vision domain where the image is the premise and a text fragment is the hypothesis. The task is to predict whether the image semantically entails the text. This type of reasoning is shown to be crucial for fine-grained image understanding (Thomas, Zhang, and Chang 2022). Recent research has identified perception gaps in reasoning (Ghosh et al. 2024a).

# 3 Audio Entailment

Entailment (Routley and Meyer 1973; Anderson, Belnap Jr, and Dunn 2017) holds when there is a directional relationship between the premise $( \mathcal { P } )$ and hypothesis $( { \mathcal { H } } )$ . Specifically, for our work, we use a relaxed definition: “p entails h” $\mathcal { P } \Rightarrow \mathcal { H }$ ) if, typically, $\pmb { a }$ human observing $\mathcal { P }$ would infer that $\mathcal { H }$ is most likely true. This relation is directional, meaning that even if $\mathcal { P } \Rightarrow \mathcal { H }$ , the reverse $\mathcal { H } \Rightarrow \mathcal { P }$ is uncertain. Entailment helps determine whether a hypothesis logically follows from the premise, allowing us to infer relationships between premise and hypothesis fragments. We consider various definitions of audio entailment, and specifically choose a definition based on inferential analysis (details in Appendix).

In Audio Entailment, the premise $\mathcal { P }$ is audio recorded inthe-wild and the hypothesis $\mathcal { H }$ is a natural language description. The aim of the Audio Entailment task is to determine if the hypothesis $\mathcal { H }$ can be concluded by a human listening to the audio recording premise $P$ . This leads us to the following three scenarios (Fig. 2):

• Entailment is determined when the audio recording $\mathcal { P }$ contains sufficient evidence to affirm the truth of the hypothesis $\mathcal { H }$ .   
• Neutral holds when the audio recording $\mathcal { P }$ does not provide enough information to either confirm or deny the hypothesis $\mathcal { H }$ . Simply put, while $\mathcal { H }$ may be true, it cannot be substantiated solely from the audio recording $\mathcal { P }$ .   
• Contradiction is determined when the audio recording $\mathcal { P }$ offers substantial evidence to deduce that the hypothesis $\mathcal { H }$ is false.

Sounds of water $\mathcal { H } _ { 1 }$ interacting with a $$ Entailment surface are present. P $\longrightarrow \mathcal { H } _ { 2 }$ A boat is being $$ Neutral gentlypulled along a quiet beachfront. Weshingion:waterthathis $\mathcal { H } _ { 3 }$ Aoatbpatispeedir $$ Contradiction shoreataveryslowrate through the water. The sound of $\mathcal { H } _ { 1 }$ escalating knocking $$ Entailment ona door is heard $\mathcal { P }$ A person is urgently l $\longrightarrow \mathcal { H } _ { 2 }$ trying to get someone'sattention $$ Neutral by knocking on a Description:Someone is door knocking on a doorand $\mathcal { H } _ { 3 }$ The sound of a steady more intenselyas time and calm knocking on →Contradiction goes on. a door is heard

# 3.1 Audio Entailment as a Classification Task

We formulate the Audio Entailment task as a classification task. The input consists of $\{ a _ { i } , h _ { i } \}$ , with audio premise $a _ { i }$ and hypothesis $h _ { i }$ , and the target is to predict $\bar { \{ } c \}$ , where $c \in \{$ entailment, neutral, contradiction . To make an accurate prediction $c$ , the model has to understand the relation between $a _ { i }$ and $h _ { i }$ , enforcing and verifying a step of logical reasoning.

# 4 Audio Entailment Datasets

In this section, we describe the creation of AudioCaps Entailment (ACE) and Clotho Entailment (CLE).

# 4.1 Audio Premise

The premise $\mathcal { P }$ for Audio Entailment is a real-world audio recording. We source audio files and their corresponding natural language annotations from two Audio Captioning datasets, AudioCaps (Kim et al. 2019) and Clotho (Drossos, Lipping, and Virtanen 2020).

AudioCaps. The AudioCaps dataset comprises 46,000 audio samples sourced from AudioSet, each labeled with a single caption. These captions were collected through the Amazon Mechanical Turk (AMT) crowdsourcing platform, complemented by automated checks for the quality of annotations. Annotators were given the word labels from AudioSet and had access to the corresponding videos for the audio clips they were annotating. It should be noted that providing annotators access to visuals may introduce bias if annotators focus on the visual elements rather than the auditory ones. Furthermore, limiting the data to a single caption for each file hinders the ability of ALMs to learn and assess a wide range of descriptions. Finally, as AudioCaps derives its content from YouTube, there has been a gradual loss of videos over time, resulting in the unavailability of certain audio files. To counteract some of these limitations, we rely on Clotho as an additional dataset.

Clotho. The Clotho audio collection is obtained from the Freesound platform. This platform enables individuals to share their audio recordings and accompany them with descriptions. These recordings range in length from 15 to 30 seconds. For each audio clip, there are five captions, each containing 8 to 20 words. These captions are gathered using AMT, following a detailed protocol for crowdsourcing audio captions to promote variety and minimize grammatical mistakes. The annotators had access solely to the audio tracks, without any additional context such as video or textual tags, during the annotation process.

Other existing datasets (SoundDescs (Koepke et al. 2022), MACs (Mart´ın-Morat´o and Mesaros 2021) and WT5K (Deshmukh, Elizalde, and Wang 2023)) do not contain human annotations and are therefore not considered for building the first version of the audio entailment dataset.

# 4.2 Hypothesis

From Clotho and AudioCaps, we obtain audio recordings and the natural language description of the audio. The natural language descriptions in these sets are created by humans, and aim to be as descriptive as possible, often including the source of the sound, the action taking place, and any additional context that can be inferred from the audio. For example, a caption will not only state “dog barking” but expand to “a dog barking loudly in the distance, with the sound of traffic in the background,” giving a more complete picture of the auditory scene. Hence, the language description can serve as a succinct substitute for a typical human description of the audio recording. This text-based version allows for the generation of hypotheses through the use of an LLM.

<html><body><table><tr><td colspan="2">Sample1.A person is flipping quickly the pages of a book.</td></tr><tr><td>[Entailment] [Neutral] [Contradiction]</td><td>A person is moving the pages of a book or paper. A person is organizing documents and occasionally flipping through pages.</td></tr><tr><td colspan="2">A person is typing on a computer keyboard. Sample 2.Avariety of birds chirping and singing and shoes with a hard sole moving along a hard path. Birds are chirping outdoors while someone with hard-soled shoes walks on a hard surface.</td></tr><tr><td colspan="2">[Entailment] [Neutral] A child is playing outside where birds are singing and someone is walking on acobblestone path nearby. [Contradiction]</td></tr><tr><td colspan="2">A choir is performing in a concert hall. Sample 3.Many people are speaking simultaneously in a public place before a man hollers out something. [Entailment] A noisy indoor environment with multiple conversations happening and an occasional shout from an individual.</td></tr></table></body></html>

Our approach consists of two steps, hypothesis generation and hypothesis verification.

Hypothesis Generation. LLMs are known to exhibit reasoning abilities when they are sufficiently large (Huang and Chang 2023) (Wei et al. 2022b). For instance, using techniques including a “chain of thought” approach, such as reasoning examples, or even a straightforward prompt like “Let’s consider this one step at a time,” these models can tackle queries by outlining clear, logical steps. This method has been demonstrated in studies (Wei et al. 2022a; Kojima et al. 2022) and enables logical deduction like “if all birds have wings and all wings enable flight, then it logically follows that all birds can fly”. Hence, we use a closed-source (GPT4) and an opensource LLM (Llama3) to generate potential hypotheses for the three cases, entailment, neutral, and contradiction. We experimented with various prompting techniques, and identified three primary strategies that yielded results anchored in audio descriptions: (1) Directing the LLM to explicitly utilize knowledge from audio, acoustics, and psychoacoustics for hypothesis generation. (2) Incorporating hard examples within the prompts to obtain a better hypothesis for the neutral case. (3) Explicit instructions to avoid negations and “easy” neutral and contradiction examples. The exact prompt used is described in Appendix.

Hypothesis Verification. Our rationale for employing LLMs to create hypotheses is based on the assumption that “language descriptions can act as a compact and precise alternative to a typical human description of the audio recordings,” although this may not be reliable if errors occur in the annotator’s audio descriptions. To counteract this, we employ five distinct descriptions from separate annotators for each audio file to formulate three hypotheses. Providing the LLM with five varied descriptions ensures that it capitalizes on the commonalities among them, thereby minimizing the impact of human annotation errors on hypothesis generation. Subsequently, once the LLM generates hypotheses for each scenario—entailment, neutrality, and contradiction— we engage human annotators to either reject or validate these hypotheses. Should a hypothesis be rejected, the annotators will listen to the audio and propose an alternative hypothesis. This verification step ensures the Audio Entailment dataset is devoid of problematic hypotheses. Our two-step method—–leveraging LLM for initial hypothesis generation followed by human verification and correction of challenging hypotheses—–provides a balance between cost and time efficiency.

Table 1: Audio Entailment examples from the AudioCaps Entailment and Clotho Entailment datasets we introduce in this study   
Table 2: Statistics of AudioCaps Entailment (ACE) and Clotho Entailment (CLE) (cf. Sec 4.3).   

<html><body><table><tr><td>Data</td><td>Split</td><td>Dur. [hrs]</td><td>H</td><td>Median [chars]</td><td>Max [chars]</td><td>Vocab. [words]</td></tr><tr><td>CLE</td><td>train</td><td>23.98</td><td>3839</td><td>68</td><td>195</td><td>4678</td></tr><tr><td>CLE</td><td>val</td><td>6.56</td><td>1045</td><td>69</td><td>208</td><td>2828</td></tr><tr><td>CLE</td><td>test</td><td>6.50</td><td>1045</td><td>67</td><td>192</td><td>2759</td></tr><tr><td>ACE</td><td>test</td><td>2.63</td><td>4785</td><td>57</td><td>207</td><td>3901</td></tr></table></body></html>

# 4.3 AudioCaps and Clotho Entailment

The Audio Entailment dataset consist of $\{ a _ { i } , h _ { i } , c _ { i } \}$ triplets, with audio premise $a _ { i }$ , hypothesis $h _ { i }$ , and the target $c _ { i }$ where $c \in \{$ entailment, neutral, contradiction . We create this dataset for AudioCaps (Kim et al. 2019) and Clotho (Drossos, Lipping, and Virtanen 2020) using steps described in Sec. 4.1 and Sec. 4.2. The dataset statistics and samples from the dataset are shown in Table 2 and Table 1 respectively. We generate hypotheses for all sets of Clotho and restrict to only the test set of AudioCaps. The train set of AudioCaps has only one caption per recording and leads to generated hypotheses not aligned with the audio content. Hence, we only generate hypotheses for AudioCaps test set which has five captions per audio recording. To calculate median and max number of characters per hypothesis in Table 2 we preprocess the hypotheses $\mathcal { H }$ by dividing them into words, converting all letters to lowercase, and removing punctuation. The total vocabulary size per set is in the last column. Duration of the total audio is in hours. An analysis of the audio content in the proposed datasets can be found in the Appendix. We conduct experiments using ACE and CLE in Section 5 on 80GB A100 GPU.

<html><body><table><tr><td>Data</td><td>ALM</td><td>AE (params)</td><td>LLM (params)</td><td>ACC↑</td><td>P个</td><td>R↑</td><td>F1个</td><td>EACC↑</td><td>NACC↑</td><td>CACC↑</td></tr><tr><td>CLE</td><td>MS CLAP 22</td><td>CNN14 (80M)</td><td>BERT (110M)</td><td>0.4590</td><td>0.5499</td><td>0.459</td><td>0.4656</td><td>0.6000</td><td>0.4029</td><td>0.3742</td></tr><tr><td>CLE</td><td>LAION CLAP</td><td>HTSAT (31M)</td><td>RoBERTa(125M)</td><td>0.5113</td><td>0.5544</td><td>0.5113</td><td>0.5161</td><td>0.6679</td><td>0.3646</td><td>0.5014</td></tr><tr><td>CLE</td><td>MS CLAP 23</td><td>HTSAT (31M)</td><td>GPT2 (124M)</td><td>0.5164</td><td>0.5155</td><td>0.5163</td><td>0.5159</td><td>0.4153</td><td>0.4038</td><td>0.7301</td></tr><tr><td>ACE</td><td>MS CLAP 22</td><td>CNN14 (80M)</td><td>BERT (110M)</td><td>0.4334</td><td>0.4435</td><td>0.4334</td><td>0.4332</td><td>0.4332</td><td>0.5641</td><td>0.4508</td></tr><tr><td>ACE</td><td>LAION CLAP</td><td>HTSAT (31M)</td><td>RoBERTa (125M)</td><td>0.5872</td><td>0.5767</td><td>0.5872</td><td>0.5693</td><td>0.2867</td><td>0.5900</td><td>0.8848</td></tr><tr><td>ACE</td><td>MS CLAP 23</td><td>HTSAT (31M)</td><td>GPT2 (124M)</td><td>0.4860</td><td>0.4678</td><td>0.4860</td><td>0.4656</td><td>0.4880</td><td>0.2002</td><td>0.7699</td></tr></table></body></html>

Table 3: Zero-Shot performance of Contrastive Audio Language Models on Audio Entailment.

# 5 Deductive Reasoning With ALMs

This section benchmarks the deductive reasoning capabilities of SoTA ALMs. The deductive reasoning task is framed as a 3-way classification task, and hence we use classification metrics including accuracy, precision, recall, and F1.

# 5.1 Audio-Language Models

Recent ALMs in the literature can be broadly divided into (a) contrastive and (b) next-token prediction.

Contrastive ALMs use a two-tower structure consisting of audio and text encoders. The two branches are trained using contrastive learning and learn a joint audio–text multimodal space. After training, the model can be used for Zero-Shot inference for close-ended classification and retrieval tasks. Examples are MS CLAP (Elizalde et al. 2023) and LAION CLAP (Wu et al. 2023). In the case of contrastive ALMs, the audio premise and text hypothesis are encoded by the audio and text branch, respectively. We compute the dot product between the audio and text embeddings to obtain a score. We use non-overlapping similarity thresholds to predict the three classes entailment, neutral, and contradiction. The specifics of the thresholding method can be found in the Appendix. Classifying predictions into three categories via score thresholds eliminates the need for post-processing.

Next-token prediction ALMs take an audio recording and text as input and generate free-form text as output. The input audio is converted into a sequence of continuous embeddings using an audio encoder and is used to prompt a frozen or near-frozen (LoRA) LLM. Examples are Pengi (Deshmukh et al. 2023), LTU-AS (Gong et al. 2023a), QwenAudio (Chu et al. 2023). In this case, the audio premise becomes the audio input and the text hypothesis becomes the text prompt. The output of next-token ALMs are complex descriptions. Therefore, we use an LLM to classify the ALM descriptions into 3 classes. The text prompt used for each ALM and details on LLM-based evaluation is available in Appendix. Results are 5-run averages.

# 5.2 Zero-Shot Performance on Audio Entailment

The Zero-Shot performance of contrastive models is summarized in Table 3 and Next-token results are reported in Table 4. We make the following observations: (1) Larger language models improve deductive reasoning but are challenging to ground in audio. Among the next-token prediction ALMs, Pengi uses GPT2-base, a 128M parameter decoder while the rest use 7B LLM or larger as the decoder. We observe that the larger the LLM and its pretraining, the better the F1 score on the audio entailment task. For example, GAMA outperforms LTU-AS. Both models use largely the same training data based on OpenAQA, but GAMA uses Llama2 7B instead of Vicuna (based on Llama 7B) used by LTU-AS. However, with larger language models and their pretraining, we observe models hallucinating responses more; minor changes in prompt lead to ALMs hallucinating audio events and completely changing their deduction. For example, changing stopwords like “it” to “the” in the prompts of SALMONN and GAMA leads to them changing the deduction from contradiction to “yes, the audio events are present in the clip and hence it is true”. Without any instruction-based fine-tuning, the models rely heavily on language statistics without aligning with audio or human intent. For example, Qwen Audio uses Qwen-7B as the initialization of the LLM, and Whisper-large-v2 as the initialization of the audio encoder. The Qwen-Audio Chat version utilizes the base Qwen-Audio and undergoes instruction-based fine-tuning to improve the ability of the model to align with human intent. We observe minor hallucinations with QwenAudio Chat version compared to other ALMs. (2) Training ALMs to predict uncertainty improves their ability to detect plausible scenarios. All evaluated next-token prediction ALMs have the lowest accuracy for determining whether the hypothesis is plausible given the audio premise, compared to entailment or contradiction. We observe models like Pengi, Qwen-Audio are more likely to predict entailment instead of any other response. However, GAMA and LTU-AS are the two-top performing models in determining if the hypothesis is plausible given the audio premise. This can be attributed to the training recipe used for the model. GAMA and LTU-AS are trained on more than $3 . 7  { \mathrm { M } }$ QA pairs generated using GPT-3.5 Turbo, and about $6 . 5 \%$ contain “I don’t know” or “cannot answer due to insufficient information”. By training on these pairs, the authors aim to reduce model hallucinations and avoid answering questions that cannot be addressed solely by audio. For the task of deductive reasoning, the model can now use this ability to better predict if the audio recording does not provide sufficient evidence to either confirm or deny the hypothesis. However, this increase in detecting neutral is only achieved when the prompt matches the training data (details in Appendix). Also, the increase in detecting neutral comes at the cost of entailment accuracy, where the model is more likely to say “I cannot say” even if the audio has sufficient evidence to determine the hypothesis is true. Our proposed “captionbefore-reason” method improves this behaviour (Sec. 5.4) (3) Contrastive models are competitive on the task of deductive reasoning. The contrastive models perform comparably to the next-token prediction models on the task of deductive reasoning. One main reason is that contrastive models include both audio and text encoders that capture sentence-level information, making them ideal for classification tasks. Second, Contrastive models need a classification threshold, unlike next-token prediction models that give direct answers. Tuning this threshold can improve their performance. We use non-overlapping thresholds (details in Appendix) to test the natural separability of the latent space of these models. Even with non-overlapping linearly increasing thresholds, we see F1 scores of around $50 \%$ . This indicates that the CLAP similarity score, which is the distance between the audio and text embeddings in the latent space, changes linearly with the alignment of the hypothesis with the audio premise. This makes contrastive audio encoders a viable initialization for the audio encoders in next-token prediction models. (4) ALMs fail to follow instructions. This is especially true for the complex task of logical reasoning. The next-token prediction ALMs have to be prompted in a specific way, usually matching their training data, to get responses relevant to the user question. If not prompted in a specific way, the ALMs revert to generating text independent of the audio. For example, Pengi’s instruction following rate is $6 1 . 2 \%$ while QwenAudio follows instruction $8 4 . 4 \%$ , even after matching prompts to training data. This makes it especially challenging to evaluate the ALMs and their responses. We observe that traditional parsing methods are not sufficient to evaluate ALM responses, and hence devise a method to use LLMs to evaluate ALM responses. We setup an ablation study, where we employ human annotators to evaluate ALM as ground truth (details in Appendix). By using LLMs as evaluators we obtain a higher accuracy $9 6 \%$ with Llama3 8B and $9 9 \%$ with Llama3 70B) compared to traditional string parsing or logic methods $( 7 0 . 3 \% )$ .

<html><body><table><tr><td>Data</td><td>ALM</td><td>AE (params)</td><td>LLM (param)</td><td>ACC↑</td><td>P个</td><td>R↑</td><td>F1↑</td><td>EACC↑</td><td>NACC↑</td><td>CACC↑</td></tr><tr><td>CLE</td><td>Pengi-noenc</td><td>HTSAT (31M)</td><td>GPT2 (124M)</td><td>0.2781</td><td>0.1843</td><td>0.2781</td><td>0.2216</td><td>0.4967</td><td>0.0000</td><td>0.3378</td></tr><tr><td>CLE</td><td>Pengi-enc</td><td>HTSAT (31M)</td><td>GPT2 (124M)</td><td>0.3726</td><td>0.2465</td><td>0.3726</td><td>0.2888</td><td>0.7541</td><td>0.0000</td><td>0.3636</td></tr><tr><td>CLE</td><td>LTU-AS</td><td>Whisper-L (640M)</td><td>Vicuna (7B)</td><td>0.3681</td><td>0.3737</td><td>0.3681</td><td>0.3420</td><td>0.6278</td><td>0.3187</td><td>0.1579</td></tr><tr><td>CLE</td><td>Qwen-A</td><td>Whisper-L (640M)</td><td>Qwen (7B)</td><td>0.3620</td><td>0.4012</td><td>0.3620</td><td>0.3117</td><td>0.7675</td><td>0.1388</td><td>0.1799</td></tr><tr><td>CLE</td><td>Qwen-AC</td><td>Whisper-L (640M)</td><td>Qwen (7B)</td><td>0.5442</td><td>0.5604</td><td>0.5442</td><td>0.4975</td><td>0.9024</td><td>0.1569</td><td>0.5732</td></tr><tr><td>CLE</td><td>GAMA</td><td>CAV-MAE (85M)</td><td>LLaMA2 (7B)</td><td>0.4826</td><td>0.6151</td><td>0.4826</td><td>0.4534</td><td>0.8144</td><td>0.4124</td><td>0.2211</td></tr><tr><td>CLE</td><td>GAMA-IT</td><td>CAV-MAE (85M)</td><td>LLaMA2 (7B)</td><td>0.3974</td><td>0.5604</td><td>0.3974</td><td>0.3433</td><td>0.7923</td><td>0.2947</td><td>0.1053</td></tr><tr><td>CLE</td><td>SALMONN</td><td>Combined*(730M)</td><td>Vicuna (13B)</td><td>0.5222</td><td>0.5054</td><td>0.5222</td><td>0.4515</td><td>0.6775</td><td>0.0708</td><td>0.8182</td></tr><tr><td>ACE</td><td>Pengi-noenc</td><td>HTSAT (31M)</td><td>GPT2 (124M)</td><td>0.2629</td><td>0.1699</td><td>0.2629</td><td>0.2045</td><td>0.5312</td><td>0.0000</td><td>0.2575</td></tr><tr><td>ACE</td><td>Pengi-enc</td><td>HTSAT (31M)</td><td>GPT2 (124M)</td><td>0.3867</td><td>0.2558</td><td>0.3867</td><td>0.3039</td><td>0.7335</td><td>0.0000</td><td>0.4265</td></tr><tr><td>ACE</td><td>LTU-AS</td><td>Whisper-L (640M)</td><td>Vicuna (7B)</td><td>0.3633</td><td>0.3772</td><td>0.3633</td><td>0.3334</td><td>0.6702</td><td>0.2435</td><td>0.1762</td></tr><tr><td>ACE</td><td>Qwen-A</td><td>Whisper-L (640M)</td><td>Qwen (7B)</td><td>0.3563</td><td>0.3562</td><td>0.3563</td><td>0.3219</td><td>0.6669</td><td>0.1323</td><td>0.2696</td></tr><tr><td>ACE</td><td>Qwen-AC</td><td>Whisper-L (640M)</td><td>Qwen (7B)</td><td>0.5216</td><td>0.5669</td><td>0.5216</td><td>0.4918</td><td>0.9300</td><td>0.2821</td><td>0.3528</td></tr><tr><td>ACE</td><td>GAMA</td><td>CAV-MAE (85M)</td><td>LLaMA2 (7B)</td><td>0.5248</td><td>0.6531</td><td>0.5248</td><td>0.4933</td><td>0.7827</td><td>0.5885</td><td>0.2031</td></tr><tr><td>ACE</td><td>GAMA-IT</td><td>CAV-MAE (85M)</td><td>LLaMA2 (7B)</td><td>0.4167</td><td>0.5672</td><td>0.4167</td><td>0.3828</td><td>0.7852</td><td>0.2696</td><td>0.1954</td></tr><tr><td>ACE</td><td>SALMONN</td><td>Combined* (730M)</td><td>Vicuna (13B)</td><td>0.5622</td><td>0.5551</td><td>0.5622</td><td>0.4826</td><td>0.7114</td><td>0.0698</td><td>0.9055</td></tr></table></body></html>

Table 4: Zero-Shot performance of Next-token prediction Audio Language Models on Audio Entailment. The combined∗ Audio Encoder (AE) indicates a concatenation of Whisper-Large and BEATs audio encoder.

This LLM evaluator can be further improved along with instruction tuning methods to provide a stronger grounding in audio and instructions.

The highest F1 scores are $51 \%$ for the CLE task and $56 \%$ for the ACE task, indicating that there is ample room for improving deductive reasoning in contrastive and next-token prediction models.

# 5.3 Evaluating Audio–Text Representations

The choice of thresholds and prompts used affects ALM performance on the task of entailment. One way to circumvent thresholding and prompting limitations is to evaluate the audio and text representations learned by these models. Therefore, we setup a linear-probe experiment. The audio premise and text hypothesis are encoded by the audio and text encoder, respectively. The audio and text representation are then concatenated and fed to a classifier. In this linearprobe setup, the audio and text encoder are frozen and only the classifier is trained on the target data. We use the CLE dataset, the development set, to train the classifier, the validation to choose the checkpoint, and the test for evaluation.

The linear-probe results are shown in Table 5. The linearprobe leads to an average absolute $30 \%$ improvement for Contrastive models while for next-token-prediction we see an absolute improvement of $44 \%$ . We can make the observations: (1) The learned audio–text representation can differentiate between possibly true and definitely true, and hence shows primitive reasoning capabilities. The difference between the zero-shot and linear probe performance shows that the current methods of similarity computation and thresholding can be improved. (2) Small parameter count decoder can be compensated by introducing an encoder. This is achieved by using attention throughout audio and instruction (hypothesis), while having autoregressive attention on the suffix. For example, Pengi, which has decoder of 128M, improves performance by having full attention on audio and instruction, while autoregressive attention on output. This aligns with recent findings in training vision-language models (Beyer et al. 2024). This improves linear-probe performance, but is not effective for zero-shot setup. (3) Despite training the classifier specifically for the audio entailment task, the F1 score remains in the lower 80s. This indicates that the pretraining method could be improved to develop representations capable for logical reasoning.

Table 5: Linear-probe performance of Audio Language Models on CLE dataset. Each ALM has an audio encoder and a text encoder to compute embeddings for the audio premise and text hypothesis. The audio embedding and text embedding are concatenated and passed to a linear 3-class classifier.   

<html><body><table><tr><td>ALM</td><td>Train pairs</td><td>ACC↑</td><td>P↑</td><td>R↑</td><td>F1个</td><td>EntACC↑</td><td>NeuACC↑</td><td>ConACC↑</td></tr><tr><td>MS CLAP 22</td><td>128k</td><td>0.7110</td><td>0.7130</td><td>0.7110</td><td>0.7118</td><td>0.6890</td><td>0.6775</td><td>0.7665</td></tr><tr><td>LAION CLAP</td><td>2.6M</td><td>0.7435</td><td>0.7470</td><td>0.7435</td><td>0.7445</td><td>0.7483</td><td>0.6957</td><td>0.7866</td></tr><tr><td>Pengi-enc</td><td>3.3M</td><td>0.7627</td><td>0.7674</td><td>0.7627</td><td>0.7642</td><td>0.7598</td><td>0.7100</td><td>0.8182</td></tr><tr><td>MS CLAP23</td><td>4.6M</td><td>0.8329</td><td>0.8361</td><td>0.8329</td><td>0.8336</td><td>0.8182</td><td>0.8440</td><td>0.8364</td></tr></table></body></html>

# 5.4 Captioning Before Reasoning

Humans employ deductive reasoning by accepting a premise as true, breaking it down into its parts, applying logical principles, and drawing conclusions. Similarly, in audio entailment, models should identify audio events, understand their relationships and order, and infer based on these elements and the hypothesis. This process is similar to creating captions for the audio before engaging in deductive reasoning.

Zero-shot setup Linear-probe setup Turn 1 ALM Audio Caption Text GU Caption Encoder Decoder Encoder Turn 2 Hypothesis →ALM Text Encoder Answer Hypothesis

To evaluate this approach, we conducted two experiments: zero-shot prompting for next-token prediction models and linear probe for contrastive models. We select the best performing model on the CLE dataset, i.e., Qwen-AC, as a representative for next-token prediction models and MS CLAP 2023. For linear probing, we included an explicit audio captioning step using the model’s latent embeddings. The generated audio caption was then encoded with a text encoder to produce a sentence-level representation. This encoded hypothesis, along with the caption and base audio representation, was fed into a classifier to make predictions. For zeroshot prompting, we instructed the model to first caption the audio before performing the actual task of audio entailment. We adjust the task prompt to consider both the audio and the generated caption. The setup is illustrated in Figure 3, with results shown in Table 6.

By incorporating an explicit captioning step before making predictions, we observed an absolute improvement in deductive reasoning performance (F1) by $6 \%$ for zeroshot prompting and $3 \%$ for the linear-probe setup. Using the “caption-before-reason” approach, we observe an increase in accurately predicting contradictions. Previously, the model tended to agree with the hypothesis. However, with explicit captioning, it can better reason and identify misalignments with the audio information. This approach helps the model avoid hallucinating sources based on the hypothesis, and improves grounding in the audio input. Qualitative examples are shown in Appendix. Our prompting approach improves the deductive reasoning performance of ALMs at test-time without requiring training or finetuning.

Table 6: Proposed “caption-before-reason” method for ZeroShot prompting (top) and linear-probe (bottom).   

<html><body><table><tr><td>Model</td><td>Method</td><td>ACC↑</td><td>P↑</td><td>R↑</td><td>F1↑</td></tr><tr><td>Qwen-AC</td><td>base</td><td>0.5442</td><td>0.5604</td><td>0.5442</td><td>0.4975</td></tr><tr><td>Qwen-AC</td><td>cap</td><td>0.6083</td><td>0.5964</td><td>0.6083</td><td>0.5601</td></tr><tr><td>CLAP 23</td><td>avg</td><td>0.7512</td><td>0.7529</td><td>0.7512</td><td>0.7515</td></tr><tr><td>CLAP 23</td><td>sum</td><td>0.7780</td><td>0.7812</td><td>0.7780</td><td>0.7785</td></tr><tr><td>CLAP 23</td><td>concat</td><td>0.8329</td><td>0.8361</td><td>0.8329</td><td>0.8336</td></tr><tr><td>CLAP23</td><td>cap</td><td>0.8640</td><td>0.8671</td><td>0.8640</td><td>0.8647</td></tr></table></body></html>

# 6 Conclusion

We introduce the Audio Entailment task to evaluate deductive reasoning capabilities of Audio-Language Models (ALMs). We propose two datasets, ACE and CLE, and benchmark state-of-the-art contrastive and next-token prediction ALMs, revealing significant limitations in their logical reasoning abilities. Surprisingly, contrastive models, which learn similarity, performed competitively to nexttoken prediction models, which learn to produce descriptions. We show limitations of ALMs for following instructions and report quantitative results for the first time in the literature. Finally, we propose “caption-before-reason” to improve zero-shot and linear-probe performance of ALMs by an absolute $6 \%$ and $3 \%$ , respectively.