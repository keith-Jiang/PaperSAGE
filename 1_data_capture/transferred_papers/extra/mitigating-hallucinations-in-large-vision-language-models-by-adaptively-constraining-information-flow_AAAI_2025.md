# Mitigating Hallucinations in Large Vision-Language Models by Adaptively Constraining Information Flow

Jiaqi Bai1,2, Hongcheng $\mathbf { G u o } ^ { 3 }$ , Zhongyuan Peng4, Jian Yang3, Zhoujun $\mathbf { L i } ^ { 3 }$ , Mohan $\mathbf { L i } ^ { 1 , 2 * }$ , Zhihong Tian1,2

1Cyberspace Institute of Advanced Technology, Guangzhou University, China 2Huangpu Research School of Guangzhou University, China 3CCSE, Beihang University, China 4University of the Chinese Academy of Sciences, China {jqbai, limohan, tianzhihong}@gzhu.edu.cn, {hongchengguo, jiaya, lizj}@buaa.edu.cn, zhongyuan.peng $@$ cripac.ia.ac.cn

# Abstract

Large vision-language models show tremendous potential in understanding visual information through human languages. However, they are prone to suffer from object hallucination, i.e., the generated image descriptions contain objects that do not exist in the image. In this paper, we reveal that object hallucination can be attributed to overconfidence in irrelevant visual features when soft visual tokens map to the LLM’s word embedding space. Specifically, by figuring out the semantic similarity between visual tokens and LLM’s word embedding, we observe that the smoothness of similarity distribution strongly correlates with the emergence of object hallucinations. To mitigate hallucinations, we propose using the Variational Information Bottleneck (VIB) to alleviate overconfidence by introducing stochastic noise, facilitating the constraining of irrelevant information. Furthermore, we propose an entropy-based noise-controlling strategy to enable the injected noise to be adaptively constrained regarding the smoothness of the similarity distribution. We adapt the proposed ADAVIB across distinct model architectures. Experimental results demonstrate that the proposed ADAVIB mitigates object hallucinations by effectively alleviating the overconfidence in irrelevant visual features, with consistent improvements on two object hallucination benchmarks.

Smoitisimilarty Imoder Sharp similarity   
1.0 FC FC →1.0 distribution   
0.8 0.8   
0.6   
0.4 SoftVisual Tokens 0.6   
0.2 ① 0.4   
0.0 1 2 Textual Prompts 0.2 1 2 Large Language Model   
Theimage featuresa bowl filled The image features a fruit bowl   
withvarious fruits,including filled with a variety of fruits,   
bananas, oranges, and lemons. including bananas, peaches and   
The bananas are placed in the apples. The bananas are placed in   
middle of the bowl, while the the middle of the bowl, with some   
orangesarescatteredaround them. of them extending.. Hallucination-Free Hallucinated

Code — https://github.com/jiaqi5598/AdaVIB

# Introduction

Large Vision-Language Models (LVLMs) (Zhu et al. 2023; Dai et al. 2023a; Bai et al. 2023) have recently attracted increasing attention. Due to the superiority of generating contextually relevant natural language descriptions grounded on visual patterns, LVLMs have shown impressive performance on various vision-language tasks, including image captioning (Deng et al. 2009; Lin et al. 2014), visual question answering (Antol et al. 2015; Li et al. 2023c) and multimodal machine translation (Yao and Wan 2020; Guo et al. 2022). Despite their success, LVLMs are prone to object hallucinations (Rohrbach et al. 2018; Biten, Go´mez, and Karatzas

2022), where the generated image descriptions are inconsistent with the objects that appear in the grounded image. This inconsistency has significantly affected the reliability and applicability of LVLMs, especially in scenarios that demand precise judgment.

Most existing works focus on mitigating object hallucinations by reducing over-reliance on prior knowledge of LLMs through the decoding process (Huang et al. 2024; Favero et al. 2024). They devise various methods to penalize specific patterns that may induce hallucinations (Huang et al. 2024; Leng et al. 2024) or curate a training dataset to address statistical biases arising from co-occurrence and spurious correlation among objects (Biten, Go´mez, and Karatzas 2022; Zhou et al. 2024; Liu et al. 2023). Despite their efforts, most of these works rarely focus on narrowing the modality gap between visual patterns and textual descriptions. Recently, researchers have revealed that the vision-language connector plays a significant role in mitigating object hallucinations of LVLMs (Li et al. 2023a; Sun et al. 2023; Jiang et al. 2024). On the one hand, despite their impressive performance on various visual understanding tasks, existing vision encoding techniques are still challenging in expressing visual patterns precisely (Cho et al. 2022; Li et al. 2024; Jain, Yang, and Shi 2024). Hence, compressing irrelevant visual features encoded by the vision encoder is essential to generate hallucination-free descriptions. On the other hand, although previous work investigated various modality alignment approaches, such as Q-Formers (Li et al. $2 0 2 3 \mathrm { a }$ ; Dai et al. 2023b) and lightweight projectors (Liu et al. 2022; Alayrac et al. 2022; Gao et al. 2023; Chen et al. 2023; Liu et al. 2024a), the encoded soft visual tokens are still far from ideal in compressing irrelevant while preserving relevant visual information faithful to input images.

In this paper, we reveal that object hallucinations can be attributed to overconfidence in irrelevant visual features when soft visual tokens project to the word embedding space of LLM. As Figure 1 shows, we present two distinct cases illustrating how the smoothness of the similarity distribution between soft visual tokens1 and the LLM’s word embedding correlates with the emergence of object hallucinations. Both frozen and fine-tuned fully connected (FC) layers, referred to as the vision-language projector, are introduced for comparison to project encoded visual features into soft visual tokens. By examining the normalized dot product similarity between soft visual tokens and the word embedding of LLM, we observe that the LVLMs equipped with a frozen FC layer (the green block on the left) generate a hallucination-free description with a smoother similarity distribution. In contrast, the variant equipped with a fine-tuned FC layer (the red block on the right) generates a hallucinated description with a sharper similarity distribution. This phenomenon indicates that the smoothness of the similarity distribution strongly correlates with the emergence of object hallucinations. A sharp similarity distribution indicates the occurrence of the overconfidence problem, which results from overfitting on statistically spurious correlations with the irrelevant visual features during training.

Therefore, alleviating the overconfidence on irrelevant features when soft visual tokens mapping to the LLM’s word embedding is essential to mitigate object hallucinations, and it is critical to constrain information flow to soft visual tokens by elaborately devising the vision-language projector.

Motivated by the above analysis, we propose ADAVIB, a lightweight fine-tuning method that uses Variational Information Bottleneck (VIB) (Alemi et al. 2016) to mitigate object hallucinations. The proposed ADAVIB only requires fine-tuning weights of the vision-language projector with other modules frozen. It mitigates overconfidence problem by introducing a compression term to regularize the training of vision-language projector. Introducing the compression term can be regarded as adding stochastic noise to soft visual tokens during training, and increasing this noise constrains the information flow to them, which decreases the overconfidence in irrelevant visual features when visual tokens mapping to the LLM’s word embedding space. To adaptively constrain the visual information flow to the soft visual tokens, we propose an entropy-based noise-controlling mechanism. The proposal adaptively constrains the injected noise regarding the smoothness of the similarity distribution between soft visual tokens and the LLM’s word embedding, capturing the dynamic nature of a specific sample. We adapt ADAVIB on two distinct LVLM architectures, including MiniGPT4 (Zhu et al. 2023) and LLava-1.5 (Liu et al. 2024a). Experimental results demonstrate the effectiveness of ADAVIB in mitigating the overconfidence problem by effectively smoothing the similarity distribution between soft visual tokens and LLM’s word embedding, with consistent improvements on two object hallucination benchmarks.

Our contributions are three-fold: i): We are the first to use VIB to mitigate object hallucinations, which decreases the overconfidence in irrelevant visual features when soft visual tokens map to the word embedding of LLM. ii): We propose ADAVIB, an entropy-based noise controlling strategy to adaptive constrain the information conveyed by visual tokens, regarding the smoothness of similarity distribution to LLM’s word embedding. iii): Comprehensive experiments demonstrate the effectiveness of ADAVIB in mitigating object hallucinations. The proposed approach yields consistent improvements on two object hallucination benchmarks across different model architectures.

# Related Work

# Information Bottleneck

The Information Bottleneck (IB) principle (Tishby, Pereira, and Bialek 2000; Fischer 2020) is an excellent concept for regularizing internal representations to minimize the mutual information by compressing the original input representation. The compressed representation can further improve the model’s generalization capability by ignoring irrelevant features in the original input. IB has been widely adopted for many machine learning tasks (Peng et al. 2018; Li and Eisner 2019; Belinkov, Henderson et al. 2020), such as image generation (Peng et al. 2018), explanation regeneration (Li et al. 2023b), retrieval-augmented generation (Zhu et al. 2024), and so on.

Based on the IB principle, Alemi et al. (2016) introduced variational information bottleneck (VIB), a variational approach that can be instantiated in deep neural networks, inspired by a similar approach in variational autoencoders (VAE) (Kingma and Welling 2013). It has been applied in the study of parsing (Li and Eisner 2019), natural language inference (Belinkov, Henderson et al. 2020), and graph structure learning (Sun et al. 2022). Compared to the above work, our work, to the best of our knowledge, is the first attempt to investigate VIB as a regularization technique to mitigate object hallucinations in LVLMs.

# Object Hallucinations in LVLMs

Mitigating object hallucinations (Rohrbach et al. 2018; Biten, Go´mez, and Karatzas 2022) has been a long-standing challenge in realizing a trustworthy AI system. This issue can be attributed to several possible reasons, e.g., insufficient understanding of the real-world knowledge (Leng et al. 2024; Huang et al. 2024), statistical bias in training data (Tang et al. 2021; Biten, Go´mez, and Karatzas 2022; Zhou et al. 2024) or uncertainty to the objects present in the image (Cho et al. 2022; Liu et al. 2022; Li et al. 2024).

To mitigate object hallucinations, existing studies typically involved methods such as contrastive decoding (Leng et al. 2024; Huang et al. 2024), balance co-occurrence patterns through data augmentation (Zhou et al. 2024; Liu et al. 2024a) and devise an aligner to narrow the modality gap between vision and languages (Zhu et al. 2023; Dai et al. 2023a). For example, Chuang et al. (2023) and Leng et al. (2024) introduced distinct decoding approaches to estimate output distributions by contrasting different sources. They effectively alleviate object hallucinations by reducing the over-reliance on the prior knowledge of a single source. Some studies (Biten, Go´mez, and Karatzas 2022; Zhou et al. 2024) augmented training data by analyzing key factors underlying object hallucination, effectively reducing the statistical bias to alleviate object hallucinations. Compared to the above work, our work starts with an observation that the object hallucinations stem from the overconfidence problem. Based on this, we propose a VIB-based approach to mitigate object hallucinations by alleviating such a problem.

# Methodology Problem Formulation

We consider a general image-to-text generation problem with a dataset $\bar { \mathcal { D } } = \{ ( x _ { i } , q _ { i } , y _ { i } ) \} _ { i = 1 } ^ { N }$ , where the $i$ -th triple $( x _ { i } , q _ { i } , y _ { i } )$ consists of an image $x _ { i }$ , an textual prompt $q _ { i }$ and a image description $y _ { i }$ . The goal is to learn a probability distribution $p ( \boldsymbol { y } | \boldsymbol { \bar { x } } , \boldsymbol { q } )$ with $\mathcal { D }$ , and thus given a new pair $( x , q )$ in an image-to-text generation, one can generate a response $y$ token-by-token in terms of $p ( y | x , q )$ . The optimization of $p ( \boldsymbol { y } | \boldsymbol { x } , \boldsymbol { q } )$ can be formalized by maximizing the following conditional probability:

$$
p ( y | x , q ) = \prod _ { t = 1 } ^ { | \mathbf { y } | } p ( y _ { t } | y _ { < t } , x , q )
$$

# Large Vision-Language Models

Given an input image $x _ { j }$ , a pre-trained visual encoder (e.g., CLIP (Radford et al. 2021)) first encodes $x _ { j }$ into a dense visual representation $\mathbf { x } _ { j }$ , which is then fed to a visionlanguage connector to obtain an intermediate representation $\mathbf { v } _ { j }$ , denoted as $\mathbf { v } _ { j } ~ = ~ g _ { \boldsymbol { \theta } } ( x _ { j } )$ , where $\theta$ is a set of parameters. The vision language connector is optional, and can be arbitrary architectures, such as a Q-Former (Li et al. $2 0 2 3 \mathrm { a }$ ; Dai et al. 2023b). After that, a vision-language projector receives $\mathbf { v } _ { j }$ and maps it to the word embedding space of LLM, yielding a sequence of visual tokens $\mathbf { z } _ { j }$ , the vision-language projector can be a Multilayer Perceptron (i.e., MLP):

$$
\begin{array} { r } { \mathbf { z } _ { j } = \mathbf { W } _ { h } \mathrm { G e L U } \left( \mathbf { W } _ { z } \mathbf { v } _ { j } \right) } \end{array}
$$

where $\mathbf { W } _ { z }$ and ${ \bf W } _ { h }$ are trainable parameters. Lastly, by embedding the textual prompt $q$ into embedded representations q, the gathered visual tokens $\textbf { z }$ are concatenated with $\mathbf { q }$ , yielding outputs $y$ token-by-token. Suppose $p _ { \phi } ( y | \mathbf { v } , q )$ is an approximation of $p ( y | x , q )$ parameterized by $\phi$ . The above process can be optimized by minimizing the following loss:

$$
\operatorname* { m i n } \mathcal { L } _ { C E } = \mathbb { E } _ { ( x , q , y ) \sim \mathcal { D } , \mathbf { v } \sim g _ { \theta } ( x ) } \left[ - \log \left( p _ { \phi } \left( y | \mathbf { v } , q \right) \right) \right]
$$

In this paper, we focus on optimizing the vision-language projector with other modules frozen. Our approach can be easily extended to LVLMs with and without a visionlanguage connector (e.g., Q-Former) by simply replacing the vision-language projector with our component. For the convenience of description, we introduce our method upon the structure without the vision-language connector (e.g., LLaVa (Liu et al. 2024b,a)), and the inputs of the visionlanguage projector denote as $\mathbf { v }$ unless explicitly specified.

# Adaptive Variational Information Bottleneck

Information Bottleneck The Information Bottleneck (IB) (Tishby and Zaslavsky 2015) has been demonstrated as a promising principle to find a compression $\mathbf { z }$ for the original input representation $\mathbf { v }$ , that maximally compressing irrelevant information to $\mathbf { v }$ while maintaining relevant information to y. The objective of IB is to minimize the combination of compression loss and prediction loss, which is formalized as follows:

$$
\begin{array} { r } { \operatorname* { m i n } \mathcal { L } _ { \mathrm { I B } } = \underbrace { \beta I ( \mathbf { v } ; \mathbf { z } ) } _ { \mathrm { C o m p r e s s i o n } } - \underbrace { I ( \mathbf { z } ; \mathbf { y } ) } _ { \mathrm { P r e d i c t i o n } } } \end{array}
$$

where $\beta \geqslant 0$ is the Lagrange multiplier for balancing the compression and prediction terms. $I ( \cdot ; \cdot )$ is the mutual information. The former term of Equation 4 improves the conciseness of the input signal $\mathbf { v }$ by minimizing the inclusion of irrelevant information, encouraging the network to concentrate more on useful content. The latter enables the network to selectively maintain useful information for supporting the predicted content $\mathbf { y }$ faithful to the input $\mathbf { v }$ .

Variational Information Bottleneck Alemi et al. (2016) proposed Variational Information Bottleneck (VIB), a variational estimation of IB by approximating the probability distribution via a neural network:

$$
\begin{array} { r } { \operatorname* { m i n } \mathcal { L } _ { \mathrm { V I B } } = \underset { \mathbf { v } } { \beta } [ \mathrm { K L } ( p _ { \theta } ( \mathbf { z } | \mathbf { v } ) \| r ( \mathbf { z } ) ) ] } \\ { + \underset { \mathbf { z } \sim p _ { \theta } ( \mathbf { z } | \mathbf { v } ) } { \mathbb { E } } [ - \log p _ { \phi } ( y | \mathbf { z } , q ) ] } \end{array}
$$

where $p _ { \phi } ( y | \mathbf { z } , q )$ is an estimation of response $y$ parameterized by $\phi$ , given compressed input representation $\textbf { z }$ and textual prompt $q . \ r ( \mathbf { z } )$ and $p _ { \theta } ( \mathbf { z } | \bar { \mathbf { v } } )$ are the estimation of prior and posterior probability to $\textbf { z }$ , respectively. The former part of Equation 5 serves as a compression term, which provides an explicit way to compress input representation $\mathbf { v }$ into $\mathbf { z }$ . The compressed process can be regarded as introducing stochastic noise during training, where the noise can be injected by sampling $\textbf { z }$ from $p _ { \theta } ( \mathbf { z } | \mathbf { v } )$ . Increasing this noise decreases the information conveyed by $\textbf { z }$ . The later part of Equation 5 is a prediction term, which preserves the useful information for the prediction of $y$ . When $\beta = 0$ , there is no incentive to inject a noise perturbation; thereby, the VIB degrades to deterministic dimensionality reduction with an MLP, similar to the implementation of Equation 3. During testing, we use the expected value of $\mathbf { z }$ to predict answer $y$ through $p _ { \phi } ( y | \mathbf { z } , q )$ .

![](images/543beabff0564234e7198a3d4cc4706bf0325f3bfb1f938c1b36a7d72aa4a8b1.jpg)  
Figure 2: The model architecture of ADAVIB. ADAVIB compresses the input representations $\mathbf { v }$ into soft visual tokens $\textbf { z }$ with mean $\mu _ { \boldsymbol { \theta } } ( \mathbf { v } )$ and constrain the irrelevant information by injecting the Gaussian noise with variance $\Sigma _ { \theta } ( \mathbf { v } )$ .

In our experiment, the prior distribution $r ( \mathbf { z } )$ and posterior distribution $p _ { \theta } ( \mathbf { z } | \mathbf { v } )$ are modeled by parametric Gaussian distributions, formalized by:

$$
\begin{array} { c } { r \left( { \bf { z } } \right) = \mathcal { N } \left( { \mu _ { r } } , { \Sigma _ { r } } \right) } \\ { p _ { \theta } \left( { \bf { z } } | { \bf { v } } \right) = \mathcal { N } \left( { \mu _ { \theta } } \left( \bar { \bf { v } } \right) , { \Sigma _ { \theta } } \left( \bar { \bf { v } } \right) \right) } \end{array}
$$

where $\mu _ { r }$ and $\mu _ { \theta }$ are mean vectors, $\textstyle \sum _ { r }$ and $\Sigma _ { \theta }$ are diagonal covariance matrices. $\bar { \mathbf { v } }$ is the average-pooling of $\mathbf { v }$ over all tokens. Because each dimension of these variables is independent and identically distributed, the Kullback-Leibler (KL) divergence of the multivariate Gaussian distribution can be estimated as follows:

$$
\begin{array} { l } { { \displaystyle \mathrm { K L } ( \mathcal { N } _ { r } | | \mathcal { N } _ { \theta } ) = \frac { 1 } { 2 } [ \log \frac { \operatorname* { d e t } \left( \Sigma _ { \theta } \right) } { \operatorname* { d e t } \left( \Sigma _ { r } \right) } - d _ { \mathbf { z } } + \mathrm { t r } ( \Sigma _ { \theta } ^ { - 1 } \Sigma _ { r } ) } } \\ { { \displaystyle ~ + \left( \mu _ { \theta } - \mu _ { r } \right) ^ { \top } \Sigma _ { \theta } ^ { - 1 } ( \mu _ { \theta } - \mu _ { r } ) ] } } \end{array}
$$

where $d _ { \mathbf { z } }$ is the dimensionality of $\mathbf { z }$ . Similar to Li and Eisner (2019), we apply the reparameterization strategy (Kingma and Welling 2013) to approximate gradients backpropagate to $\mathbf { v }$ , formalized by:

$$
\mathbf { z } = \mu _ { \boldsymbol \theta } ( \mathbf { v } ) + \Sigma _ { \boldsymbol \theta } ( \mathbf { v } ) \odot { \epsilon }
$$

where $\epsilon$ is modeled with a standard Gaussian distribution. In practice, the compressed input representation $p _ { \theta } ( \mathbf { z } | \mathbf { v } )$ is modeled by two distinct linear layers, each projects input to the same dimensionality with $\textbf { z }$ for computing $\mu _ { \boldsymbol { \theta } } ( \mathbf { v } )$ and $\Sigma _ { \theta } ( \mathbf { v } )$ , where $\mu _ { \boldsymbol { \theta } } ( \mathbf { v } )$ is initialized from the pre-trained weights of the LVLM’s vision-language projector, $\Sigma _ { \theta } ( \mathbf { v } )$ is randomly initialized and apply a softplus transform to ensure outputs non-negativity.

Adaptive $\beta$ Previous studies (Peng et al. 2018; Li and Eisner 2019) have shown that balancing the compression and prediction terms using a Lagrange multiplier $\beta$ is crucial for removing irrelevant information while preserving information that is predictive of the model output. While setting the $\beta$ to be fixed may prevent the constrained procedure from capturing the dynamic nature of the specific sample.

Inspired by our observation that the smoothness of the similarity distribution between soft visual tokens and the LLM’s word embedding strongly correlates with the emergence of object hallucinations, we propose Adaptive Variational Information Bottleneck (ADAVIB). The proposal adaptively constrains the injected noise regarding the smoothness of the similarity distribution for capturing the dynamic nature per sample. Specifically, we use the entropy (Shannon 1948; Zhai et al. 2023; Farquhar et al. 2024) of the similarity distribution as an indicator to reflect the degree of soft visual tokens suffering from the overconfidence problem. A sample with a low entropy level refers to a sharp similarity distribution, indicating that the soft visual tokens are prone to be overconfident (Pereyra et al. 2017) when mapping to the LLM’s embedding space. In contrast, a sample with a high entropy level indicates a smooth similarity distribution, corresponding to the uncertainty of the probability distribution (Alemi, Fischer, and Dillon 2018). Therefore, ensuring the entropy within a proper value is essential to effectively delivering essential information. With this intuition, our method computes the smoothing indicator on the fly during the forward propagation in training, relying on the entropy of the similarity distribution per sample, which is formalized as follows:

$$
H = - \sum _ { i = 1 } ^ { | V | } p ( E _ { L L M } ^ { ( i ) } | \mathbf { z } ) \log p ( E _ { L L M } ^ { ( i ) } | \mathbf { z } )
$$

$$
p ( E _ { L L M } | \mathbf { z } ) = { \tt s o f t m a x } ( \bar { \mathbf { z } } \cdot E _ { L L M } ^ { \top } )
$$

where $\bar { \mathbf { z } }$ is the average pooling of $\mathbf { z }$ for all soft visual tokens, $E _ { L L M }$ is the word embedding of the LLM, $| V |$ is the vocabulary size of the LLM. Since entropy $H$ has an unfixed range, we normalize $H$ between 0 and 1 with its maximum value $\log ( | V | )$ . The adaptive $\beta$ updates as follows:

$$
\beta  - \beta \cdot \log ( \frac { H } { \log ( | V | ) } )
$$

With this mechanism, a sample with low entropy is trained with high $\beta$ , in which the compression term in Equation 5 dominates the optimization process, thereby constraining the information conveyed by input through adding a significant noise perturbation. Conversely, a sample with high entropy corresponds to low $\beta$ , where the prediction term dominates, preventing the learning process from converging to a worse performance. Note that the gradient computation for $\beta$ is excluded from the computation graph, thereby the gradient does not flow through adaptive $\beta$ .

# Experiments Datasets and Evaluation Metrics

MSCOCO (Lin et al. 2014) The Microsoft Common Objects in Context (MSCOCO) stands as a comprehensive dataset for evaluating various visual tasks, including image recognition, segmentation, and captioning. It has more than 300k images with annotations for over 80 object categories. In this paper, we employ COCO2014 to assess object hallucinations on the image captioning task. To train the vision-language projector, we randomly select 5000 imagetext pairs from LLaVA-150k (Liu et al. 2024b), which is a set of GPT-generated multi-modal instruction-following data grounded on the images from COCO2014. Following Zhou et al. (2024), we additionally select 5000 unique images from the training dataset of COCO2014 to evaluate object hallucinations, ensuring that the selected images do not overlap with those used in training.

We use Caption Hallucination Assessment with Image Relevance (CHAIR) (Rohrbach et al. 2018) metric to evaluate object hallucinations in the image captioning task. CHAIR assesses object hallucinations by counter objects mentioned in the predicted caption but not present in the grounded image. It has two commonly used variants, $\mathrm { C H A I R } _ { S }$ and $\mathrm { C H A I R } _ { I }$ , where the former assesses object hallucinations at the sentence level, and the latter assesses at the instance level.

POPE (Li et al. 2023c) The Polling-based Object Probing Evaluation (POPE) is a widely adopted benchmark for assessing object hallucination on the Visual Question Answering (VQA) task. We conduct the evaluation on this benchmark to verify the generalization capability of ADAVIB on different tasks. POPE designs a binary question format, requiring the LVLM to deliver a binary-like answer to discriminate whether the mentioned objects are within the grounded image. We use the official question sets introduced in Li et al. (2023c). The dataset comprises three splits: In Random split, the absent objects are randomly selected from the whole dataset. In Popular split, the absent objects are chosen from the most frequently appeared objects in the dataset. In Adversarial split, the absent objects are selected from those frequently co-occurred with ground-truth objects. Each split is composed of 3000 questions on images taken from the validation set of COCO2014.

We use Accuracy and F1 scores to evaluate model performance, where Accuracy reflects the proportion of samples that correctly predict the golden answer. F1 score is the harmonic average of precision and recall. Following Li et al. (2023c), we use it as the major metric for evaluation.

# Baselines

We employ the following competitive baselines to compare with our approach: Chain-of-Thought (CoT): Wei et al. (2022) decomposed a hard problem by generating intermediate steps for the final answer. Following Zhou et al. (2024), we leverage CoT by asking the model to first list the identified objects and then describe the image in terms of these objects. Decoding by Contrasting Layers (DOLA): Chuang et al. (2023) mitigated LLM’s hallucinations by contrasting the differences between output logits from the later and earlier layers of LLMs. Teacher: Saha, Hase, and Bansal (2024) integrated several short descriptions into a long-form version. Following Zhou et al. (2024), we leverage BLIP2 (Li et al. 2023a) to generate short descriptions as contextual guidance, facilitating a long-form description using the caption generator. Visual Contrastive Decoding (VCD): Leng et al. (2024) alleviated hallucinations by introducing a visual contrastive decoding strategy, effectively decreasing the over-reliance on statistical bias and unimodal priors. LVLM Hallucination Revisor (LURE): Zhou et al. (2024) introduced a post-hoc rectification method to train a hallucination revisor for rectifying initially generated descriptions.

Apart from the above baselines, we employ two distinct LVLMs as backbone frameworks, i.e., MiniGPT-4 and LLaVa-1.5. We employ both original and fine-tuned (FT) version as baselines. Additionally, we investigate the effectiveness of the variant without adaptive $\beta$ (w/o Ada $\beta$ ) in Equation 12, and the one without reparameterization strategy (w/o Repara.) in Equation 9 for ADAVIB.

# Implementation Details

We employ Vicuna-7B as the caption generator for both MiniGPT4 and LLaVa-1.5. We train all models in one epoch to avoid overfitting. All hyperparameters of baselines are selected via cross-validation on the training dataset of MSCOCO. Specifically, the Lagrange multiplier $\beta$ is set to $\beta = 1 e ^ { - 7 }$ unless explicitly specified. We set the batch size to 2 with gradient accumulation steps to 8. The maximum sequence length during training is set to 512. We use greedy decoding with a maximum decoding length of 256 during inference. We set the learning rate to $\bar { 3 } e ^ { - 5 }$ with a weight decay of 0.05, and use a linear warm-up schedule for the first 1/10 optimization steps, followed by a polynomial decay. We use an A100-PCIE-40G GPU for training, which takes approximately 20 minutes for MiniGPT4 and 40 minutes for LLaVa1.5. There are around 6.3M and 25.2M trainable parameters for MiniGPT4 and LLaVa-1.5, respectively.

# Evaluation Results

Results on MSCOCO Table 1 reports the results on MSCOCO, we have following observations: First, the proposed ADAVIB yields consistent improvements across both MiniGPT4 and LLaVa-1.5 model architectures. Specifically, it outperforms one of the strongest baseline LURE13B by around $1 4 . 5 \%$ under both $\mathrm { C H A I R } _ { S }$ and $\mathrm { C H A I R } _ { I }$ metrics, especially under the $\mathrm { C H A I R } _ { S }$ metric, it surpasses LURE by around $2 4 . 9 \%$ across two distinct model architectures. Second, ADAVIB substantially outperforms the vanilla finetuning method. The improvement of ADAVIB over finetuning under $\mathrm { C H A I R } _ { S }$ and $\mathrm { C H A I R } _ { I }$ metrics are $2 3 . 6 \%$ and $2 1 . 2 \%$ , respectively. This indicates that the vanilla fully connected layer may not be strong enough to model the sophisticated mapping relations between vision and language. In contrast, our approach can improve the vision-language alignment by better compressing irrelevant visual information while maintaining relevant information for generating hallucination-free descriptions. Third, the ablation results indicate the effectiveness of each component. Specifically, by removing adaptive $\beta$ in equation 12, the results drop over $5 . 0 \%$ on average across two model architectures. Notably, by removing the reparameterization strategy in equation 9, the results drop over $1 5 . 0 \%$ on average, indicating the significance of this component in mitigating object hallucinations on the MSCOCO dataset.

Table 1: Experimental results on MSCOCO. The evaluation is performed using $\mathrm { C H A I R } _ { S }$ $( C _ { S } )$ and $\mathrm { C H A I R } _ { I }$ $( C _ { I } )$ , with smaller values indicating fewer object hallucinations. † means rerun using their released code. The best and the second-best are marked in bold and underlined, respectively.   

<html><body><table><tr><td rowspan="2">Model</td><td>MiniGPT-4</td><td colspan="2">LLaVa</td></tr><tr><td>Cs↓ C1↓</td><td>Cs↓</td><td>C1↓</td></tr><tr><td>Teacher13B</td><td>24.0 5.7</td><td>49.9</td><td>9.3</td></tr><tr><td>CoT13B</td><td>31.6 9.4</td><td>47.6</td><td>9.0</td></tr><tr><td>DOLA7B†</td><td>26.7 7.9</td><td>31.9</td><td>8.3</td></tr><tr><td>VCD7B†</td><td>24.4 7.5</td><td>26.2</td><td>7.8</td></tr><tr><td>LURE13B</td><td>19.7 4.9</td><td>27.1</td><td>6.4</td></tr><tr><td>Original7B</td><td>27.1 7.8</td><td>47.8</td><td>10.8</td></tr><tr><td>FT7B</td><td>19.3 6.4</td><td>26.7</td><td>7.2</td></tr><tr><td>ADAVIB7B</td><td>16.2 5.2</td><td>18.4</td><td>5.5</td></tr><tr><td>w/o Ada β</td><td>17.1 5.6</td><td>19.2</td><td>5.7</td></tr><tr><td>w/o Repara.</td><td>18.5 6.2</td><td>22.3</td><td>6.4</td></tr></table></body></html>

Results on POPE Table 2 reports the results on POPE. We observe that the improvements of ADAVIB on Popular are clearer than on Random and Adversarial. Concretely, compared with one of the strongest baselines LURE on MiniGPT4 backbone, the improvements of ADAVIB on Popular $( 8 . 1 \% )$ are more significant than on Random $( 4 . 8 \% )$ and Adversarial $( 3 . 9 \% )$ . Similar observations can be found when using the LLaVa backbone, ADAVIB achieves $7 . 5 \%$ improvements on Popular, while $2 . 5 \%$ improvements on Random and Adversarial by average. These observations indicate the advantages of ADAVIB in effectively mitigating the statistical bias arising from the most frequently appeared objects during training, reducing the over-reliance on irrelevant features, thereby alleviating object hallucinations. Additionally, the ablation results on POPE indicate the effectiveness of ADAVIB. Concretely, removing adaptive $\beta$ decreases ADAVIB by $1 . 5 \%$ and $1 . 4 \%$ on MiniGPT4 and LLaVA-1.5 by average, respectively. While removing the reparameterization strategy results in performance degradation by $2 . 1 \%$ and $2 . 2 \%$ on MiniGPT4 and LLaVA-1.5 by average, respectively. This observation indicates that both adaptive $\beta$ and reparameterization strategy is important to improve the performance of ADAVIB on the POPE dataset.

# Discussions

Does the ADAVIB mitigate object hallucinations by alleviating the overconfidence problem? To address this, we figure out the proportion of hallucinated samples in a specific range of the max similarity score to overall hallucinated samples. Figure 3 presents the results that employ the MiniGPT4 as the model backbone on the MSCOCO dataset. We have the following observations: First, for the fine-tuned (FT) variant, the similarity score ranging [0.9, 1.0) dominates the hallucinated samples, with an average similarity entropy of 0.64. A low entropy denotes a sharp similarity distribution, indicating that the fine-tuned variant is prone to overconfidence when visual tokens map to the LLM’s word embedding space. Second, both VIB and ADAVIB have a smoother distribution than the fine-tuned variant, with a consistent improvement on alleviating object hallucinations from the results reported in Table 1 and Table 2. This observation indicates that mitigating the overconfidence problem is one of the effective solutions to mitigate object hallucinations. Third, ADAVIB has fewer hallucinated samples with a large max similarity score than VIB. It has a smoother distribution than VIB, with a larger average similarity entropy (1.30 v.s. 0.97). This result indicates that the proposed adaptive $\beta$ mitigates object hallucinations by effectively controlling the smoothness of the similarity distribution.

Table 2: Experimental results on POPE. Larger values indicate less hallucinations.   

<html><body><table><tr><td>Model</td><td colspan="2">Random ACC↑F1↑</td><td colspan="2">Popular ACC↑F1↑ACC↑1↑</td><td colspan="2">Adversarial</td></tr><tr><td colspan="7">MiniGPT4</td></tr><tr><td>DOLA7B† VCD7B† LURE13B† FT7B</td><td>76.6 78.4 78.0 79.0</td><td>77.9 80.2 79.2 80.8</td><td>65.8 68.5 66.0 70.0</td><td>69.3 72.0 70.3 72.9</td><td>63.3 64.4 63.9 65.0 66.7 73.6</td><td>68.5 70.3 71.1 71.2</td></tr><tr><td colspan="7">ADAVIB7B 81.2</td></tr><tr><td>w/o Ada β</td><td>80.0 79.4</td><td>82.2 82.0</td><td>69.8 69.5</td><td>75.6 74.0</td><td>65.3</td><td>73.1</td></tr><tr><td>w/o Repara.</td><td></td><td></td><td>LLaVa</td><td></td><td>65.6</td><td>72.5</td></tr><tr><td colspan="7"></td></tr><tr><td>DOLA7B† VCD7B†</td><td>84.0 86.8</td><td>84.4</td><td>79.5 83.2</td><td>82.2 84.3</td><td>76.6 80.1</td><td>77.2</td></tr><tr><td>LURE13B</td><td>86.3</td><td>86.2 85.8</td><td>80.3</td><td>80.7</td><td>77.2</td><td>79.6 78.4</td></tr><tr><td>FT7B</td><td>84.3</td><td>85.8</td><td>84.9</td><td>84.0</td><td>76.5</td><td>77.7</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ADAVIB7B</td><td>86.3</td><td>87.0</td><td>86.3</td><td>86.8</td><td>78.0</td><td>80.4</td></tr><tr><td>w/o Ada β</td><td>85.4</td><td>86.0</td><td>85.0</td><td>85.6</td><td>76.9</td><td>79.0</td></tr><tr><td>w/o Repara.</td><td>84.9</td><td>85.3</td><td>84.5</td><td>85.1</td><td>76.2</td><td>78.1</td></tr></table></body></html>

![](images/b1f420bc0c74df100b656ca383c9dc654023ed4bd27168b6ca6f71630c144d75.jpg)  
Figure 3: Distribution of the max similarity score between soft visual tokens and LLM’s word embedding. We use MiniGPT4 as the model backbone. The $\mathbf { \boldsymbol { x } }$ -axis denotes the normalized similarity score, ranging from 0-1. The $\mathbf { y } .$ -axis denotes the proportion of hallucinated samples in a specific range to overall hallucinated samples.

![](images/a6ea737119a65fdd1b812437831e566951898909d6e84d3b81dee121de024eaa.jpg)  
Figure 4: Correlation between the KL loss (Equation 8) and the similarity entropy (Equation 10) over the course of training. All curves are smoothed by exponential moving average for better understanding the tendency.

Does the ADAVIB alleviate the overconfidence issue by effectively constraining irrelevant information? In addressing this question, we present the learning curves of the KL loss for ADAVIB, as well as the curves of similarity entropy for both ADAVIB and fine-tuning. From Figure 4, we have two main observations: First, with the training progress, the KL loss of ADAVIB (red curve) is decreased, accompanied by the increase of its similarity entropy (green curve). This phenomenon indicates that the compression term in Equation 5 effectively compresses the irrelevant information by adding stochastic noise, thereby progressively reducing the overconfidence in irrelevant visual features between the visual tokens and the LLM’s word embedding. Second, compared with the curves of similarity entropy between ADAVIB and fine-tuning, the entropy curve of the fine-tuning variant severely decreases with the training process going on, indicating the emergence of the overconfidence problem. In contrast, ADAVIB improves the performance by progressively smoothing the similarity distribution, avoiding overconfidence in irrelevant visual features.

How does different $\beta$ impact the model performance on mitigating object hallucinations? To answer this question, we employ both MiniGPT4 and LLaVa-1.5 as the backbone, adjusting $\beta$ and analyzing the change of the CHAIR score on MSCOCO evaluation dataset. From Figure 5, we observe that the performance of both $\mathrm { C H A I R } _ { S }$ and $\mathrm { C H A I R } _ { I }$ score improves along with the reduction of $\beta$ from $\beta = 1 e ^ { - 1 }$ to $\beta = \bar { 1 e } ^ { - 7 }$ , followed by an obvious performance degradation with $\beta = 1 e ^ { - 9 }$ . This observation indicates that setting the $\beta$ to a proper value is essential to effectively constrain the information flow while preserving the useful information as much as possible. A large $\beta$ introduces a big noise during the model optimization, resulting in the model scarcely capturing the essentials relevant to the golden answer, thereby converging to a worse performance. When the $\beta$ is too small, the model tends to preserve relevant information rather than compress the irrelevant features present in the original input, delivering a sub-optimal performance in mitigating object hallucinations. Moreover, setting $\beta$ to a fixed value, regardless of the dynamic nature per sample, cannot achieve the optimal performance in mitigating hallucinations. This conclusion is consistent with the results reported in Table 1, where removing the adaptive $\beta$ results in a worse performance compared to the variant with the adaptive one.

![](images/7b59487464b99abbb0c1828fcf65f3eb9d2df2eb3001b1af92139def84e599cf.jpg)  
Figure 5: Impact of different $\beta$ on object hallucinations. Figure 5(a) and Figure 5(b) present the results that leverages MiniGPT4 and LLaVa-1.5 as backbone, respectively.

How does ADAVIB perform compared to other regularization strategies? In addition to the fine-tuning baseline that applies the weight decay (Loshchilov and Hutter 2017) as a regularizer. We also deploy dropout (Srivastava et al. 2014) to the input $( \mathrm { D R P } _ { i n } )$ ) and output $\left( \mathrm { D R P } _ { o u t } \right)$ ) of the vision-language projector upon the fine-tuning method, with the dropout rate of 0.1. Table 3 presents results. We observe that variants equipped with dropout effectively reduce object hallucinations compared to the FT, while they still have a performance gap between ADAVIB. This indicates the superiority of ADAVIB in the trade-off between compressing irrelevant while preserving relevant information to represent visual tokens precisely.

Table 3: Impact on different regularization strategies. “DRP” is the abbreviation of the “dropout”.   

<html><body><table><tr><td rowspan="2">Model</td><td colspan="2">MiniGPT4</td><td colspan="2">Cs↓LaVC1↓</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>FT</td><td>19.3</td><td>6.4</td><td>26.7</td><td>7.2</td></tr><tr><td>FT+DRPin</td><td>18.5</td><td>6.1</td><td>23.4</td><td>6.4</td></tr><tr><td>FT+DRPout</td><td>18.6</td><td>5.9</td><td>22.7</td><td>6.2</td></tr><tr><td>ADAVIB</td><td>16.2</td><td>5.2</td><td>18.4</td><td>5.5</td></tr></table></body></html>

# Conclusions

In this paper, we propose using VIB to mitigate object hallucinations. The proposal is based on our observation that the object hallucination can be attributed to the overconfidence in irrelevant visual features when soft visual tokens project to the word embedding space of LLM. Motivated by our observation, we propose ADAVIB to adaptively constrain irrelevant information regarding the smoothness of similarity distribution. Experimental results and comprehensive analysis demonstrate the effectiveness of our approach with consistent improvements over competitive baselines.