# Towards Verifiable Text Generation with Generative Agent

Bin $\mathbf { J i } ^ { 1 * }$ , Huijun Liu1\*, Mingzhe $ { \mathbf { D } }  { \mathbf { u } } ^ { 2 }$ , Shasha $\mathbf { L i } ^ { \dagger }$ , Xiaodong $\mathbf { L i u } ^ { 1 \dagger }$ , $\mathbf { J u n M a ^ { \scriptscriptstyle 1 \dag } }$ , Jie $\mathbf { Y } \mathbf { u } ^ { 1 \dag }$ , See-Kiong $\mathbf { N g } ^ { 3 }$

1College of Computer Science and Technology, National University of Defense Technology 2Nanyang Technological University 3 National University of Singapore {jibin, liuhuijun, shashali, liuxiaodong, majun, yj}@nudt.edu.cn, {mingzhe, seekiong}@nus.edu.sg

# Abstract

Text generation with citations makes it easy to verify the factuality of Large Language Models’ (LLMs) generations. Existing one-step generation studies expose distinct shortages in answer refinement and in-context demonstration matching. In light of these challenges, we propose ${ \mathrm { R } } ^ { 2 }$ -MGA, a Retrieval and Reflection Memory-augmented Generative Agent. Specifically, it first retrieves the memory bank to obtain the best-matched memory snippet, then reflects the retrieved snippet as a reasoning rationale, next combines the snippet and the rationale as the best-matched in-context demonstration. Additionally, it is capable of in-depth answer refinement with two specifically designed modules. We evaluate ${ \tt R } ^ { 2 }$ -MGA across five LLMs on the ALCE benchmark. The results reveal ${ \tt R } ^ { 2 }$ -MGA’ exceptional capabilities in text generation with citations. In particular, compared to the selected baselines, it delivers up to $+ 5 8 . 8 \%$ and $+ 1 5 4 . 7 \%$ relative performance gains on answer correctness and citation quality, respectively. Extensive analyses strongly support the motivations of ${ \tt R } ^ { 2 }$ -MGA.

EQQ Which is the most rainy place on earth? Question [1] ... Cherrapunji has often [2]  Based on the "Guinness been credited as being the Book of World Records", D wettest place on Earth ... Mawsynram received of ...   
Retrieved [3] ... Cherrapunji has often been credited as being the   
documents wettest place on Earth, but for now nearby Mawsynram currently holds that distinction.. (a) Question and retrieved documents EQb Cherrapunji is the most rainy place on earth [1]. R LLM (b) One-step generation The official record is held by Mawsynram [2][3], although Q H nearby  Cherrapunji holds ACT the record for most rain in a D calendar month for July 1861 and most rain in a year from August 1860 to July 1861 [1]. Agent (c) Agent generation

# Introduction

With the advent of Large Language Models (LLMs), they have shown exceptional capabilities in completing various complex tasks like text generation (Wang et al. 2023b). Although these generations are usually coherent, they may be unfaithful because of LLMs’ tendency to hallucinate. To authenticate the factuality, a novel research topic of text generation with citations is brought to the public (Gao et al. 2023; Funkquist et al. 2023), which focuses on instructing LLMs to provide citations to support their generations.

Research on text generation with citations begins with commercial search engines such as Bing Chat1, which integrate with LLMs like GPT models (OpenAI 2023). In particular, LLMs generate answers to given questions by synthesizing and citing searched results. However, these scenarios fall short of automatically evaluating the generation quality, which call for high-cost and low-efficiency human evaluations instead (Liu, Zhang, and Liang 2023). In light of this challenge, Gao et al. (2023) propose ALCE, an automatic evaluation benchmark for text generation with citations. Extensive efforts have been taken centering around ALCE. The one-step generation study is a representative line of work, which directly prompts LLMs to generate answers to questions by synthesizing retrieved documents, as the example in Figure 1(b) shows. Gao et al. (2023) explore a series of LLMs in the one-step generation study and propose several off-the-shelf document retrieval settings. Ji et al. (2024) incorporate Chain-of-Thought into the one-step generation study. Li et al. (2023) and Li et al. (2024) investigate advanced techniques to improve document retrieval accuracy in the one-step generation study.

Despite the great success achieved by existing one-step generation studies, they expose two distinct shortages: (1) They fail to provide matched in-context demonstrations. To be more precise, they typically use the same demonstrations across all cases. However, the question type in the demonstrations may mismatch the type of the to-answer question, which is likely to confuse LLMs and mislead them to generate inferior answers. (2) They cannot further refine their generations. To be specific, like humans, LLMs do not always output the best generation on their first try (Madaan et al. 2024), indicating the great potential in improving the generation quality.

To tackle the above challenges, we propose an LLMpowered agent, named Retrieval and Reflection Memoryaugmented Generative Agent $\left( \mathbf { R } ^ { 2 } { \cdot } \mathbf { M } \mathbf { G } \mathbf { A } \right)$ , which consists of four modules, i.e., Memory, Initialization, Assessment, Planning & Action. $\mathbb { R } ^ { 2 }$ -MGA is designed to generate highquality and verifiable answers to questions by using the bestmatched demonstration for in-context learning supervision and further refining the answers to improve their quality. Specifically, the Memory module contains a memory bank that stores three types of $\mathbb { R } ^ { 2 }$ -MGA’s histories, namely fullgeneration history, assessment history, and planning history. Given a question and retrieved documents, the Initialization module first generates an initial answer; then, the Assessment module assesses the answer quality from multiple aspects and returns feedback; finally, the Planning & Action module first plans future actions based on the feedback and then executes concrete actions to refine the initial answer, as shown in Figure 1(c). Each module necessitates a 1-shot demonstration for in-context learning supervision. To obtain the best-matched one, we first retrieve the best-matched memory snippet from the memory bank; then, we reflect the retrieved snippet by generating a high-level reasoning rationale; finally, we combine the memory snippet and the rationale as the best-matched in-context demonstration.

We evaluate ${ \tt R } ^ { 2 }$ -MGA on five state-of-the-art LLMs. Experimental results on the ALCE benchmark demonstrate that $\bar { \mathbf { R } } ^ { 2 }$ -MGA significantly improves the generation quality, including answer correctness, citation recall and precision. Specifically, (1) compared to one-step generation baselines, $\bar { \mathsf { R } ^ { 2 } }$ -MGA delivers up to $+ 5 8 . 8 \%$ , $+ 1 3 9 . 3 \%$ , and $+ 1 5 4 . 7 \%$ relative performance gains on the Correctness, Citation Recall and Precision, respectively; (2) the LLaMA-70Bpowered ${ \tt R } ^ { 2 }$ -MGA consistently outperforms the ChatGPTbased one-step generation baseline, and even achieves competitive performance compared to the GPT-4-based baseline. Extensive analyses and ablation studies further firmly verify the effectiveness of ${ \tt R } ^ { 2 }$ -MGA.

# Related Work

Text Generation with Citations. This research topic is brought to the public due to LLMs’ tendency to hallucinate. In practical scenarios, existing efforts incorporate LLMs into commercial search engines like Bing Chat, Perplexity2, and Coral3. These systems directly generate answers to questions with verifiable links to web pages. However, it’s hard to automatically evaluate the answer correctness with these links. In academic research scenarios, several benchmarks are specifically designed to evaluate text generation with citations automatically, e.g., ALCE (Gao et al. 2023) and

CiteBench (Funkquist et al. 2023). Centering around these benchmarks, Li et al. (2023), Ji et al. (2024), Li et al. (2024), Wei, Chen, and Meng (2024), and Xia et al. (2024) explore the one-step generation approaches. Ye et al. (2024) and Huang et al. (2024) fine-tune LLMs for the task. However, existing studies use the same in-context demonstrations across all cases, which may mismatch with the to-answer question, decreasing the utility of in-context learning. In contrast, $\mathbb { R } ^ { 2 }$ -MGA retrieves its memory and generates reasoning reflections to obtain the best-matched demonstration.

LLM-powered Agents. AI agents have long been regarded as a feasible way to achieve artificial general intelligence (Wang et al. 2024). Compared to LLMs, agents should learn and complete tasks in dynamic environments and borrow experience from their memory to facilitate coherent and believable actions (Park et al. 2023). LLM-powered agents take LLMs as the backbone and have been comprehensively explored, such as general agent (Wei et al. 2022; Yao et al. 2024; Qiao et al. 2024), simulation agent (Park et al. 2023), tool agent (Qin et al. 2024; Gou et al. 2024), embodied agent (Cho, Yoon, and Ahn 2024; Choi et al. 2024; Zheng et al. 2024; Zhang et al. 2024), conversational agent (Deng et al. 2024a,b), and game agent (Wang et al. 2023a). As for the text generation with citations, Lee et al. (2024) and Sun et al. (2023) propose agent-like studies to refine the initial answers. However, these studies neither borrow experience from agents’ histories nor provide matched demonstrations. In contrast, $\mathbb { R } ^ { 2 }$ -MGA tackles these challenges.

# Task Formalization

For fair comparisons, we follow the task formalization presented in ALCE (Gao et al. 2023), as shown below: Given a question $\mathcal { Q }$ and retrieved documents (i.e., $\mathcal { D } \ =$ $\{ d _ { 1 } , d _ { 2 } , \cdots , d _ { m } \} )$ that contain the knowledge to answer $\mathcal { Q }$ , a generation system is required to generate an answer $\mathcal { A }$ by synthesizing $d _ { i } \in \mathcal { D }$ . $\mathcal { A }$ is composed of multiple statements, i.e., $\mathcal { A } \ = \ s _ { 1 } s _ { 2 } \cdot \cdot \cdot s _ { n }$ , where each $s _ { i }$ is a factual statement summarized from a set of relevant documents, i.e., $\mathcal { D } _ { i } = \{ d _ { i , 1 } , d _ { i , 2 } , \cdot \cdot \cdot \}$ , and $s _ { i }$ should cite each document included in $\mathcal { D } _ { i }$ using the format of [1][2][3].

Note that ALCE regards each sentence of $\mathcal { A }$ as a statement, which can be distinguished by the full stop symbol. Additionally, it allows for at least one and at most three citations for each statement.

# Method

We introduce the ${ \tt R } ^ { 2 }$ -MGA framework, a generative agent augmented by memory retrieval and reflection. It is expected to generate answers to given questions by synthesizing retrieved documents and properly citing them. The overview of ${ \tt R } ^ { 2 }$ -MGA is presented in Figure 2, consisting of four modules: Memory, Initialization, Assessment, and Planning & Action, with Memory being the core module.

# Memory Module

Memory Snippets. The Memory module contains a memory bank, which stores ${ \tt R } ^ { 2 }$ -MGA’s three types of histories, i.e., full-generation history, assessment history, and planning history. We first illustrate three symbols in the following:

![](images/9ed43b9f96de15b2c0594fa473d117c60514c93b071d5c7d4d1ff9399d599f81.jpg)  
Figure 2: Overview of ${ \tt R } ^ { 2 }$ -MGA. Given a question $\mathcal { Q }$ and retrieved documents $\mathcal { D }$ , the Initialization module first generates an initial answer $\mathcal { A }$ ; then, the Assessment module assesses $\mathcal { A }$ from multiple aspects and returns feedback $\mathcal { F }$ ; finally, the Planning & Action module first plans actions $\tau$ based on $\mathcal { F }$ , and then executes concrete actions to refine $\mathcal { A }$ and outputs the final answer $\mathcal { A } ^ { \ast }$ . In each module, we retrieve the memory and reflect the retrieved snippet to obtain the best-matched in-context demonstration.

• $\mathcal { F }$ , the answer assessment feedback returned by the Assessment module.   
• $\tau$ , the future actions planned by the Planning & Action module.   
• $A ^ { * }$ , the final answer obtained by refining the initial answer $\mathcal { A }$ .

The full-generation history is constructed by answer generation steps. Each memory snippet can be represented by $\mathcal { M } _ { i } = \{ \bar { \mathcal { Q } } , \mathcal { D } , \mathcal { A } , \mathcal { F } , \mathcal { T } , \mathcal { A } ^ { * } \}$ , including all generation steps in the Initialization, Assessment, and Planning & Action modules.

The assessment history is built with the feedback generation steps, which assess the initial answer $\mathcal { A }$ from multiple aspects and return the feedback $\mathcal { F }$ . Each memory snippet can be denoted as $\mathcal { M } _ { j } = \{ \mathcal { Q } , \mathcal { D } , \mathcal { A } , \mathcal { F } \}$ .

The planning history is built with the action planning steps, which plan future actions based on the feedback $\mathcal { F }$ . Each memory snippet can be expressed as $\begin{array} { r l } { \mathcal { M } _ { k } } & { { } = } \end{array}$ $\{ \mathcal { Q } , \mathcal { D } , \mathcal { A } , \mathcal { F } , \mathcal { T } \}$ .

We set a specific indicator for each type of memory in the memory bank to distinguish them in the memory bank.

Memory Retrieval and Reflection. An extensive token budget is required to inject each memory snippet into the agent’s current running memory, which will be limited by LLMs’ context window. Meanwhile, some memory snippets, due to the inconsistency of their question types to the to-answer question type, reduce the utility of in-context learning supervision. As such, we introduce a best-matching approach to retrieve the best-matched snippets within the memory bank to maximize the utility of in-context learning supervision.

Formally, in the Initialization, given a to-answer question $Q$ and retrieved documents $\mathcal { D }$ , we take ${ < Q , D > }$ as the query to retrieve $\mathcal { M } _ { i }$ type memory snippets. Similarly, in the Assessment, we take $< Q , D , A >$ as the query to retrieve $\mathcal { M } _ { j }$ type memory snippets; in the Planning & Action, we take $< Q , { \mathcal { D } } , { \mathcal { A } } , { \mathcal { F } } { > }$ as the query to retrieve $\mathcal { M } _ { k }$ type memory snippets. Figure 2 also presents the query constitutions.

Following Park et al. (2023) and Deng et al. (2024b), we use the language model to generate an embedding vector of the query and each memory snippet. Then, we compute the cosine similarity in the embedding space to retrieve the bestmatched memory snippet.

Memory reflection enables agents to independently summarize and infer more abstract, complex, high-level information (Wang et al. 2023b). To enrich memory snippet information, we leverage LLMs’ exceptional reasoning capability to generate intermediate reasoning rationale as a supervised signal. In particular, for the retrieved memory snippet $\mathcal { M }$ , we prompt OpenAI’s $\mathtt { g p t } - 3 . 5 \mathtt { - t u r b o - } 0 1 2 5$ to generate an in-depth rationale $r$ explaining the reason for the decision-making process included in $\mathcal { M }$ .

We combine the retrieved memory $\mathcal { M }$ with the reasoning rationale $r$ as the best-matched demonstration in the Initialization, Assessment, and Planning & Action modules.

# Initialization, Assessment, and Planning & Action Modules

Given $\mathcal { Q }$ and $\mathcal { D }$ , the three modules generate the final answer through the following three steps.

(1) Initialization. This module generates an initial answer for $\mathcal { Q }$ . To be specific, first retrieve the best-matched memory snippet $\mathcal { M } _ { i }$ and obtain the reasoning rationale $\boldsymbol { r } _ { i }$ ; then, build the initialization prompt, which can be represented by $\mathcal { P } _ { I } = \{ \mathcal { T } , \mathcal { M } _ { i } , r _ { i } , \mathcal { Q } , \hat { D } \}$ , where $\boldsymbol { \mathcal { T } }$ is the instruction; at last, instruct LLMs to generate an initial answer $\mathcal { A }$ and pass it to the Assessment module.

(2) Assessment. This module assesses the quality of $\mathcal { A }$ from multiple aspects, including answer completeness, answer correctness, and citation correctness. To be precise, first retrieve the best-matched memory snippet $\mathcal { M } _ { j }$ and obtain the reasoning rationale $r _ { j }$ ; then, build the assessment prompt, which can be represented by $\mathcal { P } _ { A } \ =$ $\{ \mathcal { T } , \mathcal { M } _ { j } , r _ { j } , \mathcal { A } \}$ ; at last, instruct LLMs to generate the feedback $( i . e . , \mathcal { F } )$ of $\mathcal { A }$ and pass it to the Planning & Action module.

(3) Planning $\pmb { \mathscr { k } }$ Action. This module first plans answer refinement actions based on $\mathcal { F }$ , and then executes concrete actions to refine $\mathcal { A }$ . In particular, first retrieve the bestmatched memory snippet $\mathcal { M } _ { k }$ and obtain the reasoning rationale $r _ { k }$ ; then, build the planning prompt, which can be represented by $\mathcal { P } _ { P } = \{ \bar { \mathcal { Z } } , \mathcal { M } _ { k } , \bar { r } _ { k } , \mathcal { Q } , \bar { D } , \mathcal { A } , \mathcal { F } \}$ ; next, instruct LLMs to generate future answer refinement actions $( i . e . , \tau )$ ; at last, instruct LLMs to refine $\mathcal { A }$ based on $\tau$ , obtaining the final answer $\mathcal { A } ^ { \ast }$ . There are two paradigms to execute actions: 1) one-time execution and 2) step-by-step execution.

We update the memory bank according to the above three new experiences. To avoid updating the memory with bad experiences, we first prompt an LLM to re-assess $\mathcal { A } ^ { \ast }$ , and then update the memory if and only if $\mathcal { A } ^ { \ast }$ is assessed to be correct and have proper citations.

# Discussions

Sun et al. (2023) propose a framework (VTG) to first generate an initial answer, and then verify and simplify the citations. Madaan et al. (2024) and Lee et al. (2024) propose two similar frameworks (SELF-REFINE and $\mathbf { A } ^ { 2 } \mathbf { R } ^ { \cdot }$ ), which prompt LLMs to generate, assess, and refine their outputs. We demonstrate that our ${ \tt R } ^ { 2 }$ -MGA significantly differs from the above studies. In particular, ${ \tt R } ^ { 2 }$ -MGA manages a memory bank, enabling $\dot { \mathsf { R } } ^ { 2 }$ -MGA to borrow its successful experiences by retrieving the best-matched in-context demonstration for new generation cases, which ensures consistent agent behaviours and improves the generation quality. In contrast, the above studies don’t retain such a memory bank and always use the same demonstrations across all cases.

We claim that the best-matched demonstration can maximize the utility of in-context learning supervision. For example, given a to-answer question “Who has the highest goals all-time in men’s football?”, the demonstration regarding “Who has the highest goals in men’s world international football?” is more informative than the demonstration regarding “When was the first apple i phone made?”, since the former is more matched with the to-answer question than the latter one.

Research on retrieving high-quality in-context demonstrations has been widely explored. For example, Wang, Yang, and Wei (2024) train dense retrievers to identify suitable incontext examples. Shum, Diao, and Zhang (2023) propose a variance-reduced policy gradient strategy to measure the importance of candidate in-context examples. Yu, He, and Ying (2024) propose analogical prompting, which instructs LLMs to self-generate relevant in-context examples. Different from these sophisticated approaches, we first compute the cosine similarity between the query and memory snippets to retrieve the best-matched memory snippet, then combine the snippet with a reasoning rationale summarized from the snippet as the best-matched in-context demonstration. We conduct investigations in the Analyses section.

# Experiments

# Benchmark, LLMs, and Implementation Details

The ALCE benchmark collects three datasets, i.e., ASQA (Stelmakh et al. 2022), QAMPARI (Rubin et al. 2022), and ELI5 (Fan et al. 2019) and pre-defines automatic evaluation metrics. In particular, it uses Correctness (Correct.), Citation Recall (Rec.) and Precision (Prec.) to evaluate ASQA and ELI5, and uses Correctness Recall-5 (Rec.-5) and Precision (Prec.), Citation Recall and Precision to evaluate QAMPARI. We report more benchmark details in Appendix A.

We build ${ \tt R } ^ { 2 }$ -MGA upon five LLMs including closedsource ChatGPT $( \mathtt { g p t } - 3 . 5 \mathtt { - t u r b o - } 0 . 3 0 1 )$ and GPT$4 \ ( { \tt g p t } - 4 - 0 \in 1 3 )$ , and open-source LLaMA-2-70B-Chat (“LLaMA-70B” for short), Vicuna-13B, and LLaMA-2-7BChat (“LLaMA-7B” for short).

We use four NVIDIA A100 40GB GPUs to run ${ \tt R } ^ { 2 }$ - MGA. For ${ \tt R } ^ { 2 }$ -MGA built upon open-source LLMs, we evaluate it by setting the LLMs’ temperature to 0.001, 0.1, 0.3, 0.5, 0.7, 0.9, and 1, respectively, and report the averaged performance. Limited by the API costs of ChatGPT and GPT-4, we solely set the temperature value to 1 for them. We report more implementation details in Appendix B.

# Baselines

For fair comparisons, we use the one-step generation approaches proposed by Gao et al. (2023) and Ji et al. (2024) as the baselines, which are denoted as ALCE-base and ALCECoT, respectively. In particular, the ALCE-base is the benchmark baseline included in ALCE; and the ALCE-CoT integrates the Chain-of-Thought into the ALCE-base, empowering LLMs to mimic the human thought process. We report more baseline details in Appendix C.

There exist some other studies, including fine-tuning taskspecific LLMs (Huang et al. 2024; Lee et al. 2024) and onestep generation with answer refinement (Sun et al. 2023). We demonstrate that these studies adopt quite different LLMs and experimental settings. Despite these differences, we make attempts to compare our $\bar { \mathsf { R } } ^ { 2 }$ -MGA with them fairly. Appendix D reports the comparison results.

# Main Results

We use the one-time paradigm to execute the answer refinement actions and report performance comparisons between our ${ \tt R } ^ { 2 }$ -MGA and the baselines in Table 1. We have the following observations:

<html><body><table><tr><td rowspan="2">Strategy</td><td rowspan="2">Approach</td><td colspan="3">ASQA</td><td colspan="3">QAMPARI</td><td colspan="3">ELI5</td></tr><tr><td rowspan="2">Correct.</td><td colspan="2">Citation</td><td colspan="2">Correct.</td><td colspan="2">Citation</td><td rowspan="2">Correct.</td><td colspan="2">Citation</td></tr><tr><td></td><td></td><td>Rec. Prec.</td><td>Rec.-5</td><td>Prec.</td><td>Rec.</td><td>Prec.</td><td></td><td>Rec. Prec.</td></tr><tr><td colspan="10"></td></tr><tr><td>VANILLA</td><td>ALCE-base R²-MGA (Ours)</td><td>41.3 42.6</td><td>68.5</td><td>GPT-4 75.6</td><td>22.2</td><td>25.0</td><td>25.9</td><td>27.0</td><td>14.2</td><td>44.0</td><td>50.1</td></tr><tr><td colspan="10">84.7 85.2 25.3 27.6</td></tr><tr><td rowspan="2"></td><td>ALCE-base</td><td></td><td></td><td>ChatGPT</td><td></td><td></td><td>32.4</td><td>33.9</td><td>18.2</td><td>69.8</td><td>70.5</td></tr><tr><td></td><td>40.4</td><td>73.6</td><td>72.5</td><td>20.8</td><td>20.8</td><td>20.5</td><td>20.9</td><td>12.0</td><td>51.1</td><td>50.0</td></tr><tr><td rowspan="2">VANILLA</td><td>R²-MGA (Ours)</td><td>41.2</td><td>84.4</td><td>84.3</td><td>24.2</td><td>23.0</td><td>29.8</td><td>28.1</td><td>16.4</td><td>74.5</td><td>75.1</td></tr><tr><td>ALCE-base</td><td>43.3</td><td>68.8</td><td>61.8</td><td>23.6</td><td>21.2</td><td>23.6</td><td>25.7</td><td>12.5</td><td>51.5</td><td>48.2</td></tr><tr><td rowspan="2">SUMM SNIPPET</td><td>R²-MGA (Ours)</td><td>44.6</td><td>81.4</td><td>80.2</td><td>25.2</td><td>24.1</td><td>32.9</td><td>33.4</td><td>16.9</td><td>72.4</td><td>74.3</td></tr><tr><td>ALCE-base</td><td>41.4</td><td>65.3</td><td>57.4</td><td>24.5</td><td>21.5</td><td>22.9</td><td>24.9</td><td>14.3</td><td>50.4</td><td>45.0</td></tr><tr><td rowspan="2">ORACLE</td><td>R²-MGA (Ours)</td><td>43.0</td><td>77.4</td><td>75.1</td><td>25.1</td><td>24.0</td><td>34.1</td><td>33.6</td><td>15.8</td><td>73.2</td><td>73.8</td></tr><tr><td>ALCE-base R²-MGA (Ours)</td><td>48.9 51.1</td><td>74.5 89.6</td><td>72.7 86.9</td><td>37.0</td><td>36.9</td><td>24.1</td><td>24.6</td><td>21.3 27.2</td><td>57.8 77.2</td><td>56.0</td></tr><tr><td colspan="10">42.1 41.4 34.4 34.7</td></tr><tr><td rowspan="3">VANILLA</td><td></td><td></td><td></td><td>LLaMA-70B</td><td></td><td></td><td></td><td></td><td></td><td></td><td>79.2</td></tr><tr><td>ALCE-base</td><td>41.5</td><td>62.9</td><td>61.3</td><td>21.8</td><td>18.4</td><td>15.1</td><td>15.6</td><td>12.8</td><td>38.3</td><td>37.9</td></tr><tr><td>ALCE-CoT</td><td>43.9</td><td>70.2</td><td>69.8</td><td>23.2</td><td>21.7</td><td>22.3</td><td>24.0</td><td>13.9</td><td>51.1</td><td>49.1</td></tr><tr><td rowspan="2">SUMM</td><td>R²-MGA (Ours) ALCE-CoT</td><td>44.7</td><td>77.6</td><td>73.9</td><td>23.7</td><td>23.2</td><td>24.2</td><td>25.8</td><td>18.1</td><td>57.4</td><td>57.7</td></tr><tr><td>R²-MGA (Ours)</td><td>44.6</td><td>71.4</td><td>68.0</td><td>23.6</td><td>22.0</td><td>23.1</td><td>23.7</td><td>12.7</td><td>49.7</td><td>48.4</td></tr><tr><td rowspan="2">SNIPPET</td><td>ALCE-CoT</td><td>45.1</td><td>79.0</td><td>74.2</td><td>24.2</td><td>23.7</td><td>23.9</td><td>25.8</td><td>17.4</td><td>56.8</td><td>57.2</td></tr><tr><td>R²-MGA (Ours)</td><td>42.9</td><td>70.4</td><td>69.4</td><td>22.9</td><td>23.1</td><td>22.3</td><td>23.8</td><td>13.9</td><td>49.6</td><td>45.2</td></tr><tr><td rowspan="2">ORACLE</td><td></td><td>43.4</td><td>79.3</td><td>75.6</td><td>24.9</td><td>24.2</td><td>23.8</td><td>25.2</td><td>18.6</td><td>56.6</td><td>55.4</td></tr><tr><td>ALCE-CoT R²-MGA (Ours)</td><td>46.2 50.0</td><td>73.5 87.5</td><td>72.9 83.8</td><td>36.1</td><td>36.7</td><td>24.4</td><td>24.4 32.0</td><td>20.8 26.8</td><td>56.7 76.4</td><td>55.3 78.2</td></tr><tr><td colspan="10">40.8 41.1 33.7</td></tr><tr><td rowspan="3">VANILLA</td><td>ALCE-base</td><td>31.9</td><td>51.1</td><td></td><td>Vicuna-13B</td><td></td><td></td><td></td><td>10.0</td><td>15.6</td><td></td></tr><tr><td>ALCE-CoT</td><td>36.4</td><td>56.6</td><td>50.1</td><td>14.0</td><td>15.9</td><td>12.5</td><td>13.4 15.4</td><td></td><td></td><td>19.6</td></tr><tr><td>R²-MGA (Ours)</td><td>37.1</td><td>73.5</td><td>56.4 67.9</td><td>18.1 20.1</td><td>17.4 19.8</td><td>11.6 23.9</td><td>23.4</td><td>13.3 14.3</td><td>21.3 38.9</td><td>24.1 40.4</td></tr><tr><td rowspan="3">SUMM</td><td>ALCE-base</td><td>43.2</td><td>52.7</td><td>50.0</td><td>21.1</td><td>17.1</td><td>15.7</td><td>17.8</td><td>4.9</td><td>9.7</td><td>12.2</td></tr><tr><td>ALCE-CoT</td><td>43.2</td><td>56.9</td><td>56.8</td><td>25.6</td><td>19.8</td><td>17.2</td><td>19.3</td><td>10.2</td><td>11.7</td><td>15.8</td></tr><tr><td>R²-MGA (Ours)</td><td>43.7</td><td>71.2</td><td>69.3</td><td>25.8</td><td>20.6</td><td>27.4</td><td>26.9</td><td>16.2</td><td>28.0</td><td>28.7</td></tr><tr><td rowspan="3">SNIPPET</td><td>ALCE-base</td><td>42.1</td><td>53.4</td><td>48.7</td><td>21.9</td><td>18.2</td><td>16.8</td><td>19.7</td><td>11.2</td><td>27.2</td><td>27.9</td></tr><tr><td>ALCE-CoT</td><td>42.9</td><td>55.6</td><td>52.3</td><td>25.6</td><td>21.3</td><td>19.4</td><td>24.4</td><td>13.6</td><td>29.3</td><td>33.6</td></tr><tr><td>R²-MGA (Ours)</td><td>43.1</td><td>70.1</td><td>68.2</td><td>27.0</td><td>23.4</td><td>26.4</td><td>27.3</td><td>19.7</td><td>38.9</td><td>40.4</td></tr><tr><td rowspan="3">ORACLE</td><td>ALCE-base</td><td>42.5</td><td>52.2</td><td>50.7</td><td>25.9 27.8</td><td>28.4 31.2</td><td>15.8 20.2</td><td>16.8 19.7</td><td>17.1 22.1</td><td>20.2 24.3</td><td>26.5 31.1</td></tr><tr><td>ALCE-CoT R²-MGA (Ours)</td><td>42.7 44.0</td><td>56.2</td><td>54.8</td></table></body></html>

Table 1: Performance comparisons between our $\mathbb { R } ^ { 2 }$ -MGA and one-step generation baselines (ALCE-base and ALCE-CoT) on the ALCE datasets. Bolded values denote the best performance scores under each $<$ Strategy, $\mathbf { L L M } >$ setting.

(1) From the holistic perspective, ${ \tt R } ^ { 2 }$ -MGA consistently improves the answer correctness and citation quality by large margins.   
(2) On ASQA, $\mathbb { R } ^ { 2 }$ -MGA delivers up to $+ 1 6 . 8 \%$ (VANILLA, LLaMA-7B), $+ 4 1 . 1 \%$ (VANILLA, LLaMA-7B), and $+ 3 9 . 8 \%$ (VANILLA, LLaMA-7B) relative gains on Correctness, Citation Recall and Precision, respectively. On average, it brings $+ 3 . 7 \%$ , $+ 2 1 . 6 \%$ , and $+ 2 0 . 5 \%$ relative gains on the three metrics.   
(3) On QAMPARI, ${ \mathrm { R } } ^ { 2 }$ -MGA delivers up to $+ 3 1 . 5 \%$ (VANILLA, LLaMA-7B), $+ 3 0 . 7 \%$ (VANILLA, LLaMA7B), $+ 1 0 6 . 0 \%$ (VANILLA, Vicuna-13B), and $+ 9 9 . 1 \%$ (VANILLA, LLaMA-7B) relative gains on Correctness Recall-5 and Precision, Citation Recall and Precision, respectively. On average, it brings $+ 9 . 6 \%$ , $+ 1 1 . 2 \%$ , $4 1 . 9 \%$ , and $+ 3 2 . 8 \%$ relative gains on the four metrics.

(4) On ELI5, ${ \tt R } ^ { 2 }$ -MGA delivers up to $+ 5 8 . 8 \%$ (SUMM, Vicuna-13B), $+ 1 3 9 . 3 \%$ (SUMM, Vicuna-13B), and $+ 1 5 4 . 7 \%$ (VANILLA, LLaMA-7B) relative gains on Correctness, Citation Recall and Precision, respectively. On average, it brings $+ 3 0 . 8 \%$ , $+ 5 1 . 3 \%$ , and $+ 5 1 . 0 \%$ relative gains on the three metrics.

We attribute these performance gains in answer correctness and citation quality to the memory-augmented retrieval and reflection strategy and the answer refinement strategy, where the former provides the best-matched demonstration to facilitate in-context learning supervision, and the latter further improves the answer quality.

${ \tt R } ^ { 2 }$ -MGA built upon LLaMA-70B consistently outperforms the ChatGPT-powered ALCE-base across the three benchmark datasets and the four prompting strategies, and it even outperforms the GPT-4-based baseline in 6 out of 10 evaluation cases under the VANILLA setting, indicating ${ \tt R } ^ { 2 }$ - MGA as a strong baseline to facilitate future study.

Take inspiration from SELF-REFINE (Madaan et al. 2024), we investigate iteratively executing Assessment and

Citation-R Correct.-P Rec.-5 Citation-R 75 20 25 50 55 30 3 Correct. 0 Correct. Citation-R Citation-P   
Citation-P Citation-P one-time + step-step ·one-time → step-step +one-time ·step-step (a) ASQA (b) QAMPARI (c)ELI5

Planning & Action to continually refine the answer. More investigation details can be found in Appendix E.

# Analyses

# Investigation on Action Executions

We pre-define the one-time and step-by-step action execution paradigms, and use the one-time paradigm to report the main experimental results. In this section, we compare the one-time paradigm with the step-by-step paradigm under the $<$ <VANILLA, Vicuna- $\phantom { + } 1 3 \mathbf { B } >$ setting. Figure 3 presents the comparisons, from which we can observe that the onetime paradigm consistently outperforms the step-by-step one across all datasets and all metrics. A convincing reason is that the step-by-step paradigm may cause new answer flaws that can be solved in previous actions but cannot in subsequent actions. In contrast, the one-time paradigm refines the answers holistically, avoiding the above problems.

Motivated by this investigation, we use the one-time paradigm in the other experiments.

# Investigation on Memory Bank

The memory bank records ${ \tt R } ^ { 2 }$ -MGA’s experiences (i.e., memory snippets) and helps to obtain the best-matched demonstration for in-context learning supervision. The memory snippets accumulate along with ${ \tt R } ^ { 2 }$ -MGA consistently answering questions. Theoretically, the more snippets there are, the better the best-matched in-context demonstration is since more snippets provide more retrieval cases.

We conduct detailed analyses to explore this assumption. In particular, we first split the 1,000 dataset entries into five buckets, i.e., 1-200, 201-400, 401-600, 601-800, and 801- 1000, where ${ \tt R } ^ { 2 }$ -MGA processes them from 1 to 1000 and consistently accumulates memory snippets;4 then, we calculate the evaluation metrics of each bucket. In particular, we consider all LLMs for each dataset and reported the average results, as presented in Figure 4. We observe that the answer quality consistently improves along with the memory snippets increase, validating the goal of the memory design and our assumption. In addition, we conduct investigations on memory snippets and report the results in Appendix F.

# Investigation on the Best-matched Demonstration

In this section, we investigate the effectiveness of the bestmatched in-context demonstration under the $<$ VANILLA,

Correctness Citation Recall Citation Precision 45 85 85   
40 80 80 35 75 75 [1] [2] [3] [4] [5] [1] [2] [3] [4] [5] [1] [2] [3] [4] [5] (a) ASQA   
25 35 35 20 25 25 [1] [2] [3] [4] [5] [1] [2] [3] [4] [5] [1] [2] [3] [4] [5] (b) QAMPARI   
20 60 65 15 55 55 [1] [2] [3] [4] [5] [1] [2] [3] [4] [5] [1] [2] [3] [4] [5] (c) ELI5

Vicuna-13B, $\mathrm { \ A S Q A > }$ setting. In particular, we conduct another four groups of experiments, which replace the bestmatched demonstration with the second-, the third-, the fourth-, and the fifth-matched demonstration, respectively. For scenarios in which the memory doesn’t have enough memory snippets, we use the last-matched memory snippet as an alternative. For example, if the memory solely has three snippets, we use the third-matched snippet to construct the fourth- and the fifth-matched in-context demonstration.

![](images/94f35ccc379b3c8c1515df02caea0bd4dd4123bed33f2a139c0518bccf7bba76.jpg)  
Figure 3: Comparisons between the one-time and step-bystep action executions.   
Figure 4: Changes of answer quality along with memory snippets increasing. We use ‘[1]’, ‘[2]’, ‘[3]’, ‘[4]’, and ‘[5]’ to represent the five buckets for short. For QAMPARI, we average the Correctness Recall-5 and Precision as the overall Correctness score.   
Figure 5: Comparisons of different best-matched in-context demonstrations.

Figure 5 shows the investigation results. We observe that: (1) the best-matched in-context demonstration consistently outperforms the other matched demonstrations across all three datasets; (2) the performance scores generally drop as the demonstration’s matched-degree decreases, i.e., from the best-matched to the fifth-matched. These results verify our motivation for the best-matched in-context demonstration.

# Investigation on Computation Costs

Compared to one-step generation baselines, ${ \tt R } ^ { 2 }$ -MGA needs additional LLMs’ operations in answer assessment, action planning, action executing, and rationale generating, which call for computation resources. To reduce the costs, ${ \tt R } ^ { 2 }$ - MGA adopts a 1-shot in-context demonstration when generating the initial answer $\mathcal { A }$ and other generations, which dramatically reduces LLMs’ input tokens compared to the 2-shot in-context demonstration used in one-step generation baselines like ALCE-base. Additionally, we store the generated rationales for memory snippets to avoid repeated generations, which also reduces the costs.

<html><body><table><tr><td></td><td colspan="3">ASQA</td><td colspan="4">QAMPARI</td><td colspan="3">ELI5</td></tr><tr><td></td><td rowspan="2">Correct.</td><td colspan="2">Citation</td><td colspan="2">Correct.</td><td colspan="2">Citation</td><td rowspan="2">Correct.</td><td colspan="2">Citation</td></tr><tr><td></td><td>Rec.</td><td>Prec.</td><td>Rec.-5</td><td>Prec.</td><td>Rec.</td><td>Prec.</td><td>Rec.</td><td>Prec.</td></tr><tr><td>R2-MGA</td><td>37.1</td><td>73.5</td><td>67.9</td><td>20.1</td><td>19.8</td><td>23.9</td><td>23.4</td><td>14.3</td><td>38.9</td><td>40.4</td></tr><tr><td>-best-matched demo</td><td>36.4 (-0.7)</td><td>68.7 (-4.8)</td><td>63.1 (-4.8)</td><td>18.9 (-1.2)</td><td>19.1 (-0.7)</td><td>17.6 (-6.3)</td><td>18.2 (-5.2)</td><td>13.5 (-0.8)</td><td>28.2 (-10.7)</td><td>29.1 (-11.3)</td></tr><tr><td>-answer refinement</td><td>34.2 (-2.9)</td><td>58.4 (-5.1)</td><td>58.6 (-9.3)</td><td>16.3 (-3.8)</td><td>16.6 (-3.2)</td><td>15.4 (-8.5)</td><td>14.9 (-8.5)</td><td>11.7 (-2.6)</td><td>22.3 (-16.6)</td><td>25.1 (-15.3)</td></tr><tr><td>-rationale</td><td>36.7 (-0.4)</td><td>70.6 (-2.9)</td><td>65.2 (-2.7)</td><td>18.5 (-1.6)</td><td>18.2 (-1.6)</td><td>21.0 (-2.9)</td><td>22.6 (-0.8)</td><td>13.6 (-0.7)</td><td>35.9 (-3.0)</td><td>37.7 (-2.7)</td></tr></table></body></html>

Table 2: Results of ablation studies. We obtain the results using the $<$ VANILLA, Vicuna- $\mathbf { 1 3 B > }$ setting.

Table 3: Comparisons of computation costs between ${ \bf R } ^ { 2 }$ - MGA and ALCE-base. For each ${ \bf < L L M }$ , Dataset $>$ setting, we use the counts of LLM’s input and output tokens across all dataset entries as the computation cost. For simplicity, we always take ALCE-base’s computation cost as the unit and the scores calculated by comparing the costs of ${ \tt R } ^ { 2 }$ -MGA to ALCE-base as ${ \tt R } ^ { 2 }$ -MGA’s costs.   

<html><body><table><tr><td>Approach</td><td>LLM</td><td>ASQA</td><td>QAMPARI</td><td>ELI5</td></tr><tr><td>ALCE-base</td><td rowspan="3">LLaMA-70B</td><td>1</td><td>1</td><td>1</td></tr><tr><td>W/RERANK</td><td>4</td><td>4</td><td>4</td></tr><tr><td>R²-MGA</td><td>2.42</td><td>2.87</td><td>2.56</td></tr><tr><td>ALCE-base</td><td rowspan="3">Vicuna-13B</td><td>1</td><td>1</td><td>1</td></tr><tr><td>W/RERANK</td><td>4</td><td>4</td><td>4</td></tr><tr><td>R²-MGA</td><td>2.66</td><td>3.04</td><td>2.89</td></tr><tr><td rowspan="3">ALCE-base W/RERANK R²-MGA</td><td rowspan="3">LLaMA-7B</td><td>1</td><td>1</td><td>1</td></tr><tr><td>4</td><td>4</td><td>4</td></tr><tr><td>2.72</td><td>2.90</td><td>2.65</td></tr></table></body></html>

We compare the computation costs between ${ \tt R } ^ { 2 }$ -MGA and ALCE-base with the three open-source LLMs and under the VANILLA setting. Additionally, we also consider the RERANK prompting strategy of ALCE-base, which instructs LLMs to repeatedly generate four answers for each question and select the best answer using the citation recall metric. Table 3 reports the comparison results. We can observe that (1) compared to ALCE-base, ${ \tt R } ^ { 2 }$ -MGA consumes additional $1 . 4 2 \times$ to $2 . 0 4 \times$ computation resources, but it delivers significant performance gains (See Table 1); (2) compared to ALCE-base w/RERAN K, ${ \mathrm { R } } ^ { 2 }$ -MGA consumes much less computation resources. It’s worth noting that ${ \tt R } ^ { 2 }$ -MGA consistently outperforms ALCE-base w/RERANK. Appendix G reports detailed comparison results.

# Ablation Study

We conduct ablation studies on ${ \tt R } ^ { 2 }$ -MGA across all three datasets under the $<$ VANILLA, Vicuna- $\phantom { + } 1 3 \mathbf { B } >$ setting. Specifically, we investigate the effectiveness of the best-matched in-context demonstration, the answer refinement strategy, and the rationale by ablating them individually. Table 2 reports the ablation results, where “-best-matched demo” denotes always using the same demonstration for the in-context learning supervision; “-answer refinement” denotes removing the Assessment and the Planning & Action modules; “-rationale” denotes not providing reasoning rationales in the bestmatched in-context demonstration. We have the following observations:

(1) “-best-matched demo” decreases the Correctness scores ranging from - $. 0 . 7 \%$ to - $. 1 . 2 \%$ , and the Citation Recall scores ranging from $- 4 . 8 \%$ to $- 1 0 . 7 \%$ , and the Citation Precision scores ranging from $- 4 . 8 \%$ to $- 1 1 . 3 \%$ . These performance drops indicate that the best-matched demonstration is more informative and provides better in-context learning supervision.

(2) “-answer refinement” dramatically decreases the Correctness scores ranging from $- 2 . 6 \%$ to $- 3 . 8 \%$ , and the Citation Recall scores ranging from $- 5 . 1 \%$ to $- 1 6 . 6 \%$ , and the Citation Precision scores ranging from $- 8 . 5 \%$ to $- 1 5 . 3 \%$ . These decreased scores prove the claim that LLMs do not always generate the best output on their first try (Madaan et al. 2024). They also strongly support our motivation for further answer refinement.

(3) “-rationale” showcases obvious negative impacts on the three metrics. Specifically, it brings performance drops on the Correctness scores ranging from - $- 0 . 4 \%$ to - $1 . 6 \%$ , the Citation Recall scores ranging from $- 2 . 9 \%$ to $- 3 . 0 \%$ , and the Citation Precision scores ranging from - $0 . 8 \%$ to $- 2 . 7 \%$ . These results firmly support our motivation to use reasoning rationales in in-context demonstrations, enhancing the in-context learning supervision.

# Conclusion

In this paper, we present a novel framework named Retrieval and Reflection Memory-augmented Generative Agent $( \mathsf { R } ^ { 2 }$ - MGA), which leverages the memory retrieval and reflection techniques to provide the best-matched in-context demonstration, as well as an answer refinement strategy to first assess answers and further refine them accordingly. We design a rational memory architecture and a succinct but reliable memory updating mechanism to maintain this core module. Experimental results on the ALCE benchmark indicate ${ \tt R } ^ { 2 }$ - MGA dramatically boosting LLMs’ capabilities in text generation with citations, outperforming those one-step generation baselines by large margins. Extensive analyses firmly verify the various motivations of $\mathbb { R } ^ { 2 }$ -MGA.