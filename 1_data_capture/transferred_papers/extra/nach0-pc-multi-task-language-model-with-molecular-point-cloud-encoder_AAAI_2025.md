# nach0-pc: Multi-task Language Model with Molecular Point Cloud Encoder

Maksim Kuznetsov 1\*, Airat Valiev 2, Alex Aliper 2, Daniil Polykovskiy 1, Elena Tutubalina 2, Rim Shayakhmetov 1, Zulfat Miftahutdinov 1

1Insilico Medicine Canada Inc.,   
2Insilico Medicine AI Ltd.

# Abstract

Recent advancements have integrated Language Models (LMs) into a drug discovery pipeline. However, existing models mostly work with SMILES and SELFIES chemical string representations, which lack spatial features vital for drug discovery. Additionally, attempts to translate chemical 3D structures into text format encounter issues such as excessive length and insufficient atom connectivity information. To address these issues, we introduce nach0-pc, a model combining domain-specific encoder and textual representation to handle spatial arrangement of atoms effectively. Our approach utilizes a molecular point cloud encoder for concise and orderinvariant structure representation. We introduce a novel pretraining scheme for molecular point clouds to distillate the knowledge from spatial molecular structures datasets. After fine-tuning within both single-task and multi-task frameworks, nach0-pc demonstrates performance comparable with other diffusion models in terms of generated samples quality across several established spatial molecular generation tasks. Notably, our model is a multi-task approach, in contrast to diffusion models being limited to single tasks. Additionally, it is capable of processing point cloud-related data, which language models are not capable of handling due to memory limitations. These lead to our model having reduced training and inference time while maintaining on par performance.

# Introduction

Language Models (LMs) have shown exceptional natural language understanding (Devlin et al. 2019), and performance across diverse natural language tasks (Raffel et al. 2020; Lewis et al. 2020). LMs also show efficiency as conversational agents, engaging in meaningful dialogues (Brown et al. 2020).

Recent studies (Pei et al. 2023; Livne et al. 2024) have demonstrated the ability of LMs in processing specialized chemical languages, such as SMILES (Weininger 1988) and SELFIES (Krenn et al. 2020). These models exhibit proficiency in understanding and manipulating textual representations of chemical data, enabling their application across various tasks. For instance, LMs have been utilized for molecular property prediction (Ross et al. 2022), molecular generation (Edwards et al. 2022), and chemical reaction prediction (Lu and Zhang 2022).

While SMILES and SELFIES capture chemical graph, they lack spatial features crucial for drug discovery methodologies involving precise atom arrangements and interactions. Recent studies (Flam-Shepherd and Aspuru-Guzik 2023) show LMs can generate meaningful 3D chemical structures in text formats like PDB, CIF, and XYZ, representing atom coordinates and features. However, this approach suffers from excessive length, requiring dozens of tokens per atom, becoming impractical for larger structures like proteins. Additionally, these formats lack information on atom connectivity, necessitating the use of external software tools like Open Babel(O‚ÄôBoyle et al. 2011) to determine chemical bonds. Such external tools are sensitive to noise in atom positions, which can significantly alter reconstructed chemical graphs or cause molecular fragmentation.

In our study, we introduce nach0-pc, a Language Model designed for generative tasks involving 3D molecular structures. This model combines domain-specific encoder and textual representation of spatial atom arrangements, enabling effective handling of 3D molecular structures both as inputs and outputs. We leverage a molecular point cloud encoder to derive a concise, meaningful, and order-invariant representation of molecular and protein 3D structures. Furthermore, we propose a textual format for generating spatial molecular structures by initially generating SMILES representations, followed by specifying atom coordinates in accordance with the SMILES sequence order. This format allows LM determine molecular graph and eliminates the reliance on external software for reconstructing bonds.

The key contributions of our work are as follows:

1. A novel nach0-pc model that enhances standard encoderdecoder language models by integrating a specialized molecular point cloud encoder and tokens, incorporating unique features like point embedding calculation and position bias adjustments tailored for point cloud data.   
2. A new pre-training approach that uses dropout on entire subfragments to train the model to predict missing parts of incomplete molecular point clouds, generated through fragment omitting and/or blurring strategies.   
3. Extensive experiments demonstrating superior or comparable performance to LM baselines and state-of-the-art

Dilsetarirbniuntigon Input Output text Point cloud cgoeSndheiatripaotein-oend Cgoenfnoerrmataiotinon cploiuntd encoder c1(ccc(c(Brc)1OC)Br)N coPnodictikoent-ed Standard text2text C -01..57230..903 --1..0186||C --01..74290-.08.474-1-.07.0 2| | Convert dLienskigenr Input text Br Scaffold Generate molecular Text encoder Text decoder C 0.76 0.15 0.08 | O 2.01 0.21 0.65| decoration 3D structure for pocket C 2.06 1.26 1.62 | Br -2.92 -1.82 0.60 | N 1.56 1.61 -1.72 (a) Summary of tasks (b) Overview of nach0-pc

diffusion approaches across six spatial molecular generation tasks.

# Related Work

Language Models in Chemistry The sequential nature of molecules enables the use of Transformer models and masked language modeling pre-training methods (Lu and Zhang 2022). Recent advancements have introduced domain-specific LMs (Edwards et al. 2022; Pei et al. 2023), based on T5, designed specifically to incorporating both chemical and linguistic knowledge. MolT5 (Edwards et al. 2022) uses initial pre-training on a collection of molecule SMILES and texts, followed by single-task finetuning on downstream tasks. Another recent model, multidomain nach0 (Livne et al. 2024), undergoes fine-tuning on a diverse set of 28 task-dataset pairs, employing instruction tuning in a multi-task fashion.

Spatial molecular structure generative models The majority of recently published spatial molecular structure generative models is based on the denoising diffusion probabilistic model (DDPM) paradigm (Ho, Jain, and Abbeel 2020). EDM (Hoogeboom et al. 2022) follows this methodology to solve spatial molecular distribution learning. A notable drawback of the diffusion approach for generating 3D molecules is its reliance on external software like OpenBabel to reconstruct molecular bonds from atom coordinates. This limitation has been addressed in several further works. The MolDiff model (Peng et al. 2023) integrates an additional bond predictor to guide diffusion and ensure bond consistency alongside 3D atom coordinates. GeoDiff (Xu et al. 2022), Torsional Diffusion (Jing et al. 2022) models can take molecular graph to perform conformation generation task. GeoMol(Ganea et al. 2021) is also a significant advancement in the field of cheminformatics and drug discovery. It is an end-to-end, non-autoregressive, and SE(3)-invariant machine learning approach to generate distributions of low-energy molecular 3D conformers. DiffLinker (Igashov et al. 2024) and LinkerNet (Guan et al. 2023a) models are a 3D equivariant diffusion model learned to generates the linker fragment between given disconnected molecular subfragments in linker design task. These models can find a stable linker and connect the fragments, resulting in a low-energy conformation for the entire molecule. DiffDec (Xie et al. 2024) model was proposed for scaffold decoration task and generates R-groups given a core of the molecule called scaffold. The ShapeMol (Chen et al. 2023) model utilizes a diffusion approach for shape-conditioned generation, where a reference ligand molecule is represented as a blurred spatial area approximating the molecular surface volume, and the model suggests new molecules with shapes closely resembling the reference. The pocketconditioned generation task involves creating molecules that seamlessly fit within a designated pocket space, establishing favorable interactions to enhance binding affinity. D3FG(Lin et al. 2023), TargetDiff(Guan et al. 2023b), and DecompDiff (Guan et al. 2023c) are capable of designing novel 3D molecules from scratch that effectively bind to specified protein pockets.

# Architecture

As shown in Fig. 1b, the architecture of the proposed nach0- pc model extends the standard encoder-decoder LM with a domain-specific molecular point cloud encoder and point cloud tokens in a plug-and-play manner. We use the T5 architecture (Raffel et al. 2020) as a base.

The method‚Äôs plug-and-play nature allows one to choose whether to train a model from scratch or fine-tune a pretrained LM model alongside a point cloud encoder for text and point cloud tasks. We initialize nach0-pc‚Äôs LM component with nach0 (Livne et al. 2024), the state-of-the-art natural language and chemical LM. Ablation of LM components is presented in Supplementary material (Kuznetsov et al. 2024).

# Input/Output Data Format

The model accepts textual input and optionally molecular point cloud input, while the output is textual. An input molecular point cloud consists of an unordered collection of points, denoted as $\{ p _ { i } \}$ , where each point $p _ { i } = ( c _ { i } , f _ { i } )$ is described by Cartesian coordinates $c _ { i } = ( x _ { i } , y _ { i } , z _ { i } )$ and an unordered set of tokens $f _ { i } = \{ f _ { i } ^ { j } \}$ representing its features. For example, the features of a point corresponding to a ligand‚Äôs atom might include its atom symbol and atom charge, such as $f _ { i } = \bar { \lbrace } \mathrm {  ~ \ ` ~ 1 \rbrace ~ g a n d ^ { \prime } ~ , ~ \unboldmath ~ } \mathrm {  ~ \ " ~ N ^ { \prime } ~ , ~ \unboldmath ~ } \mathrm {  ~ \ " ~ } + \mathrm {  ~ \prime ~ } \rbrace$ . The feature set for a protein‚Äôs atom point can be extended to encompass its atom name and the amino acid name it belongs, such as $f _ { i } =$ $\{  { \mathrm {  ~ \cdot } } _ { \mathrm { p o c k e t } ^ { \prime } } , \quad  { \mathrm {  ~ \cdot ~ } } _ { \mathrm { C } ^ { \prime } } , \quad  { \mathrm {  ~ \cdot ~ } } _ { \mathrm { G L Y } ^ { \prime } } , \quad  { \mathrm {  ~ \cdot ~ } } _ { \mathrm { C } \bar { \mathrm { A } } ^ { \prime } } \}$ . All point features are represented as word tokens and can be utilized in textual input and output. For instance, atom symbols and charges are present in point features and SMILES tokens.

![](images/04370711d29898f226e96c977c5888f7f802b9092cfbbb50b2ab1d720708a665.jpg)  
Figure 2: An overview of the encoder architecture adapted for point cloud data and standard text input. For point clouds, tokens represent features at specific spatial positions. Tokens are embedded via a token embedding layer, followed by summation pooling to optimize memory and processing efficiency. Scalar Sinusoidal Embeddings (SSE) integrate continuous spatial coordinates and relative pairwise distances.

We represent a 3D molecule as text by combining SMILES and XYZ formats. Starting with SMILES to describe the molecular graph, we concatenate lines of XYZ format, describing each atom‚Äôs symbol and positions in the same order they appear in SMILES. To limit the token count, we use two digits after the decimal point and tokenize each coordinate by splitting at the decimal point $( \cdot - 1 . 2 3 ^ { \prime }$ $ [ { \bf \Phi } ^ { \bullet } - 1 ^ { \prime } ~ , { \bf \Phi } ^ { \bullet } . 2 3 ^ { \prime } { \bf \Phi } ] ;$ ), thus each coordinate can be described by two tokens. It takes only 12 tokens to describe one atom.

In our work we consider hydrogen-depleted molecular graphs for input point cloud and output SMILE $\mathrm { \ s { + } X Y Z }$ representation. To prepare the input point clouds, we center coordinates by deducting the average position from each point. To augment inputs and outputs, we apply the random rotations to the point clouds and make use of non-canonical SMILES representations for textual inputs/outputs.

For large ligand and pocket point clouds, we perform prioritized point downsampling - we keep ligand, C-alpha, and terminal atoms of protein amino acids and downsample other points. This procedure reduces the number of points to the fixed number and so the model fits memory constraints.

# Point Cloud Encoder

The point cloud encoder utilizes the standard Transformers encoder architecture with several changes (see Fig. 2) inspired by the nature of point cloud data. The first key difference is the point embedding calculation. While word to

![](images/5fd49e5b0656f40bdeef7cc6c3a88f7174a0dba54c7768886d82a4393e64228a.jpg)  
Figure 3: The pre-training scheme for 3D molecular structures datasets. The model learns to reconstruct blurred or masked molecular fragments. The model generates the missing parts during pre-training, including their SMILES representations, attachment points, and atom coordinates.

Input: Point features $f _ { i } = \{ f _ { i } ^ { j } \}$ and pos $c _ { i } = ( x _ { i } , y _ { i } , z _ { i } )$   
Output: Point embeddings $n _ { i } ^ { o u t }$   
1: $\begin{array} { r } { n _ { i } ^ { 0 } = \sum _ { f _ { i } ^ { j } \in f _ { i } } E m b ( f _ { i } ^ { j } ) } \end{array}$ $D$ embed and aggregate feats   
2: for $l = 1 , 2 , \dots , L$ do   
3: $b _ { h i j } ^ { l } = M L P _ { h } ^ { l } ( S S E ( | | c _ { i } - c _ { j } | | ) )$   
4: $D$ for each head $h$ compute relative attention biases   
5: $n ^ { l } = S e l f A t t e n t i o n ^ { l } ( \bar { n } ^ { l - 1 } , b ^ { l } )$ $D$ update embs   
6: end for   
7: $n _ { i } ^ { o u t } = n _ { i } ^ { L } + L i n _ { x y z } ( S S E ( x _ { i } ) , S S E ( y _ { i } ) , S S E ( z _ { i } ) )$   
8: $D$ embed coordinates

kens are transformed using a token embedding layer, the same layer is applied to point tokens, followed by a summation pooling operation to aggregate point token embeddings. Representing spatial molecular data with point clouds uses far fewer embeddings than conventional text representation. For example, a molecular point cloud employs only a few dozens of embeddings whereas text-based representations require several hundreds of embeddings. It significantly decreases memory usage while increasing the speed of attention layers sensitive to input size.

The second notable distinction can be found in point spatial coordinates. Contrary to textual representation, which is a sequence of discrete token indices, every point‚Äôs position in a point cloud is depicted using continuous Cartesian coordinates. First, we modify the relative position biases computation to embed pairwise distances instead of relative sequence positions. Also, we embed the coordinates of the point and sum them with point embeddings. We do it on the last step right before passing embeddings to the decoder, ensuring that the self-attention layers remain invariant to any point cloud translation or rotation.

To embed scalar continuous values of distances and coordinates, we are introducing Scalar Sinusoidal Embeddings (SSE). It takes inspiration from positional sinusoidal embeddings (Vaswani et al. 2017) and extends it to map continuous scalar values into a high-dimensional vector. We divide the input scalar $s$ by wavelengths $w _ { i }$ , uniformly initialized on a logarithmic grid, and use the yielded sine and cosine function values to form the resulting embedding vector. The step-by-step description can be found in Alg. 1.

Table 1: Spatial molecular distribution learning performance metrics on GEOM-DRUGS.   

<html><body><table><tr><td rowspan="2" colspan="2">Group</td><td rowspan="2">MolDiffSOp-nf-La-aAt</td><td colspan="3"></td><td colspan="2">Multi-TaskhSingle-Task</td></tr><tr><td></td><td></td><td>nachO</td><td></td><td></td></tr><tr><td rowspan="3">Basic</td><td>Validity and Connectivity (‚Üë) Uniqueness (‚Üë)</td><td>99.3% 92.8%</td><td>95.0% 99.8%</td><td>98.6% 99.8%</td><td>97.8% 99.2%</td><td></td><td>99.0% 99.8%</td></tr><tr><td>Novelty (‚Üë)</td><td>97.7%</td><td>85.0%</td><td>98.0%</td><td></td><td>96.2%</td><td>97.1%</td></tr><tr><td>Diversity (‚Üë)</td><td>0.763</td><td>0.745</td><td>0.739</td><td>0.781</td><td></td><td>0.734</td></tr><tr><td rowspan="3">Druglikeness</td><td>QED (‚Üë)</td><td>0.679</td><td>0.673</td><td>0.667</td><td>0.771</td><td></td><td>0.664</td></tr><tr><td>SA (1)</td><td>0.875</td><td>0.808</td><td>0.851</td><td></td><td>0.872</td><td>0.848</td></tr><tr><td>Lipinski (‚Üë)</td><td>4.981</td><td>4.972</td><td>4.938</td><td></td><td>4.992</td><td>4.938</td></tr><tr><td rowspan="3">3D substructures</td><td>JS. bond lengths (‚Üì)</td><td>0.436</td><td>0.257</td><td>0.148</td><td>0.193</td><td></td><td>0.142</td></tr><tr><td>JS.bond angles (‚Üì)</td><td>0.181</td><td>0.136</td><td>0.096</td><td></td><td>0.100</td><td>0.94</td></tr><tr><td>JS. dihedral angles (‚Üì)</td><td>0.198</td><td>0.110</td><td>0.110</td><td></td><td>0.132</td><td>0.113</td></tr><tr><td rowspan="4">Bonds</td><td>JS.# bonds per atoms (‚Üì)</td><td>0.121</td><td>0.064</td><td>0.111</td><td></td><td>0.233</td><td>0.094</td></tr><tr><td>JS.freq. bond types (‚Üì)</td><td>0.170</td><td>0.031</td><td>0.046</td><td></td><td>0.052</td><td>0.041</td></tr><tr><td>JS.freq.bond pairs (‚Üì)</td><td>0.153</td><td>0.028</td><td>0.037</td><td></td><td>0.040</td><td>0.033</td></tr><tr><td>JS. freq. bond triplets (‚Üì)</td><td>0.137</td><td>0.034</td><td>0.046</td><td>0.054</td><td></td><td>0.041</td></tr><tr><td rowspan="3">Rings</td><td>JS. # rings (‚Üì)</td><td>0.079</td><td>0.033</td><td>0.56</td><td>0.270</td><td></td><td>0.036</td></tr><tr><td>JS.# n-sized rings (‚Üì)</td><td>0.102</td><td>0.024</td><td>0.026</td><td></td><td>0.058</td><td>0.024</td></tr><tr><td># Intersecting rings (‚Üë)</td><td>8</td><td>8</td><td>8</td><td></td><td>9</td><td>8</td></tr></table></body></html>

$$
S S E _ { 2 i } ( s ) = s i n ( s / w _ { i } ) , S S E _ { 2 i + 1 } ( s ) = c o s ( s / w _ { i } )
$$

# Molecular Point Cloud Pre-training

Drawing inspiration from the token dropout pre-training objective of T5 (Raffel et al. 2020), we introduce a novel pretraining scheme for molecular point clouds (Fig. 3). In our approach, we feed the model with incomplete molecular point clouds and train the model to reconstruct the missing pieces. We employ the BRICS (Degen et al. 2008) algorithm to split the molecule into several fragments and randomly select a subset of these fragments with some predefined probability (ensuring that at least one is chosen) and exclude them from the molecule. The pre-training scheme is designed to be flexible, handling both datasets consisting only of spatial molecular structures and datasets with protein pockets and ligand pairs. In the latter, the ligand is masked, while the protein pocket remains unchanged in the input point cloud. Further, we form the input point cloud by (i) removing or (ii) obfuscating chosen fragments by omitting the features of a point (such as atomic symbol, charge, and valency), replicating the point multiple times, and introducing Gaussian noise to the coordinates of these copies.

During pre-training, the model‚Äôs task is to accurately recover the absent (or blurred) components, specifying their SMILES representations as well as atomic coordinates. Each recovered fragment should include a connecting point indicated by the symbol $\cdot _ { \star } \prime$ . In cases where the model should reconstruct multiple missing fragments, the fragments are separated with the ‚Äò.‚Äô token. We note that 3D pre-training enhances the model‚Äôs performance on downstream tasks (see Impact of 3D pre-training in the Supplementary material (Kuznetsov et al. 2024)).

# Experiments

We evaluate the quality of the nach0-pc model across several established spatial molecular generation tasks: (i) 3D molecular structures generation: spatial molecular distribution learning, conformation generation, (ii) molecular completion: linker design, scaffold decoration, (iii) shapeconditioned generation, (iv) pocket-conditioned generation. We highlight the best and the second best metrics values for better readability and provide sampled spatial molecular structures for visual inspection of results.

We compare our nach0-pc model with well-established baseline models proposed for each mentioned task. We also compare it with text-only language model - we train OpenLLaMA model (Geng and Liu 2023) with the same (350M) number of parameters as nach0-pc, along with nach0 (Livne et al. 2024) on spatial molecular distribution learning, conformation generation and linker design tasks, where input/output text and molecular condition in $\mathrm { S M I L E S + X Y Z }$ format can be described with fewer than a thousand tokens. The rest of tasks either require large protein pocket condition or involve blurring that substantially inflate the size of the point cloud representation, making these tasks practically unfeasible in text-only paradigm.

We train our proposed nach0-pc model in both multi-task and single-task regimes to fairly evaluate it against both LM and non-LM models. Our work adopts small molecules ZINC (Irwin et al. 2020), MOSES (Polykovskiy et al. 2020), and GEOM-Drugs (Axelrod and Go¬¥mez-Bombarelli 2022) datasets, as well as the CrossDocked2020 (Francoeur et al. 2020) dataset, which includes pocket-ligand pairs. In cases when tasks use the same dataset, to avoid any potential data leakage, we use the same dataset split. We combine all stated datasets to both pretrain and finetune the nach0- pc model in multi-task regime. To address the discrepancy in dataset sizes, we normalize the training process by employing a batch-balancing technique that involves retrieving random object from uniformly sampled task. See Supple

MolDiff LLaMa nach0 nach0-pc nach0-pc MT ST Tor.Diff. LLaMa nach0 nach0-pc nach0-pc Ground MT ST truth gMraolp.h DiffLinker nach0 nach0-pc nach0-pc Input MT ST Frags Orimgionl.al 2 2 9 3 3 A 1 Â•Ω Âìà (a) Distribution learning (b) Conformation generation (c) Linker design

mentary material (Kuznetsov et al. 2024) for additional details on datasets, models, and metrics.

# Model Parameters and Training Details

In our research, we primarily utilize a model based on the nach0 (Livne et al. 2024) architecture. Our experiments involve a base model variant, characterized by 12 layers, a hidden state of 768 dimensions, a feed-forward hidden state of 2048 dimensions, and 12 attention heads. We utilize the same parameters to build our point cloud encoder. The total model size has 370M parameters. The model was trained using two NVIDIA A6000 GPUs. The pre-training and finetuning stages were executed using the following hyperparameters: a batch size of 64 for both pre-training and finetuning, a learning rate set to 1e-4, a weight decay of 0.01, and a cosine schedule. Both the pre-training and fine-tuning stages lasted for 100000 steps.

# Spatial Molecular Distribution Learning and Conformation Generation Tasks

This section evaluates the model‚Äôs ability to generate structurally and physically plausible spatial molecular objects. The spatial molecular distribution learning task assesses whether the model can produce novel 3D molecular structures whose distribution is close to the ground truth. Following (Peng et al. 2023), we adopt high-quality GEOM-Drugs (Axelrod and Go¬¥mez-Bombarelli 2022) dataset, which offers gold-standard conformation ensembles generated using metadynamics in CREST (Pracht, Bohle, and Grimme 2020). We evaluate generated molecules from several perspectives, including basic (validity, novelty, etc) druglikeness, and 3D substructures, bonds and rings distribution divergences.

In the conformation generation, we focus on generating plausible conformations given molecular graph. The conformations of a molecule are its energetically favorable 3D structures, each representing a local minimum on the potential energy surface. Similar to the previous task, we employ the GEOM (Axelrod and Go¬¥mez-Bombarelli 2022) dataset for this task. We follow (Jing et al. 2022) and employ the

Table 2: Generated conformer ensembles quality on GEOMDRUGS (baselines‚Äô metrics from (Jing et al. 2022)).   

<html><body><table><tr><td rowspan="3">Method</td><td colspan="4">Recall</td><td colspan="4">Precision</td></tr><tr><td>COV (‚Üë)</td><td></td><td colspan="2">AMR (‚Üì)</td><td colspan="2">COV (‚Üë)</td><td colspan="2">AMR(‚Üì)</td></tr><tr><td>Avg</td><td>Med</td><td>Avg</td><td>Med</td><td>Avg</td><td>Med</td><td>Avg</td><td>Med</td></tr><tr><td></td><td></td><td>28.6</td><td>Single-Task baselines 1.06</td><td>1.00</td><td>40.9</td><td>30.8</td><td>0.99</td><td>0.89</td></tr><tr><td>ETKDG GeoMol GeoDiff Tor. Diff.</td><td>38.4 44.6 42.1 72.7</td><td>41.4 37.8 80.0</td><td>0.87 0.83 0.58</td><td>0.83 0.81 0.56</td><td>43.0 24.9 55.2</td><td>36.4 14.5 56.9</td><td>0.93 1.14 0.78</td><td>0.84 1.09 0.73</td></tr><tr><td colspan="9">Multi-Task Text-OnlyLMbaselines</td></tr><tr><td>OpenLLaMA nachO</td><td>10.2 54.0</td><td>0.8 54.55</td><td>1.48 0.74</td><td>1.43 0.73</td><td>19.7 30.7</td><td>1.7 20.7</td><td>1.31 1.06</td><td>1.28 1.06</td></tr><tr><td colspan="9"></td></tr><tr><td>Multi-Task Single-Task</td><td>50.1 57.7</td><td>50.0 59.5</td><td>nach0-pc 0.76 0.70</td><td>0.75 0.69</td><td>27.7 32.4</td><td>16.7 23.1</td><td>1.11 1.03</td><td>1.11 1.03</td></tr></table></body></html>

Average Minimum RMSD (AMR) and Coverage metrics.   
These metrics are evaluated from Recall and Precision view.

We utilize the same train/validation/test splits as in the conformation generation task from the Torsional Diffusion(Jing et al. 2022) paper and retrain baseline if they were trained on another split.

The results for two tasks are presented in Tables 1 and 2. Examples of generated structures are given on Figs. 4a, 4b. The proposed nach0-pc model shows the best or second best results for the majority of metrics on both tasks. As shown in Tab. 1, text-only OpenLLaMA shows slightly better results on bonds and rings distribution metrics. Our hypothesis, it is due to the fact the input of this task is the same all time, so decoder-only model better utilize parameters rather than encoder-decoder. On the conformation generation task, the only model outperforming nach0-pc on the whole set of metrics is Torsional Diffusion, which utilizes samples from external software RDKit (Landrum et al. 2023) as an initial generation point on the inference stage. Nevertheless, nach0-pc shows stronger results than all other purely neural baselines that produce molecular conformation from scratch.

Mean Tanimoto Sim. SQUID nacMh0T-pc nacShT0-pc Gtrouthnd 0.4 0.5 0.6 0.7 0.8 0.900 1.0   
ùúÜ = 1.0 ùúÜ = 0.3 ùúé = 0.5 ùúé = 0.3 ùúé = 0.1 ùúé = 0.5 ùúé = 0.3 $\sigma = 0 . 1$ Model Name œÉ=0.0 ‚óè-nach0-pc Multi-Task 3 0.9 squiD-pc Single-Task œÉ=0. œÉ=0 Gsy Y œÉ=0.3 0.7 0=0.4 B X 3 Y 2 g=0.2 œÉ=0.   
838 0.6 0=0.3 Y Y Âèå Œª=0.3 =1. 0=0.4 0.5

Figure 5: (left) Generated molecular structures and (right) structural/shape similarity trade-off for various noise parameters on shape-conditioned generation task.

Table 3: Models performance evaluation for linker design task (metrics for baselines from (Igashov et al. 2024)).   

<html><body><table><tr><td>Method</td><td></td><td></td><td></td><td>Val.(‚Üë)Uniq.(‚Üë)Filt.(‚ÜëRMSD(‚Üì)|SC(‚Üë)</td><td></td></tr><tr><td colspan="6">State-of-the-art</td></tr><tr><td>DeLinker 3DLinker DiffLinker nachO</td><td>98.3% 71.5% 93.8% 55.2%</td><td>44.2% 29.2% 24.0% 19.9%</td><td>84.88% 83.72% 86.26% 98.78%</td><td>5.48 0.11 0.34 1.41</td><td>0.49 0.92 0.93 0.85</td></tr><tr><td colspan="6">nachO-pc</td></tr><tr><td>Multi-Task Single-Task</td><td>81.6% 89.7%</td><td>27.6% 12.3%</td><td>99.00% 99.55%</td><td>1.28 1.04</td><td>0.86 0.88</td></tr></table></body></html>

Table 4: Scaffold decoration task metrics (metrics of baselines from (Xie et al. 2024)).   

<html><body><table><tr><td>Model</td><td></td><td></td><td>Validity(‚Üë)Uniqueness (‚Üë)QVina Score(‚Üì)</td></tr><tr><td>Reference Scaffolds</td><td></td><td></td><td>-8.47 -7.73</td></tr><tr><td colspan="4">State-of-the-art</td></tr><tr><td>Pocket2Mol</td><td>51.14%</td><td>44.27%</td><td>-8.11</td></tr><tr><td>FLAG DiffDec</td><td>87.95% 98.00%</td><td>65.30% 48.54%</td><td>-7.62 -8.25</td></tr><tr><td></td><td></td><td>nachO-pc</td><td></td></tr><tr><td colspan="4"></td></tr><tr><td>Multi-Task Single-Task</td><td>97.63% 99.17%</td><td>42.58% 18.12%</td><td>-7.931 -8.123</td></tr></table></body></html>

# Shape-conditioned Generation

This section focuses on producing molecules spatially similar to the reference structure but structurally dissimilar. This is achieved by representing the reference molecule as a shape - an area where molecular atom nucleus and electron clouds are located. We represent molecular shape as a point cloud - we replicate each atom several times and add Gaussian noise with predefined standard deviation $\sigma$ to atom positions while removing all point features completely. $\sigma$ allows to balance between spatial and chemical similarity.

Following the methodology outlined in the SQUID paper (Adams and Coley 2023), we conduct training and teststage sampling using the RDKit (Landrum et al. 2023) conformations computed for the MOSES dataset (Polykovskiy et al. 2020). We provide a comparison of nach0-pc with the SQUID (Adams and Coley 2023) model in Fig. 5. We alternate noise injection parameters, standard deviation $\sigma$ for nach0-pc and prior interpolation coefficient $\lambda$ for SQUID, to show available trade-offs between structural and shape similarity. nach0-pc provides a wider range of available tradeoffs, covering a high structural to high shape similarity area. Moreover, nach0-pc produces more spatially similar objects for low structural similarity values than SQUID.

# Linker Generation and Scaffold Decoration Tasks

These tasks assess the ability of models to complete disjoint or partially-defined molecular structures. In linker design task, models operate with several disconnected fragments and should produce small molecular structures that spatially and chemically connect the given fragments and complete into one chemical structure. The same as in the DiffLinker work (Igashov et al. 2024), we employ a subset of the ZINC dataset, comprising 250000 random molecules with conformations generated using RDKit (Landrum et al. 2023). This dataset also provides an input fragments/linker splits.

In the second task, scaffold decoration, the model takes the core part of the molecule called scaffold and complete side-chain specific motifs called R-groups. Usually, scaffold decoration is employed to enhance some molecular properties, for instance, binding affinity with a specific protein. Following DiffDec (Xie et al. 2024), we adopt Multi RGroup Decoration Task on CrossDocked (Francoeur et al. 2020) dataset, containing $1 0 0 K$ ligand-protein pairs, where each ligand is split into a scaffold and R-groups.

Our nach0-pc takes molecular fragments/scaffold and protein pocket, if available, as an input point cloud and produces the linker/R-groups without repeating input atoms. The produced molecular substructures contain the attachment points described by a symbol $\cdot _ { \star } \prime$ and coordinates.

Table 5: Pocket-conditioned generation performance metrics (metrics of baselines from (Guan et al. 2023b)).   

<html><body><table><tr><td rowspan="2">Model</td><td rowspan="2">Valid. (‚Üë)</td><td rowspan="2">Div. (‚Üë)</td><td colspan="2">Vina Dock (‚Üì)</td><td rowspan="2">High Affinity (‚Üë)</td></tr><tr><td>Avg</td><td>Med</td></tr><tr><td>Reference</td><td>100%</td><td></td><td>-7.45</td><td>-7.26</td><td></td></tr><tr><td colspan="6">State-of-the-art</td></tr><tr><td>AR Pocket2Mol</td><td>92.95% 98.31%</td><td>0.70 0.69</td><td>-6.75 -7.15</td><td>-6.62 -6.79</td><td>37.9% 48.4%</td></tr><tr><td>TargetDiff</td><td>90.36%</td><td>0.72</td><td>-7.80</td><td>-7.91</td><td>58.1%</td></tr><tr><td></td><td>91.78%</td><td>nachO-pc 0.32</td><td>-6.52</td><td>-6.86</td><td>38.2%</td></tr><tr><td>Multi-Task Single-Task</td><td>89.82%</td><td>0.40</td><td>-6.50</td><td>-6.62</td><td>41.1%</td></tr></table></body></html>

We utilize these attachment points to combine input fragments with generated ones into a coherent molecule. The model can produce several R-groups in the scaffold decoration task by separating them with a symbol ‚Äò.‚Äô. We compare nach0-pc with DeLinker (Imrie et al. 2020), 3DLinker (Huang et al. 2022), DiffLinker (Igashov et al. 2024) on the linker generation task, and benchmark against LibINVENT (Fialkova¬¥ et al. 2022), FLAG (Zhang et al. 2023), and DiffDec (Xie et al. 2024) for scaffold decoration. As shown in Tables 3 and 4 nach0-pc can complete input molecular point clouds with a high success rate, producing molecules (Fig. 4c) that pass 2D filters such as PAINS (Baell and Holloway 2010). Despite moderate structural diversity, nach0- pc produces spatially diverse molecules. Moreover, it enhances scaffold binding affinity, working on par with other state-of-the-art models in scaffold decoration.

# Pocket-conditioned Generation

Finally, nach0-pc was trained to generate novel high-affinity structures for a given protein pocket condition. Similar to TargetDiff, we used the CrossDocked2020 dataset (Francoeur et al. 2020) and trained our model on 100000 highaffinity ligand-protein complexes. During the test stage, we randomly sampled 100 molecules for each protein pocket in the test set. One can find generated ligands visualisation in Fig. 6. We assess the validity, diversity and docking scores of generated structures and provide the comparison with AR(Luo et al. 2021), Pocket2Mol(Peng et al. 2022) and TargetDiff(Guan et al. 2023b) baseline models in Tab 5.

Our model shows high validity and diversity. While there is a significant gap between nach0-pc and the TargetDiff performance, it demonstrates comparable results to AR and Pocket2Mol based on docking scores and binding affinity.

# Model Training Time and CO2 Impact

All experiments were conducted utilizing the CoreWeave infrastructure. Our model the training and evaluation were performed on an Nvidia A6000. The total training and evaluation time for our model was 164.5 hours, resulting in an estimated CO2 emission of $2 0 . 7 3 \mathrm { k g C O 2 e q }$ . For the training and evaluation of MolDiff and EDM models, we utilized an Nvidia A4000. The models required 1020 GPU hours, leading to an estimated CO2 emission of $9 4 . 3 6 ~ \mathrm { k g C O 2 e q }$ . For more details see (Kuznetsov et al. 2024).

![](images/d9124f7e566dd25929df850ab1f1fffbcc6b624e17724bcd53687a7c619d472d.jpg)  
Figure 6: Generated molecular structures for pocketconditioned generation (reference name 4iwq A) task.

# Conclusion

We have introduced nach0-pc, a novel model adept at generating diverse and physically plausible molecular 3D structures. By combining a domain-specific point cloud encoder with an encoder-decoder language model along with combined $\mathbf { S M I L E S + X Y Z }$ textual format and novel molecular point cloud pre-training technique, nach0-pc effectively addresses challenges associated with handling chemical 3D structures and a SMILES sequence. Through extensive finetuning within single-task and multi-task frameworks, nach0- pc exhibits comparable performance to various state-of-theart diffusion and LM baseline models. As future work, it would be valuable to explore training on a broader range of NLP and Chemistry 2D/3D tasks in a multi-task fashion, including molecular properties prediction based on a spatial input and protein-related tasks. The integration of proposed ideas into decoder-only approaches remains to be explored.