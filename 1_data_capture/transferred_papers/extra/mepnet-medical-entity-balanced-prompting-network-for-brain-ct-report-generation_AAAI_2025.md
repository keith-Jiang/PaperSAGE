# MEPNet: Medical Entity-Balanced Prompting Network for Brain CT Report Generation

Xiaodan Zhang1, Yanzhao $\mathbf { S h i } ^ { 1 }$ , Junzhong $\mathbf { J i } ^ { 1 * }$ , Chengxin Zheng1, Liangqiong $\mathbf { Q } \mathbf { u } ^ { 2 * }$

1College of Computer Science, Beijing University of Technology, Beijing, China 2School of Computing and Data Science, The University of Hong Kong, Hong Kong, China zhangxiaodan $@$ bjut.edu.cn, yanzhaoshi0927 $@$ outlook.com, jjz01@bjut.edu.cn, chaunxey_zeta $@$ emails.bjut.edu.cn, liangqqu@hku.hk

# Abstract

The automatic generation of brain CT reports has gained widespread attention, given its potential to assist radiologists in diagnosing cranial diseases. However, brain CT scans involve extensive medical entities, such as diverse anatomy regions and lesions, exhibiting highly inconsistent spatial patterns in 3D volumetric space. This leads to biased learning of medical entities in existing methods, resulting in repetitiveness and inaccuracy in generated reports. To this end, we propose a Medical Entity-balanced Prompting Network (MEPNet), which harnesses the large language model (LLM) to fairly interpret various entities for accurate brain CT report generation. By introducing the visual embedding and the learning status of medical entities as enriched clues, our method prompts the LLM to balance the learning of diverse entities, thereby enhancing reports with comprehensive findings. First, to extract visual embedding of entities, we propose Knowledge-driven Joint Attention to explore and distill entity patterns using both explicit and implicit medical knowledge. Then, a Learning Status Scorer is designed to evaluate the learning of entity visual embeddings, resulting in unique learning status for individual entities. Finally, these entity visual embeddings and status are elaborately integrated into multi-modal prompts, to guide the text generation of LLM. This process allows LLM to self-adapt the learning process for biased-fitted entities, thereby covering detailed findings in generated reports. We conduct experiments on two brain CT report generation benchmarks, showing the effectiveness in clinical accuracy and text coherence.

(a) Biased Learning of Medical Entities 0。 Entity Learning Loss $\checkmark$ arning Status Thalamus Basal Ganglia GBaansgalila 1.54 Limited DeHingsihty 0.25 Proficient   
High Density Low Density Density Low 3.87 Inadequate (b) F1 of Medical Entities in Generated Reports   
Models   
WGAM PGCA   
WGAM-HI Baseline Ours 0.4 0.5 0.6 0.7 0.8 0.9

Code — https://github.com/YanzhaoShi/MEPNet.

# Introduction

Radiology reports provide interpretations of detailed pathological findings from medical images, which are crucial for patient treatments. Despite the significance, for radiologists, hand-crafted writing practices are time-consuming and may cause error outcomes due to subjective factors (e.g. diversion and exhaustion) (Brady et al. 2012). The advent of automated medical report models can produce accurate reports and offer potential benefits for medical examinations, such as reducing the workload shouldered by physicians and saving the limited clinical resources in densely populated countries.

In recent years, medical report generation (MRG) (Jing, Xie, and Xing 2018; Yang et al. 2021, 2023; Shi et al. 2023; Li et al. 2023) has emerged as a core topic within the medical multi-modal learning. Different from natural image captioning (Xu et al. 2015; Ji et al. 2020), MRG models focus on reporting tiny pathological conditions from sparse and similar visual information. This motivates MRG models to learn specific patterns associated with medical entities, encompassing entities of anatomy regions such as “basal ganglia” and entities of lesions such as “high density” (refer to Figure 1(a)). To achieve this, task-specific attention mechanisms (Jing, Xie, and Xing 2018; Wang et al. 2018) are early designed to ground medical semantics with specific visual areas. Memory mechanism (Chen et al. 2021; Qin and Song 2022) is then proposed to store cross-modal patterns of diseases for generating accurate content. Moreover, knowledge graph (Liu et al. 2021; Li et al. 2023; Shi et al. 2023) is another effective approach for MRG models to grasp prior relations of medical entities. Recently, as large language models (LLMs) showcase impressive capabilities in language processing, a growing number of studies have started to employ LLMs as decoders for MRG. Through carefully designing medical prompts (Bu et al. 2024; Chen et al. 2024) and fine-tuning strategies (He et al. 2024), these methods have been proven to be more effective than traditional MRG models.

Despite the burgeoning popularity of LLM-based methods, the focus of most existing models remains on 2D imaging, e.g. chest X-rays. In contrast, brain CT data encompasses more complex pathological conditions in 3D volumetric space, with a broader range of medical entities (Shi et al. 2023). These medical entities display diverse visual distributions, grounded by unique shapes of cerebral organs or contours of intracranial lesions (Bhadauria and Dewal 2014). This visual diversity is vital for radiologists to identify and diagnose. However, empowering LLMs to effectively capture these diverse visual clues is challenging (Chen et al. 2024; Blankemeier et al. 2024), which results in a huge learning bias across different entities (see Figure 1(a)). This bias disperses the language model’s focus towards entities that should not be overly emphasized or ignored, leading to the generation of repetitive and inaccurate diagnostic sentences. Although logit-adjusted classifier (Jin et al. 2024) is recently proposed to mitigate this bias by prompting the language model with well-classified textual labels of entities, it reduces the LLM’s sensitivity to interpret visual patterns since it relies too heavily and too early on classified texts. Thus, how to prompt LLMs to fairly grasp visual patterns across various medical entities remains a critical concern in MRG tasks.

In this paper, we propose a Medical Entity-balanced Prompting Network (MEPNet) to enhance the capability of LLM in balancing the learning of diverse medical entities for high-quality brain CT report generation. The core idea is to introduce the visual embedding and learning status for individual medical entities, which are integrated as enriched clues to prompt LLM to fairly understand entities’ visual patterns. Specifically, the learning status is employed to evaluate the quality of entity visual embedding. To derive visual embedding for entities, we propose a Knowledge-driven Joint Attention. It captures and distills entity patterns from CT scans via explicit medical knowledge (summarized from a knowledge graph developed under expert guidance) and implicit medical knowledge (explored by the model itself). Next, a Learning Status Scorer is introduced to evaluate the learning statuses of entities, which are then converted to status words, e.g. limited, moderate, and proficient, to represent varying degrees of learning. Finally, we integrate these entity visual embeddings and related status as entity-balanced prompts, which are elaborately combined with scan visual embeddings and task-specific instructions to form multi-modal prompts, guiding LLM in generating texts. As evidenced in Figure 1(b), our method achieves a balance in precisely generating diverse medical entities in reports.

Our main contributions can be summarized as: 1. We propose a novel method that balances the LLM in learning diverse medical entities for accurate brain CT

report generation. This is achieved by integrating enriched medical clues, which include visual embeddings and related learning status of entities, to prompt LLM fairly generate findings.   
2. We design a Knowledge-driven Joint Attention to exploit high-level visual embeddings of medical entities from 3D volumetric space by using explicit and implicit medical knowledge; We further introduce a Learning Status Scorer to evaluate the quality of entity visual embeddings.   
3. We comprehensively evaluate our model on the BCT-CHR and CTRG datasets. The results confirm the effectiveness of our method in generating accurate brain CT reports.

# Related Works

With the development of intelligent healthcare, medical report generation (MRG), as an essential yet challenging application, has received notable attention (Jing, Xie, and Xing 2018; Li et al. 2019; Jia et al. 2020; Zhang et al. 2020; Song et al. 2022; Zhang et al. 2023a; Shen et al. 2024; Chen et al. 2024; Zheng et al. 2024). To satisfy the accuracy in generating long medical descriptions, existing efforts can be roughly divided into three types. The first type is enhancing visual features via domain knowledge for producing precise texts. Jing, Xie, and Xing (2018) propose to inject medical concepts into visual embeddings by a co-attention mechanism. Li et al. (2019) and Zhang et al. (2020) mine disease relations from graph knowledge and use them to enhance visual contexts. The second type of MRG is to conduct visual-textual alignment to improve the cross-modal correspondence. To achieve this, memory mechanism (Chen et al. 2021; Qin and Song 2022; Wang, Bhalerao, and He 2022) is designed to map multi-modal features into mutual space for generating accurate sentences grounded by relevant visual patterns. Contrastive learning (Yan et al. 2021; Shi et al. 2023; Yang et al. 2023; Li et al. 2023; Shi et al. 2024) is also widely applied to model cross-modal representations in an unsupervised manner, showing efficiency in small-scale medical datasets. The recent advance in LLMs inspires the third type of MRG, which can replace previous complex model designs with unified LLMs while improving task performance (Zhou et al. 2024). Yan et al. (2023) froze an LLM and leverage incontext learning to generate human-style chest X-ray reports. Dia-LLaMA (Chen et al. 2024) fine-tunes LLaMA2-7B (Touvron et al. 2023) by using discriminated disease labels as knowledge prompts. Bu et al. (2024) extract knowledge of pulmonary lesions from patient instances to facilitate X-ray report generation. Liu et al. (2024a) further bootstrap LLMs for MRG by instance induction and coarse-to-fine decoding. Compared with them, we focus on more complex 3D brain CT scans and offer a novel prompting paradigm to represent medical entity clues with informative embeddings and learning status. This activates the inner power of LLM to balance entity learning and reduce the hallucinations in generating diagnostic sentences.

Multi-scan Visual Prompting Instruction Brain CT Report: Scan Visual Embeddings Patchy highScan Features density with lowdensity edema Vision Vf Visual Ve LLM around in the left CT Encoder Adaptor (LoRA) cpoarmieptraelslosiboen, owfith Scans the left lateral ventricle. The   
Entity-balanced Prompting midline structure is Vf Entity Visual Embeddings right shifted. Medical Entities Speckled low  
low density high density Text Knowledge-driven Ef Entity Ee tdhenlseitfty ibsasaelen in   
lateral ventricle edema Encoder Joint Attention Adaptor ganglia, and the basal ganglia Balanced white matter of the Ef Entity lateral ventricle is Status Scores Learning Status Embeddings Learning descreased Learning Status Scorer low density Co bilaterally. ? (0.58) “Moderate” Se Medical Entity high density “Proficient” Classifier (0.21) Trainable Entity Losses Normalize Losses bas(a0l .g7a6n)glia “Limited” Frozen

# Methods

# 3.1 Overall Framework

An overview of MEPNet is shown in Figure 2. Given a patient sample with $N$ brain CT scans $\boldsymbol { S } = \{ s _ { 1 } , . . . , s _ { N } \}$ , the target is to generate a brain CT report $\boldsymbol { R } = \{ r _ { 1 } , . . . , r _ { M } \}$ with $M$ textual words. We employ LLM as the decoder for report generation, with enriched prompts derived from two distinct branches: The multi-scan visual prompting branch and the entity-balanced prompting branch.

Multi-scan Visual Prompting To obtain visual features from volumetric CT scans, we follow Zhang et al. (2023b) and utilize a finetuned ResNet101 model to extract scan features $V _ { f } ~ = ~ \{ v _ { f _ { 1 } } , . . . , v _ { f _ { N } } \} ~ \in ~ \mathbb { R } ^ { N \times P \times d _ { s } }$ for each sample, where $P$ and $d _ { s }$ denote the patch number and feature channels, respectively. We then employ a visual adaptor to convert $V _ { f }$ into the word embedding space $d _ { w }$ within LLM:

$$
V _ { e } = ( V _ { f } W _ { v _ { 1 } } ^ { \top } + b _ { v _ { 1 } } ) W _ { v _ { 2 } } ^ { \top } + b _ { v _ { 2 } } ,
$$

where $V _ { e } ~ \in ~ \mathbb { R } ^ { N \times d _ { w } }$ denotes the scan visual embedding, $W _ { v _ { 1 } } ~ \in ~ \mathbb { R } ^ { ( P * d _ { s } ) \times d _ { s } }$ and $W _ { v _ { 2 } } ~ \in ~ \mathbb { R } ^ { d _ { s } \times d _ { w } }$ are learnable weights, $b _ { v _ { 1 } }$ and $b _ { v _ { 2 } }$ and are learnable biases.

Entity-Balanced Prompting Adding medical information into prompts is proven to enhance the diagnostic capabilities of LLMs (Jin et al. 2024). Unlike previous methods that construct text prompts based on classified categories, we propose to use informative visual embeddings of medical entities as prompts. This forces the LLM to grasp medical patterns from redundant visual features, rather than just taking shortcuts by rewriting the classified entity labels. We first collect a set of medical entities $E = \{ e _ { 0 } , e _ { 1 } , . . . , e _ { k } \}$ , where $K$ entities $\boldsymbol { e } _ { 1 : k }$ are summarized by Shi et al. (2023) and $e _ { 0 }$ denotes a global entity (i.e. [global]) to represent overall conditions. We extract textual features of k + 1 entities as Tf ∈ R(k+1)×dw via LLM’s text embedding layer. Then, $T _ { f }$ is used to capture detailed entity visual features $E _ { f } \in \mathbb { R } ^ { ( k + 1 ) \times d _ { h } }$ from the whole scan features $V _ { f }$ via Knowledge-driven Joint Attention (KJA) module (see Section 3.2.). Next, we remove the global entity from $E _ { f }$ and map the remaining feature into LLM’s embedding space, resulting in entity visual embeddings Ee Rk×dw.

Additionally, we introduce a learning status scorer to evaluate the learning status of $k$ entities, which is denoted as $T _ { s } = \{ t _ { s _ { 1 } } , . . . , t _ { s _ { k } } \} \in \mathbb { R } ^ { k }$ . We convert $T _ { s }$ into status words $S _ { w } = \{ s _ { w _ { 1 } } , . . . , s _ { w _ { k } } \}$ , which is embedded as learning status embeddings $S _ { e }$ to depict the learning degree of each entity.

Model Training We integrate the scan visual embedding $V _ { e }$ , entity visual embedding $E _ { e }$ , and learning status embedding $S _ { e }$ into multi-modal prompt $M _ { P }$ , which guides a LLMbased decoder to generate reports. The template of our multimodal prompt is shown in Figure 3, where $V _ { e }$ , $E _ { e }$ , and $S _ { e }$ are denoted as {scan}, {entity_embedding}, and {entity_status}, respectively. The remaining contents are instructions, where {entity $i \dag$ is the entity word $e _ { i }$ . We also add special tokens, e.g. [Img] and $I / I m g { \cal { I } }$ for visual input identification, and $I M R G { \cal I }$ for task identification. We finetune LLM by LoRA $\mathrm { H u }$ et al. 2021) and use cross-entropy loss for optimization:

$$
\mathcal { L } _ { g } = - \sum _ { t = 1 } ^ { M } \log P ( y _ { t } \mid y _ { 1 : t - 1 } , M _ { P } ; \theta )
$$

where $P ( y _ { t } | * )$ denotes the probability conditioned on multimodal prompt $M _ { P }$ and the embeddings of previous words $y _ { 1 } , y _ { 2 } , \dotsc , y _ { t - 1 } . \theta$ denotes the trainable parameters in LLM.

[Img]{scan}{scan}...{scan}[/Img]   
[MRG]Describe multiple brain CT images in detail   
and generate a Chinese brain CT report.   
You shoulf refer to the embeddings and learning   
statues of the following {K} medical entities:   
{entity_1}{entity_embedding}{entity_status};   
{entity_2}{entity_embedding}{entity_status};   
{entity_K}{entity_embedding}{entity_status}. Scan Visual Entity Visual Learning Status Embeddings Embeddings Embeddings E Knowledge Graph   
Vf Scan Features Add & Norm   
Tf Entity Textual Features 4 Ef Entity Visual Features Feed Forward Graph Node & Edge Explicit ↑ Knowledge Weighted Sum Add & Norm ME Medical Vf Knowledge-masked AdjMactreinxcy Self Att ↑ → M I Predicted Cross Tf Att Implicit Adjacency Entity Relation Knowledge Matrix Projector

# 3.2 Knowledge-Driven Joint Attention

Given entity textual features $T _ { f }$ and scan features $V _ { f }$ , this module aims to capture detailed visual features for entities based on entities’ medical relationships. Unlike directly injecting relations into visual features (Zhang et al. 2020) or textual keywords (Li et al. 2023), we argue that they may fail to learn multi-modal medical clues. Instead, we first integrate $T _ { f }$ into $V _ { f }$ by cross attention, and then use knowledgemasked self attention to inject relations.

Cross Attention We project entity textual features $T _ { f }$ into $d _ { h }$ channels to form query vector $Q _ { c }$ . The channel of scan features $V _ { f }$ is also projected to $d _ { h }$ to form key vector $K _ { c }$ and value vector $S _ { c }$ . The cross attention can be formulated as:

$$
T _ { f } ^ { ' } = F F N ( M H A ( Q _ { c } , K _ { c } , S _ { c } ) ,
$$

where $T _ { f } ^ { ' } \in \mathbb { R } ^ { ( k + 1 ) \times d _ { h } }$ primarily depicts the visual features of medical entities. FFN and MHA denote the feed-forward network and multi-head attention, respectively.

Knowledge Extraction The obtained $T _ { f } ^ { ' }$ is still limited to representing complex visual patterns since it overlooks the inherent medical relations of entities that are vital for diagnosis. To tackle this, we extract entities’ relations from both explicit and implicit perspectives as medical knowledge.

Explicit entity relations depict empirical medical knowledge defined by experts. We acquire this knowledge from brain pathological graph (Shi et al. 2023). This graph divides medical entities $E$ into anatomy regions $E _ { r } \in E$ and lesions $E _ { l } \in E$ , with three types of fine-grained edges connecting them: 1) $R _ { t 2 t }$ : edges among anatomy regions; 2) $R _ { l 2 l }$ : edges among lesions; 3) $R _ { t 2 l }$ : edges between anatomy regions and lesions. $R _ { t 2 t }$ and $R _ { l 2 l }$ are fixed by medical priors, and $R _ { t 2 l }$ is built based on the specific tissue-lesion relations in ground truth report. Note that during model inference, we do not extract $R _ { t 2 l }$ from ground truth report to prevent data leakage. Instead, we substitute it with the statistical information of tissue-lesion relationships derived from the training data. Based on this graph, given one entity $e _ { i } ~ \in ~ E$ as a query, we can find its relations with other entities, which can be denoted as $R _ { i 2 E } \in \mathbb { R } ^ { k + 1 }$ . The relations associated with the global entities $e _ { 0 }$ are set to 1. We extract all entities’ relations and concatenate them as medical adjacency matrix $M _ { E } \in \mathbb { R } ^ { ( k + 1 ) \times ( k + 1 ) }$ (see Figure 4).

Besides, there also exists some implicit yet essential relations among medical entities that haven’t been explored by radiologists. To fully consider these potential entity relations, we introduce an entity relation projector to predict them automatically, which can be formulated as:

$$
\begin{array} { r c l } { { { \cal R } _ { e } } } & { { = } } & { { { \cal W } _ { r _ { e } } { \cal T } _ { f } ^ { ' } + b _ { r _ { e } } , } } \\ { { { \cal M } _ { I } } } & { { = } } & { { { \cal S } i g m o i d ( { \cal W } _ { r _ { p } } { \cal R } _ { e } { \cal R } _ { e } ^ { \top } + b _ { r _ { p } } ) , } } \end{array}
$$

where $S i g m o i d ( . )$ is the sigmoid function. $ { \mathcal { M } } _ { I } \in$ R(k+1)×(k+1) is the predicted adjacency matrix. Wre , Wrp are learnable weight matrixes, $b _ { r _ { e } } , b _ { r _ { p } }$ are learnable biases.

We combine two types of matrixes into the entity adjacency matrix $M _ { a d j }$ , which can be formulated as:

$$
{ \cal M } _ { a d j } = \alpha _ { E } { \cal M } _ { E } + \alpha _ { I } { \cal M } _ { I } ,
$$

where $\alpha _ { E }$ and $\alpha _ { I }$ are hypermeters to balance the weights of $M _ { E }$ and $M _ { I }$ .

Knowledge-Masked Self Attention We employ $M _ { a d j }$ as the attention mask and conduct knowledge-masked self attention to inject medical knowledge into $T _ { f } ^ { \overline { { \prime } } }$ :

$$
\begin{array} { r c l } { { e _ { a t t } } } & { { = } } & { { L N ( K S A ( T _ { f } ^ { ' } , M _ { a d j } ) + T _ { f } ^ { ' } ) , } } \\ { { E _ { f } } } & { { = } } & { { L N ( F F N ( e _ { a t t } ) + e _ { a t t } ) , } } \end{array}
$$

where $K S A$ is masked multi-head attention with $M _ { a d j }$ as mask, $L N$ is the layer norm. $E _ { f } ~ = ~ \{ e _ { f _ { 0 } } , . . . , e _ { f _ { k } } \} ~ \stackrel { . } { \in }$ $\mathbb { R } ^ { ( k + 1 ) \times d _ { h } }$ denotes the entity visual features. $e _ { f _ { i } }$ denotes the visual feature of the i-th entity, which aggregates its neighbor entity features according to relations in $M _ { a d j }$ .

# 3.3 Learning Status Scorer

To address the biased learning of medical entities, we propose a learning status scorer to evaluate the learning status of each entity at each training step. These learning statuses are used to prompt the LLM to balance the learning of diverse entities.

Score Generation The scorer is based on calculating discriminative loss for each medical entity. Followed by Jin et al. (2024), we design a medical entity classifier to obtain discriminative loss. The classifier can be formulated as:

$$
\begin{array} { r c l } { E _ { h } } & { = } & { W _ { e _ { h } } E _ { f } + b _ { e _ { h } } , } \\ { E _ { p } } & { = } & { S i g m o i d ( C l s ( E _ { h } ) ) , } \end{array}
$$

where $\mathit { C l s } ( . )$ denotes a classification head, $\begin{array} { r l } { E _ { p } } & { { } = } \end{array}$ $\{ e _ { p _ { 1 } } , . . . , e _ { p _ { k } } \} \in \mathbb { R } ^ { k }$ denotes the predicted labels for $k$ entities. Then, given a ground truth label $\hat { \cal E } _ { p } = \{ { \hat { e } } _ { p _ { 1 } } , . . . , { \hat { e } } _ { p _ { k } } \} \in$ $\{ 0 , 1 \} ^ { k }$ , we gather labels of individual entities in a batch level to measure the loss of each entity, e.g., the loss for entity $e _ { i }$ can be calculated as:

$$
e _ { s _ { i } } = - \sum _ { j } ^ { b } W _ { e _ { j } } [ e _ { p _ { i } } ^ { ( j ) } \log \hat { e } _ { p _ { i } } ^ { ( j ) } + ( 1 - e _ { p _ { i } } ^ { ( j ) } ) \log ( 1 - \hat { e } _ { p _ { i } } ^ { ( j ) } ) ] ,
$$

where b denotes the batch size, e(pj and $\hat { e } _ { p _ { i } } ^ { ( j ) }$ denote the ground truth label and predicted label of entity $e _ { i }$ in the $j - t h$ sample, respectively. Then, we apply the Exponential Saturation Function $y = 1 - e ^ { - x }$ to normalize the loss to the range of 0 to 1. We use the normalized entity loss as the score of learning status, where a higher score indicates a worse learning of related entity. The scores for all entities are represented as $E _ { s } = \{ e _ { s _ { 1 } } , . . . , \dot { e } _ { s _ { k } } \} \in \mathbb { R } ^ { k }$ .

Meanwhile, to ensure a good evaluation of learning status, we optimize the classifier during the training:

$$
\mathcal { L } _ { d } = - \sum _ { i } ^ { b } W _ { d _ { i } } [ E _ { p } ^ { ( i ) } \log \hat { E } _ { p } ^ { ( i ) } + ( 1 - E _ { p } ^ { ( i ) } ) \log ( 1 - \hat { E } _ { p } ^ { ( i ) } ) ] ,
$$

where $E _ { p } ^ { ( i ) }$ and $\hat { E } _ { p } ^ { ( i ) }$ represent the ground truth label and predicted label of sample $i$ .

exceptional proficient moderate limited inadequate 1 1 1 0.0 0.2 0.4 0.6 0.8 1.0 Status Score

Status Word Matching To let the LLM understand the status scores for balancing entity learning, we convert the generated scores to textual words, as LLM has an inherent sensitivity to interpreting texts. As shown in Figure 5, we set 5 types of status words to match different ranges of scores. Given scores $E _ { s } \in \mathbb { R } ^ { k }$ , the converted words can be denoted as $S _ { w } = \{ s _ { w _ { 1 } } , . . . , s _ { w _ { k } } \} \in \mathbb { R } ^ { k }$ . We then employ LLM’s text embedding layer to encode $S _ { w }$ into learning status embeddings $S _ { e } \in \mathbb { R } ^ { \tilde { k } \times d _ { w } }$ . For status words that are tokenized into multiple tokens by LLM, we average their embeddings to represent the centroid semantics, which reduces computational costs while preserving the initial semantics (Li et al. 2023).

# 3.4 Training Objective

Our overall loss contains report generation loss and classification loss, which can be represented as:

$$
\mathcal { L } = \mathcal { L } _ { g } + \lambda \mathcal { L } _ { d } ,
$$

where $\lambda$ is the coefficient to balance the module losses.

# Experiments and Results 4.1 Datasets, Metrics and Settings

BCT-CHR BCT-CHR (Yang et al. 2021) is the benchmark for brain CT report generation. It comprises 49,152 brain CT scans paired with 2,048 Chinese reports. Each example includes $2 4 \mathrm { C T }$ scans along with a report. We split the training, testing, and validation sets in a ratio of 7:2:1. Note that in this paper, we only give English translations of Chinese reports for better understanding.

CTRG-Brain CTRG-Brain (Tang et al. 2024) is an opensource dataset, which contains 6007 samples with cranial abnormalities and includes 160,352 brain CT scans and 6007 Chinese reports. We employ brain CT data cleaning pipeline (Zhang et al. 2023b) to filter redundant scans and ensure each sample contains 24 scans, aligning the data with medical standards. Since the official split misses a validation set, we align its data split with the BCT-CHR in 7:2:1.

Metrics We use natural language generation (NLG) and clinical evaluation (CE) metrics to evaluate the report generation. The NLG metrics contains BLEU (Papineni et al. 2002), METEOR (Lavie and Agarwal 2007), ROUGE-L (Lin 2004) and CIDEr (Vedantam, Zitnick, and Parikh 2015). For CE metrics, we follow Shi et al. (2023) and collect 24 keywords, e.g. “basal ganglia region", “cerebral sulcus" and “stagnant blood", to calculate Precision, Recall, and F1 scores.

Settings Our baseline follows encoder-decoder architecture. We utilize ResNet101 (He et al. 2016) to process visual features, which is fine-tuned for the brain hemorrhage classification using the CQ500 dataset (Chilamkurthy et al. 2018). For the textual decoder, we utilize LLaMA3-8B (Meta 2024), configured with a 4-bit quantization setting. LoRA (Hu et al. 2021) (with a rank of 64) is also adopted for efficient finetuning, with $0 . 3 4 \%$ trainable parameters in LLM (27.3M). We use Chinese instructions to prompt LLM, and the template in Figure 3 is translated into English for better clarity. The feature channels $\{ d _ { h } , d _ { s } , d _ { w } \}$ , knowledge weights $\{ \alpha _ { E } , \alpha _ { I } \}$ , and classification loss weight $\lambda$ are set to {512, 2048, 4096}, {0.9, 0.1}, and 0.1, respectively. We train the model on two RTX 3090 GPUs via AdamW optimizer with a learning rate of 1e-4 for 10 epochs, and the batch size is set to 4.

# 4.2 Comparision Studies

We compare our model with advanced brain CT report generation methods WGAM (Yang et al. 2021), PGCA (Shi et al. 2023), and WGAM-HI (Zhang et al. 2023b). Besides, we also reproduce some other models initially designed for image captioning (Up-Down (Anderson et al. 2018)), chest X-ray MRG (WCL (Yan et al. 2021), PromptMRG (Jin et al. 2024)), and chest CT MRG (HILT (Liu et al. 2024b)) for comprehensive comparisons.

As shown in Table 1, our model achieves superior performance on both two datasets, highlighting the effectiveness of our entity-balanced learning strategy. In contrast, Up-Down yields inferior results (especially on B4) due to the limited focus on medical information. WGAM utilizes weakly guided visual attention to learn spatial features, while WGAM-HI enhances it in a multi-scale fashion and achieves higher CE scores. Compared with traditional decoders, HILT adopts LLM as decoder, which performs even better than competitors originally designed for brain CT MRG on the CTRG-Brain dataset. Surprisingly, HILT performs worse on a relatively small BCT-CHR dataset. The reason may be that without appropriate prompts to connect the visual encoder and text decoder, standard LLM struggles to efficiently learn from limited medical data. Although PromptMRG prompts the language model with classified disease labels, it overly relies on the given labels and only gains modest scores. Notably, our method excels with significant gains in CIDEr, showing the advances in handling the repetitiveness in generated reports. The SoTA score on Recall also indicates the comprehensive coverage of medical keywords. In Figure 1(b), we are encouraged to find that our method shows the largest mean and smallest variance for generating diverse entities, indicating the achievement of balanced medical entity learning. Despite the B4 rises on CTRG-Brain, the score on BCT-CHR remains lower. This may be due to the more flexible writing styles in BCT-CHR reports, as we do not regulate grammar structure or filler words in LLM, leading to sentence rephrasing and reducing multi-gram word matching. Additionally, our model has lower precision, likely because our status embeddings prompt the LLM to generate a broader range of entities, lowering precision but highly boosting recall and F1 scores, which are more beneficial for diagnosis assistance.

Table 1: Performance comparison of our MEPNet with state-of-the-art models on two brain CT report generation datasets. Models are categorized into those utilizing traditional decoders and those employing LLM decoders. Superior results are highlighted in bold. Note that CE metrics are solely evaluated on the BCT-CHR dataset, as the CTRG-Brain dataset lacks a unified evaluation criterion. $\dagger$ denotes the re-produced models.   

<html><body><table><tr><td rowspan="2">Datasets</td><td rowspan="2">Methods</td><td colspan="4">NLG Metrics</td><td colspan="3">CE Metrics</td></tr><tr><td>B4</td><td>M</td><td>R-L</td><td>C</td><td>F1</td><td>Recall</td><td>Precision</td></tr><tr><td rowspan="9">CTRG-B</td><td>Up-Down(Anderson et al. 2018)t</td><td>24.4</td><td>31.6</td><td>42.5</td><td>27.3</td><td></td><td></td><td></td></tr><tr><td>WCL(Yan et al. 2021)t</td><td>25.1</td><td>31.3</td><td>42.8</td><td>33.3</td><td></td><td></td><td></td></tr><tr><td>WGAM(Yang et al. 2021)t</td><td>25.4</td><td>32.0</td><td>42.4</td><td>31.9</td><td></td><td></td><td></td></tr><tr><td>PGCA(Shi et al. 2023)t</td><td>26.5</td><td>32.5</td><td>43.0</td><td>34.0</td><td></td><td></td><td></td></tr><tr><td>WGAM-HI(Zhang et al. 2023b)t</td><td>26.1</td><td>31.4</td><td>43.8</td><td>33.2</td><td></td><td>■</td><td></td></tr><tr><td>PromptMRG(Jin et al. 2024)t</td><td>26.6</td><td>31.0</td><td>47.4</td><td>50.3</td><td></td><td>■</td><td></td></tr><tr><td>HILT(Liu et al. 2024b)t</td><td>33.3</td><td>33.0</td><td>50.3</td><td>80.0</td><td></td><td>1</td><td></td></tr><tr><td>MEPNet (Ours)</td><td>34.4</td><td>34.6</td><td>50.4</td><td>86.6</td><td></td><td>■</td><td>■</td></tr><tr><td>Up-Down(Anderson et al. 2018)t</td><td>13.5</td><td>27.0</td><td>35.4</td><td>20.1</td><td>47.2</td><td></td><td></td></tr><tr><td rowspan="7">BCT-CHR</td><td></td><td>14.2</td><td></td><td></td><td></td><td></td><td>59.8 50.2</td><td>49.5 51.0</td></tr><tr><td>WCL(Yan et al. 2021)t WGAM(Yang et al. 2021)t</td><td>14.5</td><td>26.9</td><td>35.3</td><td>18.8</td><td>42.4 41.6</td><td>52.6</td><td>52.8</td></tr><tr><td>PGCA(Shi et al. 2023)t</td><td>15.5</td><td>27.8 28.7</td><td>35.1 36.5</td><td>17.3 19.9</td><td>49.6</td><td>61.9</td><td>52.0</td></tr><tr><td>WGAM-HI(Zhang et al. 2023b)t</td><td>15.6</td><td>29.0</td><td>36.5</td><td>22.2</td><td>51.4</td><td>66.8</td><td>51.1</td></tr><tr><td>PromptMRG(Jin et al. 2024)t</td><td>12.9</td><td>26.8</td><td>34.0</td><td>16.1</td><td>49.6</td><td>52.9</td><td>43.3</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HILT(Liu et al. 2024b)t MEPNet (Ours)</td><td>12.6 14.7</td><td>27.3 29.2</td><td>33.6 36.8</td><td>15.9 22.3</td><td>43.8 52.0</td><td>52.4 68.9</td><td>39.9 50.6</td></tr></table></body></html>

# 4.3 Ablation Studies

We conduct ablation studies on components of entitybalanced prompting (i.e. entity visual embedding $E _ { e }$ and status embedding $S _ { e }$ ) and types of medical knowledge within knowledge-driven joint attention (i.e. $M _ { E }$ and $M _ { I }$ ).

As shown in Table 2, our baseline performs worse than other settings, which indicates that it is difficult for LLM to directly understand 3D visual features. Comparing the results of baseline and (a,b,c), we find that using visual embeddings of medical entities substantially enhances the scores, especially when using both the $M _ { E }$ and $M _ { I }$ in KJA (see setting (c)). We speculate the reason is that the explicit and implicit medical relations within $M _ { E }$ and $M _ { I }$ effectively enhance entity visual embeddings with medical grounds, thereby guiding LLM to generate precise texts. Besides, (b) highly improves F1 by $8 . 5 \%$ over (a), which illustrates the explicit entity relations extracted from the expert knowledge bring more positive effects than predicted relations. We also observe that (d) and (e) exhibit significant improvements over (a) and (b), respectively, especially in Recall and CIDEr. This indicates the contribution of learning status, which guides LLM to balance the learning of diverse entities and generate precise findings. Our model’s lower precision compared to (c) stems from the reasons discussed in Section 4.2.

To further illustrate the advantages of our entity-balanced prompt, we compare it with prompts that rely on classified categories (Jin et al. 2024; Zhou et al. 2024). We collect the categories (“exist” or “not exist”) of each entity, which are predicted by our medical entity classifier. These categories are mapped into textual embeddings (denoted as Category.) and added to the prompt. Comparing the first two lines and the last two lines in Table 3, it is clear that Category. decreases the scores, which is inconsistent with current 2D MRG works. The reason may be that for more complex 3D data, classified category allows LLMs to take more shortcuts than interpreting visual patterns. This attributes the upper limit of performance to the classifier, rather than LLM itself.

Table 2: Ablation studies of the proposed MEPNet on the BCT-CHR dataset, where different settings of medical entity-balanced prompts and knowledge-driven joint attention are evaluated. KJA denotes the integration of knowledge-driven joint attention. $M _ { E }$ and $M _ { I }$ denote the medical adjacency matrix and predicted adjacency matrix in KJA, respectively.   

<html><body><table><tr><td rowspan="2">Method</td><td colspan="2">Entity-balanced Prompts</td><td colspan="2">Entity AdjacencyMatrix</td><td colspan="3">CE Metrics</td><td colspan="4">NLG Metrics</td></tr><tr><td>Entity Visual Embed.</td><td>Status Embed.</td><td>w/ME</td><td>w/ M1</td><td>F1</td><td>R</td><td>P</td><td>B4</td><td>M</td><td>R-L</td><td>C</td></tr><tr><td>baseline</td><td></td><td></td><td></td><td></td><td>33.7</td><td>43.1</td><td>46.3</td><td>10.2</td><td>24.0</td><td>32.3</td><td>12.8</td></tr><tr><td>(a)</td><td>√</td><td></td><td>√</td><td></td><td>35.4</td><td>43.9</td><td>45.8</td><td>11.3</td><td>24.5</td><td>35.2</td><td>17.1</td></tr><tr><td>(b)</td><td>√</td><td></td><td></td><td>√</td><td>43.9</td><td>56.1</td><td>47.9</td><td>13.1</td><td>26.5</td><td>36.7</td><td>19.0</td></tr><tr><td>（c）</td><td>√</td><td></td><td>√</td><td>√</td><td>44.0</td><td>55.9</td><td>49.3</td><td>13.9</td><td>26.8</td><td>36.4</td><td>21.3</td></tr><tr><td>(d）</td><td>√</td><td>√</td><td>√</td><td></td><td>45.4</td><td>60.0</td><td>44.6</td><td>12.7</td><td>28.8</td><td>34.5</td><td>21.7</td></tr><tr><td>(e)</td><td>√</td><td>√</td><td></td><td>√</td><td>49.4</td><td>67.8</td><td>46.5</td><td>14.2</td><td>28.2</td><td>35.9</td><td>19.6</td></tr><tr><td>Ours</td><td>√</td><td>√</td><td>√</td><td>√</td><td>51.4</td><td>68.8</td><td>48.8</td><td>14.7</td><td>29.2</td><td>36.8</td><td>22.3</td></tr></table></body></html>

Ground Truth Brain CT Data Classification Confidence Generated Brain CT Reports Methods 0.80 0.83 Large patchy high density observed in the right basal ganglia region, surrounded by 0.66 low-density edema. The right lateral ventricle is compressed and Baseline narrowed. Midline right shifted slightly. 0.13 Large patchy high density seen in the left thalamus, surrounded by low-density edema, with  compression  of the left  lateral ventricle.  Midline  right shifted  slightly.  Multiple scattered punctate low density  observed bilaterally in the  basal ganglia PGCA 9li region and lateral ventricle. The white matter of the ventricle is decreased bilaterally. Entity Adjacency Matrix Large patchy  high density  seen in the left  thalamus, surrounded by low$( M _ { E } + M _ { I } )$ density edema, with compression and narrowing of the left lateral ventricle. Midline right Findings: Large patchy high density seen in the shifted slightly. Multiple scattered punctate low density observed bilaterally in the basal WGAM-HI lweifth latetrhianl  lvaeynetr colfe oawn-d ebnasistay  egdaenmglai.a Trheeg iloenf,t tghaen vgleian ricler iesg idoencreasaend db ilatelratlleyr.al ventricle. The white matter of 0.8 lateral ventricle is compressed. Midline  right 0.6 shifted slightly. Multiple scattered patchy low Large patchy high density observed in the left basal ganglia region, surrounded by lowdensity observed bilaterally in the centrum 0.4 density edema. The left lateral ventricle is compressed and narrowed. Mildline right semiovale, lateral ventricle, basal ganglia region, shifted slightly. Multiple scattered punctate low density seen bilaterally in the basal Ours and thalamus. The white matter of the lateral 0.2 ganglia region and lateral ventricle with some borders clearly defined. The white matter ventricle is decreased bilaterally. of the ventricle is decreased bilaterally.

Figure 6: Comparison of brain CT reports generated by different models. Correct medical entities of anatomy region and lesion are marked in green and purple, respectively. Incorrect contents are in red. Classification confidence and entity adjacency matrix are also visualized for supplements.   
Table 3: Comparisons on using different prompts. Embed., Status. and Category. denote the entity visual embedding, status embedding, and classified entity category, respectively.   

<html><body><table><tr><td colspan="3">Entity Prompts</td><td rowspan="2">B4</td><td rowspan="2">M</td><td rowspan="2">R-L</td><td rowspan="2">C</td><td rowspan="2">F1</td></tr><tr><td></td><td>Embed. Status. Category.</td><td></td></tr><tr><td>√</td><td></td><td></td><td>13.9</td><td>26.8</td><td>36.4</td><td>21.3</td><td>44.0</td></tr><tr><td>√</td><td></td><td>√</td><td>13.3</td><td>26.2</td><td>36.3</td><td>18.9</td><td>45.8</td></tr><tr><td>√</td><td>√</td><td></td><td>14.7</td><td>29.2</td><td>36.8</td><td>22.3</td><td>51.4</td></tr><tr><td>√</td><td>√</td><td>√</td><td>14.2</td><td>29.1</td><td>35.4</td><td>20.6</td><td>46.6</td></tr></table></body></html>

# 4.4 Qualitative Studies

In Figure 6, we give examples of brain CT reports to better understand our approach. As we can see, the baseline generates the worst report with errors and missed findings. PGCA and WGAM-HI generate more detailed medical content but still have critical errors, such as misdiagnosing the “ high density seen in the left thalamus” and “surrounded by low-density edema”, which indicates the weakness in capturing diverse medical entities. In contrast, our model correctly describes a border range of medical entities, e.g. basal ganglia, lateral ventricles, and high density. It suggests that, in our approach,

LLM is supported by enriched medical clues within entity visual embeddings and learning status, thereby aligning well with ground truth reports. To further understand the learning process of medical entities, we visualize the confidence score of the medical entity classifier, which manifests aligned results with entities in ground truth reports. We also visualize the medical entity adjacency matrix (a weighted sum of $M _ { E }$ and $M _ { I }$ calculated by Equation 6). It reveals that the model can integrate both explicit and implicit medical knowledge and leverage it for report generation.

# Conclusion

In this paper, we propose to boost brain CT report generation via a medical entity-balanced prompting network, namely MEPNet, which seamlessly enhances the capabilities of the LLM to fairly learn and generate various medical entities. Experiment results show the effectiveness of our model in comparison with SoTA methods on both the BCT-CHR and CTRG-Brain benchmarks. We also verify the improvements brought by each prompt component, i.e. entity visual embedding and learning status embedding. Notably, our approach provides a practical paradigm for activating the capability of LLMs to balance the learning of diverse entities, showing the potential of extending the approach to other medical tasks.