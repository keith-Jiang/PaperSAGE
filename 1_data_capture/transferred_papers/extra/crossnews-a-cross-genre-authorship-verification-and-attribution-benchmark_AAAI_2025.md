# CROSSNEWS: A Cross-Genre Authorship Verification and Attribution Benchmark

Marcus $\mathbf { M } \mathbf { a } ^ { 1 }$ , Duong Minh Le1, Junmo Kang1, Yao Dou1, John Cadigan2, Dayne Freitag2, Alan Ritter1, Wei $\mathbf { X } \mathbf { u } ^ { 1 }$

1Georgia Institute of Technology 2SRI International marcus.ma, dminh6, junmo.kang, douy $@$ gatech.edu john.cadigan, daynefreitag @sri.com {wei.xu, alan.ritter}@cc.gatech.edu

# Abstract

Authorship models have historically generalized poorly to new domains because of the wide distribution of authoridentifying signals across domains. In particular, the effects of topic and genre are highly domain-dependent and impact authorship analysis performance greatly. This paper addresses the existing data gap in authorship for these resources by introducing CROSSNEWS, a novel cross-genre dataset that connects formal journalistic articles and casual social media posts. CROSSNEWS is the largest authorship dataset of its kind for supporting both verification and attribution tasks, with comprehensive topic and genre annotations. We use CROSSNEWS to demonstrate that current models exhibit poor performance in genre transfer scenarios, underscoring the need for authorship models robust to genre-specific effects. We also explore SELMA, a new LLM embedding approach for large-scale authorship setups that outperforms existing models in both same-genre and cross-genre settings.

# Introduction

Accurately identifying the author of a document - also known as authorship analysis - plays a critical role in applications spanning forensic linguistics (Yang and Chow 2014), digital security, and content verification, all of which depend on identifying an author’s unique and invariant writing style markers. Existing authorship models are able to achieve near perfect performance on popular authorship benchmarks such as Enron (Klimt and Yang 2004) and IMDb (Seroussi, Zukerman, and Bohnert 2014), but these datasets are confined to a single text genre.1 Simply identifying the topic of a document and using n-gram features, for example, appears to be a reliable approximation of authorship on these datasets (Rosen-Zvi et al. 2004; Tyo, Dhingra, and Lipton 2022). This oversimplified approach limits the scope and scale of authorship models in real-world scenarios, where text documents are heterogeneous and one person may write multiple different types of texts on varied topics. Prior work studying genre transferability (Rivera-Soto et al. 2021; Sousa-Silva

![](images/c88aeb7994913fc6fcc157b7df063ef47a0e721629f6a57f060cf14b6ba1d316.jpg)  
Figure 1: Authorship Verification asks if two arbitrary texts are written by the same person. Authorship Attribution is the task of choosing the author of a document from a set of known authors. CROSSNEWS features a cross-genre setup, where texts from the same author span different genres, such as formal news articles and informal social media posts, reflecting more challenging real-world scenarios.

2018) also assumes documents in different genres are written by separate sets of authors, such that any conclusions based on the impact of changing genre is also confounded by the change of the author pool.

To address these limitations, we introduce CROSSNEWS, a dataset that connects two different genres of texts written by the same author: (i) formal, long news articles, and (ii) informal, short Twitter/X posts. CROSSNEWS consists of a silver set and a gold set. The silver set is formed by matching news articles to Wikidata entities of writers with Twitter/X accounts, while the gold set is created manually from three news agencies with document topic labels. Although previous authorship research has investigated formal writing (Zhang et al. 2018) and social media (Barbon, Igawa, and Bogaz Zarpela˜o 2017; Boenninghoff et al. 2019) separately,

<html><body><table><tr><td></td><td>Dataset</td><td>Genre(s)</td><td>Cross-Topic</td><td></td><td>Cross-Genre Train Authors Train Docs Test Authors Test Docs</td><td></td><td></td><td></td></tr><tr><td></td><td>Blogs</td><td>Online Blogs</td><td>×</td><td>×</td><td>5000</td><td>70k</td><td>5000</td><td>70k</td></tr><tr><td></td><td>MUD</td><td>Reddit Comments</td><td>×</td><td>×</td><td>100k</td><td>300k</td><td>20k</td><td>60k</td></tr><tr><td></td><td>Amazon</td><td>Online Reviews</td><td>×</td><td>×</td><td>100k</td><td>1M</td><td>35k</td><td>350k</td></tr><tr><td></td><td>PAN 2021</td><td>Fanfiction</td><td>√</td><td>×</td><td>60k</td><td>120k</td><td>30k</td><td>60k</td></tr><tr><td></td><td>PAN 2022/23</td><td>Emails,Essays, etc.</td><td>√</td><td>√</td><td>56</td><td>8k</td><td>56</td><td>8k</td></tr><tr><td></td><td>CROSSNEWS</td><td>News Articles,Tweets</td><td>√</td><td>√</td><td>2236</td><td>100k</td><td>500</td><td>20k</td></tr><tr><td></td><td>IMDb62</td><td>Movie Reviews</td><td>×</td><td>×</td><td>62</td><td>20k</td><td>62</td><td>5k</td></tr><tr><td></td><td>Blogs50</td><td>Online Blogs</td><td>×</td><td>×</td><td>50</td><td>50k</td><td>50</td><td>16k</td></tr><tr><td></td><td>CMCC</td><td>Interviews,Essays, etc.</td><td>×</td><td>√</td><td>50</td><td>4k</td><td>50</td><td>1k</td></tr><tr><td></td><td>Guardian</td><td>Book Reviews</td><td>×</td><td>√</td><td>13</td><td>333</td><td>13</td><td>111</td></tr><tr><td></td><td></td><td>CROssNEWs News Articles,Tweets</td><td>√</td><td>√</td><td>500</td><td>15k</td><td>500</td><td>7.5k</td></tr></table></body></html>

Table 1: Overview of existing authorship verification (top) and attribution (bottom) datasets.

our work is the first to examine how stylistic properties persist across both genres. Furthermore, as shown in Table 1, CROSSNEWS is an order of magnitude larger than any existing dataset for cross-genre verification and attribution.

We evaluate a wide range of authorship models on both tasks using CROSSNEWS. Our experiments show that, while prior work finds statistical learning models, such as N-gram method (Koppel and Schler 2004), achieve the state-of-theart performance in the single-genre settings (Tyo, Dhingra, and Lipton 2022), these models generalize poorly in CROSSNEWS’ cross-genre settings. Also, while LLMs perform well in small-scale attribution setups (Hung et al. 2023), existing methods generalize poorly to large-scale attribution experiments. We demonstrate a new LLM embedding approach, SELMA, which achieves state-of-the-art accuracy for authorship verification and attribution without requiring additional training. Furthermore, while there is a general consensus that verification and attribution are closely linked, little work has been done to compare the tasks side by side. We find that topic impacts verification and attribution in contrasting ways. In summary, our main contributions are:

• The creation of CROSSNEWS, the largest cross-domain authorship dataset for both verification and attribution. • A zero-shot LLM-based method for verification and attribution, SELMA, that utilizes instruction-tuned embeddings with task-specific prompts. • A combined analysis of experiments on both the verification and attribution tasks, which have been frequently studied individually but rarely in conjunction. • Authorship experiments of the effects of genre and topic. • Publicly accessible code to support future research.2

# Related Work

Authorship analysis aims to identify unique signals of individual authors.3 There are two main tasks: (i) verification, where models determine if two documents are by the same or different authors, and (ii) attribution, where models assign documents to one author among a set of known authors (see

Figure 1). For authorship verification, the PAN organization4 provides the most widely used benchmarks. The verification datasets from both PAN 2020 (Kestemont et al. 2020) and 2021 (Kestemont et al. 2021) contain texts crawled from FanFiction.5 While the original 2021 test set was designed to be harder than the 2020 dataset, top-performing models from the PAN competition actually see better results on the 2021 test set. However, Brad et al. (2022) recreated the PAN 2021 setup with different splits and yielded the opposite result, underscoring that model performance tends to be dataset-specific and not generalizable. In addition, RiveraSoto et al. (2021) adapted the Amazon review dataset (Ni, Li, and McAuley 2019) and Reddit Million User Dataset (MUD) (Khan et al. 2021), which were not originally verification datasets, to the verification task by sampling text pairs from the most active users. Standard authorship attribution benchmarks include the IMDb62 (Seroussi, Zukerman, and Bohnert 2011) and Blogs50 (Schler et al. 2006) dataset, which contain documents from a small number of prolific online writers. SOTA models perform very well on IMDb62 $( 9 8 \% )$ and Blogs50 $( 7 5 \% )$ (Tyo, Dhingra, and Lipton 2022), highlighting the lack of challenging attribution datasets.

Authorship datasets typically consist of a single genre due to the difficulty of linking authors across genres, as unique identifiers in one data source do not align with author labels in other sources. As a result, cross-genre authorship datasets are scarce and very limited in size. For attribution, the CMCC dataset, (Goldstein-Stewart et al. 2008) consisting of a collection of interviews, written essays, and emails from 21 authors on six sociopolitical topics, and the Guardian opinion dataset (Stamatatos 2013) contain 756 and 444 documents, respectively. For verification, the PAN 2022 and 2023 verification tasks (Stamatatos et al. 2022, 2023) use documents written by 112 individuals in four different genres (i.e., essays, emails, texts, and memos) from the Aston 100-Idiolects dataset (Heini, Kredens, and Pezik 2021). However, these verification and attribution datasets are too small to leverage modern techniques of authorship analysis (e.g., embedding-based methods). To address this need, we introduce the large-scale CROSSNEWS dataset.

# The CROSSNEWS Dataset

Cross-genre authorship datasets are particularly hard to construct because authors often change their pen names, and usernames on social media may not match real names. Because of this difficulty, we collect two separate sets of data - a large, silver set from automated Wikidata-Twitter linkages, and a smaller, gold set collected manually. Combined, they make CROSSNEWS the largest cross-genre authorship dataset to our knowledge (See Table 1).

# Silver Set Construction

Data Collection We utilized Wikidata (Vrandecˇic´ and Kr¨otzsch 2014), a knowledge base derived from Wikipedia. We queried Wikidata for all entities who are Englishspeaking journalists, have Twitter/X accounts and have written for free public news websites. We linked entities to the articles they authored from these websites. Then, we extracted the author names from the articles and matched their names to possible Wikidata entities. We define the two names as matching if they have a Jaro-Winkler similarity (Winkler 1990) of above 0.95. After retrieving article texts for these authors, we removed duplicate articles via an LSH filter (Indyk and Motwani 1998) on the body text. For each Wikidata entity that had at least one linked author, we pulled their tweets via the Twitter/X API6. We capped the number of tweets written per author to 600 and the number of articles written per author to 200. This process yielded 2,260 journalists with a total of 65,589 articles (articles per author: mean 29.1, median 6) and 1,083,221 tweets (tweets per author: mean 479.3, median 599), which constitutes the silver data portion of CROSSNEWS.

Manual Verification To verify the accuracy of the above approach, we hired three annotators to manually link 300 journalists and their articles to their corresponding Wikidata entry, if it existed. All of the annotators had a college-level education. Annotators compared the journalist’s name and news articles side by side with a list of potential Wikidata entries and selected the entry that corresponded to the journalist, or None or Unsure. The three annotators had a Fleiss’ Kappa (Fleiss 1971) of 0.878. From the 300-journalist sample we estimate the automatic linkage method has a linkage accuracy of $9 3 . 6 \%$ .

# Gold Set Construction

We constructed a manual gold set for evaluation by selecting authors from the New York Times (NYT), the Guardian, and the Times of India, three news organizations not in the silver set. We randomly chose authors who have written at least 100 articles, manually identified their Twitter/X handle, and collected their tweets via the Twitter/X API. We collected 175 NYT, 155 Guardian, and 170 Times of India journalists. For each of the 500 total authors, we collected 100 tweets and 100 articles for a total of 100,000 gold documents. For text processing, we removed location tags and author bylines. Each article is labeled as one of five topics (politics, economy, sports, culture, other) based on the news

sports (7.7%) culture economy (ri2ke) (1.nis (16.1%) (8.2%) (2.3e) ther (sports) (5g) (4.9%) $( 6 . 8 \% )$ tech other (culture) $( 3 . 3 \% )$ (3.9%) politics other (60.7%) (8.4%) global politics interest $( 1 1 . 6 \% )$ (5.3%) national politics health $( 2 8 . 6 \% )$ (0.9%) local politics home $( 2 0 . 5 \% )$ $( 2 . 2 \% )$

organization’s categories. Additionally, we manually annotated 2000 articles with more fine-grained topic labels within each topic category (see Figure 2). The most frequent article topic is politics with $6 0 . 7 \%$ of articles. The vast majority $( 9 7 . 6 \% )$ of authors primarily write articles on one topic.

# Authorship Models

We benchmark three types of state-of-the-art authorship models on the CROSSNEWS dataset: (i) non-Transformer models, (ii) embedding methods, and (iii) LLM prompting.

# Non-Transformer Methods

Unlike other areas of NLP where Transformer models dominate, authorship methods that do not utilize neural networks are still very competitive (Tyo, Dhingra, and Lipton 2022). We consider N-gram, PPM, and O2D2, all three of which have been top-performing models on previous authorship benchmarks (Tyo, Dhingra, and Lipton 2022; Neal et al. 2017; Kestemont et al. 2021).

N-gram (Koppel and Schler 2004) This method constructs character, part of speech, and n-grams features to train a logistic classifier, and is reported to outperform modern verification models (Tyo, Dhingra, and Lipton 2022). In the authorship verification task, N-gram creates a single model that runs binary logistic regression on the difference in features between the two texts, while in the attribution task, one model is made per author and the prediction is selected via the max of these models.

PPM (Teahan and Harper 2003) Prediction by Partial Matching (PPM) predicts future words based on the context of previously seen words via hierarchical word probability graphs. For verification, PPM creates a compression model for a single text and applies it to the second, calculating cross-entropy between the prediction and the true text. For attribution, a compression model is calculated for each author based on their known texts, and test documents are labeled as the author whose model produces the lowest cross-entropy.

O2D2 (Boenninghoff, Nickel, and Kolossa 2021) OutOf-Distribution Detector model was the highest-performing model in the PAN 2021 verification task event (Kestemont et al. 2021). This method first produces authorship embeddings from a trained LSTM model and modifies the final result based on uncertainty modeling and Bayes factors from the underlying training text distribution.

# Embedding Methods

Authorship embedding models use contrastive learning (Goldberger et al. 2004) to encode representations of documents such that the similarity between two documents composed by the same author in a vector space is maximized. Embedding models can be used for both verification and attribution. For verification, embedding methods classify pairs based on a specific threshold of the cosine similarity between the two documents, where the similarity threshold is calculated as the value that classifies the most number of correct labels on the validation set. For attribution, embedding models create author embeddings by averaging all of the known document embeddings of a given author, then match an unknown document embedding to the closest author embedding via cosine similarity. We consider the PPM, LUAR, and STEL embedding models. For all embedding models, we use a pre-trained RoBERTa (Liu et al. 2019) encoder and train further on CROSSNEWS’ silver set.

PART (Huertas-Tato et al. 2022) Pre-trained Authorship Representation Transformer (PART) uses a Transformer encoder to initially embed the text document into a sequence of semantic word embeddings, then trains an LSTM to create a style embedding from the word embeddings via a contrastive loss function. Notably, the Transformer encoder is frozen, so the LSTM itself is the only trainable part of the architecture.

LUAR (Rivera-Soto et al. 2021) Learning Universal Authorship Representations (LUAR) also contrastively learns authorship embeddings by finetuning a Transformer encoder. LUAR samples many windows of 32-token excerpts across a single document (for verification) or multiple documents (for attribution) to encode, then applies selfattention to these window embeddings to produce a single embedding. LUAR is trained to create authorship embeddings from a variable number of windows to improve model generalization for arbitrary document numbers and lengths.

STEL (Wegmann, Schraagen, and Nguyen 2022) STyle Evaluation framework model (STEL) fine-tunes the Transformer encoder with a contrastive loss, but designs training data to create data pairs that discuss the same topic with the aim to disentangle writing style from topic association.

# Zero-shot LLM Methods

Given its effectiveness, LLM prompting has been recently applied to authorship tasks. These recent works, including PromptAV (Hung et al. 2023) and LIP (Huang, Chen, and Shu 2024), have achieved considerable success on the verification task; however, prompting LLM for the attribution task has the key limitation that performance degrades quickly once a single prompt contains too many documents. While

Huang, Chen, and Shu (2024) reported competitive performance by LLM prompting for attribution, their experiments contained no more than 20 authors with a single document per author. Applying their methods to CROSSNEWS’ 500- author attribution setup barely outperforms a random baseline. To handle an arbitrarily large number of documents and authors, we explore a new method that combines LLM embeddings with prompting, namely Style Embeddings from Lanuage Models for Authorship (SELMA). We describe three prompting methods (for verification) and SELMA (for verification and attribution) below.

Task Description Only (TaskOnly) Prompt This baseline prompting method involves only providing the task description in the input prompt and querying the model for the result. We adopt the task description from Huang, Chen, and Shu (2024) and construct the input prompt for the verification task as follows:

Verify if two input texts were written by the same author. Provide your answer simply with True or False.   
Input Text 1: (text 1)   
Input Text 2: (text 2)   
Answer:

PromptAV (Hung et al. 2023) Besides the task description, this method includes the eight most relevant stylistic variables for the model to attend to in the input prompt, such as special characters and punctuation style and prompts the model to use Chain-of-Thought prompting (Wei et al. 2023).

LIP (Huang, Chen, and Shu 2024) Linguistically Informed Prompting (LIP) describes the task in the input prompt, and explicitly provides examples of stylistic markers for the model to attend to, while asking the model to describe the different writing styles of the two authors.

SELMA (this work) In our SELMA method, we utilize an instruction-tuned LLM specifically designed for text embeddings to encode unknown and known documents into a shared embedding space to measure stylistic similarity. In particular, we use $\mathtt { e 5 }$ -mistral-7b-instruct (Wang et al. 2024), which was instruction-tuned on the sentence similarity embedding task by synthetically generating text retrieval data by querying LLMs. SELMA compares document embeddings in pairs, where one document has an instruction concatenated before the text and the other document does not. An [EOS] token is also appended to this input to be fed into the Transformer-based LLM, and its corresponding embedding is extracted from the final layer’s [EOS] position. We use the following instruction:

# Instruct: Retrieve stylistically similar text. Query: (text)

For each verification task pair, one document is chosen randomly and embedded with the instruction prepended, and the other is embedded without instruction. The documents are classified via cosine similarity following the same procedure as the other embedding methods. For attribution, the test query document is embedded with the instruction prepended and each of the known reference documents are

<html><body><table><tr><td rowspan="2"></td><td rowspan="2">Model</td><td rowspan="2">Train Tyere</td><td colspan="6">Test Genre Pair Type</td></tr><tr><td colspan="2">Artile-Aril t</td><td colspan="2">ATwet-weF1↑</td><td colspan="2">Aricl-T1↑</td></tr><tr><td></td><td>Random Baseline</td><td>-</td><td>50.0</td><td>50.0</td><td>50.0</td><td>50.0</td><td>50.0</td><td>50.0</td></tr><tr><td rowspan="8"></td><td rowspan="4">N-Gram (Tyo et al. 2022)</td><td>Article-Article</td><td>69.5±3.10</td><td>73.1±1.59</td><td>49.5±1.64</td><td>66.3±0.35</td><td>52.4±2.01</td><td>60.1±4.75</td></tr><tr><td>Tweet-Tweet</td><td>51.1±0.04</td><td>66.6±0.02</td><td>69.4±2.58</td><td>71.9±2.04</td><td>52.5±0.28</td><td>51.2±2.22</td></tr><tr><td>Article-Tweet</td><td>58.3±1.70</td><td>69.5±0.56</td><td>55.9±1.99</td><td>67.6±0.38</td><td>55.8±1.49</td><td>62.2±1.85</td></tr><tr><td>Article- Article</td><td>67.5±1.03</td><td>62.8±0.19</td><td>61.2±0.13</td><td>38.4±1.35</td><td>53.4±0.27</td><td>17.7 ±0.18</td></tr><tr><td rowspan="3">PPM(Teahan and Harper 2003)</td><td>Tweet-Tweet</td><td>48.1±1.47</td><td>66.9±0.37</td><td>70.2±0.44</td><td>68.7±0.17</td><td>52.5±1.42</td><td>54.9±0.25</td></tr><tr><td>Article-Tweet</td><td>50.2±0.70</td><td>66.9±1.12</td><td>68.9±1.23</td><td>69.2±0.01</td><td>54.7±1.22</td><td>55.1±0.12</td></tr><tr><td>Article-Article</td><td>69.8±0.86</td><td>73.9±0.25</td><td>53.9±1.36</td><td>63.9±0.13</td><td>57.0±1.05</td><td>55.3±2.29</td></tr><tr><td rowspan="3">O2D2 (Boenninghoff et al.2021)</td><td>Tweet-Tweet</td><td>69.8±0.75</td><td>75.8±0.48</td><td>61.0±0.48</td><td>70.0±0.12</td><td>63.1±0.02</td><td>63.4±0.72</td></tr><tr><td>Article-Tweet</td><td>65.6±0.82</td><td>73.4±0.19</td><td>62.6±0.44</td><td>68.6±0.12</td><td>61.9±0.46</td><td>68.3±0.27</td></tr><tr><td></td><td>70.0±0.55</td><td>69.5±0.52</td><td>66.9±0.82</td><td>60.5±0.74</td><td>58.9±0.72</td><td>40.0±0.55</td></tr><tr><td rowspan="6"></td><td rowspan="3">PART(Huertas-Tato et al.2022)</td><td>Arte-Artile</td><td>76.8±0.09</td><td>78.4±0.83</td><td>67.1±0.00</td><td>66.0±0.83</td><td>61.1±1.45</td><td>55.0±0.52</td></tr><tr><td>Article-Tweet</td><td>72.9±1.16</td><td>78.5±0.74</td><td>59.0±1.03</td><td>69.6±1.41</td><td>63.7±1.16</td><td>69.8±0.91</td></tr><tr><td>Article-Article</td><td>73.7±1.84</td><td>70.8±3.38</td><td>61.1±2.98</td><td>66.2±1.93</td><td>59.8±1.15</td><td>58.8±5.19</td></tr><tr><td rowspan="3">LUAR (Rivera-Soto et al.2021)</td><td>Tweet-Tweet</td><td>68.6±1.17</td><td>70.9±5.28</td><td>72.9±4.50</td><td>70.8±6.34</td><td>63.7±3.72</td><td>61.1±9.56</td></tr><tr><td>Article-Tweet</td><td>75.52±3.32</td><td>74.7±2.78</td><td>69.61±3.47</td><td>68.47±2.74</td><td>67.8±3.02</td><td>66.8±3.64</td></tr><tr><td>Article-Article</td><td>54.2±6.02</td><td>63.3±1.91</td><td>49.4±2.06</td><td>65.4±0.78</td><td>48.2±1.14</td><td>58.0±7.89</td></tr><tr><td rowspan="4">LLMr-3pting</td><td rowspan="3">STEL (Wegmann et al. 2022)</td><td>Tweet-Tweet</td><td>52.8±4.01</td><td>63.7±0.36</td><td>50.3±3.47</td><td>62.6±3.90</td><td>50.6±1.66</td><td>61.6±6.21</td></tr><tr><td>Article-Tweet</td><td>51.0±4.42</td><td>21.3±30.77</td><td>50.7±1.76</td><td>20.8±30.03</td><td>48.6±1.00</td><td>19.6±2.76</td></tr><tr><td></td><td>58.6±2.65</td><td>38.1±1.28</td><td>73.1±0.99</td><td>57.5±0.96</td><td>48.8±0.48</td><td>6.12±2.33</td></tr><tr><td></td><td></td><td>76.1±1.18</td><td>77.2±0.89</td><td>84.9±0.93</td><td>79.1±0.48</td><td>58.9±0.93</td><td>36.9±0.64</td></tr><tr><td rowspan="4">SELMA</td><td rowspan="4">No Prompt</td><td>---</td><td>73.7±1.13</td><td>77.1±0.09</td><td>82.0±1.93</td><td>78.4±0.95</td><td>64.0±1.08</td><td>59.3±1.70</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>77.1±0.28 85.8±0.27</td><td>80.9±0.38</td><td>53.3±0.64</td><td>68.4±0.55</td><td>64.3±0.25 78.5±0.12</td><td>73.3±0.38 79.8±0.23</td></tr><tr><td>Task Description Only (TaskOnly)</td><td></td><td>86.0±0.18</td><td>66.1±0.34</td><td>74.6±0.2</td><td></td><td></td></tr><tr><td rowspan="4">Mistral-7B</td><td>LIPrHuatAy nta224)</td><td>--</td><td>85.8±0.18</td><td>85.8±0.18</td><td>65.0±0.24</td><td>73.8±0.16</td><td>78.1±0.29</td><td>79.3±0.23</td></tr><tr><td></td><td></td><td>82.5±0.21</td><td>84.2±0.16</td><td>58.8±0.38</td><td>70.6±0.05</td><td>72.4±0.36</td><td>77.0±0.24</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>

Table 2: Accuracy and F1 of non-Transformer and Embedding-based models (all trained on the silver data in CrossNews), as well as zero-shot LLM prompting and SELMA, on the three test pair types in CROSSNEWS for authorship verification. Darker cell colors indicate better performance and bold fonts indicate the best within a column.

embedded without instruction. An author embedding is created by averaging the embeddings of the known documents, and the test embedding is compared to each candidate author embedding via cosine similarity to rank and identify the most similar author.

We also conduct experiments of SELMA with different instructions in the prompt. We consider no instruction (SELMA $+ ~ \mathrm { N o }$ Prompt), with only the task description (SELMA $^ +$ TaskOnly), and by adopting the prompts that were originally designed for authorship verification in PromptAV (SELMA $^ +$ PromptAV) and LIP (SELMA $+ \mathrm { L I P } _ { , } ^ { \cdot }$ ).

# Authorship Verification Experiments

We formulate the authorship verification task as a binary classification problem: given two documents, output 1 if they share the same author and 0 otherwise.

# Experiment Setup

To prepare data for the task, we sample document pairs from CROSSNEWS to form positive (if two documents have the same author) and negative (if two documents have different authors) examples. To balance between data efficiency and diversity, following (Stamatatos et al. 2022, 2023; Hu et al. 2023), we ensure each document is present in exactly one negative and one positive pair, and the number of positive and negative pairs are equal. We create three different train and test sets based on the genres of the two documents in a verification pair: (1) Article-Article: both documents are news articles, (2) Tweet-Tweet: both documents are tweets, and (3) Article-Tweet: one document is an article and the other one is a tweet. For each genre pair type, we sample 100,000 document pairs from CROSSNEWS’ silver set, with an average of 67 pairs per train author, and 20,000 document pairs from CROSSNEWS gold set, with an average of 60 pairs per test author. The document pairs from the silver set are then used to form train and validation sets, with the ratio of 8:2, while the pairs from the gold set are used to construct the test set. For evaluation, we report accuracy and F1, as this combination informs the overall predictive power and bias towards predicting one class. Given that authorship performance is highly correlated to input text length (Koppel, Schler, and Argamon 2011; Eder 2013), following Stamatatos et al. (2022); Embarcadero-Ruiz et al. (2022), we concatenate tweets to a minimum length of 500 characters.

For all experiments in this paper, models are run on a single NVIDIA A40 GPU, except for LLaMA-3-70B for prompting, which is run on six A40’s, with a total compute time of approximately 400 hours to train and evaluate all verification and attribution models sequentially. Experiment results are averaged over five runs with different random seeds.

# Results

The model performance for each combination of train/test genre pair types is reported in Table 2. Overall, models perform best on the same-genre test pairs (Article-Article and Tweet-Tweet) and perform worst on Article-Tweet pairs. These results are consistent with previous findings that models have a hard time with cross-genre generalization (RiveraSoto et al. 2021; Wang et al. 2023). Both N-gram and PPM, two of the non-Transformer methods, see large dropoffs in performance in cross-genre settings, marginally improving over the random baseline in terms of accuracy on the ArticleTweet test pairs. To explain this performance drop, we compare the important N-gram features between the ArticleArticle and Tweet-Tweet models (Table 4). We find the model attends to punctuation that is commonplace to specific genres, such as “- , :” for articles, and “#, $@$ , !” for tweets, which limits the models’ transferability to other genres. O2D2, however, sees better performance on the ArticleTweet test pairs as it continuously re-samples its training data to prepare for domain shifts in testing. Meanwhile, the PART and LUAR embedding models perform better across more test and train setups. STEL, however, predicts almost every label as “different author”, yielding its close to or worse than random baseline accuracy. For LLM-based approaches, Prompting $+ \ \mathrm { L I P }$ notably achieves state-of-art accuracy of 84.9 on the Tweet-Tweet pairs. SELMA achieves the best accuracy on the Article-Article and Article-Tweet setups, outperforming LLM Prompting despite the underlying LLM having ten times fewer parameters. This indicates that comparing documents in a latent embedding space may be a better approach to capturing stylometric features than directly prompting the LLM for a comparison.

Table 3: Results for the four Known-Unknown Genre combinations for the authorship attribution experiment on CROSSNEWS, consisting of all 500 authors in the gold set with 30 known documents per author. Avg. Rank is displayed in the format of (Median Rank)/(Mean Rank). Darker cell colors indicate better performance and bold fonts indicate the best within a column.   

<html><body><table><tr><td rowspan="3">Model</td><td colspan="10">Known Genre-Unknown Genre</td></tr><tr><td colspan="3">Article-Article</td><td colspan="3">Article-Tweet</td><td colspan="3">Tweet-Article</td><td colspan="3">Tweet-Tweet</td></tr><tr><td>Acc ↑</td><td>R@8↑</td><td>Avg. Rank</td><td>Acc ↑</td><td>R@8↑</td><td>Avg. Rank</td><td>Acc ↑</td><td>R@8↑</td><td>Avg. Rank</td><td>Acc ↑</td><td>R@8 ↑</td><td>Avg. Rank </td></tr><tr><td>Random Baseline</td><td>0.2</td><td>1.6</td><td>250/250</td><td>0.2</td><td>1.6</td><td>250/250</td><td>0.2</td><td>1.6</td><td>250/250</td><td>0.2</td><td>1.6</td><td>250/250</td></tr><tr><td>N-gram</td><td>61.4</td><td>84.7</td><td>1/9</td><td>8.12</td><td>20.9</td><td>101/151</td><td>3.45</td><td>10.4</td><td>160/182</td><td>24.4</td><td>43.6</td><td>14/62</td></tr><tr><td>PPM</td><td>50.8</td><td>71.6</td><td>1/36</td><td>7.01</td><td>19.1</td><td>89/144</td><td>7.59</td><td>19.6</td><td>107/149</td><td>32.0</td><td>49.8</td><td>9/64</td></tr><tr><td>PART</td><td>26.0</td><td>61.4</td><td>5/17</td><td>2.81</td><td>12.6</td><td>88/133</td><td>7.61</td><td>28.4</td><td>28/50</td><td>28.0</td><td>55.9</td><td>6/31</td></tr><tr><td>LUAR</td><td>28.3</td><td>61.3</td><td>4/22</td><td>7.48</td><td>23.6</td><td>51/104</td><td>8.32</td><td>26.0</td><td>40/92</td><td>19.0</td><td>41.61</td><td>16/57</td></tr><tr><td>STEL</td><td>1.91</td><td>9.47</td><td>84/116</td><td>0.35</td><td>2.05</td><td>235/238</td><td>0.75</td><td>4.15</td><td>181/204</td><td>1.24</td><td>6.96</td><td>139/173</td></tr><tr><td>SELMA + No Prompt</td><td>52.8</td><td>80.2</td><td>1/7</td><td>15.8</td><td>38.6</td><td>19/70</td><td>18.7</td><td>47.8</td><td>9/40</td><td>31.0</td><td>55.9</td><td>6/40</td></tr><tr><td>SELMA+ TaskOnly</td><td>56.9</td><td>87.9</td><td>1/5</td><td>18.4</td><td>42.2</td><td>15/69</td><td>20.1</td><td>50.1</td><td>8/40</td><td>33.3</td><td>59.4</td><td>4/31</td></tr><tr><td>SELMA+PromptAV</td><td>55.8</td><td>87.3</td><td>1/6</td><td>17.8</td><td>41.5</td><td>16/70</td><td>20.3</td><td>49.8</td><td>9/41</td><td>31.1</td><td>56.4</td><td>5/35</td></tr><tr><td>SELMA +LIP</td><td>55.6</td><td>86.1</td><td>1/6</td><td>15.7</td><td>37.8</td><td>20/72</td><td>21.5</td><td>51.3</td><td>8/38</td><td>31.2</td><td>56.0</td><td>5/36</td></tr></table></body></html>

Table 4: Ten most important N-gram features for the ArticleArticle and Tweet-Tweet N-gram verification model, ordered by magnitude of feature weight.   

<html><body><table><tr><td colspan="3">Article Features</td><td colspan="3">Tweet Features</td></tr><tr><td>Type</td><td>Feature</td><td>Weight</td><td>Type</td><td>Feature</td><td>Weight</td></tr><tr><td>Word 1-gram</td><td>said</td><td>0.215</td><td>Char 1-gram</td><td>#</td><td>0.532</td></tr><tr><td>Word 1-gram</td><td>trump</td><td>0.198</td><td>Char 1-gram</td><td>，</td><td>0.400</td></tr><tr><td>Char 3-gram</td><td>gam</td><td>0.187</td><td>Char 2-gram</td><td>：</td><td>0.374</td></tr><tr><td>POS 1-gram</td><td>：；</td><td>0.168</td><td>Char 2-gram</td><td>.[space]</td><td>0.343</td></tr><tr><td>Char 1-gram</td><td></td><td>0.167</td><td>Char 1-gram</td><td>@</td><td>0.325</td></tr><tr><td>Char 1-gram</td><td>0</td><td>0.166</td><td>Char 1-gram</td><td>！</td><td>0.321</td></tr><tr><td>POS 1-gram</td><td>NNPS</td><td>0.165</td><td>Char 2-gram</td><td>,[space]</td><td>0.266</td></tr><tr><td>Word 1-gram</td><td>says</td><td>0.164</td><td>Char 1-gram</td><td>；</td><td>0.258</td></tr><tr><td>Char 1-gram</td><td>；</td><td>0.156</td><td>Char 3-gram</td><td>：</td><td>0.229</td></tr><tr><td>Char 2-gram</td><td>-[space]</td><td>0.151</td><td>Char 3-gram</td><td>tps</td><td>0.213</td></tr></table></body></html>

# Authorship Attribution Experiments

The Authorship Attribution task aims to determine the most likely author of a document from a predefined set of authors.

# Experiment Setup

For this task, we use only the gold set from CROSSNEWS, as both the training text (known-authorship documents) and test text (unknown-authorship documents) must come from the same author pool. Our attribution setup contains all 500 gold authors with 30 known documents and 15 unknown documents per author. We create a set of known and unknown documents for both the Article and Tweet Genre, resulting in four separate evaluation setups for each combination of known-unknown document sets. We report accuracy, $\mathbb { R } \ @ 8$ (the probability the correct author appears in the top 8 predicted authors), and the average rank of the true author (median and mean).

# Results

Attribution results are presented in Table 3. Similar to verification, models perform much better in single-genre settings than in cross-genre settings. While N-gram performs the best in the Article-Article setup, it does not perform as well in cross-genre setups as grammar structures and vocabulary, the two main feature groups that N-gram utilizes, change across genres. Meanwhile, for embedding models, PART and LUAR perform better for both cross-genre setups, with $\operatorname { R @ 8 s }$ of 28.4 and 26.0 for the Tweet-Article setup. Besides the accuracy of the Article-Article setup, SELMA outperforms all other models, particularly in cross-genre settings. For LLMs, we see that, as opposed to verification, prompt choice does not impact performance very much. This could be because the verification task only has 2 documents per prompt, so prompt engineering to mention specific stylistic aspects has a large influence on model predictions; attribution, on the other hand, has many more document references to disambiguate style without the need for explicit prompting. Finally, despite the dip in performance between the same-genre and cross-genre settings, all models still perform better than the random baseline. This observation indicates that markers of authorship are present across genres, and that strong identification of one genre has transferable properties to other unseen genres.

Table 5: Accuracy for verification and attribution on two Article-Article test sets, one containing documents from all topics and the other containing documents from a single topic (global politics). Change in accuracy between test sets (All Topics and Single Topic) is shown in color. The verification task uses LLM prompting and attribution uses SELMA.   

<html><body><table><tr><td rowspan="2">Article Setup</td><td colspan="2">Verification</td><td colspan="2">Attribution</td></tr><tr><td>All Topics</td><td>Single Topic</td><td>All Topics</td><td>Single Topic</td></tr><tr><td>N-gram</td><td>70.4</td><td>64.5 (-5.9%)</td><td>45.6</td><td>65.8 (+20.2%)</td></tr><tr><td>PPM</td><td>67.9</td><td>58.9 (-9.0%)</td><td>39.0</td><td>45.5 (+6.5%)</td></tr><tr><td>PART</td><td>70.3</td><td>64.9 (-5.4%)</td><td>32.6</td><td>39.5 (+6.9%)</td></tr><tr><td>LUAR</td><td>73.9</td><td>70.9 (-3.0%)</td><td>38.5</td><td>43.1 (+4.6%)</td></tr><tr><td>STEL</td><td>54.3</td><td>58.5 (+4.2%)</td><td>4.04</td><td>5.04 (+1.0%)</td></tr><tr><td>TaskOnly</td><td>73.0</td><td>70.4 (-2.6%)</td><td>64.4</td><td>52.2 (-12.2%)</td></tr><tr><td>PromptAV/AA</td><td>76.1</td><td>72.3 (-3.8%)</td><td>60.0</td><td>50.3 (-9.7%)</td></tr><tr><td>LIP</td><td>73.7</td><td>65.5 (-8.2%)</td><td>54.3</td><td>44.1 (-10.2%)</td></tr></table></body></html>

# The Influence of Topic on Model Performance

In addition to genre, topic diversity significantly influences performance (Kestemont et al. 2021; Khan et al. 2021; Stamatatos 2018; Wang et al. 2023). To test this, we create two test sets of documents for both news articles and tweets - one containing 75 random authors from the CROSSNEWS gold set with documents from all topics, and one with 75 authors who wrote primarily in “Global Politics”, with only the “Global Politics” documents selected. We present model accuracy in Tables 5 and 6. For verification, models tend to perform worse when all pairs contain the same topic, while in attribution, models tend to perform better. This could be because for verification, models have no previous context for any other documents written and have much less data to work with. As a result, these models tend to rely on topic first as a predictor of authorship, so models will over-classify matching topic as matching authorship. This is especially pronounced in the Tweet setup - models perform significantly worse when both tweets are on the same topic. Individual tweets contain much less information than articles, so models rely more on non-stylistic information like topic.

However, for attribution, non-LLM models perform much better in the same-topic context when compared to an alltopic dataset. Previous literature has assumed that authorship verification and attribution are very closely related (Tyo, Dhingra, and Lipton 2022; Koppel et al. 2012), which would imply that isolating the topic effect would produce the same change in performance for both attribution and verification. However, the increase in accuracy in attribution for the article experiment indicates that current models utilize topic differently between verification and attribution. While verification models only have access to two documents, attribution models are exposed to many more documents per author. Because topic diversity adds variability to many features these models use, such as vocabulary and frequently occurring phrases, when topics are isolated, attribution models are able to better distinguish between authors. The size of the all-topic document vocabulary was $5 3 \%$ larger than the single-topic vocabulary. This explains why the top-performing attribution model, N-gram, sees a $2 0 . 2 \%$ increase in accuracy between the all-topic and singletopic setup, as unique word n-grams that correlate highly with specific authors are much more likely to occur with a smaller vocabulary size. A notable exception is that LLMs perform worse in the Attribution setup for the Single Topic dataset. Looking to the Tweet setup, the relative accuracy difference between the All Topic dataset and Single Topic dataset is much better for the Attribution setup than the Verification setup. This provides further evidence that models are much more sensitive to a topic in verification setups as opposed to attribution setups.

Table 6: Accuracy for verification and attribution on two Tweet-Tweet test sets, one containing documents from all topics and the other containing documents from a single topic (global politics). Change in accuracy between test sets (All Topics and Single Topic) is shown in color. The verification task uses LLM prompting and attribution uses SELMA.   

<html><body><table><tr><td rowspan="2">Tweet Setup</td><td colspan="2">Verification</td><td colspan="2">Attribution</td></tr><tr><td>All Topics</td><td>Single Topic</td><td>All Topics</td><td>Single Topic</td></tr><tr><td rowspan="2">N-gram PPM</td><td>69.4</td><td>52.2 (-17.2%)</td><td>38.7</td><td>37.1 (-1.6%)</td></tr><tr><td>70.2</td><td>57.9 (-12.3%)</td><td>44.9</td><td>42.5 (-2.4%)</td></tr><tr><td rowspan="3">PART LUAR</td><td>67.1</td><td>64.4 (-2.7%)</td><td>48.9</td><td>48.2 (-0.6%)</td></tr><tr><td>72.9</td><td>61.8 (-11.1%)</td><td>34.7</td><td>36.1 (+1.4%)</td></tr><tr><td>50.3</td><td>53.3 (+3.0%)</td><td>4.71</td><td>3.91 (-0.8%)</td></tr><tr><td rowspan="2">TaskOnly PromptAV/AA</td><td>82.8</td><td>67.3 (-15.5%)</td><td>52.4</td><td>49.7 (-2.7%)</td></tr><tr><td>84.9</td><td>65.9 (-19.0%)</td><td>42.0</td><td>42.6 (+0.6%)</td></tr><tr><td>LIP</td><td>82.0</td><td>64.6 (-17.4%)</td><td>34.7</td><td>40.4 (+5.7%)</td></tr></table></body></html>

# Conclusion and Future Work

In this work, we present CROSSNEWS, the largest crossgenre authorship dataset. Evaluations of authorship models on CROSSNEWS show existing models perform poorly in genre transfer setups. Additionally, investigations into topic show that verification and attribution models process document topicality differently, a departure from existing authorship literature that suggests that verification and attribution models behave similarly to each other. Our findings show that future work should focus on building generalizable authorship models that explicitly avoid domain-specific signals. Moreover, different training approaches should be investigated that contain a wide range of train genre combinations. Finally, we demonstrate the feasibility of a new zeroshot LLM-based approach, SELMA, which outperforms all other models on CROSSNEWS. These promising results indicate that the future of authorship may lie with LLMs capable of robustly differentiating between genres and domains.